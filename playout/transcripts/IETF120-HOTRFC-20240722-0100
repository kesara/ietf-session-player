[
  {
    "startTime": "00:00:01",
    "text": "We are, oops Since this is a very time-constrained activity, we will start very promptly I am Pete. I am your lovely host for the evening, which means I just get to interesting you to all the fun stuff we're going to do tonight So welcome to Hot RFC. This is an IETF activity. Therefore, it falls under what we call the note well if you are a new participant This means that by participating in this group, you are agreeing to participate under all of our rules and guidelines There's a lovely list of them here If you are in particular aware of intellectual property contributions that need declarations, you have a responsibility to do that. If you do not know what those obligations are, you might want to read over the documents listed down at the bottom of the slide there Intellectual property is not the only thing we care about. You understand that there are going to be video and audio recordings of this, that personal information will be handled in accordance with the privacy policy and that you agree to work respectfully with other participants. If you have questions about these things, let someone know and go chat about them outside of the room Next slide So we do want to remind you that all of these activities that were engaged in here are under the IETF Guidelines for Conduct the IETF anti-arassment policy and procedures If you have any concerns about any behavior that you see, please let the appropriate folks know I happen to be on the Ombuds team. You're welcome to contact me. You are also welcome to contact any of the leadership who can help you out with that stuff. We're going to try and maintain an environment"
  },
  {
    "startTime": "00:02:01",
    "text": "in which people from different backgrounds and identity are treated with dignity, decency, and respect So please do You must not engage in harassment and if you believe you have been harassed or notice someone else who has been harassed, please do contact the Ombuds team All right. As far as participation for in-room folks, please make sure to sign into the session You can use the little phone app or you can do it on your laptop, but please do turn off your sound so the poor folks remote don't get weird feet feedback. You can join with the Meetecho Light Clown And you do want to use the join the Q in join with the Meetecho Light client. And you do want to use the join the queue if we were having questions, which we were not in this session So don't worry about that. Again, keep the audio and video off if you're on site. For remote participants, please make sure that your audio and video are off unless you are recognized by the chair to present something. And please you a headset if you're remote because that will make life much easier to hear you. So here are the ground rules. Hot RFC is how you make a request for conversation so these are short talks that will in invite people to talk to you about topics you're interested in. Each person gets exact four minutes from go to please applaud. You guys are going to do the applauding part. At four minutes, we will start applauding immediately Ready? Everybody, please applause Okay. So, um yes, four minutes is really the limit We do want to get out of here in a reasonable amount of time We don't do questions in here, but each person will provide follow-up info, and in-person attendees can follow"
  },
  {
    "startTime": "00:04:01",
    "text": "presenters to the bar or wherever else you want to have a quick chat with people about their topic So you can follow along. We've got all of the slides for the presenters in the Datatracker so you can go take a look at those slides as you see fit So are we ready? All right. Please applaud it's a little good we get started and our first person up to bat is Jean You ready? And the word is go We can help if needed. There you go Okay. Do any? need the microphone? Please. Okay. So the remote people can hear you. It's running. All right So what I wanted to try to discuss today or to present to you was the idea that I had a few weeks ago about what would a world with SDOs without standard developing organizations could look like, whether that work will be materialized In particular, I wanted to just flavor it a little bit for the IETF this time. So I was first thinking about what was happening before the IETF. And so if you thought about what was this state of designing protocols in the TCPIP stack, so you would have a lot of experimental proprietary situations a lot of ad hoc solutions, think about the lack of resilience between nodes, et cetera. And it's it made a lot of sense to invest time to put together people in one single room, try to get to agree as to what would be the parameters that you would like to negotiate on the standard or with a proposed standard and move forward with that And essentially, what that brings to was having collaboration so on the standard, or with a proposed standard, and move forward with that. And essentially, what that brought was having collaboration, some ensure interoperability between systems"
  },
  {
    "startTime": "00:06:01",
    "text": "and you have cost reduction, etc., etc., and so the way I look at it is a bunch of people were coming together to agreed to help machines to talk to each other because at that point, the machines were stupid enough to not know how to talk to each other. So basically machines needs us at the moment. Now the question that I'm asking myself is whether that actually changed with the technologies that are coming over will we reach a point in which the collaboration between a vast amount of people having to move into one single point, like in here in Vancouver, for instance? will be still necessary or not? So right now, we establish protocols to be able to have a number X of systems communicating with each other But what happens the moment that companies may have their own agents being able to talk to each other and solve those problems at all? point, point at very low cost? And so I have the feeling that we're going back to ad hoc solutions, very likely to proprietary ones, and the cost reduction will be relative. And I'm not sure if we might be facing in a near future the end of what we understand us protocols nowadays and so I have these questions that I would like to discuss with people see what you guys think about where technology is going where these things are going to be happening or not So would it be the end of protocols? What could be the role of the IETF? Should we evolve into the are we going into an upper layer to try to give some very generic or general regulations or indicate as to why or how those add hoc implementation should look like and what they should not do? should not do? How far we are from that scenario and if that scenario ever happens, what are the kind of things that we should be preparing in advance to avoid certain possible negative"
  },
  {
    "startTime": "00:08:01",
    "text": "side effects? That's my email. If anyone wants to talk with you Tomorrow I'll be running a side meeting to go ahead little bit further because of course in four minutes I cannot explain all this things I've been looking into in that particular topic. I hope to be able to discuss this better. So, Tennyson, tomorrow 3.30 to five and since Petyakucha is cancelled that there will be no extra conversation. And I got 12, 11 10 seconds. You can upload now if you want Thanks All right, Stuart and Nate, you are up to bat or just Stuart Are you ready? Would you like your clicker? I am Oh, click along So I am here today on behalf of Nate Can't make it So this is something that came up at the last IETF, Nate and I were talking about this, Nate works for Garmin, he's doing some things with multicast, he had some problems with all the multicast receivers have to have the same port number to receive the UDP packets And, and I said, oh, it's funny you should say that when I was a student 25 years ago at Cidcom, I gave one of the outrages opinions talks, where I said, it's stupid to use UDP for Multicast. You should have a different protocol, because the port numbers are relevant And then I forgot about it and that came up with a solution so a little reminder for unicast traffic, port numbers tell you which endpoint on the system the packets are going to. And arguably"
  },
  {
    "startTime": "00:10:01",
    "text": "it was a mistake that that's not in the IP header, but we fixed it by having this other thing called UDP on top which tells you where the packet is going For typical unicast applications, they're allocated statically by ion Iyana, but it's a scarce resource and that static allocation doesn't work if you want more than one thing, right? You want two web servers running on the site host. They can't both have the same port so that's never worked very well dynamic allocation and advertising it within a service discovery means you can have as many HTTP services you want on a host and discover which port they're on. But when it comes to multicast, you can't have each thing on a different port and and that leads to this problem that multicast applications need to get static ports allocated but you still have this problem with what if you want to run two on the same host? And then the insight is, but when why are we doing this at all? The multicast destination address, the group address, tells you who the recipients are. You don't need to further demultiplex with ports, which is why I thought we should have another protocol, multicast datagram protocol that doesn't use ports. Nate's insight was well, that's going to be a big change. We're not going to change all the kernels but how about we just do this hack? We just pitch two, one or two, ports that are just dummy ports. They're ignored. They mean nothing and we have all the applications Oh, this is a bit of an overview yet. Multicast addresses can be static and dynamic, but this is the key point Just have all the applications joined the right multicast group address, and we want those multicast group addresses to be unique because the whole internet infrastructure is building its forwarding tables"
  },
  {
    "startTime": "00:12:01",
    "text": "it doing routing to get the right packets to the destination that want them but once they arrive at the destination throwing it away because the port does match, it's incredibly wasteful, right? We've already expended all the internet resources to deliver the packet So Nate Carson's proposes we just take the last two ports of the current dynamic range, all multicast applications use those ports. They use ESS reuse port and or SO reuse adder to share them. And they just become a dummy 16-bit filled in the packet They're not used for anything, but for compatibility reasons, UDP packets have to have a port so we'll stick a port there but in fact that's not what the de-does the demultiplexing The destination group addresses what does the demultiplexing So I think this, the reason I agreed to support this, I think this is an elegant solution, works with today's operating systems, and solves the problem so please come to the PIM session on Wednesday to talk more about this Next up, Swap Neil Hello, and Swapnil-Shed and I'm joined here by my colleague, Andrew kaizer. Our co-authors from ENS and Blue Sky were not able to be and I'm joined here by my colleague andrew kaizer. Our co-authors from ENS and Blue Sky were not able to make it for this IETF Pointed this way. Oh, you got it So what we are here to talk about is DNS integrations your DNS integrations allow or enable a DNS domain name to be used as an identifier within an application or approach We know this very well, we know the use cases"
  },
  {
    "startTime": "00:14:01",
    "text": "email and website, right? Over the last few years what we have seen is there is a lot of demand on use cases for DNS domain names with the new applications in blockchains web 3 applications decent-like social media applications to name a few That's quite interesting and exciting to extend the utility of DNS domain names, but we want to make sure that that's done in a responsible manner. And that's how we get to responsible DNS integration And what we mean by that is it's not just enough to have your DNS domain name act as you identify with an application or protocol but also be able to extend this important to extend the utility, extend the security and reliability of DNS into the application into the new use case, right? And so what we've been doing is as part of research, studying these different integration And so it's listed here, we've studied Blue Sky, which is a source media platform that allows you to use your DNA domain name as your social media handle on the platform ENS, which is Ethereum name service, and that's a blockchain-based naming service, a naming application that allows you to import your DNS domain name so that that can act as your digital wallet identified as an example and last is Microsoft gethub which is we all or GitHub, GitHub has something called GitHub organizations If your organization has a GitHub page, you can associate your domain name and also verify the control the domain name, which allows you to get to agree check mark so as to build trust with your end users And so we published some measurement studies to understand if these are integrations are responsible and as part of our diner 2020 presentation you're welcome to take a look at"
  },
  {
    "startTime": "00:16:01",
    "text": "that slide that I've linked here There are three findings. One is there is a lot of interest in integrating with DNS into these new applications and protocols second is that there is all these integrations have been done in a variety of ways where people are trying to reinvent the wheel. And third, is that there is definitely a gap in understanding of how DNS works works these different communities and so what we want what do we want to do is you know give them some guidance in terms of how to integrate so that we do this response And that's the draft that we just published last week that draft goes over modi motivations to use DNS domain name you know, global consistency, stability, flexibility but also comes with a lot of challenges and so we have listed in our draft in detail some of these concentrations that you see on this screen and the one that I can talk to about about today briefly is the domain name lifecycle, right? So we all know domain names once they are registered they have a lifecycle they are they can be transferred they can be they can expire or they can be deleted right? And so if a domain name is integrated, into one such application, how does up this application track the domain name lifecycle? and synchronize it? Thank you very much Next up, Andrew Andrew, there he is We are ruthless, ruthless with the timer Showtime"
  },
  {
    "startTime": "00:18:01",
    "text": "Okay, I'm here to do it about an MTL mode draft we wrote up for being at SAC So MTL mode is a way to address the issues that large post-quantum signatures will introduce to some protocols and also the processing overhead. So how does it does that? Basically, you smirkle trees Not a single Merkel tree, it can be a Merkel tree note set with multiple tree roots So what we do is use the merkel trees to be able to approve membership of a message in case of the DNA SEC and RR set that it's in the zone and that it's been signed But then you've got to bind it into the DNSSEC Trust chain. So what you do is you take the roots of the Merkel tree and you sign them with a post-quantum signature algorithm. And so normally if somebody's holding that sign what we call a ladder with those roots, they can then if the authentication pattern, authenticates to a wrong in one of those ladders, then they can show that you've got the chain of trust from the signed ladder with the DNS key and you can show membership So unique approach So there's a number of benefits that we see that, so it can address the mainly the, it's hard to transmit large things over UDP. It's not reliable The memory footprint of the zone for large signals could be huge. And the CPLO2 load for signing stuff, like Spanx Plus takes quite a while to sign. So we have a our draft that we've evolved over time And we also looking for resilience with this approach and we think it's"
  },
  {
    "startTime": "00:20:01",
    "text": "implementable hash functions are out there You can see the other types of benefits so the drawbacks are you know they support are out there. You can see the other types of benefits. So the drawbacks are, you know, it's the protocol update, you know, this two-step process of authentication So we have a draft that defines the signature formats even as zero options so it can be utilized and we have a detail examples in the draft of the example zone that's been signed so we have an IPR declarations that we've done according to the IETF standards So our next steps are basically we're asking for the community out here to review this draft. It's early stage draft. We're just defining the protocol elements And what we want to do is get implementations. We want to be able to do interoperability testing. And there are some test beds being formulated And we also would love to have our of the aspects of a applying this in the real world. We need the expertise of others who are familiar with Dina particularly in the Resolver community the software developer for name service authoritative we get their feedback and perspective of where we should go with the draft and what additional things that needs to address. So with that thank you. I finished early David, start making your way up the clicker oh yeah please I'll do this that way hey can y'all hear me cool I'll try not to hold them"
  },
  {
    "startTime": "00:22:01",
    "text": "microphone. I tend to drop them Okay, my name is David. I'm here to talk about authorization and more specifically a language called Alpha 2.0, which is a proposal I'm making an IETF. And if it's called Alpha 2 2.0, it's probably because there was a 1.0 before that Good point. So first of all, um, let's just take an example. Here's Mario. He's 25 He's Italian. I know it's a little stereotypical to having him called Mario, but anyway, apologies. And all these things are actually things that the person is proving They're authenticating something about themselves, right? It could be their identity, it could be their date of birth. You know, when you go drinking, the bartender doesn't really care who you are, they just care that you're a lot to drink wherever you are. So the age of 20 in Canada, EMB in the States, and 18 in lucky places So that's all just authentication. It's pretty straightforward. There's not said lots of standards around authentication including here at IETF Oath, of course, in the many derivatives of OOF, SCIM is another authentication protocol or related protocol, SAML back in the day, so on and so forth. Very mature, very well understood. Lots of innovation happening into things like strong authentication, so on, so forth. But what about The next slide. There we go. Thank you What about authorization, right? Now that we know who you are, what it is, you represent, now that we know that you're 18 or 21 or 25, what is it you can do? with that information? What is it you're trying to do? In the case of Mario, he's trying to enter Canada to go to IAT right? Is he allowed to go in as in Italian? citizen? Can you travel to Canada? Is he on a blacklist? or an unwanted list? Okay, so these are all the things that author Is he allowed to go in? As an Italian citizen, can you travel to Canada? Is he on a blacklist or an unwanted list? Okay. So these are all the things that authorization is trying to solve. Just making sure that you can enter Canada as an example side funny note weed is federally legal in Canada. It is stately legal in the state of Washington yet you cannot cross the border with weed. Don't try You just can't. Because it's not federally legal in the states. And the border, although it's at the state of Washington, is actually a"
  },
  {
    "startTime": "00:24:01",
    "text": "federal border. So how do we solve authorization? My friend and peer Alex Batch the border, although it's at the state of Washington, is actually a federal border. So, how do we solve authorization? My friend and peer Alex Babineau put this graph together. What you really want to, if you get the size later, you can look at it in depth, and I put a link at the bottom where you can find the full-blown picture. But there's two big models, Mac and DAC, mandatory access control is whatever the enterprise dictates the government dictates it's things like you cannot drink if you're not 18 or 21 versus DAC discretionary access control, which is more along the lines of, I've got a Google Doc, I want to share it with you. There's no real policy around it. I'm just going to share it with it not 18 or 21 versus DAC discretionary access control which is more along the lines of i've got a google doc i want to share it with you there's no real policy around it i'm just going to share it with and under back and doc there's different ways to tackle authorization, things like Rback, role-based access control A-back, attribute-based access control, re-back, relationship-based access control. It's a lot of backs. They really have your backs It's a very, very rich environment, many different ways of tax tackling authorization. There's a messier picture here that shows all of the standards that exist and what's interesting is on the left-hand side, yeah, on this side, you have all the vendor-specific or industry-specific standards and on the other side you have all the technical standards the IA IETFs, the OAS, so and so forth This is kind of a timeline of how authorization has a evolved over time. It's not new. We've been at it for the past 30, 40, 50 years, depending on what model you pick, and it's still evolving, especially in the past few years If you go to identity-related conferences, you'll see a lot of new things happening. And that's kind of what sort of worries me is that I don't really want to reinvent the wheel. I just want to make existing standards better, which is why I'm talking about Alpha 2.0, right? And alpha is an abbreviated language fault for his authorization based on Zachwell, much simpler syntax, much friendlier on developers A lot of companies use Alpha today, my company axi- aximatics, but also bigger companies like Salesforce, Talis, Huawei Waxall and knowledge, and all their customers Why Alpha 2.0? I want to make it easier on developers, make it easier on everyone, get rid of the legacy of Zach Zackville, promote reuse. Who's interested in"
  },
  {
    "startTime": "00:26:01",
    "text": "Alpha 2.0? A bunch of companies, including bigger names like Ping Identity, and I'm out of time time Rich Go and encourage people to help contribute or ideas and code and anything else. Quick TL was a fork maintained by Akamai and Microsoft All we did was add patches came from Google that enabled other quick implementations I think at least six, maybe eight or ten, quick implementations can run on top of it. OpenSSSL decided to go the other way and do their own quick implementation Not so great. So what we're changing, is we're going to allow outside contributions. We're going to follow the board SSL model in that it'll we're not going to treat open it we're not going to treat open SSL as upstream. We, it's, uh, will pull we will make sure we'll pull down security fixes we will make sure that the FIPP's provider still works. We'll remove silly things VMS we'll address some of the performance issues and we will have close collaboration with the other major forks they owe me Okay, so why change? The Open SSL project is all about quick these days. They are other people who made other pull requests, see them languish they have to keep pounding you know hey would you look at this look at this look at this and so on. And the IETF is about more than just quick, surprise We need you. We want people who are interested in writing code, documentation contribute ideas if you know of cryptographic or other things within the IETF that are related"
  },
  {
    "startTime": "00:28:01",
    "text": "to security, if you're process warm are interested in writing code, documentation, contribute ideas. If you know of cryptographic or other things within the IETF that are related to security. If you're Process Wong, like a former IAL member, help set up the governance If it succeeds, we'll turn it over to an Apache project. There's the project link. There's the email list for setting up the governance of the project get in touch with me you know as I say, I'll be here all week. Please tip your week waitresses and bartenders. That's it Thank you get rich a beer for doing it in half the time. Nick, you are up Sneaks up behind us Hello, hello Hey, everyone. I'm here too talk about a draft that we've put together with some co-op co-authors, including folks who couldn't be here, Louis Brian, and George. The topic at hand is privacy and privacy on the web in particular, privacy policies and privacy settings. These things are very inconsistent. This is something that's not standardized across the internet. There are certain privacy policies that you can't, that are not even machine readable, that has to have a special renderer to open up to find. This is wildly inconsistent. But what why are these here these are happening because of various legislative rules, consent rules. So in order to collect information, on website, you have to get consent So there's been a previous e-privacy directive consent 2002, it's been updated to 2008 GDPR has this as well. There's certain aspects that have to be clear with respect with consent. So freely given unambiguous specific, informed, limited purpose And as of today, these are implemented in wild"
  },
  {
    "startTime": "00:30:01",
    "text": "widely, widely different ways. There was a previous effort to try to formalize this at the W3C. It was very, very complex We're proposing a very, very simple solution called privacy.txte. And it's inspired by some of the other .txte files that we've come to know and love, ads.tex, robots.com, security Text. It's very simple There's several different sections here And these are usually the aspects that are difficult to find. So who is the entity? and what is the privacy policy in text form? or as an HGPS link? What is the contact address for the private? contact for saying opting out of? personal data deletion or opting out of third-party sharing? How do you find these things? These are actually very very difficult to find usually there's also banner information is there a banner on the site what consent? style is implemented in there? And then there's a section for cookies, which lists out which cookies would be presented. And some of the statistics. So the advantage of this, is that it is machine readable, very easy to adopt No changes on the user side are necessary. Doesn't interfere with browser functionality, and in the most part it does help websites fit into GDR compliance and do this correctly doesn't interfere with browser functionality, and in the most part it does help websites fit into GDR compliance and do this do this correctly and if widely implemented this would introduce a lot of transparency on the web so in support of this draft this draft exists, you can take a look at it It's been submitted, but it's also on privacy txt txt.dev. In addition to that, we have a data collector tool and a cookie compare tool that we built to automatically create privacy text files from scraping and verifying the validity of privacy text files. These will go under slash privacy text or under well known And this is supposed to have a real-world impact"
  },
  {
    "startTime": "00:32:01",
    "text": "if this gets adopted. So it's empowering users to have a better user experience when trying to understand the privacy policy of a site It helps websites comply with GDPR, and it facilitates auditing And with this, GDPR, know, GDPR compliance on the web is relatively low, privacy text is one solution. I know this has been tackled in various ways, but this is an attempt to do so in a very simple to write way. And these auditing and generation tools are very, very simple. So what are we looking for? Any interest, first of all, reviews of the draft. The draft's been submitted. Anybody wants to email the authors. We would love to hear any feedback from this If you're interested in working on this and collaborating with us, working on the tools, we're very open to this as well. And yeah, any feedback? on the document or password? Thank you Pascal A clicker Ready? Go Close to the mic, please And you can read here. So this project is called online tele-securement and it's achieve a low-power high security personal server The motivation so what we want to do we want to develop online vote for internet users let's say individuals and small and medium company and the main idea is to use online security elements. You'll know your app 10 billion security manufacturers every year among which is 6 billion of Java card and this Java card can be programmed with a subset of"
  },
  {
    "startTime": "00:34:01",
    "text": "the java language which is called the Java card. And so you are use a secure element mostly in your bank card, on your e-passport and you know that it's very difficult to break this kind of GED device. That's the idea So the security level is highly security level and six plus according to common criteria So we use open technology Basically, we want to open a non-disclosure agreements and this kind of stuff. And we also want to use open hardware things like Arduino for example when it's needed, because hardware is important part for this kind of server. And the kind of service we target, we target, key management services and secure storage please speak a little closer to the mic speak closer to the mic. Ah, yes, sorry. And so there is a draft called TLS SC which means C so secure it means and basically TLS for security amendment is a TLS 1.3 profile using Preciarkey. Pressure keys which is a 128 bits high security and we target two kind of servers and nano server, which is really a nano server is basically the same Preciarkey. Pressure key is 128 bits, so it's high security. And we target two kinds of servers. The nano server, which is basically a security element and TCP IP socket. And personal server, based on it's based on grid of security element and so then it's mean you have a TCP IP socket that manage a set let's say Certit security events and so every secure element is identified by a uniform resource identification say 32 security. And so every secure element is identified by a uniform resource identifier and so it's come from the facts that we have a TLF server inside the speech security element. And so we use the pressure key as a secret as a route of security. We have a secure channel unturned, secure channel with the secure element and to when the"
  },
  {
    "startTime": "00:36:01",
    "text": "routing is needed, we use something imported with from TLS which is called TLS service name and which calls that secure element name. And we put it in somethings which is physical and which is collected upon reset which is called the answer to request or so you you you get something a 50 byte field and we use that for routing and so it's give you a kind of uniform resource at ntifer so this is an example of personal server for personal server we use for example i2c bus So we use, which something's called the security man processor and so the security element has to address. One is physical, which is the I2C address and the bus and the second one is a servant name for TLS and for nanoservants different we just have some sock. It's very cheap And we just use the five wire ISO-70-16 bus And so you have a single secure element connected to the internet we have low power conception and it's illustrated over there and so you can for example, use solar panel to feed it and I'm done and so we are looking for people interested to develop services thank you Thank you first Ready? Yes. Go ahead. Yeah Actually, we have a couple of use cases over one with leads to higher performance"
  },
  {
    "startTime": "00:38:01",
    "text": "that cannot be provided by the current technologies And actually, to do traditionally, the massive data will be processed in local servers. Well, about not more and the more use-case system, the will be massive data transmission will happen among models sites and multiple DCs so so there's just a new requirements of the wild world requirements of the, the, the, the network networks, which will be needed for these use cases When it comes to multiple sides, the long distance transmission between two sides for example over 1,000 kilometers such as the high performance computing for scientific research and distributed storage and data expressed surveys and a multimedia content production and the data backup and disaster recovery and the second use cases is the multiple deceased intercollection for the collaborative training across multiple DCs disease. An objective scenario comments of these use cases. What does the characteristics of high performance about our network? The first one is a massive aniphant flow state with a large burst which have to be our have to coexist to be a lot of services and with dynamic flows And because this is wild error net lot of services and with dynamic flows. And because this wild error network is involved, so those non-discipline, to the multi hops and pause and diversified the domains between the disease and the science"
  },
  {
    "startTime": "00:40:01",
    "text": "The objective goes for high-performance wild error network is the ultra-high bandwidth to the utahs and ultra low packet loss ratio and no latency in Jeter When it comes to gaps for the existing technology, the optical fiber directive collection is because to the limited scale and the cost. And the DC technology, such as PFC, there's just a slow feedback and high-round trip and time latency in nature And there are three routing technologies, EC and ACNP worse than the network is passive and unavoid of it status. So it requires the coordination with the answer systems. Network resources will be in insufficient with no utilization rate requires to improving that bandwidth utilization And long-distance transmission requires auto low packet loss, latency, and jitter guarantees And we have set up a site meeting on Tuesday after long, and we will have more discussion on detailed use cases and trying to identify some common requirements and identify a rough concern of what IDF could do and what the next is step if possible. Thank you I really can applaud. Thank you Always extra points for finishing early You have your clicker So my name is Si Peng Shio I'm the V6 op co-chair So if you want to find me later, you know where to find me me My topic is about IPV6 performance comparison with IPV5 and when people hear that you know one of the"
  },
  {
    "startTime": "00:42:01",
    "text": "first questions came to their mind is why why would IPV6 have different performance? from IPV4? And the answer is that IPV6 first is why would IPV6 have different performance from IPV4? And the answer is that IPV6, first you have known that. So you have certain advantage but because IPV6 is relatively new idea that. So you have a certain advantage. But because IPV6 is relatively new, I think at least for the network of the mini-tradation So because people don't know IPV6, as well, they may be some mistakes And as a result, you know, IPV6 have a search disadvantage. So overall, I think that from the stat of APNIC, we already know that from the end users perspective for worldwide average IPV6 have an advantage on latency, but have a disadvantage on the connection setup fluid So, okay, so now you say that, okay if there's a performance difference what's the big deal? The big deal is that if IPV6 have worse performance, then IPVs then in a dual-stack environment, the happy eyeball will pick IP4 and will not use IPV6 and worse because happy eyeball protocol hide the problem from the angle user. So nobody even report the problem and the network administrator may not even know so you may end up deploying dual stack and thought that you are using IPV6, but you are not. So here is what we are trying to find out what we are trying to find out you know is there a performance really a performance difference? So what can we compare? You know, between a DO stack holds, a DO stack soft and a DO stack destination if, you know, the end-to-end process is like, okay, first you need to do the DNS"
  },
  {
    "startTime": "00:44:01",
    "text": "and then after the DNS, you will set up the connection and if the connection is successful, you'll try to find out whether, you know, there's a difference in packet loss, rate, etc. So this is the whole process that we want to connect from, you know, the DNS performance to connection set up success rate to connection set up speed, and then to the connections package loss rate, and at the end, if there is a package loss you know where exactly is the package loss? Is it in the the thing this? the end, if there is a packet loss, you know, where exactly is the packet loss? Is it in the same domain or in the transit network? or at the, you know, destination domain? And if you can, you know, collect this kind of statistics, then you'll find out that, okay, you know, if IPV6 perform is worse. Where is the problem? Is it in the source or in the network or at the data? destination? You will find out Here is how you can help First, we need somebody to review the test case For example, here we propose the test case But some people say that, oh, you know, they have a piece need somebody to review the test case. For example, here we propose the test case. But some people say that, oh, you know, they have opinions, like, you know, whether this is the best way or not. We like to hear that We need people to write the Python script because you know, things need to be, you know, automatically best way or not we like to hear that we need people to write the Python script because you know things need to be you know automatic as much as possible we need the people to contribute the probes For example, if you have a few stack holes you have a deal stack laptop then I think that you can help. Almost everybody can help, or you can just help with ripe dual stack laptop, then I think that you can help. Almost everybody can help, or you can just help with, uh, uh, right, you know, credit so that we can use the right atlas probes. You can also help to analyze results In the end, you know, write up the result to public threats"
  },
  {
    "startTime": "00:46:01",
    "text": "stress draft. All right, Bill, do you want to turn on your microphone? Make sure we can hear you. Yep. Howdy all? Yay. Just just yell out next slide when you're ready. Indeed Very sorry not to be there in person. I fly out early tomorrow morning. I'll be there by lunch So I want to talk just a little bit about the whole digital emblem thing um and the work that's going to, we hope get moving into the IETF Next slide so when people are talking about digital emblems what they're talking about is the markings that are needed to indicate things under international law. So really quickly, clockwise from the top left, there's somebody with a press badge on of the sort that you would wear in a battlefield or whatever The next one with the orange background is an ISO container marker And you've got UN Blue Helmets. The four markings on the cardboard carton there are components of chemical weapons The end of the shipping palette there is branded This is a hot metal on wood brand with variable data in it And that's required for any piece of wood that crosses the national border. The blue and white sheet there is what UNESCO puts on a world heritage site Then it's all kinds of markings for radioactive, um, uh, on a World Heritage Site. Then it's all kinds of markings for radioactive fuel or waste, if they cross next national borders, on the orange bag, there is a diplomatic pouch Next slide so what all those have in common is that they're physical markings right now and people are certain a diplomatic pouch. Next slide. So what all those have in common is that they're physical markings right now and people are sort of gradually trying to figure out how to digital"
  },
  {
    "startTime": "00:48:01",
    "text": "the markings and customs agents are terrified that everybody is going to do these in different ways and this has already started. So there are customs Agency that have something like 30 different scales already sitting around to do verification of different proprietary markets So the idea here is you have some of the issues marking, they sign it it is in the form of this digital emblem which describes an asset a validator who wants to figure out whether all this is legit validates the signature, and they validate the description. If they validate the signature in the description, they've got a link between the issuer and the asset. Next slide So, there's a binding of the issue to the asset via the digital emblem. And this slide just talks about everything in generic terms, which you could implement in JSON or whatever Then on the right-hand side, they're all generic terms, which you could implement in JSON or whatever. Then on the right hand side, there are all the different ways of moving the data between the validator and you know, wherever they're going to get the digital emblem from. Next slide This is the team that you will not see on Wednesday, which is the example of how you could implement this entirely on top of the DNS if you didn't feel like, for instance, doing it on top of check Last slide come to the Regency CD Wednesday morning if you want to chat about this there are a zillion things to talk about We have two hours. Would love to see you there Thanks a lot. Earlier applause Thank you, Bill Manu, are you ready to go? Yep, I'm here. I just need the clicker. Hold on this one works. Great"
  },
  {
    "startTime": "00:50:01",
    "text": "Hi, I'm manu fontaine. Over the past few meetings, I introduced the work that we're doing on the Universal Name System and the Universal Certificate Authority. I want to continue that conversation And really, the goal that we are pursuing is the creation of an information space like the web, but that has all that is architected from the chips up to have least trust principles built in where we're coming from we are a public benefit startup We have venture-backed with some money from Paradin and Akamai and then we also got some funding from NATO Diana and the DHS innovation Group to build a messaging application on top of this information space and to build a privacy-preserving cryptographic quality for DHS. So while these trust and what do we mean by least trust because we believe that that's North Star that we should all agree to We think it just makes sense to try to minimize reliance on human trust and automate security everywhere And so we systematically follow the principles of least attack surface, least knowledge, least authority and least privilege And to do that, we start at the chip level at the physical computing infrastructure level We leverage continental computing chips, trusted execution environments, so we can use rats principles, remote attestation, verify the chip verify each and every process running on the infrastructure, verify the cryptography that we're in these processes, and then be able to verify the information space itself And so that's what we use and we built this universal name system. So it's like imagine the naming system for every entity in the universe people, organizations, physical things, digital things a unified namespace, it's cryptographic all the identifiers are cryptographic identifiers And then the UCA is the automation of, you know,"
  },
  {
    "startTime": "00:52:01",
    "text": "management for all those entities And that gives us the ability to solve things like province integrity, authenticity, reputation, confidentiality, and privacy within the infrastructure It looks like this very much like a hierarchy recursive, chip level, process level, at the stage verification network that automatically generates decentralized identifiers and then automates pairwise cryptography I need to go a bit fast here. So lots of stuff to save there, but essentially any organization will be able to deploy search and software and then run their own what we call agents on the infrastructure When you take that diagram and you form it onto itself, you see that what emerging more like a mesh network, where every connection is you know pairwise all cryptography is automated. There's not a single human Inside, there's no insider at all It's fully verified, fully automated quantum resistance, and all that good stuff So, because if we entity and every agent's you know their own key in a inherently least trust infrastructure they can generate their own encryption keys Every entity now has its own information space that is encrypted with keys that no humans have access to We call that information insulation, isolation, isolation entity now has its own information space that is encrypted with keys that no humans have access to. We call that information insulation, isolation, which is kind of an extension of the concept of process isolation, where every individual process, can actually have its own secure and isolated information space fully encrypted from the rest of the world So we're going to have a site meeting on Wednesday. We want to continue that conversation, give you an overview and then updates on the later stuff that we've built we believe that this eventually this should find a place somewhere at the ATF We want to collaborate. We want to discuss how this type of infrastructure could be"
  },
  {
    "startTime": "00:54:01",
    "text": "managed and governed in a through a coalition of companies. Thank you very much Thank you We're making good time here Chin? Whenever you're ready Well done. Ready? Yep. So my name is Chen Wu, and one of the green ball program. So I want to give a quick update of these green ball for activity in the week. And as we know, the power management is not new already get adopted in the enterprise network, such as smart building automation, IoT power management together with sensor data connected from the IoT device, like temperature, humidity data However, the power management in the enterprise is not a focus of today for this green ball, actually we more targeted to the network operator who manage a larger scale mobile network, backbone network, data center network With explosive growth of the network track and rapid development of network technology like, 5G or AI computing, the network capacity and site are increased, continue to increase and AI infrastructure actually consume, you know, much more energy and power, which is drive tremendous growth of, uh, equipment energy consumption costs In the meanwhile, actually large proportion of the vast amount of the energy to be consumed are unnecessary waste waste. That's why actually more and more net of the vast amount of the energy to be consumed are unnecessary, the waste. That's why actually more and more network operators actually thinking for the automation tools and a solution to better at"
  },
  {
    "startTime": "00:56:01",
    "text": "access and control energy consumption across the whole network. So the tagger device or device can component we focus on is the one to consume most of the energy. Actually, for example, in mobile network where you can use across the whole network. So the tactile device or device component we focus on is the one to consume most of the energy. Actually, for example, in mobile network where you can focus on pure network device and also we consider like, you know, power distributors unit combined with a switch server in data center network. So the typical use case is provide energy consumption hit mapper to better outstanding you know, where to consume so much energy Also we can measure energy efficiently ratio and power usage effectiveness for different type of network However, the power consumption monitoring on the network device is not sufficient because, you know, for monitoring is just to focus on, you know, how much energy or power can be consumed on specific network device but, you know, we also need to care about you know, where and why consume so much energy and what a measure we can take to to reduce energy consumption for special location and to tackle these challenges, actually, energy efficiency and management can already become the technology trend, so this really allow net operators to optimize the energy usage based on discovered capability in the mean, while really can help improve overall network utilization and the energy efficient ability really provide global visibility to not only network topology data but also inventory data power data and and and data actually allow easy and flexible correlation and mapping for various different type of data So the ball of the green actually is kind of working forming both and it's a really a topic to create a working group and the focus is a green bulb where to explain what information can be exposed using young data model and what the requirements look like for the energy efficient management and actually, we will really, you know, have used this"
  },
  {
    "startTime": "00:58:01",
    "text": "to, you know, address the challenge faced by today's network data operator. So this green ball, we organized on Wednesday afternoon, the first session, so it's two hour discussion. So your input, you know, for like a, you know, concrete input on the use case report usage, like government regulation method rules actually also on good work in some other STU will be welcome to, you know And if you are only interesting, you know, to take it to like an environment impact program actually green ball for will be welcome to, you know. And if you are only interesting, you know, to take it to like an environmental impact program, actually GreenBoff is not a right place to go. Yeah, thanks for listening Right on time, well done And last but not least, Coriou and Giulio Ready? Good. Hey, everybody I'm Julia. I work at the Freedom of the Press Foundation which is an NGO that tries to develop safe tools for journalists, who have historically been developed and maintaining secure drop, which is one of the most widely known whistleblowing platforms, open source secure drop like most of with the building platforms in use today have this kind of model where there is a centralized server held by the whatever institution is looking for leaks it could be a newsroom, it could be a private or public engine that has to comply with whistleblowing laws in europe on specific regions and at the moment it works like this so there is a centralized server and there's then Tor encryption in transport and there's generally two types of user source which are people who want to leak stuff and journalists who are supposed to receive that And historically, this hasn't really had any complete entry-end encryption from source to journalists for a lot of reasons, but right now this is the kind of state of things, and it's the state of things for most solutions. The commercial ones actually don't even implement encryption most of the times"
  },
  {
    "startTime": "01:00:01",
    "text": "And we've been trying to redevelop this to bring through end-to-end encryption, but the problem is that most messages protocol nowadays don't really fit this use case because they are mostly aimed for instant mass instant messaging, signal or matrix while our threat model is relatively different from that and um um this we have kind of different goals we can really have client applications we have to strickx to web browser mostly because that's a usability constraint at the same time we can really have key material on the source side, and we want to actually have really as little as metadata as possible because volumes are pretty low Imagine maybe a submission for a server in a week so messaging is volume is very very low. And so we have some kind of trade-off where we don't really need real low latency. We don't need a lot of traffic at the same time we have some more goals especially about met metadata. We don't want to store recipient or send that information but at the same time we don't want to leak anything about the state of the state because that's still metadata and someone could know if for instance we do trial decryption, so we broadcast encrypted message and we allow clients to do trial description locally at that point we would have added all the do trial decryption so we broadcast encrypted messages and we allow clients to do trade description locally at that point we would have other threat model implications and so we have tried to develop kind of our own thing. This is just a quick tease, just to show that we are compromising some properties such as scalability of groups in favor of other properties just to overstate that current protocols don't really fit our niche This is still at ease but what's most important is the open question for us first is to understand if there could be other use cases for what we are doing Think maybe where regulation doesn't reach especially in the European Union and whistleblowing platforms,"
  },
  {
    "startTime": "01:02:01",
    "text": "are quite not so security, well done if actually a technical solution could help in that, if there are post-quantum ex experts, we have some problem at parting some part of the protocol in the post-quantum world at the same time we're not entirely sure that our threat model is the perfect one for this use case, and many more And to just add that this was is in progress, but we've done a cryptographic audit, it sounds out to be quite okay, someone at an institute brought the manual security proof someone at the ATH Zurich Luca and Felix are working on a Tamarie model. We are funding for this and this is our contact information come and for us if you have anything to say, if you want to look at that, if you have ideas this can actually be useful beyond us and yeah that's all Thank you everybody for being incredibly efficient, which leaves you with extra time to chat with each other in the hallway or in this room, if you like, and extra time to get to dinner. Thank you for joining us It's easy That's that's all I was required to do"
  }
]
