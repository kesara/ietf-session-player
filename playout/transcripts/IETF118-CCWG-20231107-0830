[
  {
    "startTime": "00:00:44",
    "text": "There's"
  },
  {
    "startTime": "00:03:35",
    "text": "Alright. Good morning. Let's get started. Welcome to CCWG at IATF 118. Happy Tuesday morning session 1. Can I steal slide control? That we're it actually if if That's good"
  },
  {
    "startTime": "00:04:01",
    "text": "This is an IETF Hybrid meeting. The main thing here is that this session is being recorded. Hopefully, you know, by now, how to join and leave the queue. No that all the buttons have changed position, color, shape, and everything else. Make sure that you've scanned the QR code over there to sign in. That both helps us figure out what size room to have as well as make sure that you can be counted as having participated in this awesome session. This is the IETF note well. Please take a moment to read it, especially since we're a little bit earlier in the week, and you've maybe only seen it four or five times by now. These are the terms by which we do business. In the IETF. Make sure you actually do give it a read. We also operate under a code of conduct. Essentially make sure that we're all being professional courteous and respectful to each other. And if you ever feel that there is a place where there isn't, you can reach out to us as chairs. You can reach the AD, and there's also an on-site team. Got some helpful links here. This is mostly if you're looking at the PDF of the slides. You don't wanna click on some links. Note that we have a chat in Zulip, which is also, gonna be here at me. We will echo things there. So prefix any comments with Mike if you'd like us to read them out in the room. And this is our agenda for this morning. The first thing on that agenda you'll notice is Scripe selection. This is the fun part where everybody gets to look really busy with their laptop. We need a notetaker or 2. Often, I find it's easier if you've got somebody backing you off anybody would wonderful Thank you very much. With that, does anybody want to bash our agendas? Is there any last minute adjustments or changes that we need to make. So we're gonna spend a a good chunk of time talking about issues for a 5033 biz since that is our primary working group document and kind of the thing we said we wanted to start off with is let's figure out how we standard adjusting controls and kind of update that for"
  },
  {
    "startTime": "00:06:00",
    "text": "where we are today. We're gonna spend some time talking about a couple of individual drafts. We've got one on safe congestion control and also some analysis of differences different congestion control schemes. And as time permits, we've actually got an invited talk at the end that we think is, of interest to the group. So after we clear out the rest of our working group business, we've got an awesome group of here to talk about that. With that, let's start with 5033 biz. It's a long hike over here. Good morning, everybody. If this takes 60 minutes and something has gone very long, wrong. So there's been, significant editorial vision of this draft that you haven't looked at it in the last 6 weeks or so, I might encourage you to do that. It's been The old draft is kind of a unordered list of considerations, and we try to put some structure in it to kind of separate criteria versus scenarios for evaluation. Even better than reading it. If you go through it, you'll see a lot of sections that have a to do there. Like, look at the to do file a PR. That would be amazing. There are a few issues that are that are, worth discussing here, but before that, yeah, there's there's the PR that reorganized the draft. You can look at the PR, just read the draft. I think it's probably a much cleaner way to do Okay. Multipath. So Oh, and by the way, could I ask one of the shares again to, like, annotate the GitHub if Thank you. So at 117, we asked about multipath"
  },
  {
    "startTime": "00:08:00",
    "text": "let me phrase this one. Okay. Multipass. So, of course, there's there's different kinds of Bathers failover. There's also concurrent paths, which is a somewhat more difficult topic. We kinda got mixed feedback last time about what to say here. Gory took a good Cobs try at it. And it's now in section 5.7. Please have a look at that and see if already been committed to the editor's draft. Please take a look and see if that is agreeable to you. If you can live with it, okay. So this is a this is something where the editors were a little bit and some of the more active contributors were not, complete agreement. So we thought we'd bring it up. So, particularly in the real time case, there are a lot of nonstandard and unspecified congestion controls out there. And they're some of them are important enough to be maybe systemically important to the internet. So, there's one extreme where an algorithm is, you know, it's it's it's not it's closed source code. It's you know, it's not really well documented anywhere. And it's it's difficult to under to expect anyone to to, like, have any real testing evaluation against that. They're they're more reasonable cases where it is open source code, so you could drop it into some similar to, like, NS 3 or do, like, a lab test or it's described in is you know, described in a paper in some detail or even better. It's, you know, specified more precisely in in some sort of SEO document. So right now, like, we were arguing about and shoulds and and things like that, but I don't know if anyone has any thoughts about particularly like to come to the mic and share. And what we should expect here Matt, Yeah. So one of the things that that"
  },
  {
    "startTime": "00:10:01",
    "text": "Well, I I don't know how to precisely say it, but It is generally the case at best effort is faster than than real time. Because Real time, best effort doesn't need to slow down, and real time is constrained by some other external system. And in general, there's a danger. If you ever observe a real time flow that runs faster than the best effort flow under similar condition. And and That is, I believe, that to be simply a sufficient criteria. Okay. Well, thank you for that. So there's there's okay. That's actually good. Well, I I think we might add that. But this is actually a 2 way relationship. So one thing when you were evaluating real time crash control, how do we do that? But also should we be looking at the effective best effort flows on real time flows. And, you know, the gist of this slide is how fair is it to to expect analysis of the impact on things where We don't have an absolute chapter on verse and how it works. do they just kinda is that just kinda the price they pay for not being fully specified? Or Who was buying this the queue and behind Matt in the queue? Christian. Christian. That that'd be me? Yes. Good good evening, everybody. We Yeah. It's it's midnight here. Damn, I have a hard time saying thinking that anything that is not specified impose an obligation on somebody else. I mean, look, I mean, if people want to play by the rule, they play by the rule, and they are effectively, a draft in the ATF and things like that. And a document. The the flip side of that is that There is no protocol police."
  },
  {
    "startTime": "00:12:01",
    "text": "So we cannot prevent people from deploying whatever they want. And we have to be humble there. And and we we have to recognize that what we are doing is providing advice a and maybe providing a a caveat as in If you do play by the rule, will take you in consideration whenever you're doing something else. That should be about it. Okay. I I'll count that as a vote for, maybe not pozy minor requirements on new proposals. Who's next in the queue? Zahid. Zahad. Zahad here. So, I was actually not sure, like, what exactly we're asking for here. But if you're looking for some sort of requirements, has requirements on real time app application. There is a, like, when you say, like, real time condition control or conditional control for real applications. I was not, like, I was not able to distinguish between between these 2. Mean, is it the same thing or different thing? And if you are looking for some kind of requirements that Arm can actually have requirements on the real time applications, when it comes to condition control. Okay. So I may have people down the wrong path when I spoke about real time. Yeah. Real time the reason real time comes up in is is a it is a particular ecosystem where there are a lot of these sorts of algorithms which are not don't have RFCs attached to them, etcetera. But the general principle here is whether systemically important congest controls, like, how should we handle the effort to interact with them carefully when we're evaluating a new proposal. Whether whether it's real time or best effort or whatever. Okay. Now, like, RN Cat is an example of, obviously, well specified real dash controls. And I would expect even a best effort proposal to take a look at how it impacts"
  },
  {
    "startTime": "00:14:00",
    "text": "Yes. You know, the arm cat, algorithms. Now how how widely are those deployed. I don't know. Yeah. So so, I mean, I I'm not sure, like, how widely the arm cat specified contract deploy, but, I mean, I think the deployment. So if you look into the WebRTC and, this kind of deployment, are conscious control. Even if it's not like IT certified, they actually fulfill those sort of requirements. So maybe here was what we're talking about actually mentioning that this is the there's a requirement and asking people to when they are developing best effort. They actually consider this kind of requirements and, and how to interact with that So that might be the one way of doing it. So maybe like an uppercase should proper pursuit. Yes. Okay. And that's please capture that, Eric. Thank you. Who's next? Next, Matt. Yeah. The other thing to remember is real time protocols are generally Looking to avoid causing queuing delay? Which is most often a lower threshold than what throughput maximizing protocols are trying to do. I mean, implicit in congestion control, and this discussion here is we're looking at through point maximizing protocols. And So I would go back to my earlier conjecture that real time should always be slower than throughput maximizing. And, although we do need to pay attention to It would be nice to pay attention to not having too much adverse impact on real time protocols. At the end of the day, the real time protocols need to be in separate queues. In places where the queuing delight or the time and the surgery times elect to be significant. This is because throughput maximizing always measures the bandwidth by creating a queue. It's fundamental to the class of protocols. And real time doesn't want to be in those queues. Yeah. And so I I really think it's sufficient merely to say that real time protocols should not under the same circumstances use as much"
  },
  {
    "startTime": "00:16:00",
    "text": "bandwidth. This is is throughput maximizing close. K. Thank you. Next Spencer. Yeah. Thank you. Thank you. So I've just want I want to, want to agree with the, the idea that, it is not going to be super helpful to have a lot of normative language on this topic, but that I think it can be very helpful to provide guidance just pushing on the, RMCAT requirements a bit. I don't remember off the top of my head if that is experimental or informational. But, get 5033 Best is targeting BCP. So, it might be it might be helpful for, the group to think about to think about where, the guidance can be, provided most effectively and, how it can be stated most effect way. Having said that, I think I think that, This is going to be very, important and useful for people like, in ABT core where we're doing RTP over quick. And we've act we've actually been working quite a quite a lot on trying to say something health without saying too much, as guidance for implementers. On that, on that, in that particular case, it may be, it may be helpful to, just make sure that we are talking with each other as as,"
  },
  {
    "startTime": "00:18:02",
    "text": "as the work here and in AB key core on our on our RTP over quick, moves forward. Thank you. Thanks, bud, sir. Next, Corey. Hi. Gary Furst. Trying to recap a little bit what we've I've heard So I've heard 3 different topics in response to the one slide, I think I heard about some sort of guidance for people who are designing these calls outside of the IETF, which we might express to them. We have these information evaluation and things, which that are information in our own cart, we probably have other BCPs we can point to. So that that's one thing about how to help people who haven't written a protocol spec which we are providing as a community. This, how do we evaluate their protocol when they haven't defined what it is, And I heard a few things about that and a few envelope but basically it's hard because it doesn't exist in terms of a spec. But we heard a few useful things. And the third thing was I heard some thoughts about how we compete with such protocols because do exist in the wild, and they are important to some customers. So how do we think it's fair to exist with these. Yeah. The more I stare at the slide, the more real is how bad it is because it leads people astray. Great. I mean, you've you've made some good points on some arguments that these are coming up here. So I don't think this This is this document I'm not trying to process, we go out and evaluate unspecified protocols to to consider their fairness. The issues when we have a well specified Postload is coming before the IETF. How much do we consider the impact and the interaction with things that are out there in the internet that that maybe are not written down super carefully. And what do we ex like, we do expect someone to run a SIP, at least a simulation. On how it does with Reno. Right? Or cubic."
  },
  {
    "startTime": "00:20:04",
    "text": "Or maybe even VBR depending on how it's supposed to spot you think it is. But for, for some of the other things, like, what what is a reasonable expectation for a proponent to to bring forward on other things that they're not proposing. Okay. I mainly said that so people can argue with me because I'm one of the I was going to try and write something afterwards. So he's continued to comment on what I said another that Lars, please. Alright, Lars. This is not specifically on this issue, but maybe sort of more generally So so we we used to have the the the time in in TCPM when people came in and, you know, presented something and many of them were academics and then Sally tried to help them along in terms of because everybody always thinks they've done all the experiments, right, and then they come here, or they came to TCPM, and it turned out that the community that looks at that. Frequently, they always want more experiments and more analysis and we try to help that along by sort of the finding or trying to define this little set of scenarios that you might wanna consider running your algorithms through before you come. That didn't work super well. I I I think it still wouldn't work super well, but one thing we might try here is that, somebody comes and and they really have an interest to engage and and specified us in some form in the end. We need to explain to them that This will require some effort, some continued effort by them, and also by us. And by them, it means that they probably need to run further testing or or scenarios that they haven't filled and they need to be on board with that. And for us, it means that some of us will need to engage with particular group or that particular individual and sort of try to help them understand why we put up these hurdles and why do we wanna see these experiments? So it needs to be sort of a commitment on both sides from the"
  },
  {
    "startTime": "00:22:01",
    "text": "proponents of new CC and also from this working group to actually help that and not just say you didn't run scenario blah, come back when you have done so not super productive. And I don't know how to formalize this. But if if we can sort of I don't know if we agree on this, right, but if we agree example, this was a way we wanted to try to do the spelling that out so that people aren't surprised when they come and also reminding ourselves that we said we were gonna put some skin in the game ourselves and and some cycles towards that that could work. So a large aggregate is not, directly pertinent slide, but actually I think that's a good point. And, you've already given me an idea for some the additional text that I can write to exactly that thing. Like, this is not Like, don't you don't have to cross all the you have to check all these boxes before you come to us. Kind of statement or would that satisfy your concern. Yeah. Or but, I mean, you don't have to check all initially, but you need to have sort of a willingness to do some more work. Yeah. As part of this review slash standardization process. And then also like, on our side, we need to, like, maybe come to some understanding of what what is acceptable, right, because there's always more, that people can do. And that was what killed a lot of the the work way back then, right, because there's it was never an was always somebody going to the microphone here and says, you need to run experiment blah before I will consider the safe or at least not unsafe. Right? Yeah. I I certainly agree Lars. I think that's a danger. And, that's another idea of some introductory text to kind of avoid that sort of mission creep. I I would encourage you to take a look at the latest driver of the draft. Leave me the editor's copy and just see if the framework that we have is too much, too little, Just up after March. Yeah. Fair enough. We have time. We won't be done by then. Russ in the queue. Howard, please. Yeah. Apologies for coming in and not having read the drafts. But, replying to the"
  },
  {
    "startTime": "00:24:00",
    "text": "discussion about the real time protocols and evaluation of furnace thereof. Seems to me like the traditional fairness has been It's anti social to hog more than bandwidth than you're the serve. But in the case of real time protocols to share your network It's anti social to build queues in front of the real time packets. And perhaps we should focus when looking at criteria on What kinds of behavior are are antisocial? And say that should test that you're not doing this. I mean, Standard Reno is anti social. In that sense because it builds queues. Without any significant limits. Real time. Needs shorter queues. In order to be efficient. Real time algorithms usually When they encountered queuing, go into a fight or flight mode, Either day, Chopped down too hard, or they start fighting for that? Fair share of the bandwidth. But, I think when you want to discuss fairness. In the context of real time, the anti social behavior you want to prevent this excessive q building. Thank you for the comment. We do have some text in there about the voting blow is sort of our first order. Concern. So hopefully that addresses your your issue. I will say that mean, you mentioned something else about the the the share that one deserves and would say that I I think everyone would agree with that principle. That is a much more"
  },
  {
    "startTime": "00:26:03",
    "text": "loose term that I'm comfortable with, and we're we're one thing really working on is be going away from the traditional definition of fairness to maybe something else. And, I think there's already some in there that at you could take a look at. I'd appreciate your comments on it. Next, Question. Christian? So jump to video. Let's Hello. Hope you can hear me. I just wanna say that we shouldn't go in with the notion that best effort, condition control algorithms cannot or or they don't look at latency for example, PVR does look at it and With L4S, we also look at latency, in turn, pipe marking CE. We cannot go in with that notion that best effort is not looking at latency. And the second thing I wanna says, about real time, we should also not think that the real time algorithms are not bandwidth maximizers. And why they why shouldn't they be? You know, if the video can use better quality and better bandwidth, why make sure and it use So, these are maybe traditionally the voice protocol didn't need it. So but now things have changed. So we cannot go in with such notions that these two are always going to be like that in the future. But agree that we should say something about it. Like, how these to different use cases are supposed to compete at a bottleneck."
  },
  {
    "startTime": "00:28:03",
    "text": "Something can be said about it, like the basic requirements, not building up latency. And responding to congestion as early as possible, things like that. More generic than saying real time should not build queues and best effort is spent with Maximizer. So just just wanna point out that Yeah. Thanks, Vidi. I I'm I hope that we've succeeded in not giving that those impressions and the drafts if anyone detects that that implication, please let us know and we'll fix it. Okay, Bob. Hi, Bob Brisco. I've this is in reaction to Harold, what Harold said, and I think biddies was as well. I wanted to put a slightly different nuance on it. How Harold said was a also an example of what Lars said is a problem for people bringing congestion control to here. In that you know, they can be blamed for the problems of the internet, not of their congesting control. Yeah. And and that doesn't help. In getting them forward because you're essentially coming to the IETF and the IETF is saying, the internet is your fault. Our way. You know, there are other reasons why, you know, as Harold said, Reno is a problem and it exists. So, yes, we want new congesting controls to be better than Reno. But we can't sort of say unless you're perfect don't come here. Yes. Thank thank you. I think I actually reinforced a large point quite well, and I'm going to file an issue when I get off the stage to to write something encouraging to people early in the document. Next. We have Madam. How, notable Yeah. like, the recent internet traffic, which I'm seeing, like, the gray box texting So it's not like, only the best effort is affecting the real time."
  },
  {
    "startTime": "00:30:02",
    "text": "So since the the boxes, the middle boxes started implementing different queues for real time. So it's like even the real time kind of affect the best effort So we should have a holistic view, like how both can work in harmony, like, without affecting each other. That's number 1. The other thing is, like, even within the real time traffic one can be, like, sending at a higher rate And, I see a few few real time application does not So if it's built on a same queue or, I mean, a small queue. Whatever was mentioned earlier. So the fairness amongst the real time traffic also might be really, I mean, something which has to be looked upon in the future. That's something, which has to be Thanks. At the risk, well, So right now, we're operating under the assumption that we need to look carefully at the interaction definitely real time traffic. So a couple people mentioned separate queues. Do we think this is actually not a big problem that in general, real time sort of traffic assurances are queued separately. And so, therefore, the the the interaction between these algorithms is not that important. Anyway, let's focus on the queue. Question is next. The the short answer to your question Martin is yes. I don't think it's important. Okay. I've I I think that, in the in the in the evolving set of the art. We are going to see more and more AQM kind of deployments. That do either some classification or some form of a queuing And that takes a great care of most of the fairness issues. So I I I think we are going to see that because"
  },
  {
    "startTime": "00:32:04",
    "text": "I don't think it's theoretically possible to ensure fairness by end to end means. If a network wants to enforce that user using network fairly in the network should deployed active queue management. That's pretty clear. I want it also to support Harold There are cases in which we don't have active human management. We don't have queues queuing system in the waters because it's a maybe legacy device, maybe this of whole network or ad hoc networks or whatever. And building queues is antisocial. We we have this idea of best effort But that is silly. In this damage, We should take it as a first order thing that network protocol should not be used period. And a network protocol that requires building queue for its operation. Like, should we can win those specifically should be basic key considered obsolete. And should we remove from the network at the first location? Okay. Thank you. Next. Yeah. The the the problem is that in order to actually measure capacity efficient efficiently, you need a queue, because you can't you're sending if you're not sending enough traffic to build a queue, you can't tell if you filled it. The way I look at them is they're actually symmetrical. The 2 classes of protocols. One is that delay sensitive traffic is delay sensitive first and throughput maximizing second and And bulk transport is throughput maximizing first. Dolly minimizing second."
  },
  {
    "startTime": "00:34:00",
    "text": "those 2 are not inconsistent with each other at all. And At very, very high speed. You can coexist because you can maintain use that are at very small in an absolute sense. At the edges, you can't, and the optimal strategies are in conflict at edges. And The only real solution at the edges is to to somehow separate them into different queues. Or put limits on the real time such or the throughput provide other signals through throughput maximizing. Such that it can run, without creating a lot of queues and get more explicit signals from the network. Thanks. Who's next? Next is Vedi. I just want to answer the question about different queues for best versus real time. On a local device, yes, you have different queues per se. But once it leaves the device, accused are not different. It's just either, legacy queue, which could be a you know, just a 5 4Q. It may or may not have a QM or it could be enough for us. Whatever the network deploys. But there's no separate queue unless the network operator is doing some kind of deep packet inspection or using some kind of you know, DSCP, which doesn't really get used these days. So, and and on the point of best effort has been with Maximizer I I still don't fully agree because we use best effort per browser. We use best effort for streaming. We use best effort for a lot of things. Which are not real time, but they're still interactive. So I think best effort might hurt itself and it probably is by building up queues it doesn't hurt other Sorry. It just doesn't hurt others. It also hurts itself too. So buffer plot is not something I think"
  },
  {
    "startTime": "00:36:02",
    "text": "should be sort of accepted going forward but I don't I'm not gonna say that we should just eliminate anything that does do buffer blue on the network, We could employ tech techniques like AQM on the network. Which help avoid buffer bloat even if it's Reno or Cubic. End to end protocol. Thanks, Katie. So the next slide is about AQM. So if if your comments are that in that vein, like, maybe leave the queue and come back when we go to the next slide, but whoever's next in the queue. Corey. Sorry for her. Digital on this one. The network is very heterogeneous. There are parts where it runs quite slowly and where for queuing might be great on the low rate interface where that really is good separate queues or just lots of parallel queues. There are places where we really don't want this. I'm not sure we should go too much into it where we talk about congestion controls because these have to work over everything. So kind of thinking the networks very varied, and we have to be careful. We don't produce like an enormous different things to test for. I'm not sure how that helps, but it's kind of what came to mind. Okay. I'm gonna go if you ever could use the hold for a second, go to the next this is all munging in a one giant conversation. And that's this one. So there's obviously a lot of, AQMs out there. Some of them have RC, some of them don't. There was some interest in taking AQMs in scope to this document. I do also think there's a chance of sort of this explosion of test cases, that that Lars alluded to and and Bob alluded to. So, yeah, Do we wanna do we wanna evaluate these? How how much is too much? Cracked."
  },
  {
    "startTime": "00:38:03",
    "text": "And so you could just incorporate in any expenses to those questions in your comments. And that's that's my last slide. So just let the queue run until the chairs are sick with it's But, please. Yeah. Okay. What a try again at saying this about real time and not real time working together. Coming back on what Christian said. That that that if if we if we think that the internet and it's getting more and more AQMs. Yes. That's true. But it's probably more and more in the sense of point 1% to point 2 percent. There are billions of FIFO buffers in the internet. And there are a very large proportion of the bottom x are still FIFO buffers. And I, you know, I haven't got solid evidence for this, but I have got the evidence when I talk to access network operators, 99% of them don't understand AQM anyway, let alone have it have deployed it. They understand this serve. They think if you put a number on a packet, it will get low latency because it's got the number saying low latency on it. They don't understand AQM. And that's universal amongst the networking community. Apart from a few enlightened people. So the idea that congestion control has to be specified without FIFO in that list. Is just crazy. Okay. Right. So, I mean, I I there's no RSC for FIFO, but I would would certainly expect people to evaluate if I feel cute. So I will I will take your comment as a as a vote for considering the cross effects of real time congestion controls with with, best effort, congestion controls. It almost sounds like you're arguing to, like, not worry about all these different all these different AQMs and the evaluation because they're so rare. But I don't wanna put words in your mouth."
  },
  {
    "startTime": "00:40:03",
    "text": "No. No. No. I'm not saying that at all. I'm not I'm not saying that at all. I'm saying that you need to evaluate them against some AQMs and actually there's pi squared is also should be in that list. When you see others That but And and what Matt said is true that you can Writer congesting controls so that it behaves better. In a, in a FIFO queue. And and we should only be allowing those sort of things to come into the internet. And yes, Reno and Cubic ought to be being some set it if you like. But I I don't think we should be too dogmatic about this. Because Pe people get the FIFO buffer that they buy on their service. And if they want something better, they need to be able to go to an operator that understands this and and does something better. And if they haven't done that, they're not gonna get good service. And so you know, the the trying to make a congestion control, fix the problems of the network, you can do a certain amount, but you can't go the whole way. So, the, so like the other slide, this slide actually to, like, related questions in it. One is, so when someone comes to us with a garage control, what extent are we going to ask them to value or evaluate with them. You know, inviting bottleneck working through bottlenecks with all these different AQMs. And then the second question is, does document, Is this document also one that is providing a framework for evaluating AQM algorithms themselves. I I think I wrote 7567, which I think is Gory, that's that already that document already has that. Right? Okay. So maybe it's out of scope unless people Since already as a document, maybe it's fine. And we don't need to bite that apple. Anyone disagrees, please get in the queue to say so."
  },
  {
    "startTime": "00:42:00",
    "text": "Then we could keep but but aside from that, let's focus on Like, do to how far do we wanna go in evaluating new congestion controls versus in this presence of various AQMs. That are prevalent on the internet. Store it, please. I've been setting, listening to the discussion, And there's a a ton of things that occur to me. But I'll just make a couple of points 1, I hear a lot of people talking about real time traffic. As if that's, a weird strange thing. And beneath really think we need to stop thinking about that way. What traffic network protocols don't operate in real time, It it seems like almost everything I do. I mean, for start, we're using real time Meaning low delay. Which is sort of a misuse of the term, but assuming people need no load delay, do I not care about low delay? I'm looking at maps where the forecast stock quotes, driving directions, Anytime I'm waiting for the result, I want it to be quick. So, I I don't see low delay or delay sensor traffic because being a weird special case niche for minority. I want everything I do on my computing devices to be fast. Nope. Nope. Nope. Nope. Maybe an operating system update. There's multiple gigabytes that downloads while I'm sleeping is the one thing that isn't delay sensitive because I'm asleep when it's happening but I think we ought to be thinking about delay sensitive traffic as being most of what people use their devices for. And then on this question, We absolutely have to talk about AQMs. Because I don't think you can ever achieve consistent low delay. On a packet switch network like the internet, if you don't think about how long you're queuing packets for. And"
  },
  {
    "startTime": "00:44:01",
    "text": "FIFOQ is sitting in for, I didn't think about it. So we're never gonna get good performance until we start thinking about how long the package sitting in the network. Going nowhere. So AQMs in the network and compression control on the end systems go hand in hand. They work in partnership. Neither of them can work in a vacuum in isolation from the other. Thanks, Stewart. Laurie is next. Yep. That was a good point. And and I think it's actually sort of recognized now, especially sort of in on the academic side, because a lot conduction control workers for data centers and that specifically always ties to specific switching fabric and and the queuing of that. In the internet, we have the problem that, like, Bob said, right, is really way too much 54 out there still. And so in order to talk about what you said on on the slide here, right? So we should definitely, unfortunately encourage people if they haven't done so to evaluate their condition controller, in, in FIFO Networks or mostly FIFO Networks. We should also, like, if they have a pedestrian control, that it is tied a QM deployment or intends to work better in the presence of those AQMs do an evaluation for that. But I I I don't think someone should, like, require them to do, like, all of these. Right. So we should look at at the IETF and what we have recommended or are recommending like, l for s is something that we are trying to promote. Right? So that might be something we wanna encourage people to look into. There might be others, right, that we have recommended, like, I think cuddl and Thai. Some of those that we did in the past, I don't we probably haven't had the conversation if we are still sort of recommending those or if it's now l for only or mostly, or what is our belief as the IETF where we're going with this? Because all of these directions You really can only have one for a queue and not like 4. I think that might be helpful, but you know, it it needs to work in a in a five 4 internet and it ideally should work"
  },
  {
    "startTime": "00:46:00",
    "text": "better in in a in a future in it with less 54, I think, would be where I would lean towards. Thanks, Lars. Yeah. I mean, I would lean towards some sort of big statement about, you know, you know, significant, AQMs or like, you know, prevalent AVI AQMs or something like leave it a little open because that will of course change over time. Like, obviously, L4S is probably not this is a little significant today. But but but have hopes that it will be. At least I have the hope it will be. So, yeah, so so but certainly, just having them run through all the AQMs just because they exist and they're specified seems excessive. Given how they're actually deployed. Next in the queue, madam. Yeah. So Yeah. Sorry. You're breaking up. I just want to add, like, since we are emphasizing on the best effort So it's it's better to call. Distinguish between a saturating best effort and, other best efforts. Like, the recent traffic though there is lots of, best of work, we see a lot of pattern mostly the streaming traffic, which is, like, more prominent in the modern internet. So which, does not affect as matches and download this So when we are evaluating, such an, the ATM, from my experience, I've seen, like, how the real time reacts to a real time streaming is completely different from how, it reacts to a saturating download. So I think, this should be carefully considered and some kind of suggestions when the queue is getting, through them. So that's that one should be, part of, the recommendation, I believe. Thank you. Who did I go? Guys, don't you, telecom. Bob said that"
  },
  {
    "startTime": "00:48:03",
    "text": "transport can solve all the problems of the network. One should also be careful not to try it the other way around to have the networks solving all the problems of transport. And, the point is whatever we do, shouldn't be overly complex or scalability is an issue for network providers and simplicity of operation is another one. So all the designs should be done carefully, easy to operate, easy to configure. And, and well standardized. I am happy that AQMs are present. I'm a backbone doing backbone engineering and Yep. Yep. It simplifies all work so That's a good idea. Thanks. Queue would occur? Tim. Hi, Tim Churn. So I'm from what at the RNE network just to operate the UK network called China. And the challenge we have, I I think Stuart's point of Saxon that we need this, but equally, we also, for on board system moving around data from the large hadron collider where there's projected aggregate flows of over a terabit so we're looking at we are evaluating, and we're seeing really good results from BBRB3 using jumbo frames, etcetera. Interest. So it's a single infrastructure that needs to support these quite varied different requirements from the very bulk huge movement of data, research data down to students and staff who may need the sort of applications that the Stuart was hinting at and having that on both structure. So I think it's really on the same infrastructure. So I think something that needs to be in here is you know, how you can make that happen and, what the options are for you. And to bear that in mind all of its own. Thank you. Next. Corey. Am I nearly at the end? Yeah. That was okay. So trying to play the same thing again, what he would learn. We didn't have, well, we didn't have a sensibly configured red as an option."
  },
  {
    "startTime": "00:50:02",
    "text": "I think that is a reasonable possible target AQM, I mean, obviously not a badly configured one, but it's a simply enough scheme that it might be useful for evaluation. What did we what did I hear I wonder if I heard that we should Consider fee for queuing and AQM, and people should be encouraged to look at both. But not all. And I think that not all is important. I think the second bullet, if I heard correctly, should have had the word new inserted before AQM. Because I think 7, 5, 6, 7 is about designing anybody working AQMs. We're talking about designing and evaluating congestion control. And if we do this 2 together, well, then we require an IETF working group if it worked right, I think. So what we do with that for us. So let's separate the new bit out for the the second bullet on the first one. Are we hearing that we should do fee for and at least an AQA? Is that what you think, Mark, Martin, other people? I mean, I think based on what I've heard today, I would probably write something like you should evaluate the scenario with feet briefle queues for sure. Should also consider the impacts of various common AQMs, and, like, use a word I consider rather than, like, run detailed simulations against Ed that maybe name check a few particularly relevant ones at this time. Although I'm not sure which ones I would I would mention. But, you know, I I think at the at the high order bid is, yes, The high runner's bit is, yes, we should. It should be a criterion and and definitely we should not make them just run and by end of all the of all the AQMs out there. Not an interop matrix. Yes. Next."
  },
  {
    "startTime": "00:52:05",
    "text": "Samantha's next. We were getting a bit of noise earlier. Yes. Yeah, there was a reported noise for me when I was muted, which is interesting. So, desynchronizing flows, the key point there is flow shouldn't control against Q full. Because Q full is what causes synchronization. There's a bunch of other reasons flow shouldn't control against Q full, including lock out. I didn't prepare a full presentation on the draft that I'm presenting next. I don't go into details on it, but there's a lot of these considerations which are can be evaluated by looking at a single flow in a vacuum or a single instance of the protocol or a single collection of instances. And controlling against Q Four is something that should be forbidden. And that is the origin of of a whole lot of bad behaviors that we see in the internet. And, AQM is one way of avoiding hitting queue full. Delay sensing and transport is one way of hitting control full, Q full. And subsized queuing is another way of making it very, very hard to persistently hit q 4, and all three of those mechanisms should be present everywhere. Oh, well, you don't wanna mandate undersized queuing, but, it's inevitable in parts of the network. But, AQM and delay sensing should both be present. So Thank you. Alright. Well, yeah, I think it's out of scope of this document to, like, mandate AQM, but, Yeah. I mean, I I think if, Matt, I think you've looked at the text and you can see that there's a notion of like, avoiding, like, correctly utilizing the link, like, neither underutilized, you know, over over utilizing it, which, you know, obviously overutilizing leads to large queues. So you drain the queue. Yes. Speaking of queues. Great. So this is, we've gotten actually a It may not sound like it, but I I certainly got a lot of great input here."
  },
  {
    "startTime": "00:54:02",
    "text": "Got a lot of ideas for how to resolve these issues. And I are gonna keep hammering away at it. We've gotten a lot of help from Christian and Matt as well. It's pretty active on the GitHub, so please take a look at what's going on there. Like I said, I'll reiterate, like, sorry. Corey, did you have something else you would need? Yeah. I wonder if I could just say. Martin and I are going to try and capture what we heard here. If you said something or you know, I think something. Please, please, please, send email to the list to us in the GitHub, you know, like, this is a team thing. It's not just Martin and I trying to write it. We're just the editors, which implies other people writing things. So, look at the draft. Like I said, it's been restructured considerably. I'll return to what I said at the the beginning. Take a look at it. There are a bunch of things that just say you do. Like, file a filed PR. It'd be great. Yeah. I also, enabled a summary email every week that we get to the email list now. On the GitHub issues and PR system. So, should see that. We had Magnus in the Q And A. Yes. I have a scoping question for this document related to multicast. Uh-huh Because we actually have 2 experimental multicast, congesting control algorithms mean the in ours in the RC series. We have protocols that use them. Is one of them. So I asked wonder about the relation with multicast on this this something we need to cover? Or should we explicitly rule it up to scope? This document. I don't think we should be silent on the that's an amazing question. I'd not even really thought about it. That's And I don't I don't know if the document specifically doesn't sit and says this is not related to multicast. But but Nadia doesn't really talk about multicast in any way perform. So I would be curious what people thought. Is it is it too different to come into this? Do we need a different document for that? Is this just to create I mean, I don't really know much about multicast congest control, frankly."
  },
  {
    "startTime": "00:56:01",
    "text": "Is it so crazy and different that we need to just do something completely separate, or could it fit into a frame work like we have here. Least one of the algorithms is very similar to because it just tried to correlate to that its lowest client works, so to say, but the way one is interested because it's completely received driven. So Okay. Sorry? Corey, up. Done. Quite a lot of work in the deep past on multicast and transport and congesting control and things, This can be a very big pot in itself. And I guess we need more than Magnus to pop up here, and then we'll find out whether we have enough energy to write a section here or whether you're enough energy for a separate document. I don't feel encouraged to write too much myself unless people pop up and start talking. I will say that if we're gonna put it in scope, like, you know, to use the cliche, like, syntax, because I don't know what to write there. I'm a I'm a gory might because he seems to Lars is in the queue. Magnus, could we interest you in filing an issue if one is entirely there? Yes. Please do. Thank you. Large circuits, I would suggest to rule it out of scope. Because it's a pretty special animal and it's by no means, as widely deployed as unicast traffic is. And and also for, Corey might remember 8085, which was the UDP considerations for the version, we actually did add multicast to it. It really was sort of like in the appendix that was high to get written and hard to get reviewed and I don't think has seen nearly as much use as the rest of the, the document has. So I would, until there's a need, I would rather just rule it out of scope because it's burn a lot of time for very little benefit. Yeah. Thanks, Lars. Yeah. Probably special animals are always two questions. Like, do we want to evaluate You wanna, like, have a framework for you evaluating, in this case, multicast algorithms."
  },
  {
    "startTime": "00:58:03",
    "text": "And Lars just said, no, which is great. And the other question is, do we want to, when you have an, like, a, a, quote unquote, normal congestion control, do you want to its impact on these special animals and Multicast is being rare. I would say, no. We do not wanna require that kind of evaluation. Hi. I'm I've also got a grain up there to have done multicast congesting control for about 10 years. And, I I I think I agree with Lars that it would be better to put it in the document However, I think that document would be a lot easier to write after this one. Because lot of the things are common. Congestion control itself is common. It's just the feedback. Has to be different. And the the judgment of you go for the minimum or not and things like that. But otherwise, the congestion control itself once you've dealt with those issues on top of top of those issues is pretty much the same. Okay. Thanks, Bob. And I think that drains the queue. Yeah. Thanks everybody for the excellent discussion. Thanks everybody. Chrome. Careful, And next, we have a presentation from Matt. Next. Next slide, please. Oh, yeah. Sorry. I just passed your slides control. That works. Should be able to do it yourself. Oh, I can do it. Oh. Okay. Never never use this tool. Cool. So I started this document, about a year ago. Actually, it's been sitting in fragments in my brain for a long time. In the, the intent was to have a sort of a first principles design or approach to,"
  },
  {
    "startTime": "01:00:01",
    "text": "understanding what congestion controls might what how they might harm each other, how they might harm the internet. And write down some of those rules. And I started this document before CCWG was formed in particular. And I didn't really know how to expect to be be scoped, but and I'm using this as a placeholder to keep a bunch of thoughts, But in the end, I think most of the information here is going to end up in 50, 33 biz, and some of it is too far out to make sense. But the the idea is just just to have a bunch of criteria, which you could apply sort of independently to a given congestion control and decide whether or not it was safe, whether or not it was gonna do something that might harm Internet. The current draft list 13 criteria, I'm only gonna talk about couple of them here. I'd I'd did did decided I didn't wanna make it 3 hours long and try to talk about the reasoning all of them in detail. And it doesn't directly address anywhere how congestion control of compete or anything that resembles fairness. And, I've come to think that those issues are actually very often secondary compared to the other issues here. I said, I think a lot of it will end up in 33 biz or has the potential of ending up in 33 biz. It's it. But there are other parts that are just too far out. So One of the things that that I actually have come to think is a very serious problem, and it's sort of independent of all sorts of of of other things is how much loss can a congestion control inflict on itself which it implicitly inflicts another flows. The current documents is 2%."
  },
  {
    "startTime": "01:02:01",
    "text": "Oh, I didn't put it in the document. That number happens to at least one version of BBR. I don't know if it still matches BBR. But that was where it was picked from. I've always been a little uncomfortable with that 2% being a little bit high because it adversely affects all connection establishments for all other protocols. And I don't have a good mind mental model for how to decide. If that, what's the right what's the right balance there? And it does it is sort of a global constant. A global because transport in some parts of the internet will inflict that level of loss on everybody else. I've in my Some of this, actually, the process of producing this debt caused me to change my thinking about some of the point to the document. In general, but several places. There is also some properties that you want, the steady state. It's actually average steady state loss and average rate to have. And you want that relationship to be gone on time. And although this doesn't seem complicated. Some of the previous congestion control algorithms people have presented have submitted by stable behavior or a latecomer advantage or latecomer disadvantage and things like that. And those are all symptoms of not being monotonic in their in the atmnan like, in, like, control functions. Yeah. So if you across the board put this requirement in that the average is is monotonic, that actually would have excluded some of those algorithms before even getting into complicated testing. Another issue, which is which is sort of probably the definition scaling is that the control period or the control frequency has to be related to the RTT and not to the performance of the of the stream. If you don't do this, the protocol will not age well as, you know, hasn't."
  },
  {
    "startTime": "01:04:02",
    "text": "And is cubic is is running out. And one of my reasons thinking about this document this way. As this independent list of criteria is I thought a lot of the complaints about other algorithms like BBR in particular about not being renowned friendly or not being cubic friendly was actually inflicting bug compatibility, on newer protocols with bugs that were present in the older protocols. And this is one of the places where I mentioned that Reno and Cubic do things that I think should be forbidden. to all different types of control signals. It's not monotonic, it will do something weird in some some states. Queuing delay. All condition controls have to have a consideration queuing delay. It has to be a bound, upper bound. And if you don't do this, the pathological case of a of an of a very large FIFO queue is gonna cause very large delays. And even if you're throughput maximizing. Like I said, in the other part of the of the meeting. When you're throughput maximizing, you do wanna have at least a small cube because that's the way you you actually know that you're filling the link. And small might be very small. But it turns out that without ECN and other signals, you can't make it too small, And so there's a there's a compromise, and I would expect throughput maximizing flows to always create small queues. And it has to be bound. And it has to be bound actually relative to the minRTT. The minRTT, however, has a robustness problem. And that is there's an old result. It's now more than forty years old. That,"
  },
  {
    "startTime": "01:06:04",
    "text": "mean RTT estimators are sort of fundamentally fragile because you cannot a given flow cannot tell if the min RTT that it's was actually inflated by another flow. And this failure was actually demonstrated for Vegas TCP. And I believe it to apply to all delay sensing t TCPs all day delay sensing congestion controls. That The late mendeley measurement algorithm in BBR is designed to evade this problem. I'm not positive it does completely. And it it One solution is to really have AQM everywhere. Because of not having AQM is one of the conditions for getting into the failure mode. And the other thought that comes out of this is that it may be the case that one of the things we need standardize. Is a min RTT estimator, which is runs in all transport protocols because the algorithm in BBR depends on being able to synchronize min RTT measurements. And if you do different strategies for doing that, it And EBR is not a majority of the traffic that algorithm won't work. Bisco, you have Bob, you have a clarifying question? Right. No. He wants to leave it to the end. You didn't hear that. Let's leave it till the end. Okay. And there's also another problem with with with measuring cues, and I don't have a good answer here. And, actually, the document doesn't talk about this, but it kinda glares at me as is there's a bunch of cases where the minimum delay is not fixed, and the protocol needs to respond appropriately to those. Transport. Condition control. Another criteria, which I actually think is much more important than"
  },
  {
    "startTime": "01:08:04",
    "text": "than many forms of fairness, any form of fairness, is freedom from starvation. And the way I've defined that here is that small flows Large flows don't start small flows. And But the distinction between large and small has to be somehow defined by the environment and the network. A point that I don't make in slide and actually don't make any document either. Both of those thresholds are very, very sensitive to them. I'm on a noise in the ax. If you have something like a a wifi link where this channel arbitration and the return path and the acts are very unevenly spaced and and covered different numbers of packets on the forward path, the ability to Precisely control anything is is impaired. And in fact, both of the officials get smaller. And so the the their, Aaron's theorem on, on filling arbitrary networks, actually hints at some of the issues was trying to do this properly in a formal way that you can't I I I I wish that his paper was actually showing that you what the limits star of starvation was rather than showing that you can't do starvation. I mentioned earlier that that a a a simple item, which actually helps goes a long way. It's just for bidding condition controls from maintaining full queues. Shouldn't be it shouldn't be okay, and it does harm everybody else. Always. So and like I said, I think I think 5033 is gonna say something vague here because I think there's some deep research in this question. So, my"
  },
  {
    "startTime": "01:10:01",
    "text": "document defines a a concept of under adverse conditions. And the way to think about this is a way of talking about making small incremental changes to the network, makes the network slightly worse, and that certain things must not get the condition control must not do other things in response it. So you look at the entire operating space that you might imagine the entire operating space that you wanna run the condition control in. And then making sure that the congestion, the CCA doesn't do something worse, when you make the conditions it works. And so this is a a test for, Well, all sorts of things. I think of it as being very, very similar to the way Epsilon Delta Group's work in mathematics and stuff like that. Is the way I used it. I've come to realize that this is actually probably over a fied in in for at least one case. But, rf33 5033, this actually contains a definition of for condition collapse, which, is essentially a requirement of doing back off and ways of operating in environments where the the fair share window size is substantially smaller than a single segment. I've observed 2 other con definitions of of congestion can collapse. One is that the overhead must not increase under adverse conditions. And the other is that future transmissions are always delayed by some small amount of congestion. And I talk a little bit more about each of those here. So the the increasing overhead is what caused the con condition collapses in 8687. Which was a little bit before my time doing this work. I wasn't didn't have a front row seat."
  },
  {
    "startTime": "01:12:00",
    "text": "But at that time, the TCP in use, which predated Reno was go back in was fixed window go back in. And it turns out that when it gets when it gets heavily loaded, you end up with a lot of duplicate data at the receiver, which isn't necessarily the data you're looking for, and you keep re transmitting the same things because the same segment keeps getting lost. It it it it This particular metric over the overhead must not increase. It's precisely definable receiver. It's easily testable to receiver. You can imagine doing this in a bench bench testing rig and just look at it all over the place. It's also actually pretty easy to discover, but just by inspecting the design. Well, this this is, this is going to misbehave under certain conditions. Which shortcuts a lot of testing and avoid testing things that You don't need to test. I think we as a transport community understand this pretty well. I I don't believe the application community does. Which is a separate story. And the other issue, which is observable at the sender, is that you shouldn't increase the load in response to congestion. And I know there are a lot of applications that do this. Or have been the The the basic criteria is if you look at the total elapsed bite versus elapsed time, if the if under adverse conditions, that curve has to shift to the right it's simply said it is that. And, again, It's This is real easy to test in the bench test. And it easy to think about in in thought experiments and just to cover places where people, for instance, don't account for the fact that they're doing the retransmission one common strategy is to do the retransmissions that are sort of synchronous with the fact that caused them. If you don't do that carefully, you end up actually increasing slightly increasing the window in a it's very easy to make them design. We do pretty well."
  },
  {
    "startTime": "01:14:03",
    "text": "And and this one, I believe, even though it's not written down. So my my plan is to try and push the stuff into 5033. I I'm a little bit of a loss at how to generate mergeable PR. Because some of them feel like they're gonna be too big to fit nicely. I would love some discussions about what the self limit on self induced law should be. And there's a whole bunch of open issues about transient behavior startups and such. So questions. Guess we can have Bob now Alright, Bob Briscoe. Thank you, Matt. I I think I nearly agree with everything, and I I the reason I raised my hand was the order you presented these in made me realize there's a possible conflict between 2 of them, the first two you brought up, which is the limit on loss probability and then volatility increasing. Because the congestion control once it gets into the loss regime, say like BBR. As more flows enter the system, the loss has to monotonically go up. So how can it limit it? The so this is the difference between right. The loss goes up so the rate goes down. Yeah. And so That's the monetization. So the rate goes down and as more flows enter. So a congestion control has an upper bound on its loss. You're Right. This is true. One of the one of the things that that I've been using about is when Rina was first implemented. We didn't have SAC at all."
  },
  {
    "startTime": "01:16:03",
    "text": "And the only recovery algorithm that's fast retransmit. And it under moderately high loss. I I I actually don't know the number. I would love love to number a few percent loss, it becomes time out driven. And so you could not see the state which was synchronous self clock. Driven by acts with sacks as necessary. To run the flows. I have seen in I believe in a real test bed, although it maybe was a simulation. I it don't remember it was years ago, maybe 20 years ago, a flow that was running sustained 30% loss or something because every time it increased the window, for the window would be 3 packets, and every time it increased the window, it would lock lose the the the window increase. And so it did it was a fixed maximum pipe size path and it would run an astronomical rate. Loss rates. And that's that shouldn't be okay. And it I should say, speaking about average rate, it's actually very complicated because the way BBR limits the loss rate is it sort of makes has it built in assumption that loss only happens during probing? Because other mechanisms reduce the rate if there's loss, not during probing. If loss only happens during probing and your loss rate is too high, the thing to do is reduce the probing schedule. And that's the way BBR addressed this particular problem. But Okay. Are we You I see you still have questions. Well, well, no, I mean, I I'm not sure that answered. The question because if you you're still homes Or are you saying that the loss rate doesn't have to increase once you get into an RTR regime because you're doing the financial back off. So"
  },
  {
    "startTime": "01:18:04",
    "text": "don't actually get higher loss rate. You just get longer between the losses. Right. So this is this was specifically, this is actually something that Somebody caught later, and I didn't fully internalize it until actually preparing these slides. The document doesn't make it clear that it's self induced loss And so the situation is if you have a path where the maximum windows the maximum pipe size is 2 packets. What loss rate do you cause? If you have a path path with a maximum, path sizes. 3 packets, what loss rate do you cost, and those numbers have to be below some threshold. And if you go below that, the only way to get below one packet is is is either exponential back off for self clock protocols or for for PACE protocols, you can go below that by having the pacer rate pacing rate. Below their own trip time. Right? They can they can And there's there's actually a phase change that happens with clock protocols that you don't have to happen with a PACE protocol. Yeah. Can make page protocols go arbitrarily smoothly from from average queue occupancy of 0.1 packets to 0.01 packets to any packets. Okay. So so the other point that that raised in my mind, which needs to go into this document, I think, is testing over very low round trip time paths. Right. That's the way to where that's the way to do the experiment. And and that's That's where the slide makes a note about methodology is not mentioned because, yes, specifically, it's it's testing and paths where the window size is 1, where the window's max window size is 2, max window 3. And and and, you know, just just to justify that for everyone else, the there are becoming more and more low rand time parts in the internet as edge caches get closer and closer to to, the engine across the point, you can few There are"
  },
  {
    "startTime": "01:20:00",
    "text": "Our cousin. Yeah. There are paths in data center where the round trip time is smaller than the TSO time. So you get the act from the first fragment of a TSO before you finish sending the last segment. I'm just trying to justify it in the public internet not just in data centers. Yeah. Okay. We have Corey next in the queue. We're likely to cut the queue shortly. So if you have something you'd like to comment on. Now is the time to stand up. Alright. Next, and I was worried about minority tea. Mean, and how we understand this, big cause we have a growing number of layer 2 networks, which do quite a lot of work to get their capacity sharing between different user equipment, different propagation conditions, and these naturally vary the RTT. And What is minRT in that sort of environment? I mean, because if we model it, it's easy. In that environment, it's quite hard to pick a good min RPT on that only lasts for a short period of time. How do we reconcile this now? Well, I consider I wish that the queue occupancy limit was a limit of last resort that is other mechanisms like like AQM ought to kick in first. The the The likely value for the queue size has to be approximately the round trip time. I think or one and a half times around trip time, I think is what BBR hits. I I don't actually know. I've asked Neil to think about BBR in terms of my questions and have gotten all of the answers But but the minimum RTT at best scales Excuse me. The the the queue occupancy scales with MinrTT. And"
  },
  {
    "startTime": "01:22:02",
    "text": "you have to put in something. And even if MN RTT estimator has got all of these warts on it. You still need to arrive at a number, and still need to use that number to make decisions about whether or not you need to slow need to stop inflating the window. Okay, slow down. Just because you don't want it running in you full. And if I have a five o'clock that's not doing AQM and not dropping packets, All of the condition controls without delay controllers are going to hit full queue, and I don't wanna do that. I fundamentally don't wanna do that. It'll it'll cause starvation. It will cause Buffer Bloat, it It does all sorts of evil things that you don't wanna do. Okay. And this implies that, for instance, AQM based algorithms, have to be sensitive to delay. So advising part of the supposition that the delays due to queuing but part of the delayed due to queuing is due to radio interface. Queuing where you're queuing to get an access opportunity to send and that queue, you don't want to drive to 0. So we have 2 different queues, and I've still intellectually trying to separate how we detect the 2 different queues we have. So I like I said, I don't I don't think it matters. The the minRT estimator will get a number which you can debate about its validity on a particular path, And then the transport will use that number to limit how much how much backlog it's gonna create. How much backlog that the transport knows about it's going to create into the to the to the extent that the MNR estimator is wonky, the backlog will be wonky, but It's better than infinite. Infinite is what we have to avoid. I agree. Infant is what we have today. So Okay, Stewart. Matt, on one of your previous slides and on this slide, You talked about goals for self induced loss. And Right. Maybe there are some"
  },
  {
    "startTime": "01:24:01",
    "text": "different assumptions underlying your slides that I'm not seeing. But, I think you asked if targeting 0.1% loss is too ambitious. And I don't think it is at all. I think that's what we should be doing in this group. In a world before ECN where transport protocols are designed to cause loss Loss is inevitable. On the internet. You can't avoid it. However, big you make the buffers, If the senders will increase their rate without limit, until they cause a loss. Then We've built our transport protocol, so by design they cause loss. And that is just the nature of it. With ECN congestion marking, I would hope that the bottleneck queue And we don't need this at every hop on the path, just the the bottleneck where the queue is building up, if it signals congestion, before it actually runs out of space and loses packets. And if the sender responds to that by slowing down. I think we should be setting our goal to be point 1 percent or or much less than point 1 percent self induced loss. So am I missing some assumption you're making there? No. You're you're You are exactly correct except there's a flip side of the problem. And that is the other thing is you can't avoid the situation of slow start hard hitting the the Q Max on a short queue that on the path that's long enough round trip time that you don't get the ECN marks in time. And, that situation causes typically 50% loss. And it At one point, I designed a congestion controller, a module that went into BBR. Was years ago, and I never published it. Design the module that went into BBR. That could enforce a strict policy on total losses because it impair the"
  },
  {
    "startTime": "01:26:01",
    "text": "probing, BBR's probing, if it didn't have the loss budget to do it. And the consequence of that controller was that you did one slow start smashed into the wall, and then you were never allowed to open the window again. And Okay. It Hey. There has to be, unfortunately, there has to be a compromise there. And The other things that I talk about, which I didn't mention at all today, is There really needs to be a lot of of mechanism, which is not strictly transport, but connection state reusing or something. To minimize the number of slow starts, applications that do 100 of slow starts in a very short interval are not safe. Full stop. Thanks. Okay, Martin, please. Martin Duke Google. So reactions to 2 things on this slide. First of all, self induced loss. That, that concept does not I mean, obviously, 5033 biz has things like buffer bloat and and so on, but doesn't explicitly have that metric. It sounds sort of reasonably that we could have that, is an explicit criterion in 5033 biz, although I'm not fully convinced of that. I've I've just created issue 42 on this exact subject. Please to GitHub, go to issue number 42 and, and, like, share if you think this a metric like self induced loss should be explicitly in a draft as, like, as, like, evaluation criteria. And, like, what number you think and how we should like measure that, etcetera. The other thing is, you're as an editor like your your your difficulty in contributing to the 33 best. Matt, have you had a chance to look at the big rewrite we did, the big reorg we did. Yes. I actually didn't look at it hard until after the rewrite was done. Okay. And and so in that framework, you're still having a lot of trouble. Well, I'm not"
  },
  {
    "startTime": "01:28:02",
    "text": "I I find it difficult to to understand how you use text merging and get on Okay. Unflowed text. We'll do it that way. Let's Let's let's connect offline and work through whatever issues you're having because I definitely would love your contributions directly into the document. Thanks. Okay, Howard. Harald Armstrong, as usual, coming in from a different angle, You had one slide, earlier where you basically said that when packet loss increases, the number of bytes sent must must decrease. I think that's what they said. And that No. So there were 2 different things. One was was, overhead at the receiver. It actually turns off the overhead receiver should be constant. It never occurred to me, but this is this is completely straightforward statement. If I do a pat particular piece of workload, And I look at the payload delivered, and I look at the overhead delivered, where the overhead includes any duplicate data at the receiver, includes headers of an all that kind of stuff. The ratio of overhead or ratio of total bytes to payload bytes should be constant. Under all conditions. It's like it never occurred to me before, but once you see why it's obvious, and it's like that test can be applied 2, 2, a simple transport or an entire stack. So the the scenario that, A friend of mine, a colleague of mine suggested, And that made me nervous. Was, a scenario where When you when you start seeing packet loss, over a certain threshold. I think it was 5% of something. You start sending forward error correction data. Now that would increase"
  },
  {
    "startTime": "01:30:02",
    "text": "the number of bytes, the overhead, and their adverse conditions. It would only do it once. So if I could claim that this criterion is okay if if this in piecewise, I could get away with this. But, it's really a bad idea Then I should go tell my colleague that this is a bad idea. So I suspect that fails the regenerative congestion. And, I mean, the question asked is if you have have 2 flows that occupy together, 70% of the bandwidth of a link, and then you add a third flow. What happens? Where you end up the total end up being a 110% of the of the link. At the end of the day, the payload delivery has slowed down. And if the FEC tries really hard to avoid that, something's gonna blow up. Yep. So if the, if we, if all the flows transition to FEC, which, and then occupy 50% of the bandwidth, we would have a 50% of the script section. Right. Then if it if they slow down at a decent rate, you would get back to a steady state Eventually. At the end of the day, you have to slow down the content delivery. And if the FEC has got a control loop in it where it keeps ramping up the FEC, to try to deliver all the data. It can't fit. Okay. It's when they can't fit. So I am trying to be nervous. Thank you. We cut the queue, but we had Lars in the queue earlier. So I'm just gonna let him Thanks for letting me jump back in, and I'll try to be brief. So I wanna zoom out. So, usually, there's, with med med meds presenting. There's a lot of deep stuff in here."
  },
  {
    "startTime": "01:32:01",
    "text": "And it it kind of feels like we're sort of maybe revising our understanding bit of what the criteria should be, and that and that's great. But 5033 is a BCP and the for current, best current practice or best future practice. And so I I worry a little bit that if we start merging these very sort of emerging understandings into this document and then people come here to present their conditional control algorithm having not been exposed to this before, we're gonna set them up for disappointment because we're gonna go. You did not, you know, adhere to this, and you're like, I've never seen this before. So so it almost feels like, you know, there's a there's a so if this is the direction in which our thinking emerges in terms field and the working group. Right? We gotta sort of communicate this widely out there. And and sticking into BCP is probably not really do that. We need to go to, like, the the sitcoms and whatever else workshops with this and and tell the researchers. This is really how you should start thinking about these sorts of That's right. So I I I don't think this merging into 5033 is is really that productive, or we shouldn't really worry about it, but we should sort of really figure out What is the emerging direction? How do we explain this? How do we, like, and then how we socialize it? Brought more broadly. So some I made the point earlier that there's actually 13 different criteria on this document. Some of them, I think, are actually ready to be merged sort of immediately or I mean, there's a bunch of wordsmithing that needs to be done because the tone is very different than 5033. Some of them are not ready. My default intent is to keep this document alive sort of a wiki of what my current thinking of all the criteria is, and you can point people to it being unofficial. It's like, but if your protocol doesn't pass item number 5, Go think about it because it probably should. And and you can say that without the document being fully vetted. And at some point, somebody thinking about item number 5 might discover that item 5 is wrong."
  },
  {
    "startTime": "01:34:02",
    "text": "And if item 5 is is not well thought out yet, it shouldn't migrate into 5033. This this this whole idea the the the definition of collection can congestion collapse in 5033 is is actually completely inadequate because Although the exponential back off is absolutely critical avoiding other there was a bunch of video players Yes. That responded to congestion by opening new connections. It's like Forget all that. What I'm saying is like exposing people to these thoughts when they arrive here to maybe they think they're starting standardization, and we you tell them you gotta redesign your algorithm. So I think sort of exposing these ideas earlier like, ideally when they're still, like, in the design phase of the algorithm is what we want. I would love to in my copious spare time. Do you have funding? Right. And I would also love to see some issues in discussion on the GitHub or maybe the mailing list, Let's move on to our next presentation. So We have Yossi here. Hi, Yossi. You know, can you hear me? Yes. We can hear you in a past due control of the slides, so you should be able to advance yourself. Okay. Thank you. Okay, Harold. My name is Yossi. Today, I would like to talk about analysis for the difference between and that condition control schemes. My talk will be very simple and straightforward. Okay. So let me start background of this draft. So we already have a several consistent control standards such as a quick And then I personally think these, you know, concession control standard, provide a consistent guidelines, they should not contradict each other."
  },
  {
    "startTime": "01:36:02",
    "text": "And this there is a very good reason for it. However, when I check this draft, it seems to me that there are several, you know, differences more precisely. It's a contradiction in the draft. That's why I started writing this kind of draft. And then, I think writing this kind of difference of consistent control standard. Could be useful when you try to, you know, do some kind of analysis of the concession control. Who I, you know, eventually you're going to develop next generation with the contest and control. So that's why, you know, this kind of document may be know, use refinance. That's, no, my another motivation to write this Okay. So so corrupt this draft is to publish it as, informational RFC as a reference. And then so, basically, you know, this draft try to clarify that defense between a consistent control standard. And So that's no we can, Think about, you know, what we should do next. So they said that this document is just the information. Just describe this is the difference and how it is different. That's all. This draft doesn't say We should change it. I'm gonna reset the rebate. So because, you know, the motivate goal of this draft is, you know, try to initiate the discussion for the next step. And then, also, another goal of this document is know, it this kind of document that can be used as a critical reference, if you, you know, I guess later, maybe someone contacts some more a comprehensive thing consistent control analysis or, developing a new concession control standard. K. So what is in this class? Since I already"
  },
  {
    "startTime": "01:38:03",
    "text": "you know, have a presentation in the last idea of meeting. I just try, you know, keep it simple. So this draft just provide the list of the difference on a certain topic in congestion control standard. And this product mainly focus on 3 standard. Which is TCPilo Creek, you know, and cubic. And then the main topic of, this track is, describing the difference between this video and the click, you know, so this is because, ideally, TCP, you know, the pre could, you know, should not be different regard to aggressiveness. And so that's why, you know, I create a list for the differences. Oh, And then another topic of this draft is, device between Dina and the Cubic in terms of fairness. So this is because, you know, when we standardize cubic specification in TCP and working group. We have very lengthy discussion about between dinner and the cubic, And fairness has been with, you know, difficult matrix to evaluate, but, we have very a nice, you know, discussion when we stand at the Cubic So I want to keep it as a record and then publish it. And then you didn't study people for read this kind of information, contact another very find such, and they do it really good work. And then we can get the Amazon inside. That's a much additional this stuff. Okay. So this is a last slide of my presentation. So, in TCPM, we started discussing about updating 56, 81, we are not right now, at this point, we are not sure we're doing whatnot. But, at least, you know, I would like to think about the relationship of this raft."
  },
  {
    "startTime": "01:40:00",
    "text": "And this, RXT 56 month, 5681 District. So what I'm thinking is, you know, even if, you know, we are working on updating 5061, 81 B slot. This document that can be a useful reference. When we try to revise 5681 because, you know, this draft contains several discussion points such as window, rose window, me are ACbike County. And I think this topic will be important topics, and we, you know, think about uploading 50, 6, 81. As a reference, this document, should be usable. And then also, this document also contains a discussion other than Reno. As I, you know, mentioned, that this draft discussion have a discussion about QB and Renovianization. And this kind of information will be useful for a difference, for the future condition control analysis. Or she's a condition called the development. That's why, you know, I want to publish this graph if you have any comments or suggestion, with the remainder. Excellent. Thanks, Rashi. Question. Yes. I I have a a problem there. I I have 2 problems. First, I have a prime as a concept of a TCP concession control. Because we we have a a number of documents that make reference to TCP position control. And 5681 is one of them as if there is only one economic transition control when in fact that his tree in deployment right now and probably more like 4 or 5. So, yeah, there's this idea that there's one genetic PCB control that we should make reference to. And I think we should Go away from that."
  },
  {
    "startTime": "01:42:03",
    "text": "One reason we should go away from that is because the one other entity's reference like that is Reno and Reno is actually too bad, and we should not be using Reno anymore. So, I'm I struggle a bit with this business of reference TCP condition control when a. That's not true. There's not one classification control. And, b, when people say that the English company TV is we know which Matt was describing as being but compatible with the bugs that I met 20 years ago, And And that's that's really an issue, though. And we we have to be careful. Okay. So, there are several point I want to make so fast. I think, Iman understanding if you talk about concession control for TCP, and 56 one, it's, basically, it's a basic congestion control for this and then we also another which is cubic. And that's only 2 transition control algorithm we have for TCP. Right now in my understanding. And then And it's not true. Not not not not in deep traffic that is not true. Which one you are talking about? Bb alpha, example. Maybe it's not the standard. That's what I mean. No. But it it but it is widely pride. Well, this is in from my case point of view to understand that's part time then. Basic Yes. But that's that that's precisely the problem is that we are blinding ourselves and saying, hey, It is not in the IETF, so it doesn't exist. You know? Well, but, if we, you know, do you know, is it really bad? Then if we I can believe so then we have to make it obsolete. That's the right way to do it, but I don't think I I personally disagree with it because, you know, if you think about cubic,"
  },
  {
    "startTime": "01:44:01",
    "text": "cute because they're being, you know, friendly remote. It actually behaved like burrito. And So, you know, even, you know, they not algorithms, you know, basic, I used to especially, latency is ready. Round trip time is very small. And that's why I think, you know, having been is also useful. But, yeah, that's my opinion. And then, yeah, I take your point. Okay. And that is next. Yeah. The whole reason I worked on the safe CC draft just because of exactly the problem that Christian mentioned. That that comparison that that Reno and Cubic both do things that should be banned. And, Yeah. We need to fix that. And until comparing to them is just not a good thing. Cubic is is very good at Keeping queues absolutely pegged. And And that causes lock out and buffer bloat in extreme unfairness. And and a bunch of different ways. And And both Reno and Cubic with SAC are very dangerous at very short round trip times. Those they're just not okay. And to We need to get past them as soon as we we can. But, which kind of a alternative if I if you don't just, you know, Pardon? I didn't understand that. Sorry. Which kind of alternative we've had? Right now, at this point, Well, I think BBR does a pretty good job, and I think the IETF is built and then also barrier to BBR. Because trying to Right. Because of force BBR to be bug compatible with Reno. Happily. That's part of what we're here for. That"
  },
  {
    "startTime": "01:46:02",
    "text": "why we're here. Yes. Let's let's keep going now. Thank you. Sorry. That's a that's a rat hole hot button, but yes. Yeah. Martin. Martin do Google. Thanks for the presentation. You know, she like, rather than rather than, like, keep putting effort into this draft, to click out as to create a reference for 5681. I I I I think it would be more useful for the community to actually just put effort into, doing, participating, like, whatever document energy you're putting into this just to put in if it's 681 biz. I am not in any way in charge of the 5681 bis, effort, given how, you know, given how how long my term is. But, if it were me, I would revise 5681. To essentially update the loss recovery bits of TCP, and they just have a pointer to whatever standard congestion controls there are. And then Well, in my mind, I would then have a separate document maybe run through here that it breaks down what Reno does. Now, like, I I guess Matt and Christian would say, no, we should we should just like just obsolete, Reno, and not bother to do another document and, like, that's a discussion we can have. But, to me, that would be, like, the most useful if if if you would like contribute in this space. I think that is the most useful thing to do. Mhmm. Yeah. If we, you know, if we want to update 50, 60 I will directly contribute Of course, Okay. We do. The queue, Let's follow-up on the soft line. Yep. So y'all see are you happy with what you got Okay. Yeah. I don't have an in sync today. Thank you so much."
  },
  {
    "startTime": "01:48:01",
    "text": "That's what I want to do for the you for the presentation and for the discussion and we have one last, fun talk here. Are you sure you're ready? Uh-huh. Yep. I'm ready. Can you guys hear me? We can see and hear you. Thank you. Okay. Awesome. So hi, everyone. I'm Ayush, and I'll be presenting our people title to containing the Cambrian explosion and quick congestion control. This work was done in collaboration with my adviser, Ben, at the National University of Singapore. Next slide, please. So I think the discussion in this working group so far has concerned itself with deployability of congestion control algorithms However, what I hope to do through the stock is motivated the fact that deployability is a property better assigned to an implementation rather than the algorithm itself. The work I'm gonna present today shows that this already true for CCI implementations in popular quick stacks. So even though these quick stacks try to take the pragmatic root by trying to implement well deployed and well understood algorithms like cubic and DBR. There's still significant speciation between these implementations. Next slide, please. So in our measurement study, we set out to measure how conforming the quick implementations of standard CC as well. With, in comparison to their TCP implementations. So obviously, the assumption over here is that Linux TCP implementations were safe and deployable versions of these algorithms like Cubicran on BBR, but what of them being well tested and well understood. So in this case, quick, you know, becomes, case study for 5033bis. Where it would mean that we are determining the deployability of these implementations by measuring how close they were to the safe and deployable versions of these algorithms where safe and deployable versions were there, Linux, TCP Implementations."
  },
  {
    "startTime": "01:50:00",
    "text": "Next slide, please. So how do we actually measure the similarity? We have one implementation of an algorithm that we're happy with, and we want to see how a new implementation compares to this well established similar, stable implementation. So a fine grained approach to making this comparison could be just purely comparing the semen graphs. So We could give both the implementations, the same network profile, and then compare this even graph. But in the people, we argue that the might be too restrictive and unrealistic because especially in quick, depending on how you implement your algorithm, what kind of act frequencies you use, but they use you're using a fast path or a slow path, your actual semen graph might be different even though you're implementing your algorithm reasonably well. And of course, if you have a super fine grained approach, you also run into the of setting a unrealistic target that, you know, no one's interested in meeting. The at the other end of the spectrum, you could take a course creative approach. Which is to compare relative fairness. And I guess this is something that's discussed in the current version of 5033 Best where, you know, we have some standard background traffic, and then you see how it competes with the standard background traffic and you're, like, you try to figure out whether that's reasonable or not. So I guess for deployability, it's still okay. But when you're trying to compare implementations of the same algorithm, it will not work because it wouldn't mess the final algorithm. Make differences and just reduce the performance of a congestion control algorithm to the throughput that it achieves. So the middle ground that we found, that's sort of it's not too unrealistic, but still allows us to characterize the implementation of CCA is a new metric that we call the performance Next slide, please. So the performance envelope is a metric that's built on one key insight"
  },
  {
    "startTime": "01:52:00",
    "text": "And that key insight is that different congestion control algorithms represent different trade offs. In the network. So when we are comparing different congestion control algorithms and their implementation you want to sort of capture the trade off space in which they exist. And then we want to compare the straight off space among the different implementations to see how well, and these implementations conform to each other. Now by definition, the state of space can be depending on what you care about in your network. But in this talk, I will limit the performance envelope to being a two dimensional metric where the two dimensions are super and delayed. Next slide, please. So how would a performance envelope actually look like? So Let's see if you're interested in benchmarking. Some suggestion control algorithms implementation, and it has the following throughput and delay in response to some fixed network condition. Next slide, please. We would then sample this time series data at regular intervals and then plot them as payers and the throughput versus delayed. Then once we have these through, we would be free to, you know, fragment them into different clusters. Get rid of the outliers, compute the convex hull and basically come up with the region that sort of defines the throughput delay space in which this algorithm exists. Next slide please. And then this region, which you would get by the end of this procedure, is what we call the performance envelope. In the interest of time, I would not be going over the specifics of how we get rid of the outliers or how we have a modified clustering algorithm to come up with the number of clusters for all these details, you can get in touch with me offline or find these details in the paper. Next slide, please. So once we have the performance and love of the implementation that we're interested in which over here is represented as the red blob, we could compare it with a standard or a reference implementation and the overlap between these super"
  },
  {
    "startTime": "01:54:01",
    "text": "formats envelopes would actually give us an idea of how conforming they were to each other. So, again, the details on calculating this overlap are more specified in the detail, but overall, it's pretty intuitive. Conformance lies between 1, which is a complete overlap and 0, which is no overlap at all. So naturally, the higher your conformance value the more, the better your implementation is and the more faithful it is to the actual algorithm. Next slide, please. So in order to so once we had come up with this measurement metric. We benchmark all the quick stacks that were deployed source and implemented some congestion control algorithm. I list all eleven of them here. And next slide. And what our measurements actually found out was that there were 7 implementations of standard congestion control algorithms that actually showed pretty poor performance compared to the kernel counterpart. And some of these of these nonconform variants are quite significant. Because they have been widely deployed, and they support a lot of traffic. An example of this would be Mufas BBR. Which in our measurements showed zero conformance yet meta users, move fast to support 75% of downstream traffic. So if it's using DBR for this, it's a it's a pretty a significant deviation Next slide, please. So what what is the impact of having you know, these non conforming implementations. Of course, all of us are very well versed furnace. So a very well expected repercussions of this would be having unfairness between this these different periods. But I think another impact I want to bring all of your attention to is the subversion of expectations and how we expect different condition control algorithms to interact So"
  },
  {
    "startTime": "01:56:00",
    "text": "As we all know, since PBR was, introduced the interaction between Quebec and DTR has been the focus of majority of congestion control research. In fact, I've done almost my entire PhD thesis on just this interaction. And in this, basically, in this interaction, one well known trend that everyone agrees with is that Cuba gets more bandwidth indeed buffers. And BBI gets more bandwidth than shallow buffers with the virtue of one being a buffer filler and the other one being delay and bandwidth sensitive. But when you come to evaluating quick implementations, it turns out this trend can change and even reverse depending on the quick implementation you're looking at. So if you have a shallow buffer. There's a heat map on the left. You expect your DBR implementation to lose out consistently to Quebec. So you would expect the entire heat map to be red. But as you can see, extra cubic, which is one of the rare a nonconformant cubic implementations it actually reverses the trend. And you see the same reversal of trend between cubic and BBR and deep buffers as well where, you know, you're expect the entire heat map to be blue, but you see certain red streaks for nonconformant implementations of BBR. Next slide, please. So where does this nonconformist in these implementations come for come from. So in the paper, we discuss in more detail how we can provide hints as to how you correct your implementations, but here's the crux of the implementations we looked at. With DBR often it's improperly set parameter us. And examples of this is move fast and quick. Both these implementations actually did a good job implementing the algorithm, but you know, because they want to get better throughput, they tweak the final, pacing in to 1.5, and then they are actually just sending 50% faster than what they're supposed to. Then, with key stupid, for example, we also saw that depending on how the rest of your transport stack is implemented, that can also impact the performance of your congestion control algorithm. So for example,"
  },
  {
    "startTime": "01:58:01",
    "text": "their cubic implementation was implementing spurious loss detection, and that was actually creating a big difference between the vanilla TC the implementation we were comparing it against. And lastly, we even found instances where you look at the algorithm, all the parameters are correct, The algorithm looks reasonable, yet the conformance is very poor. Which would actually point to the fact that you can do your best effort to implement in algorithm, but you can still have stack level details that can decrease your performance. Next slide. So to put all of this in context with the discussion I think you've had here. I would like to point out that in its current school, 5033 best recommends evaluating deployability of congestion control algorithms But given what, our work has seen with Quick, there is probably a possible direction where we want to attach a standard implementation to the RFC that we are considering deployable. And then we can measure the conformer of every other implementation to the standard implementation in order to make sure that other implementations that are claiming to confirm with this deployable RFC are actually, you know, following suit. The last is it also opens up questions regarding, you know, how do we want to deal with differently tuned congestion control algorithms. So, let's say, standardized in CCWG, but someone implements the BBR with a different again? How do we want to address those cases? And finally, with quick, we've seen that making modifications for existing algorithms or implementing new algorithms is very easy to do. And, you know, you don't have the high value of actually, fiddling around with the TCP stack anymore. So last question I have is how do we actually police the deployment of CFCC is even after we have, you know, agreed on what a safe CC is. That's all. And I'll be happy to take questions. Okay. And we have one minute. So let's have Randall first."
  },
  {
    "startTime": "02:00:02",
    "text": "I'll be, extremely quick here. It seems like this is more a, measurement of similarity of congestion controls as opposed to conformance to, safety or compatibility with on the internet. So the, you know, perhaps a conformance would be a, you know, at this or less. Compare, by comparison, such that it's not gonna be any more aggressive than the original design of the algorithm. I I would not say any more aggressive, but I think right now, in its current form, let's say we come up with a draft that has some recommendations for how you want to implement a congestion control algorithm you have no concrete way to correlate an implementation of that algorithm to that trap. So the assumption we make here is that, you know, let's say your Linux CCPM implementation already abide by the draft, then you can have a apples to apples comparison between the TCP implementation and the new implementation you're trying to standardize. Okay. Let's briefly hear from Matt, and then we're out of time. Yes. I think this is an excellent presentation, and and there's actually elements in that this that I I never thought of that you are you are absolutely correct there's a difference between standardizing the algorithm and certifying and giving implementation is being deployable. And I like your suggestion of pinning a reference implementation at least in the short term. Before we understand better how to do this. If you look at slide 11, the disjointedness of some of those clusters. Actually makes me think of those flows as being self turbulent. And I see bugs in the algorithms just from looking at this graph. Because you expect the contours to be kind of smooth and the fact that they're not smooth doesn't bode well. So Yeah."
  },
  {
    "startTime": "02:02:03",
    "text": "Anyhow. Thank you. Yeah. Okay. Let's, continue discussion on the list or we could also have an issue on the GitHub. For this? Okay. Thanks everybody for the excellent contributions. You also, Andrew, for taking notes. We appreciate you. Right? Have a great week. See you all next time. Oh, yeah. It's such a creature. Right. So"
  }
]
