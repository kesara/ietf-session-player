[
  {
    "startTime": "00:00:23",
    "text": "Okay. I did this wrong. Okay. But I will start now. Okay. Hello, everybody. Welcome to map. As you can see. I'm here. And Dave remote joining us from the middle of the night, I guess. So thank you for being here. And we have a packed so we will very run very quickly through the chest light and then just start with the interesting part. Next slide. Are you running the slide at the moment, David? Dave. So we're noel. Yes. So this an. I I rt f group research group, but we also have a note well. It still applies to you. Also, if you go the next slide, This session is recorded. So please be aware of that. Next slide? Please also be aware of the privacy and code of conduct conducted. It also applies to ir sessions and it's always good to be nice. It's a very short summary. And yeah, This slide is just explaining what the difference kind of is between the Ir f and the, but you can read it on your own. Also, the deck has a couple of useful pointers if you need links to the material from today or our group in general, Please join mid echo even if you're on the room because that's our blue sheets. So you want to be really crowded it in the blue sheet so we get the right size of room next time. And you also needed to join the queue of course."
  },
  {
    "startTime": "00:02:01",
    "text": "We set this already. Yeah. Please also wear mask. Speak clearly and be. And that's our out of data agenda that I didn't manage to update. So what's missing on this agenda, that's why it's correct like we have two little nice head up heads up top. Both of them are five minutes we squeezed in in somewhere, and you can check the agenda on the Internet because that's updated. But we go immediately to the first presentation. Yes. So he's Marc local there or a he remote? He's coming to the mic case. Oh great. Thanks. Yeah. You actually can request... Or, like, can you transfer the control over martin. I think he has to be in the queue right. Multiple ways. You can. Can you request it again? This the other one There you go. No. You should have. Confirm. Perfect. Right. Go ahead. That's maybe not long it's not on. Okay. Yeah. Now you can hear. Awesome. Okay. So hey guys, Martin, I'm a phd inform and this and presenting today, but this work was possible also because my great car offers rafael thomas."
  },
  {
    "startTime": "00:04:02",
    "text": "So before we jump into the results, I want to to make sure that all on the same page it comes to the quick handshake process. So let's start with the basics here. So before quick, we had Http over tcp and this means we had the Tc the Tls and the Http handshake, which resulted in the least three round trip times. Now was quick, the design here was that we reduced this round trip to exactly one round trip time. So as you can see, the client basically sends an initial message to clients allow. The several response with its initial message and the handshake message and then the client confirms with x. What is important here is is that The second design goal was also to reduce amplification. So quickest space on Udp and this makes this protocol susceptible to reflect the simplification that. And this is why the server is only allowed to respond with three times the bytes it's originally received from the client the client hello. So the client's hello here is not a small packet like sim, but actually around one thousand three hundred bytes. If the certificates, for example, way too large and the server needs more data in this handshake messages then The server has to wait for the acknowledgement from the client and then send the rest of attention data. So what then happens is so called multi rt handshake, so this means we do not have the one Rt, but actually an inefficient multi rt. And if you ask yourself okay. So why is there a lot of to estate well, we have a lot of... First of all, we have the certificate change. So we do not only have one certificate but actually multiple. Certificates. And each of the certificate actually has... For example, the key, which can be very large think of Rsa, for example, also we have the subject alternative"
  },
  {
    "startTime": "00:06:04",
    "text": "alternative names field, which can consist of a set of about of names, especially in the case of Cdn. So today, I want to talk about findings from our paper. Where we will talk about why hyper giants purposefully ignore actually an amplification because of the trade that we observed we'll talk about how Tls data still interferes with the quick hand performance And in the end, we will also talk about incomplete handshakes which resemble amplification attacks. So this as my part g, so we have to talk about our measurement methodology. We used open source source to perform active scans And we started with the trent of one million domain list. And for for these domain, we end the radar it redirects we checked whether they support the conventional Http over Tcp. And if they do so, we also pass the tls certificates. For each of these names, which supports the or reachable with the conventional protocol We also checked the with support quick. So here we did the complete hand that we just all in the beginning. But we also did tested the income incomplete which as I said, resemble more of the reflective deals attacks. Now. While performing the complete, we did not only observe which services are reachable with quit, but also classified the hand. So again, we have the optimal one rt which basically to handshake this that we want that was designed. So one Rt and for below the amplification limit But also, there's also the retry case which is less efficient, but still a great option. However, there are two things that we do not want. And this is the unnecessary remove the rt hand. And also the amplification case where which is basically not not compliant with Rf because it exceeds the free x anti application"
  },
  {
    "startTime": "00:08:03",
    "text": "So what did we find? Overall what we saw is that Rt handshakes are very rare. So in this plot on the y axis, you see the number of quick reachable domains that we saw, around two hundred and fifty thousand. And around sixty percent actually were amplification honey handshakes and around thirty percent multi or handshakes. Now What you see on the x axis is actually the client initial size that we use So we started with a common size of one thousand three hundred fifty. This is a default used by Firefox. And then we move to the left, so this is more decides that Chrome users up to the minimal size of one twelve hundred bytes, but also went to the right which is basically limited by our M. And the picture almost the same. So we had a couple of un issues on the far. Right, but overall, you see a lot amplification and rt hand. Now in the next points we'll basically explain why the happens, and we'll start with the red bar. So why do we have amplification and how bad this amplification So here you see that for all the amplification handshakes, you see the amplification factor and what we found is that usually the amplification factor is not larger than And look more into the details here and actually found out that a lot of these were completed with cloud. So we talked with them, and they were helpful and explain to us why they actually have this four amplification and this was a design choice. So overall, n the end deployments the quick server which completes the handshake and the process that has the Tls material can be two different entities. So... And this actually can lead to delay, which overall"
  },
  {
    "startTime": "00:10:02",
    "text": "can have a negative impact of on the client R estimation. Let's take a look what I mean by that. So overall, declined again here performance client handshake, And the server response. And before we had one initial message, with contain the act in the server. But now, What the the C now tend to you do is sent the initial packet with the required padding instantly. So this allows client to have a good Rt estimation And then as a process, they communicate with the Tls third store. Which introduce a delay and then they prepare the second initial packet again, they add the padding and then they said send second the second packet. So overall, we see that that we have this delay, And if the initial message would be sent, call less, we would have the delay for both. But here, we just have a delay server hello and not an act. So of all the instant act prevents play Rt estimates This is great. However with two initials, we have to slight amplification of four x Alright. Now let's look about the Tls data and why we have moved multi rt handshakes. So this is basically the orange part. And Are retry talking which could also cost multi rt hand, but the majority at least for the handshakes that we saw, I usually caused by certificates, and that's why. So we took all the multi rt handshakes and sorted them by ascending by the total Udp payload. Just the x axis here. And then we we I see on the y axis and the bytes or the Tls pay payload. And as you can see and eighty seven percent of the cases already the Tls bytes exceed the three x limit. When we then add"
  },
  {
    "startTime": "00:12:01",
    "text": "the remaining quick bites like headers padding and so on. We we're far worse. But overall the main contributor here tell tls data. So we wondered why is there. And for that, we looked into the certificate change. And In this plot, we show the non leaves overall we start off the cloud for no certificate which is around thousand bytes and it it's used to sign sixty one percent of the quick that we saw. And the medium leaf size is in last here in the yellow bar. So Overall, if you visit a cloud hosted site on average, the certificate chain will have around two thousand bytes. We then also visualize the worst case in the worst case, we saw for this certificate chain around three out three thousand five hundred bytes and shows here also the common amplification limits of two vertical lines. If we show Also certificate chance that we saw, so this basically the top ten here. We see that on average, median chains are very likely to exceed the common application limits. And this is just a chain. So give me... Okay. So Good message here these top ten search chains are used to sign ninety six percent of quick domains. So this means if we have a change here, which would improve the performance, we would have a massive impact on on the quick ecosystem them So how do we compensate for such large difficult way that are currently Two options we could, for example, move from Rsa to smaller keys sizes for example, like the curve. And the other thing would be for example, the certificate compression, which would compress the certificate change during the transmission However, we saw that many clients do not support this."
  },
  {
    "startTime": "00:14:03",
    "text": "Alright. Let's quickly end up of the us finding incomplete quick handshakes. So What do we mean by that? And during incomplete quick the client only sends exactly one packet. It's the initial client. And the server response. And now the client is not react. No acknowledgment. So the server assumes to packet this loss. Resent it and no response. So the server continues and this can probably lead to really large amplification factors. So The thing here is that these incomplete acute during reflect simplification attacks. So this is why this must also be subject to the nt amplification limit. So we checked for implementations in the wild really respect this. And of the implementations that did not was actually the implementation of meta So what we did here is we scan the pops which Are usually identified by slash from four of matter and for each aggregated by each host that. So for example, in this plot you see that all Ip addresses that and with dot sixty three on average have an amplification factor of fe. Now if it comes to meta, usually if the hotspot at the sixty three, the service behind the the same like. So we had also we saw that specific services, we're more likely to to show high education. And we also talk with Meta and they were already working on another issue and, then this actually fixed this and Does it work? Yeah. So between August and the october, we saw fakes, So before we amplification factors on up to thirty, And now we have amplification factors around five for incomplete hedge. So of, way better, but still we are over the amplification limit."
  },
  {
    "startTime": "00:16:07",
    "text": "So here we saw that basically launch let's data on leads large because we simply transmitted the data that we have. And respecting the anti implication limit decreases the chances of lost correction because we have not so many reasons that we could that we can do. And overall just leads to one of our open challenges that we saw how we actually deal with pe during the hen process and this secure official way. And if you're interested in that, I will talk tomorrow at a quick working group about more challenges possible solutions. So overall, to conclude my talk, we saw different points to different results a certificate ecosystem, which saw the tls configurations have now a direct impact on the transport layer performance so quick. We saw that if we use certificates change might be could get ways smaller. I think this is already no. But still it now affects the the quick process And up to non leave certificates would have beneficial scaling effect. Now if it comes if it comes to quick deployments, we saw that we have the design goals of one rt and the anti amplification limit. Unfortunately, because of specific deployments or implementations these design have not been met in the world because of trade offs usually between the space efficiency lights that could occur Also in the last step during the incomplete, we saw that adding and three transmissions can significantly worse the amplification factor we observed And with that, I'm happy to answer your questions. As the last step, maybe a little bit of advertisement"
  },
  {
    "startTime": "00:18:01",
    "text": "during the last idea of effect, we implemented actually two which performs for any domain that supports quick exactly the classifications that I talked today. So... Yeah. Thank you. Thanks a lot. I guess, you can leave the queue. And we can have brine. Fine. You can also stay it. No. Stay here, but you can eat the online. Okay. Hello. Good. Morning. Afternoon evening. Whatever it is there. Thanks for the talk. This is really awesome work. My question would be Let me preface this. So the reason that the design goal of quick was to have three x and amplification limit was in part due to an analysis of the size of Tls certificate chains at the time that that limit was chosen. And I think you see there's, like, the got encrypt that two parts of that led. I don't think that was a thing that was part of the ecosystem at the time. So like, that was selected sort of as as a trade off between amplification limit. And hey, can we actually get through the whole tls chain? And what we're seeing is the current Tls ecosystem doesn't work with three. So network operators are around ignoring right, which is correct. Right? Like they're they're trading off. Sort of this abstract design goal for security for an availability goal, which is that's working as intended. What would the that graph look like with respect to how much amplification we have if we had just set the limit to four. I I think that would actually if the one were for, that graph would be green and not and not yellow. I think if I'm reading a numbers right, which means maybe they're advice to a quick working group you'd beat up. You know, hold for document update. The that limit to four x is opposed to three x. Right? Because it's it's a little bit more amplification risk for actually working with tls queries. Yes. So absolutely. The the thing that I think the problem here is that we more or less have a static"
  },
  {
    "startTime": "00:20:04",
    "text": "and amplification factor limit here and It's not complete. So It's like that. How often will we have to change the anti application limit if we if we see that deployments changed. So maybe keeping the free x case limits will motivate better deployments in the future. So maybe keeping the pre x's the better way to do it, but I'm not sure how often we could otherwise or would have to change the limit. I think it would happen once and the Ec is gonna get deployed for its own reasons anyway. Right? So You know, It it was just interesting it's like, wow, this is super yellow, but that's artifact of an arbitrary choice that was made in the working group. So Alright. Thanks a lot, Brian. I'm gonna interrupt you guys here, and I apologize to Michael b in the queue. Because we have the presented next ahead of him Thanks a lot, Marcy Martin. We're gonna thank you to five. If if it didn't get in, please talk with merchant in the chat. Alright. By that, I'm sharing your slides right now. See things come on. Do you know, Mario, do I need to do something? Just grant the, like, next to. Sorry. I I have connected you from next to his name, just grant him this slide and I did that. Okay. Still see... You have select the right slides and then we're done. I revoked margins. And that's what I was missing. Alright. Vi bet. Can you say something?"
  },
  {
    "startTime": "00:22:03",
    "text": "Yeah. Okay. We can you... Let's get this lines up here. Okay. There we go. Alright. You ready go. Okay. Yeah so. Yeah. Hello. My name is Right. And can you please student georgia tech? And I'll be talking about my work on. Route ability in low satellite networks. And like, it's not just my work. I have Arabic support of awesome go with this one of whom, Emma who is also attending the session as well as med get another So let's let's start So certainly of, like, most of you would Heard about Leo satellite net boots especially with starting grunt world three thousand satellites in the past few years with more than ten thousand of them being planned for the next decade. And like, we settle have been shown to to be able to provide high bandwidth with and low cost internet access in rural as well as disaster affected areas such as you can it Australia and Florida. Further a lot of recent work has shown that the satellites have the potential to outperform. The current still that networks. And the key reason for that is the ability of these satellites to form a network using the inter labeling links of the Cells. Through which they can route data to reach the destination. So let's take a step back and look at what exactly makes these networks? So fundamentally, like, every satellite consists of four laser links that will be used as the intercept labeling links of the Is. And these links are arranged in it plus grade Esl top. I'll explain this using the satellite in the center of this figure here."
  },
  {
    "startTime": "00:24:02",
    "text": "This satellite connects to its neighbors in the same orbit using the yellow alert advertisers while it connects to satellites in its neighboring Orbits using purple colored io. These networks. Paths are made up of two brown satellite links for the, or the jason's. And zero or more depending on the distance between the source and destination. For example, here I show you a path between New York and London. Like you can see the two Gs dsl here as well as the three These networks have a highly dynamic investor share satellites moments speeds of about twenty seven thousand kilometers per results in satellite being accessible for a maximum of four and a half minutes. No deal that four and a half minutes is the maximum time. Since some satellites could be visible or even lower. So how does this affect So how does this affect networking algorithm or why should we get about these this new infrastructure or the variability in this even first infrastructure. So a key reason for that is fundamentally in networking looking algorithms would assume stable parts and. For example, control and adaptive video streaming would try to exploit the stability to converge to the capacity of the path selected similarly, traffic engineering algorithms were that would assume a stable ranking of paths to perform functions such as load balancing. So I just... Mentioned in the previous slide, this is a highly dynamic network. So some variability is to be expected. However, the scale of this variability would determine whether the current networking algorithms would require any modifications or if they can be used as it is. So therefore, in this work, we try to study the x variability in these networks. That is certain whether the... This is small enough."
  },
  {
    "startTime": "00:26:00",
    "text": "So that the current networking algorithms can handle that or if it is large, which might require changes to these algorithms. So I'll briefly first describe the objectives of first, we first look at churn, which will define as a measure of the see of changing in the notes. Between the source destination pair. And And give you spider here that there is a lot of. So we try to see if this disruption deserves any significant performance gain or in other words Is this option city. We also look at Rt, which will define as a measure of the variation in between a source destination. There. And as you can imagine there is a lot of, so we try to see if the exhibit bit any patterns? But if this can be explained So before getting into more details, first, I'll talk about a simulation methodology, For this work, we look at the three constellation the star link and using in the publicly available filings. For this talk, I just focus on the first sale of the sterling constellation consisting of fifteen hundred eighty four satellites. At a multitude of five fifty kilometers we use the satellite path models to predict the decide satellite for positions at a given time step using the using the publicly available configurations. For all our experiments we use the shortest path routing Algorithm to minimize the between this and destination. We are able to do so by calculating the path using the positions of the ground stations and satellites. By using speed of propagation. Also, I'll I'll put the add here that we do not perform to complete. Packet level simulation. We are just looking at propagation delay."
  },
  {
    "startTime": "00:28:03",
    "text": "We measure route characteristics for a total of hundred minutes when computing routes every second. And for our, we look at the hundred most popular cities as ground stations results in a total of a nearly five thousand source destination pairs. This five thousand pairs v ran four number. Total of hundred minutes result nearly six hundred thousand routes being observed. And emphasize here that the six hundred thousand it's six hundred thousand beds. At the shortest ones at some point, during these hundred minutes. Now let's get... Now let's look at the more interesting part, which is the observations that we had. I'm sorry. I had some animations, but the loaded slides that seem to not be the So So the first thing I brought up was how much out is of observed? For this, we look at all the six hundred thousand parts that I just talked about and we brought a cd of the usage time and the lifetime of these packs. In this figure here. If you look at the one minute mark here, you will see that well more than eighty percent of the parts exist for more than a minute. Just about thirty percent of them are being used for that duration. This implies that most paths are being used for far lower time than what they exist. So this led us do wonder, like, what are the reasons for option. So Fundamentally, there can be two reasons for that. The first one is that the path to exist. Or that there is resulting between the experts for reasons such as lower existence of a shortcut path. It so we tried to see if we can characterize this route with these two reasons. So we've got another here, we'll put the issue between the bad lifetime time and depart usage time. If you look at the left side of the Cd, you can see that"
  },
  {
    "startTime": "00:30:01",
    "text": "about twenty five percent of the paths are being used for their entire lifetime which means that in this case, option occurred because the path seats to exist However, on the other side of this Cd, you can see that more than fifty percent of the balance are used for less than half their lifetime. Which means that... Mh. Yeah. Which means that in this case, route Jennifer because of the existence of another shelter path. So essentially, like, this disruption would be justified if you're getting some significant gains in latency And that is what we tried to see next whether the gains observed a significant enough or in other words this disruption necessary. I'm sorry is Sorry, Dave can I share my deck here instead because I think for this slide, I needed the animation? Go on for now, we also short on time. We only have two minutes left, but like people can look up the worst on the internet that is where. Okay. So we think we have something those seven minutes I think. So. Okay. So we try to see that if this disruption is necessary and then we... We look at an example, like, unfortunately, I can go into the details here, but I'll briefly talk about very specific point that I wanted to raise here. Is that while in some cases, as I mentioned, rick. Ra in some cases occurs because of the previous part easiest to exist while in some other cases it occurs because there is another shelter but So here, like... I'm I'll try to emphasize both the cases, Like, if you see the first the the bottom, that is the first one that was selected by the shortest routing algorithm. However after about sixteen to eighteen seconds that bad us to exist meeting the routing algorithm heard of them switch back to the blue pad."
  },
  {
    "startTime": "00:32:00",
    "text": "After that, the red part becomes shorter than the blue one, making there... Routing I'll go them switch back to the red. Only further red part cease to exist, ignore them switch back to the blue one. And then it switches to another path later. Essentially, what I'm trying to highlight here is that there is a lot of... There are a lot of paths which is happening here. For different reasons. And like I'm certain most of you would agree with me here that at least two of these bus switches were unnecessary, especially after if I tell you that the relatives again while switching between the blue hundred red path and back just five micro seconds while the while the average observed yeah. It was around one fifty milliseconds. So it's like, the latency again is just about five orders of magnitude smaller than the of being observed here. And it's not just for this specific example. Like, what I had in this Cds below here, when we look at the ratio between the when we got a Cd of the ratio between the minimum and maximum we observed that in more than eighty percent of the cases. The latency gain observed is less than twenty five percent. So essentially, we saw that there's a lot of routes and we also see that in most cases, the latency gain is fairly low. And it's not just latency. It's also application performance, Like, in the paper it'll be showed the impact of this churn on Dc and it messes up all the different algorithm go... Or like, all the... Like, right five different Tcp algorithms and it all. So have then look that route I'll on to Okay. Yeah. Even this slide has so issues sure I think you it would be great if you also can wrap up Soon because we're running a little bit out of time. Sorry. Okay. Yeah. So for this, we briefly we'll look at"
  },
  {
    "startTime": "00:34:03",
    "text": "we we we tried to block heat maps with a point in the center as the source and twenty seven hundred points globally So the key here we observe was that in while in most cases, does not change by much over time, however in certain cases for certain points or, like, based on the structure, Can change by up to two point five times. We we are able to show this for three different points and of some of their examples as well. And then it's not just the magnitude. We also see that exhibits a special structure and Okay. And we try to see if this can be explained then we look at like, in the paper as well as in the slides here I had we look I was gonna show two examples, the first one is for shutter paths. For shorter path here, what happens is over time the the composition of the path can change very rapidly. Are very drastically due to which the patent can change heavily, like, the exam... One of the examples issue is that over a six second duration, the three parts being selected by the shortest routing algorithm can have lend length setting by to two point five x, which is what we see in the circle here, available you keep moving further, there are other reasons that could, like, the type of iso... The type of links that are being used as well. So This is how, like, Yep. In the... If you look at the paper, you'll find more details here or either if you can look at the slides later, and Yeah. With this I Yep. With this, I would summarize the observations we had. The first thing I we like to look at was how much option exists. And if this results in any significant performance gains,"
  },
  {
    "startTime": "00:36:04",
    "text": "You saw that there is a lot of and in which in most cases does not result in any significant performance game. When and then we looked at how much variability exists and if this can be explained in some manner other that can change by to two point five times. While exhibiting the special structure. And... Yep. Finally, I would like to thank my records there's game and responses and you can reach out to me at my this year. Sorry for the Big thanks to you for joining us in the middle of the night. I'm sorry for the trouble with the slides. We didn't check this carefully before. But you just published the paper last week. So people can read everything up and can reach out to you and we have, like, minute or two for questions. Jeff we wanna come up or you just take it on the tip. Marie, we have one person in the queue. Yeah. Jeff at the mic. Sorry. Let me just ask the question Rather than hitting the queue button because it's easier. What kind of service model were you simulating? Because when you've got this large mesh the sky, you can either drop the packet back to earth close to the original client and use the earth infrastructure to get there. You keep the packet in space close to the destination. We're oddly enough mains, you get asymmetric routing in round trip time, because the earth system presumably takes it close to the client before it leads into space. But secondly, it it actually amplifies the variance in Rt t because that long space is switching between the two inclined orbital patents."
  },
  {
    "startTime": "00:38:02",
    "text": "You know, the satellites going left in the satellites going right. Which actually amplifies the problem. I really wonder if those constellation are carrying a full route set or they're doing hot potato drop which actually makes the entire problem far far easier because you drop it back onto the earth infrastructure, just do normal routing. We're used simulating the worst case. You're right. So first, like, the way that works today, it's more of the hot potato drop the way it works is in the bend type mode that any packet that is satellite receives. It will drop it to our station and then let the still internet infrastructure take care. But for for this work, we are actually looking at the mesh that is there in this sky. In the space, that is using just the satellite links, like, you'll like, this packet will hit the satellite, and then it will try to reach to a satellite that is accessible by the destination, and that is what that is the model that you're using here. But I think that is that model has a massive impact on the results you get because you're keeping it up across the switching points And while the propagation time is faster, speed of light in the vacuum. The sort of jagged and the dynamic rights of the curves And, I think you're also assuming a routing system that converge in zero time. Because you kind of know where to go and you're not using dynamic routing. Where so spec dynamic routing, but again, make this worse than it need be. Yeah. You're right. I mean so this is why we first try like, we we wanted to look at a broader scope, but we found out that even if you know these things and you had on record telling you that, okay. This is where supposed to go even then there can be a lot of which is what we found to be quite interesting and That's why talk about sharing this before."
  },
  {
    "startTime": "00:40:00",
    "text": "Thank you. Very helpful. Okay. We have one more person in the queue. See do you want to speak up I. Perfect. Okay. My question is is a setup really but it's Rt or os every second. Because you have the whole information on the global level, right. That in reality, Each rotating node this or not really have the global information. So that they will have on this sure. Right. So if I understood your question correctly, you're are basically trying to ask that is this is routes every second necessary, or is that possible? I got yeah I mean Again. It's but not practice so they will doors so. This is a I mean, so again, like, here we are doing as a a flame exercise than reality because you have the... You know, think you have I as one as you have the configuration information. You can reload this information for like, up to a day Like, for example, all the configuration by starting are valid for a day, so. You can reload this for a deal, for example, and do this So it's more than the period of pre computation, it is what the routing algorithm you are using and what factors it priorities. That is what leads to the churn here"
  },
  {
    "startTime": "00:42:00",
    "text": "Yeah. If you want to can discuss it over the chart as well. Amount of time. You so much. Right. Hello everyone. Robert from the right cc. Some of you know me as demand or Man behind right atlas. I'm not going to talk about atlas right now, but instead couple of things that we observe because we have atlas less and it's deployed all over the world showing us some interesting behavior out there. One would think that when you do measurements, data that is coming back to you is going to be clean. Well, probably many of you know that's actually not true. And I think we reached the uncle state soon as we deploy like the second probe so so we try to look at these things all the time. One thing that we see is middle behavior. There is of course, all kinds of things that you can see. This is a beautiful example that we hit. I think last year it was it was really popped on our radar. And, you know, when you're not prepared for such a thing, you you it's it kind of taint your analysis. So be aware that these kind of things happen for those of you, who cannot read the slides, the the letters are small This is a trace route going outwards and the responding hub is the same for every hop. The Ip address of the help is all the same. The rt change. So this is a middle box middle with the packets. What's further interesting is that sometimes this happens on Udp and this Ip but not on Ic. So you think that on one protocol, you get the answer the other, you may not get the same one. So be aware this could queue. Something else. We believe what that Ip address is can and cannot be used on the Internet is known Well, that's a good theory. The practice is slightly different."
  },
  {
    "startTime": "00:44:00",
    "text": "We discovered that on the tracer that we do, in some specific region. For some strange reason, this was close to Kazakhstan. We saw packets coming in from common one twenty eight, which is explicitly something that should not happen. There was a discussion on the right labs. Some operator saying this is just what it is deal with it and then some others agreeing with it. It can happen. If you will get the slides, those are actually links, so you can get to the articles describing this A bit more controversial two forty slash four. Okay. Active debates or somewhat less active nowadays, I guess about whether that should or should not be used on the Internet feed reserved for future use or not. Fact some operators do use this in particular, Aws is using that for intra, Aws aws routing. I think we have seen very long business as well using it in their own network, which is quite interesting because if it's going to be ever categorized as useful for the usable on the Internet I don't know what will happen to Aws. That's could be fun. Right. Now moving on to geolocation space, you think you know where France is, but let me remind you this is where France is. Okay? To just... You know, if you happen to know where your vantage points are, just double check before you actually say, well the Us is this far from France. No. It isn't. I think there's... There's it's true that France has the longest border with with something like Brazil. And not Germany. That might be wrong. In any case, so this is not. But we go on to anomalies This is one. Bright bad things or at least we ask our host to tell us where the probes are, and some of for the... In some cases that's not exactly true. What you can see is the probe in Brazil that can get to a German endpoint in less than thirty seconds. So that's that's actually not bad. What we do about"
  },
  {
    "startTime": "00:46:01",
    "text": "is we try to flag these probes so that the measures can actually put do something. Whatever they want to do at at the minimum of I'm just ignore and the leads. But then... Or I think I sky skipped this night. Let me just maybe I didn't. That's fine. Excuse me. All right. Then finally, there is when you do know the geolocation, and you're absolutely sure that your vantage point is there, and yet your probe or that vantage point can get to destinations which our far away in un reasonably fast packets. This happens as well. So this is a Russian probe trying to get to anywhere within less than one millisecond. And this is not, you know, not the Russian problem. This these probes exist all over the world. There are some which whatever your question is, you're going to get the answer within a minute millisecond. And sometimes the answer is actually meaningful. So in this case, there's some contents as well. So, you know, be aware. I do not have conclusions. This was just, you know, I random of visions that we have. But if if people would like to think together about what we can do, order to at least flag these to to let the measures know that these things exist out there. Just talk to me. I'm around Thank you, Robert. So this was one of the five head of that I promised you that just because robert is around, I gave for some data. And I think... And I will actually tell matt more about the column now. So probably you want me to run the slides. Yes. Voice control, please. Okay. Hi, Everyone. I'm Anna. I'm from the University of Aberdeen. Where I do Internet measurements, specifically for protocol standardization. So my work has been presented in various Groups across years, and I've helped shape standards particularly on transfer. Next slide, please."
  },
  {
    "startTime": "00:48:03",
    "text": "So measurements are super useful for protocol standardization. It can tell us work with standards whether or not I proposed standard has any barriers to deployment or whether or not an existing standard is used it helps us find a various bugs. That might exist. In Standards. And if you find a bug, then that's the first step to fix seen it. Right? So measurements are useful. But in order to perform this measurement in a useful way, you kinda have to target a lot of the Internet. The Internet is absolutely huge. And that's the challenge here. Comprises billions of parts and there's lots of diversity within it. You can have mobile networks you can have satellite networks and so on. So you have to target as many diverse fast as possible. So how do you do that? Next. Next slide, please. Well, first, you have to design your measurement type the campaign. And you have a few choices there. You can choose to generate packets and then throw them at the internet and see what comes back, and that's an active measurement. Or you can choose to observe topic that already exists. That's a passive measurement and then you can your test so that you control either one endpoint or both of the endpoints in your measurement or maybe you're doing something from network device. And then you have to consider which metric and at which level of you're actually measuring? So you can have performance metrics or you can have something like a functional metric like connectivity So you choose all of these things and you design a great measurement campaign. But you there are, of course some pit pitfalls. And this is what my talk is about today. And as an example, I'm gonna use Ip six extension headers."
  },
  {
    "startTime": "00:50:00",
    "text": "Measurements I'm gonna talk about are going to be mostly active measurements and functional. Connectivity. Next slide, piece. So very brief review of what Ip six extension headers are. So Ip six was design from the start to be, and you have these x headers that should enable on new functionality. They had a bit of a rocky start. So historically, you have different return architectures not necessarily supporting process in some packets with some extension headers in hardware. And for this historical reason, networks to this day, some some of them drop back with extension. Across the years, many different groups within the idea have tried to measure extension headers. If you look at the table there, you can see what appears to be complete like conflicting results. But These results are not really conflicting. They essentially work together to tell a story. Next slide, please. So that's why Chose extension headers because extension header measurement is pretty hard. And this is the kind of measurement where you're most likely to mess up. In one way or another, and that's because the broken nets can exist in many different places. So some devices maybe don't support extension to begin with. Some of them maybe don't have the capacity to look deep into a package where your extension head is. Some maybe need to access layer protocol information and they can get to it or because in extension address in the way. And finally, you have either by configuration or by mis miscommunication network that actually filter them. Next slide, please. So how do you actually measure extension headers? You a way that makes sense? Well, because they're not very widely used. You have to generate traffic with extension headers. So you generally traffic advantage point. You throw it across the Internet. It gets to the destination and maybe you get some feedback from you to"
  },
  {
    "startTime": "00:52:01",
    "text": "nation that your packet has actually arrived and that's how you do an end end test. So you can measure any property end to end and you can work out if it actually works or not. But with an end that doesn't tell you is where the problem has occurred if something has gone wrong. So say your exact your packet was dropped. End test. I won't tell you where it stop, but you need to do path measurements for that. Next slide, please. So with that in mind, I'm going to give you some examples of where you can mess up your measurement and divided this in three categories. I don't quite about two of the marshall. Can you Go to the next slide, please. Okay. Three you of them now. So the three categories are very measured from. So your vantage points when we measure two? Destinations and finally how we measure methodologies. The example, that... Are here, I'm gonna go through them, but you can always refer back to slide something doesn't make sense. Next slide, please. Vantage points. So it falls around vantage points related around the of diversity Vantage points. So if you're gonna be measurement from health provider, you better make sure that your cloud provider is transparent to what you want to measure. I can't remember the number of times after that I've started to measure something from Aws or from digital ocean, I need to find out the call provider was messing with the protocol that I was trying to test. And all of my results were garbage. So Yeah. If you can try multiple cloud providers, and mix in active measurement platforms. Something like red atlas will give your connectivity to thousands and thousands of different vantage points and they also have the advantage. That some of the vantage points will be edge networks. This will help you avoid sampling by fall. Now I'm gonna move to some exact move on to some examples. Next slide, please."
  },
  {
    "startTime": "00:54:05",
    "text": "Right. Example one. In this table here, what you see is the percentage of servers that reply to a packet that was sent to them with an extension header. So the set of destinations for each of these measurements wasn't changed. Do you only thing that is different here. Is the vantage point where I run the measurement from. And as you can see there digital ocean and. When I run my measurements from there, I received really literally no answers from my servers. So that's because these two providers, they dropped packet with extension headers or they get lost in transit to the the cloud provider upstream. Right. So this is still a good valid measurement point is just that if you want to run a white scale measurement paint to lots of destinations, this is another a place to do it from. Next slide, please. I mentioned a split between the core and the edge of the Internet. And by that mean, in general, the core of the Internet is a lot more transparent to the of different protocols. That is because you have less devices in the core of internet that are likely mess with your packet. So let's me boxes. Also the more specialized network, like mobile and satellite, the more weirdness you like to get. So it's always good to have in a mix of edge core parts. It leads you to think about the devices that your packets travel over. Next slide, please. Moving destinations. And the store here is more or less the same. You have to choose diverse destinations if you want to understand how things work because the results may look different for for different types of servers. Now one thing that comes up again and again is the top one million domain This is the list. It has to million domains in it."
  },
  {
    "startTime": "00:56:02",
    "text": "You can resolve this list? And then you can filter it so that you only see unique Ip addresses. And after that, you can run your measurements to those unique ip addresses that you found. And this gives you multiple web mail and Dns server targets that you can use. The problem with this list is that is not diapers. Apart from this topic list you can also use crowds sourcing for your measurements. And this is great because you end up hitting a lot more clients edge. So than you're you can't start the measurement. And then well, it your server when your order your vantage point. And then you're run the measurement back to them. These are great, but they're harder to reproduce. So that's something to keep in mind. Because it can affect your ability to reproduce your own results. And it's the same for other people who might want to improve your results. Looks slide please. This is an example, I mentioned top one million domain are not very diverse. And you can hopefully see it in this table. This is... Has to call us. And in the two columns, you have the exact same data, except it's a per host for you in your first column and a per in the second one and for years, I've been presenting that results per host is supposed to wear yes, but it they might have been much more convincing if I if I'd actually gone and and shown the per split. Because you have many different hosts concentrated. That maybe do something and drop extension headers. Right. So general point here for the slide, measurements that you do, you have to provide the priority split. I'm gonna last quick really slide basically, this shows it's not my data. Is data from"
  },
  {
    "startTime": "00:58:04",
    "text": "Rf seven eight seven two, and then some crowds source measurements from I think you can skip a ahead. And one more. These are all essentially examples to show you that if we choose different types of destinations, then you can understand the picture a bit better. Because if you think about it, some of the servers that you target might be behind in first So if you look at web servers, they might be behind the City or behind a proxy or load that and so on. If you're going to clients india... In networks, then you might have network specific metal boxes in the way and so on. So infrastructure may look different for different server types. That's something that's the keep on here. Next slide, please. And finally, we don't want to methodology. Here, the is to not approach the problem from all of the angles that you can approach it from? So it's always good to combine measurement approaches. You can combine passive and active measurements if you can, and that will help you understand things better. Another thing to keep in mind here is that it's always useful to compare your methodology in results to anything else that has been done in the field. And in fact, I would recommend, to any researcher to try and first reproduce results that exist because this kind tends to help you get out of the way, all of those silly issues that you might find when you set up your experiment. And something else. I would advise this to open source your data because this allows other people to validated and be build upon it. Next slide, please. And in terms of performing measurements, of course, you should always measure multiple upper layer protocols."
  },
  {
    "startTime": "01:00:04",
    "text": "Because sometimes the choice of our protocol will influence at the network level. And then don't necessarily assume you understand how internet works. There's lots of balancing in there. And there's lots of weird middle boxes that two weird things as Robert pointed out. Next slide, please? So the final two examples, are around protocol differences. So this is just the slide to show you that I've measured loads of devices in edge networks. It looked there like there's a split between Udp and Tcp. So this is an opportunity to understand why is for Tcp is different than for Udp. Is because lots of edge devices might mess with Dc. And then you can start thinking whether or not there's link between those devices, and reversal. Next slide please? And then I say the best example for last. Load balancing. It existing Internet, and you can measure it with something... Well The main tool you can measure this with this paris restaurant and I'm sure menu are already familiar with this. Paris tracer route is a tool that attempts to final load balances. Between a source and a destination by running multiple choice routes And in between each measurement it varies some of the header fields. So in that first picture here, you can see that An azure a source in destination with various and we find four different parts. I find at least total balance in this network, and that's great But then if I repeat this measurement, except instead of sending regular packets without extension headers. And I sent a packets with a destination option extension header instead."
  },
  {
    "startTime": "01:02:02",
    "text": "That balancing is completely lost. I longer detect for past. And that's why that's probably why a... That's probably because load balance. Ones in this particular example. Were using bike offsets in order to route packets on the different parts. But all of that is lost because now you have an extension header the way. Next slide, please. This is just to close up. So next slide please. And conveniently, this is my last slide. To sum up always try multiple approaches for the same measurement use multiple vantage points in destinations And never expect anything on the Internet really works the way you you think it should that's it. We can move on to taking questions. Thank you. Thanks a lot. We actually don't take questions, but there's a lot of discussion said already. Okay. And this was a talk about more meta point of view. So if you if group wants to work more on this, then I would also be excited to see some more session on the main list. Thank you very much for bringing it. And as you said yourself like the summary on the on the chat is kind of don't assume that you understand the. So that's a good takeaway. And we move on. Simon you also should probably request sharing. I think I think he did that with video and I accidentally close that. So timing of the request video again too. You go? And can you... Can you request to share your slides or you did that?"
  },
  {
    "startTime": "01:04:00",
    "text": "Thank you. Okay. You're ready. Maybe... I just see the video. So Simon mean you things? Yep. Okay. Yeah. We got you know really. Perfect. Go ahead. Yeah. You can hear me and you can see my slides. Yes. Great. Hi so my name is Sam Sand. I'm bitch student at Council university. And I'm here today present some work we've been doing at Cox University in collaboration with Red Hat on enabling efficient continuous latency z monitoring by using Ed etf. So as you're hopefully all our network latency say probably matters, it has a large impact on the quality of experience for basically any type of interactive application that runs the network. That's true already for many of our current applications today such as this video conference on in right now, but it will become even more important if we look towards future applications, for example, r or and many of the epic patients emission for the tactile impact. Will require low and consistent latency in order should be possible to realize. So we need good tools to continuously network latency in order to determine if our networks can actually support these different applications. And the continuous part here is is very important because network latency in very rapidly change. Giving issues such as jitter, and you can't really detect this you're only occasionally probing the network. So if we look at the tools we have to today for monitoring network latency, most of them rely on actively probing the network. Such as for example, ping a mesh and the right atlas measurement platform. These are all great tools. But the problem with them is that they don't actually capture the application network latency see. Just give the"
  },
  {
    "startTime": "01:06:00",
    "text": "latency that this probes experience. So as an alternative we can use passive and monitoring where we are able to infer the network latency directly from application traffic. And some of the tools that we have today that can do that includes wireshark and as a thing or keeping application. Now the really nice thing with these tool is that they do actually give us an application pursuit. Actual agency can see because they operate on actual application traffic. But the problem is that they rely on packet capturing to do so and this comes with a lot of overhead is that they often don't scale as well to the high packet rates that we can counter button gigabit and network links. So our work is very heavily inspired by this as a ping or keeping tool. So let's just take a very quick look at how it works. So the basic id behind beeping is basically to match up packets with the replies and then calculate the by taking the time difference when you. So the original packet, I want you sort of reply to this pack and clipping does this by using tcp timestamps stamps where each header will container It just balances. Is your field? The senator would put it sometimes stamping to to well them. And the receiver will echo it back in that this. So you can simply match the battery that is one with our value that is or yes. It's our field too match up. By packet reply. Get thought that way. So we think that this keeping tool is really nice. But as mentioned, it doesn't scale that well to high packet rates. So our proposed solution here is on volt or. We take logic very similar to that of keeping, but we implement it directly in kernels base by using it? And by doing this kernel space, we avoid the need of having to clone the packets"
  },
  {
    "startTime": "01:08:03",
    "text": "with packet capturing. Because we're able to operate directly on the packet buffers in the linux network stack. And we also don't have to send the entire packets to user spaces that we don't have to send that what it is that we compute. With this passive monitoring kernel space. And now I realized that most of you probably don't know what we have is So the very short version is that the path is basically a technology that allows you to attach small custom programs to various hooks in that Kernel at run runtime. And by doing this, you can then modify the behavior of the Linux kernel without having to rec the load and the kernel modules. So in our case with basically implemented this passive monitoring logic in Theft programs that we've then attached to the very early parts of that. That stack. So they was a performance gains from moving this path mono logic to Kernel space Well, test that, will you stay rather single setup. Consisting of a sender and a receiver that were connected by our metal box. Then must forwarded the traffic between these two vent hosts. We then sent various some amounts of hyper flows from the center and then has a be monitored the round trip times. From the middle box. Either by using the original tilting or or e program. We then also compare that performance against that baseline where we introduce enable it monitoring at all and and just forward the traffic. So the results you see here are when we had the ten concurrent hyper flows, which was when we observe the highest highest throughput on our test. And as the left graph shows, we basic observed same throughput in all cases regardless if we're using passive monitoring or not because the end host. What of them bottleneck. So roughly fifty five gigabit per second of throughput regardless."
  },
  {
    "startTime": "01:10:00",
    "text": "But if we move to the middle part from this the it'll session on the box. And as you can see there this passive monitoring does have some over overhead. However, the overhead for our editing tool is on about a third of overhead and compared to the original. And if we move to the right plush, being operates on every in line with the network kernel. Or network stacking in the kernel? So it's this every packet that the kernel actually handles. While on other hand, beeping operates on these clone package from the packet capturing. The packet capturing is not able to keep up with a hog packet right here. So original keeping ping only handles about six percent the hold the packets that this machine actually forwards. This despite it's much higher security utilization. Now in this previous test the end hosts with bottleneck. Which means we weren't pushing this metal box or our passing monitoring solutions their limits. So we moved the bottleneck to the middle box by At to using a single cpu superior core and cpu for was done fully utilized when forwarding the traffic. And any overhead from this passing monitoring resulted in a lower forwarding rates. So first of, I want to point out again that's the original headed misses most of the packets. So we actually have two lines for it. The upper dash orange line. Shows the forwarding rates of the middle box while running dipping. Why the lower dotted orange line shows the rate at which the process itself was actually monitoring practice. It's only actually handling a small fraction for the panels that are forwarded here. If we look at the performance for our e tool, which is in the green line, showed the see that at a few concurrent flows it performs much better than the original beeping."
  },
  {
    "startTime": "01:12:03",
    "text": "However however as the number of concurrent flows increase, we also set that performance drops and at a thousand concurrent flows. The forwarding performance is not really better here. But as mentioned the runs on every packets. So we're actually at a thousand close monitoring packets at eighteen times higher rate here than the original tipping And the reason we see the drop in performance is basically, that's the algorithm that's keeping in keeping uses. Results in more or samples. The more concurrent flows we have. So at a thousand concurrent flows, the think is try to report roughly a hundred and thirty thousand odd per second. And this comes with quite a lot of overhead that original beeping sort of accidentally by missing most of these audited samples to begin with So we then also looked a little bit. How to deal with all these audited samples by doing some internal sampling and aggregation and the blue line in the left slot here basically shows the same performance. As the green line in the previous plot is when udp is trying to report everything single value. Then we basically reduced the frequency that each flow could report what is. And those are the lines above, which show the dutch help of which performance? But by doing sampling, we we of course, also do some granularity or measurements so also looked at doing some internal aggregation. Instead of reporting every single part value to user space. We them into a his that will only product reported. And that's the right hand side plots where the blue line, which is for reporting all of the authorities. We see that overhead now is not near as affected by the number of flow since with just the modest amount happening we can largely. Get away from this drop performance."
  },
  {
    "startTime": "01:14:05",
    "text": "So to quickly recap what we've basically done is that we've implemented continuous of latency monitoring directly in Kernel. By using Etf. And by doing this, we've been able to drastically improve the performance to packet capturing solutions such as sleeping, being able to monitor packets that's over an order of magnitude high rates. In our case, we show that we were able to handle over one million packets per second corresponding over ten gigabit of, traffic on a single core. Also looked at using some simple income and sampling and aggregation to reduce of the overhead that was associated which having to report a lot amount of you guys. So right now, we're currently working on furthering improving this internal documentation forty values. And we also want to evaluate from an isp advantage points to get a more realistic traffic load. In the future, we are considering having support for additional protocols. Such as, for example quick by using the quick spin. Finally, I just want to mention that is open source. So if you're interested, please go ahead and try yourself. And if you have an ideas how we can improve this tool this reach out to us. So thank you for your time and attention and if you have time for any questions, I do my best answer. Thank you. We would have time for, like, one very short question. We can't also just move on because you're short time so thank you for presenting those very we thing, and I hope people provide you more comments in the chat. Yep. Thank. So next we have Martin T. Hello? Can you see any hear me? We can hear you. List still"
  },
  {
    "startTime": "01:16:00",
    "text": "waiting for your slides. Okay. I asked her to share my Cr We can try them. You know Still I. Okay. I stop the slides. Yes. I really one to sherman. So the slides already updated uploaded. Sorry. Okay. And I think Dave is now sharing them for you. I meant I just stop the slides right before you. Yeah. Now you're sharing your screen. Yes. We could also use the upload slides. Okay. But I think you go with this you could. Okay? Can you see the slides? We can can. Yep. Okay. You ready. Go ahead. Okay. Good morning. Actually, good afternoon. Everyone and thank you for inviting me and his in speaking good. I'm going to present the paper amazing the performance of a cloud delay. It was presented week before in the pam conference. So let me first introduce beat the context very quickly the context of newspapers is about privacy and web. So let's just see steps encryption it. Was taken during last years. On the beginning of the Internet or most any protocol field wasn't clear. And so this was a problem for security because passport can got numbers with vie for privacy because you're a speaker traffic website visa then also your interest. And so"
  },
  {
    "startTime": "01:18:03",
    "text": "starting from two thousand ten, we observe trend or encryption with more and more the protocols like and more for fields. Got encrypted. Let's see briefly how the picture changes from the beginning of web on the beginning of the Web. We had the H which wasn't clear with, any client needed for to those buckets. Then with the Https, Is encrypted. But you my name is still clear because it is available in the seven main indication field and the ip addresses are visible to the network Now we have Http three T one point three and greek any feature change it again? Now you can also encrypt the tls Cla tel and the ceremony medication using the Tms extension hello But client instead of an Api that still clear. And if you really care about your privacy, clients in and Ip addresses can be problem because looking at the server of Ip address your isp can more less understand the websites and the port visit and commerce, our website can track you by looking at your client even if you delete cookies is and you delete all the this information. So the classical solution to this problem, has been beautiful Networks that are there since the nineties Ips was in nineteen ninety five. And one of the most popular Vpn like services is to."
  },
  {
    "startTime": "01:20:01",
    "text": "With Vpn you speak cannot not understand the website you are visiting. And our the website cannot get your client at Now we have the for Vpn which is called the cloud a. Which was launched in twenty eight fifty one. It is included if you have an cloud plus plan, and is a Vpn like service directly integrated in any Apple device running In the Us. And it's also very easy to use because it's interacting in the Us, you just go in the settings and you never eat private can be very interesting to study for several reasons. First because it can become really widely adopted because to use and got a lot of season and And also these can be can really change and the traffic needs to web is rice center I paper. Already for a traffic shift due to parameter delay. Also can be interesting to study because it relies on a very. Because it is based on Aka cloud and fast And also echo glass that we are more than two hundred thousand subnet of single delay of this network. All around the world, and we're looking at this is this image again. From the revenues mentioned at the I paper. Private I can be also problems for job blocking services, we because is harder there to get to understand that firmware where a user connects. When using it again. Everything they can be interesting also because it has a kind of novel technical operation"
  },
  {
    "startTime": "01:22:03",
    "text": "first because it is based on a chu architect turn the traffic passes is through two process before each server, And also because it is the first Vpn based on greek. It also implement some novel features of quickly we will briefly mention next. It First, let me explain very quickly how this multi architecture works in this architecture the the neck locate entity, which is not a very going be and the internet usage which is identified by the sub ip. In this way. The traffic from a client passes through two proxies before reaching the destination side. And the client establishes a first term towards. Second and then it uses this tunnel to serve the package to the destination. The proxy is managed by Apple. And those can get only client ip address but doesn't know the set of ip. The second proxy managed by Aka fast three, can see clearly the address but do not know the client Ip address. So there is no entity that at the same time knows both client in. And these privacy, also for the from the Vpn point of view. As I said this is first attempt to have a Vpn and a based on quick. That we had to be modified a bit because there are some problems. The main problem is that having a Quick or quick intel of encryption and two levels of reliable protocols. And these can be called. Sort was to avoid having two instances of reliable protocols. And then also"
  },
  {
    "startTime": "01:24:05",
    "text": "With the Tls you typically use the connector when you use a proxy, we'll click an http that was not connected equivalent event. So the solution was to use this recently extended connector or quick But let's come to one as I said, having nested instances reliable protocols can get problem because expect long delays in frequent connection, sir. So in this case, this solution was to use a quicker enable that again extension standard. It was standard dies really soon, also called master, in mask you disable transmissions in the outer reconnect and you keep initials on in the in internet quick connection so that there is just one instance of reliable saying about only the problems that could arise So let's see the goals of our worker our work is about studying the performance of so we wanted to study. There is a performance penalty using credit battery relay. If you experience lower throughput, the rolling files takes more time or where they used To designer, we deployed a a small test based on three species. One in my city. And one in honolulu. Each Pc was connected we are to the internet. And had a of subscription cloud delay. We are three set of experiments. First throughput measurement using an automatic version of the old class speed test photo. To understand which is the the uploaded troubleshoot throughput when use today. Then we tested the the routing a large file"
  },
  {
    "startTime": "01:26:03",
    "text": "So how fast is private when you don't know the large this file in this case. One he gonna be fired posted on a on the head. And finally, we tested web browsing, visiting the top one hand wednesday for each of the account please using browser time, which is an automation tool set well to made the visits for website using Safari. And we measure among all the page look dime which is how long a page takes and so the time meeting between the first and the last Http request because our web page is composed of many objects. That must be attracted and also page times and good nature because we know it it is with the user's quality experience. So let's briefly see our results. Let's start from throughput observe that icloud private delay, download milan is not as fast as the native network. Here we see the the lines are shifted to the left then the new lines. Also we noticed that the performance really eyes on the blocks see on the that is chosen by the system when you turn on delay, in France, if the system chooses a cloudflare a, you go much slower than like an I and in the Us club was much faster than Aka I. So the infrastructure still has some some issues to be choose server of delay. In plot again, is not as fast as the not network and you see in this plot, sir except for Us because our test bed was located Honolulu then the problem was bottleneck neck in the app link. And so the total was limited ten to twenty five megabits per second."
  },
  {
    "startTime": "01:28:04",
    "text": "Regarding a large single files or single very long http request. There is not the winner. In Italy and in the Us, it appears that is lower but in France, actually private actors to be faster and than using the united network. And we supposed that this due to some routing the issues to some selection of Cd notes. Finally, regarding timer using icloud we noticed that page time is larger. So what page date more to the increase in time is about seven sixty percent depending on the on the website but still it's sizable. So what page loads lower. Probably due to the extra I'm timing pause by all this routing days. But there is a sizable penalty on using private delay. So to conclude we observed that five three impacts the in a negative way. And it's also can be tore because it is the first attempt or making a Vpn based on using some other features. A week but still some aspects to study, then we plan studying the next future. So the implications for job blocking as they say the payment delay engineer massive adoption of Vpn can trade province to development services and also massive adoption plan today has a cost in terms computing with resources system, w creek this traffic that the must go the towards these encrypted tunnels. And also there is an energy problem because"
  },
  {
    "startTime": "01:30:01",
    "text": "your additional increased traffic and this consumed some. I think that's all I hope I sent some minutes. And I'm happy to answer new question. Sorry for this strange effect. Thanks, Tina. We have two questions tommy. Yeah. Thank you for doing the measurements. I appreciate I'm tommy Paul with from Apple. I work on credit really. So I wanna specifically talk about the speed tests. Know, I I think there's definitely implications of, you know, where the speed test servers are, etcetera. But I'm curious when you're doing the Oo gl b test, do you know if it's doing the multi flow or the single flow? Mode. Because by default it often does a multi flow. Yes. I think multi flow. Browser speed test automated using. Yep. Right. So it's interesting. So when you do a bulk download, that's generally a single flow and I think as you see, it's generally comparable. What we have observed. So probably really when you're running Oo test through it is putting it through a single quick tunnel. So it's essentially... From the network perspective, one U two. On some networks, like when we have networks that are... Let's, a wired it with a gigabit we're able to get gigabit throughput with and without it's the same. And often what we're seeing is actually that there's per flow per five tub load balancing or other network slowdown that are happening. One of the things, you know, we're looking at is mp quick as a way to expand the number of flows. But I think you'll be curious to do be curious to see more experimentation with single flow versus multi flow mode and trying to find different types of networks and identify networks that are doing per socket t versus allowing a single socket to take up the full bandwidth. Yes. Thank you. I think that's something that"
  },
  {
    "startTime": "01:32:02",
    "text": "can really happen. If there is some part that is link bond, which is pair flow or the balance said probably the single quick algorithm connection cannot get last. Also this we've survey there is some issue in the choice, not to me. Some impacting the choice of the speed test server because with the parameter. You choose speed test seven, which can be find a bit way, then we're using it at network. Alright. Eric, can you also from Apple thank you for doing a bunch of this work and and for bringing it here. I think we'd love to to chat more offline as well and see if we can help collaborate. I had a question about some of the actual page load metrics and kind of where did you start measuring the page load in the in the graph that you showed. Because we we have some metrics from from more than three max. And they they seem to think that France is like, faster in general for page floods. Rather than slower. And so it'd be interesting to to hear some of your speculation about why that might be faster and then maybe compare some notes and and see if we can help move things words in that space. So thank you for for doing us. Okay. Thank you. Okay. Thanks. Thank you. We're completely out of time over overtime actually. Thank you for presenting here. We have this last head up heads ups talk Mo is just presenting on investment work two minutes. That. Two slides two minutes. Ip six only. What is stopping them. So if you have an Ip six in a way we resolve that always has And an Ip v six address. You can't resolve all names because they all Ip four single stack, names service out there. Next slide, please. So I did a measurement trying to resolve one million domain names using. I v only"
  },
  {
    "startTime": "01:34:00",
    "text": "iterating resolve and Six owned resolve So for six success rates for all domains, I was able to resolve most domains with I four only resolve before using an six only over, I was only able to resolve sixty four point one of all the twins. And that is That makes sense because Ip v six deployment is still being done. Not everyone has Ip six activity, not everyone has Ip six Space. But what I thought was interesting was I saw the domains with cloud a with it. So I try to resolve domains, and I found twenty four percent domains that has called a records using a... I mean before only it good but I was only able to find twenty two point five with an six only way to be open good. So that is ninety three point eight percent success rate it is not a hundred percent. So domains with called a records, They are supposed to have Ip six connectivity Ip six address space. Maybe they're all the name service or using different services, but it'll be nice if this one was a hundred percent. So next slide, please. Oh, this is my third lines. Sorry. So these are some of the slide... These were some of the domains I cannot resolve with an Ip six only resolve that has I cloud a wicked. Next slide, please. Oh. Both slide So this is me trying wikipedia dot org. It has a call a, but it's named of ns wiki media oh, does that have a cut sad. Next slide threes. Oh, no. So how dot edu How has it called a. But when I try to all hot h dot edu, guinness auto the main service. This is me asking m"
  },
  {
    "startTime": "01:36:02",
    "text": "Services dot net. I can get the Ibm four address the names survey from the additional section, but it did not tell me about the Ip v six are just very sad. So these they... So People... So they are doing that have got a with But they are now overwhelmed by Ip six only already I'm not being able to resolve all domains make sense because Ip six the is not a hundred percent, but for domains that have called a. It will be nice I could resolve them with my Ib release section way to resolve it. Thank you. Thanks a lot. So I'm sorry for running over time. I hope you enjoyed the sessions you everybody for joining online. And see you next time. Thank you. Have a good week folks."
  }
]
