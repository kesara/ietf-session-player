[
  {
    "startTime": "00:00:00",
    "text": "Yo, my God, hello everyone. Let's get going. Please take a seat Thank you Please take a seat including that includes the chair There's a reason why it's called chair. Yeah Okay, welcome everyone This is the third session of OAS Work Group And you might notice there is a different background here So there is some ongoing competition right now Look at this to you Honest, do you want to talk about this? That's just a attempt from amateur photographers to come with Brian You're safe, Brian Do you understand? anything? To the mic It is a really nice photo and our responsible AD told me you took it while you out with a run with her in the morning and I encourage her to encourage you to encourage you to include it in the presentation I love the tradition. Thanks for sharing I thought you would say that you encouraged me to take a real camera with me while I go running I would like to see that. Okay this is the novel well I'm gonna to linger a little bit on this one because our lead"
  },
  {
    "startTime": "00:02:00",
    "text": "teams are asking us to kind of talk about this a little bit more So, yeah, we had a chairs meeting and they want us to talk a little bit about this more. Anyway, so this is not well this applies here as you know I'm sure you're sick and tired of seeing this, but it's important. This covers everything that we do at the IETF, including IPRs, if you have any concerns about that your privacy details about your information our conduct, if there is any abusive behavior, whatever So if you're not familiar with this, please take a look and make sure you're familiar with this Okay, tips again, please make sure to scan the QR code so we know you are here and you're able to use that to join that queue And if you are remote, please make sure you are in muted, and I must you want to speak up? Okay, let's skip to today's agenda so we have a Poland team will be talking about attestation-based client authentication and token status list we had some internal meetings to talk about those but hopefully we'll hear the latest on those documents and will be talking about fetechm profile for oath And I think he's going to be remote unfortunately, this time richard wilhelm be talking about PICA and David will wrap it up with the two kind of new topics al-zhen profile of authorar and the Alpha Authorization Policy Language So that's our agenda. Any questions? comments? If not, let's kick"
  },
  {
    "startTime": "00:04:00",
    "text": "going Oh, yes, we have two Yes, okay, George and Heather, thank you for taking notes notes Okay, Paul Chris, Tobias, Tobias it is, Tobias, it is I just handed you the reins Go for it. Or do you all to drive from here? Cool. Thank you All right. So the is a bit of an update on all with two attestation-based Clonorth attestation-based Kline Orth. So starting off with a bit of a refresher. I can get this to work Is that updating on your side refer? Say it again. Is that update? Just checking that the slide changes with should be able to move it around. If not, I can move it from here. That's okay. Are we on slide two? I'm just checking if my end's update Yeah, we can see slide number 2. OK, cool. All right So a bit of a refresher So this draft really in the classical sort of owl security model, generally the client, when we're conducting client authentication, it's generally happening through a back channel and this draft is namely sort of focusing on a client authentication method that can provide an attestation back to a front channel based client such as a native application or a web application through a key attestation scheme that can then allow that that front-channel-based application or client to then authenticate itself directly with an authorisation server. The main reasons"
  },
  {
    "startTime": "00:06:00",
    "text": "for this are often privacy authenticate itself directly with an authorisation server. The main reasons for this are often privacy-related reasons, especially in credential issuance where there is a desire in deployment to not have the transaction transact through the back end of the client So with that, I will move on. So this is the architecture that we have shown, but it hasn't changed since the initial creation of the draft generally the client instance will generate an attestation based key. It will then perform a protocol which is steps two and four, which the steps of two and four are out of scope for this draft, the actual protocol and what is in scope instead is the syntax and structure of the generated attestation that is generated in three And the syntax and structure of the attestation proof of possession or pop that is generated in five along with the interaction or the interface that the authorization server uses between the client instance and the authorization server in step six, such as authentication at the token endpoint or various other old two related endpoints And so generally, there is two signed jots involved here. We have a client attestation and a client attestation pop. The client attestation signs over, the client attestation key authenticating it and then the proof of position as well fresh proof of position generated by the client instance where both are sent to the authorisation server to authenticate the client instance in its interaction to the client client Okay, so, so changes"
  },
  {
    "startTime": "00:08:00",
    "text": "since IETF-119 we've published two new revisions, draft two and draft three Draft 2 was a small, simple change I'm just going to move on to that, which is we added some text on the inability to rotate the client instance key so this is generally just calling it that because within one client, client ID, we would have many client deployments and those individual instances we need to be able to when we're issuing say a refresh token be able to ensure that that refresh token can only be exercised by that instance and the way that that instance is identified in a given transaction with a refresh token is through that instance key That basically means that in order for the client instance to rotate the key, it would have to start a new transaction in get a new refresh token because that is what's used for continuity in the transaction I'm just checking there's no one in the queue on that. Okay, cool Changes and draft three, so based on feedback, at IETF-119, we have removed the usage of RFC-7521 So we have and we've also shifted from that syntax to find an RFC 7521 for conveying the assertion across to more of a header based syntax introducing two new HTTP headers one is always client attestation and the other is client Oath client attestation pop with the syntax being somewhat akin to how Deepop conveys proof of possession and JWTs in headers And we also firmed up some of the language"
  },
  {
    "startTime": "00:10:00",
    "text": "around the notion of the client instance in the terminology and improved text around that concept to make that a little bit clearer So this is the previous token request as discussed on the previous slide. It was prior to the changes we've made based on RSC 7521, where we were using the parameter of client assertion type and client assertion defined by that RFC to convey this attestation and the attestation pop however we've now shifted to a model on the next slide that uses header-based syntax So we have moved to this syntax, which allows us to convey this attestation and attestation pop more easily in other endpoints to the authorisation server, not just the token endpoint or endpoints that support RFC-75 and yeah, so this is the main significant change we've made. And with that I think I will hand over to Paul. Have you got slide control, Paul, or would you like me to I don't have if you can do that, it would be fine. Yep. I'll also wanted to add the header base syntax gets rid of them squigglys so i'll never if you can do that it would be fine i also wanted to add the header base syntax gets rid of some squigglys as some of members like to say uh just joking on this one. Okay so next I will talk about the current discussion that we have So the next topics for this, for the iterations of this draft. One is the depop optimization, which was delayed or where the move to the header based syntax was"
  },
  {
    "startTime": "00:12:00",
    "text": "kind of a prerequisite So the motivation for this is that for a lot of use cases where we envision at a station-based client authentication, we assume that we want a high level of assurance for our use cases So usually when we have this, we also want to protect our access tokens And yeah, the most common way that this is done today is Deepop So we get into a situation where we could actually have some benefits where we could say, well, we know which kind of key is used for Deepop because we get some information over the client at a station and maybe we can do some optimizations where we use the same key both for Deepak and the client attestation, and it would be nice if we could combine this and just have a single proof of possession because otherwise we will have two proof of possessions, one for DPA Deepop and one for the client authentication with the same key given we're using this in high assurance scenarios this might be a hardware key or a key in a cloud hsm so doing this operations kind of costly um so there was a lot of discussions if we can optimize the combination with Depop, which can be seen on the next slide slide Tobias, can you move on the next page? Thanks So yeah, instead of having the Oath client, adaptation pop pattern, instead we have here the depop header and the ruling could be somehow"
  },
  {
    "startTime": "00:14:00",
    "text": "along the lines if you've got it check if there's like an off client adaptation pop header if that's not present and there is a depop a header for approval possession then please use this one so this is something that's being discussed. There's some people that really like this some people that really don't like this I think we still need some more input in this and some more likely discussions and I think this will be the next big discussion post-drawn Beside that, we have another live discussion that has been ongoing Yeah Paul, do you want to take questions or? opinions on this one before you move on? Anybody has any thoughts about? this? Yeah, I think generally refer to what we're looking at before you move on or anybody has any thoughts about this yeah i think i think generally refat what we're looking for is that um so just to explain the trade-off here as well when we use this client authentic method with depot without this optimum we'll end up with three JOTs, basically, in the H-P request, and two of those are proofs of possession that in we would we would contest that in many common use cases the key that is being used for the proof of possession could be the same so those proofs and possession are in fact somewhat identical right And so the proposed optimization is that if you are using Deepop and you're using this client attestation based authentication method, that you could drop the Oath client attestation pop and just have a depop, right? So we're not saying that this is the only syntax that becomes valid, but is this a valid optimization? And do we want that conditionality? when depop is present? Or is that a"
  },
  {
    "startTime": "00:16:01",
    "text": "code path that implementation? don't think would be useful? Yeah, thanks for the clarification I think Peter has some thoughts on this At the risk of repeat myself, I feel like this is potentially dangerous behavior, right, where we're using a key that is many to do attestation authentication, and then we also use it for this other proof of possession thing I think there's plenty of examples of how this kind of thing has gone wrong in the past, and so I once again run to add the risk of being repetitive, say, I haven't seen any reason why this is not a risk and I haven't heard any arguments apart from there's an efficiency gain so Okay, thanks Peter. Brian I've been a long-time proponent of only using the depop header for proof of possession. I understand there's desires on both sides, but I do think this is a very reasonable compromise as far as compromises go. No one will really like it, but actually it think it's reasonable and at the risk of repeating myself I think Peter is just sort of fear-mongering on this one I cannot see any real way that it could go so terribly wrong. Maybe I'm not imaginative enough but it is also optional So those that don't want to make the horrible mistake that Peter's saying they would make a don't, aren't required to because we're making it optional this seems very reasonable to me so I would be in favor of and do we do we have some security proofs like it would be like coming out of the TLS meeting for every small change you need to have a security proof And we basically decide things based on gut feeling and kind of looks nice mood of the day doesn't sound right to me actually"
  },
  {
    "startTime": "00:18:01",
    "text": "no but we don't have that kind of proof coming on any of our work So I think we just don't So to try to ask for it, it's just not, it's not we don't have that kind of proof coming on any of our work so i think to we just don't so to try to ask for it it's just not well refa and i can can ask people to do that yeah but we don't have that kind of proof coming on any of our work. So I think to, we just don't. So to try to ask for it, it's just not. Well, it's not. Riefart and I can ask people to do that. You could Go ahead, Christina Christina i don't know this idea if i just realize how short i am am So, I think previously, the pushback to this optimization has been to the idea that this is the only way to pass a pop for OS client doesn't attestation but as an optimization I think I kind of changed my mind but I think as optimization as an option it does make sense and does simplify things for deployments where deep depop and the pop or client of the station is needed And now to the previous conversation on and comments on security so we don't have this, you know, like official you know, threat, official, like, I don't know how did you call it they you know like the whole peer review paper or anything but within the german wallet product we did invest a lot of time in analyzing the sequence diet diagrams and the key management you know what happens if this key is an hsm this key is low locally, like, and if you look at the German architecture diagram, which is open source, we do have, like, the amount of keys and involved in all this is actually fascinating you know, back end, instance, and all that. And I think the outcome of that is, and it wasn't just like you know as usual suspects but you know folks from ISAC and some other like, you know, BSI the german security agency being involved And I don't think they've heard a huge security concerns, but regardless"
  },
  {
    "startTime": "00:20:00",
    "text": "this optimization So, yeah, obviously it's not like peer-reviewed paper or anything, but I think it's something a little bit more than just a gut feeling that this actually might be okay as an optimization Could you post something to the chat or the mailing? list or whatever? Yes. Thank you Awesome This is amazing because I get to concur to the three different people that talk before me I do like to optimization. What I do fear, however, is because of the potential amount of keys involved the fact that it is optional for use may trigger or may cause issues in implementation So is it possible? and I mean gauging consensus here, if Deepop is used? and all client attestation is used, make this a required pairing so that you don't end up in a scenario where I have to create an implementation that handles two, three, four different cases Okay, thanks, Philip yang justin the points that were previously made here in different ways. I'm with Peter in that it feels like this could go sideways but I also don't have a specific thing in mind for it to go sideways From an architectural standpoint, I like the optimization I like the cleanliness of saying, if you have one key, do it in one place, because that type of simple also leads to better security and better implementations, which is a bit of what I think I felt was getting at So I think what our best way for better implementations, which is a bit of what I think Philip was getting at. So I think what our best way forward might be to do would be to put this in and immediately begin writing security"
  },
  {
    "startTime": "00:22:00",
    "text": "considerations of the ways that it could go wrong if you use it That way, if it does end up being a really bad and insecure idea then we'll at least have documented it and told people when not to use it and why Kind of like we did with the implicit flow and resource owner password flow in Oath 2, and that worked out great Boy, that was not a good example example John John No, I'm feeling short So I like the optimization. I agree that with Philip that we should probably reduce some of the optionality by saying that if you're going to combine the two, this is the way to do it I can also see on the other you know, I understand that you may not be able to specifically opt optimize having two different key for this but on the other hand the it might actually be better from from the perspective of saying look, if you're, if you're, you should use key, a key which has a single sort of security property for both things because, you know, if you have to keys that are two different security properties, then you know, the attacker goes for the lowest bar. So you're, by having multiple keys you're, you may be insinuating that you have security that you, that you don't you don't so the fact that you're securing both you know two different legs with the same key, which has the same security property and storage is attracted and I'm always in favor of doing fewer signatures"
  },
  {
    "startTime": "00:24:00",
    "text": "That causes problems on its own Tobias and Paul, when, when you do the key at the station, that key presumably resides in some hardware security module. If you do the depop, would that key also be in that hardware security module or would that be elsewhere? well it's not a certain could be right And I think that's where the expense comes from and the design I guess, to do less signatures if possible right because yeah if that if it keeps were to reside in hardware and there is a expense involved, then then that's another reason to yeah have just once a signature instead of two yeah and you get the assurance on top what kind of value the depop key has, which you don't have with vanilla depop Right, John And back to my point, having a much weaker depop key than the authentication attestation key may lead to people mistakenly have lower security. Yeah that's kind of a point I think here, John, right? is tying these two keys together and a lot of these interactions so they have an equivalent security means you get better consistency around the client security client incident security across the transaction I think that the general guidance of you have a key which has security properties which the issuer needs to know about or the OAT server needs to know about, you kind of want the same security level for all your proof of possession interactions If you start mixing them, that's harder to explain"
  },
  {
    "startTime": "00:26:01",
    "text": "harder to reason about. Okay, thanks John. Philip, quickly, we are running out Very quickly. So what I'm hearing maybe is that I don't see why we should support the optionality. You know, if if you use deep open this together this is the way It's, I think it's not that easy because we, one of the motivation, that we move to the header based syntax was that we don't only want to use this with the authorization server, but people explicitly asked if we could use this at other endpoints and other endpoints can't have Deepop or maybe less likely to. Okay, so this is meant is if you're using depop, and this together, then this is the way Okay, not the other syntax And yes, using this for other end endpoints such as at the resource server, this is perfectly fine and that's why we moved away from the body-based syntax, which JSON-based APIs can support Okay, Brian, go ahead very quickly though, and Paul and Tobias, we're not quickly switched to that other presentation. I'll try to be quick. Sorry actually, I kind of agree with Philip, but it has been put totally to me several times when I've been arguing that point that there are real world use cases of wallets for which the computation of a particular key is both expensive and very important for the attitation and at the same time they want to be able to do depop separately with a less expensive and less secure key but I've when I've been pushing for this I've been told that that's an important use case so that that's part of why it developing this way. Yeah, we should definitely double check that, whether that's still true today That's a good point point"
  },
  {
    "startTime": "00:28:01",
    "text": "yeah and I'll just I just briefly mentioned the other discussion when is the naming discussion because some people, especially from the Reds community, don't like the attestation term. And the other one is about a non-satching But yeah, due to time, let's not give it this topic. Okay Do you have too many slides after this? Do you want to briefly talk? about those and then switch to the other presentation? Do you need the full 20 minutes on the other presentation? Um, not sure yeah I think there's not that much about status list I think that's it's fairly but let's spend one minute on the other two discussion points Okay Otherwise, can you move to the next slide? As I mentioned, the first one is the naming discussion discussion Just make it very short, Toursen and I had one proposal that says, keybound job client authentication It's there if there are any opinions on this? There's an issue on this or just cry out loud now But you are doing key at the station in the document, like calling it also authentication would be like it's both at a station and it's both key bound right yeah or do do you want to do you want to state your proposal here? Hi, where is Steele. As the author of an another draft that also used the word attestation incorrectly or not incorrectly, but"
  },
  {
    "startTime": "00:30:00",
    "text": "whatever, just use assertion for most of the cases where you get this sort of pushback assertion seems like it can be a better option sometimes But in this case, it's really at the station. I don't I can't answer that. But if the rats people are saying it's not like you can continue i think it's there i would talk with them about that okay okay John and then let's close this Um, geez, naming has come up conflicts has come up a couple of ties, hasn't it? Um, I guess the rats didn't invent attestation Attestation is used lots of, a term used lots of place for lots of things. I think we can ask the area director, perhaps to say come up with a determination of what or not it's okay for us to have a different attestation format. I don't know whether there's an expectation that all attestations go through that particular work group I I have a really hard time seeing that they can completely squat on a generic term I don't think rats is doing that. Rats tries to be kind of helpful with the terminology. And because it's used in so many different ways, like having consistent terminology, as we all know, also in the identity space is really important if we have the same properties of course if it's different properties then we shouldn't use this this terminology, as we all know, also in the identity space, it's really important if we have the same properties. Of course, if it's different properties, then we shouldn't use this name, right? That's the key point. But I had it over to Kathleen. kathleen moriarty, one of the three Rats chairs. I don't think necessarily the problem is your use of it unless somebody has said that explicitly I think it's that we all wish we could backtrack and never have used the word attestation And the reason is because if you say it outside of these"
  },
  {
    "startTime": "00:32:01",
    "text": "circles, people are so utterly confused and they only think about paper or processes that result in a paper report that says this is an attestation tied to an auditor process I am finding this I have no idea of those people. Well, so I can tell you 20 so years ago, I did an attestation against the certificate authority that I managed and it was to attest that the practices, the operational practice matched the practices statement and the practice statement was aligned to the policy right? And so it's this auditor process that people assume with the word attestation. So I think if we're able to use it in rats, you should be able to use it, but be warned, it's confusing Okay, I think there is probably more work to be done and discussion but Hannes and I will kind of take it offline and talk maybe with that RADS team and see how we'll proceed here Okay, thank you all Paul and Tobias, let's do you want to discuss the last one? quickly, but you're not. Let's move just too much time for the other one, well Yeah, let's move to status list You want, you want to, okay, switch You can switch from your side Yeah, so I don't have a screen control, unfortunately Do you still have a device or does it work? Yeah, I do. I do, so just tell me when to. Yeah, okay. Then let's go through it. Togets status list, it was formally called Jodcot status list That's a revocation or status mechanism"
  },
  {
    "startTime": "00:34:00",
    "text": "We have tokens or credentials or whatever we issue and maybe the semantics about this change and we want to communicate this afterwards and therefore we have a revocation or a status mechanism so that we can convey this afterwards Here's an example of the entities involved so usually we have something that we call an issuer and he issues a reference token This can be an access token, this can be a very foul credential this can be anything so we don't make any assumptions on this. This can be communicated to a client, but it can also be directly communicated to a reliant party and then the relying party has the possibility from within this reference token to get information about the status and in our case of the status list, he has then the ability to fetch it status list, and this status list is provided by the issuer himself or by a status list provider and by this means he can get information about the status of the reference token next slide So how does this work in particular? Here's a JSON example of a reference token in form of a jot. We have a new claim that reaches the that is the standard Here's a JSON example of a reference token in form of a jot. We have a new claim that reaches the status claim, and at the same point, this is the extension point for other status mechanisms as we have the status assertions that was presented on Tuesday So we register a subclaim under status, which is status list. If you see status list, then you know this is a status list according to this draft If you see other parameter names, then you could look up in the IANA registry and then we have a point or two different draft"
  },
  {
    "startTime": "00:36:00",
    "text": "The status list in parameter then contains an object with two members The first is the URI, which point to the URI where the status list is host and then IDX communicate the index within this list where the status for this particular reference token is available Next slide So how does this status list if we fetch this a URI that look like on this two? possible ways to communicate this. One is a job, one is a cot And yeah, there's some validity information and some yeah jason web token usual stuff the most important one is the status list property member parameter and it has two parameters one is bits this communicates How many bits are used to communicate the status of one? reference token. So if this is one, this means there's one bit for each reference token which means it's either valid or invalid but that could also contain more than one bit And then LST is the list of the status list And in this case, this is base 64 encoded and deflated status list byte array. Next slide we can then see how it looks like so this is a byte array or we call it byte array instead of bit string for certain reasons I don't want to go into here at this point, but you can see so from the top one is the reference token, the URI then matches to uh lower black box, which has"
  },
  {
    "startTime": "00:38:00",
    "text": "the status list and then you deflate the list and then you look at the specific index and then you know the status of the reference token This is how it works And then handing over to Christian Yeah, and looking at the time, we need to be a bit fast, to have a bit of time for discussion. So changes since Brisbane. Next slide, please A lot of editorial changes, fixes with with slides we need to be a bit fast to have a bit of time for discussion. So changes since Brisbane. Next slide, please. A lot of editorial changes, fixes with like stuff we did before. Two big ones that is validation rules for status list token and a status list aggregation mechanism. Next slide please. So the first one, we did the first shot at the validation rules which was also like a good thing for us to figure out how easy is it to write it down And we realize we have some problems with complexity because of the different options So right now we have C-Bord, Jason, Jodd and CWT encoding and that makes actually writing validation rules pretty complex So we would like to make that easier We would like to simplify the draft a bit and of course feedback is welcome. Next slide please Status list aggregation mechanism. If you have ideas for better name, please go ahead so this is basically the idea to enable a relying party to do caching. So beforehand if you know you are going to get tokens of a specific issuer or um idea to enable a relying party to do caching. So beforehand, if you know you are going to get tokens of a specific issuer or OOS server, then this enables you to beforehand basically cache status lists, and this is just a mechanism which basically gives you a list of these other statusists that are relevant for that party. We have two modes to retrieve this. One is basically it could be embedded in the status list token So this is basically as part of this spec And the other one that we foresee is it can be provided out of back"
  },
  {
    "startTime": "00:40:01",
    "text": "was basically additional metadata of the issuer So for example, with open ID for VCI or similar methods next slide please Okay, discussion, this came up a few times already especially after writing the validation rules we are considering again removing the unsigned option because basically all implementations so far are using the signed option. We would like to simplify the other option we were thinking about is ALK equals none but we are pretty sure this is going to be not liked so brian this orbiting the queue, I guess I would argue that the only option you support is the unsidable option. That would simplify a lot of things and the perceived needs around having this thing signed I think are somewhat mIESGuided the one real use case I've understood for it is something akin to the draft that was presented the other day, which is like a stapling type approach of which there are other ways to do it when you're doing this as a verifier. There's really no redesign it. And if you do want to have more than one options, there's now a whole working group spun up for the seaboard based digital credentials. So you could punt that stuff If you demand a signed option, you could slide that out over into the other working group. But it'd be better just not to sign them all sign it all so i can see I'm being laughed at and I appreciate that because I know it's in good humor okay we're running half time so um Roy, go ahead. For your Williams Microsoft. Christian, is there still a time to live? to control the cash lifetime?"
  },
  {
    "startTime": "00:42:01",
    "text": "Yes. Thank you Jeff. Yeah, Jeff. Very cool quick. So it sounds like now there are like multiple identifiers. There is like the index into the list There is the GTI. I think it might be complex I'm coming from the PQI, CRR OCSP world. That sounds like very complex. And if I'm thinking from the aggregator perspective, now if I want to aggregate an entry that has an index of one in one list five and ten I need a very large entry that will have a lot of value that means nothing because I'm just referring to of one into one list five and ten I need a very large entry that will have a lot of value that means nothing because I'm just referencing those different index along the way so that sounds very complex and just to like reemphasis what Brian said, so far everything was on to the G-Zone world and then a translator to make it to the CBO Seabor world. Here you are trying to solve the both side of the equation in one singular is here in maybe that's where the complexity is coming from Thanks Richard. richard barnes PICA co-author. I was just responding to what brian campbell was saying about signing this. And so it just wanted to get up and say we should absolutely sign these for the same reasons we sign PICA to avoid runtime dependency verify time dependencies on HGTP servers i don't know about all you like wealthy people with 5g connect and whatnot but like a lot of the world is not connected all Yeah, absolutely I mean, even, even, you know, my American in-laws are out in the country and don't have Wi-Fi all the time Sometimes I need to authenticate to get in the house So yeah, let's avoid those runtime dependencies Thanks, Richard. Christina, do you have a quick comment? You were in the queue there okay thanks Christina okay"
  },
  {
    "startTime": "00:44:01",
    "text": "Thanks, Richard. Christina, do you have a quick comment? You were in the queue there? OK, thanks, Christina. OK. We are out of time for this one. Tobias and Paul. I'm christian thank you very much And yeah, let's continue the discussion on the list, right? Go ahead. One more important slide for the timeline that we envision Okay, go ahead. You move to the last one? Yeah, I would look. So our plan was we feel that there's not that many major issues that are being discussed So our intention is for next IT to have the draft ready to get to working group last call. So our intention We know there's been some discussion about whether this needs to merge with status assertions or not. We are all three convinced that we do not want to do this because this standard is pretty far and is required by a lot of people and will add a lot of confusion if we merge this with status assertion So this is not our intention and contrast is our intention to kind of get this draft ready and done. Okay, awesome Thank you all. Appreciate that Okay Aaron? Yes, hello Can I get the slide control? Great All right. Here's a picture of Vancouver from before I had to leave and go home It's nice to see everybody. So today I'm going to talk about feds in particular the FedCM profile for OAuth, which I've been working on I do not have a draft that to"
  },
  {
    "startTime": "00:46:00",
    "text": "read, so pay attention to the slides The reason I don't have a draft, we'll get into it in a little bit I'm going to firstly quick, first quickly give a recap about what Fed CM is and the problem it's trying to solve But I'm not going to go too deep into this because we already had a presentation on this at the interim but I do just want to quickly recap this to set this stage. The problem is that there are a lot of different ways to track users around the web and using various technology The two we're focusing on here are links decoration and third party cookies, specifically because those actually end up having a lot in common with Oath and OpenD Connect So third-party cookies is when you have a website that's embedded or some sort of content from another domain embedded on one domain and the cookies get sent along from that website gets sent along to it, which means that if two different websites have the same page embedded, the two different websites can actually end up tracking the user and knowing who the user is in common common Unfortunately, this is also how some open-endty connect features work. So in particular front channel logout, you have the identity provider which embeds a iFraming from different relying parties, relying on the fact that no pun intended, relying on the fact that those cookies for the identity provider will be, or for the relying parties will be included in the requests And the opposite is also true when a client or a relying party embeds an eye frame from an identity provider It is also relying on the fact that that cookie for the identity provider will be sent in the request from the eyeframe On another news, link decoration is another way that users can be tracked For example, example.com wants to use a tracking website to"
  },
  {
    "startTime": "00:48:00",
    "text": "figure out who a user is and connect them when they visit a different website they can send the user through it tracking website, which all of these cookies in this case, are first-party cookies, but because we can use query parameters, we can actually end up building a common user profile across all the different websites This is unfortunately how OOF and OPI connect and SAM will work. So we have clients that do redirects through authors servers that might redirect to another identity provider, et cetera, et cetera and back These query parameters that are used in our protocols are effectively tracking the user because that's what they're designed to do. They're designed to live the user between the client and the authorization service So all of the essentially it means that they look like trackers especially when we're talking about multi-hop federations that might send the user through three or four different hops before getting back to the RP So the FedCM, well, in particular, the working group, federate identity working group and community group are created as the W3C to solve this problem to basically allow federated identity flows to work without relying on features that can also be used to track users The community group was formed a few years ago This is open to anybody. You can join us you can just join the community group at the W3C The working group was formed last month, and that is limited to W3C member companies So there are currently two proposals for this in the working group. One is FedCM one is cross-site cooking access credential I'm going to go briefly into how these work and then talk about how we can use them in Oloth So FedCM it's a browser API that manages identity flows on the web, and the important part is that nothing is disclosed between the RP and IDP until the"
  },
  {
    "startTime": "00:50:00",
    "text": "user allows it so essentially the way it's the way this goes is the user starts at the RP or the client. I'm going to use these terms interchance because, even though they're from OWath and OPA-D-Connect So essentially, the way it's the way this goes is the user starts at the RP or the client. I'm going to use these terms interchangeably because even though they're from OAuth that RP will initiate the API which causes the user to see a prompt, agrees to log in and then importantly the browser, not the client starts making requests to the IDP endpoints and eventually gets data from the IDP to return to the client. So this is what it looks like, but let's go through the flows Let's go through a couple of requests. I'm not going to go too deep into this but this is important in order to figure out how we're going to insert OAuth into this. So this is the client code that it would be in a RP JavaScript code It's a JavaScript API. It'll say, hey, I'm trying to use this authorization server. Here's the client ID. We'll come back to that parameter in a RP JavaScript code, it's a JavaScript API, it'll say, hey, I'm trying to use this authorization server. Here's the client ID. We'll come back to that params in a minute. At this point, the client, it doesn't see anything that's happening and the browser takes over. And the browser starts making requests of the different endpoints that are part of the config here. It'll make a request to the config endpoint with no cookies, no information identifying what website the user is on. It'll then make requests to endpoints to find in that config file. It'll fetch the accounts endpoint with IDP cookies to know which accounts the user is logged in as. It'll also have to endpoints to find in that config file. It'll fetch the accounts endpoint with IDP cookies to know which accounts the user is logged in as. It'll also fetch the client metadata which yes, it seems backwards, but that's because the client presumably has pre-registered at the IDP. There's a link to an issue for when that is not the case. Then, the user will see a prompt and say, and it says the browsers finally able to collect all that information and say, hey, this user is trying to sign into this application. Do you want to continue? doing this? And only after the user click yes finally the browser goes to the identity assertion endpoint with IDP cookie and it tells it which RP origin is making this request finally"
  },
  {
    "startTime": "00:52:01",
    "text": "and which account the user chose. And it's a assumed that the IDP will return back some strength In the current API, this is called token mainly because the Google's implementation of this returns an ID token, which was kind of the assumed way that would work. But we're going to come back to that as well Okay, putting FedCM on hold for a minute. I want to also talk about the other proposal which is cross-site cookie access credential. This is meant to be a lighter weight version of the previous API. And you'll notice it has far fewer steps. This is essentially a browser API that enabled a website, like an IDP, to put some and you'll notice it has far fewer steps. This is essentially a browser API that enables a website, like an IDP, to put something in the browser that a different origin can then fit after prompting the user. So think of it as the IDP can drop something into the browser a string, and it can say it's only allowed to be fetched by certain origins, certain clients or RPs And then I when the RP makes a request to fetch that, again, the user will see a prompt saying, do you want to let this one website get it? from this other website? And then it will eventually get that string So that's the extent of this one. You'll notice it's much smaller surface area of an API so extent of this one. You'll notice it's much smaller surface area of an API. So, okay, how can we use these for OAuth? I do have a link to a GitHub repo, which I've linked to on the slide here. It's not really written up as a spec right now It's more of an implementation guide for how you could build an OAuth flow using FedCM and attempt a spec right now. It's more of an implementation guide for how you could build an OAuth flow using FedCM. It kind of walks through everything I explained and all the requests that get made I don't think it's really ready for a drag yet until the FEDCM spec itself stabilizes a bit because there's still a lot of open issues and detail here and there that need to get resolved and stabilize"
  },
  {
    "startTime": "00:54:01",
    "text": "and put into the spec before we can really formally provide it The alternate profile, the Crosslight Cookie access credential, spec, I did not, unfortunately, did not have time to prototype this before the session today mainly because the UI for it is actually not shipped in Firefox Nightly yet This one's being mainly driven by by Mozilla but I am interested in trying this So, okay, let's go through how we do this in FedCM first. Do you want to take questions now? It depends on the question. Let's see what Dick has. Go ahead, Dick And Dick Hart, how relevant do you think? this is going to be, given that Chrome is not going to do it deprecate third-party cookies? And do you, I mean, is there any activity on any browser besides, you know, the people that are building their, have their own thing of support? this, like a safari or mobile going to support? FedCM or anybody else? Sure. My impression of FedCM was it was a way for Google to continue to have the user experience they had a third-party cookie in Chrome when third-party cookies went away because they're a little widget used third-party cookies Yes, fair, good question So I assume Tim is also up there to hand used third-party cookies. Yes, fair. Good question. So I assume Tim is also up there to answer this. But my take on this is that yes, third-party cookies not going away anymore, apparently as of Monday or whatever. I'm not going away with Chrome. Not going with Chrome anymore, uh, means that does change the priority of this possibly however it's possible I think it's very possible in very likely that this will provide a better user experience"
  },
  {
    "startTime": "00:56:00",
    "text": "for users even if their product cookies weren't going away and redirect and link decoration weren't going away. And there's a couple features in here in particular the experimental IDP registration feature, which in a brings a whole brand new user experience to the browser that previously wasn't possible at all. So I still think there's reasons to look at this for those reviews As a provider, I find it super unlikely that I would ever support FED-CM because it breaks a number of things Thanks, and Dick. Tim Hey, Tim Cabelli. First, I'd ask you to create an issue with the things I broke because I'll be helpful Fed CM is an actively, you have to actively invoke CN FedCM, so it doesn't break anything Yeah, I was going to say um i filed issues that are still sitting there 18 months later, so. Come back that are still sitting there 18 months later, so come, come back, come back and have the discussions. That's not what I wanted to say I would argue FedCM is more critical now than be before because we should be careful saying third party cookies are staying. Third party cookies will now be a choice for the user. And as we've seen on the app platform, when users are given a scary prompt that says, do you want to be tracked, they click no, of course. So FedCM provides a consistent layer above all of that to provide these experiences because arguably the it's going to get even more fragmented. Today, you can at least detect based on user agent and client hints the behavior of a browser now you're going to have the same browser version down to the exact version number that behaves differently for different users because the user changed their consent. So I would argue if it's the the behavior of a browser. Now you're going to have the same browser version down to the exact version number that behaves differently for different users because the user changed their consent. So I would argue FedCM is more important than it was before Thanks, Tim. Heather heather flanagan, um Federated Identity Working Group Chair"
  },
  {
    "startTime": "00:58:00",
    "text": "So we're definitely continuing to work on this. It's interesting to me that, of course, the W3C tag took Google's high-level statement that they were no longer to deprecate third-party cookies and basically to sum up what are you thinking? and you know that Google made their choice perhaps, but the regulators aren't happy with it The W3C tag isn't happy with it I think we're still going to see this development in that space is just going to be odd. If you talk to the Chrome team directly and say, so what does this mean? They say, you know, this was news to us too. So we'll see how it all goes. Thanks, Heather Okay, I want to keep this move So I want to get to the Elwath parts So slotting OWOTH into this it's not entirely obvious at first because of all the documentation and existing examples that you'll find will return an ID token in this as the response to Fed FETCM. So instead, what I've done is you instead of treating this as a as a assertion, you treat it as an authorization endpoint in OAuth, meaning that you'll actually get back an authorization code. So you start the flow with sending OAuth request parameters, you get back an authorization code from the browser, and then you can go and exchange that for a token like you normally would. If you hadn't, you you start the flow with sending OAuth request parameters, you get back an authorization code from the browser, and then you can go and exchange up for a token like you normally would if you hadn't used FedC up. Importantly, this means we can do things like enable confidential clients by actually doing all the OAuth parts on the back end. We can do things like PAR. And this doesn't involve redirects, which means it's roughly equivalent to prompt none so here's what this looks like in in back end, we can do things like PAR. And this doesn't involve redirects, which means it's roughly equivalent to prompt none. So here's what this looks like in using the API, and this is using the new params feature, which is, yeah, still relatively new, not actually in the spec yet"
  },
  {
    "startTime": "01:00:00",
    "text": "but this is what's in Chrome So you put all your OAuth parameters in this Params not actually in the spec yet, but this is what's in Chrome. So you put all your OAuth parameters in this in this params object and then at this point you know, the user will see a prompt and then once they like click continue, I skipped over all the other stuff But eventually the request goes to the assertion endpoint which contains the custom params that the client had put in that first call to the browser. This looks very very similar to what would be sent in the query string of an OAuth authorization request or in the post body of a PAR request which means the response of this is actually an auth authorization code rather than an token directly. This means that the once this once this once we're back in JavaScript land So the JavaScript can just make a request of the token endpoint at this point plain OAuth request Now we're totally done with the FedCM flow. We're done with, we're out of that context and we're just in plain Oath So this works. I've prototyed this. Philip has prototyped this and I'm pretty happy with that architecture just because of the way we can pretty seamlessly slot everything else in it. And that's this what's documented on that thread pretty happy with that architecture just because of the way we can pretty seamlessly slot everything else into it. And this is what's documented on that, on that thread. Philip I wanted to get to the question I just want to interject very quickly It's not just the authorization code you can actually use any of our response types and response, well, not response modes You can use any response mode, so you can do your code ID token to do FAPI. And then likewise do response types, sorry, response modes such as charm, because the token can be assigned jot which contains then the authorization response and you can combine that with the number and plethora of response"
  },
  {
    "startTime": "01:02:01",
    "text": "types. That's all I wanted to say. Thanks yeah and there is an open issue to track this response changing to actually allow the IDP to return JSON data instead of just a strategy So if you're interested in that, there's an issue for that Okay, so the big question here you may have noticed this Params underscore situation Oh, I guess I don't have time to go over this This is how you could use a back to do this. But the main question here is, how do we want this params thing to work and how would it best work? for OAuth? Because there's a few different options um whether to prefix the custom parameters with param underscore whether to prefix the FedCM parameters with FedCM underscore and leave the rest of the custom parameters as is, or do everything as JSON and namespace them both effectively realizing that this is no longer a simple request, so it does require course. This is something that if you do have an opinion on, I guess we don't have a lot of time to discuss it here, but it's okay, it's pretty much just bike shedding but this is feedback that the working group is interested in getting from the Oath group So I'm going to leave a link to this issue on the screen here. And you can go comment on GitHub here if you would like to chime in Yeah, Christina has a comment here, so go ahead ahead If I understood correctly, this feels very similar to what we have already done in the for Open ID for VP for the browser, for the digital credentials browser API And it depends on your goal. For I us, the goal was to enable existing a open ID for VP implementation to use this new digital credentials browser API, this minimal changes. So it'd be we've done is, I think, very close to your last proposal"
  },
  {
    "startTime": "01:04:00",
    "text": "where it's, you know, JSON object that where you can just put in existing OAS parameters without changing parameter names or prefixing or anything you know, just put in there and you just add code to, you know, like take it out from this namespace part. So that's would be part of like our experience Great Okay, thanks, Christina um anything else um eran do you want to wrap up quickly here Yeah, I guess I'll just wrap up with, I collected a couple of, oops, button moved out from under my finger I collected a couple of issues that are on the FedCM GitHub here that are relevant to OAuth related things of deploying this in an OAuth ecosystem So if you are curious, please do check these out These are issues that are going to come up if you start building an OAuth, mainly IDP to try to actually integrate that into the FedCMAPI And hopefully I will be able to come back later and give a demo of how this and alternate version of how this looks with the cross-a-key access credential API. The big thing for to remember there is that it's essentially just you're able to drop the IDP drops one thing that gets pulled up by the RP so kind of have to think about the flow a lot differently rather than being more dynamic like FedCM Okay. Thanks, Aaron. Thanks Philip Philip Do you want? control? You want to drive? Oh, however you like, you've already got much thing go hi, I'm richard barnes. I'm here on behalf of my"
  },
  {
    "startTime": "01:06:01",
    "text": "myself and my co-author Sharon Goldberg to talk about this pico draft, which we had some discussion on me mailing list and this is going to kind of recap and see if we can have final a path forward. Next slide, please There we go. So what's the problem we're trying to solve here? As you may have heard me mentioned a moment ago at the microphones sometimes you don't have internet connectivity So if you look at how a lot of JOTs get validated right now and things like Open ID Connect you look at the issuer of the JOT The issuer is in the form of a URL. You do some HGTP BAT discovery to look up the keys that are associated with that issuer with which you will validate the JOT, which means that if you with that issuer with which you will validate the jot, which means that if those keys are unavailable for some reason, you fail to value the JOT. Now that can happen because that HGTP endpoint happens to be down at the moment you're validating. It might be because the open ID provider of the issuer who's working their keys and the keys you're looking for are no longer there So the idea is to make a object that can provide a little bit of persistency to patch over these cases where there's spurious failure in validation. Validation should have succeeded but it failed because there was this kind of mechanical failure in the middle. Next slide, please There's a lot of echo up here So just to give it out folks an idea of you know, why this is salient, you know, folks are looking to take a bunch of the verifiable credentials that are coming out in very to give folks an idea of why this is salient, folks are looking to take a bunch of the verifiable credentials that are coming out in various domains now and starts to pass them into application So, for example, you might use these in an intended encryption WebEx call to authenticate the other participants in the call Or you might use it to, you know, that your authority to go into a concert where there's some overwhelmed cell phone network The idea of PICA here is that it should be possible to cash these proofs of an issuer key authority"
  },
  {
    "startTime": "01:08:00",
    "text": "you know, the fact that these validation keys, these verification keys belong to a given issuer either in the application or in a client of a wallet that holds a credential, could cash it as well, be kind of like cashing intermediate certificates. But the idea is that even if the issuer of the credential is unreachable at the moment of validation, it should be possible to verify that, you know, a credential came from a given source including verifying that the fight looking up the keys that go with a given issuer Next slide, please So that was kind of addressing the availability case you know, case where you might have a connectivity, reachability problem with the server. Some folks in the context, sign community have a different kind of time-based challenge with these kind of live key lookups, which is that they have these use cases where they want to sign be able to reason about the past to be able to reason about something that was signed in the past So they have some of these timing authorities that will prove that something existed at a prior time And so they're looking for some artifacts that they could make that says at time T, authorities that will prove that something existed at a prior time and so they're looking for some artifacts that they could make that says at the get at time t issuer example.com was using this set of signing keys so that says at the get at time t, uh, issuer example.com was using this set of signing keys. So, you know, they're missing a thing that has that sort of statement about which keys is an issuer is using it a given time that is traceable back to the issue. Yeah roy williams So I would like to understand why skit and counter signatures wouldn't work here I don't know the technology you're talking about I'm happy to talk offline I think this may be a slightly so what I'm building up to is a very minor variation on what OAuth and open ID are doing today. So maybe there's a little bit simpler there, but happy like I said, happy chat off line I'm sorry? Okay"
  },
  {
    "startTime": "01:10:01",
    "text": "Honest, please join the conversation all right next slide please let's give you an idea of some of the use cases so like transient non-reachability and all this kind of rotation problem. So really the idea of PICA is to make this thing in the middle So right now, these things on the right hand exist, the things on the left-hand side exist So verifiable credentials get issued in the form of JOTS or CWTs underneath an issuer and when you go to do the HTTP requests to get the key for that issuer, you end up relying on the X500 certificate when you do the HTTP request So what the PICA does is sit in the middle and bridge those two together So instead of that glue being the bytes of the HTTP transaction between the two, the glue is just another job that is signed underneath the certificate you would have trusted anyway and contains the keys that you would have felt the two. The glue is just another job that is signed underneath the certificate you would have trusted anyway and contains the keys that you would have fetched over the HTTP connection. So this is basically like the minimal possible deviation from whatever you're doing for discovery today. It's just taking an HTTP connection, which is transient and happens in time, and transforming it into an object which is signed and can be cached and stored and reused. Make sense? Cool. Next slide I think just puts that in yeah so this is just you know an illustration of the structure it's just a jot. Tailout of the jot has keys Thanks to Mike Jones, at the last IETF meeting pointed us to a bunch of really good prior art from Open ID Federation, which is making similar statements of issue of key authority But basically the idea is you have some keys with life last IETF meeting pointed us to a bunch of really good prior art from Open ID Federation, which is making similar statements of issue key authority. But basically the idea is you have some keys with lifetimes and key IDs in the payload You sign in the signature blog in the header you have an X5C header, which has the certificate chain that would have, you know, the moral equivalent of what you would have had in the HDT had the HTTP server present in that live page"
  },
  {
    "startTime": "01:12:01",
    "text": "request. So again, the trust, the idea is to keep the trust structure exactly the same as we have today, not make any changes to that, just translate it into something that is an object instead of HTTP transaction Next slide, please. So, we had some discussion of this at the last IETF. We had some discussion at an interim and the chairs of thank, uh, it's adoption call after that interim. I think the conclusion, if I were to summarize out of that, correct me if I'm taking liberties here, but my, interpretation of the feedback in the adoption call was that we got a lot of good interest a lot of folks interested in solving the use case we've talked about here. But a couple of challenges to address One Mike and Giuseppe raised, the mic of observed that there's been kind of a long-term trend away from using x509 at the application there, which is totally sensible because X-509 is like old ASN1 technology doesn't align well with a lot of current ways delegations happen And it's just a lot easier to program with things like like jot so ASN1 technology, doesn't align well with a lot of current ways delegations happen. And it's just a lot easier to program with things like Jock. So the observation is that this the reuse of X509 in the way we've proposed here kind of goes, cuts against that trend a little bit. And it may be that it the future, like we would want to take that use of X509 and generalize it away in the same way we've done it in the past so the thing the the the issue to clear up here and to make sure and i appreciate mike thanks for discussions we can in clarifying that i done it in the past. So the issue to clear up here and I appreciate Mike, thanks for discussions week in clarifying that. I hadn't quite gotten it from the mailing list discussion So I think the two do here, which I'd be glad to I haven't filed as an issue yet, but like it definitely would be like issue number one on the on a working group repro if we adopt this is to make sure that we make clear that this that other ways of authenticating this are possible in the future we're being coming in the moment so that we have a definite algorithm that you can write code to. But we want to have upgrade paths available and i think we can kind of sketch out what"
  },
  {
    "startTime": "01:14:00",
    "text": "would be necessary to establish that upgrade path So I think I would be that was kind of the feedback in the adoption call I think that's how I would try translate it into an issue if we were to adopt something The other thing is that other kind of issue that came up is that there are some security considerations we need to keep track of when we take certificates of that are of a form that is accepted in the WebPKI and use them for non-TLF applications so as the draft is written right now, it has not great security properties I will freely admit that right now In particular, the draft right now says that if the issuer URL is HGTPS colon slash example.com then the end entity certificate and the certificate chain has to authenticate example.com Not great security properties out there because you know, I've got two boxes I might like to consider here. I've got my peak assigning box sitting in the back of the end infrastructure under lock and key. And I've got my web servers out facing the public internet So like very different compromise risk, but this says I now have certificates on both of these attesting the same domain So if that web server gets knocked over, it can get used to issue PICOS that this high security box would normally be expected to issue. Not great So we need to kind of obviously write up this sort of stuff. And there's a variety of mitigation we can apply to make this better. For example, you know the easy one of the easiest and very effective things is to add a prefix. So, like, instead of saying the PICA, signing certificate has to have example.com, you could attach a prefix that says the PICA signing certificate for example.com has to authenticate the name, say, under pika.example. Right. So you'd have some certificate that would only be expected to exist on your PICA signing box there would be no reason to have that certificate on a web server. There are also some mitigations you can have around lifetimes, limiting the lifetime of these things"
  },
  {
    "startTime": "01:16:00",
    "text": "so that if someone, a temporary compromise wouldn't lead to like a three-year-long pico or something like that So definitely some things to keep track of here, but I think they're tracked The next slide is my last slide, which is just to raise the question of what we think, you know, obviously, hopefully, we've established that there's interest in solving this problem and I'd love to get to a question of whether we think there's enough clarity around the issues. We think that this is a solvable problem that there's a track solution here. We should go for us working through. Yeah, Lee So, yes, I actually like this. And it was, as you point out, it gets us a way to get away from the WebBKI as a kind of way to security solve, so all security problems on the web, which is a good thing, in my opinion As you kind of alluded to one of the good side effects here, is I can have, like, operationally sound web PKI certificates on my web server while having things that sign the sensitive stuff the security sensitive stuff on my my article my well-known artifacts, with things like an EIDAS sort of compliance certificate, right, while not having to kind of mix those two together. Like, it's an excellent thing right? And I'm sure that whatever you said and Mike has told you about open idea federation we can figure out the way to kind of fold the things into this will be a very very useful thing. I will note though that. So I'm up here supporting like Adobe we can figure out the way to kind of pull these things into this will be a very useful thing. I will note though that so I'm up here supporting like adoption and working on this. I will note that there might even be prior art in some work that Stefan Santos did very useful thing I will note though so I'm up here supporting like adoption and working on this I will note that there might even be prior art in some work that Stefan Santos on did early in a few years ago but basically to record identity data in notary evidence associated with digital things signatures of PDF and PDF and XML where it's you're basically building a notary system, right? Anyway, so there the people have kind of been"
  },
  {
    "startTime": "01:18:00",
    "text": "circling the same kind of problem for now. So I, I think it's great yeah thank you if you if you have links to that prior i love to see okay thank you Thanks, Leif Thank you. Hi, everyone again Can we go back to the example with the X5C header? Well, I get it what I'll talk. Thank you, this one So there's hinges on, I see two things. First of all, having the possibility to properly process the X5C JWS, JWS, JW parameter, which generally isn't widely widely supported in your Jose implementations And on top of that, even if you were to actually have the possibility to parse an X5C and traverse the traverse the chain for which the support is even less, the support of the programming language is at least the ones that I have encountered that would allow you to take a certificate and throw it against the the route CAs you have on your system you're going down the funnel, right? You have your JWT libraries that don't support this You have your programming languages that don't give you structs or constructs in order to work with certificates and then they don't even have the possibility to throw the validation against the route CAs that you have on your system So this is I wouldn't expect wide adoption from implementers, is what I'm trying to say Yeah, I totally understand that there are introduces some implementation challenges I think a light bulb went off from you when I was talking to the mic, when he distinguished between kind of transport level usage of X509 and application level usage"
  },
  {
    "startTime": "01:20:00",
    "text": "so yeah there I mean there would definitely be additional work to be done in verifier our libraries um we had some feedback on the list from a couple of verifier implement potential verifier implementers who said they thought this would be tractable I am also a potential verifier implementer and we have implemented some comparable certificate chain validation logic in this kind of parallel code path so I think it would be doable. But you're right, you're totally right, it would be worse I think the moment you start thinking about you start thinking about the subject ends and you start thinking about alternative domain names, at that point, it just you're at the mercy of the implementer to get everything right, and this is traditionalism a system, that's traditionally an area which Jose library implementers just want to avoid completely myself included on at least two libraries Okay. Thanks, Philip. At all yeah hi um i mean this look like a certificate for the OPP, right? is that sort of a good assumption and you know, how long is it going to be valid for? How do you revoke it? All the concerns that you would get with certificates, right? Yeah, yeah, definitely. This is a sort of extending the PKI to talk about the open ID or the OAuth issue jot issuer keys And so yeah, you would have similar concerns about lifetime and revocation, things like that Okay, does that answer your question at all? It's good Okay think, I guess what I heard is that the question is valid I didn't hear an answer at such maybe could you restate the question? That, you know, how do you revoke this certificate, correct?"
  },
  {
    "startTime": "01:22:00",
    "text": "example? Excellent question I don't think that there's anything in the draft right now, but you could certainly see adopting things like status lists for this in parallel with other types of dots that were being discussed earlier That's a good question We should definitely address that in the draft Thanks at all Peter. pieter kasselman, single key use enthusiast and peer mongerer First of all, I want to thank the authors for this work, I think it's a great problem, or it's a problem that's really great solution to be working on or problem to be working on. Also, thank you for tech work. I think it's a great problem, or it's a problem that's really, it's a great solution to be working on or problem to be working on. Also, thank you for taking into account the feedback on the security pieces. I read the latest draft I'd be very supportive of this and would love to work with you to figure out if there's a way to make sure that we can separate these concerns in a safe way So thank you Okay, thanks Peter Yeah, like Hannes and I think we want to kind of get a feeling of the room here and make a call for adoption again Obviously, the official one will be on the mailing list, but let's try to do it here and see what we get here Obviously, if anyone has any last concerns, if you have a further jump to the mic mic mic Yeah, this please Yeah, I mean, I'm just not sure if I want a new certificate format, right? why not just use x-519 certificates in that case"
  },
  {
    "startTime": "01:24:00",
    "text": "I think the idea a new certificate format, right? Why not just use X519 certificates in that case? I think that the idea is to reuse the WebPKI, which in which case the end entity certificates talk about domain names, not about open or oath issuer key job issuer keys you know, much like TLS delegated credentials, the idea this is extended the PKI into an application specific domain and so it makes sense to have something that's not an x509 certificate certificate certificate Thank you. I don't see any changes there So we have some have some some rough consensus here people that any changes there. So we have some rough consensus here, people mostly in support. There are some people that have some concerns with this Let's try to continue resolving those issues and then see what these people that are against adoption. You want to say something? So I just want to repeat the value that we have in the room and of course from remote as well, today we have a to say something here? So I just want to repeat the value. So we have in the room and of course from remote as well. Today we have 11 yes, 3, no, and nine who have no opinion And so I think I will as always repeat that on the mailing list and then, yeah, but looks so far pretty promising It is promising, but we would like to hear from those people that voted know and what what's their concern so we could hopefully richard and team can address those concerns right? Yeah, absolutely happy. Okay. Have more feedback awesome all right thank you thank you thanks to folks for feedback yeah Yeah, I don't know what his clapping comes from these days I heard that a couple of times Like, I don't know where this is going from. Like, we didn't used to do that Some new people came here"
  },
  {
    "startTime": "01:26:00",
    "text": "and okay david it's your show now This clapping wakes up half of the room Okay. Which one do you not? Yeah, yeah. Okay, actually. Yeah, yeah you pick if you want control you rar first yes okay so did you ask for, can you ask for it? Because I did. Otherwise, I don't see it here David, David. Meanwhile it should be David, here he is up david hayes. Yeah, did that work? Yeah, now you can share whatever you want Oh, you need to pick, yeah Cool. I'll try not not trip. There's a thingy behind me. All right, my name is David Some of you know me. I'm the Zachal guy No judgment, please And I do tend to insert the word authorization everywhere I go So I think OAuth stands for open authoriz. Oh, wait, no that is actually true. It is open authorization. Anyway it is not open authorization. Not anymore? No, that's not. All right I'm going to start the war here. And, um, authorization. Anyway, it is not open authorization? Not anymore? No. No, that's not. All right. I'm going to start the war here. And, yeah, I'm going to talk about the profile that my colleagues in the Altzan open ideas Althazan Working Group and I put together. I think there's a few members of Althazan in the room. Yep, Steve Phil. Anyway as I was writing these slides, of course, the whole crowd strike debacle happened, so I thought it would have a bit of fun with that. Normally, this is animated, but I guess animation doesn't work. Also, as I was posting the profile in the past week or a couple weeks, I got a lot of feedback. Thank you very much both from this community, but also the ID Pro community and the"
  },
  {
    "startTime": "01:28:00",
    "text": "open ID community on the fact that someone already attempted to do a RAR profile at IETF1 119. I saw the video. And I just want to say that it's really legitimate feedback, but this is not about Cedar or policy language, just to put that you know, to clarify that One more thing, we were working on this profile at Identiverse with my, with my coach or policy language, just to put that, you know, to clarify that. One more thing, we were working on this profile at Identiverse with my co-chairs, with Eve Mailer, with Jerry Gable and at some point we had to plug our computers in and all we found this is in the U.S., this was the plug we had to deal with. And I've never seen such a plug before. I think it's Swiss, not too sure Italian? Okay. European plugs are weird because they're all the same, except they're not the same It's kind of strange. And that's actually the matter here with all the Zen The reason why we came up with al-Zhen in the first place is that if you look at authorization systems I'm going to call it like true authorization systems as in you send a question, you get an answer back. So can I do this? Yes, sir no? If you look at, say, Opa or Zackville or Alpha or Cedar or other systems, Serbos, also, there's many different systems you can choose They're all 95% the same, but they're 5% different. And so that's why we created AlphaZan. We really wanted to make sure that we also spoke the same language given that the question we're asking is a very simple, sensible question. And at the end answer that we're returning is essentially a yes or no, a bullion right? It's incredible that we can't even standardize that The other reason for doing all this then, of course, is that if you look at OWASP, if you look both at the top 10, but also the API, top 10, the number one issue on both lists is broken access control. I don't know if any one of you saw the article on GitHub and GitHub Access Control that came out a couple of days ago They don't talk about broken access, well, the IDOR is the term in, in OWASP. They call it something else for GitHub, but essentially if you delete something in GitHub, it's still accessible. So there's an access control issue"
  },
  {
    "startTime": "01:30:01",
    "text": "Anyway, all the Zen is under the open ID file Foundation. It's been around officially for the past two years and practiced for the past year it was sort of revived at Identiverse 2023. We within six months we had a draft of what a request would look like, a draft of what the response would look like and then we got 12 implementations to adhere to that request response format. And of those 12 implementations, we have a mix of policy-based approaches, we have a mix of graph-based approaches and then a mix of other approaches, proprietary approaches Al-Dazan has really three goals. Number one is to standardize the PEPP, so PEP policy enforcement point, PDP policy decision point approach. And also education teaching people about external authorization. This is also where there's a bit of work that we need to do between Oath, I think, and this call it the ABAC community, for lack of a better word and it also has a mission of creating design patterns For those of you who were at IIW in October, so not the last one, the one before that, even just justin and Alan and I ran a session called P-StarP versus ASR SmackDown. It was all about finding common pattern that we can use for authorization. Avoiding the abuse of O-O-O-O-O-F, where this is a personal opinion, by the way Sometimes I feel that O-O-O-O-O-T is being abused to do things it shouldn't be doing and using a back where it should be All right, let me okay, this is what a request looks like It's very simple. It's essentially just JSON You specify a subject, you specify your resource, an action, and then context data In plain old English, this would be, hey, can user Alice read account one, two, three, by the way it is 10 p.m. it's way past, sorry, 122 I don't know how to read time. A.m. I guess in this case"
  },
  {
    "startTime": "01:32:01",
    "text": "it's way past my bedtime. And the response that you get back, the same is 10 p.m. It's way past, oh, sorry, 122. I don't know how to read time. A.m. I guess in this case, it's way past my bedtime. And the response that you get back, the simplest form that you could get back is a decision true This is the anaerob that everyone went, so the 12 thunders and frameworks went against This is what the pattern enabled a decision true. This is the anaerob that everyone went, so the 12 thunders and frameworks went against. This is what the pattern in ABAC looks like, okay? Everyone knows this picture, I guess very familiar to everyone. The key components to remember is the PEP policy enforcement point, and that's the thing that protects your infrastructure, and the PDP is the brains It's what I guess you could call the authorization server or service It's a good thing that AVAC never called it that. We avoid you know, term collision with co-off and then of course this is the this is the RSAS pattern that although I have a slide there, I still don't fully understand. I have to, you know confess to that. I'm, although I've been doing O-O-O-O-F for the past three years, I still don't fully understand O-O-O-O-O-O-F Please, please don't shame me This is not recorded, is it? and it yeah you can put it in your CV And it's going on YouTube, too too What? What is real? That is recorded Oh, yeah, I know it's recorded. No, and it's going on YouTube Oh, yeah, I know that, too. You sent me the 119 video. There's way worse on YouTube Anyway, so before we get into the RAR profile, itself, just a note on the combined architecture some places where we could actually extend OAuth with ABAC is have an authorization server call out to PDP. Does there need to be an RFC for that? Maybe we could talk about it but it's not really the point of RAR, or I don't want to abuse RAR for that. And then of course you could call from a resource server to PDP That's outside the scope of Oath altogether So in RAR, when I read the RR, what I heard is that RAR lets you create a way to send a more fine-grained of authorization request or authorization details from the CLAR client to the authorization server. I think I missed"
  },
  {
    "startTime": "01:34:00",
    "text": "misunderstood the intent, though. Because when I read that initially I thought, ooh, it's a pep talking to PDP, which it's not. I understand that And I want to get all your feedback. yang justin, you already gave me a lot of feedback, so thanks for that which by the way in your blog post, you say, what are the key motivations behind RAR was admitting as a community that a lot of times a lot of times you need more than a set of scope values to properly describe access to an API. I misinterpreted that. I think it's because I'm biased. I think that deep down I knew that you meant something else, but I read it to mean, I need a pep in a PDP Joke aside, this is also, I think, a picture from your blog post or from one of the, one of the, deep down I knew that you meant something else, but I read it to mean I need a pep in a PDP. Joke aside, this is also, I think, a picture from your blog post or from one of the RAR resources, maybe Aaron, I don't remember But I get it that now essentially what the client is saying, okay, you user needs to transfer so much money, and I'm sending that to the authorization server, so the authorization server can actually give me an access token back But even then, is there room for an all-the-zen formatted request? in RAR? What I'm really trying to do is to bridge between OO and let's call it fine grant authorization or ABAC. This is not a picture I took I took it from unsplash So I guess I took it. This is in Vancouver though, in north Vancouver. Capilano. Yep, Capilano The Elfzan Request Response Profile for OA Oath, you can go look at the profile already. It's essentially to define the structure of the RAR request instead of having it roughly freeform JSON Today, it's actually an array of JSON objects. What I'm suggesting is that it still is an array, of course, but it's an array of ZEN requests instead of just plain JSON And because we do that, initially, and in the current draft, I mentioned that the type of the request should be all the zen. Now, Dave Highland in APEC or in Australia, pointed out that, the type actually means something else for, at least in FAPI and it opened banking, they use that to distinguish between different types of banking. So maybe using autosan as a value is not a good"
  },
  {
    "startTime": "01:36:00",
    "text": "thing. He's wrong? All right right So, yeah, I think I don't remember how many slides i had but i already got some feedbacks to the you of the type feel and i want to hear what you have what you guys in the room and online have to say. Also, the, the fact that RR is really about X scoping access delegation, I understand that, and obviously the Zen is more about direct access control. I still think the format is legitimate in RAR Ralph, I'm just sorry, I think ralph bragg, am I getting the last name right? Give you some feedback as well in terms of where it would fit or not fit And then one more thing though that's interesting One of the things in the RAR profile is in this standard RAR profile, not the other than one, is that you have this ability for the client to say, okay, the user wants to move money from A but, or I'm sorry, to B, you don't necessarily know where from, and then in the the authorization server has the ability to complete the authorization details and send that back to the client. That's something that I'll then will be able to do in the future we have this idea of of course, the yes, no authorization API, as I showed you a minute ago but also something that a tool calls search which is this idea that sometimes you don't want to get a yes, no authorization request, but rather you want to know, hey, tell me, which accounts I can list or which accounts I can take money from And that fits really nicely with the second and intent in others and RAR rather you want to know, hey, tell me which accounts I can list or which accounts I can take money from. And that fits really nicely with the second intent in others and rar. And sorry, oh, authorar, not not zan rar Anyhow, just other things because I'm new to iETF and fake fakingly newish to OOath, kind of, sort of I'm interested in seeing how OTH potentially can benefit from transaction tokens or the other way around transaction tokens can benefit from OTHZN and other areas of OOF, you know, selective disclosure is something I haven't looked at, but as Dijad, I was wondering, oh, well, if you do selective"
  },
  {
    "startTime": "01:38:00",
    "text": "disclosure, who decides on the disclosure and could there be a policy? there? You know, I'm trying to fit a policy everywhere in my mind And that's it for the for the presentation. A couple links here if you want to read up on what I use And that's it Okay, we have two people in the line here Darren. Hey, David, thanks for writing this. And sorry, I couldn't be there in person My comment isn't about Cedar, although I can answer that if anyone has Cedar questions I was more just reading the spec and just trying to make sure I got the right mental model. And I saw that you can pass in sort of AuthZN contents when required a token. Most of the RAR profiles then, or I think all of them, then includes some artifact in the token that comes back that represents the scoped down permissions for the continual use of that token. And I didn't see that in the spec and then as a result I was just sort of having trouble piecing together how someone would actually use this to solve certain problems, like what comes back in the token How is that evaluated with AuthZan and so on? yeah that's a good point um i gotta hit that that as well. Initially, when I wrote it, I had not fully understood RAR. I guess I made a mistake And so when I realized that the RRR response is actually essentially a list of things you have access to, not just a bullion, that got me thinking that well, instead of returning a non-desend response, you actually want to return the original autism request potentially enriched with the things you know that you're allowed to do. So very much in line with what an authorization details looks like today. Got it. Maybe just one last thing there, and I'll yield the floor. If that's true then I think there is a bit of slippery slope in that what would be in that response in autism format In effect, is a policy if it defines what's allowed and so it's i think there's still some questions about how to connect the dots then to the rest of ottsin i I don't see that as a policy. It is, it isn't so I think there's still some questions about how to connect the dots then to the rest of odds then. I don't see that as a policy. It is, it is in a way a policy. You're right. But I don't see that"
  },
  {
    "startTime": "01:40:00",
    "text": "as such i see more that as a fully uh fully filled out question. You know, the question is can Alice view document one? And you reply, Alice can view document one? As it turns out, the way to write this is the exact same way We can take that in the list. I think it's, I think there are some questions there but we can dig into that then. But good points, Darren. Thanks Justin. justin richer. And I would not prepared for that slide But I do think that this is interesting work As I say in the blog post that you referenced there you know, the primary use case of RAR for a lot of people is the client expressing to the AS in a more rich way the kinds of things that I want, but really under the hood, it is just a fancy set of scopes So all of the other places that scopes show up in OAuth RAR stuff absolutely makes sense to show up including this notion about what the token is good for, which might be a different view to the client and the RS So the stuff that's either in the token or an introspection response or something like that might actually be different from what you from what you're telling the client that the token is good for. Additionally, when we wrote introspection, wen lin intended for it to be allowed, to be extended with additional context and stuff like that to say, this is the token that I got, what do I do with it? The whole, one of the major intents there was to also say and here's all of the other information that I have about this, so is that token? good? Is it not? What is it? You tell me what to do And so it very much fits this type of you know, Zachomel protocol-ish kind of concept pretty well. Same feedback that I gave to the Cedar profile I think that this is good work. I think that there's a lot of syntax that needs to shake out. What I think is interesting about"
  },
  {
    "startTime": "01:42:00",
    "text": "this is that it took the opposite of approach of the Cedar profile instead of using it to pass policy language. It passes a data structure, whereas the Cedar profile is you know, here's the compilable language that you can run I think that there's probably going to be space for both. And so I think it's a question of if we do decide to go down this path, our we want to handle both cases? What's the trade option? there? I don't have the answers for that. I am not an expert in the policy language type of space All that said, I do think that RAR not only gives you a place to put that stuff, also gives you a bunch of sort of additional common properties that makes sense So like actions, locations, all of that other stuff would need to be interpreted in terms of that, would need to be defined in terms of that if you wanted to reuse that But I think that there's potentially some value there. Yeah, and Brian's shaking his head in all directions over here. And I'm not no, it's good because I honestly, I probably agree with all of Brian's head shaking there because I think that there are a lot of dragons down this path, but I do think that there is also some value here So I agree with what you said, and thank you for the feedback both today and prior days and also like what you said is interesting because maybe if there was to be an autism profile of all, it would be more about mapping what a raw request looks like into an all of the Zend request that might be then sent off to PDP, in which case that profile would sit somewhere else in the pipeline. It wouldn't be sitting in the client to AS or RS to AS. It would be like AS to some which case that profile would sit somewhere else in the pipeline. It wouldn't be sitting in the client to AS or RS to AS. It would be like AS to something else, kind of what I had in my diagram. So that's worth exploring as well because then we can leverage those additional properties Okay, Jeff. Yeah, Jeff, just for the record for the minutes R is full of open banking and financial example and all of that but now"
  },
  {
    "startTime": "01:44:00",
    "text": "smart on fire as declared R as a experimental function, so I'm super eager to see more profile for R due to that it was just a comment cool Thanks, yeah. At all Yeah, hi. If you go to slide 9, nine it sounds like nine what you're proposing is between, yeah, that one So it sounds like what you're proposing is like the authorization server or the resource server being the requester in this case and the PDP actually being the responder in your r request and respond. Is that no, no, no, no, this is a this is a diagram that is outside of RAR. This is just showing where Oath and all this end could work together This is not tied to RAR it's i guess my slide is misleading Okay. Yeah, I mean, then I'm sort of just a plus one to darren is i guess i just need to understand this a little more about how this is, you know, what the risk response is going to be like and, you know, how is it going to be used So, we can take that in the, in the, in the office david weekly calls a tool, but the use of the office-end requests in the rar profile would be between the client and the authorization server or the resource order to the authorization server, where RAR exists today This diagram was really, I'm sorry for the confusion I should have split the presentation into two, one just for RAR, and then one for, where does OAuth an authorization? where do they work together, what can they work together? Okay. Thanks, Hetzel as all. Gonna switch gears to the next one All right. Now, how would liu chang slide? Do you do something? No"
  },
  {
    "startTime": "01:46:00",
    "text": "he's asking I need authorization I gave him. The last one All right, this will be quick All right, so like I mentioned in the previous talk all I talk about is access control policies policy, policy, policy, and nothing else. So bear with me Okay, so I present this last Sunday, and given that I didn't know the IETF crowd, and I didn't know how identity focused or identity unfocused they would be, I did an introduction and what authentication is and the bottom line here, there's lots and lots of standards for authentication. And when I saw authentication I meet it in the broad sense. It's not just about who you are but it's also what you can be. So proving that you're 25 is a form of authentication, it doesn't really matter that you're Joe or Mario in my example. So lots of standards in that space. Oath is one of them. There's, of course, SAML, SCIM, many, many standards. You guys know this better than I do But beyond authentication, we also want to do authorization. And when I say authorization, I don't mean access del delegation, I mean actual proper authorization So like when you enter a country, for instance, um, border control folks will check that you're allowed to enter the country. So this is the case here. And what it means is that somewhere, there's a policy and I mean policy in the broad, neutral, non-technical sense, there's a policy that dictates what can or cannot happen. So there's a policy in Canada that says if you come from the US, you just show your passport and you're in If you're under 16, you can actually show your birth certificate and you're in So there's rules like that that are defined in the policy Okay. Generally speaking, when I talk about authorization, I think of Mac, but we also have to think of other models. There's two major models, right? Mac mandatory access control. If you all know this, just not in we can move on. And then DAC discretionary access control The work I've focused on for the past 15 years, is very much Mac, but in the past, say, five years with the"
  },
  {
    "startTime": "01:48:01",
    "text": "advent of Zanzibar and the event of frameworks and companies on top of Zanzibar, we're also talking more and more about DAC. And then below Mac and DAC, there's different ways of employment those. There's, grossly speaking, you know, policy-based systems. There's a graph-based system So the author of the diagram, Alex Babineau, is a graph nerd, I guess, I would say. If you think I talked to you much about policy, you should, you know, listen to him talk about graphs And there's also ACL-based approaches to authorization. And they're all they all have their pros and narcons, right? But then if you focus on authorization languages more specific you'll see there's a flurry of languages. There's alpha there's Rigo, there's Cedar, there's Polar there's IDQL that Phil, you've been involved with there's Ackmole, and there's possibly a minimum more languages I'm not even thinking of, there's vendor-specific languages too. So liu chang views. If I look at the lay of the land from a different angle, Alex's view is very much from like the types of authorization and you walk your way down My view was just writing down all the things that relate to authorization and trying to group them. There's vendors view is very much from the types of authorization and you walk your way down. My view was just writing down all the things that relate to authorization and trying to group them. There's vendor-specific stuff that exists as well. Microsoft has a language called SDDL or used to have a language called SDDL for file-based access control. Programming language like Ruby and Java and other programming languages have their own specific ways of doing authorization. So it's a mess And if you look at the timeline, of course, it goes back to the 70s within ACLs and then it moves all the way to 2023 with the introduction of Cedar and also some of the work that's happening within Oath, actually, like some of the new profile they try, where I feel that they try to get closer to find greenoff authorization which is great great And of course, when you feel like there's too many standards, what do you do? You create another standard Is that not what you do? So alpha, for those of you not familiar, with alpha, it's the abbreviated language for authorization You can do any kind of authorization model, R-back-A-backer"
  },
  {
    "startTime": "01:50:01",
    "text": "reback in a single language. It doesn't really matter as long as you have an attribute to model, you could express the model. It was initially drafted under Oasis-Zackville, and then nothing happened. It's data draft. For a lack of engagement, I want to say and maybe standard fatigue or whatever you want to call it Today, Oasis, Zackwell is only meet something about every month there's only four or five different members, so it's not as active as it is used to be I'm not myself a member. Maybe I should rejoin. But that's a different conversation. Alpha has all of the goodness of Zach Ball. Allegedly, now that the bad namely the XML notation is out the door I'm sorry, Eve A bunch of people use Alpha today the XML notation is out the door. I'm sorry, Eve. A bunch of people use Alpha, Alpha today from Axiomatic, Salesforce, Tiles, Huawei, Rocks Holland Knowledge, and then of course, customers of some of those companies as well well But the question is, now, why Alpha 2.0? Well, alpha is not really moving it's not really going anywhere and it's still has a lot of the legacy of Zach Wall. And what I want to do is essentially create a net new standard inspired by Zach Wall, inspired by Alpha, inspired by Open Policy Agent and Rigo, because there's good things in those languages, inspired by Cedar as well but make it a proper standard that belongs to a proper standards organization right? So that's why I'm the other reason is, of course, we're seeing more and more need for fine grain authorization if we look at past conferences like Gartner, like EIC, like Identiverse, we're hearing a lot of talk about authorization and the need to do that If we're looking at, you know, all this stuff around zero trust, I know it's a buzzword, but definitely zero trust calls for runtime real-time authorization. And if we have runtime, real-time authorization, we need something to back it up. Also, I want to do that reuse. I want to tell the other working groups here at IETF and in other organizations as well, hey guys, don't reinvent the authorization wheel. I was, I attended one of the sessions on Monday or Tuesday on constrained devices"
  },
  {
    "startTime": "01:52:01",
    "text": "and they have their own language. I don't remember the RFC number, but they have their own language for authorization what if we could bridge that to Alpha, or Alpha 2.0, because then we have one single language no matter the environment you're in And then one of the goals not wanting to pinpoint anyone specifically, but I would like to avoid cloud platform specific solutions not named names, and I want to simplify the existing standard Who's interested? We have a bunch of people that are interested right now in Alpha 2.0 Thanks Peter for reviewing the slide. The fact that people are interested even remotely doesn't mean they endorse Alpha 2.0. I want to be clear on that, right? But at least the three most active parties here are Huawei, X-Matic and Wachshel and knowledge were actively working on issues to fix in Alpha II. So the question, to this group, I think I already know the answer, because Richard, most active parties here are Huawei, X-matics, and Roxelian knowledge, were actively working on issues to fix in Alpha II. So the question to this group, I think I already know the answer because, Justin, not Richard, Justin, Justin rock-selling knowledge were actively working on issues to fix in alpha 2.0. So the question to this group, I think I already know the answer because Justin, not Richard, Justin and I, Rich, Justin and I had a chat yesterday about where it should live. So that's it, questions Okay. Any people? has any thoughts, questions? comments comments? Richard, it's going tom strickx justin richer, and I get called Richard quite a lot when people that can my last name wrong So just to repeat what I repeated to Dylan over here the other day It might make sense as a work group within the IATS That's not really for me to say It absolutely does not make sense inside of Oath This is a really very, very orthogonal kind of thing. It's something that if it widely existed it would work with a bunch of Oath tech. I do think that, you know, RAR"
  },
  {
    "startTime": "01:54:00",
    "text": "and introspection give some really promising hooks like I said during the last presentation for this kind of stuff. I think that there's a really to me, a really clear alignment I can also see this slotting into spaces like whimsy and spice, at least for the processing of spice credentials not necessarily work within Spice itself But I don't think any of those venues are the right place to do this. I will also repeat the caution that I had give Jim here The when you have an existing technology and you bring it to the IETF, it is going to get chopped up, it is going to get rearranged, it's going to get repainted, and it's going to be very, very different if you want to bring it through sort of the traditional working group process Right. So, yeah, keep keep that in mind when you are uh looking for an appropriate venue for this kind of work, the kind of work that you actually want to do. Thanks Jeff, I just want to be sure because I was not sure the node was made for AWAS and the CEDAR language with a very clear platform proprietary option but I just want to mention for the minutes and the record that CIDAR is not AWN technology is made to be used at the application layer wherever you are based even in your own basement, if you want, it is supported by a large number of other parties with are not AWS employee so maybe that it was for some other authorization language but not CEDA yeah and that's actually important Thanks. My name is Richard but you can call me Justin Hi, Justin. Actually, Richard hit all the points I was going to hit, so plus one, what he said"
  },
  {
    "startTime": "01:56:00",
    "text": "Darren said. Darren. Hey, full disclosure I do work on Cedar. I think it's, it's a tricky thing because it's you know it's sort of from one perspective, it could sound like we have so many programming and languages We have Go, we have Java, you know, rest. Why don't we just have? one standard? And it's, the end of the day if this new standard is yet another actually full full-fledged implementation of a programming language with SPAC and an evaluator, we should just come out and say like, we're building a new authorization language. And then the follow-up question, is, you know, that you'll see knock on as all those other vendors that provide solutions you know what stops them from piling on and calling themselves standard and in our that you'll see knock on as all those other vendors that provide solutions, you know, what stops them from piling on and calling themselves standard? And are we actually converging anything here or are we just saying, hey, let's build another one? which I think is probably what's going to happen. So I just wanted to share my perspective there. Thanks, Darren thanks there you on Yeah, I go back, John. Do you hear me? Yes. And yeah hi, David. Do you want us? share some light? What's the relation with the OZAN? Is it working parallel? Is it achieving something similar? And also, how it connects to all our yeah so with regards to OTHZEN, there are Yeah, so with regards to all the Zen, they're not orthogonal, but they're parallel, I guess, I'm going to say In Open ID, Allthe Zen, we specifically orthogonal, but they're parallel, I guess, I'm going to say. In all the zen, an open idea of Zen, we specifically said we would not deal with policy because we do not want to dictate what model you're going to be using for authorization behind the scenes. We just want to provide a request response interface, both a back yes-no request response interface or a search interface or a batch interface. But either way, we don't want to dictate whether you use a graph behind the scenes and access control list behind the scenes, a policy language behind the scenes The whole point of all the Zen is actually we within all the zen, we call it to have the o-off moment of authorization. Let me explain what I mean by that"
  },
  {
    "startTime": "01:58:00",
    "text": "You know, 10, 15 years, if you bought a SaaS, you would put up with the fact that the SaaS would force you to define using passwords within that SAS, right? When I say SaaS or COTS by the way. But then at some point, because perhaps first sample and then, of course, OAuth and Open ID connect matured so much, we were in a position to tell the SAS vendors and the cost vendors, I will not store passwords in your system. You will have to do federation or you will have to delegate authentication to my enterprise identity provider, right? That's what we call the OAuth moment in in, uh, and so with all the Zen, we want to help authorization get there. For authorization to get there, all the authorization implementations, be it graph or policy, they need to speak the same request response format because otherwise, why would a Salesforce switch out its authorization for a third party if they have to implement different interfaces depending on the model? OPA or Zach World or whatever else? So that's that's uh, of Zen. And all the Zen specifically says, we don't want to deal with the policy. It's, it's a big i mean alpha 2.0 right now the folks that I'm working with, um, yourself included, of course, um, have sort of a consensus because there's so much prior art, right? If we're successful in getting a working group within IETF, maybe it's going to open up a whole bigger kind of worms of people that want to introduce new features and so on so forth So that might actually delay standardization which is okay if we get to much better language. But of Zen did not want to be in the middle of that because we want to create interfaces now that work, right? And in authorization, those interfaces are pretty simple It's not as complex as OAuth for instance. So that was why Okay. Thank you one there. I think maybe a next step for you is probably go to all dispatch. Dispatch is a great that looks at a new work that doesn't look like it fits anywhere so and you we can talk offline because I chaired that group too So, okay. Thanks for the time"
  },
  {
    "startTime": "02:00:00",
    "text": "But, and thanks for coming to this community So my pleasure Okay. Yeah, that's it And that's it that concludes our three sessions And thank you all for joining for participation and hope to see you in Dublin. And thank you for the good process process process process Brogan make sense. Yeah. That's good good Oh wow, well done whenever you're flying back Thank you very much Yeah, it's all the case All right"
  }
]
