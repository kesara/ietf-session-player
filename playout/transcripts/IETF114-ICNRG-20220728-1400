[
  {
    "startTime": "00:00:07",
    "text": "okay so give give us one minute we are trying to um make the coin code shares delegate so that they can actually run the meeting later uh um um okay um if mary jose and jeffrey i think if you would kind of lock out and log in again um you should have delegate privileges like lisa yes and then that could make you could allow you to control the session i'll try thanks give that a shot so"
  },
  {
    "startTime": "00:02:26",
    "text": "[Laughter] so it didn't work let's wait for marie jose no no luck okay then we figure out another way okay okay let's start welcome everybody to this ietf 114 joint irtf session on icn and computing in the network so this is a joint meeting of icnrg and coinrg so we have some common topics some mind share in like distributed computing and networking so we thought it would be a good idea to try a joint session and um so that's what we're doing today so today um the first session will be on icn topics and the second half will be on on coin topics um i'm the culture my other remote coach is dave oran and we have lisha zhang in the room who kindly agreed to help us as a local co-chair today thanks a lot"
  },
  {
    "startTime": "00:04:03",
    "text": "yeah as all meetings this one is recorded so just a quick a few housekeeping announcements first of all if you are in the room please make sure you're wearing an n95 or at least ffp2 mask um also we are here as all other sessions are using the meat echo queue management so please sign in to the session using the meet echo client or lite client and make sure you have your audio video off and um yeah i think remote participants uh by now probably know how to use their headsets and microphones so please um do that and um okay let's let's skip this um so we are following the ietf ipr disclosure rules so um in essence that means you will expect it to let us know if you um here see or say anything that has ipr relevance in a short timeframe um okay this is obviously recorded um and um there's also a privacy and code of conduct please consult these links if you are not familiar with these rules and finally um yeah we like to remind you that um well we are here to do um research uh although that we are using the same same mechanisms for managing documents and some research groups are also publishing our seas um we are typically not we're not doing standards so this is"
  },
  {
    "startTime": "00:06:01",
    "text": "purely for research for enabling experimentation and so if you are unfamiliar with this check out rc7418 okay so this is i see energy please join our mailing list if you haven't done so um so we only agreed um on having jose as a notetaker for the first half and um me for the second half so that's all sorted and this is the um icn uh agenda for today so we're gonna hear about um and the update on ping and traceroute and the update on um the data time encoding spec and then nicos is going to present um research work on selective content disclosure disclosure using um zero knowledge proofs and yeah that's it for for now and let's start with the ping and traceroute update unless there's anything else you want to suggest for discussion today all right so i'll stop sharing if i can just bring up the slides if you want so if i can hear a song"
  },
  {
    "startTime": "00:08:06",
    "text": "so he may be experiencing some connectivity or audio let's issues him a few more seconds okay can you hear me yeah yeah oh finally okay so this will be really quick uh let's see i think i can drive slides right how do i drive the slides just a second [Music] i need to take control of the slides maybe you should just start tearing yourself and then you can control them wait i i can do it for a second i just i had to share it myself okay there we go now i think i can drive them right huh how do you drive them i've done this before i've got the slides shared so i i can't control them left and right arrow okay here we go okay so um these two specs have been around for quite a while they've been advancing slowly we got them through a research group last call a few months ago they went into irsg review um colin did a pre-review um and chris wood did the irsg review so thanks to both of them and during the review cycle um we actually we actually got the attention of uh some some folks who hadn't"
  },
  {
    "startTime": "00:10:00",
    "text": "participated earlier thank you very much and jude chao offered us some comments uh at the same time we went through the irs g review and we've even we've gotten a few more uh since then mostly relating to uh some technical problems with how we decided to do the field encoding in ndn packets since the spec supports both ccnx and ndn as underlying protocols so we the latest versions are the ones that were in review um and uh so we made one set of updates uh colin seems to think his comments were addressed uh during his initial review to the irsg um i already mentioned uh june chaos um comments about um mdn packet format and uh we got some really good sort of general comments from chris um and those have been responded to on the list um and we'll try to get those changes into the spec uh pretty quickly so uh there's one other area where uh we seem to have blown it on the packet encoding for ndn which is how the path steering uh capability which is not actually in that spec it's another spec gets placed in the traceroute packets so um we need to do a little bit of technical work there both on the ping and tracer outspecs uh to point to the path steering spec and then uh since the path steering spec is mine i'm going to do an update of that to get the ndn packet coding uh correct so moving forward we're going to update both of those drafts plus the path steering draft in the next few weeks uh and hopefully that will be able to move everything forward i'll make one quick note"
  },
  {
    "startTime": "00:12:01",
    "text": "uh that the past steering draft is still an individual draft and um dirk is going to talk probably at the end about uh whether we uh are ready to adopt that as an rg item so that we can move it forward in parallel the pan and pad steering the ping and trace route can go forward without path steering uh since it's useful in the absence of that but they work uh i hate to use the word synergistically but you know that's probably the right word uh together uh so having them all in the in the game here uh is probably a good idea so i'll take any questions folks have this is pretty straightforward going once going twice okay anything yeah you come up with please uh either contact spiros and me directly or post on the list preferably post on the list so thanks thanks dave um so actually i since you mentioned past during actually jumped ahead a bit um too fast let's just quickly look at our documents and so um here's a list of our current active documents so we just heard about ping and traceroute we are going to hear about the delta time encoding in a bit we have the ate 4g draft that i think is about to be published [Music] and we have ccn info which also has been um around in isg reviews and certain and and for some time so colin if you're listening do you know what's the latest state of this"
  },
  {
    "startTime": "00:14:11",
    "text": "sorry i'm trying to listen to too many meetings at once yeah sure um as far as i know we're still waiting for um elastic checked we're still waiting for responses to the ballot i think it's got two yeses but i think one of them is you and you're the chair so you would have to recuse i think he doesn't quite have enough positions so i mean i think we if i'm remembering right we need to i need to remind the irs and see if we can persuade someone to either change their no objection to a yes or something else to review it okay yeah okay thanks great problem here we just need to finish off the reviews although i think there are some comments on the list that they get reflected yet um i think yeah they have been reflected um i think this is should be ready to go and to be honest um i think uh probably good to remind people once and and get this yeah i will remind people again after this meeting uh i've reminded send a couple of reminders already but i will do yeah i know okay great thanks and um yeah so dave just mentioned um past steering um which is not kind of really required for ping and trace mode but it's like in the same mental model of using icn and just to remind everyone that there has been an ipr declaration um on past d-ring um so just to"
  },
  {
    "startTime": "00:16:03",
    "text": "i think we share this on the main list as well which just for completeness and last time we discussed adopting it as a research group document and we didn't really follow up on that so far um just checking if there are any uh opinions on that okay so then it's up to us to follow up on the main list um so maybe if you could note this uh on the meeting notes that would be great let me just uh mention what's what's the deal with this uh um ipr generation so um this is cisco ipr and cisco of course has the the standard itf uh you know mad type ipr disclosure which says you can use it just buy no royalties anything like that for itf documents now the problem is that cisco has no policy for irtf documents since they're not destined to be standards so um cisco did not put in an ipr declaration on path steering because they didn't know what to say so this is the third party declaration by me so just so people understand this is um in this sort of nether world where cisco doesn't know what to say about it since they don't have a policy for non-standards documents yeah thanks that's that's good to know okay so um with that let's move on and um so next on the on the agenda would be um thomas um who i think is in the room and i think i should bring up the slides all right um yes i'm in the room and it would be nice to if you drove my slides so this is um this is an update of the alternative delta time encoding for ccnx using"
  },
  {
    "startTime": "00:18:00",
    "text": "compact time formats uh this was recently adopted or after last ietf some sometime then as a research group document so this is actually the research group document version 0 i'm presenting which has addressed a couple let's say cosmetic things and also editorial aspects so i will give a brief recap next slide please so what is the objective we are looking at uh constrained iot networks here and in this context the the research group has produced the rfc 9139 which is icn lopen and that was one open question on how can we can we make the time and coding more efficient because in these constrained environments the bandwidth is low and the latency is high we have slow links lossy links so the more data you put on the link the more you lose and just the um i mean generally the field of power is that the the node processing capacities are larger and less battery energy consuming than using the links so we want to to pre-process or process on the nodes to save link bandwidth next slide please and for this we look at the um tlvs that represent time so this this is the way it uh it is encoded currently in the ccnx specs so we have relative times uh that is um that is an offset with a given in millisecond seconds with a variable length so it can be can be larger than one byte so there's a length field and then you can have a corresponding longer time"
  },
  {
    "startTime": "00:20:00",
    "text": "time value and then there are absolute times that that always have a length of eight and uh this is a ut utc time encoding an epoch timer encoding in the length of eight bytes so next slide please um so the the mechanism we want to introduce here is a compact time encoding that supports a dynamic range it is built in milliseconds um from an exponent in mantissa so that you have a very you are have a relatively high precision in the small time values and a relatively uh coarse-grained uh precision in the in the larger values so if you look at the formula you have the subnormal part where you actually start uh start with a i mean that the distinction between the two uh uh formulas is that the the the upper formula starts with a zero so you have basically you you divide the mantissa and then then you divide further uh for the value of an exponent value of zero if the exponent value is larger then you actually go into an exponential representation which is uh which is of course grained in larger values so next slide please so the existing time values are in interest and data messages so in in the interest lifetime you have a relative time in the hop by hop header and in the interest and the interest message itself has a signature time which is an absolute value and for the data you have an absolute value for the hop by hop header which is a recommended cache time and you have an once again a signature time which is also an absolute value so the idea is to convert the recommended cash time to relative times and to compress interest lifetime but but leave the leave the signature times intact so next slide please so what uh changes is um"
  },
  {
    "startTime": "00:22:03",
    "text": "in the k if you're given the the currently existing protocol in the case where it is encoded a length of one so that means a short length field then we put in the we want to put in the compact time offset and in the case the length value is larger than one in the encoding we use the regular i mean we just keep the regular timing intact to make to make the uh the protocol to plug into the existing into into the existing protocol encoding because i mean as we understand not all ccnx packets are on constrained devices so next slide please so this is the case for the protocol integration for the recommended cache time before it was an absolute value of length eight now we change uh to this duality if it's uh if it's encoded with a length one then we use a compact time offset of of uh one byte if it's uh if it continues to be length one length eight and it's the regular timing values on the next slide please uh uh sorry uh we already have the cache name sorry yeah i'm fine i was just i was confused um so what are the the diffs to the version zero five of the individual draft uh um so we we uh um updated the the integration of the interest lifetime and the recommended cash time to to uh to in into interpret the corresponding values we added equations to approximate the conversion and just to to estimate the error that actually occurs because you you cannot represent any value every value anymore there's an approximation formula which helps to to see what actually what what error you you're producing it's a simple"
  },
  {
    "startTime": "00:24:02",
    "text": "formula we also re arrange the references and the acknowledgement section and from our side this is a pretty pretty mature document but please feel free please uh feedback to get this document finalized okay questions um rick taylor industries uh just a sort of general question on this one how far does this deviate from ieee 754 formatting i noticed you've got no sign bit i'm assuming you don't support infinite values no it is um i've actually we had this discussion i mean there are different time formats existing there's this ieee format there's also a time format by the ietf that was uh introduced i guess in the context of of some routing protocols so my question is not about the time format it's about the floating point format so everyone's pretty well experienced with ieee 754 floating for floating point for 32-bit floats and 64-bit floats with silicon support i know there's an 8-bit float support used in in graphics quite heavily if this deviates too far away from that are we losing the ability to use silicon that already understands how to do this this work i know i i'm just interested in the in the diff i know you don't need negative times no and i know you don't need unrepresentable times you know uh division by zero infinity and so on i'm just wondering whether it whether the working group has considered the benefits of not including support for those weighed against supporting something that's pretty industry standard from a sort of silicon perspective as a sort of well-known way of dealing with floating point numbers i mean we had the uh i mean the discussion we had a longer discussion"
  },
  {
    "startTime": "00:26:00",
    "text": "about the the actual format and also about we were also discussing the ieee format um i'm not sure i recall everything uh correctly in the moment so maybe uh i mean if this is we should probably have a follow-up on the list if uh if we need to recap all this uh discussions it's actually something like two years ago or so it's it's great work done don't get me wrong i just think there's some new ones in here but thanks all right see no other nobody that's on the queue at the moment yeah thanks a lot thomas and chank and team for progressing this and also for everyone paying attention to details and providing comments so we hope to get this finished in the near future okay so let's move to our next agenda item and this would be nikos was his presentation and he wanted to run the slides himself can you hear me we can access granted okay so i'm sharing my screen now and [Music] i don't see the conference tool so if there is a comment or someone do you want to intervene please yeah let me know so uh i'm going to present you our ongoing work on uh selective conduct disclosure using zero knowledge proofs i am nikos fatihu from the athens university of economic and business and also in this meeting is george xilomenos"
  },
  {
    "startTime": "00:28:00",
    "text": "who is a part of the thing doing this stuff so a bit of a bulk round related to this talk the main building block of the solution that i'm going to present is a digital signature scheme called bbs which was initially proposed by pone and others in the paper included in the first bullet of this list and this signature scheme was initially designed to allow users to sign group of messages then many others extend this scheme to include zero knowledge proofs and one of the the latest and most important extension to this scheme is by governments and all and you can find it in the paper included in the second bullet of this list so earlier this week in this iedf meeting there was a head talk by tobias lucker about this signature scheme tobias and others are using this signature scheme and they are developing a solution in the context of identity foundation that applies this signature in a technology called verifiable credentials and using this technology they allow users to selectively disclose your claims about themselves and all these claims are included in this so-called verified credential so the purpose of the talk of tobias in this meeting was to propose the adoption of this signature scheme by iatf and in particular by"
  },
  {
    "startTime": "00:30:01",
    "text": "cfrg and if this happens it's going to be very exciting so many related works uh are investigating this signature scheme mostly in the context of digital credentials what we are doing in our lab is we are trying to apply this signature scheme for implementing selective disclosure of content so currently we are running two projects the first one is called second and the it is funded by nga atlantic this is in cooperation with the university of memphis and in this project we are trying uh we are exploring the application of this signature scheme in nbn and as a matter of fact we are we have scheduled some experiments to take place in the indian testbed the second project is called select share it is funded by ngidabc and there with two sms we are applying this signature scheme for providing a solution that allows a secure sharing of iot measurement data so you can find more information about this project as well as reports pointers to source code and the url included at the bottom left part of these slides so what is our goal with these projects we want to develop a solution that will allow casting of data on which we can make computations so we are not talking about bulk data but we are talking about data like relational database or data streams"
  },
  {
    "startTime": "00:32:03",
    "text": "generated by iot devices this data can even be the [Music] the log of operations in a distributed ledger or it can be the graph of a distributed social networks you name them so the applications are countless so we want to allow the storage of this data uh in third party storage providers we want to allow users uh to request portions of this data and at the same time we want to enable these storage providers to provide proofs of integrity but without sharing with them signature schemes signature keys so let me give you an example so this json object represents a data item that includes information about some measurements generated by an iot device and this file has been assigned by somebody for example the device owner and this file is stored as a whole in a storage node which can be something like into the web server to a cdn node or even to a cast and we want to enable users to perform queries like this runs so i want from this particular file to access the temperature measurement generated by this particular device and the reasons for making such queries are either this user recently is interested only in these two these two fields included in this file or even more importantly this user is"
  },
  {
    "startTime": "00:34:02",
    "text": "authorized to access only these two fields and we are working on a solution that allows firstly users to perform such a qrs and we do that in a way which is compatibly with ndn api and secondly we allow storage snows to respond to these queries in a way that allows users to verify the integrity and the correctness of the generated response so i will first introduce the signature scheme which is the main building block of our system so i guess you are all familiar with digital signatures traditional digital signatures allow a signer to sign a message and then a verifier can validate the digital signatures of this message using the public key of designer so group signatures are very similar the only difference is that the sire instead of signing a secret message it is capable of signing a a group of messages and then the verifier can in a similar way validate the digital signature of this group of messages so so far there is there are no significant difference but the group signatures have a very nice property they allow a third party which we call the prover that has access only to the this group of messages into the digital signature to hide some elements of this group of messages and at the same time provide"
  },
  {
    "startTime": "00:36:03",
    "text": "a proof that proves that the revealed items are correct so more formally this proof is a zero knowledge proof that proves that the prover knows a digital signature that covers both the revealed and the hidden messages and this and since this digital signature can only be generated by the signer this zero knowledge proof is a proof that the revealed messages have not been modified uh so uh works in this area are most are focused on two problems the pr the first problem is related how on how to transfer structure data in something that resembles to this group of messages so so that the bbs signature scheme can be applied so in our work we focus on data items and coded association objects and we are using a mechanism called canonicalization and this mechanism transports adjacent object into an array of messages so you can see in this example how this json object on the left has been flattened to this list of messages on the right so this is a very simple and straightforward example but for more let's say advanced objects like those that includes arrays they the economicalization approach is not straightforward and we are using a economicalization algorithm that we have created by ourselves"
  },
  {
    "startTime": "00:38:01",
    "text": "and it's a the security properties of this algorithm have been formally verified and they are included in the in the publication that i is located on the bottom of this slide but there are also other working groups working in similar colonicalization algorithms so there is a great space for research in this area so the second problem is related how on how do we request specific properties of a of a data object so how do we ask from a storage snow to give us to generate to generate a new item that includes specific properties and in our work we are using an approach called framing so a frame is just a json object that includes the keys in which we are interested so in this example frame we indicate that we want to extract from a stored item the device id the temperature and they created the key of the metadata field so if we apply this frame to our example json object we'll get a new json object with which is the same as the original one if we extract the fields that are not included in the json frame so we are working on a framing mechanism that allows us that allows us to make more advanced requests so for example"
  },
  {
    "startTime": "00:40:00",
    "text": "here we have a json object which includes an array called measurements and this array includes key value pairs and we can have a frame like this one so this frame in essence says that from the measurements array we want to learn all values whose id equals to temperature so if we apply this frame to this array we get something like that so again this is a an open area with a lot of uh space for research so let's see how do we apply all these mechanisms to ntn so we have a an entity we call it the data owner that generates this structure item so in this example this json object and this data owner signs this item and the stories in a storage snow which we call it the producer then the producer assigns an identifier for this item and advertise it in the indian network then any user can send a an interest message that includes the identifier of this advertised item moreover this interest message and the application parameters field includes the json frame that we want to apply in this item and by convention the identifier included the in the interest message is appended by the hash value of everything included in this app parameters field so"
  },
  {
    "startTime": "00:42:00",
    "text": "this interest will end up in the producer the producer will extract the json frame it will derive the new item you will perform the canonicalization algorithm it will calculate the zero knowledge proof and then it will send the output as a data packet back to the consumer now a very nice interesting property of this approach is that the data packet that the producer will send back it can be cast by an entity which may be as well oblivious about our zero knowledge proof mechanism and since all consumers interested in the same portion of the data item will calculate the same json frame they will send interests that includes the same coded identifier therefore the cast can respond with the cast item and the consumers can verify their provided jury knowledge proof just like they have this if they have received this item directly from the producer so casting here is not prevented so some performance results we have implemented an evaluated scenario we in this scenario we have a json object which includes 100 fields this field represents measurements from iot devices and in this graph we saw the time required to generate a zero knowledge proof as well as to verify zero knowledge proof as a function to the number of the revealed items so as it may observe generating the zero knowledge proof"
  },
  {
    "startTime": "00:44:00",
    "text": "the time required to generate the zero knowledge proof i is almost constant it is not affected by the number of revealed items whereas the more items we reveal the less time we need to verify zero knowledge proof but in any case the time required for these operations is less than eight milliseconds and i have to say here that for these measurements we are using an unoptimized the python implementation of the signature scheme uh in an ubuntu machine with a two cores and four gigabyte of thumb so it's a it's an ordinary machine and these times can be greatly improved so so far we have seen that this approach has some nice security properties and it has a low computational overhead but it turns out that it also has some nice properties when it comes to storage and to communication overhead so we compare our solution i guess an alternative in which its measurement in this file is individually signed using a eddca digital signature scheme so we assume that if we assign it's a record of this file individually and we achieve the same security properties which of course this is not the case but and we measure the computational and the storage overhead of course it's straightforward that having 100 different digital signatures requires more storage space as opposed to having a single digital signature"
  },
  {
    "startTime": "00:46:02",
    "text": "but also when it comes to the communication overhead the the the bandwidth required to transmit the the adca these are signatures of course is proportional to the number of the revealed items since we reveal one digital signature per item whereas when it comes to zero knowledge proof this is the opposite so the size of a zero knowledge proof is a 272 bytes plus 32 bytes for every revealed items so the more items we're revealing the less is the size of this zero knowledge proof so if we refill 32 items or more uh we need less bandwidth for the zero knowledge proof compared to an edc8 digital signature of course in both cases the size the the the computational overhead required for transmitting these digital signatures is small but in any case this may be important in some use cases so how can we move forward this approach is using a new key type and this a key is you is required by the bbs signature bbs signature algorithm and moreover it defines two new signature types once generated by the data owner and the other is the zero knowledge proof generated by the the prover so we can we can research how these in context can be integrated into ndn or in other related"
  },
  {
    "startTime": "00:48:00",
    "text": "proto architectures as i told you data framing economicalization is still an upper problem where many things can be done especially if we consider non-json recorded objects and this also um many similar activities are taking place in other working groups not only in idf but also in the body such as identity foundation and w3c so there is there are great opportunities for a collaboration there and and of course the sky is the limit when it comes to use cases for this signature scheme and for example i can imagine this scheme being integrated into rooting protocols where routers advertise only a portion of the network graph but many applications can be imagined especially in the context of a of the coined rd so that's all uh so as i told you we are working actively in this area please contact us if you want to learn more information we can provide the source code and many other helpful pointers so thank you thank you very much nicholas are there any questions for nikons so do you have a paper published on this already not yet published we have under submission okay great thanks"
  },
  {
    "startTime": "00:50:00",
    "text": "um so i was wondering about um this json object integration what you call framing um is this actually a crdt like operation so like an addition where this commutative and so it basically doesn't really matter which order you do these additions you can add or combine several objects in a row actually this is an open issue so it is mostly no it is only a read operation okay so but suppose that the you want to access an object and your response is cast somewhere and then somebody else wants to access a subset of the object that you requested so there it is a very nice problem if this intermediate that has cast the previous response how to generate a new object that can satisfy this uh the new request which is which in essence request no as a smaller portion of the of the original message and of course combining such responses for and creating a new object is also another very interesting open topic yeah i think so too great yeah thanks for bringing this work to us i think it fits really nice into the spirit of this session so icn and coin rg i see no other questions um oh i think let's"
  },
  {
    "startTime": "00:52:00",
    "text": "follow up on the main list if people have questions and looking forward to the paper thank you thanks nicholas um so just quickly i realized that some people seem to be having connectivity issues or audio video issues um which cannot explain directly so it works perfectly for me um but could be a good idea to just share some new experience on in the chat so just to maybe figure out what this is um so i'm connecting from germany i don't have any issues today but i heard from others then it doesn't really work so well okay let me just bring up um our slides again um okay so just a few things that we shouldn't forget about um so we really wanted to get um the flick specification um finished and published um so it looks like it's currently um stored again um so yeah dave and i have been trying to um motivate authors and the group to kind of see how we can get this to last call um we we think it's a it's a really important specification and it should really be published um so um if you have any idea how to to move this forward please let us know or please free also feel free to suggest text or anything if you think there's something missing or and so on and um yeah then of course we don't have that much time today but um so potentially there could be many"
  },
  {
    "startTime": "00:54:00",
    "text": "interesting work items uh to uh discuss here and here just a few examples that we came up with so you're probably aware that there is a media over a quick discussion in the ietf so a buff is uh this week and so this is essentially a proposal to do something like um yeah named data networking you can say um over quick or like an overlay network of quick relays you can say um of course this could be done much better uh maybe that's an interesting topic for this community um as well um i kicked off a discussion on self-learning auto configuration and also potentially nd and switch design on the main list and so there is some interesting work that has happened in ndn cxx um as a code base um i still think this could be optimized and maybe um that's an interesting topic if people are interested to discuss further then um so uh we we often you know um explain icn is a good way to to access arbitrary content in network which is true however if you think about doing something equivalent like um web protocol so like http 3 for example and um there are a few other things you have to um care about so um things like name privacy for example um maybe um setting up something like a tls security context and so on"
  },
  {
    "startTime": "00:56:01",
    "text": "and maybe other things so we think that that's actually an interesting topic and maybe something to work on so yeah we just saw an example for um say icn security work um that um kind of is connected to disability computing and coin and we have talked about others before um so we would also encourage you to um maybe if you do if you're looking in this field um please share your ideas um in this group and i see alicia on thecube you can hear me i think you have to move a bit closer to the microphone i look at the your slides searching for ideas for new work i thought maybe a different way to ask the question is what are the important problems that really need the solutions and start the look around in that direction so like uh the kind to suggest that the list is give me a impression sorry if i'm saying the bad words again this is like you really look for work to do on the other hand research group really should addressing the burning challenges not trying to figure out what to do next but rather among other problems are the important ones and still manageable to make progress"
  },
  {
    "startTime": "00:58:02",
    "text": "yeah point point taken um okay i didn't explain this um well enough and of course some of these points um are actually motivated by actual problems so for just for example um like the like this first one meteor over quick or icn i mean they're clearly um is a problem in uh distributing um real-time multimedia content um over the internet and um so these media over quick ideas um directly stem from these um from this change in the like cdn environment and so you could you could say that that this is the problem and i see um solution proposals right now that are say leave room for optimization so if i speak again i think if you look at the issue from that direction that could give people much better ideas if we just look at to say media over icn instead of quick it wasn't clear as opposed to many people why you do that even quicker it's really they transfer the protocol that is catching all the attention if you state the stand in a different way to something like media over quick does not address the following problems and which could be addressed simply by over icn that will really get people moving to look into why there are issues with quick and how action can solve the problem i would also i take the floor now"
  },
  {
    "startTime": "01:00:02",
    "text": "i had some private exchanges i guess you know you people remember with you cheers also with the collins i think ic is a fundamental secret sauce is named and they secure the data i am a bit afraid that the names are not really fully utilized as a semantic power and the security seems to be i don't know ignored forgotten or otherwise people don't know how to do it so they just escape it i worry that without showing people how i think help address the security challenges facing the internet we we are really losing the real yeah thanks no i i fully agree so that like just quickly um this this first item here is not intended as just like a protocol drop-in replacement or something is more likely referring to the like bigger picture um the architectural problems um but i yeah we don't have time today to to discuss this um so there's like yeah there's i think there's a lot to um you know unwrap here but um we just wanted to like um inject a few ideas for for maybe deeper discussions or like actual work um later in this group but i i agree about the security topic and that's definitely so also when you think about web over icn of course one of the really pressing issues okay thanks um so we are done with the icn part but there's more to come on computing in the"
  },
  {
    "startTime": "01:02:00",
    "text": "network um so uh yeah we we have this first bullet item for a couple of meetings now um and we are really hoping things will clear up um but yeah no promises at this point um just quickly uh please mark september 19th to 21st in your calendar and this is where the icn conference um happens um in osaka japan um this year okay thank you very much um now let's just directly continue with uh with coin rg and i would like to invite the coin rg chance to take over now hello see i guess because i'm not a delegate i have to request i don't have to look fast to share slides yeah probably i think that's the best way yep okay okay so strange being on this side of the presentations okay uh hello everybody we're thrilled to have part of this session uh i'm eve schuler and my co-chairs are jeffrey hey and mauricio is a mopati and uh who are also here um but here is"
  },
  {
    "startTime": "01:04:01",
    "text": "that we are all remote some of us had hoped to be there in person and were disappointed due to cohort that and other airborne diseases that we are not there we hope you all are staying safe i don't believe we have to go through any of these slides because that was um [Music] already stated at the outset of the icnrg um but we have three very interesting presentations today um and uh we'll begin with uh dirk who will be talking to us about traffic steering at layer 3. um andy uh who will discuss name spaces security network addressing and uh tushar swamy uh building adaptive networks with machine learning and and then we will discuss the need for an interim uh there are several things that we've had on our to-do list for quite some time namely a more appointed scoping discussion really synthesizing many of the conversations discussions and debates we've been having on the mailing list for for many months uh and um really to take a a step back and uh try to scope as our charter states you know re-scope or scope more articulately or deliberately and the other task that we would like to accomplish in an interim possibly a separate interim is to revisit the many drafts that have been written and how they fall into the architectural space"
  },
  {
    "startTime": "01:06:01",
    "text": "so with that let's see i think marie jose wanted to oh well you are here so it's kind of funny you know if if you're not here this is how you get to meet echo you won't get to see that but nonetheless um because this is a shared meeting it's a little funny how meat echo tracks or doesn't track the history of this it will appear in the icn meeting minutes and meeting under the data tracker the meeting directory but not in the coin rg so that i think is an oversight um and i for meet echo and hopefully we will submit that as something we'd like to see changed our minutes are being tracked in the same space as where icn is which is great and thanks to people to all the people who've been contributing there marie jose i think you had said you wanted to talk about our document status okay yeah i just wanted to say that we have two rg documents that we would like to move forward one is expired um and uh the other one uh i think needs updates we have a ton of other documents and that as um eve said uh it's there's a lot of them that maybe they're they're still good maybe that some of them should not um be um um maybe would you know that they just expired and we don't want to continue anything but it would be good for the authors to um um tell us what they intend to do and again i think this is going to be very much what we want to do uh at the interim where we want to take some more time uh we also want to re-look at some of the um"
  },
  {
    "startTime": "01:08:01",
    "text": "the charter uh i don't think we need to be recharted but there's uh goals that we had that we need to uh to look into and we will do that again uh in a september time frame um yeah we've been hit pretty much by the covid and um i think we're all a bit under the weather um and uh yeah so i think without taking more time because we're already uh our terminates late uh dirk trossen uh will present uh some work uh for actually raider routing and addressing and i would like to mention that tushar who's going to have our last presentation but not least was also the applied networking research prize winner and um since that the last the last session uh dirk said that he was looking forward to start having so maybe some work in uh you know and and machine learning intelligence networking that type of stuff and this is exactly what shaw is going to present and of course the name space is always a common a common thing between icn and us and so i think uh those three presentations will be very um complementary to what was presented before uh so dirk uh please uh we're waiting for your presence take the floor yes slide sharing um see okay what am i for some reason i'm not able to un ah right you have to stop sharing first eve yes i'm trying but my my machine is giving me beeps it's not um let me just do that for you okay perfect thank you and uh can you see um share preloaded slides in the top or is that just for sure for chairs no i don't i can see that i erase the"
  },
  {
    "startTime": "01:10:02",
    "text": "slide request oh okay yeah right um i think it has to go through you now yeah let me just bring your slides up and then i can drive them if you like thank you yeah yes as as i mentioned this is some work that's related to um what you could call advanced um packet forwarding if you will more routing this is the joint work um with my uh colleagues karima i mean zoro and artur and also with george khaled two hence the two logos at the bottom they can see if you can go to the next slide please oh do i um so the problem of random scheduling that we that we looked at and there is an accompanying draft um wasn't the list that were usually um before so scheduling and then and joined network compute optimization has been talked about in uh contributions to coin before so the environment that we're talking about here is execution of services in the distributed service environment then particular virtualization drives the distribution of a service implementation in one or more servers instances right um they are available in one or possibly more network locations um and and and you have to make a choice which of the instances you would like to like to use for your computation that's essentially the runtime scheduling problem the additional problem comes in through when we capture this to the notion of service transaction it it may require an affinity to service instance after you made an initial decision because of ephemeral state that has been created so this type of affinity needs to be observed otherwise you're putting additional requirements on the application to move state which you can do through shared data layers but not all of the application frameworks necessarily work with shared data layers so the problem that we that we outlined is to find the best service instance to serve the client transaction runtime um"
  },
  {
    "startTime": "01:12:00",
    "text": "while we also preserve the um at the the the the affinity after machine has been made and and the solution is called computer where distributed scheduling you can see the two key aspects the one is it is computer where so best that we put in a in in quotes there is is an awareness of the compute capability of the service instance and it is distributed scheduling it does not um go wire in in inflection point it's it's done at the inquest to the network okay yeah there is no thank you very much i can see them a little bit faded on on some of the browsers funny enough that was great sorry so we're basing the the the the cards idea on a system um that routes service requests based on service identifiers that's that's also there's a certain proximity there to icn if you will uh in in the sense that the um so we have distributed geographically distribute sites over the service instances they are shown in red on the more right hand side of the picture client issue service request which is destined to service identifier and the incoming semantic router which are the the uh the red uh boarded one sr one two and three for each of the clients forward the service request towards a suitable destination which is one of the possibly many substances the five that you can see here on the slide that's three different locations so you have three different locations um but five different instances it performs an on path forwarding decision and that's the the key part um that we proposed and that's compared to an existing dns plus ip of pass decision that you could do as well the affinity is insured in the system by using iplocator for the subsequent request so the service request is a special request to serve identifier it makes its way to let's say this instance"
  },
  {
    "startTime": "01:14:02",
    "text": "and then the subsequent requests for the transaction are using the ip locator of that instance to root the request directly to the instance so only the very for the very first request the decision is being made what is computer where the swivel is getting now one of the things we wanted to achieve is that we that we wouldn't need to signal permanently a lot of work on computer wear uh forwarding decisions that are also cited in the paper um but but we wanted to avoid very very frequent signaling so we attached to computer awareness to something you can derive from the deployment of a server so each service instance is assigned a normalized compute unit this could be depending on your orchestration framework something like how many cores am i assigning to the service instance when i deploy this how many threads how many containers the different ways and examples are given in the paper how you could represent the complete unit all the convenience are flattened and joined in an identify specific routing identifier interval you can see this on the right hand side um so that in in this case it means you know you have exactly one compute in it for the first one you have two for the second one again one for this one uh four this is a rather big server at the bottom and two again for the last instance and it's then distributed to all routers correctly the semantic router so not the routers the the standard ipv6 which is only the bridges at the inquest that are shown here the scheduling now that you implement is a distributed round robin so you essentially run through this interval in a round-robin fashion you have an incoming request at the and each of the identifiers uh exist at each of the increases so each inquest is independent from the others a decision of sending the first request to the first one the next two requests to the second one the the fourth one to this one etcetera until it it wraps around"
  },
  {
    "startTime": "01:16:01",
    "text": "when it reaches the last one that's what's being implemented it's it can be implemented like a link speed we have a separate paper we published last year um where we did a very similar mechanism uh in t4 and showed what are the issues before in doing that uh successfully so we implemented this in in in assimilation and we also have now um by the way a a real implementation ebpf but at the time when we published a paper simulation and these results see our simulation we went by simulator um we used five sites for servers service instant um per server you can also have multi-host but uh we didn't do this in the simulation um the compute units are signed at the start so really emulating a deployment of these compute units and and the instances run for a certain amount of time until you may redeploy or reorchestrate uh your your your service setup you have five english semantic widows as well uh and the sales requests only go to one service function uh and there are single packet requests uh um with which they're sent the main metric that we're interested in is the request completion time so how can we improve the actual um latency at the request level of these service requests so the scenario one we we looked at was generally in scenario one um and there's one a and one b and then b i would skip over in the interest of time um we had different design aspects so so what's the impact of the centralization versus the distribution um so we distributed the scheduling over a growing number of of inquest points and even in the end into the clients themselves clients could be seen as an increase point very very much um close to the actual application versus an idealized center central scheduling so idealized means we neglected the actual path latency to"
  },
  {
    "startTime": "01:18:01",
    "text": "move to a central point and only centralize the actual logic so not necessarily the actual latency obviously we'll have different latency there and and and the observation we got from the civilization is there's a natural effect of the distribution of the on the mean rcts which means the distribution itself the fact that these schedulers run independent from each other is not particularly large when you start growing this to to very many increase points when you see an increase in rcts when the uh when the system load approaches 100 but generally uh the the impact uh um is relatively small so that was one of the aspects so and again this obviously does not take into account the latency through running wire centralized point if you take that into account the numbers would get better again for the distributed scheduling where you do not have this additional latency i skipped this one this was actually in the pptx that i sent in the powerpoint it was actually hidden you can look at it separately if you download the slides we then compare it with other network level solutions so we wanted to compare cards performance against other distributed scheduling mechanisms um that in in the sense of both factoring compute capabilities in the scheduling decision so they needed to be computer aware um they uh to perform scheduling at the inquest versus at sites so we wanted to compare this design aspect of cards and we also wanted to uh evaluate the impact of distributing compute units across sites and with insights so if there are imbalances of distributions compute units what's the impact of that we use two um schedulers one is a random scheduler it's it's position of the increase a note it is not compute away at all it performs random load balancing it by selecting an instance uniformly at a random and then just sends it to the actual network location the second one um is called steam that's an an infocomm uh from instagram paper in 2020 so it was one year before we started our work one of the courses"
  },
  {
    "startTime": "01:20:00",
    "text": "ramen is also on this paper this is positioned at the site increases not that the client increases at the site interest and they for the the actual network interest notes forward the request to the sites uniformly at random but then the uh is therefore compute unaware at the network interest but at the site increase it is using node estimation to find the right uh compute instance within the site so it is kind of like a mixture of compute unaware in the network but computer wear at the side the what we found in the comparison not surprising is the card significantly reduces the the the rct in particular in high load settings um because it is uh uh is taking into account even the distribution to the sites the compute units of the individual instances which steam doesn't do steam on the other hand has issues when the the system load goes significantly up above eighty percent as you can see in the right hand side it jumps quite significantly even above the renmin scheduler uh what we then looked was the imbalance of the computing distribution so we we created imbalances across sites the normal configuration was roughly the same compute unit um number per each of the sites and then within the sites roughly the service instances were you know almost the same distributed and we changed this now um in uh making one side very very big and the other one's relatively weak in compute units uh and equally uh we had in the second sub-scenario we uh create the same imbalance within the site so one of the service instances was very very big compared to the the other um service instances and what we could see is that the steam handles the the contention uh uh um quite well in the in when the"
  },
  {
    "startTime": "01:22:00",
    "text": "when there's an imbalance within the side because it uses load uh balancing with inner side hence it can obviously deal with the imbalance but it cannot deal with the imbalance across sites because it's computer unaware cards is performs well across all of the scenarios because it is generally computer where even across sides and hence it outperforms all of the other ones the random schedule is particularly bad in the in the in all across the scenarios because it is simply unaware of the compute capabilities if you have imbalances it shows that quite badly we also use the use case driven analysis and here we set the simulation parameters um this is kind of like a media scenario since they mentioned media before in the ic energy as a potential work as well where um where we have individual service requests for instance for content retrieval um or you know which could be either video or it could be software uploads where i would like to have something i'm getting a larger chunk back so it's kind of like a rather icy energy scenario if you will right and we compare this to existing long lift approaches what we mean with long lift approaches is an approach where a decision which server to be chosen is relatively long and this can either be a and we chose to it's a one minute transaction so after one minute we essentially recalibrate um that would mean we issue after flush in the dns cache we are issuing another dns request hopefully getting another choice now and we used random as the the comparison there we also compared um random choice at packet level so where you would make a change even at the packet level and what we can see is is the the performance of cards um even up to very very high the vertical line here is 100 system load even when the system not approaches 100 percent um and what's more important because in the use case to written analysis we set the latency at the end arrival time at"
  },
  {
    "startTime": "01:24:00",
    "text": "two seconds i said well what about if you if you have a um an up about legacy of 1.5 seconds so meaning i still have enough time to receive the packet and do whatever decoding i need to do but one and a half seconds should really be the upper bound latency and what is the number of clients where this upper bound latency is is is is being exceeded and we can see that cards in comparison to the other mechanism significantly this is significantly more client so if you draw in our horizontal line which would be very much at the bottom of this graph you can see that cards serves 24 000 more clients than uh than uh steam and 162 percent more even more clients for the uh for the long-lived affinity scheduling oh sorry for the packet level scheduling versus the long-lived dfinity scheduling so the the the packet level computer variant decision a little bit expected has a significantly higher performance what's the conclusion well the conclusion is that we we we try to show with this work that we can integrate computer awareness into the steering decision of at the data plane level these are two pieces of work at a system that we outlined in the paper but also the accompanying p4 work that we've done before to show that you could implement this at the data pineapple through programming frameworks like p4 the computer awareness here is a relatively static computer awareness so it doesn't require frequent load signaling it just says well do you are you going to put a big server versus a small server somewhere that's already enough to make fairly good decisions and it doesn't have much signaling overhead that's the good thing significant performance improvements and the simulation we've seen that um including also to serve more clients so where do we want to push this work this was as i mentioned ethic networking paper um and my apologies for the mix-up was getting the proceedings um i accidentally disclosed the author link as i was told later and the website had"
  },
  {
    "startTime": "01:26:00",
    "text": "to be shut down so and this to the coin list it was only the link was only live for a couple of minutes um the proceedings should be available now even though i don't have the link handy at the moment what this did it is it is a horizontal comparison it it compares cards with two other mechanisms at layer 3. what we've done as follow-up work already is to answer the question what about if you use cards so there's no particular reason why you use cards at l3 right you could use cards at l7 as well right and that's a vertical comparison and we did this um at the moment um uh in a new paper that we published in the upcoming uh firearm workshop where also the namespace paper that you will hear next is going to be presented which compares will be then positioned as an off-pass traffic steering at l3 against a sorry on-pass traffic steering at l3 against an off-pass indirection based resolution at l7 and it compares so that allows us to somewhat compare the usage of the same mechanism computer by mechanism but with different systems and tests and you can find those results in that um paper i think the proceedings will be live at some point the workshop is going to be in in in in august um third week of august that was it thank you very much any questions reactions i'm dirk yes um so i i don't want to start a rightful discussion here but um why call this semantic routing um so why why what do the forwarders have to know about semantics isn't this just names that they need to know and then make the following decisions based on on that knowledge yeah so so the semantic here is the service identifier um we actually changed the name in the apk in the fire paper so you won't find some anecdotes"
  },
  {
    "startTime": "01:28:01",
    "text": "anymore this paper i think was written initially actually for infocom last year when we called it semantic reader and we called it after a service with it because it's a service identifier so it's a bit more descriptive as to what the semantic is for the same reasons you weren't the first one that actually asked for you or what if it is a specific semantic while you just put the semantic into the name which into the description which we did i just use semantic reader because that's what's written in the paper okay thank you uh jeffrey um yeah there's this is marie there's a question also uh from the uh the chat uh dirk which is from oh sorry i haven't i didn't get i didn't go there sorry so uh what is the advantage of doing this at l3 versus seven and we will put the um the paper that's sitting that is um cited there in in the in the minutes so maybe you can answer to to cat to ken yeah so so i mean this obviously goes after you know at the the paper that i didn't talk about just the one that comes in fiverr um we looked at this and and of course the first thing you have you have the initial uh uh resolution latency which is you know you can quantify we do this actually in the paper what's the the typical initial resolution latency in dns was optimization in dns has significantly gone down but the other one that we outlined in the paper is actually not necessarily an improvement of the average latency but the variance of the latency and this is quite clear if you think about you know a problem when you instead of distributing um a smaller number of clients to a fixed server you now have the option to pick among a larger number of clients more servers you actually so you haven't you have an uh mm-1 system with n divided by k clients versus n clients in an mmk system queuing theory will already tell"
  },
  {
    "startTime": "01:30:00",
    "text": "you average latency is about the same it's the variance that is impacted the variance significantly is reduced so if you have use cases like which we have in the firewall paper like ar vr actually the reduction of the latency variants is a very very good thing right what you also have is a scenario that we have in in the file workshop where about resilience um you you you share the damage so we're overloading one of the servers um if you if you have been attached to the server in a longer lift affinity obviously you're being affected immediately and you will be affected for the duration of the outage while in the in in the lc mechanism uh and the scheduling happening um across the service allows you to distribute the impact of that it's not a fail server we actually reduced we increased the latency from that server um is distributed across all of the clients and and hence you get overall better performance still good enough performance for everybody while nobody's really negatively affected so these are some of the takeaways we have in this fire paper but thanks for the reference for you certainly at this one as well look are you still in queue or you're again enqueue i i just wanted to make sure that there was uh who else is in the queue yes dirk i i would also say that in the interest of time this should be the last question or maybe we even take this question to the list uh so that we have time for the next couple of talks yeah i'm happy to do that and yes colin thank you thank you for finding the the the paper calling to put the paper into the chat when i checked the last time they weren't online yet thank you for pasting it okay i think andy uh"
  },
  {
    "startTime": "01:32:01",
    "text": "to you thank you that was easy uh dirk you're running you're driving the slides great i passed control to andy so he should be able to do it now okay this is all fairly new to me oh and it's come up on the wrong camera as well so uh you're getting a side view i won't try and fix that uh yeah this is uh this is some uh put together uh fairly recently i don't maybe go on to the next slide um i think you can do it yourself with your cursor keys ah uh yes uh yeah this is uh obviously it's like a preview we've uh had a paper accepted for the same sitcom fire workshop uh that uh crosston just uh mentioned uh so this is like a a preview of what's in that paper without necessarily getting into all its detail uh it's also and we've got the that's the the reference for the workshop we think the agenda hasn't actually been fully published yet but presumably will be very shortly uh the this is also come out of work within a european uh collaborative project under celtic next uh between uk and germany uh so there's there's one of the nice things about this is we've had a pretty broad range of uh input including from automotive and video surveillance partners which all come to uh in a couple of slides uh with background is the question that we were asking ourselves uh"
  },
  {
    "startTime": "01:34:00",
    "text": "was looking at some of the applications we've got uh the general go-to solution that exists at the moment is microservices based architecture normally based on on very one or other form of container modularization uh and hosting and arrangements uh and there's an awful lot of traction of that in in the industry at the moment and we started by noting that there's a lot of good reasons why the modularity associated with containers seems to be a good thing uh covering quite a wide range of things so starting even from the code development uh if you're following the agile agenda and looking for a modularity ability to refactor and so on uh it provides a very good way a good unit of modularity in your application development is the basic uh point of abstraction service abstraction by which the point of which you don't see inside to the implementation again which goes alongside some of the ability to to refactor without impacting the wider system uh it provides a heterogeneity between development uh or different runtime environments different language environments and so on it's also a point of integration and module test and end-to-end system tests in ci cd pipelines it is our basic unit of distribution and i think from our point of view here it's also the primary point of"
  },
  {
    "startTime": "01:36:01",
    "text": "interaction between the application and networking so that's the way we'd encountered it uh at the moment and certainly what we were taking is there's a lot of momentum to say this is a jolly good thing all these things come together at one point but it does still ask the question does one size really work for for all and so we were identifying we'd be identifying uh a number of issues where it's not immediately clear that this is the best long-term answer or it can't be improved uh so at the moment a lot of the distribution is focused are around distribution within a data center for some of these other aspects uh not necessarily distribution as in physical distribution and when we think about physical distribution there are some more demanding constraints uh that don't present a great uh more concern more complexity in working out the distribution of the management of the distribution uh the scale of the service abstraction the point of service abstraction is largely fixed so once you've decided your web services for example your web services interface it's hard to go back and then break it open into sub components that are within it and say i want to distribute a part that's within a container module take that out and distribute it somewhere else which they also sets up a a trade-off that you have to decide fairly early on in the"
  },
  {
    "startTime": "01:38:00",
    "text": "in your in your application development if you choose small modules that gives you a lot of flexibility particularly in the way you might choose to implement the distribution but you've now created a whole lot a large number of networking interfaces between these small modules which create a performance overhead if you select large modules you can greatly improve the efficiency of the communication by internalizing a lot of the communication within a module but now it's inflexible and it's much harder to break open bits in the future and then coupled with that some of the solutions that are out there at the moment side cars and proxies for some of the compute environments we've been looking at would be very heavy weight indeed uh so some of the the background to the way we've been looking at this uh the couple of use cases that have been really interesting certainly i found them uh cause cause a lot of good thinking in understanding what drives the potential for physical distribution there's been an awful lot of discussion about edge compute one that we've been looking at distributed video processing does appear to really produce a use case where out of the edge processing of upstream video does make a lot of economic sense if you were so if you've got a video application where you're wanting to extract features from the video in real time if you try and do that to the camera then that's quite heavyweight and costly uh addition to the camera uh"
  },
  {
    "startTime": "01:40:00",
    "text": "it also makes upgrading or changing the any algorithms you're using much more complex because you've got to get it right up to the uh to the camera and there are all sorts of security uh issues associated with trying to upgrade the software on it uh if you bring all the information up to the cloud well firstly you require the full video bandwidth all the way up to the cloud he's also got the uh potential issue that what you've uploaded is the full raw video stream with all sorts of other information which you don't necessarily want exposed and cross-correlated in a way that you never intended for the particular application so the edge doing the video processing at the edge would appear to give you the best of both the other one completely in to many ways completely different is looking at the automated production filter facilities in a smart factory uh where the sort of things that we understand are developing there is going for a greater scale of production automation uh moving uh where you've got and changing from an environment where you've got a lot of legacy uh or existing uh interfaces for sensors and actuators we do have a a an architecture that's a lot more modular where the compute facility may be in very small uh uh boards where raspberry pi may even appear fairly heavy weight that is connected directly to the the actuators and uh and sensors but is also doing a lot of the compute uh in a distributed way and when you start looking at this it looks like architecturally a very similar sort of"
  },
  {
    "startTime": "01:42:01",
    "text": "architecture to some of the applications we've been looking at that are much more uh large-scale wand type distribution so that seemed to us that there's a growing convergence between these what a very at the moment very specialist uh very small network environments and production facilities are becoming to look much more like the sorts of distributed applications that we might see in a wider network i think uh coupled with that is one of the things that drives this is that as the production gets more complicated the time to reprogram is a big concern and the modularization of the compute potentially can help that very considerably so the sort of basic architecture being looking at is represented here and i won't particularly go into the detail key points are that at the moment there is almost complete isolation between application namespaces and network addressing and we end up with these fairly heavyweight uh adjunct devices that amongst other things are essentially mapping the application namespace to network addressing so the sidecars and the proxy load balancers uh and so on uh the so what's what we're looking at and what uh and what we've been and what paper comes to is what ways can we improve the way the network uh works that can give a much greater uh visibility and connection between the name spaces of the application and network addressing uh"
  },
  {
    "startTime": "01:44:01",
    "text": "and so three things that we've been they've been looking at and the three things that the paper uh concentrates on that's firstly can i just request in the interest of leaving a little time for our last speaker that you try to wrap up in the next minute thanks okay so the uh the uh the first one is bringing together the compiler and the orchestrator which are the things the compiler maps uh the application name space to addressing in the computer architect architecture the orchestrator maps uh application things to or the services to network addresses if we could bring the two together then the application could see uh much closer to the way the network addressing works uh and and this is one of the things that we've been uh that would potentially avoid the need for the side cars and the proxies uh there's a there's a lot that would look into this and uh happy to to take further discussion on uh the next one is that a an efficient way of of trying to bring this into a common framework is defining all the layering whether there's uh layer seven layer four layer three whatever rather than by an intent of what should be we're actually looking at what a function executes on and what it's transparent to by an observation of what it does that then gives a clear framework for both the application and for the network can you please we we have another presentation and we promised the person full 15 minutes so uh we really need to conclude now okay this is the last this is the very last point here the final one is uh what's more most radical is"
  },
  {
    "startTime": "01:46:02",
    "text": "that in order to join up the network addressing much more uh coherently with the uh namespaces of applications the network addressing would work much better if it started as fundamentally private addressing that then can be an extensible and contextualizable in the same way that name spaces work this would also facilitate security okay thank you um we we really need to move on uh we can move the discussion on this uh to the list uh i see there's discussion on the chat maybe you want to have a look at that and respond and we're going to put that in the minutes thank you very much and to shar please uh we're waiting for you and tushar is local so this is wonderful can you hear me yes um is it possible to uh give control the slides um i'm on the uh yeah i'm just trying to find you in the list uh just so i don't see you in the roadster here and what's going on ah here we are okay sorry you have control awesome uh so hi everyone i'm tushar swami so i'm gonna be talking about building adaptive networks with machine learning and more generally the the role of machine learning and networking uh infrastructure so um"
  },
  {
    "startTime": "01:48:00",
    "text": "more and more uh we're seeing like network complexity increase and they can benefit from data-driven decisions rather than the many um hand-tuned heuristics that we find in the network today and so machine learning is a good solution here in the sense that we're essentially customizing our algorithms to the traffic and data that we're seeing in the network so this isn't in and of itself a novel idea uh there's been a bunch of papers published on anything from security control and analytics different kinds of machine learning applications but the issue that we found was that a lot of these are just running in something like tensorflow pi torch essentially in software and it that means that it's not really feasible to deploy them into a network because it's not clear how exactly they would fit into the network and where they would run so that led to the first piece of our uh our project which was taurus and the i'll go over this quickly because i talked about this on monday but the general idea is that we're going to take our software-defined network and we're going to slightly modify it where policy creation is takes the place of not just flow rules but also machine learning training and then in the data plane in addition to our typical packet forwarding with match action tables we're also going to do decision making with machine learning inference so the control plane will be developing new models based on information that's taken from the network and it's going to be installing model weights into the data plane similar to flow rules so the issue here was if we're operating in the data plane we need to be uh doing our ml inference fast to keep up with typical data plane operations and so that led to taurus which was a switch architecture um"
  },
  {
    "startTime": "01:50:00",
    "text": "a pipeline for enabling ml inference at a line rate at a per packet level um and being able to essentially give you a programmable fabric so that you can put in different kinds of machine learning applications while still meeting your per packet line rate up operation and so we're reusing a lot of typical programmable switch hardware like packet parsers and match action tables but now the packet parsing is doing things like feature extraction we're using match action tables for rule based pre and post processing we have our typical traffic manager but we've inserted this mapreduce unit so based on the mapreduce abstraction and that's what implements our machine learning inference so the the takeaway there was we the robustness of our network is going to be based on the quality and speed of your reaction and that means that machine learning inference should happen at a per packet level in the data plane and this is what the taurus architectures aims to do so we published that in s plus but um there you know that brought up some follow-up issues namely how do we program uh taurus like architecture because now what's happening is that we're asking network operators to be familiar with networking they should be able to do all of our the machine learning hyper parameter tuning and architecture search and then they have to have some knowledge of the hardware itself in order to efficiently program their models so it's actually kind of a lot to ask and um this was one thing we found by talking to different people the network in the networking community he said if he gave us a an easier way to program this an easier stack we'd be more inclined to use it"
  },
  {
    "startTime": "01:52:01",
    "text": "and so that led to our next project which was homunculus which was essentially a high-level compiler or framework for generating these data plane models um depending on what kind of hardware you had available so the idea was that the user gets these very high level directives that they can use to essentially request their applications and they can provide the different network and resource constraints that are available in this environment and then the compiler will simply generate binaries for your different data planes in this case a taurus switch with optimized ml models so this is the general architecture the homunculus compiler there's some more complicated stuff on the uh the inside but really can be broken down on the left here into three core pieces which are the the front end the optimization core and then the back end so the idea here is the user is inputting some sort of data set for their application let's say in the case of anomaly detection maybe you're having the uh the kdd intrusion detection data set uh constraints so they're saying that my switch has this many resources maybe some limitation on sram um on chip sram or dsps match action tables and whatnot and um also network constraints like i need to run at one gigapacket per second or i have a latency requirement for my slo objectives each switch needs to run in under you know 500 nanoseconds or something like that so you input that data it goes through the compiler and then you get a binary for your data plane so a little bit more concretely you can imagine the user is programming their"
  },
  {
    "startTime": "01:54:00",
    "text": "switch in say p4 and then the machine learning portion is programmed by providing this data and configuration information and then we're going to generate these ml models and this is sort of the key here we have so many different constraints between the network and the physical resources available that it makes it very difficult for a human to program it but that means that it actually reduces the search space in an automl fashion to the point that we can actually reasonably traverse the automotive space and come up with optimized models here and so to do that we actually use multi-objective bayesian optimization with feasibility constraints generated from the network and hardware constraints and so we're generating different ml models and testing their feasibility and using that to guide the automl search and then we're generating whatever um back-end specific code you need for your uh switch so in the case of a tour switch we program the machine learning portions in the spatial hardware description language so that's what's being generated from the machine learning models so just going through quickly a little bit more in depth on the the different pieces here um at the front end uh we call it alchemy it's really just a python library so this is the full code that you need for generating a simple model in this case we're doing anomaly detection so you can see at the top there you import your alchemy library in the second block here you provide some user function for loading your data this case we're just loading it from a csv file and it's under this uh data loader annotation that allows the compiler to wrap it and figure out what to do with it"
  },
  {
    "startTime": "01:56:02",
    "text": "you specify your model some sort of optimization metric and what kind of algorithm you want to use you can actually even leave these blank and let the compiler choose and then our constraints so we have performance constraints you can see through put in latency here and resource constraints and this is all these constraints are being placed on a torus platform and then finally we ask it to generate the binary or bitstream in this case so moving to the optimization core once the user has provided all of this information we actually want to generate our models so this is the piece where i mentioned earlier we're doing bayesian optimization using the hyper mapper package and the hypermapper is suggesting batches of hyperparameters so this is everything from say the number of neurons and layers in the dnn or the different unrolling factors on different layers in your dnn and different parameters in the hardware that would allow to be mapped more efficiently so uh this these are that will be sent to homunculus which is then going to um start producing candidate models based on these hyper parameters it's going to test models and see how well they're performing say how good or bad your accuracy is and then doing these feasibility checks so if someone requested one gigapacket per second uh throughput did this model that i tried out uh actually need that does this model map properly onto these resources um and so all that information is going to be sent back to hypermapper which is going to continue the bayesian optimization process and based on that information it's going to refine its search more and more so jumping quickly to the back end here the back end is responsible for the"
  },
  {
    "startTime": "01:58:01",
    "text": "actual code generation for your final switch hardware but also doing these feasibility checks like did i meet the throughput or did i run out of resources and that's really just a built from a template library so this is all supposed to be modular so you can slot in different backends into homunculus but in this case we're building um a multi-class classifier with a dnn with packet parsing and d parsing so we're just building out of these um smaller component functions and these will all be customized based on those hyper parameters that um that were suggested by the bayesian optimization process so just some quick results here um we tested it with a baseline of three different applications versus our homunculus version that was automatically generated the baselines are hand tuned and you can see in actually in all of these cases we're getting a higher f1 score um and this is without any human intervention and the the real the secret behind this is that the baseline applications are done abstractly with they're just put into tensorflow or pytorch whereas in homunculus you're specifying the actual platform so you can make better uses of the resources because you know what's available so just to wrap this up here homunculus gives you high level interface it makes use of the network and resource constraints and generates binaries for your models so there's a link for the the paper here it's actually under submission but there's an archive paper so the final piece of this then is if we're looking to"
  },
  {
    "startTime": "02:00:00",
    "text": "work on network infrastructure how do we supply data to homunculus so now we have our tourist data plane which is doing line rate per packet machine learning inference we have homunculus on the far right there that's producing models for the data plane and the last piece here is and this is current ongoing work so how do we take telemetry data and clean that data to feed it to homunculus so that we had this loop of the network essentially taking measurements uh from its own data plane um and then building progressively newer and better machine learning models which then it installs back into the data plane and just keeping this loop going so we have a simple pipeline here set up with a streaming database doing basic cleaning data extraction and repair augmentation transformation and then some automatic labeling with different oracles but the really the the takeaway here is uh completing this loop which gives you this whole this adaptive sort of feedback loop within your network allowing it to modify itself for the different ml applications that you're working with so uh that's it for me um i have links here for the taurus and homunculus papers and we actually have an fpga testbed for taurus if people want to try out and at sitcom this august we're giving a full day tutorial if people want to get their hands dirty so i'm happy to take any questions thank you very very much this is the type of research that this group is absolutely interested in uh not only because it com it has um this this idea of the computing in the network but"
  },
  {
    "startTime": "02:02:00",
    "text": "also the data-driven approaches and and the idea that ai can be can be a tool in networking not just a uh some kind of magic that people think they put everywhere um i don't see any questions um oh i have a question sorry i'm jumping okay this is eve schuler and um i guess what this goes back to i mean so what i heard you say um was that you're putting the ml needs to be line rate the feature extraction and that seems to be sort of one of the challenges here is um not just match action as we know it for packet headers but there's this whole other kind of algorithmic stuff that has to happen at very high speed um what kinds of feature extraction i mean so you talked about you said there were sort of three use cases that you applied this to but i've been very curious for a long time in this group we've talked about something called ubiquitous witness which is really taking image data and doing feature extraction and have you done anything along those lines that would potentially have a larger overhead and kind of thwart you in this regard uh so we haven't done much with um image data uh at the moment so um in this case feature extraction is is more based around like uh predefined headers so say um matt like our packet purchase and manchester we would be pulling out to like an ip address or something of those along those lines but yeah the we're looking to to start dealing with um like uh some sort of uh image classification networks in the in the data plane but um yeah it's not we're not certain actually what like the the best use case here would be to motivate um say grabbing images from your packet um and then how that applies to the like the networking things"
  },
  {
    "startTime": "02:04:00",
    "text": "further into the packet right right but yeah it's definitely an interesting thing there is uh there is the the somewhat of the the downside that a convolutional neural networks that are used for image classification are they tend to be very large and we're still fairly resource limited um on the switch even though we can do some you know data plane machine learning the networks do have to on average be smaller than um like the the resnets and uh you know um those kind of like the c410 um networks that you see in in like the typical ml competitions thank you that was a great talk yeah thank you very very much uh other question there's somebody at the mic yeah i have a question to show can you identify yourself please i am hisham so my question is uh you update the model in line yeah do you update the model in line in the data plane itself or the model is created in the control plane automatically using these tools and but it gets updated also by the control plane not in the database right right okay that's it yeah yeah so so the the data plane is as far as is as far as the data plane is concerned just as a static model it's doing inference by applying it and then the control plane is responsible for taking uh measurements in the network refining models and then in its own judgment when it's time sending those models out to the data plane okay well i guess we're over time so thank you we're going to see you at the um at the interim and to shar please join our list and participate in our discussions because your work is absolutely related to what"
  },
  {
    "startTime": "02:06:01",
    "text": "we want this group to evolve into and not just uh the original you know using p4 to to do match action but what else we can do with all of this uh once we have a framework i would like to thank lucia to have been or uh well for me an avatar but for you guys a real person uh thank you very much lucia for uh for having uh given your time for us and hopefully um it was good i would like to thank also dirk and and dave to have allowed us uh to be hosted in their two hour uh we reorganized this a little bit at the last uh a bit late because we were we gave our own slot to somebody else who could not use their thursday slot and i would like to really thank icn to icnrg to have hosted us um i think we are showing more and more that there are so much in common between um the two communities so that i think it was it was good and maybe we will have other uh similar uh co-located meetings i was thinking this afternoon there's a distributed networking meeting which also has a lot of overlap with what we do and i think um in terms of the research that was presented today i think it shows that coin rg is still very much a very active topic and i think a more and more active topic in networking and thank you very much everyone and for those of you who are local in philadelphia have a wonderful end of meeting and for those of us who are a bit under the weather uh somewhere else in the world um i wish us all to feel better and for the other ones who are not sick well don't get sick and please um stay safe everyone and have a good"
  },
  {
    "startTime": "02:08:01",
    "text": "flight back and hopefully uh we'll be able to see you in london thank you thanks for doing this with us and thanks alicia for being our culture hello bye-bye bye thank you so the question do i need to do anything shutting anything down or we just do it okay great from okay you"
  }
]
