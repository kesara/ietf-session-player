[
  {
    "startTime": "00:00:03",
    "text": "Serial? Just the new UI is better. It's less busy, but you have to know what all the icons are. Yeah. Right. Have a pretty light agenda. Then then People only requested 15 minutes working 25 Yep. Yep. We should be able to making up for last time where we were a little impressed. So first list? Yeah. Well, I requested more times for the meeting this time. That reason Yeah. Well, I got some terrible infections, but that that evening some of the So, yeah,"
  },
  {
    "startTime": "00:02:00",
    "text": "And I'm sick of a respite IETF. Hand wheel. That's good. It's fine. I don't remember the rest of the week, but I remember Just gonna give it one more minute in case, Anyone else needs to join on on remotely. We have a light agenda today. So we'll hopefully be able to finish early."
  },
  {
    "startTime": "00:04:14",
    "text": "She's still Yeah. Okay. Let's go ahead and get started. Welcome everyone to Brisbane Australia. we're here at IETF 119. For the ML Kodak work group session. Machine Learning for audio coding. First of all, note, well, the IETF IPR policy. Please review that in detail. But a short summary is that as a participant, and IETF activities, which includes this meeting, activities on the list. That You should declare any IPR that you're aware of that applies to any of the materials that are discussed the meeting or that you contribute Please read the BCPs there. For the full details. And also note the code of conduct Please treat all of the participants professionally, And finally, the meeting tips if you're in the room, you can scan that QR code that's up on the screen or you can click on the on-site tool and the meeting agenda. That'll take you to the, meet echo client for in so in room participation. You'll use that client to join the microphone queues and that'll all automatically, sign the blue sheets for your attendance. If you're a re and also for in room participants, please keep your video and audio off"
  },
  {
    "startTime": "00:06:00",
    "text": "the entire time. You'll be captured on on video and audio if you go to the mic. For remote participants, please use the full client and that's the full video icon in the meeting agenda. And, again, also leave your audio and video off until you're actually speaking it's your turn at the queue. So first of all, we need to find, notetakers for the session. Be really easy. Hello? Thank you very much. And, just, enter the, the ether pad the link of the agenda. Taking notes directly there. Sorry, Jonathan couldn't hear. The notes dotitf.org. Let's not give it a name. Just give it a URL. And think we're I think the chairs can monitor the chat. the K. So on the agenda there's, three items, each of which correspond to one of our chartered work items. So there's first the OPUS extension mechanism. Tim's gonna talk about that. That's a work group draft. There's the deep redundancy Jean Marc is gonna cover that. And that's still an individual draft. And then finally, the speech coding enhancements And Jan is gonna cover that. Again, that's also an individual draft Is there anything that people would like to bash and the agenda. Okay. Hearing nothing, Tim, Europe. And I will share the slides for you if that's Okay. Yeah. Make it easy."
  },
  {
    "startTime": "00:08:05",
    "text": "Alright. So Tim Terry, and I'm gonna talk about the OPUS extension draft. Next slide. There've been no changes to the draft, since the last meeting. So 1 is still the current version. Next slide. One thing I do want to talk about at this meeting, however, is the extension ID numbering scheme, which is something that we need to sort out before this can be published. So, basically, the the question is is how should we Number the 0 or 1 byte extensions. So if we use one ID for both the l equals 0, and l equals 1 version. So that's one with with no payload and one with a 1 byte payload, then we have to allocate them in pairs and the INA 3, and we can't signal support independently in SDP, which, you know, if you have an extension that always wants 1 byte or an extension that always wants 0 bytes, you know, who's means you're gonna be wasting one of the code points. And a question that that was raised briefly after the last meeting is should they really be 0 or 1 byte extensions, or should there be 1 or 2 bytes or more some question of how actually useful a 0 byte extension is even though we already have one in the separator extension. So I have thrown together a straw man proposal. If you go to the next slide, And this is just sort of to get people's opinions, but here is one that we that we could go about solving this. And I think this is rough way, something on the list some time ago."
  },
  {
    "startTime": "00:10:03",
    "text": "Basically, we would split the extension ID space into short and long extensions. So The first column in that table there is the the code points of the actual byte value that will be signaled The second column is the IDs, And then the 3rd column is the length. So we have to specify the length because people need to know how to skip unknown extensions. And so for extension bytes 4 to 63, these are the the short extensions, and I'm proposing that we set the lengths to be the bottom two bits of that code point. So any any length between 3, And then for the range 64 to 255, those are the long extensions. So there, the the bottom bit tells you whether you either use the rest of the packet or whether you explicitly code a length. And, basically, the reason that that these are 2 separate cases is because We want to be able to allocate the short extensions, you know, potentially in groups. Right? Like, an extension might want to have several different possible lengths. And use those bottom bits to select which length but all you being used for the same extension, meanwhile, for the long extensions, we always want to allocate in pairs. And the reason is because if if you don't have the option to explicitly code a length, then, you know, you know, for example, if you only had the the code point with an even code word so that it would be the Sailing, you would use the rest of the packet, then you can only code 1 of those in any given packet. So to allow Basically, to allow implementation is to to you know, merge multiple extensions in a given packet or or have extensions for multiple frames in a given packet."
  },
  {
    "startTime": "00:12:01",
    "text": "You wanna be able to switch between those 2 encodings. You always wanna have both code points available. Alright. Next slide. So how do we do this? For the short extensions, It said there are fixed lengths from 0 to 3 bytes. So the idea is that there would be one, if extension ID per code point. So every every different byte value is its own extension ID. That's the a 0 day 59. And Each extension can register multiple IDs. And the way we'd signal this in in in SCP is that You must include all of the IDs registered for an extension in the extensions equals parameters. If you support that extension, And then the extension names actually include all of the The code, all the extension IDs, for that extension in the extension name. Then for the long extensions, there's one ID per extension, and it covers both the even and the odd cope point, and that works exactly like you'd expect. I see Jonathan Lennox in the queue. Yeah, I don't understand what this a 0 to 9 means. Basically, they're they're just 2 extension ID numbering spaces. So a 0 to 859 corresponds to code points Okay. 4 to 63. Oh, I see. 0 to B95 corresponds to 0.64to255. I think it'd probably be clearer to just call them by the numbers rather than This a 0b0 thing. I think that's Just call them 3 to 64 or whatever it is. Yeah. I mean, You you you could do that. It makes the numbering a little bit weird when you get into the b"
  },
  {
    "startTime": "00:14:00",
    "text": "entions because then they start incrementing by 2. The code points increment by 2, even though the ID only increments by 1. Oh, I see. That's sticking with that. That that's his presentation. So of the 2 extensions were actually proposing what kind are each of them? So so right now, I think we only actually have 1 proposed extension, not not including the padding in the frame, frame marking extension, which is which is dread. So dread would be a long Mhmm. I guess technically, we also have the the reserve for future use extension, which also a long extension. Yeah. I think the 1 of the primary use cases of the short extensions would be This enhancement stuff, when it starts using side information. Currently, it doesn't send any side information. But you would expect us to be short I would ex expect them to be short. But, like, I don't think we've actually designed how that works yet. So I'd I don't know that that's how that will work. And one of the things that I kinda wanted some feedback on for the design was, you know, once we've sorted out a bit more how that will actually work, like, you know, make sure that it makes sense to be doing what we're doing with the short extensions in that context. Yeah. I mean, I'm a little reluctant to this final, this mechanism before we know whether we're actually gonna what our extensions anything other than our one extension is gonna look like. Right. I agree. Yeah. I think Jonathan is echoing something out on the list too is that, in this design, it would would greatly inform the design if we knew"
  },
  {
    "startTime": "00:16:01",
    "text": "what the properties of the actual extensions were. So that my my first question was Do we know of any extension that will have no data other than the separator, Right. So so I might can an extension that has no data. I can give one example that I thought of anyway. Which is you could have an extension to just disable the enhancement on a per frame basis. And there's a bunch of reasons why that might not actually make sense, right, because, like, you know, to do that means you need to run the thing on encoder to see whether or not it's doing a good job and and you need a metric that tells you whether or not it's doing a good job. And if you shut it off for one frame, like, there'll be some cross fading. If you turn it back on the frame, there's more cross fading. So, like, you know, that There there's a bunch of reasons why you might not actually do that, but that's sort of an obvious case of you know, he he here's Here's a knob that you might wanna have that would require no data other than you know, one bit shut it off. Okay. So if we do expect some more extensions that have No data and just serve as a flag. Then we do need multiple code points for those I put a counter I I didn't see this proposal before I put my proposal on the list, but it was basically to flatten out space and just enumerate all the 250 six code points. And the INA registry would just list literally all the 256 code points, and we'd have you know, only, 4 or 5 of them registered at you know, at the close of these three drafts And then more extensions could come along, but, basically, it'd have reserved code points for 01, 23, and maybe 4 by whatever we think the right number is, for these short extensions, have a handful, you know, 32 64, whatever, code points for each of those sizes. And then a bucket for the variable length ones"
  },
  {
    "startTime": "00:18:01",
    "text": "that that encode their own link byte or go to the end of the packet. And and and to be clear, like, I'm I'm not proposing, like, this is the design. Right? Like, this is just a straw man. I put up to get some discussion going. I I did see your proposal on the list. And and and I think One of the things that I tried to do here that you know, your proposal didn't do. Was this those grouping and the b extensions to make sure that you have know, both representations available for every extension, right, so you can do the splitting and merging of extensions. But on on the short extension side, I agree with you. Like, there's you know, there there's tons of ways that you could splice and dice those those extensions and decide on what lengths you might have. I did talk briefly about this with my co author. We sort of looked at, you know, like, If you want to encode an extension of length 4, And it's your only extension. You know, right right now, you'll it would take 7 bytes. Right? Like, you know, the overhead of switching to a code 3 packet. You have the overhead of specifying that padding is is you know, encoding the padding length, you have the overhead of signaling the extension ID, and then you have your 4 byte payload. Whereas, you know, if you just used a long extension for that, it would be 8 bytes. And so is the difference between 7 and 8 bytes long enough to reserve a bunch of short extension IDs I I don't know. It didn't it didn't seem to be that important to me, but maybe it makes a difference if we expect to have a bunch of extensions in every packet. I I guess, like, a thing to keep in mind is that fight"
  },
  {
    "startTime": "00:20:02",
    "text": "per packet of 20 millisecond frames is 400 bits a second, which know, is is large in the context some use cases. So of of Yes. Not a quarter of the of the current dread. So, actually, that that's a good question is, what is the current size of of the current extensions For for dread, what is the typical size is it variable or is it reasonably constrained to just 45 3, 4, 5 bytes. So I think it supports anywhere between 10k bits and a 100 kilobits. So, you know, you can divide that by 400 to get bytes. Right? So some somewhere between 25 and and and more bytes. So it's already in the variable length. Category. It it would never would never benefit from the shorting coatings. So what what do we have any extensions planned that benefit from the shorting coatings. From the 2, 3, 4, 5 bucks. 1 01, 2, 3, 4 bytes. Yeah. So I I would hope the enhancement stuff would just because if you start using enough bits that It wouldn't benefit that would need one of these longer encodings, you you you know, that's an awful lot of bit rate that you could have just been using Dakota at a higher quality. But May maybe Jan has more opinions on that. I I'm personally just guessing at the moment. Go ahead, Johan. Oh, hi. I attend. So I Yeah. Yeah. Yan, hold on a second. Some something, with your audio sounds very robotic. And very loud. Try it again. Hey."
  },
  {
    "startTime": "00:22:03",
    "text": "I cannot. How about Hello, Simone. Yeah. I'm sorry, but it sounds very robotic and almost like slow motion too. Really? Oh, then, I think I have this, let's is it is it possible to to type into the chat what you wanted to say. Will we let it? Or give it a shot and let's see if it's audible. It's It's awkward, but let's let's see what Yeah. To figure any, increase or enhancement promising or at this 13. The fact that he's with great sex and at some Yeah. So even the transcription, Didn't didn't quite get unless you were talking about grade 6. don't I think I don't think we heard your I'm not sure what what corpus this this, transcription was trained on, but Talking about grade 6. Oh, boy. Yeah, just, type it out if you don't mind, Yaron."
  },
  {
    "startTime": "00:24:05",
    "text": "Okay. The enhancement will use a few bytes. So can you clarify, is that 1 2 or 3? Or could be any of them Okay. So we do have a valid use case for this. 401, 2, or 3. And clearly variable so I I think we probably should try to figure out a scheme that does divide the space, in order to make the short extensions efficient without another length and coding, So I'd encourage people to please review Tim's proposal and also the the proposal that I put on the list And, anyone has a extension that they plan to bring Please mention what the expected data size is for that extension. Would help inform This code point allocation, Good. Tim, you're back. On Alright. If there are no more questions, I think that was everything for this slide. And so, basically, alluding to a question I think that's already been raised, When should we actually aim to go to working group last call for this? Think on the schedule, we had it drawn up as as in at the end of last year, which obviously didn't happen. I I think it would benefit from being able To see one of the, you know, enhancement with side information things get a little bit more well developed."
  },
  {
    "startTime": "00:26:02",
    "text": "Just so we can make sure that the design works with that. But on the other hand, if dread winds up being ready to publish that, like, we don't wanna hold up dread waiting for this draft to be published. So trying to gauge what people's opinions are. So let me ask, show of hands if how many people have had a chance to review, read this draft. If you're online, maybe just plus one in the chat, if you've had a chance to review this draft. K. So I'm I'm only seeing 1 or the 2 in the room 3 myself. including So I think we need more reviews. And I see one online 4 reviews is not terrible, but, 5. Okay. That's a decent number, but it would I think it would be useful to have a few more eyes on this. I'm not confident that we have all of the major issues sorted. I'm not confident that the SDP negotiation section as well. Understood by everyone and what the nuances of Whether You're able to negotiate all of these individual extension IDs whether they're symmetric or whether they're receive only or send only all those nuances in the SDP offer answer model. I'm not sure are captured properly in the current version, So please Review this with the lens of everything that you think impact interoperability, that would include at least the SDP section and at least this, extension ID numbering I'm sure there's other aspects in the in the doc too So I'm re I'm reluctant to try to go to work group last call until we have a few more reviews. And some more feedback on the list on this"
  },
  {
    "startTime": "00:28:05",
    "text": "Yeah. I think that makes sense. I see you show Mark has his hand up. Go ahead, drummer. Yeah. Just to comment on the timing. I think think it's, you know, there's this trade off of, you know, obviously, we don't wanna delay dread, but if we wait longer to what extensions we have out there. Maybe we get we get to make better decisions. So I think what maybe it would be a compromise in there is to make sure that at least, what we use right now for dread. Which is basically you know, the the set that we reserve for the long extensions if we sort of agree, at least, unofficially, that we're not gonna change that then it means we can start you know, shipping experiments with dread having tests that run-in the wild and make sure that when we finalize the extension, we don't, like, randomly shuffle the code point have everything break. I as long as the fine you know, as long as the final draft maps, you know, 0 the you know, 0 and 1 IDs and, you know, the experimental IDs in the same place in the 127, you know, we're pretty good even if it takes some time for the for the extension drafts to be finalized. I don't know if that makes sense. Yeah. That makes sense. And when you say experiment, do you mean just a developer experimenting on their own, or do you mean an actual deployed experiment. Like, actually putting it in WebRTC experiment or something on a chromium flag, Yeah. I I I mean, that kind of thing, like being able to have it"
  },
  {
    "startTime": "00:30:02",
    "text": "in a in a flag or have, like, some applications start using it With this is is is using these experimental code point, like, it's not gonna break anything unless we decide suddenly to reassign these experimental things with, like, something actually useful, then, you know, these things might break a bunch of things. But but but but but Otherwise, you know, If if If there's a few things that we're using in experiments right now that don't move, then you know, nothing bad will really happen. This is equally an argument to push to get the the draft. Finished sooner rather than later as well, which also has a side effect of increasing confidence set set whatever properties of it. 7. Need to be maintained for your still maintained. Through. Yeah. There there's definitely a trade off there. And, you know, if anyone can think of, like, wild ideas of what they think could end up being an extension, like, even, like, Hosting a bunch of those on the list could help you know, figure out if there's, you know, good or bad ideas, but at least like to see if there's a pattern of what would be useful. Because We only have plans for, like, 304, and we're allocating for, like, a 100 So clearly this is forward thinking, but it's hard. So let let's shoot for trying to get more reviews in the next in the next few weeks, or certainly before the next meeting, on this. And let's try to close off on the extension ID numbering. 1st And that will help stabilize the deployments but we don't necessarily need to go to work group last call for the other aspects yet. K. Jean Marc, you can stay on. Tim, are you done? Yep. That was my last slide. Yep."
  },
  {
    "startTime": "00:32:04",
    "text": "And just to clarify, the straw men that Tim is proposing with wouldn't actually break anything. In practice. Some of the SDK might changed slightly, but you know, if experiments get deployed like that, nothing breaks. So I am Jean Marc Valle, and I am presenting, d redundancy for the opus codec. Next slide, please. So just a brief recap, dread. The idea behind red is to code a large amount of redundant audio in every opus packet. We use a DNN to maximize the compression, and that allows us code about up to about one second of audio each 20 and more second packets. So about 50x redundancy with bit rates that range from about 10 to 100 kilobits per second, of overhead for the redundancy. Next slide, please. So, changes since, the last meeting, there's been mostly 2 changes and they have to do with the signaling. The first change is we added an extended offset field. So it's an optional byte that we can add to the packet, and that can signal offsets up to about 20 seconds. We don't really need up to 20 seconds, but that's what a bite, an extra bite gives you. Without the extended offset, the offset can be plus or minus about 40 seconds. So it basically says the the redone where how is the redundancy aligned with the packet you are receiving. And, being able to signal much larger offsets"
  },
  {
    "startTime": "00:34:02",
    "text": "has a few uses. The main one for which we added that is to be able So if you have a stream and the person just stops speaking, you don't need to actually encode all of the silence. You can just say, well, there's an offset, and then it starts in the past. This is This allows for more efficient use of the bits but it is also helpful for d t for the case of DTX. This is how we discovered the issue if you're Jitter buffer doesn't know about silence, then Although, you may be in a situation where you receive the first packet after DTX and the gear buffer goes Oh, I missed so many packets because, you know, they didn't arrive and then try to fill in the buffer with silence and add delay and so on. So, Has adding that extended offset fix of that, it would still be nice to fix to your buffers, but seems like it's useful. And another use for it a bit more Farfetch, but it's something that we've discussed previously around SFUs. So if an SFU just, just switched to a news train, it could keep the the dread from a previous stream the previous active stream and just keep incrementing the offset for some time until it decides to switch the redundancy to the new speaker. I don't know that anyone would really, really use that, but it would be out there and that that use case is the reason why the extended offset is 1 wholebyte it makes it much easier to insert. So that's one of the One one quick question about your SFU use case. And for these changes. SFUs typically, the ones that don't decode the media. Typically, rely on audio levels and had her extension"
  },
  {
    "startTime": "00:36:01",
    "text": "to make their forwarding decisions, their switching decisions Should we have any guidance about These extensions don't contribute at all to the audio levels, or or do they? I don't think they contribute to the audio level. It is just about containing information about previous packets, but If you don't lose any packet, you're never going to use any of these extensions. Oh, you mean if you lose a bunch of packets Theoretically, you could get the levels from that but that seems a bit complicated. It may be useful to have some some text about how this interacts with audio levels the SFUs used to make their switching decisions In general, it it wouldn't unless you actually choose to try and do something funny with it. May still be good to have advice on it. K. I would need more clarification about what exactly that otherwise would be. I see Tim in the queue. Yeah. I I maybe Moe can correct me if I'm wrong, but I think the issue is is you know you know, the packet might contain a bunch of useful information about past stuff that had high audio levels, but if the current frame has no audio levels, then the SFU will never forward it. Yeah. I would Yeah. I would need to understand a bit better how these things work. I guess, to say something useful. My assumption there was mostly that you know, they ask if you would forward speaker a for some time, And then at some point, decide, you know, now I'm gonna forward speaker b, n,"
  },
  {
    "startTime": "00:38:00",
    "text": "the only thing that this would change is when you just switch to speaker, a, you might want to include the redundancy from speaker b because that would be more useful if you wanted to just forward something. Yeah. I'm I'm I think the the audio levels questioning can even you know, that That that question, matters even with just a single speaker. Yeah. I mean, in that case, if you stop forwarding your violence. I mean, the SFU sorry. Go ahead. Please finish. I was just gonna say, like, think the SFU, it makes sense for this if you'd not to forward silence, but at the same time, if it sees that there's dread that might as well it may decide to forward it, but I don't think there's gonna be much information in the dread packet to make that decision. Like, if, if there is redundancy, then presumably If the encoder is not too dumb, then probably there is some non silence that redundancy. Like, You could have your encoder work in any way you like, but the way, for example, the current encoder will work is, you know, if the past, sec if the past second is all silent, is gonna not encode any dread So if there is dread, then probably there is some redundancy about some speech in the past. So maybe it's useful to, to try to think about can any of this be summarized for, for useful you know, develop our guidance as a few developer guidance around whether these extensions you know, impact SFU switching, or decisions about decisions about what the audio levels apply to, whether they extensions,"
  },
  {
    "startTime": "00:40:02",
    "text": "relevant or not for for audio levels. Little bit of text, a little bit of thought about whether a text needs to go in to clarify those things because think from the discussion, it's not clear to everyone what what should happen Yeah. I I I can try adding some information there. But but Yeah. it. I think even adding what you just said of, you know, if you see dread, maybe you should forward Is would be useful guidance. Yeah. Although even then, like, if the encoder decides to use dread all the time, even for silence, then, you know, you might be aware of that, but I I can also also say that if it helps. I mean, Okay, should I switch to the next change? Oh, Jonathan? I mean, we should say encoder should not encode dread of silence. That is a fair thing to say. Yeah. And, also, there was a question on the chat about the what happens if there's If you'd putting the history and there's more than one silence period. Can you have my Do you have multiple threads in a single packet? Do you cover different time periods? No. Right now, it's a limitation that you know, we can you know, we encode backwards. So It's it's we can skip the last silence And then we have some speech. And we can skip the the earliest silence, but if there's speech, silence, speech, then we will have that gap. And there's You couldn't put 2 Different threat extensions in the same opus bucket. Theoretically, yes, in practice, it would be a nightmare. Okay. Like, I I think from an implementation point of view,"
  },
  {
    "startTime": "00:42:02",
    "text": "I looked at that. Like, you could also have, like, a mask of some sort, things I could think of but all the It's probably worth saying once again. If it's not supported, you should probably say, you know, If you receive this, ignore all but want or throw all the extensions away or something because something weird is going something you don't understand is happening here. you mean, if you Oh, actually received 2 dreads in a packet. I Yeah. I think we say something about that in the general extensions draft or Are we don't, in which case, dread will need to say only one is valid. Yeah. Okay. Okay, so Next change. Oh, Tim, did you wanna add something Yeah. The the general extension draft says that all all other All other extension draft should tell whether or not they're allowed the thing to be specified multiple times. Okay. So, dread will say then that, Only one extension is allowed for Per packet or per frame? I don't know that multiple per frame makes sense. But but but but we can discuss that. It just says per frame, I believe. Okay. Okay. So the other change was, suggested by Tim is basically adding a Qmax field. Basically, we we have a Q 0, which sets the the highest quality quantizer for the most recent speech. We have a deque parameter that says how much the quantizer will sort of degrade as we go back in time. And Qmax basically sets a cap on that. So For example, you could start at quantizer 5 and at"
  },
  {
    "startTime": "00:44:02",
    "text": "increasing quantizer number is worse quality lower bit. Right? So you could start at 5 and then it degrades, it goes down to maybe 10. That you say never go worse than 10. Otherwise, by default, it will go all the way to 15 if you have enough, friends. So is just more gen general. It's not used right now and the cost is only one bit to say you know, QMAC is just the default of 15. So That's how it's, that's how it's used, basically. And it's a variable size as well. So, you know, you start at 15, if you start at 10, then there's no point in signaling that your Q Max is 9 or 5 because it's gonna increase. Next slide, please. So This is what the current format looks like. So we start with the those q0anddq And then the x flag just says whether there is an extended offset. So with that, we have an entire bite. The extended offset is 1 byte that is optional. As I said, the reason we make it one full byte, even though we may not need, you know, 22nd 20 seconds of offset is, just because 20 seconds or Sir, actually, Yeah. So basically it makes it easy to insert the offset if you have some middle box that wants change that. The rest of the offset is the is 5 bits Qmax that is there, I don't know how I didn't know how to show that, but it's actually variable length. And then we have the rest of the that we discussed earlier. With the initial state and all the lighting vectors."
  },
  {
    "startTime": "00:46:02",
    "text": "So Next slide, please. So in terms of, you know, how we actually Standardize that. The current proposal is to normal to normalize So to standardize the part that turns the on the wire bits into the vocoder features. And standardized only that. So that would mean that all of the DNN weights for the decoder would be frozen. And I propose to publish that using a simple, a simple binary format ideally just the simple format that we use right now in Le BOPIS, just because it would ensure that it's better tested, but it's a trivial format to, interpret The, after that, we would define sort of these acoustic features they could be defined in 2 different ways. Either we find the actual processing that gives us these features or we define them from the vocoder point of view. I would we can discuss that further. I would tend to prefer probably defining most of them as just the actual processing you apply to the audio. The only slightly tricky one is the pitch which is currently specified using a neural pitch estimator. So Either we find, like, a good enough definition of the pitch that makes it possible to use different estimators eventually that get better. I think that would be preferred if possible. Otherwise, we could also just specify the weights of the neural pitch estimator. It is not very big, but"
  },
  {
    "startTime": "00:48:00",
    "text": "Maybe it can be improved also in the future. Question about that. How how many weights and what, Fidelity, What size? How much are we talking about publishing? For the neural pitch estimator, I think we're talking about tens of kilobytes, if not, like, a 100 or 2 200 kilobytes max, I would need to, check the exact numbers, but it's definitely smaller than Smaller, much smaller than the than the declutter. What's the decoder size? Decaturweights. Decoder weights is, yeah, good question. That is about a about a million parameters, and they're almost entirely quantized to 8 bits. So essentially in the order of a megabyte. And how are you proposing to to publish this? A normative aspect of this document. Plan on actually publishing in this document, I I don't think it makes much sense to actually put that in the RFC. I think we probably want to find something that looks a bit more like what we did with the test vectors. But this is normal if it's not just a test factor. Yeah. That that one Yeah. We would need to find something. I mean, we could obviously have, like, the shaw 1 or something and have it in different places. With encoding that as bay 64 is some people are not gonna like the r the the resulting RFC. To say it least, k. Keep that in mind."
  },
  {
    "startTime": "00:50:03",
    "text": "Yeah. But we'll, yeah, we we will need to find a way of publishing about a megabyte I'm still trying to shrink that down a little bit less. I Actually, maybe I managed to get it down to about 700 kilobytes. Maybe we get to 500 kilobytes, but you know, it's it's not gonna be 100 kilobytes that I can Pretty much guaranteed. Not without losing a, a lot of quality. We see where are we? So, yeah, so the encoder would be left, unspecified And, the idea, again, on the vocoder is to have really minimal constraints just to define what its features should be and to say, you know, it's generate meaningful speech out that. Next slide, please. So in terms of implementation, All of the changes that I mentioned are landed in the Opus Lane branch. It used to be Opusng. Now It's called main. It's the default branch. Everything is there. It was also all of that was also released as Opus 1.52 weeks ago, there's a link to a demo that demonstrate all of these features in that and that release dread as well as the enhancement, everything that is not standardized is disabled by default. It needs both a compile flag and a runtime flag to enable. And, for those who would like to play with it, there's actually a WebRTC patch that adds dread support"
  },
  {
    "startTime": "00:52:00",
    "text": "And, you know, we've started testing things with that. It seems to work. That's how we discovered that you know, probably this, extended offset would be useful, For DTX support, But, you know, I would encourage everyone to try it out and see for themselves and report any bugs. Is how we improve this. Question on the WebRTC support. Are you Are you asking anyone in the community to try to upstream this anytime soon, or you wanna wait until It's more mature and settled before, before you want in the wild experiments in WebRTC with it. I believe we considering that you know, these things are off by default. Like, I think it it Those should be It should be safe to talk about up streaming this. You know, maybe there's a few bugs and we want to iron them out, but I think we're at the point where it starts making sense to do that. Have you spoken with anyone about upstreaming it I see Harold in the room, but I don't know if he's interested, upstream any of this yet. We're just talking we're we're starting to again. Just just saying that I'm printing that question. I have to ask other people. But, yes, I've started speaking to some people about you know, trying it deploying experiments and so on. We'll see how that goes. Take some time. Sometime sometime Jonathan? Does this patch not only turn it on, but also wire it into the jitter buffer to you know,"
  },
  {
    "startTime": "00:54:00",
    "text": "And how how how far back can you recover because, you know, obviously, there's trade offs between length of, you know, delay from dead or buffer versus how far back you think you'll recover. Basically what it does, and I did not write that patch, Michael Kuehnbio did, but my understanding of the patch is that essentially, it leverages what is already existing in the WebRTC stack, which is if your router just decided to hold on to a whole bunch of packet and release them all at the same time. The Jir buffer would suddenly go back play that faster and eventually catch up and adjust its delay according to that. So it essentially just turns the packet loss case into the case of my buffer just hoarded a bunch of packets and really threw them all same time, So I believe it's able to cope with at least half second. Maybe all the way to a second. I'm not sure exactly. Next slide, please. And so these are actual results we got with the WebRTC patches. So previously, we had results that were just based on sort of theoretical infinite delay case, but this is WebRTC adjusting its delay according to the losses. And, We used a loss simulator that is trained on real data losses, but there's still synthetically generated, but they're bursty. They're not just, independent random loss. And we used, PLC Moss is a Microsoft model to estimate the quality of, PLC. So, you know, essentially blue line is what you get without any redundancy. False falls apart pretty quickly. LBRR in orange Eventually, it also falls apart."
  },
  {
    "startTime": "00:56:02",
    "text": "And green and red are with the red. Either with or without LBR in addition. So, you know, even at 90% loss, it still actually holds on. And, you know, Clarification question, is is LBRR just one frame back? And this and this Yes. There'll be our only ever supports frame. one And how many frames is dread here? Dread is configured with, up to one second, but it's It'll adapt based on the bit rate. You know, depending on how many bits you give it in the encoder, it might encode a bit more a bit less. But in this case, it's between half a second and one second. Okay. Next slide. So There's still a few open questions. These two are actually still from the last time because we didn't get so much feedback. The first one is should we have a maximum duration allowed? Technically, With the current draft, you could actually encode up to about a about 10 minutes of redundancy in one packet. That is pretty much useless. But but but but My current proposal is to not have a hard limit and just say the receiver can ignore whatever doesn't want to deal with. So it's free to decode 1 second. And if the encoder encoded 10 minutes then. So we had And the second one is chasing. Hi. Yeah. Jonathan Mummox. I mentioned this last time, but we should, even if we technically allow that, we should probably have warnings that, you know, should not you really think it's gonna be useful, especially because It's problematic for the"
  },
  {
    "startTime": "00:58:01",
    "text": "you know, conferencing late joiner cases. You don't want somebody to be able to scrape out of the dread, the conversation that happened for 10 minutes before they joined the That seems like a case where you would want the SDP to say don't ever send me more than that. And that one was change into a must. Okay. So there is signaling an STP. This is the most you should ever send me. Yes. Okay. Yeah. That's useful. But, basically, with the that is a minor change we did to the draft, but it basically it was like Please don't send more, and it got changed to Definitely never send more. Or change to a month. All right. Yeah. I'm not gonna say it's probably fine as long as you maybe you'd have, like, something in the draft for mining middle middle boxes, they they wanna set this parameter. 6% security considerations. Yeah. Okay. And, Another open question was, you know, lowest versus highest, useful bit rates. Right now, what's in there is what seem to me like it made sense, lowest for one second, the lowest bit rate is about 10 kilobits per second, which is equivalent to 20 bits per second in terms of know, she if it wasn't, redundancy, if it was like a pure codec, and it goes all the way to a 100 kilobits for one second or 2 kilobits effective bitrate. Essentially, like, ballpark, like, less than 10 kilobits for one second becomes not really intelligible And beyond the 100 kilobits, it was hard to measure at least with objective metrics and improvements. I mean, that's what I was gonna ask. It's a but, basically, that's the range which you see you know, from the the bottom is intelligible and and the top is a diminishing returns. So,"
  },
  {
    "startTime": "01:00:04",
    "text": "essentially, that's what it's designed to do, but, you know, I encourage people to try it out, then maybe they they go, like, you know, actually, I would just Does this seem to be something that's the standard says, isn't allowed, or is this just a engineering recommendation To say know, after implementing it, you know, you know, there might be things on, like, for the, You know, the The LibOpus API saying setting it below this value is not useful, but that doesn't not the same thing as Say, forbidding it in the standard. No. No. No. It is related to these quantizers we signal. You know, the the quantizers are fixed. We have q 0 all the way to q 15. Q 0 will give you 100 kilobits per second for 1 second redundancy. And if you keep it at q 0 all the time. If you if you if you keep it at q 15 all the time. You will get about 10 kilobytes per second for one second redone. Oh, I see. So this is about These are really, like, 7th can have set in stone. Okay. In that case, if you think this is the right reasonable range, then I believe you. I mean, that's that's what I believe. If you try out, then you go, like, you know, I'd like to support even lower bit rate or even higher because you know, case I didn't think of. I'm open to that. there's a use It's a Question about how this scales less than one down to second. You only did 500 milliseconds, does this scale lin linearly down or logarithmically down, Or can you not go lower? can definitely go Then one second. Oh, you you you lower. Essentially, you pay a fixed cost for including any redundancy. Like, you have the you know, the signaling. You have this in, this initial state So those you will code no matter how much redundancy, but after that, it is almost linear, like if you set the constant q, it would be linear. In practice, we tend to degrade the quality as you go"
  },
  {
    "startTime": "01:02:00",
    "text": "further. So Doubling the amount of redundancy doesn't quite double your bit rate, but it's not log logarithmic or like that. One that I fur one o other Opus open question that I forgot to put on these slides is it's been mentioned a few times is should we support truncation meaning should we make it possible for a mailbox to just side. I don't want more than This many bites of bread, and just discard the last bites. It It is possible to do that. The caveat is we would need to add to add one truncation bit. And when that bit would be set, if you discard a certain number of bytes, then the the I think There's, like, the last three bites you haven't discarded becomes, pretty much useless just that, but they need to be kept just so that the entropy coder is in sync. So it's a bit wasteful, but It could be supported if Someone thinks it's useful. And I say last three bites, but We'd we'd need to check. Maybe it's 4. Tim would know the answer. I don't Go ahead, Yeah. Yeah. I I would need to double check also, but I thought it was for I I think also worth pointing out is that you know, most middle boxes are probably gonna wanna to to to do some truncation based on a point in time. And you don't really have a mapping from time to bytes, other than than essentially guessing."
  },
  {
    "startTime": "01:04:02",
    "text": "No. This would only be in order to save space of some sort. Indeed, for time, it would be You could guess, but you wouldn't get anything Reliable. Dip. Go ahead, Chelsea. Yeah. I would say don't do this now and If we and I think the way we would design the extensions, we could have a separate extension that is truncated thread rather than having a truncated bit in the grid. If we wanna do the truncation, add truncation in the future, You could presumably add it. But also also fine with that. I mean, if we think it's useful, it just it just costs one bit. To, to do it now. But if there's doubts about whether it's ever useful then We can just drop it then supported layer. Next slide. That's it. Or that may be it. Yep. Yep. Yep. So That's it. Okay. So we we had asked, previously on the list whether people had a chance to review this because We've We, we're we're hoping to do a an adoption call soon on this Jean Marc, do you think it's ready? At this point, or do you plan more significant changes that you think, should be done before the war group takes this on. I don't feel like there's any really major thing that's going to happen that would change, how the working group views this. You know, it's definitely not done, but you know, I would assume that the final thing looks quite similar to how it's now. Jonathan?"
  },
  {
    "startTime": "01:06:03",
    "text": "Does adopting it mean we have to have at least a temporary solution to the where we're putting the binary blobs, I mean, in in the GitHub might be a financer, but I I don't think, a solution is required before option, but, switchers definitely required for publication. But Yeah. And I need feel like we you know, If you have a draft that says, you know, here's the binary blob, we at least have to have somewhere that we complain at the binary Bob, maybe that's on a focused.org site, but We need to have somewhere we can at least play with that marinara bob before have a speck that references a Yeah. I mean, I I think it's an important issue. Maybe Murray has some you know, guidance about it, but, I don't think it'll impact adoption. Maybe it's normal for a draft and not be complete when it's done. Right? So I don't know if IETF has any kind of resource for normative big things. Don't make more work for me just to go Oh, but, I mean, Yeah. You calling? Cullen, Colin Chang. Where where is the blob right? Now, I mean, like, I mean, computer? on to our Mark's So No. No. Right now, the Right now, the if you want to get the blob, you need to get the actual source code, plan is to also add the the links to the draft, but you get the source code, and there's a download comes from a zip.org server because it doesn't fit and get, obviously, But think we just heard Download the source code. But, whatever. right you what for know, It's fine I I don't see a problem. We'll start working on a better way to do it. No. We can I mean, I I don't expect to resolve that issue issue immediately?"
  },
  {
    "startTime": "01:08:02",
    "text": "It's it's an issue that we need to to definitely think about but I don't see any problem adopting the draft before that issue is resolved. Total labor. Anything that would probably benefit from the work group, actually working on that part and not not the individual, figuring out on their own. It's not do a call or do you think, sweep. Yeah. Difficult. Okay. So we'll we'll we'll ask, to go ahead and do a quick, pull here to see if people are ready to adopt this. The, because they should see in your client The adoption question, Do you think that This document the current red version, current draft draft version a good starting point for on the, Opus redundancy Draft. Testing the tool, causing the tool? Okay. think it's, I I pretty pretty clear consensus now. There's"
  },
  {
    "startTime": "01:10:07",
    "text": "In infinite ratio. It's Yeah. K. So, so it's it's clear clear, that people feel we are ready to adopt this draft as a group item. So we'll confirm that on the list But just for the notes, it was, you know, 10 yeses to 0 nos and 2 no opinions in the room. And, didn't see What happened on the chat? But So Jean Marc Prepare, prepared to submit this as work group drafts. We'll just confirm for 2 weeks on the list. And then you can submit it. Okay. Should I incorporate the some of the changes that got discussed today before doing that, or should I upload the current version and then make the changes. Feel feel free in the next week. If you have time the next week or 2, upload, chain make make all the changes that we've discussed. And, at the end of the adoption call, he can publish a new version under the work With those changes. Fix K. Jan, you're up. Oh, it works. Okay. I hope you can hear me. Perfectly. Okay. Great. So, hi. My name is, Janbuter. And I will give a brief update on the, speech coding enhancement graft, draft, draft, There is no update of the draft itself, but there are updates, that are relevant for the specification. So just a quick recap what, I'm trying to do is so there are basically 2 tracks on this, developing enhancement algorithms the other one is standardizing it"
  },
  {
    "startTime": "01:12:00",
    "text": "And, basically for the algorithm development. We want to do Want to get the best method that we can do with the bullet we have now, but for standardization, we want to Keep the door open for future improvements. That means we'd rather intend to standardized quality requirements instead of the method itself. Next slide, please. And this is where we are currently at, so there are To enhancement methods now that have been published, published, published, published, paper version, And, what happened since last time is that two methods are now actually integrated into Opus as part of the 1.5 release, And standardization is naturally lagging a bit behind. So, what happened there so far was valuation of quality metrics from last time and, in particular, modification of opus, of, compare the current office compare tool seems, a good candidate, but if someone has still some feedback on that, I'd be interested, urine it, next slide, please. So the OPUS 1.5 integration what it actually means is there was a further size and complexity reduction compared to the published, published, versions in the paper, Also, now the enhancement method is compatible with all the modes Previously, it only worked for Silt. Now, it also works for hybrid modes. Mode switching to CALLS It's supported. It does monostereo. So everything is there. And it's also compatible with the Neural packet loss consumed algorithm. I also included, for reference, a table of the general resource requirements"
  },
  {
    "startTime": "01:14:01",
    "text": "So, complexity is now from 100 to 400, makeup flops and I dare say it's, it requires, reasonable to net negligible amounts of, overheads on typical WebRTC edit devices, but if somebody has a different opinion, I'd like to hear it, and in particular what this means is it's the first complete implementation of an enhancement method or the a time to message inside opers, fully integrated, and they of calls and prerequisite for continuing with the internet draft. Next slide, please. One quick clarification on the compatibility modes. Is it actually improving anything beyond just silk, or it's just compatible with the Kelton Hybrid modes, it's also improving hybrid mode because silk is The low band for hybrid mode, surely not optimal yet, but it's improving for health. It's doing nothing, but it's not making things worse when you switch back and forth. Okay. And, in silk, it's actually it it also runs for stereo on both channels and that I tried a few examples and that, even improves stereo quality. Yeah, that's what it does. Okay then. Next slide, please. I think this is a good point to raise some, questions about signal in general I mean, current oh, Jonathan has a question. I had a question on the previous slide. So when you're using it in hybrid mode, is the calculptcomponent Done. All over the How does the is the the CAL component added to the Part that's had this processing done, or is it the added How how does that work? Yeah. I mean, you basically have the silk output that's processed and enhanced."
  },
  {
    "startTime": "01:16:01",
    "text": "And then you add The CELT Hybrid. And on the Android side, Yes. There's no difference on the there's no difference on the encode. The encode side is not unchanged. Right? This is It's unchanged. Yes. Oh, now it's unchanged. Yep. Uh-uh. So, I mean, these are, at the moment, still blind enhancement so they don't need any site info, but I mean, with the situation now, there are already 2 enhancement methods out. And there might be more in the future. There might be different versions of better so, the question is Should there be some signaling do we need a naming scheme for Ethan identifying and communicating, methods and versions, should the decoder disclosed what message It will use should the encoder have the power to disable or forbid using specific methods. So these are questions I'd be very interested in getting feedback now or later, Jonathan, did you mean to be in queue? Are you Okay, then then later in this case. Yes. Then next slide, please. So the next steps on the algorithm development side that's would be improving, Speech coding also for bandwidth higher than white pens. It's done now indirectly. Because of the hybrid mode, but it's really not optimal. And there's multiple be done. And for standardization, the next step would be to really Take now this full implementation, and the, pre study on quality metrics and turns this into a first proposal What requirements do to ask from? In in the hospital method."
  },
  {
    "startTime": "01:18:00",
    "text": "Such that, It would be permitted by the specification. Yeah. And that's it. Next slide, I think there's a Thank you coming up. Now, now, now, Thank you. Any other questions about speech enhancement, So just real quickly, I'd like to back up on onto the standardization question, You're currently proposing 2 different enhancements, right, linear and nonlinear. And you plan Yes. If if we want these signal, do they would be too independent extensions, that would be signaled. And there are flags. Right? They don't carry any data. I don't think it should necessarily be signaled in band, but It would be rather be like an SDP thing. But this is this is kind of the question. I mean, But Jean Marc has has an opinion about this. I believe. Yeah. I I I think the main idea there is that We don't we don't specify these techniques like Jan has 2 maybe someone else comes up with different ones. And the idea is Any decoder can turn on its enhancement and get better speech quality And the idea of what we're standardizing is just making sure that how to say that an enhancement is compliant. Like, ensuring that it's not gonna make things worse. So if it meets these criteria, The decoder can enable it with no signaling, no nothing."
  },
  {
    "startTime": "01:20:01",
    "text": "The signaling would be optional if you really want to say something in particular, but normally the encoder sends it For example, you could have a 1.0 encoder, and the receiver has the enhancement, so it just gives you the better speech, and that's it. Guess my question is if you're coding something that very much isn't speech. I mean, guess, presumably you would be using a pure prob probably been coding as pure calculpt in that case anyway. So maybe it doesn't matter, but I'm worried about, like, I don't know, Have you tested what this does to things that are not speech if there's soak in the mix, like, song or you know, percussion or something like that. Not so much yet. It would be an interesting thing. It's, I mean, I mean, monophonic, instruments would probably good candid that. So I think In this case, you sometimes get speech coding, yeah, I will put that on the list That's that's definitely something to try out. Yeah. And I think I think it's a good point that we just don't want it to fall apart. Like, it's you won't expect it to enhance, but you don't wanna ex to explode. Yeah. But, I mean, maybe you'd say Hey. This is something that's for your things, we have some silicon coding here, but it's not steep. So turned off any speech enhancements. Flag. Flag. I think that would have to be in band, not an SDP because This is something that could change over time. It also changed if you're in a multi party conference. For different different audio streams. I think something that's inbound in the audio would make would be necessary rather than putting it just an SDP Period. Thanks. So one final question on what we mean by a speech and what was tested, what what the model was developed with, Is it"
  },
  {
    "startTime": "01:22:02",
    "text": "Is it only trained on single speaker near field Mike only speech. Is that Is that the Yes. Base basically, It's clean speech. It's a clean speech data set, multilingual, but that's basically what it does, what it does, but it's never seen signals from far field speech or multi multiple talkers background talkers? No. It's it it it hasn't. But, I mean, the same is true for bulk orders. Like, what's used for URL PLC. It's been tested on that kind of speech. It's also been tested on real recordings and it, generalizes Well, So from what I've seen so far? K. I mean, even the last study that was was noisy speech for instance that has also background talkers and so on and so on. Mhmm, so so far that work, but it's definitely something that you want in your testing material. So I think, no more questions on speech coding? 10 minutes in the queue. Thank you, Jan. Tim, go ahead. Yeah. I I just wanted to say that The the Inbound flag you were talking about to to turn off the enhancements. If the use cases for content that isn't speech, That isn't really specific to any one algorithm. Right? So that that means we don't need to worry about the naming and the versioning and that kind of thing. It's just a matter of do you do this, or do you not do this?"
  },
  {
    "startTime": "01:24:01",
    "text": "But if there's another use case where, you know, you you wanna be able to disable something because, you know, somebody's deployed some model and it's gotten really popular and shipped to a bunch of devices, and it turns out it has some, you know, horrible problem. Then, you know, maybe you want some kind of you know, ability to turn that off but but but but but but but but if that's not your use case, then then these have different solutions. Good point. Okay. If no other questions, Thank you very much, Jan, and that's the last presentation. K. Thanks. Any other business that people wanted to bring up? In this meeting, Okay. So we'll we'll, confirm the adoption call for the dred draft on the list. We'll set it for, 2 weeks. 2 weeks. And please review the current, version of the extension mechanism, and highlight any issues before we try to progress with the work group last call. Thank you, everyone. Thanks."
  }
]
