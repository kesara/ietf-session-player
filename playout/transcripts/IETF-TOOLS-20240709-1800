[
  {
    "startTime": "00:00:28",
    "text": "e for for those just joining we're just past the official start time but the join rate is still very high so we're waiting a moment for people to get connected all right a couple of minutes in the"
  },
  {
    "startTime": "00:02:01",
    "text": "join rate is starting to slow down I would like to remind everyone um to share video if you can feel free to jump into the conversation at any point we're not using a queuing mechanism there um unless we end up with a conversation that needs one and then we'll explicitly put one in place remember that these sessions are recorded and the recordings will appear on YouTube the link to the notes which form the agenda for the meeting is available in the chat does anyone have any agenda bashes before we start into the agenda as currently framed in those notes all right first I'd like to thank everyone for taking the time to have these conversations they are um very useful to us in setting the direction that we're developing towards I will start in with the an update on the infrastructure transition progress if you're not familiar with the diagram that is pinned just under the title for this section please take a few minutes to um skim and understand what that diagram is saying we've moved quite a lot of infrastructure um since the June meeting the big applications the data tracker the wwwi f.org website mail archive um the uh rsync"
  },
  {
    "startTime": "00:04:01",
    "text": "service and the IMAP service um were moved over that happened on June 20th uh we had scheduled 4our outage it took two we've had very few problems with those Services after the transition um we do have much more visibility into how The Individual Services behave now that we don't have them conflated into one big server um one big service we can see their effects on the systems that they're running on uh much better and we've already identified a large number of changes that need to be made um some optimizations some redesigns um that um will come up later as we're working through the agenda we also moved the authors and chairis wikis that had previously moved into Amazon into digital ocean um I think that may have happened earlier today or late yesterday but very recently so we had planned to move the entire mail processing chain at the end of June uh we got to the end of June and the system was not ready for us to make that cut over so we are changing to a model where we are um finishing the development of the transitional phase of moving the mail system as it is implemented into itfa into the cloud with a minor set of adjustments basically splitting um mailman off into its own service um will be transitioning non"
  },
  {
    "startTime": "00:06:01",
    "text": "yeah after we get the initial testing of this in place we'll be transitioning the non- meeting affecting lists across starting with the test list to prove that um the transmission works and as soon as we have those working we will move other I non- meeting affecting lists before the meeting and then have everything else shift across just after the 120 meeting ends as everyone saw there was quite a bit of discussion um as the proc as the move of the mail processing system didn't take place noting the lack of the initial lack of support of IPv6 um in that mail processing systems ability to send it can receive over IPv6 um Jay responded with a good summary of our intent we always plan to build to as much support of IPv6 as we can um we will incrementally move towards having IPv6 support for any system that we can add it to that our initial step is necess necessarily taking us to a place that doesn't have that support before I move into details on the next few things does anybody have any questions or comments about the main um progress on the infrastructure transition project"
  },
  {
    "startTime": "00:08:01",
    "text": "not hearing anything well while while people are thinking Robert a couple of things I wonder if you could um give us a a brief explanation of um why we are favoring digital ocean now over AWS and what tools we have decided to stop using as part of that transition sure and I see a note in a chat about moving male from cloud flare to digital ocean Maria M never did go through Cloud flare so um as opposed some things moving from AWS into digital ocean the complexity in AWS um is very high it's driven by a market that looks like Amazon itself large e-commerce providers um very large Enterprises that have separated things into um uh many small services and the amount of infrastructure managing overhead to get a simple service running inside of AWS is um incredibly large compared to what we um have been able to do in other services Azure and digital ocean in particular um we've made the shift to move out of AWS into these for um the lower costs of of constructing things and um better control over the costs of operating things as we were making these transitions we also greatly simp simplified the um mechanics that we were"
  },
  {
    "startTime": "00:10:03",
    "text": "using to do deploys moving from something that um was based primarily on Helm to something that was based primarily on customize spelled with a k um this has turned out to greatly accelerate our ability to um get the services deployed into these um environments we currently you know after struggling for several months with getting to the point where we had a a quick way to uh deploy into staging deploy into production inside AWS um we made this shift of Direction in within a very small number of weeks it was um we were finished in six weeks but we were well along the way in three weeks um having cicd deployments directly from our repositories into staging in one service currently into Azure and production in another digital ocean so we've got um just as part of our normal workflow now a uh proof that we are not locking ourselves into either one of these Cloud providers um unintentionally so that we know that we can if needed shift our our production systems from one into the other so J devite were there any points that you wanted me to touch that I haven't touched yet no that's great thank you so when I read your email J and thank you for being honest and direct on this one it looks like for the email as you"
  },
  {
    "startTime": "00:12:00",
    "text": "can guess from my top right we have noeline we are waiting for a there right so theying Eric you're breaking up very very badly yeah there is a storm here could be I will shut down maybe without video it could be better so so far yeah big stor here so that's a little bit annoying sorry so Jay thank you for being direct and honest in your email of second of July it looks like the blocking or the gating factor for IPv6 email outbound is AWS so outside of our control correct I'll answer for the moment yes yes well I mean I think Robert has explained just a minute ago though that we are um this isn't you know the the only um that this isn't a fixed unchangeable decision but this is where we need to go to now certainly and over time we can review and revise certain aspects of this that's not a promise that we will change things but can look at things later I hope it's a promise to chain thing I hope that in 10 years email from iatf will be over pv6 right I don't say not a promise with a timeline but a promise on the end destination please Jay yes I understand I understand if 10 years is fine yes um the the other thing think wanted"
  },
  {
    "startTime": "00:14:01",
    "text": "to sorry Eric go ahead no um I think we are it's useless to talk about it anymore right so it's it it's there it's there I mean let's I will go over it that's no problem we can go to the next topic okay well I I want to think we should also take the opportunity to give Mark from Sirus a chance to uh um tell us a little bit more about um the work Sirus have been doing and where um how that's gone for them over the last month and things thank you Jay I'd be very happy to do that um so the first thing um the reason we didn't go live with the uh mail and the mailing list infrastructure at the end of last month was simply caution um from our experience in moving to mman 3 on uh the Inplay server we thought Extreme Caution this time would be called for um we didn't rightly or wrongly anticipate some of the challenges we had in the end of it um and we didn't want the same thing given the criticality of the systems um it was a very narrow thing we pulled out a it at the last minute but I think it was the correct decision the position that we are at now is that all of the cloud systems are set up and configured and theoretically if you like working um in the way they should be this week is set aside for testing the system my understanding is that beginning tomorrow we are going to be testing or prepping the test um and I"
  },
  {
    "startTime": "00:16:03",
    "text": "believe including Ryan and perhaps some other people from the call here for extracting the mailman data to ietf apps assuming that goes ahead and and we are assuming that goes ahead well um that is the part of the system that we're most confident we have given our experience in the last month or so with the in place migration um once we we got the apps testing done we're moving on to the final system testing so we're going to be trailing the post admin traffic also simulating individual postings via the MX to the outside world and to Maan so the position we're at is the system is in place um it seems to be functioning as we'd expect it to and over the next week we're going to be the testing of it doing the testing of it then a p pause and we should be going live by the end of the month may I address the IPv6 comments as well I need to confirm with my engineering team um but I believe that we have solved those issues on AWS so um I don't wish leave a hostage to Fortune um but we should be uh good with that I don't think my understanding is you've got IPv6 available on the hosts on the on the the the MX host and on the um mailman host and they can receive email over IPv6 but we don't yet have a solution for sending email over at least not sces thank you for clarifying Robert and J yeah thank you thank you Mark and um Mia did you want to ask your question that you put in the"
  },
  {
    "startTime": "00:18:04",
    "text": "chat I was just uh quickly jumping on going a live end of the months I assume that then after the ietf meeting either yeah yes that's right yes okay thanks Robert those are the things I just want to make sure we covered in that section sure so I mention earlier separating the services has given us greater insight into the behavior of the applications um it's allowed us to see in the data tracker that the um PDF iation of Internet drafts um more internet drafts than rsc's is consuming well more than 10% somewhere between 10 and 15% of all of what the data tracker is doing and in some cases there are edges that we have um started to protect against uh calls into the PDF isation routines on some submitted drafts never return um causing the uh worker that is handling the request that asked for that pdfi draft to um be killed um causing a brief red eduction and performance of the data tracker while a new worker is spun up to replace it so we started a conversation about the uh um whether or not we should continue PDF ising documents at all that conversation is still um playing out on ietf sorry On Tools discuss it is crossposted with working great chairs um heavily at the moment if it um continues much further we'll try to f is it back into one"
  },
  {
    "startTime": "00:20:01",
    "text": "place um there is some mixed opinion that I'm seeing on the list so far about whether or not um we should continue to provide PDF ised documents uh but we'll see we'll let that conversation continue to play out I'm gonna jump ahead and note that part of what we've done to mitigate the uh uh effect that generating these has on everyone else trying to do other things with the data tracker is we've added um quite a bit of rate limiting to how many how frequently a single um requestor can request different PDF ised documents uh this is primarily in place to slow down the heavy Rob about crawlers that hit these end points so Robert now that we're in the cloud I I had a little bit of a question I understand the need to mitigate uh attacks that are exploiting the kind of the PDF and potentially the crawler but if the my my economics on like a cloud-based model here is if this is taking up 10 to 15% of the load on the data tracker what's the next what's the cost of moving up to the next size CPU or elastically scaling a little bit that it's it's not taking that versus you know retooling to potentially do that because you know if it's another 20 30 I don't know1 $200 per month per day versus the dev time kind of like what what's the thinking on the break point do we do we understand that enough I don't think we have a a solid understanding of whether we could just throw um a larger resource at it and have it um mitigate the problem completely I don't my instinct without having done the work is that it won't"
  },
  {
    "startTime": "00:22:00",
    "text": "um that we do actually need a redesign at the very least we need to get the computation of the PDF bits out of the HTTP processing path and we're close to having finished that already so currently um in the existing implementation the HTP request pins while that PDF is being built if it hasn't been built already it's also a model where um these things aren't built until they're requested and we're doing the math to uh determine whether precom precomputation of these things make sense um or following um Carston's suggestion at least having the recent documents pre-computed by Computing them at the time that they're submitted um might be a way to move the the cost into places that don't affect the rest of the users um as severely so the just attempting to scale elastically or to apply a slightly higher tier of of compute um is not going to address the issue of um the places where we end up with these computations not returning that's going to require work to protect ourselves against um bad input causing bad outcome and the last comment and this is uh yeah current currently where we are in digital ocean we've applied the best"
  },
  {
    "startTime": "00:24:04",
    "text": "CPU that we can apply already so we have some options if we were to shift where things were to get a higher a a more powerful CPU just for the what happens with a single thread you know sorry a single process on that CPU um into um Azure for for example um this is important because just applying more CPUs more cores won't help with the um time that the computation of of one of these sets of bits is um is taking so thanks um While others are thinking about this there's one other thing um about this that um we have been discussing for a while the the tools team are increasingly getting a chance to see data um sorry Warren did you want to jump in before I go off on something separate um I don't know I joined the queue then I left and I joined the queue again maybe um just jump in don't Q um maybe you just covered this but what I'm not quite understanding is why we're not just Computing the PDFs at submission time slash you know at midnight on a Thursday and just generating them and caching them I think that what you said is that's the plan that's what we're working towards but I apologize one of the suggested alternate ways of doing this for calculating PDFs at submission time if the conversation concludes that we should compute the PDFs at"
  },
  {
    "startTime": "00:26:02",
    "text": "all right um at least for recent submissions and maybe only for recent submissions that provide the XML all right okay because we can't provide we can't compute the XML RFC style PDFs XML RFC V3 style PDFs for things that are five years old and older right yeah but I mean that that that IES at serving time as well I think I was just sort of missing the um why we would would ever compute stuff at serving time not at submission time but anyway sorry I didn't mean to open the Rat Hole yeah how we know it's useful to know how we got here the original implementation of building these PDFs was a well well here's a thing that makes PDF why don't we make some PDFs and see how it works oh this is going to change a lot over time let's just compute it into a cache that expires so that when the PDF thing changes over time it naturally refreshes um after it falls out of cache and gets recomputed with whatever the new computation looks like and it has turned out that this um introduced a heavier load on the entire system than was originally anticipated okay thank you um thanks so go going back to the um point I was talking about earlier we um increasingly the the tools team are um getting access to lots of interesting and useful data um as Robert just mentioned um having all of these Services now split out and instrumented provides a wealth of data about how they're operating um in uh a traditional organization of course that will be just kept within the um within the tools team"
  },
  {
    "startTime": "00:28:02",
    "text": "but this is an organization where transparency is its superpower and it is I think to our advantage if that information can be shared and made available to people within the community but of course it's not something that can just be openly made available within the community there has to be some level of trust and gatekeeping and it's not just about performance data that this is a general thing about data that is collected there um we have had some discussions internally about that but haven't yet had a um come to any conclusion about that and I think I've mentioned it to this group months before but if if anybody gets to the stage where they think they have an idea or they think that they would have some know good reasons to look at that data or don't even need a good reason that we could trust you looking at that data then please you know come forward about that I would like to get to a stage personally I think where some of this data is open and transparent people can access that um because that's you know the nature of the organization we are all right well to reinforce what Jay said um we do have a lot of insight into how the servers are behaving um we're primarily looking at things using the graphon universe at this point so we have um ingestor into many of the um graffan Universe databases and graphon itself as a as a way to see it and if anyone in the community here wants access to that all you need to do is ask us unless anybody has any questions Jay um"
  },
  {
    "startTime": "00:30:02",
    "text": "if you wanted to talk about the Privacy statement so so many of you may not have seen this if you haven't read ITF announce in the last um half an hour um there is a um there has been an long-standing error in the Privacy statement um stating that we don't use browser storage when actually um the uh HTML 5 web storage API has been in use from a number of systems using local storage and session storage and so the um uh privacy statement has now been corrected to correctly reflect that and uh I just want to talk about this verbally to give anybody a chance to ask a question about this if they have any um concerns or otherwise about it great thanks all right so the rest of these sections are intended to be um taken as read and take a few minutes to give people that might not have had a chance to skim through them to skim through them and raise any questions we're happy to to talk about them but we're not going to read through them in detail as we're going um through the remainder of the meeting does anyone have any questions about any of the other services topics that haven't been brought up or any other um uh topics that they'd like to discuss while we're together today"
  },
  {
    "startTime": "00:32:03",
    "text": "the Cod Sprint is happening immediately before um the iatf 120 meeting hope to see as many of you there as can make it um if we don't see you at the Cod Sprint hope to see you during the meeting thanks everyone again for taking the time to join and discuss things today thank you Robert and we'll see you all in in a couple of weeks"
  }
]
