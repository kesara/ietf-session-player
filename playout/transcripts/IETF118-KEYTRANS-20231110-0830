[
  {
    "startTime": "00:00:13",
    "text": "Morning. Thank you Yeah. It might also, this is looking at Yeah. Thank you. Yeah. Alright, folks. Let's get started. Welcome everyone to the, IETF 118 key It's our first session. A reminder, this is a recorded session. So if you don't want to be recorded. Make sure you're not visible to the camera, or you don't speak. Oh, Oh, Right."
  },
  {
    "startTime": "00:02:03",
    "text": "So it seems our projector in the room is not projecting behind me, which I haven't turned around since I sat down, so I didn't notice to the if anyone from the meet echo or staff is monitoring this channel. It's Thank you. God. While we wait, Roman and Paul, if you'd like to you know, say anything at the mic since this is our first session. You're welcome to take make use of the time as much as you want. Thanks. Exactly. I can talk for as long as I want. Good morning, everyone, and good evening, and good afternoon for those that are remote, this is the first time we're getting together. So I wanted to really congratulate related to proponents. And all those that had feedback,"
  },
  {
    "startTime": "00:04:02",
    "text": "through the through the BAF and all on the mailing list that got us to this point. So we have a fully chartered work here. It got a really wonderful. I was so pleased to see kind of all the energy and the interest this topic, and so we are competing. And the other piece I wanted to mention is thank you to both Ori and Jovan who, stood up to kind of lead us and facilitate conversation for us. So you we really appreciate your willingness to I missed a serve here. Thank you. Oh, that section. So I saw You saw the projector led blink. But one month. So for folks who are entering the room, we're setting up the projector and we'll be with you shortly."
  },
  {
    "startTime": "00:06:19",
    "text": "As we wait, the slide that I'm looking Oh, oh, Excellent. Thank you. Alright. For For me, then I've been looking at the note well for some time. But since you're now just seeing it on the screen, a reminder, to review all of the relevant policy documents ensure your contributions are consistent with the note well and, you're not familiar with the note, well, perhaps leave the room, become familiar with it before you contribute anything here. The note really well. So a reminder, to conduct conduct yourself, with professionalism, familiar familiarize yourself with our code of conduct, and if you experience any communication, you feel uncomfortable with follow our processes and procedures, please. We wanna make sure all of the communication and engagement in this this working group is consistent with, the highest quality of communication that IETF can offer. So as I said at the beginning, this session is recorded. Make sure to sign the blue sheets. This is wonderfully beautiful room with lots of space. The next one may not be this large. So please ensure you've signed in, using the on-site tool or by scanning the blue sheets."
  },
  {
    "startTime": "00:08:05",
    "text": "Shivan, would you mind, hiding your video for one sec so that we can have the QR code where your face is. Sure. Harrison. There's no clipboard up here. There should be a QR code at the mic line. There's one right here where I'm standing. Yep. Yep. So you can scan the our code at the mic line, or this, or you can click the on-site tool in the agenda. Indeed, that's that's Also, what's on the slide here. If you're remote, please keep your video and audio off unless you're presenting. And, just lost, our slides for a sec 2nd. I think I may have disconnected. Yeah."
  },
  {
    "startTime": "00:10:01",
    "text": "Apologies. My laptop just disconnected. Alright. And we're back. Alright. And, if you're remote, the use of a headset is, highly required. For recommended. Alright. Just a brief overview, as this is our first session, we have our agenda. Preparation meeting minutes, links, all of this is for you of your interested in these after the Okay. We have a relatively short agenda for today's session, we'll have a problem statement from Brandon McMillan a moment in a moment an architecture overview from Brandon some, presentation from sea. And a privacy and the privacy properties of Keith's transparency from Kevin Louis. Great. The customer control Yep. We're ready to go. Alright. And a password control to you, Brandon. Cool. Thank you so much. So this is going to be physically an updated version of same presentation I gave at IETF116 for the problem statement, just remind people what we are working on. So right now when people Build into an encrypted services, There are a lot of really good options for how to encrypt data. Kind of what their famously is not is a good way to distribute the public keys. For those encryption algorithms, if you take basically any secure messaging app, and you dissect it,"
  },
  {
    "startTime": "00:12:02",
    "text": "what you'll find is that inside there is an internal directory stores and mapping from user identities to the user public keys and directory is not really secured in any way. The security of it relies completely on trusting the service to operate correctly. Which is not great. Because If you trust the service to be honest all of the time, It kind of calls into question why you're using encryption in the first place. Because they're not on a server Would not miss user messages? But if the server is not honest, then it can bypass the encryption really easily easily. By just giving you the wrong public key to encrypt your data too. And then it can reach your messages that way. And of course, the trick is if the server gives you the wrong public key so they can reach your messages. Some that does technically require some action on the servers part. That could be detected. But realistically, you're very, very unlikely to ever actually detect that maps would give you these screens that you can use to manually verify the public keys of people that you communicate with from, parts, but in all of my time being surrounded by people who care a lot about computer security, I think that I've only ever done that twice in those two cases, it was because I was talking about key transparency. So it's a very very unused. And this is a big problem because without trustworthy, key management, It's not possible to have trustworthy encryption. So the the technical solution to this key management problem is something called key transparency. From the the bot requests, key transparency is a safe publicly auditable way to distribute cryptographically sensitive data like public keys, and they're obstruction to think about here, the KT follows is basically a key value database. And so you've got a Keyvalley database, and you have 2"
  },
  {
    "startTime": "00:14:00",
    "text": "main cryptographically assured properties about the data in that database. The first and the most important one is number 1, which is that Alice's key assumed by Alan is the same as Alice as seen by everybody else. Or another way to say that is that everyone has the same view of data. And the reason this is important is because it lets you go from a world where users manually verify the public keys belong to specific relay people to a world where users' devices can monitor their own account for to changes that could be impersonation at Brandon, one one moment as a chair to interrupt I I failed to ask for, scribes for this meeting so I'm just now remembering we had at least one volunteer to take notes. Do we have any other volunteers who'd be willing to help capture what very important things Brendan is saying for a minute. Apologies, Brandon. That's that's my fault. Anyone in the room? Anyone remote? Tivolt, thank you, for volunteering. And thank you for capturing, hopefully, the beginning part of this. Anyone would like to back him up, please, use the note taking tool. And since we have at least one note taker, proceed now, since we've already had prendon, dive in, Go ahead, Brandon. Thank you. Some So, What I was saying is that The key transparency approach is that users' devices can monitor their own accounts for unexpected changes. It could be impersonation. And the example that I normally use to talk about this is If you think about what happens when you log in to Gmail or Facebook, you get an email saying that someone logged in as you. And if it was really you, you just kind of ignored the email which is what makes logical sense to do in this scenario. And it's the model that Keith Teal allows you to follow with encryption."
  },
  {
    "startTime": "00:16:01",
    "text": "Versus this is kind of so that's what we currently do. Is the opposite of what I just said, fun. So instead of telling you that someone you logged in as you. We tell everyone else that someone logged in as you. Which doesn't make sense because sounds, the people that you communicate with are not equipped to do anything with that information something they don't know or care if you got a new device or if your old device got lost or whatever, that is kind of the responsibility that's on you. So he gives you the ability to use this consensus property to monitor your own device for changes like that. And then the second property that you get is that Alice's key today is the same as Alice's key yesterday. Plus anything new. And the reason for that is just so that Alice can go offline for a while and then come back and see that Nothing happened while she was gone. So may be thinking, this all sounds great, but what are you telling me? Like, what does the IETF need to be involved in the problem. So, despite important the problem is of providing trustworthy key management for encryption coming, And despite the fact that Kitty is relatively perfect fit to solve this problem, Katie has never really been seriously adopted. And so you wanna ask yourself why is that? And there's a number of reasons. Some to, First is that KT tends to be very technically complicated. There's a relatively large amount of academic literature in the space. And there's not much guidance on what the kind of right design choices are versus just do what's possible. There are also very few implementations and those that exist are often kind of missing important parts or that are maintained something that people pointed out at the last meeting which was a really good point. Is the p technical difficulties, are all combined with the reputational consequences for getting the KT deployment wrong. So if you think about"
  },
  {
    "startTime": "00:18:03",
    "text": "sitting down and trying to justify to, like, a product person, why you should why you should deploy KT. They're gonna think through it and they're gonna say okay the best case scenario from doing this is that our products Looks and acts exactly the same as it did before. Maybe we get, like, a blog post out of it. But generally nothing's gonna change. And the worst case scenario is that like a year from now, there's gonna be some pop up that says, you know, our company is compromised and This guy is Pauline, and it's gonna scare people, and I don't wanna deal with So, you've got really severe reputational consequences to a Katie deployment possibly if And in terms of security products, Kitty is not necessarily unique in that way. And Lauren having, earned being technically complicated either, but combined together, causes the the prospects for QT deployment to be relatively weak. So the ideal angle here is that we would have a standard and this it would be a standard that would be able to have, a lot of positive impact because it would pretty well addressed a lot of the barriers to deployment that I was talking about in the last slide. So if you have a standard sort of widely applicable protocol description that feeds into Yeah. It feeds into building industry consensus around what KT is supposed to look like, and it helps you figure out the correct choices in the design space. Some some And then once you figure that out, you can get a protocol description that pounds, would get a decent level of scrutiny from security researchers and from the general public. Once you have that, you can have trustworthy and complete open source implementations that people can actually implement it. I know that it's gonna be secure. And once you have implementations, that kind of comes naturally with, better documentation and more people that are exposed to KT and using it. People are deploying it, you have a better understanding of the issues that actually come from"
  },
  {
    "startTime": "00:20:00",
    "text": "The plane kicking. So in terms of actually getting to a standard, for this problem, I need to have 116. Which happened earlier this year that that was our bot session. We had several presentations from people who were deploying KT or working on KT generally. And the purpose of that was, number 1, just to get a broader sense of what people in the space were working on and what was possible. But also beyond that to show that despite the fact that these people were working independently, I found kind of the same problem. They were coming to similar enough conclusions. About what makes sense for solving their problem that a standard would be feasible. So since the bot happened, it was very successful. And the stuff that comes after that is actually aligning that community on a set of common and achievable requirements, which is where we are now. That kind of partially happened in the the charter negotiations that happened in the interim between the boss and this meeting. There are some some architectural requirements which have gotten baked into the charter also most of the session is going to be going through the the architecture document which was shared on the mailing list last month. And train to go kind of consensus around that and also what's gonna go into it in the future. Yeah. So that's the process for getting there. To to get a little bit further into the the charter requirements, since we've been chartered, So one change that we made from early versions of the charter was we switched talking about a service provider. You're talking about an authentication service. So early version of the charter were written in a very, like, centralized service mindset since then we've changed it to referring to kind of an abstract authentication service, and the purpose of that is to open the door to supporting federation."
  },
  {
    "startTime": "00:22:03",
    "text": "In terms of the requirements that we would put on the authentication service. The first one is it has to be transparent, which is kind of obvious from the name key transparency. The second one is that it would be user friendly. So there would be little or no awareness of the system. In the sense that, like, we're not going to assume that users are gonna be out like scanning QR codes. Like, we can provide the ability for them to do that if they would like to, but that's not going to be the critical part of the security story. Like, the QR code scanning should be really, like, an optional fun thing that you do difference beyond that, it should be private. Some information about users in the system should only ever be revealed to people who are authorized to know about that user. Important thing to point out is that that's a baseline requirement. Like, we can't do less than that. People almost certainly do more than that. As, as the protocol actually gets flushed out. Finally, it should be efficient. So it's practical to play. It would be nice if we were able to deploy the protocol. And for the phone statement, that is all I have other name? Awesome. Thank you, Brandon. We're gonna, target about 5 minutes of comments and response on this. You have any comments or questions, please feel free to enter the queue. This is Daniel Gilmore. Thank you for working on this. This is a long missing piece of the infrastructure. So I'm glad to see it. Can you bring the previous slide back Yeah. So This is great. And I like that privacy is one of the top level design requirements. But I see this as privacy, as stated here, looks like it's privacy for the key holder."
  },
  {
    "startTime": "00:24:03",
    "text": "And one of the things about directories of identity information is that if you get to know who's looking who up, You can violate the privacy of the correspondence to the key holder. And I'm wondering whether that's also in scope. Does that make sense? If I'm writing to you and I look it up and I have to go to the key transparency service and say, I wanna find Brendan McMillan's key, And it says, I'm not gonna give you Brennan Mcmillan's key unless you can prove that you're authorized to do so, and it does that by finding out who I am, It now knows everyone who wants to talk to you. So your privacy has been protected. In some sense there, but it's also been violated. And mine because now they know that I wanna talk to you. Right? It means that the that the key transparency service can build a map of relationships between users. So I wonder if that privacy, in terms of refining that privacy target. We shouldn't think also about the privacy of the people who are looking things up in key transparency. Yeah. I have some issue with that. It's kind of unfortunate that when we were working on the charter I had in my mind the idea of, a an application where the contact craft or the The social graph was not secret from the service. So that's not something like I'll put into the charter, but it is something that I think we absolutely be valuable to work on and try to include in the protocol. Hi, Demetrius Agadulan, MITDC. Question for, for the chairs and the presenter. So So far, the emphasis has been more on the Key directory. But if a traditional key transparency, also records a key rotation of So my question is,"
  },
  {
    "startTime": "00:26:00",
    "text": "it be in school? Would this be useful to record something like key rotation events for a JWK Keysight. Are similar. I think that that would definitely be a a good application of it. I would need to read the charter more closely to see if that's something that fits immediately into the charter because the charger is written from the perspective of like chat services and supporting them solving that problem HP? Hi, Phil Hamekah. Yeah. Just responding to DKG. Yeah. I think that we want the the privacy to be a bit stronger in that we want to minimize the information exchanged I'm not sure that we want to put preventing mapping the com communication graph as out of scope Look. KT. Because I've looked into ways of doing that. And in my system, I do have a very strong proof. But it is outside the key KT Peace in that what what I throw into KT is you know, hashes of signatures and blobs So there's really very little information in there. But if you make that privacy requirement too high, then you end up with, tying yourself in knots that you probably don't want to do. So I I I'd I'd suggest weasel language So make it as privacy as possible, but not tie yourself down too far. Thanks. Roman. Hi, Roman. Daniel US AD. I'm shooting from the hip"
  },
  {
    "startTime": "00:28:01",
    "text": "here because there was a charter kind of scoping question. So at the level of action that I just pulled up the charter quite and quickly looked at it, and I heard a proposal JWKs be kind of in scope. I would say the big answer is it depends. What we have scoped ourselves for is to solve solve things in the space of end to end encryption communication services And Brandon exact exactly right. Colloquially, we said that's things like video conferencing services, instant messaging kind of providers. So if JWKs, which are a solution, are kind of in scope for that use case, maybe but I don't think we're scoping our use case as, you know, bound to a particular kind of technology. But, you know, I think that details matter here. Thanks. My recollection from the discussions in the mailing list you know, prior to sort of finishing the icing on the charter was that the various different key expressions could be supported in the would not be limited to only one, key serialization. But, I'm not not making that Yep. Yeah. Dotcom. Absolutely. That is, just sort of the point about key serialization. I think we had a little bit of a thread on, like, let's not lock ourselves in at chartering time, but now we're in a different place kind of in the working room. Can certainly make make those decisions, but it sounded like folks wanted flexibility is what I heard Yep. Alright. I think that's what Alright. Brendan, you're gonna be up again in a second. So Alright. Whenever you're ready. Okay. Cool. So the the first milestone that we have as a working group is to adopt an architecture document for people that are"
  },
  {
    "startTime": "00:30:01",
    "text": "are newer to the ATF. The purpose of an architecture document is to to find the the terminology that's gonna be used in a protocol. And it also defines the the parties that are in the protocol and how they interact together. Believe the older term that used to be used to describe this is an applicability statement because you would be able to to read it and understand that a protocol is applicable to your problem. So what it explicitly doesn't do is say anything about how the protocol actually works. There's no algorithms or message formats. It's all relatively some Just high level pieces fitting together. And in terms of what it means to adopt a document, adopting document just means taking it from the individual who wrote it and putting it in the care of the working group. So the biggest implication of doing that. Is that it would no longer be just, my personal decision had to change the draft and what goes in it. And if we're gonna do changes to the tracks that are substantial would wanna see, evidence of working your consensus behind those changes they're actually made So that That's what it means to adopt a draft, in terms of actually Doing that milestone, I shared a document with the mailing list about a month ago. Which I'm going to describe to you now. So the the first section is parts of the draft that people seem fine with. In the sense that the draft discussed these topics pretty clearly and nobody complained about them. I'm assuming they're fine. So this is, the same style I used at IETF 116 web in the the basic operational model for Katie. You have is a protocol between a client and a server which is basically a key value database interaction, so users can do search requests to look up the value of the key. They can do update requests to change the value of a key, or they can do monitor requests to look specifically for changes to keys that they care about."
  },
  {
    "startTime": "00:32:01",
    "text": "If users try to look up the key that they're not allowed to, or they try change it in a they're not allowed to. Those requests can kind of just be locked without any further consideration, And the reason for that is that the draft I does not specify a transport layer. So The assumption is that all of the the request and the response payloads are carried inside of a higher level sort of application protocol. That will handle authentication and and all of that stuff. So we don't have to. Which was quite nice. And, also, the assumption is that users should Generally only need direct communication with the transparency log. Partially goes back to the constraint in the charter, which is that, There should be little or no user awareness of the system. So We're not requiring users to scan QR codes. Like a sound, like a sound, but it's actually slightly stronger than that because it's saying that clients can for the most part, stick to a really simple client server request response kind of operating mode. Which is, definitely really nice simplification So Well, clients can stick to the kind of client server model is not necessarily acceptable for servers. And the reason for that is because it's possible to make QT protocols a lot more efficient. By bringing in semi trusted kind of 3rd party and the the problem there is that it's not always so easy to actually find a third party that you like and that you involved really deeply in your application. So what the draft does is it supports 3 different deployment modes The first one is called contact monitoring, and it has no third party. And the consequence of having a third party is that everybody has to do slightly more work to make up for the 3rd party that does not exist. And then the other 2, 3rd party auditing and 3rd party management, As the names imply, have a third party,"
  },
  {
    "startTime": "00:34:01",
    "text": "which is involved in slightly different ways. And I also put here some examples of applications generally follow once the deployment move to kind of, So, I guess, exemplifying how they're supposed to work. So Google published a KT implementation a few years ago. And Apple also really recently announced that they are adding KT to iMessage. From both of these cases, they used a single party implementation. So they are probably more of a contact monitoring kind of mode. Whats up on the other hand has been pursuing having a trusted third party in more of an auditor positions. And then for 3rd party management, the examples that I play were certificate Transparency and compulsory certificates. Are not great examples because they're not, chat applications. But I will try to explain that in a few slides. So, to dig a little bit deeper into How each of the deployment modes work. This is contact monitoring. Because there is no auditor or third party to ensure that every change the log may is being done correctly. The responsibility falls on users instead. So whenever a user looks at the key for someone they wanna talk to, they had to remember some information about, what what they were shown by the server. And then in the future, on a recurring basis, they will keep checking in with with with the server and verifying that what they were shown previously is still there. And so the reason that works, sort of intuitively is you think about if the log ads and malicious entry, then it shows it to someone then either The service can try to remove that entry at some later point. In which case the person that got shown it will notice and be mad. Or it can not remove the malicious entry, in which case the person who owns that account. We'll see that it was added and getting that. Come so someone will get mad. You just get to choose who. So it's kind of the the security basis for it."
  },
  {
    "startTime": "00:36:04",
    "text": "Then the the next one is third party auditing. In this case, you have an auditor His job is basically to get upset for everyone else. So users don't have to do that. Sort of recurring check-in with the log. They can assume that what they get is usually authentic. Okay. So in this case, the transparency log does all of the work of actually running the protocol, running the server, communicating with users And what it will do is it will on a recurring basis check-in with an auditor. So as part of, checking in, The log shows the auditor all of the changes that are made to the database. The auditor checks that those changes were applied. Correctly. And if they were, the auditor produces a little signature that's kind of like a a seal of approval. Okay. And then the transparency log can can take that signature and show it to its users. As a sign that it's behaving, honestly. If the log ever does do something malicious, then the auditor will be upset as we've discussed and refused to provide its signature. And without that signature, users will know to stop testing a lot because it's done something that And then the third one is 3rd party management. Now you've got a a manager on the right side. Who does all of the work of actually running and hosting the KT server. Whereas the transparency log here is really just forwarding requests from users to the manager, and enforcing access control policies. So the manager just run the log. It doesn't know what's authentic and what's supposed to go in the log. The transparency log is not actually holding any data, but it knows the rules for what's allowed and what's not. So the reason that I gave CT as an example of this is because you can kind of think of the transparency log. As more of a certificate authority."
  },
  {
    "startTime": "00:38:01",
    "text": "Then the manager is more like a CT log. Mhmm. So, in the web PKI. Users wanna issue any certificate, their request goes to a CA. And the CA does all of the work to actually decide if it should approve the issuance request or not. And if it does, it signs it, by creating a pre certificate and that pre certificate gets sent to CT log, which is more like a manager. To produce an SCT, and then the SCT comes back to the CA. Which builds, a full certificate and gives them back to the user. So it's kind of it's kind of meant to be the same idea. The log is just enforcing access control and the manager's the one who actually keeps the state of the system. Yeah. Because they're the 3 deployment modes. The, next important part of the draft is about out of band communication, and there are two ways that we talk about to do it. So, the purpose, first of all, without of any communication is to detect forks because if you only ever interact with the one server. It's really easy for the server to present you a view of data, which is different from what it's shown everyone else, but which is also internally consistent, So would not be able to tell that anything was wrong, but you would, have a different data from anyone else. So, the 2 ways leaders frame, the first one is peer to peer gossip. This is basically just the normal QR code scanning approach where 2 users are physically together they wanna make sure the devices are consistent, and so they are scanning things on their phone. Going to important point, number 1, The difference between QR code scanning an app that uses PT and an app does not. Is that I don't have to use this KT What's your gossiping with this QR code?"
  },
  {
    "startTime": "00:40:01",
    "text": "Would be information about a tree head versus individual public keys So the reason that you would do that, of course, is that it makes sense are it makes it possible for users who are not contacts. To still gossip together. Or even if they are contacts, cusping tree heads, proves consistency over all of the contacts that they both have. Whether this context is shared or not. And it's also able to cover things that have happened in the past. So if I comparing that you have the same tree head, you sort of are checking that you have the same view of all of space and time for this application. Whereas if you gossip individual public keys, that would only prove consistency for a single chat session. So it's a a much weaker guarantee. And then one second, Wayne. Copy, and Desccribed for out of pain communication is by using anonymous channels. And PND here is basically just that users would be able to fetch tree heads, an anonymous network, And this is really nice because it does the exact same thing actually is here in a very gossip. But it is capable of being automated. So going back to the charter, it's, got the mandate to the user friendly. And the fact that this is automated, It's very nice. The users don't have to do anything. Yeah. And the second important point is the cost bin effectively requires having Let's call it linearizable view. Is what we're gonna cover in the next slide. So I'll this concept of A linearizable feel, companies. Is probably with the draft mentions the least explicitly, But the idea is that it requires users to remember the most recent tree had they've observed and Check that future queries are provably consistent against that rehab. So anytime the user interacts with the server and the server needs to present, a newer tree head"
  },
  {
    "startTime": "00:42:02",
    "text": "What it will do is it will also provide a consistency proof. Relative to the previous tree head that it showed that user. And a consistency proof. Would show that this new tree head, which is being used contains all the same entries as the previous tree had plus some new treat, some some new entries. So, we have a guarantee that nothing was removed. And we kind of always maintain this guarantee going forward. As time goes on, we continue making sure that nothing is ever removed from the tree. For the biggest implication of this decision is that, it requires clients to keep states 7, and that state is the tree head. So tree heads are, like, relatively small, and it's a constant size amount of state. Still, it would not be possible to have stateless clients. And to benefit that you get from doing this stuff is that it makes the out ofband communication much more effective than I was talking about from the last slide. Right. And the reason that you do that is because it creates a really strong guarantee that you've never been shown up for. If you were insuring if you were insuring that Every time you see a new tree head, an extension of a previous tree had If you get your own tree head ever, Then it's never possible to get you back on the same view as everyone else. So all it takes is, The server didn't speak once for that to be detectable for the rest of time. Which is a very powerful guarantee Yep. And it it helps you ensure when you gossip, that you are Coming to consensus on all of the current state of the application and all of the past dates as well. And the second benefit is that in the 3rd party auditing mode, Having a linearizable view actually allows you to have immediate updates despite Potentially the auditor being somewhat far behind And the reason for that is that as a client, if you ensure that you're always on the same view,"
  },
  {
    "startTime": "00:44:00",
    "text": "then you can accept different parts of the tree that have potentially not been audited yet because you have a guarantee that they will eventually be audited. 4 They won't. And will be to text and phone because you will see that the auditor has stopped signing the same version of the tree that you are on. So, yeah, That is all of the parts of the draft that I feel like people are probably fine with. This time, Just mailing list feedback, but I wanted to stop and briefly ask that in terms of what I've talked about so far, not not like things that are missing, but what I've talked far. Are there objections? Okay. Yeah. Hi. Thanks for the presentation, Brandon. So, I'm a I really liked your presentation, and I think, the architecture document is a great start. So I'm definitely not against adoption. I would put maybe slight remarks to what's the whether I have no objections But the interesting things, I had more objections with the document and less with the way how you presented it. So I I find first, I think what you meant by linearize of a view is something that I would intuitive fields that describe as you need to decide how much the lock should be dependent only, right, because there are different notions of a pen only. In some trees, you can rewrite leaves and then you append only with respect to a list of operations that you have on the site as well. And I'd also put what you discussed here as out of band communication more into focus, but you could say that this is a bit of an editorial decision. You you have these 3 actions saying, look up,"
  },
  {
    "startTime": "00:46:00",
    "text": "I don't remember the 3. Right? But look up, monitor search. These things I would really put this getting a consistent view of the lock as a 4th of these central things that need to be specked out I've in the draft, currently, I've handed reads a bit like, Yeah. And and then, also, look, should provide a way to to obtain a consistent view on the tree hats. But, yeah, as I said, I think my issues are more with how it's written, and I found your presentation to address these things. Thank you. Actually wanted to ask you about that. You said com. Those different notions of linearizability my understanding basically every system that I'm I'm familiar with it implements this. Puts all of the data, in a single linear order The differences was something like, I guess, CT, CT logs have the right to take, I think, 24 hours to, like, take a big batch should stop. And then They can kind of pull the big batches out, and then they put it all into line. But everything eventually gets put into a line I unfortunately didn't have time to read all the papers and data, but my understanding was that there are designs where you allow to really rewrite the tree. So when you take an old leaf and you just do change it to something else, and then auditors just need to do a lot of work to check that the new tree actually only contains these updates and no else. So you really lose this. You really lose is one that the old tree is a subtree of the new tree. Are designs that lose this property, which makes auto taking much more costly. Yes. K. Thank you. Thank you. Rowan, your next Hi. Rowan Nace. Hey. So"
  },
  {
    "startTime": "00:48:01",
    "text": "This is not particularly in objection to adoption, But more of a, a, a question that I think we need to answer at the same time as we adopt. And it's It's about the requirements. So we've got obviously, you know, we've got the 3 different deployment modes 2 of which have been used in the wild by real middle large large customers at the same scale, using this for the for sub subsequently the same application of messaging. That have chosen different deployment modes. And we have you know, a lot of the sort of privacy related questions are, well, do you get this property? Do you get this property? Do you get this property? And so I was hoping to see a stronger stronger discussion of These are the requirements that we absolutely are like, we're going to ensure that all of these requirements are met. For any protocol or any you know, any best current practice in Keytrance, these are the ones that we think are are optional, and this is why somebody might choose 1 or the other based on their either operational needs or the you know, to to to the vendor chooses different sets of privacy to suit different end users. Or different you know, you know, customers, whatever that means. 3 Sorry. And Do you, Brenda, do you wanna come on on that for a minute or maybe So just your Your comment is that we should clearly to delineate The security guarantees that a key transparency protocol would provide to an application. Well, that we should say which which security which security requirements and which, you know,"
  },
  {
    "startTime": "00:50:00",
    "text": "operational requirements we have. That are mandatory which ones are optional. Because if we go to developer protocol and we have an architecture and it says, You could do you know, you could do here are 3 models. You could, you know, people do this, people do that, people the other thing, and then we start developing over protocol, n some of the things are useful for 2 of the know, 1a half of the deployment models, then that's a recipe for us to trip over ourselves later in the process. Yep. As I understand it, you're saying it's difficult to reason about the security properties if if we don't, if we aren't clear on this point, among other things. I mean, there there are a variety of other problems that occur leader in the game when you realize, like, Oh, wait. Well, that's a requirement. Oh, wait, is it? Well, where does it say that it's a requirement? We, we, we suffered from this in big time and, you know, in SIP, for example, and that was, like, Yeah. Thanks. I believe there is already a subsection of the architecture draft called security sections or called security considerations that didn't make it into the the presentation, but it basically does do what you said, which is it says we provide this guarantee. We to be for this guarantee. You know, in the case that you use stippling the mode, your security depends sort of slightly suddenly on, you know, non collusion with an auditor or something. Right? And To do specify the rest of the privacy guarantees of the finished protocol. Yeah. I'm pretty sure it says there's a security consideration section. Yeah. Security guarantees. Okay. That was a quote from the privacy guarantee section of the architecture by the knows you haven't read it. Shivan, go ahead. Yeah. Just wanted to quickly one ruined as well. With my with"
  },
  {
    "startTime": "00:52:00",
    "text": "not much air add on. I do feel like parts of the architecture document could be clear in saying that this is what is okay, and this is what is not okay. And the architecture document is a good place to do that. So, yeah, I think it'll be great if the document is more caring. But it's, you know, inside the scope and Inside school. Thanks. Chris Chris Patton. I got a little lot. Actually, I had like similar question, but I I got a little lost there. So let me just ask Brandon Is there a core set of security and privacy requirements for which each of the deployment models are all isomorphic basically. I believe so. Yes. And that is currently written in an architecture document. It says when a user searches for a key, they're currently to receive The same resulted in the other user searching with the same keyword received. If the transparency log, you know, doesn't do this correctly to give you a fake result in either You detect that immediately and you reject what the Transmitter has said, or your client permanently enters an invalid state is detectable by how to pin communication So they're all supposed to be isomorphic with that as you have this transparency guarantee. Of, I see the same things everyone else and I don't see the same thing as everyone else. Either immediately notice, or I eventually notice So do we expect different deployments Like, do we expect a 3rd party audited 1 versus a 30 party managed one to have different, private properties you can get Like, if you if you choose one over the model over the other, what's the advantage beyond operational considerations? That is a great question."
  },
  {
    "startTime": "00:54:04",
    "text": "My immediate thought is that the 2 third party modes are gonna be the same in terms of Uh-uh. Privacy with a third party. You're gonna leak the same information to the 3rd party no matter what, but it's just, Operationally may be nicer to integrate the 3rd party in different ways. Cool. Cool. And then back, my my initial question is, are we designing 1 protocol that will work for all deployment models, or do we want to have one one protocol per deployment model. Do you have a nice have a vision there yet? Or do you Division is to have one protocol that can do all three. Awesome. That's great. Thanks. Hi, Johan Baker. Yeah. I'm following this. There might be some simplifying assumptions that could be take it. I did try to formalize this type of area about 6 years ago now. And I think that the key thing that you need to look at is work factor. Only what you get here is a different type of work that to than the typical computational work factor we used to. Because the magical thing that happens when you put something into a notary log is that the work factor for forging that particular assertion goes from essentially 0 2 infinity, at the time that you put it into the log. If you have an assertion saying, Obama's Public Key is X signed yesterday. It means nothing. If you have that assertion signed when he was a Harvard student, That's actually quite useful data because somebody would have to have either anticipated the value of that attack or found a fine time machine. And so if you're looking to formalize, I think that that's a method. It doesn't necessarily"
  },
  {
    "startTime": "00:56:00",
    "text": "I mean, there's a slight weirdness in this group in that we're talking about a particular technology that's really general Sticking stuff into notary logs is really useful but we're only looking at it for validating keys and that might be skewing our analysis somewhat. If you take the two things apart and say, What are the properties of notary logs here? And how do they apply to keys over here. I think that you'll find it more productive. The other point I'd make is that There is only one no tree Chain, and it's not a chain. It's a lattice. Because in my system, every single user maintains their own notary chain. Every service maintains their own notary chain. They cross notarize. Now if you look at the constraints on that, It's a lattice in that Each chain can link to any other chain And when you think about the, properties of that, you know, everything is eventually going to linked to everything because a connected graph is always going to be more powerful. So the graph is gonna connect. The any question is gonna be whether you use the fact it's connected not not not not And then when you're trying to analyze whether this particular assertion was made in this particular time interval, I after this date, And before this date, You can set up a situation where the you're essentially us analyzing that relative to one chosen No free log. And if that is your own notary law that you may team, the ultimate source of authority for that is yourself. And so there are some very powerful statements here. But"
  },
  {
    "startTime": "00:58:00",
    "text": "You're probably not gonna get them to work in Tamarin as it is currently configured because it's really 90 degrees to what it's designed to do at the moment. Shivan, Siobhan, Mike Cochair? Yes. Just wondering, Brennan, did you wanna finish your presentation first? Or do you think it's worthwhile to silence this discussion right now? And everyone in the queue, we will get to, the question of should we adopt this document after Brendan's presentation is over, and then we have 2 presentations on the security and privacy properties of key transparency. But, yeah, just leading it up to you, Brandon, if you wanna take all the questions right now, or do you wanna finish your presentation first Unless we're pressed for time, I would keep going now because it's a conversation a few slides that I want to Frankfurty Clarity. So so keep going with the presentation and then do the questions at the end? Do the questions now or finish the Q and Sounds good. Yeah. We'll continue to process the queue. Kevin, you're up. Yeah. Oh, sorry. Okay. Cool. Right. I just wanna comment. I think, there are a lot of really good you know, questions coming about the superior impressive properties. And I hope we we will cover them, next few presentations because I think it's logging me once behind it. And although it may not be reflecting the architecture document. I think at the very least we can start in some of these discussions and having, like, structure way to think about these different properties. And then the comment that I originally wanted to make was about the linearizable view side. I think that's one thing which I'm hoping that maybe we could relax that requirement, to some extent, because the implication of requiring, clients to store some amount of state. Maybe some applications can deal with that, but, like, for example, in WhatsApp, we decided it's, explicitly not to do that because"
  },
  {
    "startTime": "01:00:03",
    "text": "like, when users have multiple devices, if you, like, check for the latest state on one device, do you sync that with your other devices, And especially if that, like, implies, like, if if you, like, forget through the check, then it has some security implications. That ends up being, like, not a great thing to rely on. So just going back to, I guess, the question of, going for a single deployment that works for everyone. I think it's gonna be hard to do that just because different products have different requirements and there could be some possibility and, like, deployments that can allow for clients to remember state and deployment for the K. Yeah. I should say that you don't have to synchronize the linearizable view between all of saying devices that a client has, like, each each device can maintain its own view separately. I think we can take that offline. Steven, Stephen Farrell. So we have kind of history of kind of key servers that go bad in various ways. So we have kind of history of key servers that can go bad in various ways. Maybe In this case, there's new ways because they're producing proofs as well as, answering questions. Is there a kind of a vision or plan or something in the architecture document that that kinda says, when that happens, how do the users get back working again. There is not something that's actually something that I thought about earlier is we need to add a section to the architecture draft on life cycle management. So so I think that would be a good thing. I mean, because there there is history of key servers that go away or become unresponsive or be full of crap are In this case, there's new cryptographic ways they can fail. So having some model, I don't know if it affects the protocol, might, it might affect how you do Federation phosph I think having some vision for how a a gazillion users can stop using one thing and start using another would be good because it does happen, I think. Yep. That's a great comment."
  },
  {
    "startTime": "01:02:03",
    "text": "Jonathan, good morning. Can you hear me? We can. So I was wondering if we have a a backup So if there's some content that is legally required to be removed from the tree. Right? Like, somebody has decided some court has said, you know, this is copyrighted content or or sorry. This is a this account is copyrighted. So it's sharing copyright's content, so you've got to remove is there a way for us to a clueed site from the tree other than throw away the whole tree and create a new one without that account in it. We'll the the beautiful thing about this being an architecture document is sort of a protocol document. Is that I can basically assert that there needs to be one without describing how to do it. And but is is that is that in scope? Is there a are we gonna, yep, say there is a way of removing something from the tree without breaking all the proofs. Yes. That's text that needs to be added. Thank you. Yeah. And I'll just interrupt. There was some discussion on the list around the, you know, compliance requirements around removing information from logs. So you, you may wanna review some of the discussions on the list if you're interested in that topic. Alright. I think we've drained the queue. Brandon, Right. Okay. So the the second section of my talk is feedback from the mailing list kind of go over what, what we were told there. So most of the feedback that we got from the list is actually about text that was missing that people want added One that I thought of myself earlier is that we probably need support for field sender type mode."
  },
  {
    "startTime": "01:04:01",
    "text": "So, some intuitive applications have support for what's called Sealed Center, which is basically a way for people to communicate Anonymously, So from, with the key value obstruction that we've talked about so far, have to log in as one user and then you have to look up the public key of another user. And, like we just talked about in the queue, that leaks the social graph, which can be potentially sensitive So We should, register text describing how KTA How you would potentially deploy KT in that scenario. Other text to be added is the the life cycle management, which was mentioned earlier, So, like, if a log dies or if you just wanna create a new log because you don't like the old one need some secure way to migrate from the old one to the new one. We also need to add text on how federation would work as well as, on privacy low compliance or the compelled deletion of user data. So if we get, like, a request saying that some user needs to be deleted. We need to be able to handle that without to strain everything. So that's missing tax. The one of the substantive questions that came up on the mailing list was about immediate updates. So currently the draft says that requested changes are play to the log immediately. Really, what that means is that we would not provide What that means is that the protocol we write would have no way to provide basically interim inclusion groups, kind of like an SCT if you're familiar with certificate transparency. The reason that we would not do that is primarily because it would simplify the protocol if you don't have to standardized to types of proof for everything where one type is subtly less secure than the other one. And it also supports deployments that want, a very strict KT regimen where they only want the stronger"
  },
  {
    "startTime": "01:06:03",
    "text": "type of proof. They don't wanna deal with the weaker type of proof, like an SCT versus an actual inclusion proof and then CT log. So my understanding is that the argument in favor of having interim inclusion proofs. Usually comes from a concern that a Katie server is not fast enough or reliable enough for the broader application, and so people don't wanna risk calling an outage because the KT server maybe isn't sequencing changes. Quickly enough. They wanna be able to produce, like, a signed promise or something functionally, what a sign promise is is it's just But Delaine, full verification, And if KT reliability is a concern for a specific application, I think that application can probably implement KT in a way that delays verification without that need to be a core part of the protocol. Like, if you're expecting a proof to be somewhere, and it's not and you're not an application that follows a strict KTE sort of regimen. You can keep using the public key immediately anyways, and you can come And you can, like, go back and for the proof in an hour or something. If that makes sense. Right? Like, we can design a protocol that is very secure and provides very strong guarantees And then people can implement it as loosely as they they need you. Right? Like, So there's a free spill to verify or something. That is, you know, ignored or delayed or retried later. Play on. That's the gradient. That's This is actually my last slide. So that's the question I had because this is That was substance abuse. I've forgotten the list. I wanted to ask for people's opinions on this as well. Awesome. Thank you. Yeah. So, based on"
  },
  {
    "startTime": "01:08:01",
    "text": "what we've seen so far, would anyone like to come to the mic and express their views, you know, regarding this particular document, Do you have any objections to the document in its current form? See Daniel. So I'll comment maybe just on the the last slide and question. It's my understanding Our so maybe for background. I'm Danny has us from Broadband. We are deploying a version of transparency, not based on this document, but that we've been working on for a while. Mostly based on phonics, and the in our model, we we do do sort of intermediate proofs as think you called it, or interim proof. Sorry. And the the reason is because in my mind, key transparency is, anyway, detection only mechanism, right? Not it doesn't prevent a server from serving malicious keys. So just allows, the user who owns the keys to detect later. Right? So yeah. Even if you do this. Or even if you don't do this, that's still possible, I would say. Because, e even if the key is in key transparency, the owner of the key might not have checked it yet. So you might as well, do an interim proof and use it immediately. But I I see your point that maybe that doesn't need to be specified. Explicitly in the document and can just be done by an application."
  },
  {
    "startTime": "01:10:00",
    "text": "But, yeah, that's my you. Yeah. That that is more so that, most KT implementations today, I think, are actually capable of Integrating new entries really quickly. And the fact that they are capable of integrating new entries will be quickly sony sort of obviates the need for like an intern proof where I'll give you, like, a SCT really quickly, and then I'll give myself, like, 24 hours to actually update the tree. That's just kind of not necessary. Thanks. Does anyone wanna opposed, to adopting this document. Would you like to come to the mic state the reasons why. Just for context, this is, this document is our first deliverable or, we do plan to adopt a document around architecture. So If you don't think this document is ready to put option, please say so now because we really holding an adoption call for it. That is at least the plan. Let's see here objections other ways. Because the chair synced that so far, from the discussion, it seems like there's generally positive feelings towards adopting it. No. Gilmore just wanted to say, I support adoption of the document. Yeah. Thanks for working on it. Thank you. Rowan. I supported option And the thing that I said in my previous comment at the microphone, I think wanna do that. At the same time as adoption and not wait until after it's adopted. Also, I just wanted to add that in also in favor of adoption, and thanks for working on this. I think it's great to have a standardized version of this instead of, like,"
  },
  {
    "startTime": "01:12:02",
    "text": "everyone, including us doing their own thing and, also, perhaps for the benefit of open PGP, it would be great to have something like this standardized as well that we could potentially make use of for you know, openbgb key key servers as well and and so on. So, yeah, Thanks for working on this. Roman Genuity. Sorry. I couldn't find the button on my screen. Rohan, can you clarify what you meant? So you're supporting adoption, but not an current form until we adjudicate what you said, or you say, we should adopt it and then immediately make sure we worked on what you described. Until I I would feel more comfortable if somebody, you know, Brendan, somebody else took a stab at at least saying, Okay. Here's a strong person set of set of the requirements that you're trying to address. In keytrans in keytrans and put it in this document or wrote it in another document. Does that make sense? You know, simultaneously with adoption, but you know, you know, you know, you know, you know, that there is that there is some text that people can say, I agree or don't agree with. And not wait until later. But But but just to clarify, not the text in the current document, additional text. In the current document prior to adoption. Additional text in the current document, you know, as the document is adopted. Rich, Rich, I'll let you. Yeah. Go ahead. It's instantaneous. It's like a quantum event. It's either It's Adoption is instantaneous. It's a quantum event. It either happens or it doesn't. So you can say I would like something to change before we adopt it, I wanna see what happens. Or let's adopt it's adopted and then we can work on it. The other secondary bug Yeah. The key point"
  },
  {
    "startTime": "01:14:00",
    "text": "is adoption means we start work on it as a group. Not not It's almost done. Let's stamp it. Thank you. That in in my experience, adoption is not a quantum event usually something that happens on the mailing list over a week of a period of a week. So can be addressed during the the period of that week with the assumption that we are going forward with the and unless there's know, objection on the mailing list. Does that make more sense? So Yeah. But that means in the general terminology you want it changed before the adoption call was made. That's my understanding as well. So, So we'll need to take, this conversation to the list, regardless. And then, of course, as as is the case for all IETF business, it will happen on the list. So you'll have a chance to to give those comments there. Let's Felix, please. Yeah. Hi. Thanks. I I wanted to discuss the requirements that Ron brought up I I wonder what you're missing because there are 4 requirements that weren't laid out in the beginning in charter, right, and one of the doc and one of the requirements is private. And I think even the objections that have been made towards, oh, maybe we want to also protect who is looking up something. I I mean, that makes sense, but I think this is in scope of what is written in the charter. So I find the requirements nicely specific and broad at the same time. And and I find them sufficient to, like, later say as you pointed out, oh, this is what we want, right, so that we can refer to them. Thank you. Simon. Morning. And freak program in Mozilla. I just wanted to pitching with Rohan, I think"
  },
  {
    "startTime": "01:16:01",
    "text": "I think there should be, clarifications on what the privacy requirements are because what DKG said is pretty obviously a privacy requirement, but also in previous deployments, people would cross sign keys, which leagues social graphs, So what do we put in? What other specific guarantees that people and service operators get and don't get. That's should be in the document. Thank you. Honest. Yeah. I've read the document. I think it's great. You should adopt it. And, and, of course, there will be further discussions and text edit later on like, always Thank you. Alright. It seems we've drained the queue. Siobhan, do you have any comments? I think, yeah, I think maybe or you and I should talk with Brendan and, see if there's something like, instead of you know, text updates we can do. That and surface that to the list. And then you know, then do an adoption call. Right after. Because it did seem like a couple of people thought that there's some few things that I'd like to see added to the Ukrainian privacy sections. Brandon, do you want wanna ask any questions while we have folks in the room here, or do you wanna just take that to the list I think I'm going to take it to the West. Yeah. Alright. Cool. Thanks a lot. That is pretty helpful. So, yeah, I think we're ready to move on to the to the last presentations. Thank you, Brandon. Awesome. Welcome, Esha. Your"
  },
  {
    "startTime": "01:18:00",
    "text": "video is coming through. You're welcome to proceed at any point. Thanks, Zoe. Can you hear me okay? See a little bit louder. Okay. I'll try to speak louder. Better. Okay. Sounds good. Yeah. Thanks a lot. So we did I'm getting a bit of an echo. Can you hear me okay? We can hear you fine here, I think. Okay. Okay. So I wanted to talk about some of the security. So there were already some great discussion about security and privacy requirements of a key transparency system. And, Kevin will talk about the privacy requirements later. So in this presentation, I wanted to surface some of the dimensions of security properties that with with might be considering for Katie So, like, Brenton already covered, and so this is just a very quick recap that in key transparency systems, the service provider maintains a directory of some user identifier to the public key metrics. And the way users interact with the director service is by, 3 So either it has a search or hookup for its own key or for some other user's scheme. It can update its own contact. It can also monitor and audit the log, the transparency log for consistency. And roughly speaking, the service provider for the any protocol that been considered is that the service provider bills tree, some sort of tree marker like tree, and the she and post the 3 hit per epoch. And each of the clients, when they look up a key, along with the key, they get a cryptography proof of correctness."
  },
  {
    "startTime": "01:20:00",
    "text": "From the service provider and that they can verify against the latest tree hash produced. And if the proofs do not check out, it for reasons of error. Okay. Sorry. I'm getting a lot of echo. I'll try to rejoin. Sir? That's No. Just just continue talking, please, Sasha. Okay. You can you can take your headset off while you speak. And if you We can hear you, though. Is this better? It's it's fine for us. But you won't be able to hear this, maybe. Okay. It's special for me. Alright. Okay. So roughly speaking, the security property is are allowed to dimensions, and I want to see if there's agreement on that. So one is the correctness property, and this was in Brandon's draft already. Which we, I would say, is when the law of operators behaving on team, what properties we expect, from the creating system. And then the 2nd property, which would call consistency properties, what properties we would expect when the operator is potential diminishes. So the correctness properties, I think, that's somewhat straightforward, which is when a user looks up their own key, to resolve the receive, should be the same as the result that any other users searching for the key should should Yeah. Would get And the second is when a user modifies their own key, The other users will see this modification the next time the search for the key. Oh, this recording. So I'm just trying to see if there's a anyone in the bridge who needs to be muted, but I don't see any open microphones. So The headset may be close. So you can move perhaps move the headset. I'm okay. Yeah. I switched it off. Okay. Awesome. Awesome. With"
  },
  {
    "startTime": "01:22:00",
    "text": "proceeds like. We can still hear you here. like what you Okay. Okay. And then for properties, I think, at a high level, what we would expect is that when when a malicious, service provider log operator doesn't follow the correctness properties, this should be detectable. So at a very high level, when a user looks up a key and the result they receive is not the same as the result that any other user searching for the same keyboard. It will be detected. Likewise, when a user modifies a key, but that change is not reflected, and others users don't say it. That should also be reflected, but, detected. But I think these are very, high level, consistency properties, and there are a lot of, subtle dimensions of it that some of it already came up in the discussion before. So I would try to give, quick summary of the dimension that I think are interesting and we should be considering. So here are some of the list listings of it. I will go for it one by one so what the first is dissemination of 3 years. The second is what state the clients need to keep this this were already discussed some of the other things that were not discussed is when the inconsistency should be detected and by who. And there are, other things of 3rd party auditing and owner signing, which provides a slightly stronger consistency property. So the goal of this presentation is to just surface up these dimensions and discuss and see what seems to be the reasonable set of requirements. So the first point that came up and Brandon did a bunch of discussion on this is, the dissemination of tree heads. And this is basically that all the users should see the same tree head for the same. And this is important for in consistency. Otherwise, the server can fork the view, keep 2 different tree heads"
  },
  {
    "startTime": "01:24:03",
    "text": "to the owner of the key and recipient, and thus get away. So there there was gossip was already discussed upon So one way to disseminate this tree heads among the users up through Boston but there's also another mechanism which is a third party bulletin board. Where the server could post this free heads on a third party bulletin board where the clients could consume it there are 2 possible options that I wanted to surface up here. And the second dimension that I wanted to talk about is the state. So what state the clients need to keep for, consistency to to work? Or for inconsistency in kt system to be detected. So the first part of it is the key owner state. So let's say the key owner, Bob first thing to notice here is that only the key owner will be able to authoritatively decide if a key distributed on their behalf was a fake key, So for this, Bob has to look up their key history. That you give history to the service provider is maintaining And each time Bob changes his key, he has to check the change was correctly reflected. In the law. So these are some assumptions on the state of the key owner. And it's also required that Bob actually remembers the epochs at which he changed or updated. So those are set of requirements that the current Kd system almost all kt systems have on the key owner state So one question is, is this a reasonable set of assumptions? Can we weaken this is it just reasonable, and we should make it a requirement. Another, dimension of con state that I wanted to bring up is contact state. So it's not the key owner state itself, but the keyholder may also require to keep some state for other users or for its content for an systems need to be deductible and some Katie systems that are"
  },
  {
    "startTime": "01:26:00",
    "text": "proposed in the literature actually have this property. And in this example, for example, Alice is receiving Bob Alice might need to remember the last key she has seen for Bob or the version number or pass some more auxiliary information. For the inconsistency to be detected. Again, this is a question of is this reasonable assumption and and does it make sense? So that's another dimension. And the third thing to talk about is who did the inconsistency and exactly when so Again, something to notice that at least 2 checks need to happen, an effect he is distributed on an owner's behalf. So in our are examples that Bob is the owner Alice is the receiver of the key. So Alice first needs to at least needs to check that the key she received for Bob is actually logged in the tree here. And when Bob comes back online, he needs to see this fake key that was distributed on his behalf. To be also locked in the tree. So at least this to get need to happen for an inconsistent inconsistency to be detectable. And there are two ways And in consistency, it can be detected. 1 is immediate in the sense that whenever pop comes next after the fake key distribution happens. And does a key monitoring key immediately detects that there was something wrong. That the fake he was distributed But there's another, weaker version of inconsistent action where Bob cannot actually immediately detect it after he comes back online. Additionally, the checks needs to happen by Alice or possibly by other users for this inconsistency to be detected. And there are proposals in literature that achieve this weaker consistent property. Cribs, cribs, Okay. And then the other thing to talk about is the 3rd party auditing. So Brandon already"
  },
  {
    "startTime": "01:28:00",
    "text": "talked a bunch about this and this other deployment modes. So what I wanted to talk here is the the notion of 3rd party auditing from the lens of consistency checks. So third party auditor is expected to download and authenticate the logs content. And they are trusted to run this correctly and attest to the result. But something I would like to note here is the 3rd party auditing is added for efficiency. So if it's not a trust issue and it's not a concern for consistency since the clients, if they do not want to trust a third party auditor, they can also download this audit proof and check it themselves. And finally, the last property, to bring up his owner signing. So some of these skating systems have some version of it, which is the user, there's either a user level key or the device level keys that are required to sign the next update. Now, when the user of apps are, you know, routed secure apps on new device are required, sign that update. And that gives a slightly stronger consistency property in that if a malicious server published a tree head at a certain time. And a certain user's device got compromised sometime after that. Even then the client who holds that tree head will not accept any keys that the user's device did not actually authorize before it got corrupted. So it gives slightly stronger, consistency, and there are systems that also if that meant. So that's fine. Takeaways, but just I wanted to surface up the difference. I was settled security different, dimensions. Would be great to discuss what would be a desirable set of properties and all of this provides this combination of this and provide various trade offs, and that's, I think, interesting to discuss Yeah. Thanks. Thank you. It's a wonderful presentation. Any questions?"
  },
  {
    "startTime": "01:30:07",
    "text": "It's not in the room. Seems we've had a lively chat discussion as well. Is there anything from the chat that should be surfaced at the micro phone or to the rest of the group, Alright. Thank you. Alright. Excellent. We can hear you. We have your slides. Go ahead when you're ready. Thanks. I think there's some echo. So I'm gonna, like, turn down the volume on out. And if you need to tell me the thought or something, just, like, wave your hands and keep looking at me. Anyway, Awesome. Okay. Cool. So have a good one on Kevin. Gonna be talking about the proxy properties for key transparency. And similar to issues talk on this is really meant to be kind of like an overview or maybe like, like, here are some like, the interesting things that we should probably discuss about privacy. Not necessarily, like, strict recommendations of we should do this into this, because I think there's a lot of nuance behind these, topics. And so I hope that at the very least, and you, everyone, for joining up for this live discussion. I think it's been really great to hear what what people's concerns are in terms of privacy, which you talked totally earlier. Okay. So let me get into it. So, yeah, for this talk here, here's a brief outline of the topics I'll be covering. First, I'll start off just with a recap and the motivation, around the architecture and key transparency and why actually I think should be extra extra careful and coming up with"
  },
  {
    "startTime": "01:32:02",
    "text": "designed for key transparency that respects you to privacy. Then I'll go over some common examples of how Merkel trees and VRS, which stands for variable all the random functions, can be good for privacy, but still might not cover the complete rate that we want for key transparency. And this is going to be related to also how there can inherently be trade off between privacy and performance or efficiency for the for the various designs that we consider for key transparency. And then finally, I'll cover some challenges that we should think about as a group regarding user data deletion and retention because that's also something that came up in the mailing list. Alright. So, yeah, as Brendan covered and issue reviewed as well, Basically, so in key transparency, we're kind of assuming that there is some service provider. The service provider has some database that maps user identifiers. Which can be like phone numbers or email addresses, to the user's public keys that are typically used for say key exchange in an end for an encrypted messaging app Right. And the basic operations that the service provider supports where the user are one, being able to add a new to to be able to add a new entry or update existing entry and database, And this is, you know, typically corresponds to, you can imagine the user adding a new device to their account. To search and lookup, which is when a user wants to look up an entry in the database, the server checks first that they have the permission to do so, and then returns attention to the user along with some sort of, like, proof of inclusion that this is actually the entry in the database in the unique entrepreneur database. And then finally, there's like a monitor or an auto operation in which a user can do a mix of either checking the history of updates to their own entry and making sure that the survey has been representing them properly. As well as checking that the service provider has been operating honestly by not trying to rewrite history or, like, deleting old entries, for instance,"
  },
  {
    "startTime": "01:34:03",
    "text": "And for each of these operations, I wanna emphasize that the service provider can always run own access control checks, for instance, to make sure that Like, a user is monitoring their own data, And so that an adversary can't just start trying every possible phone number querying the server to see if it's been registered in database. Right? Because, like, kind of, like, the the the the the focus for privacy here is the server has this, like, high value database of all of the user identifiers that have been stored with a service. If this were just to be made public, like, today, when there's no key transparency, this is like kept strictly server side. Usually. And in key events, currency, we're kind of like, making some portions of this or, like, inclusion proof based off of this public, and what we want to be really, really careful about is that we're not starting to leak the social graph or we're not starting to leak any, like, information about how often users are updating their keys. And and I'll, I'll go into some of your examples of, I think we could potentially fall into these kinds of dangers, but specific design choices. Yeah. Okay. And I guess another thing I wanna just simplify the examples here gonna be assuming that in this key transparency design, we're talking about, there's the database for the service provider, and it's represented by, let's say, single gigantic merkel tree. Since this is predominantly the kind of structure that exists for most proposals, for Casey out there. Okay. So, the first example of a potential, like, privacy leakage, has to do with the serving of these proofs of inclusion. That a service provider sends to users when they look up their own entry. Right? So let's say that a user wants to get the latest entry corresponding to their own identifier. And a service gives back the entry along with some inclusion proof. The way that this is structured is that we find the node in the tree"
  },
  {
    "startTime": "01:36:02",
    "text": "corresponding to this entry, and then we return the list of its sibling nodes along the path as his proof. And now immediately, just based off of the number of nodes that's containing this proof, that already reveals, like, the approximate height of the tree. This means someone who is able to collect a bunch of these inclusion proofs and observe changes to their size over time can already get, like, a rough estimate of how many users are being added tree over a long period of time. And now you might say, technically, this isn't so bad because like, the way these muffled shoes work is that when the tree was really large, the height changes exponentially less and less frequently. So an adversary, you know, when the tree already has, like, say, millions of billions of nodes, might not be able to observe these deltas over time. But this is just illustrate an example of, you know, some amount of privacy leakage and how we should probably be careful when we're designing schemes that were aware of exactly what we were leaking every time we're getting out, these inclusion proofs that we're serving user queries in general and acknowledging that it isn't too much a more serious privacy issue is that actually, these inclusion proofs also leak the neighboring leaf nodes in the Merkel tree. Meaning that if we just issue like a plain inclusion proof, users not only get to see their own value, but then, like, the hash of the value belonging to any node that is, like, positioned next to them in the Merkel tree. Right. And this particular problem can be addressed to some extent with the use of verifiable random functions, which kind of like randomize the location of these nodes so that these inclusion just revealed that there's some neighboring node and not, like, a node associated with a particular user, is something that we'll talk about later when we're talking about mitigating of these privacy issues Okay. So example 2, I want to talk about with regards to how we plan to deal with repeated and with repeated updates to the same entry, So for instance, let's say that a user"
  },
  {
    "startTime": "01:38:03",
    "text": "is updating their entry 5 times in a row, right? Is it revealed based on our design to other people in the system that there's been some user that's been updating their entry 5 times repeatedly. Or does it look indistinguishable from the case that, like, 5 different people updated their key 5 times? And this is kind of like, settle issue that crops up when we're talking about we handle version updates for notes in the Merkel tree, you know, do we keep modifying the same known tree thereby leaking disinformation, or do we hide it by maybe like creating 5 brand new nodes in the tree instead each time that update happens. And this, by itself, might not seem like such a big deal to leak, But in aggregate and depending on how publicly available this information is to glean, you know, people might able to start collecting these high level statistics about user behavior in the system, like, you know, producing, like, these frequency distributions of, oh, this this particular user Although they may not know like, which user it is, it's updating their entry every epoch or something. And, and that in general, could, I guess, not clear if that's something we wanna protect. Maybe there are some deployments which care about this property and don't wanna leave that information. Maybe there are others which do not care about this properly, and we'd be okay with it. And the reason why I think that to some extent, this is a good discussion to have early, addition to the points that are mentioned before is that can really affect the design, and the performance of the system. So choice of whether or not we keep adding new nodes every time an entry is updated versus just recycling the old ones can have I guess, like, a large impact, and it'd be good for you guys earlier. Oops. I set the slide. Okay. So for the 3rd example, wanted to talk about, leaking past update history when user identifiers get reused, So this, I haven't seen discussed too much yet, but,"
  },
  {
    "startTime": "01:40:02",
    "text": "For instance, popular messenger apps like signal and WhatsApp, rely on phone number as a primary identifier for users. Right. And one kind of like issue with phone numbers is that they're often recycled meaning that you can own a phone number and then your telephone service provider might decide to assign it to a different person after you, like, giving up their phone number. And in key transparency, we're likely gonna be supporting operations where a user can get the entire history of key changes for their own identifier or at least history up to some point. So does this mean that should that they should be able to look arbitrarily far into the past to see all the updates that have been made for every user that it's owned their particular phone number, or perhaps can there be some way to, like, limit this information maybe when a user explicitly, like, I don't this is, I think, in general challenging to address, but if a user has explicitly said I'm recycling this, you know, I I'm giving up this phone number can make the service provider not reveal any history of how often the key an updating or any usage patterns to the next users of that phone number, and then for the 4th example, so the first three examples were more about How does the service provider keep private information about the database without leaking this information to use this for a query. And for the 4th example, I just wanna touch off, upon also that When clients make queries, the service provider is inherently learning information about client behavior here, so for instance, if, clients are asking to look up this a specific key very frequently, then the service provider can technically see this, and they might learn some information like this user is very popular. Right? And to some extent, this is kind of a privacy problem, even if you don't have fee transparency, because the server is maintaining this database and can monitor"
  },
  {
    "startTime": "01:42:01",
    "text": "how frequently certain entries on database are being requested. But with key transparency, you know, we're also asking clients to query for their key update history. So wanna make sure that, you know, to that with whatever design we come up with for transparency, we're not leaking more information that we then we intend to leak. Okay. So that was just kind of like a really fast review of some examples for things that I think we should be probably thinking about in terms of privacy for Keytrash privacy designs. I wanna switch over to talking quickly about the tools that have been discussed in the literature for addressing some of these property these privacy leakage problems So, the first one that comes to mind, of course, is verifiable random functions or VRS. And the way of ERF works, is basically There is, the service provider can pick some, and this is gonna be a private key that's kept server side. And there is a function of, say, BRF, which takes the server key and a phone number or a user identifier, and it outputs some random looking identifier along with a proof. And then there is a second function, verifier, which takes some public key associated with that server key. Also the identifier or phone number the random ID and the proof and outputs true or false. And basically the point of this verification function is to say, okay, this random ID and this proof kind of prove that it's associated with this phone number. And this isn't really, like, the most in-depth explanation, but basically, way that bureaus are used to preserve privacy and key transparency that the position of the LEAP node, instead of using your raw phone number, what people do is they pass the phone number through the BRAF, they use the random ID as output, to determine the position of the Ethan and the Merkel tree. And"
  },
  {
    "startTime": "01:44:03",
    "text": "as I mentioned before, the advantage to this is now know, when you give out these inclusion proofs, you're not directly leaking the values for the note next to you because that note next to you probably represents, like, a phone number that or some use identifier that, like, chronologically, is associated with or, like, it is near your own identifier. And the point of your oath is now we we are kind of like positioning the, these identifiers in random parts of the tree. And this kind of technique for privacy preserving, of these inclusion proofs was introducing conics. Back in 2015. Oops. Sorry. Yes. Okay. So I did wanna mention that there some limitations, though, to the usage of BRS. For solving these privacy problems. So for instance, things that aren't fully addressed are, for example, what happens when a user wants to update their value? You know, I haven't mentioned this before, but should we keep modifying the existing entry, or should we create new notes, because though using a BRAF, if you keep modifying the existing entry, technically, someone can observe through these inclusion proofs that Okay. I don't know who this node belongs to, but this person keeps updating it around That's, like, some information that's being leaked. One option that has also been suggested in the literature to address this specific issue is to instead of just passing the phone number or to use what I it's TWF to also give it a virtual number. So now you pass the user identifier plus some version number to the BRAF so that every time the identifier gets a new value, it actually corresponds to a completely different viewpoint. That isn't And, yeah, so so this was kind of a privacy improvement that was introduced by the seamless paper in 2019. And then I I guess still, like, you might say that this is not enough because"
  },
  {
    "startTime": "01:46:03",
    "text": "the server is still holding this verifier key. And essentially, all these techniques are predicated on the assumption that this private case never leaked. What happens to me does get leaks, technically this means that now all these inclusion proofs can be like reverse engineer to figure out exactly what identifiers or, like, which positions the tree these nodes were in. And so there is, I wanna mention 2 recent ishworks that have been thinking about basically, what happens when we deal with, obviously, or, like, the the security copies of the BRAF itself, One is called vertable 0 all sets. Where basically, There is a way to be able to rotate this private key for the BRAF without having to reconstruct tree entirely. And I I'd mentioned a a link down here for a post quantum security address. Happy with it too. Leakage, but more about, you know, if we So, basically, in the entire key transparency design, it's mainly based on hashing constructing marble cheese, which are post quantum, or quantum resistant, but the VRFs that we use are based on what's occurred. At the moment. And the most efficient ones are, and there's been some work recently to adjust on bound by inducing post-one bureaus. Okay. And then just checking on time. So this is kind of like a fun theoretical idea, but one kind of like very theoretical way to kind of get privacy instantly or or, like, Technically, you can, like, write any kind of the statements in the forms and knowledge proofs. And so there's these things called ZK Starks, that can be used 40 inclusion proofs, 4 dependent proofs, but I think it's still an open to it's still an open to these questions to see if this can actually be a real practical to put it I see there is a question, feel free to interrupt me. Alright."
  },
  {
    "startTime": "01:48:02",
    "text": "Hey, Kevin. Nice to see you. This is Rowan, on the previous slide, you mentioned, you know, post quantum implications And could you please clarify for everybody that this is that this wouldn't this need to be an active attacker? Is this something that could be you know, harvest now. I'm assuming this is not the harvest now decrypt later situation that this is more like a post quantum signature issue that you would need to kind of be actively involved in order to support these to support that algorithm. Yeah. I think that's a good question. So Ultimately, it depends on how we plan to be publishing these proofs. So for instance, if we are taking all these proofs that involve the VRAP oppa, and putting them in some, ledger that is available for forever, then that's kind of, like, the harvest now is already done. And actually, Yeah. I I actually haven't thought too much about that question of what kind of damage to redone if did have access to a quantum computer, but I think that's a great into certainly think about. I don't know if anyone else wants to chime in with with an answer to that, I personally have not. Daniel. This is Daniel Gilmore. I don't have any concrete, concrete proof of this, but it seems to me that if the secret key for the server is leaked by a quantum machine that it would break these privacy guarantees. Like, I I I think it is more of the that of the, like, harvest now decreased later sense. Like, I think I think if you if you were to take a copy of this log, which I think could get. And then you were able to learn the secret keys. I think you would be able to re reverse engineer the identifier. From that. Information. So I do think it is I mean, transparency properties would not be lost. It's the user ID privacy properties that would be lost."
  },
  {
    "startTime": "01:50:01",
    "text": "Right. Yeah. I would not be surprised if that were true. Awesome. Go ahead, Kevin. Okay. Cool. So where was I? Just two slides left. Yeah. Okay. So ZK starts, directly done, but popping that practical. Oh, yes. Okay. So handling the installation. So I think is pretty important, and I'm hoping that we can have more discussion about this and I'm calling the mailings as well. So The Merkel Tree notes that that we're using to constructs the our that we're publishing along these encryption proofs. Correspond to hashes of user data. And one kind of way in which we've proposed for handling data deletion. And to clarify what when you are data deletion is, user explicitly requests the service provider to, like, remove all information, forget that this user has everybody's story or ever existed. Right? And So one way in which we could conceived about handling this is to, you know, the tree just deals with the hashes of the user data. So let's just delete the raw user data and leave hashes or, like, the remnants of the hashes in these inclusion groups or something like And there is a question that, you know, does this suffice for actually doing user data? Can the service provider actually, like, reverse engineer later on these hashes, and I think that's certainly something that we should continue discussing, but I also wanted to bring a couple more dimensions to this, which is, 1, you know, right now, when we're talking about these transparency logs, usually we think of, like, these unbounded list that keep growing. And history has never deleted from. And and maybe that be because this is dealing with user data, you know, should service a part as so should service require providers be required to complete this complete history of all data updates. Because some senses, this is like if you register with a service, like, 5 years ago, and then you want to be forgotten"
  },
  {
    "startTime": "01:52:01",
    "text": "are you should you be concerned that there's, like, the service provider that still has a hash or, you know, if the VRQ gets leaks or a post confuse or a quantum your quantum obviously disabled server versus the VR of private key, then know, that might be something that, maybe instead what we could do is allow service providers to, like, forget some history or, like, explicitly just to lead this data. And there are different ways to approach this. 1 is to, for example, like, reset the tree. So, like, maybe the service provider has a policy for after 2 years where it's gonna clear all history and restart again. Now this has some issues with, like, what happens if you're caught among the boundary of right before the reset and right after the reset, but maybe there's some different more, like, gradual ways in which we we can relax So, maybe instead of having a security property that you can view your entire history of all updates to this identifier forever, you have the key transparency guarantees for some period of time, let's say, like, 2 years. And then all over the kind of is done or because it's not relevant to the user to check this, that might also be, like, having, like, a trade off that you make for privacy. As soon as go ahead. Honest. Yeah. I I think you you have to clarify the user data a little bit he, in this context, use it as it refers to the Kia, the the Username key identifier and the the public key, not other user data. Right? Yes. Exactly. Right. So, so I'm just talking about the user identifier and the public key, which may not seem like a big deal, but it's still, like, evidence that this user registered with the service provider And that, that metadata, I would say, I guess, is something that we may wanna protect, or users may not be okay with having service provider's store indefinitely. Thanks. Daniel. This is Daniel Con Gilmore. So with the certificate transparency logs, they don't have an infinite lifetime."
  },
  {
    "startTime": "01:54:02",
    "text": "And I don't see why these should need to have an infinite lifetime either. We obviously can't protect against somebody who copies the whole tree and keeps it around from keeping it. That's not something we can do. I think the straightforward design is just that each tree has, you know, as as part of the Merkel tree has appointed to the previous one, and you keep 2 trees around at all times. Or end trees around at all times. If you wanna make it smoother, and then when someone joins, you know, even if they join right on the cusp of a transition, their original action is from you know, tree minus 1 tree, you know, T -1, and now there's tree T, And so you you can keep the, you know, the window sort of grows and shrinks at the at the epic cadence of a transition between trees. And I, I think that's a reasonable thing to do. I don't think it's reasonable to ask providers to commit to indefinitely growing storage. Thanks. Can you speak? Yeah. Watching this, it I realized that, my mental model was, of what's of what's being applied here is different I suspect that there may be other people as well in that there are really 2 fundamentally different ways that you're gonna a approaches problem, and it's not clear to me that one is right and the other is wrong. It is important that we understand which we're talking at So the two ways are you can either take the data and compete compute the data over the tree over the data itself. Or you can do the different is, which is what certificate transparency is doing. Certificate transparency is computing a chain over the incremental changes to the certificate issuance and which of Mobley choose know, they both contain the same amount of data. But which one you choose is going to mean you"
  },
  {
    "startTime": "01:56:02",
    "text": "present more information to the outside world. So, other question that comes up is What proof do you want to give from this tree and to whom in that, what I would like to see is that when I connect to somebody else on signal, I see a little box saying, yes, this is Fred. And Fred's been on signal now for 6 years. Now that's something that I can then potentially verify myself using something in my plan if it's a Delta lot. If it's, log over the Dell data, maybe not unless that tree is self instantiated in a notary log somewhere. So just a heads up that we need to be careful about which model we're talking about at which time because, otherwise, we end up with a lot of confusion. Thanks. Scott, 3 minutes left. Kevin, you want to go ahead? Yeah. Yeah. I think I just have one slide left, but just for for the question. Yeah. I I totally agree. I think I'm not exactly sure I see functionally the difference between how those 2 from models will be in quantity because we are taking snapshots that are epoch. So the snapshots of the current state kind of I think are equivalent to the income or suffix, but definitely, like, the, in, in terms of the, supporting the functionality, like, being able to see, oh, is Fred, and he's been with us in over 6 years. That, you know, depending on how we designed this that may or may not be revealed. And so that's something to consider, I think. Okay. Sorry. Let me just go to my last slide. So Takeaways. Basically, yeah, I I hope to kind of illustrate the picture here of know, managing this kind of privacy, which can be really tricky. A couple examples that I brought up for how we might be"
  },
  {
    "startTime": "01:58:02",
    "text": "potentially inadvertently leaking information if we choose certain designs. Not necessarily an exhaustive list. And I'm hoping that we can start thinking about these issues sooner rather than later because the designs that we end up choosing, we might, you know, do this kind of thing because it's more performing or more scalable, but it ends up creating less sums in the tree, but then less limits in the tree means worse privacy or something like that. So having these, at the very least, these requirements, endeavor homes matching would be really useful. The second point I wanna mention is that, yeah, I think in terms of, like, a good design, I would love to see, like, we have, like, really good clarity around what exactly we were leaking. Almost like provable guarantees, you know, like, I I I feel like we would say, this design, this inclusion proof will leak something and then nothing else. And the important part is the and nothing else. That we can, like, very clearly quantify Okay. If you get, like, thousands of these inclusion proofs, you won't really learn too much. And then this kind of information could be presented to service routers should they choose to adopt, one design over another or, you know, were talking about, like, 3rd party auditing versus contact monitoring and, yeah, I guess that was my third point. Basically, I think that depending on the requirements of the service provider, they can make different choices about what is okay or not okay to leak. And I still hold some skepticism that we will be able to come up with one design that will all and has all the ideal properties because there's inherently trade offs between privacy performance. For teacher's parents, you know, hoping that at the, at the very least, we can have some possibility in our in designs, maybe like a more part of or a less private version depending on the setting. Awesome. Thank you. So, we're basically a time here. I'd like to thank all of the presenters. I think the presentations were excellent. Thank you to everyone who came to the mic to ask questions. And thank you to our note takers. In Roman, you"
  },
  {
    "startTime": "02:00:02",
    "text": "No. I just want to reiterate what you just said. I I think we just had a really great launch. So thank you to all the proponents going into speakers to setting us on on the right foot. Yep. Yep. And, special thanks to my co chair, Siobhan, who's been making all of this super smooth. So thank you. Do you have any comments? Oh, no. I think that went pretty well. Awesome. Thanks. And this will conclude our 1st session of key transparency. You all for coming. Excellent."
  }
]
