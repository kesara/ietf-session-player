[
  {
    "startTime": "00:00:17",
    "text": "[Music] [Music] [Music] hello everyone we are getting started now so welcome to the second meeting of the privacy enhancements and assessments Research Group since our last meeting we have a new co-chair and thank you Chris for joining us so this is the note well you\u0027ll probably have seen this or will see this a lot in the in the upcoming week please please know this well should we have a jab escribe great thanks Alison do we have a note-taker no taker no takeover great can\u0027t see you but okay did anyone have any comments on the agenda or is there any people who would like to propose amendments to the agenda I\u0027m hoping not okay let\u0027s get started then our first talk is by Ian oh yeah the official Twitter hat hashtag is te arg in case anyone wants to tweet hi okay my name is Ian I\u0027ve come here from tor project this is my first ITF so nice "
  },
  {
    "startTime": "00:03:17",
    "text": "I got to talk about some work I\u0027ve been doing a tour project and how I\u0027d like to generalize some of the principles from that to apply them to the wider Internet and I\u0027ve started working on this draft oh dear okay the the slides are not quite scrolling properly but okay so I\u0027ve been looking at internet measurement since 2013 I worked on the past wider tool for internet transparency measurement I\u0027ve been contributing to tor projects since 2015 I want to just set the the scope of this early on this isn\u0027t about the ethics of internet measurement that\u0027s a very very wide broad topic this is specifically about safety making sure that other users of networks that you\u0027re measuring don\u0027t come to harm this is a working definition of safety the is going to end up in the draft and I have a github issue for sort of revising this and working out how we can improve it there\u0027s a lot of related work in this area some people have already pointed out other related work I didn\u0027t know about on the PRG mailing list if you know of other related work that I haven\u0027t heard of please do let me know there\u0027s lots of people in different communities have come up with their own assessments of safety and ethics in their measurements but there hasn\u0027t been much inter-community discussion on this so I\u0027m going to give a little bit of the background where I\u0027m coming from this isn\u0027t gonna be a talk about how tor works but hopefully I\u0027ll give you enough that you can understand what\u0027s going on talk primarily is producing open-source code and then there\u0027s a volunteer run Network and this network provides security privacy anonymity it\u0027s robust is authenticated it provides integrity depending on how you use it it gives you these properties and we need to monitor this network and make sure that it\u0027s working scaling okay that is healthy and one of the things that we measure from this network is the number of directly connecting users at the moment we have somewhere between 2 million and 8 million according to a recent paper and daily users of Tor we can use this data to detect censorship events if the censorship in the country then maybe the number of users of Tor goes up if there\u0027s attacks against the network maybe Tory\u0027s blocked in a country the number of users goes down we can evaluate when we make changes to the software how is it affecting the performance of the software and see how it\u0027s scaling the philosophy that we\u0027ve tried to follow is that we only handle public non sensitive data and each analysis goes through a rigorous review often by academics before publication of data or analysis and we\u0027re guided by the Tor research safety board which is a group of academics and tor researchers "
  },
  {
    "startTime": "00:06:18",
    "text": "and tor developers and they basically a service for members of the Tor wider community to assess whether or not research that people want to do on tour is safe the three principles that we try and follow a data minimization source aggregation and transparency so data mineralization is where we try and capture only the the least amount of data possible to answer the questions that we have and the level of that detail should also be as small as possible so it\u0027s not just about limiting the the properties that were captured but also the resolution of them the in in the tor network we capture at relays and the relays are operated by volunteers we have distributed trust against across those relay operators and they are doing the aggregations before submitting any statistics so also we make sure that the the raw numbers are existing for as little time as possible and then we throw them away as soon as we have our aggregate and everything is open-source we publish design documents technical reports on how we are doing things and hopefully people are looking at these and if they spot any problems then we\u0027re very happy to fix them so going back to the general case the shortcut to making sure that your honor your honor network and you\u0027re performing safe measurement is to have no one else on that network so using simulations using test beds this is a pretty sure sure way of guaranteeing that you\u0027re not going to upset anyone unless of course you\u0027re running a really big simulation and upsetting all the other researchers that they can\u0027t use a simulation machine so one one case study comes in unique users of Tor the easy way the the web analytics approach is you\u0027re tracking all the IP addresses you\u0027ve seen and then you\u0027re working out how many unique IP addresses you saw over a day in 2010 we came up with a method of measuring the number of users in tor we didn\u0027t want to count all of their IP addresses and we saw there\u0027s a little bit on how it all works the first step of a talk line connecting to the networks it needs to have a view of the network so it reaches out to a directory server to get a list of all the currently running tor relays and we can count this list of sorry count the number of directory requests that are made and from that infer the number of tor users so we don\u0027t handle IP addresses at all this we have to guess how long an average session lasts because tall clients will refresh their view of the the tor network every few hours but what we do get to see here is we get trends even though we don\u0027t have exact numbers so that\u0027s how we get this graph and this this is in an area of problems known as "
  },
  {
    "startTime": "00:09:19",
    "text": "the count distinct problem where you have a number of things you want to know how many unique things there are across this data set and a few methods have been developed \u0027iv of doing this in a privacy-preserving way one of which is hyper log log from google this was originally designed where you have really large data sets and you want to count the number of unique items in it in a probabilistic way or fad or a really large data set is one IP address we don\u0027t even want to have that so this can be adapted for that use case to keep track of the number of IP addresses we\u0027ve seen proof count is another system with distributed machines with counters and they use secret sharing to submit their results to a what we\u0027re calling a tally reporter and then the aggregate from the network can come out but the individual accounts can\u0027t be disclosed and another another system is private set Union cardinality and I\u0027m told that currently this is computationally infeasible but I hope that in the future this this might also be an alternative and there are a whole bunch of others as well report and proclo from Google and prio from Mozilla so all of these schemes exist and there\u0027s lots of work going on in this area so I\u0027m hoping to generalize all of this into this draft and the next steps that I see is coming up with a really comprehensive general considerations checklist that someone could go through and make sure that they\u0027re complying with I want to have an introduction to literature on on the secrets sharing and multi-party computation systems but I think really what I want to do is describe what the properties of those systems should be and not refer to specific systems because I\u0027d like this draft to remain relevant for quite a while we need to think about future computation power that might be available so if you\u0027re saying okay it\u0027s fine everything\u0027s a es 2 5 6 encrypted and we threw away the key maybe that works for a few years but then aes-256 is broken and then you need to rethink your strategy I want to have some discussion on obtaining consent from users and often in the case of internet measurement especially if you\u0027re an IXP and you\u0027re doing pcap captures you\u0027re not gonna get consent from every user but you might be able to get proxy consent from maybe the major ISPs that appearing and they have an idea of what their users want this may or may not be something that could be considered safer I\u0027m gonna have discussion on it nonetheless and I want to ensure that all types of possible harm are covered so in some cases thinking back to a study that I did with EC n transparency through the internet we found there were some Reuters that you send an e CN packet through in their crash they refuse to root future packets now thankfully those riches are gone but that has the possibility of crashing someone\u0027s home router which is not "
  },
  {
    "startTime": "00:12:21",
    "text": "something you want to do at scale so all of these issues are up on github and I\u0027m very happy to receive any any comments discussion pointers of existing work and I\u0027m happy to take any questions now as well or any feedback or suggestions should you have any questions i guess if there are no questions asked Wendy and I\u0027m sorry John\u0027s coming up Jonathan Holland CloudFlare have you ever done any metrics where any tests where you compare the accuracy of your les d√©tails measurement to a more detailed measurement yes so going back to the the two million number hmm there was a test done with the new proof count strategy and they\u0027ve discovered eight million unique IP addresses per day and when we revisited what we were really counting in that graph we found we were actually counting the number of concurrent users so while we had thought that that was the average session length it turns out we were wrong there are in fact eight million ish daily users by unique IP address but then that also doesn\u0027t consider not and a whole lot of other issues so even counting IP addresses exactly isn\u0027t gonna get us the exact number sir but it\u0027s reasonably accurate or yeah reasonably accurate and most importantly we can see trends thank you I still felt it so I had a quick redo draft and I think it\u0027s the thing and I hope it finds a home somewhere whether it\u0027s in purity or something I think it\u0027s a good piece of work to continue and I guess so does just positive feedback basically okay thank you I guess I have a question too I mean had to what extent are you trying to reach out and get input from other people doing kind of large-scale surveys on the Internet definitely all feedback is good feedback the large scale measurements small scale measurements there\u0027s obviously different classes of measurements so you\u0027ve got your active measurement where you\u0027re sending probes out into the internet and maybe talking to other people\u0027s servers in which case you might incur the bandwidth costs or whatever and then you\u0027ve got the passive measurements which is large large scale but also as a doing active measurements is often a kind of a block list that they keep of who not to bother talking to because they complained one second I think is that covered in the draft I didn\u0027t they there is an issue about keeping a do not scan list yeah and working out what the best practices for that should be yeah yeah I think that\u0027ll be a good topic to cover if you can okay cool for remote folks can you please have the name and affiliation clearly when you "
  },
  {
    "startTime": "00:15:22",
    "text": "come up to the mic yeah sorry so I have another question you were mentioning the or if account is primarily the system that you currently use in this or project to do all this measurement and I think there were discussions in the past about potentially using prio have there actually been at the experiments done using prio for a tour and can you comment on potentially the differences between prio improve con okay so I can\u0027t comment on what the differences are I don\u0027t know the system pre or that well but what I do think prio is in the browser yes right so we currently don\u0027t have any client assistant metrics at all tor browser does not have any telemetry all of our telemetry is in the release so if prio is more suitable for climate system attrex I\u0027m not sure it\u0027s it\u0027s possible that we might do that at some point in the future but it\u0027s also possible that the community might not want that at all ever so yeah sure whether any remote questions yeah all right well thank you Anne don\u0027t cry cry she ate it like that\u0027s moving on to the next speaker right now your name yes I\u0027m here I can hear me yes awesome things so thanks for the opportunity my name\u0027s Ryan guest I\u0027m a Software Architect at Salesforce I work on our security and data privacy terms my email is up here do you want to reach me after or I\u0027m also available on Twitter siobhan Quinto next slide okay looks like there\u0027s a little lag on the presentation reckon see in there remote video where you\u0027re at so one of the things we\u0027ll talk about is is posted aces really two things one some of the techniques is for identifying personal data and application logs where a SAS provides we have lots of the enterprises using our system when I strike a balance between providing log analytics tools to developers analytics folks as well as our customers so we actually send our logs to customers of our system so they can get an idea what\u0027s happening in their organization then your next slide you want so the non-identifying personal data we have really two techniques one is store "
  },
  {
    "startTime": "00:18:23",
    "text": "a dictionary base and this is a general-purpose technique that can be used across many domains we use across acquisitions as well so with a new company joins are merges with us we have a torch our general purpose tool we can run against their log data or each type of data store the main thing here is we use common names we\u0027ve started with the top Leonard census names over a certain amount of time we also look at different location identifiers US states is a popular one and that can give us insights on to where potential customer data may be leaking into blogs next slide we also have a couple formats unique to our domain so we have user IDs we always know start with zero zero five all by 15 other alphanumeric characters and then various different types of custom identifiers is you can think of such as a DES SDK a piece or I\u0027ll follow a certain format and so things unique to our domain you can use that to grab her search for logs to find data and we use these to constantly monitor what\u0027s going on our application logs and we can derive metrics for these to see how well we\u0027re doing it anonymizing personal data next slide so then from there we go and look at you know after we found data or potential place or data maybe enter your logs how do we go and we anonymize that or then we have I\u0027ve put together a collection of eight of the most popular techniques we use we use a little more than this but really what we try to do is develop a tool set that we share with developers so they can they can have these and they can know so windy is one on one to use another so may seem self-evident but data deletion is is the first one we start so there\u0027s certain class of data that we don\u0027t want to see at all so things like Social Security numbers credit card numbers we don\u0027t want any of our downstream systems to have to deal with these so just drop it like a star next slide our next class of data is data masks there we take an input and we replace it with just six asterisks gives you insight and if a value is no or not this really helps in serve the user experience or UI type applications where you want to show user that a value is entered but maybe not exactly what it is next slide another technique uses aggregation so we generally bucket records together if we "
  },
  {
    "startTime": "00:21:24",
    "text": "can to avoid knowing exactly what causes in the record this is really popular when we talk about things like error messages in our system so a lot of times we don\u0027t care what you generate an error message but more how many times it happened so we\u0027ll minimize everything we can from the air and just bucket by it that gives us insight into how things are doing without much easier generate the error will cause the exception next slide next somewhere that is called generalization so where I live the ISPs are chronologically numbered every residents so give me an ISP give me an IP address you can drill down the exact location that someone is one of the requests our marketing team was they only track ip\u0027s but really they were using it to figure out sort of what the top countries using their system work so we\u0027d only sort of reduce the granularity and just leave the Geo IP location service and just bubble up in the logs or analytics what\u0027s the country that I need access from well I also get the same information without how many get very specific next slide a similar thing to IP address that we\u0027ve done is ok way to use category so this use case is popular with our performance team would it look a response time our service is from different places around the world really what they want to do is know how far away the client is from our originated data center and track performance there so using IPS really want to know is if you can a bucket of what the distances and I use that and and report on that or do analytics on that well I have to know exactly where that location is next slide tokenization is also something that we use we keep a key value store of that set of tokens over a vein that we define an everywhere downstream we replaced the token we replace the value with the token that gives us the advantage of serving things like right to be forgotten requests or get her attention requests where after certain amount I want to forget everything we know about the user just delete that token without having to worry about changing it in a bunch of immutable data stores so if we have a "
  },
  {
    "startTime": "00:24:26",
    "text": "central key value store we can just pop that token out everywhere the data is generated next slide I believe there\u0027s a talk later about differential privacy so I won\u0027t go into much detail but essentially that\u0027s important to us is for certain categories of numerical values what we do is we add a amount of noise along a fixed distribution and we use that to having some privacy and so when we have large amount in doing more complex machine learning models the noise really cancels out and you can hide it of a set of data without having your exact details exposed next slide and the last technique I\u0027m going to talk about it is encryption with a set of access controls so ten trustus is AEST to sit for encrypting data but really brings the access controls and key management provide some unique opportunity to do some cool things so you can have any keys for a customer for a specific tenant in our system also for a service and you can then decide how often you want to rotate those things and how long do you serve retain older legacy keys after location so you can do things like if you wanted to do sort of wide access took over the whole service you can just delete the key and then the downstream systems would be unable to decrypt the data so has some really interesting properties or you know for forcing data privacy especially over a certain time is most useful to us so next slide and thank you for letting me present some of the work we\u0027re doing we think it\u0027s some pretty interesting stuff I have my contact informations on the first side if you\u0027re doing similar things I have feedback we\u0027d love to hear from you and I\u0027ll be available to take some questions thanks Ryan I know it\u0027s really late in California are there any questions I had a question Ryan so the Salesforce planned to publish any of this work in an academic paper or like a research report we like to figure out right now don\u0027t figure out the best way to do that whether it\u0027s sort of open sourcing some tools or in a more sort of academic white paper setting we definitely want to share some of the things we\u0027re doing and get feedback on how it can prove them yeah you have a question benkei duck and don\u0027t wanna get too far "
  },
  {
    "startTime": "00:27:28",
    "text": "in the weeds but you mentioned some bits about data encryption and sort of the key management for that and I was wondering if there was anything interesting to say about you know having the keys available for different services or you know different levels of the service are different levels of access to different services since presumably the data is only going to be encrypted with one key at a given time but maybe I\u0027m wrong about that yes so one key but with the idea that scope towards the application logging use case in the same log line there may be different he\u0027s encrypting the data so a log may have sort of a service specific encrypted value and then ciphertext that\u0027s interpreted may be a user specific value and then yeah access controls around those we use a a variety of things but most important for that is we have a server complex permission system that\u0027s built on top of our platform where we can say you know who can access certain things and yeah and we found that to be pretty powerful all right thank you we have one more question hi Ryan this is Pallavi I\u0027m from Salesforce too so I had a question not about the hire like you know encryption of the data but of identifying customer data because I think to identify the data you must be following certain regex or some known type of searches so I think that would also be of interest because those need to be tweaked on a regular basis just so that something doesn\u0027t slip through the cracks and because if if the identification doesn\u0027t happen then all of these things like something might slip through the cracks so do you have any kind of standardization of that we haven\u0027t standardized per se and I mentioned kind of alluded to this it\u0027s a combination of sort of general things that you would work against any data set but also things that are very specific to our organization some strong assumptions we make about how the data is formatted and we use a combination of those were our teams right now are figuring out the right balance between how restricting me things or how are loosed investigating some ml techniques we have problems with false positives just to give an example the food web browsers user agents look like IP addresses the version numbers also hit some interesting info in positives there but it\u0027s really like you said some constantly refining trying to get that around and you know you know Salesforce "
  },
  {
    "startTime": "00:30:29",
    "text": "were very metrics driven I\u0027m trying to report you know how well we\u0027re doing and tracked that so like he said and make sure we\u0027re continuously laundering this it\u0027s not just a point in time but constantly going forward how do we improve this thank you Ryan I\u0027ve one follow-up question with respect to kind of showing or asserting how effective the analyzation techniques you\u0027re using are you had mentioned a fuse of different or privacy is one of the techniques I\u0027m wondering if you can comment further on how you choose things such as the amount of randomness budget that each client has or like each node has or whatever you want to use whatever the right terminology is and how you choose for example epsilon to you know maximize the utility versus privacy trade-off for that particular mechanism yeah and this is something we\u0027re still very early in essentially what we try and do is you know sort of parameterize this and then each specific use case can provide inputs on how you know like what their kml movie is and how you know how private they want things or how a less private they want things so it\u0027s very much right now on a smooth use a case-by-case basis I could get somewhere we published some more you know general principles but right now I\u0027d say usually a cross-functional group it sort of decides this and really we just you know sort of recommend it as a general technique so it\u0027s very much on a case by case basis thank you and to what extent do the general techniques basically overlap with the content of the draft that was just previously presented there\u0027s a lot of the same techniques that you\u0027re describing are those which are included in the date of measurement draft Rodian yeah absolutely I need you uh dig into that but yeah you\u0027re right there\u0027s a lot of overlap and a lot of different groups are doing very similar things so I appreciate the opportunity to share here and sort of give you know our perspective what we have found works for us and yeah exactly feedback like that is generally appreciated yeah great and potentially room for collaboration as well yeah absolutely great thanks Ryan yeah thanks a lot [Music] except you have Nick all right thanks everyone I\u0027m going to talk about a project called privacy pass which is interesting technology in the privacy "
  },
  {
    "startTime": "00:33:31",
    "text": "world so as a high-level overview its privacy pass is a lightweight zero knowledge protocol and before getting into the details of what it is and and how we can use it I\u0027m going to give a little bit of context as to how this came from and in relation to CloudFlare the company I work for and how what with what problem sort of inspired the solution so CloudFlare has a service that\u0027s reverse proxy so if you have a website or web service and you\u0027re using CloudFlare requests go through CloudFlare and then they come back that there\u0027s a TLS connection between the client and and CloudFlare and if something is not cached then the request goes all the way back to the origin and there\u0027s another little tiny red blob there which is for bad requests or requests that are malicious in one way or another and this is where the problem space occurs so in order to reduce malicious activity and malicious activity is spam or comet spam or requests that have have malicious payloads there\u0027s several different techniques that are used online to help protect sites against these sort of things and reduce the load on on websites one of which is a user challenge this can be in as a sort of demonstrate here as a CAPTCHA you might have seen this and so what happens is the browser is presented with some sort of challenge to prove that it\u0027s it\u0027s human and once that challenge has been passed then a cookie is issued that allows clearance bypass for this so one of the issues here is that the default security levels are such that for requests coming from clients for which the the the site has no previous information it\u0027s very difficult to distinguish bad requests from good requests a priori so there are situations in which IP reputation are taken into account and and if that\u0027s the only piece of information that there is then that\u0027s the only piece of piece of information that can be used to to choose whether or not to show this CAPTCHA and so users using VPNs or tours or something in which the source IP is shared among a lot of users so there\u0027s a lot of potentially malicious traffic coming from this will be served with these CAPTCHAs more often and solving a CAPTCHA using a cookies not portable this is important this is part of the web origin policy so every single site that you visit will be given a CAPTCHA and so there\u0027s somewhere around 11 million domains that use CloudFlare which make this sort of a bigger problem than it would if it was just one site and you have to solve one CAPTCHA if you\u0027re doing any legitimate or I guess a if you\u0027re browsing the Internet um you\u0027re going to run into quite a few of "
  },
  {
    "startTime": "00:36:31",
    "text": "these sites and it\u0027s gonna be very annoying so we want to kind of reduce that problem and one of the ways that you could think of doing it is figuring out how to potentially solve a challenge and get some back some sort of currency or some sort of proof or token or something like this that that you did solve this this and something that\u0027s anonymous so it wouldn\u0027t it be nice to have some sort of online equivalent to cash so that you can do withdrawals encourage and then make a transaction and have these two things be on linkable so withdrawing cash in and and paying for it would be would be on linkable so and in the analysis is actually not that great right so if you think of cash in a world where cameras are ubiquitous then you have serial numbers on every piece of every bill and so actually it is trackable from withdrawing from a bank account to paying somewhere else as long as there\u0027s cameras everywhere so what what\u0027s what\u0027s sort of a better analogy here and I would propose that that a better analogy would be a self printing money that gets the shirt I\u0027ll describe how this works so you would get a bill put in an envelope put a serial number on it and then take a piece of carbon paper this is a very rough physical metaphor here but um it\u0027s a piece of carbon paper in it seal the envelope send it to an official authority who then signed the outside and say yeah this is an official bill and then when you\u0027d open it up you would have the official bill and the authority that has signed it would have no knowledge of the serial number and so this would be a this would be an untrackable cash bill if you will and so this is the metaphor that kind of was Medivh ad Chomp back in the 80s when he invented the idea of ecash and from a high level it this is this is based on a cryptographic property called blind signatures or blind blind signing so there\u0027s two flows here the first is issuance of cash and then the second o is is redemption or spending it so the way it goes from a high level is you take a token you blind it in some way you send it over to the authority who then creates a digital signature using that token and sends it back and then so you have a blind signature and then you can take the blood signature and unbind it and from the perspective of RSA the way it works is the blind token and the blind signature are signed and if you unblind them together then the token and signature are also correlated so you can take the token in the signature send them to any third party who can then validate against the public key of of the issuer and then in and then return "
  },
  {
    "startTime": "00:39:33",
    "text": "goods so I may go into a little bit too much math in this but I\u0027ll try to try to make it easily easy but um in terms of RSA there\u0027s kind of two values you have an e and which is a public key and a DS it is a private key and this is how a Tommy and ecash works is you take your token k and you multiply it by a random number exponentiated by the public key send it to the server the server exponentially it\u0027s by its secret key sends it back and you can just divide out this random number and you\u0027ll get a pair K and K to the D which is essentially a token and a soakin token exponentiated by the server\u0027s private key and if you want to redeem that you send it to any third party they exponentiate by the public key check to see if it matches and if it does then great this is something that was definitely signed by a third party there are some more details here but this is this is the general general approach is is blinding and unblinding and redemption and so after some conversations at real-world crypto in 2016 some folks decided to kind of write down how this could be applied this method could be applied to solving CAPTCHAs where you imagine if you solve a CAPTCHA that would be issuing a token and then you could use that token to solve another CAPTCHA without actually having to actually put in the information and this draft led to this idea which is the foundation of privacy pass where yes you have a token you blind the token you solve the CAPTCHA send it to the server the server gives you a blind signature and then later if you see another CAPTCHA you can take the token in the signature send it to the server and get a bypass from that bypass that couch without solving it essentially so is this it this is privacy class well not exactly this is this was our original paper that we submitted to pets and and were and got rejected because you know it wasn\u0027t that satisfying to use this slow 1980s cryptography and there\u0027s been recent advancements in this field that are similar to eat things like ecash that we could have used and we decided eventually to look into and to use and the two fundamental ideas here are that of an OPR F which is an oblivious pseudo-random function this is very analogous to blinding it\u0027s it\u0027s a client and the server compute a value such that the server doesn\u0027t know if the result is but the service required to be a part of it and then another concept called vrf which is a random function that is computed using a private key and you can prove that the private key was used to compute it and so taking these two concepts we came up with something called we\u0027re calling a vo PRF which is a verifiable opr affix takes concepts from both and I\u0027ll kind of walk through what "
  },
  {
    "startTime": "00:42:34",
    "text": "that is as in terms of inspirations and work there\u0027s a lot of previous work on here Friedman first came up with a no PRF Yaqui and all came up had used this to do something called a do set intersection and in 2014 the idea the real idea of a vo PRF came about it didn\u0027t have all the features that our final privacy pass did but it was used for privacy password protected secret sharing and a type of fake algorithm later there was a paper called Sphinx which is about using o prfs to do secret secret hiding Sharon Goldberg presented at the CFR G us a proposal about verifiable random functions based on elliptic curves it doesn\u0027t it did not include anything to do with blinding in there but or or batched deal a huge proofs which are things where things we used but yeah there\u0027s a couple other papers including paper by burns and then Ryan Henry\u0027s thesis and taking all these ideas and kind of putting it all together we came up with prime seabass so what\u0027s my time like so I\u0027ve got quite a few things we\u0027re good ok great so hopefully this is this this will be clear but I\u0027ll walk you through exactly what how privacy pass works there\u0027s a few fundamental things to keep in mind one is the setting in which we\u0027re doing the computation and the setting is a prime order group you can imagine this as the group of points on an elliptic curve for say elliptic curve Elmen and it has to be a prime order group which is just a small wrinkle but um in any case group elements I\u0027ll denote them with capital letters such as P or Q and then the fundamental operation you\u0027re doing on these is scalar multiplication so if you are taking a point P and multiplying it by itself n times or I get no adding its itself 9 n times that CL in multiplication oh I heard something okay and so scalars are lowercase letters the last two pieces are hash to group element this is a function that takes a scalar a token and outputs a group element that is random in in a in a statistically average sort of random way uniformly average random and then the last piece this is the this is the only kind of tricky concept which is that of a discrete log equivalence proof this is the only place in which zero knowledge comes into play here but the idea is that two points two pairs of points can be analogously related to each other so PNR can be related to each other like Q and s using the same multiplier so if you have P and s times P and Q and s "
  },
  {
    "startTime": "00:45:35",
    "text": "times Q you can prove that this s there is an S such that you know P is e times s is is s times P and Q times s is s times Q and you can do so without revealing what s is so this is this is really used for in envy ahrefs and it\u0027s used in other places for proving that a specific private key was used and these scalars are usually private keys so this is going to be denoted DL a Q P to R is the same as cute-ass all right so with these fundamental pieces I\u0027ll walk you through a naive construction of how this would work and iterate through until we\u0027ve kind of ironed out all the problems okay so scenario one client takes a point on elliptic curve T and sends it to the server and the server has a private key secret number s and multiplies T by s and sends it back and then later the client will take this T and this s of T send it to the server then the server can check to see that had previously been issued by taking the T multiplying by s and see if it equals to the second point and so this this this step is called a routine and this is this is a very naive scenario that everything is everything here is built on is multiplying by a secret value on the server side since the server knows s you can you can compute s of T so the problem here in this situation is that during the issuance and the Redemption the same s of T is sent so these these two are linkable together so this is where we introduce the idea of blinding so in this case the client has a same point T as well as a blinding value beat which is a randomly generated value that it keeps on its own side and so you take the blinded point send it to the server the server multiplies by its secret point sends it back and then you can because you know B as the client you can multiply by the inverse of B and get back T and s of T and then redeem that way and then just like before the server can take T multiply by a so check if it\u0027s the same and say okay great this was issued before and because the blinded value was never or the value sent in the issuance is blinded then the server action it does not have link ability to link ability so the problem here is this is great but this is slightly malleable if you have T and s of T if you multiply them both by any scalar say two or three or four or five then you\u0027re going to get an infinite number of valid pairs that you can redeem so one issuance gives you an infinite amount of options so that\u0027s bad all right so how "
  },
  {
    "startTime": "00:48:36",
    "text": "is this salt well I remembered I mentioned hash to group element this if you think of cryptographic hash it\u0027s one way you can traverse it so instead of picking a point T where you start you actually pick just a number T and take that T and passion into a point T and then in this case when you redeem you yeah so you the exact same issuance so you take T and you blind it send to the server get it back and then when you redeem you actually send the the token value T not the point t and because the server can do hash into the value T and then multiply by s it can it can still validate this and because it\u0027s you can\u0027t find another a t-that hashes into something that\u0027s a multiple of the other T because this is a one-way function you really can\u0027t get another pair you\u0027re guaranteed to have a unique pair and there\u0027s a slight problem with this situation it\u0027s it\u0027s it\u0027s really not not that big of a problem but but essentially if you\u0027re sending this token it\u0027s not actually bound to any specific message so if you happen to be sending it over an insecure channel then someone could take this and you know associate it with a different message so there\u0027s a trick you can do here and which is rather than sending s times T rather than sending sort of the sign point where you can do is sign a message and an H Mac of that message with s of T and so how does the server compute this well the server takes T hashes it to the point capital T multiplies by s and then it has the key for the H back so rather than explicitly checking that the s T that was sent is actually the same as s TS are computed you just check that the H Mac is computed correctly which is which is great and so this this goes this goes through pretty nicely but um this this is really as far as you can get with just a no PRF itself this is this is essentially a no PRF construction the problem here with is is tagging is the idea that this s could be chosen uniquely for each individual and or if there\u0027s you know a nest that\u0027s chosen for everybody and then an S that\u0027s chosen for the one target when you send in your redemption then they can track you essentially and so what is the proof what is the way to fix this and this is where the DL EQ proof comes into play essentially at this point the server publishes a generator point and generate your point multiplied by its secret key which is essentially like a diffie-hellman public key and it publishes it somewhere Universal "
  },
  {
    "startTime": "00:51:37",
    "text": "somewhere like the tour consensus or somewhere like a certificate transparency log or somewhere where every client knows that this is going to be the unique value could be embedded into the software which is sort of what we did and so the idea is here when you send back the server\u0027s SBT you also proved that SBT relative to the blinded token is this is analogous to G to s to the G so that the it was the same scalar that was used to multiply this is this is how VRS work they\u0027d say okay this is a proof that we\u0027re using this the exact same s for you that is in the public domain and this is great so this is this is mostly the end of how privacy pass works except that there\u0027s this only gives you one Redemption per issuance so there\u0027s this is the text is a little small here but essentially we\u0027re doing this we can do it multiple time say three times you can get solve one CAPTCHA send in three different tokens the problem in this case is that these D leq proofs are a little bit big a little bit expensive so we came up with a small optimization that you can actually compress these three do aq proofs into one and so you can issue you know three tokens and give one deal EQ proof for all of them simultaneously and this is it this is this is essentially how privacy pass works is when you solve a CAPTCHA you come up with three unique values T or in this case it could be up to 30 depending on on how your can how your configure so let\u0027s say 30 you hash them all two points on the curve you blind them all send them to the server the server multiplies them all by its secret key gives a deal EQ proof that the same secret key was used for all of them sends it back and then you can individually redeem them each with this H max system ok so this is it for the protocol and also I guess a year and half ago we release privacy pass as a Firefox and Chrome extension so this is something that is really implemented and deployed with it says 50,000 daily active users but the Chrome extension has about a hundred and twenty-five thousand users wouldn\u0027t tell me how many were daily active and the the Firefox one says there\u0027s something about 25,000 daily active so I estimated this there\u0027s trillions of requests per week to the CloudFlare network and in terms of redemptions there\u0027s about a half a million per week using privacy pass and this ended up getting accepted at pets last year and so this this isn\u0027t public public domain this information right now as well as all the sources open at privacy passed out github god I am so looking forward we\u0027re currently integrating privacy pass with different "
  },
  {
    "startTime": "00:54:38",
    "text": "CAPTCHA providers there\u0027s a slight issue here relative to chummy and ecash where anyone could validate these these tokens so vo PRS are not publicly verifiable you have to actually have the private key to check that the token is correct so it\u0027s more like a voucher than cash but the it\u0027s one small downside relative to to the RSA version but the speedups you get by using elliptic curves and by having all the space savings is really except for it there\u0027s a no PRF submission to see FRG that\u0027s currently on revision3 and we\u0027re also looking into different applications of the idea for example there\u0027s a draft submission for TLS to use these tokens to do anonymous resumption there was a recent paper about how TLS session resumption is a tracking vector this is meant and to solve that you can think of this you can do use this to do anonymous referral codes that\u0027s an another interesting idea we\u0027re exploring and anything that\u0027s really a single bit of zero knowledge proof can be used for this so if you wanted to have a privacy pass token that validates that is run by say a government and and that proves that you\u0027re over 18 or at work X Y Z EU citizen you can use this it\u0027s very lightweight there\u0027s no advanced math for that and so with that I will open it up to questions and this is what the discrete log equality looks like if anybody\u0027s interested Wes heard of Chris I fascinating work I I really like the intent behind it and the desire to I guess the goals behind it as well as is the way to say it a couple of quick questions so one you said that there is of course some sort of expense you know related to it do you have any percentage you know CPU increased to actually do the level of math associated with it yeah so in order to do to do this from the client side and from the server side it\u0027s it\u0027s really one elliptic curve scalar multiplication so it\u0027s cheaper than TLS handshake so this was this was one of the goals with RSA it was slightly more expensive and this it\u0027s yeah I guess one elliptic curve operation per token okay that\u0027s not that bad yeah so the next question and there\u0027ll be a follow-on is from what it sounds like there\u0027s a limited number of tokens that you hand a client so maybe you hand back 30 or something like that and after "
  },
  {
    "startTime": "00:57:38",
    "text": "30 they come back and they have to solve a CAPTCHA kin is that correct that\u0027s right yeah there\u0027s not an infinite number of I\u0027ve tokens so it\u0027s deciding on the parameters is use case dependent and we found that 30 or so was enough that it reduced the it reduced the friction for users quite a lot and you know you can you can you can you can technically modify the code to do up to a hundred on the server side but um yeah it really depends on use case and the follow-on to that is I\u0027m assuming that there is no reason why the client couldn\u0027t share their cookies with somebody else that they\u0027re transferable technically if they share their secret keys with another one yes in fact on the client side once you\u0027ve unblinded your token then you don\u0027t have any secret state at all you can just you can share your actually even with the blinded token you can you can share with anybody so one thing to keep in mind that I didn\u0027t mention on the server side is that there does have to be some level of double spend protection because the these could be potentially computed multiple times but yeah if if all in all there are some other larger ecosystem things that are brought up by by issues like this such as farming you could imagine someone could do a lot of farming and solve a lot of CAPTCHAs and then kind of use it to bypass things on a wider scale but um but generally yeah we think that having key rotation is a way to to help reduce that and metrics show that it hasn\u0027t been abused in that way and well that was that was where I was heading next was you know if you if the malware is distributed that you know took over a million browsers then they could collect an awful lot of tokens to you know make endless use of if they needed to at some point in the future yep that\u0027s right and so it would be one you\u0027re essentially multiplying we\u0027re reducing the cost but you\u0027re not eliminating it so in order to get 30 tokens you still have to solve one CAPTCHA so you\u0027re you\u0027re multiplying the value of solving one CAPTCHA by a factor of X Rex\u0027s number of tokens there still nobody behind me so you talked about double spending on the server side so you actually have implemented something on that you it\u0027s not in your slides from I saw so you have something that simple that\u0027s checking double spending and is that double spending checking infinite in lifetime or yeah so it\u0027s we are our implementation of double spending is is changing we\u0027re we\u0027re rewriting this to you leverage a new platform that\u0027s a JavaScript based platform called workers that has more robust double spend protection but essentially you have to you have to keep the double spend strike register as long as the lifetime of the "
  },
  {
    "startTime": "01:00:38",
    "text": "server\u0027s private key and so key rotation is the way to actually reduce that and and so you do get in into a little bit of a little bit of a chicken and the egg problem if right now in the very first version of privacy pass we hard-code the server\u0027s keys so we haven\u0027t been able to rotate and so we\u0027re coming out with the new version in the next few months or so that allows that to be updated on a dynamic basis well anyway very cool good work ok keep going well thank you yeah Nick I have a quick question so the DL a key proof is a very elegant way to solve the key tagging problem were there any other earlier designs that you considered in order to address that like you can imagine for example the server signing what is the blinded token providing the public key back-end and clients like gossiping it to see that they\u0027re both having the same view of the key yeah the it all boils down to the same problem which is key key rotation and key distribution and Trust of key rotation so we did consider publishing that in in places that the client can be assured of a shared global view so something like the certificate transparency log is is one way to do it and distributing it from a central source that is signed by a long term key is is sort of the design that\u0027s that we\u0027re considering for the the rotation going forward and quick follow-up question I know there was a draft that was originally written not submit anywhere and github that eventually became the pets paper that was accepted is there plans for a actual specification for privacy pass for later consumption and potentially standardization or yes potentially I think that in terms of HTTP and using this as an HTTP mechanism there\u0027s nothing that would prevent this from being standardized we\u0027re first exploring Federation with respect to we have our own capture provider but we\u0027re now currently experimenting with a company that has called H CAPTCHA that does their own CAPTCHA as well as a company called arc rose labs who has something called fun CAPTCHA and and as I mentioned at the end this is potentially generalizable to a lot of things so yeah we want to see how it plays out operationally before deciding to standardize but I think there\u0027s Stefan well right yeah I think okay thank you very much it Thank You Nick haven\u0027t even not sign the the blue sheet where are the blue sheets surely they\u0027re in this room somewhere oh they\u0027re in the "
  },
  {
    "startTime": "01:03:39",
    "text": "back thank you next up we have Amelia and Christopher so hello and Christopher are you connected yeah I think so yes so my name is Samantha understand I I work for article 19 yeah and my name is Christopher Langstrom and I am a master student in Applied Mathematics at Uppsala University I\u0027m currently working on my master\u0027s thesis on privacy-preserving methods in data science and statistics so first we want to thank everyone for letting us have this presentation today this is going to be a very high level view of differential privacy some of the slides will contain mathematical notation do not be alarmed it looks like there\u0027s a lot of it but it\u0027s for completion it\u0027s in case you forgot something about Exponential\u0027s or mean values it\u0027s all in there and it\u0027s so you can go back and review the slides without feeling that you\u0027re completely lost so this presentation is going to be in approximately three parts we\u0027ll give this very high-level overview of what this differential privacy aim to do then we\u0027ll go into some different mechanisms for achieving differential privacy like input input data perturbation or output data perturbation finally we\u0027ll rotate back and see a few ways in which we believe that differential privacy mechanisms could be or could not be applied in IETF protocol standardization and the very last slide contains references that you can peruse at your own liking when you have the time that we found very useful to enhance our understanding of differential privacy so firstly where are we at well in the IETF we have RFC 69 73 that contains privacy considerations for IETF protocols differential privacy is of course a specific way of remedying some privacy problems such as identifiability which is mentioned in the RFC 69 73 or a secondary use which is also mentioned in RFC 69 73 differential privacy simultaneously provides a value to the degree of success that we have when trying to remedy these threats the overall aim of differential privacy is to provide an individual with plausible deniability when you are performing a statistical test on the database in the sense that such an individual should be able to deny being part of any database on which you have performed an aggregate query right so again there\u0027s a bit of a delay "
  },
  {
    "startTime": "01:06:40",
    "text": "in the slides but I\u0027m assuming that we have the definition up now yes right so this is our definition of differential privacy here so this is a strictly mathematical definition and this is called so-called epsilon-delta differential privacy so we\u0027re gonna break it down piece by piece and look at what each element of it means and try to link it to the underlying meaning that of differential privacy and what we\u0027re trying to achieve so as just mentioned the idea in differential privacy is to make it so that any one person in the data set is not overly exposed to any sort of risks so what we have here is we have D and we have our D Prime in the notation so this represents two data sets that differ on just a single element so they are the same up to one row and then we have M here which is a mechanism which we apply to our data set this will be something like a query life it could be a statistic computing the mean value or the variation of something and going forward we\u0027re gonna use the shorthand m to just mean M as applied to D and M prime to mean Ms apply to D prime next slide please so we have our P of M here which is then our probability distribution of our mechanism M so this is the range of all possible values that our mechanism can take and then of course these sort of the probability that it will assume each and one of those values we have here which is our standard exponential which we have taken to the power of epsilon and then we have this additive Delta here so what we\u0027re trying to achieve here is to make it so that the distributions of our of our mechanisms are sufficiently similar make it so that it\u0027s equal roughly equally likely to assume any value no matter if a certain person is included in the data set or not so and we\u0027re using our epsilon and Delta\u0027s to sort of achieve this next slide please so epsilon and Delta here become our privacy parameters these are parameters that we can choose ourselves to enforce a given a given level of privacy so we can decide on our own by how we choose epsilon and Delta if we\u0027re looking to apply every high degree of privacy or if "
  },
  {
    "startTime": "01:09:41",
    "text": "we\u0027re happier with the lower degree of privacy so depending on applications next slide please so what if we choose epsilon and Delta for instance to be very small what this inequality here will tell us is that the probability distributions of our queries on our data sets that differ by only an element are not going to be very different they\u0027re going to be quite similar or at least is going to put a bound on how similar or dissimilar they can be and if epsilon and Delta are very large well then they can differ by quite bitte so herein lies the strength of differential privacy that we\u0027re able to quantify our degree of privacy we can put a number on how strong is and it\u0027s on a continuous spectrum so we can increase and decrease to suit our needs and next slide please right yes so now we\u0027re talking a bit about some methods on how to apply this and we\u0027re going to start off with the the method as it was originally proposed in the original paper and that is to perturb the answer to a query so we have a data set and someone queries us they ask something about our data set so what we do is that we compute the true answer so to speak and then we add a bit of noise to this and then we give back the noisy answer so the person receiving the answer knows that ok this is roughly correct but it\u0027s not it doesn\u0027t know if it\u0027s higher or lower than the actual answer next slide please so the most common way of doing this is to add some sort of noise that is either Gaussian or laplacian but basically what this means is that we add noise from a symmetric distribution right so it\u0027s equally likely to take away as it is to add to our data and this noise is added in such a way that it\u0027s dependent on epsilon and Delta meaning that we\u0027re if we have very small epsilon and Delta so we want to add a lot of privacy then we\u0027re gonna add quite a lot of noise if we on a low level of privacy than just adding a little bit more this is fine but obviously there are some drawbacks to this approach right the adding noise to the responses to our queries is going to drastically worsen any estimator quality like statistical estimators on this with basically the "
  },
  {
    "startTime": "01:12:43",
    "text": "result that you need a lot larger sample sizes in order to get the same accurate accuracy as before and then we have this that if we allow people to query against us sort of indefinitely to just ask the same question over and over they can just average their answers or the answers that we give them and that will cancel out the noise that we put on so this gives us the notion of a privacy budget that we need to sort of limit how many times someone can ask us about this and finally this approach you still need to trust the database holder right that we still need traditional security measures to safeguard our data so there\u0027s a second method that you can also use for differential privacy which is to perturb the measurement so rather than perturb being an answer to a query to a database you make sure that whatever goes into the database is already perturbed when it gets there there are some different ways of doing this one of them is commonly applied at the IETF you remove or encrypt identifiers so that they\u0027re garbled when they reach the database you can also imagine perhaps swapping data between different identifiable flows so if you imagine in geolocation tracking you could divide a map into lots of grids and inside of each box on the grid pattern you just swap all the individual tracks of traces of individuals and that would provide some degree of perturbation you can also randomize responses which means that with some probability you allow anyone that inputs data into your database to just provide a false answer to your database now these methods also have drawbacks the obvious one being that if the data in your database is not true or if it\u0027s swapped you will also worsen estimated quality so if you wanted to find out some aggregate statistic like what is the average height of people in this room then if everyone was allowed to say whatever answers for their height you might not end up with a good answer for that there\u0027s also a need to trust the entity making measurements so for instance you need to trust the randomization mechanism used or the swapping mechanism used for your paths you need to trust that they haven\u0027t secrety and secretly introduced some other identifier that was not the one that was removed and so yeah it has some of these drawbacks that are really very difficult to work around with differential privacy therefore at the Delta differential privacy is not the only privacy needed it only deals with a very specific case of a very specific "
  },
  {
    "startTime": "01:15:46",
    "text": "case when we are trying to protect the identity of an originating individual for some piece of data when you\u0027re making a query to a specific database so Donna sanitization and security are still going to remain very important data minimization is still going to be probably the primary method that we can use to effectively protect privacy at the Delta differential privacy is furthermore not the only way that we can quantify how much privacy we are protecting so there\u0027s a very good study from 2015 made by Isabel Wagner and Dave akov where they found hundreds of privacy metrics that are all usable to assign a quantification of how much privacy is preserved in a given setting we can recommend highly going through that survey and looking at the various ways in which privacy can be quantified another study which is here mostly for general interest is by Duan in 2009 and he demonstrates that as long as you have a sufficiently large data set which I know many of the people that come to the IETF will work with very big data sets and they actually contain differential privacy properties in and of themselves at some levels of Etta and elta epsilon the Delta and so dawn is very mouthy so if you\u0027re comfortable with statistics I can highly recommend it to but just be warned that it invokes the central limit theorem and other methods from statistics that you might have to have prior familiarity with to to enjoy for the ietf community I think one challenge for differential privacy is that it mostly applies to api\u0027s because this is either about how you put data into a database or how you respond to the query to a database it\u0027s going to be application programming interfaces that need to apply these methods and they are not at least in my understanding standardized at the internet Engineering Task Force to a large extent but nevertheless we had some ideas when working on this which relates to the randomized responses mechanism so if you have a protocol which is providing some form of information say about an option or some other data which is really only used to measure aggregate qualities of the protocol then you might be able to use randomized responses to introduce additional level of privacy by having a client or a server lie at a predictable or predetermined rate we\u0027ve been looking at the weekend on the quick spin bit whether it\u0027s possible to have a predictable false answering to the transition also we\u0027ve been reflecting on whether explicit congestion notification could be an application of this randomized response mechanism because you\u0027re really interested in aggregate data not in this specific data of each entity that provides the response but "
  },
  {
    "startTime": "01:18:48",
    "text": "we\u0027re very open to ideas on other potential use cases that people are familiar with from their own work at the internet Engineering Task Force so any questions or comments benkei duck so with this bit on the last slide about potentially introducing random or false data into your actual protocol streams it sort of seems like the key insight you\u0027ll need to do that is to figure out what random distribution to use because you know futures try to use a uniform random distribution for like the bit values that\u0027s going to be most likely not representative of what the normal flow is and so it seems like this is inherently going to be a case-by-case sort of analysis to figure out you know what is the expected distribution and how can we add some structured noise in a way that provides the properties we want I mean so in randomize responses the typical thing that you would do is is some discrete distribution so you\u0027re not in the Gaussian or laplacian space but in for instance the quick spin bit case either you spin the bit or you don\u0027t so the natural distribution to choose is Bernoulli distribution or binary to point distribution and then you say with some probability P you give the correct transition and bid and if it\u0027s not P like yeah so the way that I imagined this would actually be implemented in a computer is that you generate the random number between 0 and 1 and then you choose some cutoff point and if the random number you generate the post below the cutoff point you spin and if it was above it you don\u0027t and maybe you know this is the very simplest case it\u0027s one of the simplest distributions out there it\u0027s like you\u0027re taking a true stream of data and sort of masking it to to sometimes not provide the treating right yeah I would just like to mention a quick thing there someone mentioned previously the Google rapport project that they published in 2014 in that one they do exactly this it\u0027s a randomized bit flip thing so if anyone\u0027s interested in that specific application you can look into that Dave wheeler I\u0027m quite glad you you had this talk I had some preconceived ideas on differential privacy and I\u0027m wondering if you can help me clear some of them up it from some reading in tutorials on the foundations of cryptography they have a chapter on differential privacy where they claim that anonymization can can be easily undone with large enough data sets and the differential privacy in and of itself protects the individual data set but can be attacked by using secondary "
  },
  {
    "startTime": "01:21:50",
    "text": "outside data sets that are large enough so in in in our world today right provide differential privacy on my data set I may be providing information that someone else will incorporate with their own large data set and be able to diya naanum eyes that information so I\u0027m wondering if you have some comments on that I mean the observation is entirely correct differential privacy is a statistical method so what you\u0027re talking about this sort of similar to this repeated query privacy budget thing that Christopher mentioned that you\u0027re creating statistical uncertainty around one additional individual in the data set and differential privacy does only exactly that this is what it can it can provide repudiate repudiated properties for a single individual that is additional to a data set with respect to another query to that data set worth individual not there but it\u0027s not a catch-all for all privacy problems it only does exactly what it claims to do kind of formally which is provide some assurances on what the answer to a specific query to specific databases okay yeah thank you Wes vertical u.s. si si so this may be naive apologize my knowledge of differential privacy is spinning up and not level off at all yet but when you are introducing noise to a system in order to deal with differential privacy and typically access to databases is only done through aggregation functions in order to you know make it actually work right how do you ensure that there\u0027s a potential error between the noise itself causes a issue with the there\u0027s multiple aggregation functions that could end up sorry I\u0027m jet-lagged multiple aggregation functions where some may be able to help distinguish the noise between the rest of the material so if you were able to run four or five or six different aggregation functions that the difference that noise would affect them each differently and therefore you\u0027re actually getting slightly less privacy because of that does that make any sense that makes sense okay it didn\u0027t make any sort of work on that well I mean it\u0027s there I it looks like again a version of the privacy budget problem that if you make repeated queries to the database then you can compensate for the noise that\u0027s added the typical way of doing differential privacy\u0027s is adding noise but many of you have would have studied engineering you\u0027re familiar with stuff "
  },
  {
    "startTime": "01:24:50",
    "text": "like the Kalman filter there\u0027s many different filtering technologies that or filtering algorithms that we already use on a day-to-day basis exactly to get rid of noise from measurements so now if differential privacy adds noise and reasonably yes we can remove the noise by performing the same types of filtering mechanisms that we\u0027ve been doing to faulty measurements already for what 60 70 years and it\u0027s important to recognize that asian and differential privacy when you\u0027re thinking about applying it to a project that it does do exactly what it says on the box and that can be very useful but it has these limitations including the ones that you brought okay thank you yes I mean there\u0027s the if if I had to aggregation functions aggregation function one and aggregation function two I could run an aggregation function twice and it would give I think the perfect mathematical level of knowledge of how much I\u0027m protecting but if I run aggregation one and then aggregation to the cost to me in terms of a budget would be the same but I may be able to use the difference between those aggregation functions to get a slightly less privacy reduction for example so that that\u0027s sort of my thought but I don\u0027t have an answer I\u0027m not familiar with if the privacy budget measures that you can compute from differential privacy now compensates for this level of threat I don\u0027t know I mean it\u0027s a concern so with respect to this spin bitch I think you know they like you said the whole thing is about the privacy budget with enough queries you can you know figure out exactly what you want to figure out it might be kind of complicated right to ask an on path observer to only take five measurements so you still have you still need some level of trust in the in the people that make the observations it\u0027s true when we were thinking about the spin bit also we realized that one additional complicating factor is that probably there\u0027s an IP address or something that identifies each flow so it\u0027s still a very very nascent idea at least for myself but I\u0027m very open to talking to more people about other use cases or about what the results were after our weekend deliberations on the quick spin bit so we can talk I just wanted to remark back on your point about api\u0027s I think we\u0027ve been a little bit narrow in this community about what is an API because it\u0027s a human being interacting with the system but there\u0027s no really good reason why an API can\u0027t be thought that something like a one end of a protocol connecting with another can\u0027t be thought of as exactly that database interaction as well and similarly a measurement a measurement agent receiving data is you could define it as an API to so I think it\u0027s reasonable to imagine that there\u0027s this this research group can come up with a lot of different approaches where people could use API like technologies in "
  },
  {
    "startTime": "01:27:51",
    "text": "protocols and measurement yeah I agree that interesting point of view so is you\u0027ve looked at the spin bit so far are there any excuse me any other specific protocols with an ITF that are on your radar and whom you\u0027d like to connect with people who developing it easy an explicit congestion notification somebody suggested to me could be an interesting application so if you have any ecn folks in the room you are very welcome also to talk to me and anybody in the room who thinks this might be suitable for them that hasn\u0027t been mentioned please please do approach remaining so I think the easiest cases to look at at least for now is places where you have on/off answers so basically you can use a binary distribution to say rather than giving a true or false answer you give a true or false answer with some probability and all the other things that are more complicated like if you can choose between five different options to communicate in your in a header packet header for instance then you would need to have a more complicated distribution than binary distribution so we could certainly think about that tomb for randomized responses but we would have to do a bit more mathematical work to make it still useful for aggregate measurements thanks a lot so also the reference list but you can check it in the sides thank you yes thank you should we have Martin yeah hi and thank you also for inviting me I\u0027m a researcher at the front of Institute for applied and integrated security and I\u0027m also contributed to the MIT project and in my research I am mostly focusing on decentralized and privacy preserving identity management when we started looking into this topic we first took a look at the consumer identity provider market which looks roughly like this with offerings from Google and Facebook claiming around 90% of the consumer identities now per se this is obviously this might not be a problem but if we look into the recent past some issues arise from this fact the first one are privacy concerns so obviously those companies want to make money and they offer the services for free and in turn they monetize this mountain of data they are sitting on also this data can be "
  },
  {
    "startTime": "01:30:51",
    "text": "used for opinion shaping and mass surveillance data collection which is an infringement on the users privacy another issue which which has become more evident recently is that those service providers because they are sitting on this huge mountain of data are now facing increased liability risks because if they lose this data or if it gets if they get hacked for some reason and they are facing severe risks and maybe that is also why one of the reasons why competition is not really there for those providers because actually collecting all this data maybe the the money you can receive from monetizing it is actually lower than the risk actually face and obviously the last one is simply the fact that this seems like an market oligopoly where there can only be one or two or a handful of so provide us that is used as maybe the reasons of liability risks but might also just be because they offer so so good services in the area of social media that they simply have this amount of users but identity Federation has been around for decades and it does not seem to take off in any way so our approach to this problem to those issues was that maybe maybe we need to approach this differently our primary objective was that we must enable users to exercise the right to digital self-determination and in order to do that we must avoid third-party services that basically allow users to match the identities and share the data and this should be done using an open and free service which is not under the control of a single organization and/or our business in other words it should be done but through an infrastructure that does not have any other stakes than the users right to digital self-determination in mind and third ideally which is part of point two is it should be free and open-source software in summary we were to empower users to reclaim the control of their digital identities this is also whether the name for for the idea comes from so let me explain what reclaim actually does and how it works in order to explain what reclaim does let\u0027s look at what or some of the tasks that those identity provider services actually provide to users and to websites and services that use them the first thing they offer is they allow identity provisioning and access control so the user is able to create an account and basically create an identity management personal data and share this data with third parties the service itself then enforces the access control and authorization decisions of "
  },
  {
    "startTime": "01:33:52",
    "text": "the user the other thing the identify that does is it verifies and asserts the information of that identity so for example if an email provider can say ok this is a verified email address of elders or a sovereign state might say yes this is the user living in Germany Italy noted that the second thing can be addressed or is addressed recently using privacy credentials based on for example non-interactive see large proofs which we have seen in the in the previous talk in privacy pass as well and reclaim actually focuses on the first issue so the reclaim ID focuses on the idea how can we actually allow the user to provision identities to to manage identities and share this identity data whether it is third party asserted or not using a decentralized system in a nutshell reclaim ID combines a decentralized directory service so a service that is used to basically hold and provision identity data with a cryptographic access control layer now what does this mean a directory service we maybe know active directory and but name systems I actually also decentralized directory services we borrowed here an idea from name ID name ID uses the name Coyne name system which is a blockchain based name system and allows users to manage identities and data within their namespaces now our implementation of this idea does not use the name coin blockchain and instead uses with new name system and if you\u0027re interested in the security properties of the human name system there\u0027s actually talk in this week I think on Thursday but I don\u0027t remember by Christian who will talk on it another research group however the problem with name ID specifically but in general when using such a decentralized directory service is that this data is more or less public so anybody who has a specific resolver is able to retrieve the identity data and read it so we added a cryptographic access control layer basically you can always add a cryptographic access control layer using symmetric cryptography and a very complex key management and but in order to reduce the complexity of the key management we\u0027re using attribute based encryption a true based encryption allows us to define access policies on the ciphertext which simply reduces the amount of keys we need so let\u0027s look at an example how it works the user would basically register a namespace in in the name system in the directory service and populated with resource records and those resource records basically hold the users information such as an email address now "
  },
  {
    "startTime": "01:36:52",
    "text": "this email address is encrypted using a true based encryption which means the user has a private key and a policy and this policy says for example my email address can only be decrypted using a key that contains the attributes email and the user does this with every attribute so in the end the namespace is populated with a number of cipher texts which represent the users identity attributes if the user now wants to authorize a third party to access a set of attributes in his identity namespace what it does is it creates a single new key which is an a B user key and the user attaches the set of attributes wants to share to that key so instead of having to create or share n keys in order to share and attributes the use only means a single key now this key can be transferred out of bonds to to the requesting party or to the third party but in order to facilitate revocation and also key rollover it makes sense to store a reference of off the key in the name system as well or in the directory service so in order to now retrieve the data the third party simply either resolves we use a key or already has it and then resolves the identity attributes from that from the direct and decrypts it attributes which are not attached to the key cannot be decrypted by the requesting party now obviously if you wanted to use such a system we do not want to burden the user other third party for that matter with any kind of key management and and name system specifics so what we did in our implementation and then our design was that we built an up MediConnect layer on top of this idea so under the hood it basically works like I just explained in this example but from the point of view of the user and the integrating website for example it works just like any generic Open ID Connect provider so it basically all searches to do or protocol although I should say that the Open ID Connect protocol and the earth protocol are not specific enough to actually address for example if the requesting party or the website wants to have a specific credential asserted by a special identity provider this is currently not possible due to the protocol being still very simplistic but in general this works and here\u0027s the standard so in summary we have implemented this idea as part of lunette which is a peer-to-peer system there\u0027s a functional proof-of-concept demo else on "
  },
  {
    "startTime": "01:39:52",
    "text": "git lab which you can find under the the link here although it does not really finished it\u0027s a very rough around the edges and we want to make it a bit more user accessible because yeah it\u0027s still coming directly out of research and we\u0027re still working on making it practical usable yeah thank you any questions thank you Alex mayor over from Nick Toth 80 I was wondering at the moment that you associate the public key of the attribute based encryption with with a name on the new name service your name system aren\u0027t you like reducing privacy significantly because then it allows somebody else to like collate various a BES via the same name so if I put like different proofs or no my no idea how you call them under a single name and it\u0027s very easy for somebody else to like discover that this was produced by the same identity yeah so basically it doesn\u0027t distinguish it so you always know which identity it is because you\u0027re looking up a specific identity namespace are you talking about if you\u0027re if you\u0027re using this the same attribute in different identity for different identities because okay well if you\u0027re using different identities then you\u0027re probably using different a be private keys internally which which makes it indistinguishable from each other however you could argue that using for example privacy preserving credentials so their knowledge groups as attributes does not make that much sense because you\u0027re always identifiable as the single identity so yeah well I\u0027m trying to compare it with other self sovereign identity systems like sovereign or you port where you have a pair Y is relation and you essentially cannot discover a real identity and unless that person was revealed so but but by putting it behind a single name that\u0027s often criticism with the DNS like oh you put it in the DNS so everything is like it I\u0027m not the same name so you know exactly who it is but what so obviously if you use DNS as an underlying name system you have that problem if you also have to name service if the problem if you have used gns yes but in DNS are also named coming for that matter you can just create new problems whenever you want so obviously if you use the same saloon um you are trackable but you can just create new ones that will because they\u0027re effectively just public private keepers okay thank you telephone exchange I have a question which is also actually I\u0027d say a comment for everyone who\u0027s interested in these kind of things having worked for some years now on a project which is very similar except that we use the DNS because we don\u0027t have a problem with the delay so I mean the the problem now is that there are like a hundred different projects that are "
  },
  {
    "startTime": "01:42:52",
    "text": "trying to do these kind of things well in the real world are only basically two and say single sign-on identity systems that are in white use googles and Facebooks and maybe in your who we have dei does public identity system for taxes and this kind of stuff so I guess that the problem that we have is they I think we have also people agree that we need something like these which can put a user back in charge but how can we get that option and the real problem is actually that we are dividing in like up like I\u0027d say 50 different projects with slightly different ways of doing the same things and some people use the block to some others but in the end with the same objective but please we also divided we don\u0027t get anywhere so I don\u0027t know if you have any reflection of these or strategy for an option yeah I agree wholeheartedly so yes but I think if all of those projects actually for example use the standard of maybe connect for example to use them then if the users become if if you offer the software to users and show the benefits in terms of privacy for example they might just start using them so there\u0027s no reason why you cannot put a generic openly connect discovery button on a website so I\u0027m Watson lad I\u0027m from South where my question is about sort of the economics of getting websites to integrate like one of the reasons I think websites have login options is that users already have these and you just click through so for users there\u0027s a benefit to SSO how do you get websites to integrate another SSO given there\u0027s limited space and user users don\u0027t like options and how do you get users to see the benefits of adopting us or so thing if you don\u0027t have websites to integrate it yeah that\u0027s the chicken egg situation under I\u0027m gonna have a solution for this I would probably address it first in getting the users to want it and then websites will just follow I think that\u0027s the path you really have to take but I don\u0027t have I don\u0027t have a solution for this let\u0027s say other questions about how does how do you so one the reasons that I for instance use Google single sign-on is for digitalocean for instance it is the most secure way to log on the digital ocean is you can use aq factor authentication device they don\u0027t have one any other way how do you address that level the authorization issues okay so the well the the digital ocean for example probably so you want to login using two-factor authentication but I\u0027m assuming digital ocean doesn\u0027t really care how you login basically that\u0027s something that Google just does for you so that that then depends on the actual complete implementation because ultimately connect doesn\u0027t really do authentication so some an authentication scheme obviously you could extend it but that\u0027s not really in scope of the system well I\u0027m not sure exactly what they\u0027re using but you have to be signed into Google account before you can sign click goodbye and design in yeah but they don\u0027t care how Google is authentication "
  },
  {
    "startTime": "01:45:52",
    "text": "features so that their piggyback on it they don\u0027t have to do the work google does it and see you as implicitly they do it right so a nation was on I think the whole notion of sort of pinning this on the back OSS always probably a mistake like progress in the web of space is probably gonna make SSO as a value proposition like pretty useless anyway I don\u0027t think I don\u0027t know what that means for Google and Facebook Simon\u0027s probably nothing but in the cases where Federation actually works and there are a number of cases where it does work is because relying parties actually want and get information about users and users want to share that information with relying parties in controlled ways add your own for instance where you\u0027re basically sharing an affiliation right I\u0027m part of the club and that gets me access to the network and in those situations it has nothing to do with SSO and it has nothing to do with at least not perceive user privacy like but it has everything to do with UX so I think if to make progress in this space you actually have to focus on UX not technology because that\u0027s where the really hard problems are yeah I think I\u0027m maybe to just comment on that briefly the what it does is actually it\u0027s just a service any of MediConnect provider just allows it just provides a service that allows the relying party or the the website to retrieve this identity data when the users offline as well so it\u0027s not just a direct interaction and a single sign-on issue it\u0027s also a service that provides this identity data as a service right and that\u0027s a perfect illustration on this right because like all of the projects that somebody mentioned there are a hundred of these right and they they all focus on on the the backend part of this right they all focus on the technology of blockchains and zero knowledge proves etcetera etcetera right but and then they will add a connect or a sam\u0027l front-end because it all has to live on the web because they\u0027re like nobody has the has figured out how to big you build usable UX for the user and whether it looks like and some people think it looks like a wallet or something right and use replacing a very large identity provider with a very large wallet provider because you know but nobody can figure out how to get users to want to deploy the client hmm I I suspect that all of these projects should just stop worrying about like block chains and shit like that and start thinking about how to get users to want to use the user new UX I think that might help I agree hello "
  },
  {
    "startTime": "01:48:56",
    "text": "Tobias from LMU I didn\u0027t really get what you store in the DNS system you store the key to decrypt the email address so in your example yes we both install the encrypted email address as a record and we also store when we also authorize a relying party then we have to issue a key that is used to decrypt and we also saw that in the name system encrypted for every relying party because if at some point we need to do revocation we might need to update some of the decryption keys and then we basically stole them the name system so and the next time when the relying party needs to access the attributes it can retrieve them from the updated keys from the name system okay and how do you do the authorization for the key basically when we when we are doing the OpenID connect flow the client ID serves as the public key of the client and we encrypt the keys with the public key of the client so they\u0027re only the client can then retrieve and decrypt the DBE okay thanks Martin looks like we have time yeah hi I\u0027m Brooks coffered I work for giant this is my first IAT fi RTF sort of a meeting so anyway I\u0027m here to talk about the next generation of the Internet so if you have been to a welcome and presumably the first time you turn up OIT if it was a welcome you would have at some point come across the mission of the ITF and this is to make the internet work better now in our net foundation have a goal of to create correct the Internet of tomorrow and as such they got approached by the European Commission to work on a series of documents because well they they discovered this to be a problem this is from a presentation by I\u0027m not here at the moment that actually the Internet is not necessarily serving what we hoped it would serve and so in a short space of time it\u0027s well as you can see from the presentations day and the problems you\u0027re trying to solve by being in this room it doesn\u0027t always do what we wanted to do and so to create a next generation internet a user centric Internet how are we going to rebuild a currently working system working to some definition of working because there\u0027s a lot of moving parts and you know trying "
  },
  {
    "startTime": "01:51:57",
    "text": "to build something new or improve the user privacy on top of that working infrastructure is is going to be a challenge so I know net foundation I did it be analysis some some consultations produced some paperwork and in a vision and that has resulted in European Commission funding stream to be able to explore this area in fact it was a very big sort of architectural document of all of the things you could possibly look at and then they decided that trying to work on all of those things individually isn\u0027t going to work the the heap would crumble so we have this next-generation internet open call process where a few of these open chords are kicking off right now it\u0027s being built with a larger ecosystem so in our net foundation happens to talk to giant the organization I work for where the association of research networks in Europe the ripe NCC lots of ISP groups in order to get their feedback on what should what work we need to undertake in order to improve improve the situation and so there\u0027ll be a lot of funding programs coming on stream in the coming years and at the moment there are there are four actually it\u0027ll be an elated slide so this is ngi vision document you can go to ng itu / vision and i get a copy of this report that Annette Foundation developed and this shows the the current funding stream that\u0027s coming on board so this is a pitch for you to come with your ideas to these open core processors and and seek funding to complete this work so at the moment there are four fact of open calls - in green run by mo net foundation distributed data distributed ledger group in in orange and this privacy and Trust enhancing technologies which is the the project that I\u0027m involved in a consortium there\u0027s also three I think the deadline for these these are the proposals for people to run these open core systems so there\u0027s three projects that are currently the deadline is real soon now it may be at the end of this month even so those will turn into they\u0027re seeking a coordinator to run a funding call so that you can target these additional areas and there\u0027s more areas that are coming on stream within this NGO sphere so a no net foundation are looking after "
  },
  {
    "startTime": "01:54:57",
    "text": "our search and discovery and privacy enhancing technology these are open calls that have a very low barrier to entry not everyone\u0027s always successful and but they offer money in the range of five thousand euros to 50 thousand euros they have a and they and they run these calls regularly every two months these calls pop up throughout the the cycle of this project a neonate foundation also offer and other funding opportunities so check their website for more giant is part of a consortium privacy enhancing technologies and we have money up to a hundred thousand euros for unfortunately a slightly weightier application process and the ledger project offers up to two hundred thousand euros on distributed data distributed ledger technology and blockchain projects and you can find this information on the ng ITU website so yeah these are the these are the currently open open calls the deadlines are ever so slightly different so any o-net foundations run every two months their deadline is the 1st of April but also on the 1st of April a new call will open for them and run for a period of two months enjoy trust and ledger have open calls that end on the 30th of April and will be running additional calls later in the year so we\u0027re not as agile as Internet Foundation to have this very rapid process that we hope these these proposals will be slightly longer more involved and we provide some other support infrastructure in getting your applications across the line you can also contact us and we can give you advice and guidance in the lead-up to submitting a call but once you submit something we we can\u0027t talk to you until the assessments out so feel free to talk to us leading up to that and this is how you can directly contact me or visit the NGO our website and look at open courts ask questions talk to me in the break I\u0027m here all week tip your waitresses any restrictions on who can apply what countries they\u0027re on obviously you can yeah you can still apply it it has to be for the benefit of Europe so it\u0027s not exclusively for projects or people resident in the European Union it does make it easier and once the individual open call projects have done their assessment there\u0027s actually a final group within the European Commission "
  },
  {
    "startTime": "01:57:57",
    "text": "that decides whether it passes this bar of being beneficial for Europe or operating within Europe so things there are things outside of Europe that I\u0027ve beneficial to Europe but it would make more sense if you had a project partner that was in Europe and you know and you definitely focused your your proposal on how it is beneficial for your just not beneficial for your pro thank you great that concludes our meeting either the blue sheets floating around thank you "
  }
]