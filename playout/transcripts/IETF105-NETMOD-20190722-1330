[
  {
    "startTime": "00:00:27",
    "text": "it looks like people are slowly coming in from lunch it is now 1:30 which is time to start someone doesn\u0027t mind shutting the back doors welcome to Montreal and net Mon I\u0027m Lou burger Kent Watson build ugly with me my co-chairs the meeting material is in the usual spots we are doing etherpad also you can see the etherpad link which somehow drifted to the bottom you can also find that off the tools page please join in and help us capture what\u0027s being said at the mic it comments on the mic and responses we don\u0027t need to capture everything that said in the slides that is not go to the next page aha we are at the IETF which means we have rules that govern our participation you can find that under our note well this is a summary of that those rules if you\u0027re interested in are unfamiliar with them please go to that URL WWI 80s org about note well that HTML that also give you a pointer to the governing documents since we are Monday it\u0027s worth taking a look at that make sure you\u0027re familiar with that as usual we\u0027re doing video streaming as well as audio streaming and recording please make sure you state your name if you come to the mic that\u0027s very important for those who are remote as well as anyone taking notes I will be jumping on jabber if there\u0027s a people who are willing to jump on jabber and to help channel those who may be remote we appreciate that I think we have had an agenda change I\u0027ll talk about that a moment we are we we wanted two and a half hours because we thought two hours was a little tight we try to squeeze into that because we asked for two hours that pump this into a second slot which means we have a lot more time than that so in theory we\u0027re here to almost 6 o\u0027clock that seems like a long time from now I don\u0027t doubt that we\u0027ll use off of time that said we haven\u0027t added a couple of topics that we think are useful use of the working groups time and I\u0027ll show that at the moment one important note when we go from this room to the next room there\u0027s no coffee break actually more importantly is we are moving rooms but we\u0027re just moving one room to the right when between the sessions we\u0027ve organized the agenda just by laying it out in terms of anticipated amount of time on each topic since we do "
  },
  {
    "startTime": "00:03:27",
    "text": "have extra time we\u0027re not going to we\u0027re going to be a little flexible and how long each topic goes if we\u0027re having good conversation we\u0027re not going to break that break that which means that we\u0027re not exactly sure what what\u0027s going to line up with the session brain so we\u0027re just going to go in the order that\u0027s on in the published agenda and that\u0027s captured here this this slide is unchanged this says the agenda is unchanged the design team items aren\u0027t change the non-charter items are promotes previously been published but we\u0027ve added a topic that\u0027s come up on some it came up on the list and there\u0027s been some more private discussion related to it but we think it\u0027s a good good topic for the work talked about and that\u0027s sort of how to deal with some of these MN NMDA transition issues so we\u0027ll be hitting that at the end of the at our agenda if there\u0027s other topics we can probably and discussed publicly on the list we\u0027re happy to take those also wash laniel Eric\u0027s on can you say a few words about what\u0027s up with yang next so there\u0027s the no update to yang next since the last ITF meeting where we have met I think primarily I asked I think the main issue is we don\u0027t have someone to drive the discussion in particular folks are probably looking at me not just because in the front of the room but because I had coordinated several of those meetings before to do that but at the moment I don\u0027t have a bandwidth to actually drive those discussions much less even coordinate the meetings to for the discussions to be had I did ask someone to do that but they also had not really signed up to do that and so that\u0027s where we\u0027re at right now it\u0027s just sort of a limbo waiting for someone to have the bandwidth to actually drive the discussions so the bottom line is that there\u0027s been no progress since last meeting yeah I also think that the design team activity that we\u0027re gonna hear about is probably taking a bunch of attention from those who might also participate in the egg next site my personal view not as someone who is contributing to that work my personal view is ones that the current design team winds down there may be some more energy so we\u0027ve had some nice progress since the last meeting we have a couple of new RFC\u0027s it\u0027s always good to see those wherever that is why we\u0027re here it\u0027s not "
  },
  {
    "startTime": "00:06:28",
    "text": "to have good discussions well that we appreciate that it\u0027s actually to produce these documents we have one document that is on its way to the isg and another one that I was hoping to push submit on but if we need an update published unless I missed it today which is possible although you know Kent is such a slacker he only had a chair another meeting this morning so he was one who needs to push okay so maybe we\u0027ll have it this week but as soon as that happens art book artwork folding will be submitted it was the real it\u0027s really some minor things related to copyright from what the trust requests on in for code segments and there\u0027s a code segment in this document so that that kind of paid it as soon as that is published we\u0027ll see each other we have a couple of documents that are not on the interface extension is on yeah so I feel like we had something else but geolocation is definitely not on the agenda I warned him I was gonna put him on the spot I guess he decided not be here there was a little bit of a discussion on the list about geolocation it looks like the bat is getting pretty close to being done so if you have opinions on it be really good to get that robust and I think we got the updated yang types is believe maybe double it\u0027s not on the agenda was on that\u0027s the one in fact it\u0027s listed right here to mention so thanks for that appreciate that so at the last meeting we had an update on these gang types and we were actually hoping to move that document through really quickly and we haven\u0027t seen an update uragan is online and if he feels like mentioning something but we\u0027re really gated by the author\u0027s pushing that pillar seeing that one in q2 that slide I feel like I should return the center control so so we don\u0027t take it all right thanks duplicated so there\u0027s been a recent discussion on the list about an errata and then we realized there\u0027s actually a whole slew of virata\u0027s that we really haven\u0027t verified as a working group now technically the aetyi is the one who verifies an errata but generally the ADEs take input from the working group so we think it\u0027s worthwhile to go through each of these we\u0027re not going to go through each of them now but we think "
  },
  {
    "startTime": "00:09:28",
    "text": "it is worthwhile to go through each of these as individuals as the working group and agree on what we think the right answer is of whether or not these should be verified or not now I think we had a couple of them that we had identified it being to be objective there\u0027s only one listed here I thought we had listed two which you can see is as an errata lodged against RC 7950 that received significant discussion on the list fairly recently we think that should be rejected as well and the other one that should be rejected we took a look at and it\u0027s actually a technical change and one thing to keep in mind is is that if you see something in a RFC and would like to modify the behavior of that RFC such that others implement that change behavior the right way to do that is in a disc or a new RFC that updates the existing RFC you can\u0027t do a technical change through a neuron so this is asking for that particular one that\u0027s being rejected is asking for a substantive technical change that would impact all implementations of the document I mean it\u0027s just adding a default so it\u0027s not like a controversial thing but you just can\u0027t do that technical type of technical change and they Iran just by our process so that\u0027s something to keep in mind it is going to be important that we go through each of these and as a work group have a response will probably push that as chairs that if there isn\u0027t something that has a clear discussion on the other 157 84 which is the recent discussion clearly that\u0027s gonna we think that\u0027s gonna be rejected will close the conversation if it\u0027s not otherwise closed if you have opinions on these we would like to hear them we can hear them right now if you came prepared to talk about any amendments dro last if you have not we can take it to us any comments okay that appears to be the last one in this package so we\u0027re going to move on to Bob and talking about blocks all "
  },
  {
    "startTime": "00:12:54",
    "text": "[Applause] yes sorry don\u0027t eat repeat that was okay okay so more reviews please then are saying that one of the issues has been raised by EC I\u0027ve had verbal comment from him yesterday I think he\u0027s happy but we\u0027ll discuss that today and then a separate issue raised on him - not on or not related to this document but more generally but I\u0027m proposing and we discuss and have a potential change to this draft for that so the issue that AC raised here I mean his actual text is there but basically he\u0027s saying the RFC 36:35 defined a load of Ethernet like counters statistics and maybe we should include a subset of those into this draft and in particular he noted that this draft has one counter for or for reporting destination map drops but none of the other ones there well there\u0027s some sort of background history to this and that\u0027s really that we looked at that RC another one we looked at the current a third like mabe and I looked at the 80223 manageability interface Clause 30 and I had a spreadsheet that I liked that effectively correlated these different counters and things and the upshot of all of that is that the vast majority of the counters that AC was asking for either included in the 802 3.2 which is the yang model a 2.3 that standardized by Triple E or all they regarded as being effectively obsolete counters and we don\u0027t want to carry them forward um but there were two exceptions to that the the aren\u0027t included in there that worth discussing the first one is whether we want to add a sub interface d-max drop counter so this would be on the trunk parent interface is if you\u0027ve got many sub interfaces they\u0027re each classifying traffic in different ways and a frame comes in that can\u0027t be classified to any sub interface and you drop it we don\u0027t currently have a drop counter for that I think that might be worth adding in these my opinion so that\u0027s relatively easy to add in a think is that anyone has an opinion on that or is opposed to that okay if not I\u0027ll assume I\u0027ll add it in and that\u0027s fine that\u0027s fairly easy then the the second one is more interesting it\u0027s about the ethernet histogram statistics and that\u0027s this is what they look like so this is defined in one of the existing MIPS or RFC\u0027s I think is a breakdown of the number of packets received based on the length of that frame come that came in and what\u0027s sort of interesting here is that this has "
  },
  {
    "startTime": "00:15:55",
    "text": "been set up to accommodate single tank frames or in fact that actually is might include CRC bytes so or untied frames and when you get to these boundaries then it gets a bit trickier where you have like queueing queue or double friend and double VLAN tags packets and also the 80223 has a interesting view of how you handle jumbo frames so I don\u0027t think they specifically disallowed but I don\u0027t think either the spectra T sort of wants to talk about them or standardize them where is you want to fill out this table properly you want to go up to about 9k as well here\u0027s a long and proud tradition of not standardizing the larger frame sizes okay well I did get a comment from them about it but anyway still so when in discussion with them they included in the standard drops so there there be classified as a shorts in two separate counts for 64 byte there\u0027s this did this counters for a comment with the name of it there\u0027s there\u0027s layton there is a 64 one as well but it\u0027s not included in this set because it\u0027s already specific counter for frames that size I think so when there\u0027s discussion when we do the work with 80 2.3 and I didn\u0027t follow that through all the way to the end I was involved with in the early parts of that as I say him they didn\u0027t really want to standardize these histogram counters and various issues and then the other sort of issue that comes up is that because there hasn\u0027t been a good standard here that goes to these higher ranges that different vendors hardware does different things they choose different bucket sizes so you can\u0027t really usefully define these bucket sizes and expect people to Rev they\u0027re raised external support that doesn\u0027t make sense so there was a proposal may be that we could standardize this in is part of ITF instead and the suggestion is David law so the chair of three I don\u0027t know how I\u0027m not sure what they would hit but it might be easy for me to email him directly first and then see and I actually think yes so this came "
  },
  {
    "startTime": "00:18:58",
    "text": "out of effectively the discussions when I City 92.3 young discussions so that\u0027s where this came up and that was the session at the time that that might be a way of getting around the rules pragmatically so so basically the proposal here is that you could and rather having strict bucket definitions we could return a list of bucket entries and where each bucket entry defines what the range is covering and low and high end inclusive and accounts the packets that match that bucket range so it\u0027s a bit more of a both in terms of the data model but it\u0027s more flexible at the same time we would give recommendations in the description to say we recommend you choose these bucket sizes so and trying to sort of encourage heading towards a consistency somewhere so I had the question is do I have these in now or can be deferred so I think the question is do we try and do this now in this working room last call at all or if we do want to try and do this I can speak to David law and find out whether he\u0027s happy for us to do this I\u0027m fact if I can knock it up and say that this is what it would look like but that\u0027s my question is is this the right time to even trying to do this that\u0027s maybe a question to the chairs there in the working room care I don\u0027t know on this specific one yeah I don\u0027t know one that I think probably he would I don\u0027t know he had an EP know with trying to lay this worked to finish that\u0027s awful not so tim carry nokia so the question I would really have is that those those sizes that were there were therefore identified for various reasons right you know for for good reasons at the time right and so the the question is is and they have tools and tests that that go around that right and so what we said is okay we\u0027re gonna provide some you know abstraction if you will or ability to do some meta work on that such that we can you know define the ranges themselves and then provide some guidelines to two different pieces the problem is that when you say you have a guideline now you don\u0027t have anything that you can standardize that I as a client and I as a server can really agree upon I just deal with section so I can\u0027t guarantee that certain servers that would implement these things would implement say packet size 256 through 511 so I can\u0027t really rely on that right I have to use a a mechanism may be an operator has to get involved in stuff like that "
  },
  {
    "startTime": "00:21:58",
    "text": "so that\u0027s the biggest certain that I have of using you know the extract portion of this is that you\u0027ve lost now the first class Ness of oil that of that range that people can depend upon when they\u0027re a client so it\u0027s a good point my aim is not to change these bucket sizes here so that\u0027s not what I\u0027m trying to change I think everyone who implements these will implement at least these bucket size yes people that are implementing things that might not know that they need implement those specific sizes yeah that\u0027s the problem that I have it\u0027s the ones that bigger than that these the ones that are go up to like 2k for kak those the ones where and especially around that 15 14 15 18 by bucket size you get more in consistency right so my question is is that where where do we put a user is there a place is a place where we would put ranges that if they do implement those that would be able to provide a normative stance to it right yeah that\u0027s the thing that maybe it\u0027s an identity maybe it\u0027s something to that nature yeah maybe right yes okay yeah yeah Joel yeagley um I definitely see the existing values as being a product of historical decisions that were made in the I Triple E like I mean you know when you decide that you need something bigger than fifteen hundred because you added a VLAN tag right that is a specific historical decisions like there\u0027s a lot of increments that you could come up with above 1500 that have various kinds of historical meaning but are not specifically like anchored to I Triple E standards so you you\u0027re going to end up with like you know 15 19 to 15 for T and then 15:40 to say 44 70 and then you know uh nine thousand nine thousand fourteen ninety 190 120 and then every Ethernet vendor trolli specification up to there and then Intel 116 Kay because it\u0027s a round number right so like I actually see the likelihood of us getting a really like good list that goes above this as like English in a short period of time that the I triple you will go yeah that\u0027s cool as being pretty low so mmm maybe we shouldn\u0027t if from my vantage point maybe we shouldn\u0027t do that between the last call and hopefully when we take it to ITF last call all right that seems like a risky item to add to progressing this see just for clarity you saying it\u0027d be best to shelve this affected a than that yeah I mean I think this is I think this is this is good enough "
  },
  {
    "startTime": "00:24:59",
    "text": "like I mean from my vantage point is a jumbo frame user you know the there are Jumbo\u0027s or they aren\u0027t so but but that\u0027s that\u0027s that\u0027s my network like I don\u0027t I don\u0027t spend a lot of time distinguishing between the say that ninety one hundred byte packets and the nine thousand five packets because I know what my empty mem MTU is set to yeah right so for clarity these counters the ones you see here not defined in any yeah model today right so we don\u0027t have those at all so they so I trapeze shied away from putting these in for historical reasons because of where they need to get up to yes but these these sorts of boundaries exist these boundaries that we have here particularly the last one are very very specific to historical yeah precedent so like these seem uncontroversial like because they\u0027re kind of widely implemented and you know encounters that we already use right okay like so doing them in yang doesn\u0027t see that weird okay correct is that true I don\u0027t know I had to check whether I cut my movie if the I Triple E Ethernet management API the clause 30 defines counters for these it I sort of suspect it probably doesn\u0027t but it should have done okay but I guess the point is my point is if we there\u0027s no reason for us to add these counters if we\u0027re going to stop at 15 18 or 15 19 whatever that dad doesn\u0027t seem a good reason for why we would choose to do that because they\u0027re choosing not to standardize these counters they could have done these if they wanted to so if we want to do it we probably want to at least fix it so it works for other use cases higher so even if we define this we could have hard-coded counters for these ones and then the ability to define extra ones and people want them above that and give some recommendations of just doubling the size each time that\u0027s what I would do I\u0027ll do 15 20 "
  },
  {
    "startTime": "00:28:12",
    "text": "then moved on which one thing is on different that people are starting to count more or jumbo frames I don\u0027t see any jumbo frames here and this Plus this whole concession yeah so because between 59 any around 1500s to with there are different numbers there they\u0027re different sizes and there are so many variations but then jumbo frames you know like the nine case one that is where they are really getting interested right that is he you know is it 9,000 9,000 190 191 2016 kay but like I would say anything about 1600 put it as a jumbo frame and you know but 1519 what we do if you have like 1520 to bid so we like put it up teen 1600 and anything about 1600 count as a jumbo frame but then most the point having these counselors told that I mean what\u0027s the point in history I\u0027m cancers for everything below 1500 say or 1519 but not having them above seems like it gets in 2/3 information are the inflation\u0027s useful or is not so I can tell you from some debugging the MTU sizes into so many variants of the 1500 it is just really annoying and then the thing does work because the MTU size is just like a different number in your likes damn it and the 15 numbers they\u0027re all over the place yeah but they are always definitely below 1600 and then next question is oh are those jumbo frames what image most trans pocket I would to add to this discussion maybe we should also add pockets in and pockets out counter because this we don\u0027t have we have unicast and multicast pockets and for various reasons about farad just don\u0027t implement differentiation between unit costs and multicast pockets so really the the ITF interfaces model is impossible to implement on those devices there are many devices like open flow for example devices you cannot implement interfaces model for open for devices you cannot diplomatist or on reach through traffic generators and these are supposed to be flexible devices that you actually should not have any problem implementing the idea interfaces and I think I am against adding eternal specific counters in this draft maybe there should be another draft that should add them but this one should be kept as compact as possible I think now it\u0027s to watch already okay yeah so the John\u0027s your question about the total packet counts "
  },
  {
    "startTime": "00:31:13",
    "text": "in and out I I could be mistaken I have a feeling that went into the a 2 2 dot 3.2 I think they might have a total package in and out so either that count I think should be neither at night if interfaces or it should be noted to do 3.2 you give me a look that\u0027s not the case but I\u0027ll double check but that\u0027s no more questions on that or comments and no it\u0027s the same thing yes well I think that I think we are at the end of these time to ask the question so the questions you want to ask effectively is who do we try and do this now do we defer is that the fair question you no harm or Powhatan so who thinks we should delay this and add these histogram counters in now which could be contentious who thinks I should do this now adding now adding the histogram counters that we do that though I seen two hands go up and down no one is holding their hands up is so very very few for the minutes whom how many think we should not now it\u0027s not a big number but it\u0027s clearly percentage-wise it\u0027s huge but so you know I think the sentiment of the room is to delay from my standpoint as chair given the level of discussion we\u0027re having what\u0027s that yeah that\u0027s the suppose we you know as chair and actually Sheppard\u0027s my view is the seems like a little contentious that ad now if it was important to add it we certainly should but I don\u0027t think it\u0027s critical yep specifically stating the opinion now because I\u0027d like the ear of anyone in the working group disagrees with that so I think we\u0027re going to leave this meeting where we\u0027re not going to add the histogram counters including any event the ones each other so these are not "
  },
  {
    "startTime": "00:34:14",
    "text": "going to show up in the document so if you disagree with that now is the time all right um can I can I ask one more question which is who thinks that we should try and stand ice if we don\u0027t do now as a separate draft or separate work item to try and standardize this and these counters in what something like it Joe Clark Cisco clarifying question do you mean standardized at the upper levels or just standardized on these buckets below 1520 I mean these on the upper levels as well coming whole range okay I think you should yeah sorry I mean sorry have bigger frame sizes higher numbers higher higher frame sizes sorry yes higher frame sizes I think this should be standardized because for the performance measurement and troubleshooting across a service topology having that information to be uniform and be able to calculate the histogram either you know at the service level effort you know is very helpful when you\u0027re trying to debug and troubleshoot the for automation okay so I propose that I try and email David law and say we think about doing this what does he think is that I mean then we\u0027ll go from there okay the MTU issue which I hope will be easier although MTU is always contentious contentious thing so there\u0027s a network thread titled question regarding rc8 three four four and those basically the premise of what came up there is I think that the Linux default loopback into you is six five five three six bytes long whereas the all the m2 use in the ITF models are limited to you in sixteen and in 65535 somebody pointed out that an IP layer having them to you above 65535 anyway and this model does define an L - M to you so it defines it into you across those protocols and these currently defined as a UN 16 the question is should we change this to you and 32 instead - specifically to cover this Linux live back into you case any thoughts on that opinion yeah definitely because different interfaces if you take out a do not take Jeff bigger number not only the Wako interface there are other interfaces well that the go above "
  },
  {
    "startTime": "00:37:15",
    "text": "us I know you can also set it to megabyte right so like this actually help helps us solve that problem okay so that someone else does it brilliant okay so assuming movies against this then I\u0027ll just switch that to you and such - that\u0027s the last one so any other comments on the interface extension model in that mail that actually brought up that it was my mail to the group and that was in the end there was another suggestion in addition to the l2 MTU I think it makes sense to have MTU which is the MTU definition actually used by Linux like when you use the if\u0027 interfaces configuration any glimpse what you get is actually the NQ without the the header all together that\u0027s very important for most people who use Linux they are used to that value so this this has to be in the model I think and there is advantage when you when you define protocol configuration based on the ITA interfaces you want to know the the the pocket payload you can use so when you compare parameters you can when you are configuring other end to use if other protocols you can put a must statement there limiting the size of that end you to to that empty you so this makes a lot of sense to have it there and it\u0027s more elegant here just interfaces interface and to you kind of elegant l to empty you and especially the limitation of taking out the view on in the description statement creates complications because you have the moxx that actually just don\u0027t care about it it\u0027s a v1 edit or not the into you is the same register value so if you standardize this in the young model it will be difficult implemented like you won\u0027t have a future not sending but it\u0027s bigger than that sizing harder and you have this difference that a v1 edit should not be a problem it should go through you cannot do it with the existing hardware that\u0027s another point I I have about this so a general in empty you that corresponds to the Linux definition of MTU and is something that I proposed vladimir can you remind me your last name bacillus I see them so I\u0027m nervous about that for several reasons one is that I think losing the l2m to your configuration value is probably a mistake I think there\u0027s lots of systems that use that as there is a configurable value on an interface so I "
  },
  {
    "startTime": "00:40:15",
    "text": "think changing that to an l-3 MTU so payload m to you as you say would be probably a poor choice I think then the question is could you have both in coexisting but that would also you could you could probably allow either of the two to be configured potentially but I\u0027m not sure how the constraints would work so if your constraint was against a payload based em to you but the user to configure than L to him to you then it wouldn\u0027t necessarily work I can certainly see how you can report both values in the operational state to say this is the the full frame MTU and this is what the payload LT payload em to you is that would be more feasible well my argument is that the paywall and view is what all the RFC is up to now in the eye idea for using yeah so it is strange that we are not going to have that end to you as part of that instead we are going to use l2 m2 which actually you can derive from the type of the interface if the interface is Ethernet there is no doubt what is the difference between the MTU and the l2 empty so why are we going to bound ourselves to the interfaces and the interface type is not going to be used as information source for that calculation it is going to create confusion s on this mailing list like this person was trying to configure NTU for the paywall thank you and he was confused that he cannot do that did you add confusion you are adding another empty which means something different from everything else that\u0027s buddy but again it is back to what the hardware will place often the hardware framers and things might place a single value and the value of everything is the l2 frame size not necessarily the IP or the l3 and the l2 payload so that\u0027s one complexity and in terms of the same discussions about historically might have standardized the l3 em to you they did do that they did that for LT VPN and they\u0027ve ended up in a world of pain because of it so in the l2 pn specs the MTU negotiate across the wire is the payload m to you but you don\u0027t know what size those headers are so you can\u0027t easily agree that value is very hard to calculate when you want L to frame coming in to say well this amount of its headers without having to analyze the PAC you don\u0027t have tags on there so it\u0027s a very strange value to use so I\u0027m still nervous of of moving something that\u0027s doing an l2 check to be using an L three values of configuration value I do get the Linux thing that\u0027s it\u0027s unusual it might be that that\u0027s a mistake in Linux for using that choice so I always struggle the tournelle to up to you okay I don\u0027t believe those are the revisited the term was used in the I "
  },
  {
    "startTime": "00:43:16",
    "text": "Triple E and the question is better than you use the same term believe it says that its maximum frame size and distinguish the maximum frame size from the IP and typically the original definition of them to you I know people are using boost yep convinced we should be perpetuating that tend to you is the maximum size of the IP back that\u0027s the that\u0027s the definition of MTU that\u0027s my memory that\u0027s my memories is it\u0027s the maximum IP size and the the use here were in the document is saying L to UM to you you\u0027re including the L to headers and I think that\u0027s more back again though eager that terminology max rate size and it may be helpful I don\u0027t know well seems we may be helpful to use that term I can look at what\u0027s in a teacher of three is wrong but I mean in to use very standard he used and it doesn\u0027t mean different things in different places pretty unambiguous what that is all framing to you but I think you\u0027re moving away from tears and of course confusion to people because since we do it already and it does I think it\u0027s whatever we decide it should be either no worse than what we do now or less I\u0027d like to see if there is any artsy anywhere that says L - M - you deal TV parents should but it doesn\u0027t my observe as an operator and maybe things have changed over the last couple of years where I haven\u0027t cared about interface configuration actually the MTU thing is very confused we have been well okay in the configuration languages it shows up as far as I remember and there are vendors that have different of interpretations between vendors there are vendors that have different interpretations regardless depending on the line of operating system running on the hardware and then my observation is that the usual the usual configuration is I remember it is kind of well okay a rough estimate anyway because I don\u0027t think I "
  },
  {
    "startTime": "00:46:17",
    "text": "don\u0027t think I ever had an MTU configuration that was actually that was actually precise for limiting my MPLS MPLS label headers kind of so I would see two questions which well okay which concept is the model to be to address and yeah well okay certainly the previous question should the concept being addressed here appear to be at a precise one or a more on mirroring of the existing situation that well okay we only give a rough number and let let the ops people someday perhaps figure out that yes there is one MPLS label too much too many so there\u0027s there\u0027s different people different ways different people implement this on some OS is as you say you put give it an L 3 or LT payload MTU and then they add on some slop and then say anything that\u0027s between this is fine there\u0027s other ones that have a values like this l2 calculated and they will check that strictly so any frames above that size they\u0027ll drop it as an ingress Rob and that there\u0027s different approaches and as you say different vendors do different ways in different os\u0027s between the same vendors can do it different ways probably a precise layer to is the one thing that is conceptually easy it\u0027s going to backfire when it\u0027s it when it\u0027s going to run out in the configuration languages and into ops okay and it looks like there\u0027s two RFC\u0027s both from the same basically set of authors in the last year or so in that space that you talked about that you use the layer two of tu contacts I think for something that\u0027s so general we should not introduce it here but stick with figure out whether the previous RFC is help them to use maximum frame size I think that\u0027s probably the safer term and it\u0027s I\u0027m not gonna say that we\u0027re gonna clarify we\u0027re gonna clean up the confusion because I think there but we won\u0027t make the worse yes nothing else I think by using the term layer to um to you that\u0027s partly okay and I\u0027m sorry to see that that one okay so that was all "
  },
  {
    "startTime": "00:49:23",
    "text": "the what questions are had on that the southern face this is also working group last call this one is much shorter in terms of what I\u0027ve said to review support publication and no comments received as I say possibly that means that I could flawless but otherwise if if you interested it\u0027d be useful to have a review even you\u0027re happy with how it stands now and this Palace worker Glasgow process I don\u0027t know when the work loss was meant to finish but we need more reason that said as Shepherd I\u0027m not going to be able to look at this until at least next week and technically all the comments are closed you still have some comments that need addressing before will be ready to go and there\u0027s going to be an IDF last call that a people can spit comments to so rather than to be really strict about it I say if you have comments it\u0027s not too late to send them I will send out the formal last call being closed when I started when I have time to start processing it after the IDF hopefully you won\u0027t get comments after that but comment whatever it shows up we should try to address it yep and I\u0027ll say it again there\u0027s an opportunity because we do expect at least one update yes before we move it forward so we encourage anyone in the working group who has not recently read these documents to do so and if you have comments please send them by the way I did it rep on max frame size it shows up in a lot of documents Marsten Jeff bar Sonia from Ericsson this has been a new edition of this young instance date the draft yes second so the concept was that we have many use cases where we want to document instance data so not the models themselves but the actual values that integer strings and we want document them offline and potentially hand them "
  },
  {
    "startTime": "00:52:23",
    "text": "out to customers or store them somewhere these are just some of the use cases I think there are seven or eight some of them are detailed in the document and it was decided that at least will needs a metadata about the instance data so when was it produced what models are documenting some administrative data like a name a description stuff and we at least need two formats XML and JSON so the draft was updated it received quite a number of comments close to the last IETF earlier the modules that define the content were called to target modules number of people didn\u0027t like that so now it\u0027s called content schema and one or two case where I have to refer to the individual modules then it\u0027s called content defining yang modules chain terminology and change wherever that came up in the inside the draft some people the yang instance data set itself has a name which most of the times is I think is needed some people insisted that they don\u0027t oh don\u0027t always want to have that so it became optional there were some up this draft is using the yang data and your yang structure is that it\u0027s now cooled it was updated according to that draft including yang trees and all that and there was a comment that entity tags and last modified the time stamps that are very quite useful in in rest conf are actually encoded in HTTP headers and we can\u0027t use HTTP headers in these two formats so now they are defined as metadata and if they are used they can be encoded as metadata and next so this is an example of an update the instance data set a few things are missing like an XML header because they didn\u0027t fit on the slide they are included in their examples in the draft but not here so here we have yeah concerning the previous slide about this entity tagging last modified does it does it mean that if you have a Content schema this content schema has to have some modules that they find these annotations or no no the the IETF instance data yang module defines these annotations because I can see of course a good use for many other annotation "
  },
  {
    "startTime": "00:55:25",
    "text": "that can that are or they available or that may be defined in the future so I don\u0027t really see any need for because as you said currently this entity tech and last modified are used by by the Raskin server as a part of HTTP headers so I don\u0027t know if somebody wants to define it as as an innovation that that\u0027s fine but in this case this can be the same for any other annotation so I would suggest really to to require in this case to have some module defining entity tags and last modified s annotations and then you can use it normally in the content schema and using in the data I think they are quite useful bits of information and they\u0027re just doing a module to define these two tags when clearly we use them here I don\u0027t see the reason to split them out so they won\u0027t harm anyone they are obviously useful in my view I didn\u0027t want to define them because I thought the rest comfort handles that but as we can\u0027t use the restaurants and coding solution I like them but it seems it\u0027s very easy to remove them but I like them I go to again other any supporters of this this formulation in the draft currently so is there any reason why to handle these two annotations in in a specific way because these are well-defined bits of data and they are quite basic information for the content sometimes they are sometimes they aren\u0027t I can argue the same way about the origin annotation for example it\u0027s mentioned actually and the origin annotation I think is defined somewhere else so I don\u0027t need to redefine it can we ask them what are you saying you think that the instant file should not have time stamped or into the last modified no what I am saying is that we should treat these two or other annotations in in the same way and the proper way and neutral way is to define a yang module that defines these annotations and include it as as part of the content schema for for this instance data that\u0027s that\u0027s the normally published either in line or in any other way but it\u0027s it\u0027s done just just in a normal standard way rather than mentioning it\u0027s "
  },
  {
    "startTime": "00:58:25",
    "text": "specifically in this human so if I understand that it would be an annotation right yes see it\u0027s a Seuss\u0027s question of what I was to find in this module or a separate module you\u0027re saying what I\u0027m saying is that I would define a module that defines these two metadata annotations and to detect and last positive oregon as a separate module and then i would include this module in every in in every content schema for the data where this two annotations are of any use of course if if i have instant data that have no use of this last modified probably I would avoid having this module him in the stream do you see other uses for these annotations these specifically these two beside the cone ITF instance data are you asking about other annotation out for these two annotations are you see for seeing any other use I don\u0027t know it could be possibly used in net conf for example for me it\u0027s easy to remove it it\u0027s not critical for me as an author that we have these on we have these annotations in the yank file I am not only think at this point to start a new draft to do this of course it\u0027s not critical for me either but I think it\u0027s a bit I rent it as being sort of tunneling the metadata you get from the Netcom protocol or rest Tom protocol and so you were trying to add a parallel piece of information where you know we don\u0027t have a perp on the wire read before that night so it was it was parallel to that because that\u0027s not what I was thinking which is quite different from where a lot of sewing where it becomes part of the content what was for beer intent are you thinking more its content or metadata is the parallel that I think it\u0027s it\u0027s a Barolo what we have been the rest but I also agree with both of you is it\u0027s not a critical item my understanding was that it these annotations could be just attached to any and in instance noting in the data in an array right we are quite I want to be liberal what can be here so if you want other annotations they find wherever yes that\u0027s also possible of course that was "
  },
  {
    "startTime": "01:01:26",
    "text": "my understanding I can have any hang no use being part of the content schema so why not why not any other annotations contributor I think a lot of what you\u0027re saying is in restaurant on the top-level node must have the e-tag and if modified tags Internode\u0027s may have them and so to your point any node may have them and so there\u0027s that but then separately was the question of why do they need to have this information in the instance file format like who would consume that what the use case behind that they would want to be with you consuming this data I think we should have that discussion on the list and and then we\u0027ll conclude whether or not it\u0027s important to define the now or later I don\u0027t know actually who could use it I think it\u0027s useful information yeah the use cases doesn\u0027t cover these tags at this point but my friend is that we needn\u0027t really care about use cases for these two annotations if somebody does have a use for them they can define this module defining the annotations and just go ahead and and use them so that\u0027s fine but it didn\u0027t be an issue for this document because it can accommodate any annotations yeah it\u0027s really the questions really that is it reasonable to have them defined here or we\u0027ll leave it to some later later point well if that ever happens that\u0027s a very very small so how many like the approach that draft right now interesting we see a few hands that were not raised before so I\u0027m really confused by Roberts ago I\u0027m not sure how important this data is so that\u0027s the point of view that don\u0027t really care that much either way the moment it just needs to be one sentence in the draft saying that these are done the same way as restaurant which is fairly minimal text I to getting ladders point about if we go to do weather why don\u0027t we do it generally but I still also see you could have this line in the draft saying this is how they\u0027re done and it still be done generically for anything else even these I\u0027ll say it\u0027s less then the first one none of these are statistically significant numbers "
  },
  {
    "startTime": "01:04:26",
    "text": "but still it seems like there\u0027s a very very slight preference of room to stay as it is I would say it let\u0027s keep it but also ask the bank again on the list if people have any comments okay on this so I\u0027m just looking down to see if there\u0027s anything from from jörgen says Iran other Arab support for annotations and have doubts that exposing HTTP internal metadata is terribly useful I doing the instant format is for all agnostic okay so to me he just said two things that are I don\u0027t think that let\u0027s say the last modified tag is HTTP specific that\u0027s datastore specific and data specific please okay so here is an example somewhat wrote an example of how this would look like the most interesting part here is that we have some metadata like the name provision description description in the description of the instance data set and then we have the specification of the content schema here we have an example where the content schema is specified in line first the yang library is used to say that the angle I bury module with format will be used to specify the content schema and then inline content schema really specifies that it\u0027s the Netcom faecium module that we are handling and this some more terrible it was nicer early originally so this is the content data that\u0027s the really thing that we wanted to communicate you can I don\u0027t know what happened with the site anyway can you skip back one more can you skip back okay so never mind here we okay here we have this in line from inline content schema definition there is also a possibility to just put a reference to the content schema if you don\u0027t want to repeat it for one in the case that you have I don\u0027t know diagnostic state data every five seconds "
  },
  {
    "startTime": "01:07:29",
    "text": "then you don\u0027t want the converse schemer repeated every time and that\u0027s it and I think I would like to bring this to work group last calls but I was like can we have that one question one non-critical question as we agreed with low down on the earliest part of the last goal another question regarding this inline specification this seems you are now using the new yang library but I think that the idea is not to define like data stores and things like that that you only really need this module set yes so does it mean that you can use other parts of the nyra here or is it really restricted to this module surface you can use the yang library in the cone in the content part and here you could have parts for deviations and for features you I didn\u0027t actually put in text saying that you should not have text about I don\u0027t know datastore lists but they are kind of meaningless and unless the data stores can be meaningful if you specify which data store you are putting it which part of the yang library I think one question is whether we really need to have this flexibility of specifying different yang library revisions in Olin different ways of doing that because possibly we could use the old yang my brain everywhere and this this would do probably for this person purposefully also agree with you except that in the last round I got those explicit statement that you must use the new young library and even there that it should be possible to use something else beside the angle ivory so I was specifically asked for being flexible one more thing that\u0027s not visible in this example that here in the inline spec you might have other modules one thing that immediately comes to mind is the yank versioning where you could which augments the angle I bury with version label that could be useful here but I was asked to put in the flexibility and it doesn\u0027t disturb it seems to add a lot of complexities as "
  },
  {
    "startTime": "01:10:29",
    "text": "you just said and in my opinion I am not sure if it\u0027s its complexity on the mailing list there was a I don\u0027t know I agree that it\u0027s not the complexity is bit too much but I was explicitly asked when the mailing list here again and maybe some others that we should have this flexibility reply to it you know raising the issue that a lot of that I think yes I wanted yes I don\u0027t want it Bob Wilson Cisco so I think almost three choices is what I prefer one is a very simple one which is just this is the list of the modules and their revisions and that defines the schema so without even needing the inline content schema just that list there\u0027s one choice the second one is what you\u0027ve done here where you specify the what this schema is in that way and the third one is a remote schema I just see that here if you just want to return the data for one or two young modules listing them in line without with having less metadata boilerplate at the top of the file is probably beneficial but xx you tend to read these I don\u0027t agree with your first method sorry because this is a simple case you need to have a place for features supportive features you need to have a place for deviations and you also need to at least specify which version of the yang library you are using or which which that\u0027s a you say that you just want the list yeah you want to say what what who defines or what defines the format of that list so theists ways we already have one in yang library wide define a new one but then you have might have multiple versions of that you need to them so I\u0027m not saying take away what you have here I\u0027m saying add add another third option that\u0027s a simpler version of it that doesn\u0027t worry about deviations doesn\u0027t worry about features it\u0027s just the data that you\u0027re uploading you don\u0027t you could potentially just have the features enabled by default I think deviations and features are very but I I think they are needed you still express them using this format it\u0027s just having my questions really is on for some files if you have this boilerplate at the top of every file is that a good thing or not is it is it that actually is just noise and for the vast majority of cases that if you\u0027re just returning a couple of modules that you\u0027re doing it quite verbose way you might gain I don\u0027t know five lines but for but with the complexity that you have add some more modules and then if you have deviations or features then you had to add come back to this one so yeah you could add "
  },
  {
    "startTime": "01:13:30",
    "text": "more I don\u0027t think it\u0027s needed just for a record I also think the new who are the me I also think the new young wire is overly complex for the purpose of creating an instance data file and you sound like you were told on the lists I think no one actually thinks that instance deadfall should contain multiple stores I would rather have multiple files containing multiple data stores so it\u0027s more like atomic and it\u0027s not overly complex but this discussion against having it done this way there was a discussion and more people should have contributed to keeping a simple single data store my way now we have a new young wire which is obsoleting the simple you need data store one so it\u0027s very difficult we are going to use the old one to make new RFC\u0027s it is going to create even more confusion so I regret not having more support when there was a discussion that we should keep the same young library and use different mechanisms to achieve the goal it does then it was only me and maybe on the Beermen who was opposing and everyone was completing to that so now he just had to use the new young library I guess I don\u0027t think while agree I would have loved to have a more simple solution I don\u0027t think these five lines are worth to discuss one question what Rob said previously in my opinion if prop writes a module with a IDF behind module list and then insert this module into this in line spectrum the simplest of modules without deviations could be possibly used given this flexibility here right so you still need at least the revision yeah we could add the third method if that\u0027s so yes I\u0027ve actually written such a draft escucha ITF young packages we could add the fourth method because there\u0027s always the matter that the receiver knows what what is the content schema in you do to some offline method or some tech implementation specific method which I think will be quite common as well but if you want we can add the for chatter bit I just like "
  },
  {
    "startTime": "01:16:31",
    "text": "the chair ask the chance to get to some decision on these because none of them you are critical and we are going back and forth and please help me with to get decision well welcome everyone my name is Hugh I\u0027m here to discuss February for setting you next so what is this job about actually a day job to actually define a new IP say we call the factory reset a PC and we also introduced a new faculty for the data store it is read-only data store and obviously no data store actually we typically use cases we use these faculty for all settings we can use these in the zero-touch conversion stage also in some cases you may actually major the each error on the conversion you can use leverage it is reset kappa reset a piece a to reset the device to the fact default state so the current status actually this chapter actually we have two code option for this job and we resolve summary issue actually I sing in so the changes we made actually in this draft actually in in a flourishing they were when working actually my issue is about terminology the young server waited fun people have some concerns so we actually try to reuse the existing terminology like a server they found in the MB architecture ii mentioned major change actually is about how so how these factory reset apply to the data store actually we we can apply all the data store but people have some consent maybe we should you know take out that candidate so we add some text to clarify this and so in second for adoption actually we also raise some of issues and in versions there were two actually the may there\u0027s a two issue we try to resolve one is the security issues and we make "
  },
  {
    "startTime": "01:19:34",
    "text": "some proposed and so we were to Scotty so as the open issue and otherwise the copy config actually this is we actually extend is a copy config operation to support security for setting but is not you know a beautiful set in specific so we so the result is we remove these copy config actually so so this just to reflect the discussion on the many needs that we already know remove these copy configure and as I mention actually diseases and not the factory before the specific is so so also we actually release the sev several MDA protocol like American for MT and the rest come dear support actually it doesn\u0027t define the kaabah config like RPC actually so but it will be useful to have geta configured because we have faculty for data stall so it\u0027s it would be useful to to have the configure to to to allow get a configure to you know get it access to these data store so I sounded the the many nice discussing we actually remove the copy configure accession from the module in the job actually we defer these to the context that your choice so the second issue is about the security issue and because for the reset our pcs and many focused actually to reset the device to the factory default state but also it will be useful you know to use these up easy to clean our the fire we started to notice some of the software process or you also can set the security password or data to the default values but all of these information may be sensitive so we need to actually issue the SSL relevant discussion Adam Kaufman in is related to the Crystal draft actually but we we sink yeah the cognitive or set they stock could be useful for the pistol draft and but we try to resolve these conditions so the proposal we that we propose a some text we can use some encryption or a sign mechanism also talked with our Khalsa we think maybe we should you know around the access control rules to to protect the sensitive data so that\u0027s what way you have but we are not a security experts we want to hear if any additional import for these are Joe Clark Cisco um any "
  },
  {
    "startTime": "01:22:36",
    "text": "input on any of these bullets are just the last one last one okay I don\u0027t have a input on the last one but I really don\u0027t like bullet number two the way it\u0027s defined in the draft I could have my device reboot if I set this RPC that as an opera squishy I would rather these things be more atomic like I use the RPC to reset the config and then I might send another RPC to reboot the device oh okay let me restate I would rather there be some more definition around this so that I know what is is necessarily going to happen and yeah it might be that I only want to factory reset the startup and then send an RPC to reboot the device as an example but angle Ericsson has a co-author about the last point that factory default might contain security data I think that\u0027s actually not a question about the factory default data store because the same data will be available in running after reset so it\u0027s the responsibility of the data model to somehow protect this the security critical items and this that man this should be the same in running and and fact factory default data store so I don\u0027t see why this is specific problem to this draft can\u0027t as a contributor and also author of the keystore draft work this is being discussed so the the actual data lives in operational but its desire it would need to be promoted to configuration in order to be with referenced by configuration and of course it\u0027s the in shipped from manufacturing it\u0027d be ideal for to be in the factory default in store or I mean perhaps startup but all right the problem startup is it could be deleted thereafter I mean it could be a choice what are the other thing it\u0027d be a convenience it\u0027s not a security issue though because the data would if it\u0027s hidden it\u0027s hidden so that\u0027s not a pretty issue and it\u0027s encrypted encrypted that\u0027s my situation okay so it\u0027s not a security issue per se okay Jim Carrey Nokia so two points one is kind of agree with the last speaker of the the fact that you know the factory default I don\u0027t understand security issue because I would understand that when we reset something the factory default the factory information is going to be used to populate the startup right you know "
  },
  {
    "startTime": "01:25:37",
    "text": "that\u0027s effectively what\u0027s happening to the other question about the options and another protocols that we\u0027ve done this with and we\u0027ve done factory resets for CPEs for going on 20 years now right and in a standard way we we actually allow for the other things that you\u0027re talking about cleaning up files or restarting to know particularly we starting to notice simply being options that go into the RPC so you just simply say hey look I\u0027m going to do a factory reset by the way restart this thing when you\u0027re done to answer the question Tim answer your question of why a security issue I think something that\u0027s a little different here is this this has to be done completely remotely and in many of the factory reset options that come up equipment you have to do it locally you can\u0027t do it over your network management wait there\u0027s something dual out network management but there\u0027s some systems that don\u0027t and there\u0027s definitely security implications if allowing you remote access to reset a device a network device sure so even with you reset the network device factory reset again we\u0027ve done this for billions right so it\u0027s not like this is new right so the the the information that\u0027s put in the manufacturing store right is is then you certainly ok I\u0027m sorry yes damn Bogdanovich TPM in the trusted computing you know they\u0027re they\u0027re solving those problems and I know many hardware vendors are putting TP and modules inside their hardware so that helps you know somehow solve some of those problems so kenta\u0027s contributor and again i\u0027m discuss the keystore craft and then that how important group indeed that isn\u0027t where or expecting the TPMS will be used to protect the the keys ship for manufacturing that\u0027s exactly what was discussed in this morning\u0027s presentation ok Rob Wilson Cisco is just an ad for the our pcs if there\u0027s security concerns with that surely knock on wood discover programs as controls of our factory reset all to see so that means anything particularly differently oh I don\u0027t have a response to you I just have another addition that\u0027s in the Mike my queue behind you so and he said I agree that the system restart I our PC and Mike and yang should be argument if it\u0027s to the new or receiving 40 steps "
  },
  {
    "startTime": "01:28:42",
    "text": "right for system restart actually this is something we already discussed actually we and I think the the property there are different them I\u0027m not sure whether we should do the argument from the system restore both Francisco assigned liked what Tim said I think that made a lot sense to me that you just have an option to say also white files and another option that says restart the device if if that\u0027s that seems fairly easy to define that if you don\u0027t specify that the device isn\u0027t restarted okay I think that\u0027s the I think probably we will resolve some real issue and probably which you asked to move to the next step Alex yes hello can you give me yes you sound great okay Nixon okay yeah just a quick update on the comparison of an MBA a co-op again reminder this Beijing this the center of this tariff is RPC did it well prepare an MVA data form and we\u0027re basing the idea is that you can this provides a view tool to report all the differences between data stores without eating two upload the entire thing in private apply application of this opposes r2 for the troubleshoot conditions which are due to unexpected failures stink issues between data stores lagging to change propagation and so forth so we did post a few new revisions the current revision is zero tool the main changes that were applied are on one hand basically the yang patch format who report differences was updated to add a source to everything a new item source value that shows the values on both sides of the comparison and the comment that were made before must it earlier basically we showed well "
  },
  {
    "startTime": "01:31:42",
    "text": "there\u0027s a source of there\u0027s a target and the comparison is done in terms of a patch it would be applied to the source reach the target and then would not tell the value on both and on both sides influences the values replaced you would then only nobody the value of one of the data source you know that it would be different in the other one but not which one it is so busy that is the thing that that has been that has been added this is probably the most important change and then in addition we added some more extended example that shows the results of a difference comparison and we also updated and extended the security considerations next slide please so this is just basically is a snippet of the main difference all of the new of the new new portion in the yang data model that has the differences format so basically what you see here is that the yang patch is the young added is augmented to include a new source value which is basically the any data value that basically indicates the value of the source data item and that is it is being replaced and which is been which applies basically whenever you are deleting something from the source off on the left after comparison if you\u0027re merging it moving it replacing it or or removing it it is there it\u0027s obviously not there if there\u0027s a create because I\u0027m busy that value did not exist earlier next one next slide and this already brings me to the last time beta to the to the to the discussion items so basically a couple of items fading to confirm here we\u0027ll discuss it with the group one thing first thing concerns dispatch format which is has been all know newly proposed here so basically this augmentation we do believe actually addresses requirement to you have to show the values of both sides of the of the comparison the earlier request was also believed that we should not allow for different formats so we should basically agree and settle on one true to help interoperability so the question is basically if that is the former that we should go forward with and other formats are possible but we just need to settle on ones nobody there\u0027s the question really in the room if you will be see if it is for instance another prayer proposal for another canonical deformity so if there are other formats we would like to hear about them so we can make the choice otherwise the proposals whether what\u0027s been presented and so that\u0027s the first aspect the first item second discussion item turned the the metadata of the original of the data items so basically the idea "
  },
  {
    "startTime": "01:34:43",
    "text": "was if an operational data store is used as a comparison target then the the it would be useful to indicate what the origin of the data is right so that you for poisons if if the assumption is that the we are they came from intended you get it yet it comes from yet the original system that person might offer explanations for instance why data is yeah why did a is different so this might be useful in troubleshooting but one question was basically whether it should be always included or if we should happen now whether an option added whether or not to actually include it or to to omit it so currently we do not have such enough basically this foot well the mo knobs you add the more complexity you add the opinion here is that is probably just to include it best so just like to include it by default but again that is something that we should confirm here and alter well Andy who I think is online actually raise another issue or we had another item that we wanted to discuss with regards to the original meta and this concern basically the action format I\u0027ll probably let him speak for itself but it is related to that same item and then the third item is an item that well there has been open for a while it is also listed in the draft currently the comparison filter is defined using sub tree in XPath as per net conf and the question is basically whether there would be a requirement to also allow for definition of filters relating to target resources per s cons and then the final item is an item that was just brought up recently by Tim on the list and concerning adding potentially adding a performance consideration section the performance reservation are way implied in the security section this concerns that basically somebody well they\u0027re there Daisy and there is potentially a hits and that is done on the system that is doing the comparison and the request is to add a section which just makes this more explicit which we can add but we which is which has cutting up in not going to add it to this current revision yet and that\u0027s that concludes what I have so if we can go through these items and get opinions in the room yes very low yes I can hear you properly I was trying to interrupts you guys you\u0027re presenting so we could ask the room case-by-case but we\u0027ll have to run the items as well so do them one at a time but we may have to ask you to repeat and go through them louder by the way you were your voice was very low "
  },
  {
    "startTime": "01:37:43",
    "text": "earlier this way I could be a good sorry we there may be a question of the mic is this regarding the first item Rob Wilson says no browsers have one other question that\u0027s not related to these at all so maybe I could raise that one never go ahead so Alex looking the draft I\u0027m still not entirely sure the diff is doing quite what I would look for so you have you all option you can be turned on or off and if the all option is off you compare nodes that exist in both datastore so intended and operational is that right yes and only only exists in both do you compare the value and if the all options on it says among you you would do a difficult for contents of both datastore so I think that would mean an operational you get all the data backs or all the operational state back or would you still apply a filter so it only only conflict true items would come back oh no no no the the ideas of course that first of all that you would only breaking report different this back right so yes yes but you consider compared running or intended to operational all the config false data in operational is guaranteed to be different from running because it doesn\u0027t exist in running so all your statistics for example all that all that beta will never ever be in runnings is never going to be there would you automatically exclude that we would well okay no we would not automatically exclude that so it is basically the question especially what you would request as what you would request as a user and this is essentially yeah that is essentially up to the user up to the client to specify and what what what it wants to have compared so I think that all options gonna if you compared any configuration data source operation is going to give you back a lot more data than you\u0027re interested in because it will give you back all operational data as well as the applied configuration okay I would be interested I\u0027ve been teresting an option that just effectively compares the conflict true nodes in operational get What\u0027s in running or intended but has all of those you know they\u0027re in exist in one or the other so I want to see configuration items that are in running but haven\u0027t yet been applied so maybe the line cards missing and conversely I\u0027d also like to see configuration that I\u0027ve removed from running but it still persists on the system due to some issue or some error that some reason my bgp peer configuration hasn\u0027t been deleted I\u0027d like to see that so I like an option that filters the the config true part of operational against what\u0027s in intended or running if possible right now okay so right now baby it would be in the way this is defined right now is that okay so maybe it\u0027s also video you\u0027re saying we need to have something between between these two two option "
  },
  {
    "startTime": "01:40:43",
    "text": "right because right now we have either you include everything or you include or you exclude data from the comparison that does not pertain to both but you\u0027re saying this you would want to have it restricted a little bit further so if your optional yeah between the two I like on a question how useful the all option will be very comparing operational to a configuration datastore I think that because the amount of data will come back by and large that\u0027s probably not what\u0027s being looked for yeah I mean the obvious you also have a filter spec that you\u0027ll then you specify as well so so when you say that that all differences are returned that would be only busy if your filter spec is empty or if your or if you\u0027re asking to to compare the entire tree which in general might however not be the case but I mean of course if you do this but it\u0027s true everything everything will come back but typically you would have a filter spec as well but if you if you consider for example say you\u0027re checking the configuration for one interface then you might have three or four lines of interface configuration in an operational you\u0027d have that plus hundreds of counters another operational data and that would automatically always be returned because I\u0027ll never ever be in intended or running so I\u0027ll always be reported as a difference if you specified your all option but make you know what they\u0027re better through it\u0027s better by the all option mm-hmm by our question is when is that useful when when does any do they don\u0027t actually really need that case is a useful case where that\u0027s useful dates to return okay I cannot give you an answer right now I mean it\u0027s clearly something that we could control I mean this I guess that is the so I think you\u0027re you\u0027re asking should we remove this all option or group and just include another option fitting instead yes that\u0027s my question okay so I don\u0027t have a strong opinion on the on the all right now I do think definitely having the option to exclude it like I get this here that that one certainly is important if you want to have everything yeah I\u0027m okay I don\u0027t have a use case actually right now at the top of my head for this so this is maybe a question to ask Rob chic a Google the only thing I can take the point I can add is we didn\u0027t implement the all option when we implemented this like we didn\u0027t implement this draft core we\u0027ve had a service doing this since 2015 I think and we don\u0027t have a no option okay okay so that drained the my queue and we\u0027ll go back to going over these points and asking the room for information or oops so first was the patch format or the for instance and I think the question was so already I think we\u0027ve had an agreement that there should only be one format that should be returned for death the question is what should that format be and the current proposal is an "
  },
  {
    "startTime": "01:43:44",
    "text": "augmentation to the patch format and the question is that have sufficient or if there should be another format in particular something that may be called gained if a dip format so instead of augmenting patch maybe we should actually have a format that\u0027s very specifically customised for returning dips is that did I capture that correct Alex yeah this is this is correct but barring my render the Yankee there is no separately and deform it that has that has been proposed yet we do think it can be done using the augmentation of the patch format but we could of course define it but yeah that is correct so yeah Vogel since this case I wouldn\u0027t define a diff format as part of this work if that\u0027s to be done I\u0027ll do it as a separate RC I so I would say do patch format now but hopefully designed such a way that it could be extended to cover a different format in future if that\u0027s feasible okay so they actually I think what I was saying before is at last time this meeting we concluded that there should only be one format that we shouldn\u0027t you know says so what you\u0027re saying is let\u0027s reopen and possibly support multiple formats it I don\u0027t think I think we\u0027re going to choose one format choose the one we already have so rather define a new one so if the jury concluded we only won one format then use yang patch that seems fine okay well objections does anyone else have a comment about this so as myself as a contributor I do worry just a little bit about if we are augmenting yang patch that the patch format it just seems a little bit awkward to me and I worry that I do think this has been the important feature and one that will be will be living with for a long time and I think that it would be Hoover ourselves to actually ensure that we pick a format that this ideal for for for this purpose and in particular imagine us needing to extend the format in the future if it or maybe yang hatch changes in the future and they\u0027re coupled in such a nasty way that we don\u0027t get to do what you know we don\u0027t get separated the music so I also don\u0027t mind Moulton I also in mind if you want to define a DIF format again I still wouldn\u0027t define it in this document something that\u0027s better to define generically and reference it from this if the proposal is to use a yang different set of yang patch that also is fine being good but I\u0027m it\u0027s going to slow down this work though if you do that I think by whatever time it takes to define a yang diplomat okay so I guess that\u0027s the question "
  },
  {
    "startTime": "01:46:48",
    "text": "well I guess I have three questions the other question is well to also revert on the earlier question and if you want to allow for yang patch augment it now and then maybe something different later I can ask a question alright say let\u0027s start with that because if we are able to support multiple bit formats or return different two formats then we can almost let go the other remaining questions first that\u0027s the first question for the person right so we\u0027re gonna ask two questions first is to prove supports and then the second would be who does not support um so for those who do support the idea of just returning a single format that there\u0027s no option for for for the client to specify the format please raise their hand I think the question you are asking is who supports restricting the return to a single format so who would like only a single format question is who wants to allow right right but I\u0027ve just think some people know okay so the format single format please raise your hand there\u0027s very few thank you support a multiple formats and also a few but statistically more a lot more so then okay so then okay the baton effectively reverse the decision that we had from last time and if we allow for multiple formats and my objection for moving forward with this format now is I\u0027ll obviated I no longer worry about it because I know we can fix it later so then I think we don\u0027t need to ask any more questions we could move forward with this format so Robert insisted to clarify I support multiple formats but I restricted small set of them probably limited to two sure okay well it would be they would presumably go through the working group and the work group would only adopt the work if it was reasonable self-limiting well one one question this well one question is how we would do that practically be earlier busy we had a flag that was allowed to specify the format and base is the preference for format and if this is about the but if you\u0027re saying we need to allow for future formats and I\u0027m not sure how we can oh how we could one say it\u0027s only this or one other formats which has not been defined yet okay great so I\u0027m moving on to the second bullet point then should parameter be included to in control whether or not to include origin metadata when operational is the comparison target so I know we all have "
  },
  {
    "startTime": "01:49:48",
    "text": "to remember everything that Alex said before but hopefully that\u0027s clear again yes No so should a parameter be included if you believe that a parameter should be included to control whether or not the origin metadata should be returned please way to raise your hand there\u0027s very few and then I think the option is that there be no parameter specified then that the Alice help me here if no parameter specified is automated returned or not returned yeah well then we\u0027ll just be returned by default we return by default okay so actually maybe if in case that wasn\u0027t clear actually let me restart that okay because I don\u0027t I\u0027m not sure if everyone was clear about that so if you think that origin met it that there\u0027s no parameter and origin but it it should be returned by default please raise your hand okay there\u0027s no one okay sorry go ahead Mike so global - Cisco sonal devices will necessarily support origin metadata so the question really is whether it\u0027s better to have that as an input parameter such that you\u0027d fail the request you can\u0027t support it or whether you just don\u0027t return it if you don\u0027t have it so so that\u0027s why I\u0027m have I prefer having parameter because then at least you know as a client whether I\u0027m not going to get this data okay I guess there\u0027s a good reason to do okay that\u0027s a good point so this will be in favor of having a parameter true to control it in which case one would expect it and then we could have an explicit but if it\u0027s not supported then obviously the request will be denied Alex you\u0027re agreeing with problem the progress yeah I think I am agreeing with Rob yes and as a contributor I agree okay so third bullet point sunfilter to define using sub tree as an XPath as per neck knob is the requirement for the definition of filters related to target resources for rest counts which okay Alice can do it with this bullet point again well then is something that we it has been trap for for y so basically the comparison filter that we use to baby yepp the filter spec well it\u0027s it\u0027s a question many but include as part of the filter spec where we say baby which part of the data store to include and that one yeah and this one basically is defined in a s pro Netcom orbiting in a dead fish weight using subtree index path and and the issue was brought up in the past betting you whether well what about what about allowing a restaurant way of of "
  },
  {
    "startTime": "01:52:48",
    "text": "defining that of putting the filters where you putting a different format for the filter spec if you look at it so if you look at the RPC but while this parameter this business pack is defined as a yeah essentially is a subtree filter or expert filter these are the things you think that you can have okay so to clear you\u0027re saying current through current definitions that either a sub tree or an xpath filter to be passed yes and the question is whether or not we should extend to support also a third which would be mimicking breast cough like semantics right that was the that was that was the issue in there yes mmm-hmm okay now when I\u0027m thinking about restaurant filters they\u0027re you know the query parameters so there\u0027s fields for instance would be one of them is that we\u0027re thinking yeah yeah this this particular item I\u0027m not exactly sure I mean this was brought up in the past obviously even when we define it we did not think we thought actually that what we have the filter specs that that happened that the survived and to be sufficient for what we need to accomplish so from that perspective yeah this is a yeah it\u0027s a contributor if you will think I I don\u0027t see the need for that but it was brought up by the group before and we have listed it as an item in the draft so this is why we need to raise a fashion so if they are if if people think we need a different format or in a national format yeah we should discuss it so the current it\u0027s an RPC not an action correct there this correct of the RPC so in our way rest comp works for most part is you specify the URL to the node that you wish to operate on the resourcing governing on which would in in Netcom parlance would be more like an action as opposed to a RPC mm-hmm to the extent that this is an RPC and then in rest comp terms that be in slash operations at a global level and and hence the rest comp query parameters would not make sense in that context you would have to have something like what you\u0027re suggesting of sub tree or XPath filters don\u0027t believe the option exists however if we wanted to support actions instead of our PC then we could have that conversation mm-hmm does anyone else have an opinion about this this actually I don\u0027t think this is something pull the room we\u0027re gonna pull the rooms just discussing this a discussion point anyone wants to commit oh all right so Alex I think we should probably take this one to the list it\u0027s pretty complicated but some examples would help all right yeah or maybe uh or "
  },
  {
    "startTime": "01:55:49",
    "text": "if or I guess believe if nobody\u0027s coming forward with the reason why subtree next time would not be sufficient then we probably just can tell this issue sure and actually we may actually raise wide support both should we support both subtree and XPath okay that\u0027s a I guess that is a six second question yeah yeah because in in that combat lis subtree is mandatory to implement we\u0027re experts not of course if you\u0027re if you\u0027re a restaurant surfer and you\u0027re not implementing that comp you might be you know that might be unfortunate you may not want it as some people dream okay lastly and the question is do we add performance considerations section or added out of performance considerations okay though this is on Tim\u0027s thing so Tim did you want to walk us through exactly what unless you raise some lists you just approaching the mic yeah Tim carry nokia so when we read the draft there were some concerns and in terms of an implementation because we have some very constrained servers right that if I was given a request to do a diff on some data stores where I don\u0027t have the compute resources to return the the information being requested how what is the question came back says what do we do what is the appropriate response that we should give back should we curtail what we have and just provide what we have should we give them a you know thumb our nose to them or not or whatever it\u0027s supposed to be we\u0027d like to have that response certainly documented of of what happens if we can\u0027t fulfill the request right and if so that\u0027s and and then alex suggests said well we\u0027ve already created some of this in the security section but in reality we probably do need some type of you know performance piece to say hey look you know if you\u0027re if you can\u0027t fulfill the request you should do this type of things for the for the these types of behaviors that you can\u0027t fulfill well yeah there was an underlying question on this is as well but one thing was I mean obviously if you cannot fulfill the request you can always just decline because you can just deny it but I guess the underlying other question relies to do you want to have some kind of prodding operation or so you can have only so many requests per time unit or or what have you or is that something that you would we weren\u0027t worried so much about a median or a throttling aspect of it if someone else might be we were just saying look you know we we might not necessarily be able to you know meet the meet meet the request coming in what we wanted was was that we wanted this the "
  },
  {
    "startTime": "01:58:49",
    "text": "RFC to specify the behavior specifically so that people that are implementing this will know you know what to do sure yeah I think and I think what you\u0027re asking for is mostly tutorial item as well so I think yeah I think let\u0027s just add it I think if we can get clearer if it terrifies the things why why not do it so on and I would suggest we just added for the politics probation so according to my clock were out it we\u0027re almost have time but lose says we\u0027re out of time she don\u0027t fully understand why it is that you wouldn\u0027t be able to fill the request so let\u0027s take a second on the mailing list yeah and actually blue is incorrect there is idem beverages and snacks in the break time and we\u0027ll see you in the room next door 20 minutes okay yeah okay right all right your lips but I right thanks Alex everyone everyone whose remote will need to rejoin "
  }
]