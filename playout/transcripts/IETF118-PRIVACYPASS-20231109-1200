[
  {
    "startTime": "00:00:06",
    "text": "Benner, I I see you online. Alright. Alright. Gonna get started. We don't have a very full agenda, so I don't think we'll take the full time, but let's get going. This is privacy pass. And, I'm sure you all have seen the note well about, your requirements for participating in the IETF process. So I'm not gonna spend too much time on this. But we want to do that, and we wanna treat each other with, respect and, Have good technical discussion. And Again, please make sure if you're in the room, you sign in with Either the full full tool and disable your audio and all that things. Or It's better to use the on-site tool. Thank you. You probably already know this information, so we skip it. Okay. On our agenda, we have a note taker Then we have some core draft status, presentation on rate limit tokens, key consistency, and then some additional discussion on metadata and the like. Does anybody have anything else that they would like to add or subtract maybe on the agenda. Alright. Ben, would you like, to give core, status update or Sure. Next slide, please. So, Apologies if I've copy pasted any of this wrong, but I believe that we've"
  },
  {
    "startTime": "00:02:01",
    "text": "made some really good progress on the 3 core drafts that we sort of started off with in this working group although they've evolved substantially since since that point. So the the architecture draft Is ISG approved? It did 80 follow-up. I have to admit that I'm not sure what 80 follow-up means here. But, but that is is through the ISG. So, That's great news. We you may recall we recently ran a repeat working group last call for the off scheme draft. That has concluded, and the that draft again has passed working group last call and and has passed on up to our responsible AD for further processing. Privacy path protocol is actually with the RFC editor. But it's misraff. It can't move forward because it has dependencies on the other two Tommy? We don't hear you, Tommy. Oh, Okay. I'll was trying to find the mic. Tommy Polyapple. So for the offscheme don't maybe I missed it. I hadn't seen an email to the list on that. Do we need to I I don't know if our AD is in the room, but do we need to formally ask him to push buttons to get it resubmitted or What's gonna happen there? So there was an I believe there was an automatic notification to the AD when we changed the draft status. Certainly, the chairs can follow-up and and just confirm that there's no further action on any of these needed from the the working group party or where the author is right now."
  },
  {
    "startTime": "00:04:01",
    "text": "Great. Yeah. May maybe if you could just write a email to the list and have a bead I say, like, AD please progress or something specture He gets that. Thank you. Okay. Alright. There is aside from the, rate limit tokens I was we also have a draft on batch tokens. I think, there was no update for for this meeting and the authors have asked that, think we should have a working group last call. So chairs will be following up on that as well. So now I think we are probably good to go Tommy rate limit tokens. Let me give you, Do you have, your device? I can a glass of Yeah. I'll Let me get you there. Yep. Okay. Oh, Oh, I'm sorry. Shouldn't have crossed that. Yes. Sure. Here we go. Alright. Hello, everyone. Tommy Poly Apple, on behalf of the various co authors on the rate limited toe issuance protocol document. So as a reminder, this is one of the kind of follow on other issuance protocol documents that we have adopted. Beyond the original two issuance protocols that were defined in the core specs that are now shipping off, And then this one is based on the publicly verifiable RSA blend."
  },
  {
    "startTime": "00:06:00",
    "text": "Signature scheme, but allows a per origin rate limiting. So we haven't had a lot of fundamental changes. I just wanted to quickly recap what we have changed in the latest update the primary thing which we had talked about last time was that we needed to align with the work on consistency since that has been evolving as we go along, So we have now created a reference to the consistent see mirror draft. And this is, the output of the design team that we'll talk about later on a consistency topic. I I think the authors for the rate limit draft are, you know, interested the in, you know, what happens to that draft? And is that something that we want to adopt here? I think that would you know, make sense to have a reference to something that we actually have formally part of the working group here. And and there are interesting aspects to the consistency for rate limiting specifically. So in the case of a generic privacy pass token. Where we have essentially one issuer wide public key, that's across everything, we just need to make sure that that key is consistent so that any we targeted it. But the rate limiting scheme involves 2 different keys going around. One is a key that is used for encrypting the origin info, which is essentially the name of the origin. And this thing is still issuer wide and is visible very publicly already, it is, visible to the a tester who is actually doing the counting buckets for the client. But we also have the actual token signing keys now since we don't have metadata,"
  },
  {
    "startTime": "00:08:02",
    "text": "associate with these tokens, and I think there's a discussion later about metadata. The way that someone verify a token understands that this indeed followed the rate limit for a particular origin, is the fact that particular origins or sets of origins going through a rate limited issuer, have different keys. And so, a client that is checking consistency wants to check both, consistency for that issuer wide key as well as per origin keys. So the we tried to come up with something so we have put this in this current version. Essentially, the fix was to allow a consistency mirror so I'm essentially doing double check or, you know, some variant of in check where I'm checking once, twice, or three times, or whatever, with some other resource to validate this key. We that need that resource of the defined configuration for an issuer could include the list of keys that it is using for the various purge and things. Previously, this was undefined. He was left as an exercise to the implementation between the issuer and different origins that we're going to try to redeem the key. And so, really, we've just added as a key a definition for this. So this allows mirrors to check it, The downside is if you were an issuer who has many different origins, you are now putting enumeration in your configuration of all the different origins. This may be large. And it also is publicly revealing kind of the list of things that you were supporting rate limiting on. This may be a very good thing for transparency, but it also may be something that deployments don't want to take on necessarily. So this is something I would, love to get feedback on. I think all the authors are likely a feedback out of the if they think it's viable or not."
  },
  {
    "startTime": "00:10:04",
    "text": "I could imagine alternate approaches such as having the mirror check something against the origin. So, like, you you could imagine instead of asking the issuer Hey. What are all of the different per origin keys, there could be instead some defined way to ask an origin that is redeeming tokens to say, hey. For this issuer, what key are you expecting? And then you could to do that in some mirror checkway, and maybe that would be preferred mechanism. So, happy to hear thoughts if people have those either now or at the end of the slides, the one other thing had is we'd also have an open issue that I think was filed by Chris. Talking about the rate limiting context. So as I mentioned, the current rate limiting limiting context is an origin, And technically within the token challenge structure, this is and and origin info. So it's it's a it's a list of things. And currently, it's a list of 1 or more host names. That works pretty well because that's the we've imagined rate limiting, But, Chris brought up to the point that for different applications, particularly maybe outside of some of the website use cases. You may want a URL path in there as for an origin. So I may have like, a a PPM DAP system that wants to have relimining tokens, and it's all on one origin name, but it has different services under on different URL paths. So there's really no reason that that string couldn't include more in it. I think we just need to define, a new text around how open that is and how flexible that is. So, again, a piece of, content where we would love some input and thoughts. So that's all I have. If people have thoughts or questions on that, please speak up."
  },
  {
    "startTime": "00:12:00",
    "text": "Otherwise, find us in the GitHub. Steven. Steven Valdez, Google. So I haven't actually looked at 18 too closely. I'm curious if you could also go in the opposite direction where you could have multiple origins and host names covered by, like, one rate limiting context. Separate from the, like, issuer, join Or if there's, like, privacy issues there that you So Right. That that I think that's that's another dimension in which you can move. Sorry. Well, that worked out. There's another dimension in which you could expand the notion of the rate limiting context. And I think it's reasonable. 1 of the here, which I think is Good. So the the thing that you're rate limiting upon is communicated in the encrypted channel all the way to the So, like, to the thing that doesn't see your IP address or your identity, but no, just the token upon which your rate limiting. The client is in full control over what it's revealing. So I think we would just need, you know, careful considerations. Like, I don't think you would want automatically say, like, Yeah. Here's, like, here's this big combination of many host names revealing my personal browsing history and using that as the thing I wanna really put it but if there are reasonable sets such as recapture for this website. Maybe that's a perfectly appropriate thing to have as a tuple That is the object of the reliveting context. Chris Wood. Yeah. This is mean, primarily just a deployment convenience thing. Rather than having to spin up separate origins, for these different, like, resources, maybe in the DAP case or whatever other application specific case. Just you could just at the application layer in the origin info and the token challenge. Specified. So it's probably pretty easy to spell how to do this."
  },
  {
    "startTime": "00:14:02",
    "text": "Saying maybe maybe the origin info elements are not just origin like, like, like, like, than what we currently call just origins, but maybe they're could be URLs. So it could be whatever other else makes sense for the application. I'm happy to send a PR to do this Cool. Thank you. That'd be great to have a PR. I'd also like to point out that this is it's very validating of the decision we made in off scheme to call this like, origin info rather than, like, origin name. Like, it it gives us the flexibility. To have many things. Glow Jonathan. Hi. To Go back to what Steven was saying. Would a reasonable set be every domain on cloudflare. They're all hosted by the same server. You might wanna rate them at them together. But but but That's gonna be millions of domains, an interesting question. So Technically, There is no Reason that the origin info must be the same thing that is serving the challenge. Like, we can open that up to be a bit more flexible. What it must correspond to is the thing that the origin sending the challenge and validating, like, it it must match up to that. Right? So it's like, like, that I expect this particular key. And so you could, like, you could very easily have a key that is cloudflare origin wide. And the origin info is, like, we need to define, but, like, somehow, explained broadly in the challenge. So they were like, okay. I'm just gonna use this one. I think the main question is, like, defining the client behavior of knowing if it's K to do that because it's very easy to say I get a challenge from example dot com and I say, okay, I allow you to apply the rate limitingexample.com. How does"
  },
  {
    "startTime": "00:16:00",
    "text": "the client know that this really is a cloud 5 origin because generally, like, unless we're inspecting, IP addresses and c name chains. We don't have a reason maybe, or maybe, like, certificates or something else. Like, I think we need just some rule to create a binding there. Could because the the reason why I was thinking it'd be impossible is because what would you do with multi CDM Right. So it's in 3 different rate limiting domains depending on chance or what DNS return Like, it's just gonna be messy. I mean, I I could certainly see that if if Yeah. Like, Don't rely on DNS here. But if, for example, I'm relying on a field in the certificate, and it happens to have like, a conflict certificate at this point then I know that this particular times came from an instant that was this, and therefore, I'm applying the cloudflare wide rate limit. And I just have a different one elsewhere But I I just need some way to know that on the client and validate a secure relationship. Thank you. I was just checking the asking doc. It does it is rather opinionated about what the the element should be. Particular says, like, the name must be, like, authority. What does it say? Name of the origin that issued the authentication challenge is included in the list of origin. So if we if we wanted to support that particular use case, and then allow us to define you know, something later that establishes that finding example.comandcloudflare, whatever. We we might need to, like, rework that tax just a little bit. Let me try this out because I I I think we don't actually need to modify that tech. Go for it. Because the rate limit document, describes that the client needs to choose amongst the things in the list of origin info, which is the one that it will use for the rate limiting context. And so even if the"
  },
  {
    "startTime": "00:18:01",
    "text": "auth schemes origin info list needs to contain the currently challenging origin, it can also contain a broader thing that we would just need to newly define the rules for which the client could accept that. So that would work as well. It's ample.com, comma, but whatever. And you would say, like, cluff.com. Exactly. Like, essentially, like, I say, like, oh, start up cloudflare and, like, because that is in my certificate, prefer the broader one I choose to use that. It's fine. Jonathan? Or Yeah. Every, I think every cloud Flash certificate has the same c name or has the same CN, like cloudflap, No. It's not it's the it's the one that isn't the SAN. Has the same, like, common name. An SN could be a list. Oh, okay. It has the same common name as, like, because it I mean, that's that's not speak, but it could be Correct. So for the proposal of having multiple things in the origin, in for that origin sends. We run into the problem with this, like, has multiple overlapping rate limiting contexts that now, like, a malicious client could ask for either And either the issuer needs to know that you should only use the like, more limited version or, I guess we do kind of wanna be able to drop the lick. Real origin. Avoid having that as an additional context Right. So, I mean, I think this is a problem that needs to be solved in this document. And maybe there's some rule about how you know which one it is. I think what you were alluding to is correct that the issuer because it does see this origin info could reject ones that it doesn't expect to be used. Like, it says, I want it to only do the CDN wide things. And so I will not let you do it. On the, individual origin host names. But that means the client needs to know"
  },
  {
    "startTime": "00:20:02",
    "text": "which ones are valid and not for the purposes of the issuance protocol. Yeah, it doesn't also feel like kind of a thing you could avoid by. Just don't not configuring things, a bad way. Like, don't configure rate limits in a way such that you would have these conflicts. You have to always send the origin name Like, you may like, for example, you could say, the the first thing in the list is the thing that you want to rate limit on. That's not understanding the problem then. So I think the correct me if I'm wrong, Steven, but the problem is if The list includes example.com and Cloudflare CDN. And the issuer accepts both of those as valid for the rate limiting context. Then the client has, like, free rein to choose which of these it's gonna come back with unessentially under the same key, I can I now can have rate limited stage for one of those 2 origin Infos? I don't I don't think that's possible to do because in particular, the challenge has to have the issuer name respond to a specific rate limit policy. So everything in that list has to have the same policy applied to it. That's just the way we've constructed it. Right. But since the thing that the attester does the rate limiting upon is based on, like, the synonymized version of the origin ID, having 2 valid origin infos that the issuer accepts gives me 2 buck that I can be on on the attester. So I can essentially double my rate limit. Oh, Okay. That's a different problem. Thought the problem was you have example.com has a rate limit x No. No. No. It's not that they're 2 great limits. Is that I can double bucket. I I can I can just amplify my ability to get tokens? Okay. Well, I mean, I guess"
  },
  {
    "startTime": "00:22:02",
    "text": "don't do that. I don't know if, like yeah. Yeah. I I think a lot of this comes down to, like, finding a reasonable heuristic. This is, like, to make sure that the client and the issuer do exactly one thing and the issuer can make sure you don't do the thing they don't want you to Yeah. Okay. Like, it it's all solvable. Yes. We just need to write it down. Hey, Nick Sullivan. I did not read this issue, so I apologize. But, how explicit are you planning on being, in the document for defining what these these particular buckets are. So, like, have a host name. That's a well defined thing. Are you are you going to define a set of schemes in front of it, like host name colon, host name versus URL colon something versus SNI matching colon some bucket. How explicit do you wanna be here and how are the client and server supposed to agree? Exactly. So I I think that's the main open issue in question here. Chris, offered to propose text. So I'm excited to see what that will be. But I'm not sure. Yet, yet, Okay. Thanks. So feedback. Absolutely welcome. Okay. I think I think that drains the queue, and I think we're good on time then. So Yeah. I think, any other, Oh, Chris. Uh-oh. Sorry. No. Can we go back to the other issue? Yeah. No one commented on this one. Yeah, I don't really like this, in particular, like, the enumeration of the origins It'll be fantastic if we could avoid it. You like this? I was going to suggest, like, perhaps changing the underlying licensing your scheme to something that is partially one solve not only this problem, but then also the origin token binding problem that we have that we worked around by having per origin keys in the first place. That would, vastly simplify consistency issues. And you really only have 2 keys, like, key and the one true token key. Yep. But I recognize it's a pretty big shift and"
  },
  {
    "startTime": "00:24:00",
    "text": "that document is even adopted by CFRG or anything yet. So I I'm curious. To hear what people think about that. I know there are limitations of this. And and I think that does get us back into the metadata. Question. Although, potentially in this context, it is actually what metadata to expect. Well, I mean, the only thing you'd be shoving into the metadata would be the origin name or yeah. Like, the exact same thing that you encrypted Exactly. Yeah. And and Just remind me because I I don't know the details of the cryptography here for the partially blind case, that that information in this project line would not be visible to Correct. attester as a transits. Yeah. It's only after unblinding that it becomes The visible, but you would still encrypt it and send it to the issuer. The issuer like like like like like like like the signature of the the pops out the other end would basically be bound to that that, that, that, that, that, that value to the origin name in this case. Yeah. But like like, like, still be encrypted in transit across the attester. Okay. I'm I'm I'm I'm fine with that direction. If if I but I think that's something that we need to bring up working group consensus on. As the direction. For sure. It's unfortunate that, like, it it There's a lot of issues that are conflated here by the the underlying choice in in crypto, but, yeah, willing to be flexible there. I think we could set my ring's That's great. That's great. From an influencer's perspective, I guess I'm still like the the partially blind stuff on the server side is Should be manageable, like, in particular, because you'll know set of metadata values that are passed in, and it's not like you don't have the same risk as you would otherwise. It's not like the arbitrary value is being passed in. So I think it would be fine from that perspective. Don't know."
  },
  {
    "startTime": "00:26:02",
    "text": "Opinions now or if you can get opinions later on like the fine side, Yeah. I need to talk to our client side crypto team. Okay. So can I ask a clarifying question here? What, sir? In in in the CDN case, which is what I'm thinking about. In your alternative approach. The origin is cloudflare, I mean, we so, like, whichever CDN is hosting. Right. Oh, it's not always cloudflare. John. It's it's not It's not always tough. So I mean, just to play out the example entirely, right, like, Yeah. I have example.com uses awesome CAPTCHA as it's rate limiting issue or whatever. And awesome capture has one key for all of its origin name encryption, and one key that's dedicated to, example, dotcom. And You would like, define some way for your mirror to ask example.com I don't know. I'm some, like, well known year. Like, some some you or I template or something to say. Heyexample.com. What key are you challenging people for when you're getting them token type 3 for awesome CAPTCHA. Okay. So so that was that was what my what I was thinking. Okay. Yeah. In which case, isn't this tussles all the way down? In that the mirror now needs to be rate limited. Oh, sorry. The dot well known endpoint now needs to be rate limited. Because otherwise because if it's sorry. If it's not rate limited, then I can just pound that endpoint, endpoint,"
  },
  {
    "startTime": "00:28:01",
    "text": "Sure. If it is reboot, I mean, that should just be, like, answering the same key all the time. This is like a static file, essentially. But it right. So if it if it is rate limited, then how do I get the token and if it's not rate limited, How do I recommend it? Cause I can't do the token thing. I don't I I I think Right? You would have to say, like, that fetching that resource would not be something that you would apply this type of very because, like, again, like, this is, like, the strict, like, I want to enforce on a per client level that you can do 5 of these per day. Right. Generally not, like, I think we can just say, like, you must not do that for this, just, like, generic key fetch resource Also, the mirror can do caching if it needs it. Like, It's not appropriate to do turtles all the way I I was I was considering the caching, but, like, Let's say somebody has a big list of Cloudfla domains. And they hit the dot well known for each of them. mirrors going to pound cloud Yeah. You can try the Sure. So you you could use that to dos Pablo. Right. And then the mirror itself needs to rate limit the click. But, like, the mirror doesn't know. Necessarily. We're talking to, Well, I I I so, again, and now this gets into the consistency to discussion about the time of the mirror. Because, you know, the point of a lot of this is you know, allow me to have better IP privacy, such aren't really meeting IP address anymore. On grid, my websites, Now, I I think our assumption is that for things like talking to the attack or whatever. You are willing to reveal more about yourself order to be great limited or identified, Potentially, some of that comes into the mirror as well, such that, like, when I asking the mirror to essentially proxy this on my behalf, I may need to be more honest with it about who I am. Just for fetching these keys."
  },
  {
    "startTime": "00:30:02",
    "text": "Okay. Now this does bring up then the problem of if you do this all alternative approach. Now you're revealing to the mirror what you're trying to access, and now it's a locus of information. So Yes. I started stepping this problem by going partially blind. Would be a great solution here too. So so But you you we're thinking on the same page. Okay. Thank after Jonathan? I think Ben Who's Hi. Just, just responding to Jonathan's point specifically, you know, my understanding here is that We can't rate limit rejection, Right? Like, a a hostile client can always spam the server with with bad add tokens, and the server has to process them and reject them. So So we we don't actually have a rate limit on failure And, serving the keys. This, like, these static keys is dramatically less expensive than processing a token to figure out whether it's valid or not. So so basically, there's a threshold of of cost below which rate limits rate limited tokens just don't apply. And this doesn't make that any worse. Very good point. Okay. I think we've beaten all of the horses we can beat for now. I think this is great input. And we'll try to follow-up on issues and lists. Great. Thank you. Alright, Steven. Yes. Where did you show up? Did you get Let me do it this way, and, I'll do this because"
  },
  {
    "startTime": "00:32:00",
    "text": "seriously, I don't see you And now I'll give this to you. You should have control now. Yes. Hi. I'm Steven from Google, presenting on the K-tech protocol a bunch of folks from the key design team, Matthew, Benjamin, Chris Scott, and Tommy. So the first thing is that this document is actually different from the one linked from the slides. We didn't have consistency between those. That's the old key consistency document from, like, a year ago while this is particularly the consistency mirror work. So we presented about this at 18. So we'll mostly be going over the changes since then and possible next steps. So the first big change is that we decided to rename this protocol because bike sheds are fun. So we're moved from paycheck protocol to this checking resource consistency with htpmers as this reflects the more current state of the document. The bigger changes where we simplified consistency checking from being generalized paycheck, lots of possible mayors who just being about one invitation and letting the application decide whether you use multiple mirrors and what the appropriate failure behavior there is. There was a lot of clarifications made around client behavior and what you do when you have a inconsistent response from the mirror. This really breaks down in two main cases. One is where you actually get, like, a different value from the mirror. Is the more critical the client needs to deal with that as a, like, critical error versus the you can't reach the mirror or the mirror has latency or time out concerns, in which case, depending on the particular application, may either want to fall back to something that's cached but not expired or wait and try refetching again later. And then we added more behavior around how the validity of different response from the mayor R"
  },
  {
    "startTime": "00:34:02",
    "text": "used and when you're supposed to recheck behavior. When common concern that's come up is this like thundering herd problem of everything expires at the same time and all clients hit the mirror at the same time, you may take down the mirror even if it's running across multiple on servers, servers we still have a bunch of open work items and issues. That we need to resolve before we can cut the script. The first one's a batch consistency check. There are some use cases where you may wanna be requesting multiple resources at one time. For some cases, you can rely on HVH3 pooling. So you don't need any of these particular behavior. Other cases you might not. Think the current plan is to leave individual applications to generate their own profile, their own definition of what the resource is. So it's still effectively one resource. I think this is per application unless there are enough use cases that come up that really need this, like, sort of batch handling a bunch of resources at one time. Possibly if you'll, like, have 100 or 1000 of keys you wanna get at one time or things like that. And there's Other big set of issues is there's a bunch of wording that we need to refine like timing attack, the thunder and herd problem, what the different application policies and how you decide where to fall back or what to do when things are in weird states and how to handle multiple errors because even though this document will, by itself, like, discuss the multiple error issue, we still need some amount of text so that applications can make the right decisions here. Yep. So Not anything super complicated, hopefully easy to handle. Think next steps here are either doing a call for adoption for the document. The other thing that came up 117 is maybe having an interim, but given that the document is all that complex. I don't know if we need that much time or if we can handle this on the mailing list. And then just fixing and arranging issues. And then making sure all the docs, both this one and the other privacy pass docs are synchronized between them. Any questions?"
  },
  {
    "startTime": "00:36:00",
    "text": "I have a question, for the group. I'll do a show of fanship just how many people have, read the current document. I'll put that up in just a second. Alright. We got k. We have a a few, would be great to get a few more, eyes on this but I think, I don't see a reason to to block on an adoption call. So I think we should probably do that, but I'll confer with been Any other comments? Tommy Poly. More just a question of what we expect the output of the working group to be around see, like, we already there there is the adopted consistency, like, general document and there's this I'm assuming we're expecting those to say separate or, like, do do we want to publish both of those separately? Do we want to only end up publishing something like the mirror or what what I guess, I mean, more for, like, the milestones and, like, what what is the intent here, long term is the output. I mean, I I don't necessarily see a problem with publishing to these two things, or we could if if we thought that there that existing document is"
  },
  {
    "startTime": "00:38:02",
    "text": "kind of superfluous for the most part and can be merged together into one document. I think that that could also be a a valid approach. Something we probably should discuss. I I don't have preference. I don't think there's a a natural order here or something required from the charter. I think we can choose. Ben, did you have a Yeah. I'm curious to hear what Ben thinks too. Hi. So, just speaking as individual, I think both of these draft are valuable you know, we have basically an informational draft that lays out some common patterns seen in different kinds of systems. For ensuring consistency across a across a pool of clients, and then we have a concrete protocol that basically implements one of those patterns. I don't I wouldn't necessarily wanna combine them. I I think, you know, having an one informational and one standard crackers. Logical enough. Specifically on this draft, I wanna say know, thank you to the authors for all the improvements and work that's gone into it. I I think it's I think I really liked the mirror protocol. I wanted the mirror protocol to exist. I think it is interesting to sort of think about where it should go. And, you know, maybe maybe privacy passes the right place, but it actually seemed like very general utility that could be valuable for even for purposes unrelated to systems it really strikes me as essentially, an upgrade, a modernization It was the the, you know, twenty five year old idea of an HTTP proxy. So I definitely would wanna get a lot of review on this draft from outside of privacy pass for more general HTTP experts, And also, as I've I've sort of"
  },
  {
    "startTime": "00:40:03",
    "text": "at this point, pester the the author is to an extreme degree. I don't think it actually substantially resolves the thundering herd problem. In its current state and that may be fine. It may be we may just decide that that's essentially out of scope here. But we do have some some more decisions to make there. Okay. I mean, we we can easily, ask for review from other groups. And and that sounds like it'll be the right thing to do. I don't think that would prevent us from adopting it as a work item in this group, though, So that that would be the intent unless somebody has other ideas. Earth. Yeah. I just wanna voice the opinion that I don't think thundering herd needs to be solved before adoption. We just need to be we don't have we have to not think that the current solution is completely Un uncompatible with solve and thundering her. If we think there's no way the solution can ever solve thundering her that they that's varied adoption unless it's at that level of and compatibility, then I think I don't think not solve it that I heard yet. Blacks adoption we can adopt and get stuttering herself before we publish. I think the plan is still to continue to work on refining that at least from the work and open work items. I don't know if it can solve it a 100% and some things might be out of scope, but think they're so work to be done. I don't think Chris, one of the questions was, to where this might be submitted. Find Russia correctly. Other candidates being like, you know, HTTP biz or Was that? I I don't personally know exactly where it would be submitted because I I think that the the other group that was concerned about key"
  },
  {
    "startTime": "00:42:01",
    "text": "assistency was, Oh, oh, hi. Yeah. And so, like, that that that could be a place because they had interest I feel like if we tried to bring it to another group, it's like voicing this upon them, which isn't really a work item of theirs. And maybe maybe that makes sense to me. It doesn't. But but but but but but but but but but but but but but but but but but I don't know what what do you think? No. I think this is the right group because it's Yeah. Specifically in the charter to address this sort of but also Tommy with your HDB share your hat on. Would you agree with that assessment. And I'd wanna talk to Mark a bit too, but I I I think the holistic notion of, like, this is the modern generation of reverse proxies is pretty big for the mirror protocol. And maybe something like it would come about, but I I certainly want a lot of consultation and review with febus, on the mirror protocol. I know there's already been some from the design team talking to some experts there. But I I think trying to make it it's goal and problem solution space too much larger, we'll just make it everything slower and hinder it. We can have an eye towards collapsing mirror protocol into something more standard if if reverse proxies do indeed get reinvented and more consistently standardized. Corey. Alright. Steel. Sorry. I don't have a ton of context, but the key consistency question is this, like, detecting? Do you plicity around, like,"
  },
  {
    "startTime": "00:44:00",
    "text": "a single identity serving or a single resource server serving different identity keys to different parties upon request. Get So, just a quick comment that consistency proofs and inclusion proofs for key transparency is obviously I particular new working group that, you know, you might be interested in having a discussion around that, and I'm the new, Key trans co chair. We're having our first session on Friday. Happy to check more offline. I don't have enough context to be helpful here. Yeah. My math model is that this is like poor man's transparency, and we would actually use something like each transparency if we wanted something of a stronger practice. In fact, the informational draft that the work group already adopted, like, described. Some transparency like system, for solving the problem if you if you can do so. So KT would be a a tremendously a good fit for that. So Okay. Alright. So now I believe we've gone through our regularly scheduled Agenda. Alright. And what we would like to do is talk a bit more. In in the previous meetings, we've had some discussion that I I feel we didn't Ben and Ben Phils also that we didn't completely finish our discussion on, They inclusion of metadata and what what is acceptable and, And, how do we"
  },
  {
    "startTime": "00:46:02",
    "text": "reason about that within, privacy pass. Ben, do you wanna talk about some of this section or Sure. So, yeah, so we spent a lot of time at 1 17, discuss saying in detail some proposals to add public metadata in general and some specific kinds of public metadata. To to privacy pass tokens. And There were there was a lot of, interest in this. Definitely a lot of people who who felt that this was useful and a good proposal, but also some significant concerns. And we didn't have a Any volunteers to present that topic at this session, but the chairs felt that it was important to see if maybe some more conversations had happened that changed anybody's perspective there. And, take the temperature of the group on what what folks would like to do in this area and with these drafts. So yeah. I've for example, if, if you were somebody who if you've had a conversation or done some thinking, about about some of the privacy considerations about public metadata, especially since 117 I'd love to hear your perspective and whether you think that basically we've solved that enough to be ready to move forward. Know, my position is mostly as it was, back at 117 and and the discussions on the list since then. There are operational, there is operational value that comes from using this."
  },
  {
    "startTime": "00:48:02",
    "text": "And we believe we can, like, reasonably handle all the privacy potential implications as described in the architecture document, but then as discussed at 117, in these specific extension drafts themselves. There are so there there is value here, but also and the core documents we in the registry, have a slot that says, like, whether or not this token team supports, like, public metadata or not. And if we're not if the group is not gonna like spend any time, like working on things that provide public metadata, We should, like, get rid of that that slot. Like, I Like, so Alright. Like, it seems like we've we said we're gonna do it, but now we're like, a bit hesitant about doing it. Like, but not I I think I wish the chairs would they're just, they're just, they're just, just make a decision as to whether or not we're gonna do this by, like, perhaps issuing adoption call for this thing because, like like like like seem kind of stuck. Nick. Apologies. No video here. I had concerns at the 117 meeting, and I trying to do a privacy review of the proposed initial individual drafts on this topic, which I but I I think, raised those privacy concerns and didn't standardize the mitigations. So that that that that That that was certainly a a concern that I had. I think we could address those mitigations in in a standardized way so that we could have an interoperable privacy preserving, system, even one that potentially has some, public metadata. But it wasn't initially Okay. Clear to me, and I haven't seen discussion on the list that confirmed that we could do"
  },
  {
    "startTime": "00:50:02",
    "text": "Alright. Tommy Polyapple. I'd be curious to hear from Nick again, after what I say. So I mean, in general, the mandate a concerns are They they do make me worried in cases, but I I think It's important to be able to have them and find a way forward. So I'd like to see do it. And I'm wondering if at least for the first cases so we can at least make progress so we can we can you know, scope the use of metadata to cases that we could clearly consider safe if we think we can define those. So I'll just throw out a couple things as possibilities. You know, one based on what we were saying earlier for the rate limited type, if we switched to a model that was partially blind you could have an instance where the metadata, is something that is already coming in the challenge from the origin. Right? So, like, essentially, it's it's information that the origin already has and you were just getting a token that covers that As the metadata, and really is just a a a way to not have an explosion of a number of keys and to also simplify the consistency problem. And then similarly, you could have a case where there's a piece of information that the client is already to the origin. And it is then challenged to get a token that proves that thing. And then you just send the token that covers that. So it's essentially guaranteeing that for uses of metadata. In those cases, Nothing news being revealed. It is only having the token Yeah. Yeah. Essentially sign that metadata. So I'd be curious to hear from Nick or others who have concerns if such a limit to the scope would help."
  },
  {
    "startTime": "00:52:05",
    "text": "Yep. Go ahead, Nick. Yeah. I don't, I don't need to be the only one talking on this I hope. But, k. I think cases where, Yep. Yeah, where where the data is included in some other part of the request. That does seem like a promising direction. I I think the question is just, are we are we going to standardize that in in some way, or just just suggest that implementers try to address that separately. or did you wanna go? Ben No. I went behind you. Okay. Respond to Nick. I mean, the the suggestion that Tom we have is, like, Yeah. You you'd standardize it by just only choosing to work on extensions, that have that particular and seems like a pretty reasonable thing to do. For example, the Geo extension that would not have that property so maybe that goes elsewhere. That's totally fine. But there are there are very reasonable extensions, like expiration. Like like like that are already basically known to the issuer. And, for. Like, they don't increase the privacy surface. They simply help with the deployability. Hi. Just just just just just as an individual. I have a question. Is it? Possible for at least some of these schemes to enumerate the possible metadata values in the configuration because if if I can see as a client, you know, this configuration contains"
  },
  {
    "startTime": "00:54:03",
    "text": "7 different possible metadata values, or for, like, country geocode, the contains 180 possible. You know, measure values. And I'm just going to pick one of them as appropriate. Then that's covered by the consistency procedure, whatever it is for the configuration. And so that that you know, lets me calculate the entropy loss. I wonder if that's something like that as possible. Hey, Nick Sullivan. Back to Chris's comment. Is the decision making process for which of these extensions get included in, the working group, could be covered by, you know, change in the charter that, would help to find which things are in scope and not in scope. Help determine whether the geo extension which is not necessarily known to the server would be in scope or not. Is that something worth considering? Yeah. Could be. Then, yeah, I think your consistency suggestion is good. It was discussed at 117. It's like, you potentially might want to enforce consistency amongst the configuration, which would include all possible mention value. So I think, you know, should be choose to take on this work. We would, of course, consider consistency. Or or like, factor and consistency into the overall privacy storage to ensure that by, like, clients can reason about, like, what they're, like, they're see posture is and what, increase fingerprinting server, so whatever they much Okay. To to me, what it sounds like is that we we we we probably have a way forward here at least for some of this work but being able to constrain"
  },
  {
    "startTime": "00:56:06",
    "text": "the metadata values and perhaps even more so if if they can be enumerated. Benjamin, Yeah. I've been doing Volus Moseda. So I wonder, like, should there be a document that sort of actually describes the guidelines for you know, Like, this is acceptable. This is not acceptable. In case we can some of those last actually go into the category. This is acceptable. We just work on it. And if it's not Yeah. I think perhaps a document or perhaps a Nick suggestion if it if it doesn't have to be a a a lot of text, then perhaps it can be know, some criteria in the charter. But something that the the group has has consensus on would be the the desire here. Right? So that you know, so, yeah, I it's documented in some way. So, as I'll attempt to speak as chair here. And so the the charter describe says that specifically instructs us to, work on including it issuer supplied metadata and including small amounts of metadata with tokens as well as associated impacts on privacy. And I think that, you know, what we're trying to square here is is figure out if there's a path here is that, that basically squares with the use of the word small and the emphasis on preserving privacy here. And we wanna make sure that that we're you know, moving forward with metadata support as as the charter instructs us and also staying within those bounds. Tommy Poly Apple. So just to voice on, like, the charter question"
  },
  {
    "startTime": "00:58:02",
    "text": "and not to disagree with you too much, Nick, but It feels like from the discussion here, We have you know, sufficient, you know, concerns being brought up and, you know, reasonableness that through the just the process of coming to consensus around what we want to adopt and what we will progress and making sure things go through appropriate sector reviews, etcetera. I I think we'll do a good job, and we don't necessarily need a charter change and a process change to make that we have a good set of things. The charter has this space. It allows us to work on it. It says, you know, we need to consider the privacy goals. I I I think We can do the right thing, and adding more limitations to the charter, not only is process overhead, and then those people in ISG and IAB have to, like, go approve that stuff. We would likely, encounter a situation where we'd want to, change those specifics again in the future if we come up with other approaches that will work. So I I think it's more just we want a careful consensus call here. On what we should do. So then it kinda kinda like we don't really know that is that this is going to be a problem of people coming and bringing all sorts of things and trying to force them through the group. And so Let's not try to solve that problem if it's not a problem. Right. And I think it's problem that you know, the the chairs can do a good job of solving by making sure that we have sufficient, time and review as we consider adoption on any new extension here Yeah. Also in the the first item, in the extensions asking you the extension's document, it describes, you know, like, yet a new registry for all the we might potentially work on well as like a policy for which things get added to that registry. So I think we could like the If if we choose to work on this, be very clear in the description of that policy, that is"
  },
  {
    "startTime": "01:00:01",
    "text": "What does it take to get something added to this registry? Example, if it's like a privacy nightmare, maybe it doesn't go into the registry, but, like, if If it's not, maybe it does. 2nd. Okay. But, you know, I I think, you know, through through like, the the the stuff that we can get consensus from the working group in the in the score and that's we can reasonably bound the work here. Without without issue. Like, I'm I'm not worried about that. Like, the the the challenge would be, like, getting consensus on what the reasonable extensions to work on. Like, GEO 1 will be, like, particularly controversial. Which is like, be expected. But there are other ones that I I think are not so problematic. Challenged you. The Jira 1 will be controversial. I just thought everyone would agree. I said it's I'm I'm surprised that it's controversial. The the tier 1, surely everyone agrees that's a privacy nightmare, and we shouldn't I'm Alright. So I think I think we have have a way for it. We'll we'll need to determine the exact mechanism here, but I I think we have a couple places we could document, and it sounds like autscheme extensions is probably a good way to go. But, yeah. Just if I can offer a suggestion, I think, a reasonable way forward would be to start an adoption call for the contents of the first two drafts. Which give us, like, basically the mechanism to add extensions later on. And then screen that goes through, and we like, decide that we can have the mechanism to do extensions"
  },
  {
    "startTime": "01:02:01",
    "text": "then we, as a group, 1 by 1, go through and consider each of the different individual extension documents. And that's where the popcorn and fun will come in. How about we do a show of hands for that idea? And let's see what will we call it? So let's see. To be clear, like, the first two documents are, like, absolutely useless. Without actual extensions. So, like, like, the they're adop adopting them is like predicated on the assumption that we will eventually do some extension work. we run an adoption call for the Should 1st you could use the OSCAM extension 1 maybe if that's what you're talking about. Have to think more about it. But, yeah, maybe. Yeah. Like, yeah, because maybe there's other use cases, but, like, all this extension stuff is Salinas, we never adopted an extension. That's true as well So I'm gonna say, should we adopt the first two documents as a base to establish a basis for Metadata extensions. Then. That sounds like an adoption call. How about should we run an adoption call. Are we ready to run an adoption call on the first two documents? Okay. I had that and then I thought it was That's fine. Are we So,"
  },
  {
    "startTime": "01:04:07",
    "text": "on the first What? While we're having this, I wanna ask some of the authors. You know, we've we've had At least 2 different types of mitigations mentioned, one of them would be a a rule to only include information that's already visible to the issue or another would be to only include metadata that's enumerated at in the config. I wonder where would you see 1 or 1 or both of those kinds of rules appearing in this in this spectrum of documents. I think that would be in the specific right now, I'd have maybe I think about it a little bit more. I think they would go in the specific extension document. But perhaps there's something general to be said. The extensions in the extensions the the first one much like we have sort of To be frank, like, sort of hand waving guidance and the architecture document says, like, you know, don't do bad things with metadata. We could probably be a bit more precise. As the, as we get more concrete and closer to specific extensions. Not sure how helpful the architecture document is. On that point. Chris Patton, does any is anybody named a use case for metadata for which neither of these is true. I not either that the metadata somehow private or, like, not visible already, or what was the other one? Or not innumerable. I think noninnumerable is probably we probably have used cases for that. It's my hunch. But the first one isn't always visible. Okay. Okay. Okay."
  },
  {
    "startTime": "01:06:01",
    "text": "Alright. I'm starting pull on are we ready to run an option call on the first two documents as you can ignore that as I couldn't see it when I was typing it Alright. Looks like we're, Give people another second to figure out how to click the things on the phones. Alright. So it looks like we're at, 19 think we're ready to think not. If anybody who's said, don't think it's ready, ready, you wanna get to the mic and let us know, that would be great. If not, then hopefully that would come out, if we do an adoption call. Nick, I I just, it was just what I said in the chat. I don't I don't feel strongly about but but but did we want to, you know, try to address this question to see if we can get one of the extensions. To meet all of our requirements before we adopt the other 2, but but I don't care strongly, but that's why I said no. I couldn't quite hear it, but I think you said that you were wondering if if"
  },
  {
    "startTime": "01:08:03",
    "text": "if any of the adoptions, if any of the extensions would meet the criteria that we would put in those documents. Is that what you had a little trouble hearing your audio. Maybe somebody else I think that's what he said. And, my answer would be Yeah. extension. It's it's very simple. At@at It is it is like basically an integer and Yeah. But Yeah. I mean, unless he disagrees, like, I I me, that seems like the obvious candidate here Expiration? You know, what, like, what was what was the suggestion? She he wanted to, like, you know, in the time remaining because we were sort of had a schedule, see if there was any actual extensions that we knew of that What? Okay. Sorry, Nick. Okay. Option. Alright. Ben, do you have any other, things, or can we close this item on the agenda? Yeah. That's, that's all for me. Alright. Anybody have anything else they would like to discuss related to privacy pass."
  },
  {
    "startTime": "01:10:04",
    "text": "There's a little is there some We're good. We're good. Okay. Then, We can close the privacy pass meeting for this ETF. Thank you everyone. Dapa like,"
  },
  {
    "startTime": "01:12:16",
    "text": "down."
  }
]
