[
  {
    "startTime": "00:00:19",
    "text": "Craft. Craft. Hey, Al. We'll get started in a few seconds. Please trickle in. Could we get a notetaker? I'm staring into people's eyes Oh, amazing. Thank you, Mallory. Could someone shut the doors? Thank you. Thanks. Alright. Welcome all to the final day of ATF 117. are the privacy enhancements and assessments research group. I'm Shivan, and we have Sara. joining remotely and Chris as well maybe. Right? This is the note well. I'm sure you've seen it by now, but please do read it carefully. mom, And just to remind you that this session is being recorded, and you are expected to follow IETf IETF"
  },
  {
    "startTime": "00:02:01",
    "text": "processes and policies. Just some meeting tips. Please make sure that you are signed into the session using Meet Echo. and and you are using Meet echo to join the mic. line, No. I'm just sure that we are fair with folks attending remotely. And if you haven't used the mid echo light agent, thing. Please scan the blue sheets. I think they're over there. if you wouldn't mind just sending across the blue sheets, QI code? Yep. Thank you. And thanks, Mallory, for taking minutes. Cool. Yeah. This is our agenda. and we have some time at the beginning for Jeff statuses, and we have our first RFCs. we have 2 new ROCs that pair PROG published. And Fernando, I believe, we'll be talking about them later on? today. And we have one draft in RFC editor queue, the survey of worldwide censorship techniques. that should also to be Thankfully, progressing soon after many, many years. And then we have the guidelines for performing safe measurement on the Internet draft. Matterie, did you wanna give a quick update on that one? Mallory, notal, CDC. Yes. So that few weeks ago, I sent a revision for this draft on safe Internet measurement And I I consider it the 1st real full version. Pre previously, we had the table of contents. We had quite a lot of text there, but there were still some unfinished sections that I've brought few times, to this group because we"
  },
  {
    "startTime": "00:04:00",
    "text": "did a sort of table of contents you know, revision and and I'm quite satisfied with the structure of the document, but now I'm also satisfied that it's bowl and it's filled in. It actually provides guidance on all the things it intends to. So if you could please review it, I think it would be getting quite close to last call just because it's not meant to be a terribly long document. and and it does have text on all of the things that we've intended to cover and care about. I'm gonna be proactive in reaching out to the folks who have provenanced this draft the tour safety board. bit bit It's it's based on their guidelines for when folks go to tour and ask to measure the Tor network They review a checklist and this is essentially the checklist but adapted for the IETF. environment, and so I'll be reaching out to the folks that sit on that board for their reviews. And if others have ideas for other groups aside from paired g that might wanna review I'm thinking of MapRG, Anyway, I'm I'm willing to do the work to send it around to get some reviews But if you all could also review it, I would appreciate that. then we can move forward. and Yeah. Thanks. Maybe a PPM as well. would be a good venue. Really obvious. Yeah. Good idea. And if folks think of something later, just send me an email or a Zulip message, and I can action that for next week, Cool. Thank you. Alright. Yeah. We can move on to our first presentation by Reza. Whereas, if you wouldn't mind sharing your screen, again, and I can grant you access"
  },
  {
    "startTime": "00:06:05",
    "text": "clicking the grand screen. It's not doing anything. that. Oh. Oops. Thanks. that Alright. Hello, everyone. I'm or or At So -- excited about Reza, your audio is very choppy. Yep. 10, Mhmm. It's perfectly financial. course. Sure. Yeah. Can you Maybe I'll just can you try again? I'll just rework with you in audio. just try requesting access again. Hello? like it's better now. Yeah. It looks"
  },
  {
    "startTime": "00:08:04",
    "text": "Yeah. Reza, is it I think can you request access to just share preloaded slides, and we can try to do that instead of sending the screen. Oh, I see. your slide deck should be on there. And I don't find it there. Just maybe try reloading meeting materials. I don't know that's a per user view. should be the privacy and language models. Yeah. Rhea, maybe I'll just share the slides for you, and you can say next slide, and we can go that way. But this screen sharing was working. Right? It was just really choppy. Like, we couldn't hear you at all. I see. Alright. Yes. So let's continue this way. Oh, Oh, Oh, okay. Can can you try this, Rosa? I think you should have control now. Okay. Sure. Yes. It seems that I go to the next slide? Yes. Okay. Very good. So, hello, everyone. I'm very excited about talking about this very interesting and important topic, the data privacy in language models. And before we dive into the details of the relation between language models and privacy, we need to understand what privacy means"
  },
  {
    "startTime": "00:10:02",
    "text": "in general, in the context of machine learning. And I want to separate 2 different notions. One is the direct privacy risks of using machine learning. similar to any other type of computation. when sensitive data is used in a system, we want to make sure that only authorized users have access to that, And we want to prevent In other words, the direct privacy risks, And this is where confidentiality comes into the picture. something that I want to focus on today and is the more subtle risk is the indirect privacy risks in machine learning, where sensitive data is used for training a model And now the model itself or the outputs of the model might leak information about the data. Right? So if the data is sensitive, and the model is leaking information the data, then in somehow unintentionally, we are allowing unauthorized access to the data. Right? So this is this is this is the concern, concern, that we have with respect to machine learning machine learning algorithms in general. So for language models, I don't want to go into the complex details of how they operate. But in a not shelf, what they want to do is to mimic The way that we used to talk based on the data that the algorithms are available to the algorithms up to a certain point in time, And what they do is that they try to in a sequential manner, complete sentences, based on the samples that they had seen, in their training So the data sometimes is nonsensitive, but in very many cases, the data on which language models are being trained"
  },
  {
    "startTime": "00:12:01",
    "text": "today, and definitely will be trained on in the future are very sensitive. There could be medical data, which are very sensitive and personal. They could be enterprise data, which are very sensitive. And, also, it could be all the conversations between users in different platforms. which contains very personal information. So the concern is that the language models that are so powerful to learn the patterns of how to place different pieces of text together based on their training data. They will also memorize some pieces of that text that could be extracted later on, and reveal sensitive information about the users who contributed data to the training set. So indirectly, unintentionally, leak information about the training data. And and this is real language models, do leak information about their training data and it has been shown repeatedly by different researchers including my team that large language models, the the vulnerability of large language models could be deployed by by inference attacks. An adversary who was just access to the model would be able to identify which data points were used to train the model. So, for example, for the sensitive datasets such as medical notes, are used to fine tune language models and which create, let's say, clinical language models, have shown that it's very easy to to extract that sensitive information. Also, for some other language models, Researchers have shown that you can extract"
  },
  {
    "startTime": "00:14:02",
    "text": "some sensitive information like phone numbers and Social Security numbers if they are used in the training set. this has nothing to do with the with the language as as we all agree. but this is the side effect of using these these models. So if the the The data is personal and sensitive. in the context of large models, models or personal data. If you are concerned about sharing the data because of the sensitive information, which is embedded in them. we also need to be concerned about sharing the models. But it's very hard to protect this decays and later on, we'll talk about that. But first, before we talk about how to mitigate the risks of training large language models on personal and sensitive data. we need to have a way to audits data privacy. in these in these cases. We need to be able to measure how much the leakage is with respect to the data that was used during the training. We need a metric. And the way that explain how to measure leakage in in machine learning, would like to use again a hypothetical scenario Imagine that there is one world in which we are training a model on some training set And there is a parallel world in which new dataset a new data record, for example, a new document or all the text produced by particular user. is added to that training set, and and we train a model. So there are these 2 parallel words. In one word,"
  },
  {
    "startTime": "00:16:00",
    "text": "My data is used in the training set and in the other world, It is not used, and everything else is the same. Now in this hypothetical setting, I place an adversary randomly in one of these words, and I ask if you would be able to detect in which world he is in. And So in one half probability is in this word or the other word. And the question is whether the adversary can distinguish between the to The only thing which is different between the two is the presence or absence of my data, the adversary observes the model or predictions from the model or samples from the model and needs to to guess that So the we all admit that if the adversary wins this game, if this hypothetical adversary wins the game and manages to find the information, and signals in the output of the models. that that enables him to differentiate between these two settings The algorithm is information about my data. the the algorithm enables the adversary to sync out me, based on the data that I contributed to the training set. Other words, there is an 40 supposed in that is running. it's or we used to simulate the 3 in tracks. So predictions, for the While using the ringtone or not, of the"
  },
  {
    "startTime": "00:18:00",
    "text": "We're all the aim and information leakage. 12. It's Reza, your audio is getting choppy again. Can you try requesting audio again? for now. Right. Let's keep the video off. just to -- Okay. Yeah. That sounds better, I think. Can you try again? Yes. Okay. Am I audible? Yes. Yeah. So the the success rate of the adversary In this membership inference attacks would measure the leakage of models about their training data. Right? So I hope the setting is is clear to just visualize this You can imagine that different users have provided their data, a subset of these or or selected for training. So this is the universe. And in this data universe, we have selected a subset of them, to train a model. and and then what the adversary does is that for any data point, in the universe, runs a transformation function to map them into a space. that gives the probability that, let's say, these data a data point could have been member of the training set of a model. So the ones that are on the left, are the ones that the adversary thinks with higher chance were part of the training set and the ones on the right or just random data points in the population. the adversary can place partitioning, or thresholds to determine which data points"
  },
  {
    "startTime": "00:20:04",
    "text": "or member or non member, this is the identification attack. Right? So any individual Given the model, would either fall into the left side of this Align or the right side, And the adversary would say, well, the ones that are on the left or the members. These are the ones that adversely identifies as members of the the training set. He's gonna make Mhmm. Sorry. We had a question, I think. Would you like to the question right now, or would you like to wait wait until the end? Yeah. I would continue the presentation. and then leave some time at the end for the for the questions. Okay. Or in the middle of the talk, basically, there's there's a part that they can pause a little bit. Okay. So then this is basically the attack. Right? This is basically the identification attack. If you are unlucky and the the adversary would correctly detect US member, then information about you is leaked through the model parameters or predictions And can't performance of the the attack is to see To what extent the adversary managed to recover all the training data, and do he was mistakes on falsely believing that some external data were part of the training set. Right? So this correctness or coverage plays a very important role in the in the metric. Also, the error plays a very important role in the metric. is the way that we can We can show this that let's say, imagine that In this spectrum,"
  },
  {
    "startTime": "00:22:00",
    "text": "where every individual gets a score, the adversary could be very conservative and say, well, I want to identify One single data point that, for sure, was part of the training set. then the threshold would be all the way to the left. all the advice I might want to say, well, I want to identify everyone that was in the training set Even though I'm going to falsely believe some other nonmembers are are part of the training data. Right? So this would result in a profile which is the dotted line, for example, shown here in the red red area. And for let's say, the language models. But but depending on where the profile is where the error versus the accuracy of the attack falls. we can is is I'm is is They're is is with is the 1. Sorry, Riza. Would you mind requesting audio access again? That seems to work. Maybe it was getting choppy again. We can't hear you. Are you speaking? Risa, can you try sending audio again? Let's see. Yeah."
  },
  {
    "startTime": "00:24:03",
    "text": "Yeah. Hello? Hello? Yes. We can hear you again. Okay. Yeah. Sorry about this. So I was saying that based on the performance of this analysis, we can we can partition different algorithms into low risk, medium risk, and high risk. that reflects what extent is the risk of singling out individuals, versus how much each individual is blended into the population. this is a a very now a very standard way of majoring the privacy risk in machine learning algorithms, has been reflected in many guidelines, and for for AI. and mitigating this risk of membership in friends, attacks, which reflect exactly how much the model leaks about this training data is part of the requirements that are of meet meet meet try the series of algorithms, including language models. So an example of using that tool would be that, for example, if you have a generative model, a language generative model, which is trained on some speeches. I mean, the text is not sensitive, but This is for the sake of analyzing the tool and and measuring the privacy risk We can see that as the amount of information that we have about an author increases, the chance that we can identify whether"
  },
  {
    "startTime": "00:26:01",
    "text": "His speech was part of the training set increases and very few sentences are enough to identify that some text was part of the training data. The tool and similar tools would also enable identifying vulnerable training data Ironically, in this case, the the vulnerable training data are data, which are talking about sensitive information. And So I would like to wrap up the the part about quantifying privacy risk is that we need to measure how much presence of certain information in the training data, would be leaked through the parameters or predictions of the model And that needs to be part of any privacy auditing and come adding Reza, we can't hear you again. I'm just gonna do the same thing, revoke audio, and just requests audio again? You have a audio access now. Can you try speaking? Yeah."
  },
  {
    "startTime": "00:28:00",
    "text": "Alright. Alright. Yeah. Yeah. We can hear you. Thanks. So the If there's any question about this first part about auditing I think there was a question. This is a good time to ask. Jonathan, did you so you wanna ask a question? Jonathan Hodeon, Kepler. This question actually came up in this unit. is the presupposition that the attacker already knows all the data in the model and wants to know there or not. because I I don't really care. if the attacker can do that if it already knows all the data. I I don't want it to have the data in the first place. k. It looks like Rizzo is offline. Oh. Yeah. Yeah. Yeah. Yeah. Yeah. Alright. So you were you were talking about the assumption, whether the adversary knows all the data and the training set. of Yeah. The slide 21, whatever it was. Uh-huh. Sorry. I didn't hear the I just heard the first part of your I I didn't hear the question. Yeah. if if we assume that the attacker already has the data. then I don't care what he can do. He's already won before the game started. Uh-huh. So, no, the the adversary does not know the the training data, He only knows That's that's what that's that's the the the problem that he has to solve. Right? He does not know whether your data is in the training set or not. Right. Right. He doesn't know whether it's in the training session or not. But if he already knows the data, then I don't care. Yes. So this is a Yeah. So no. No. No. No. No. No. This is a the the private's information, the secret is whether it's there or not."
  },
  {
    "startTime": "00:30:00",
    "text": "This is not a real adversary. This is a hypothetical adversary to just measure the leakage of the algorithm. It's not that there is a real adversity out there that has the data and wants to know whether you use it or not. that's a separate scenario Like, for example, you want to know whether some company use your data during training or not. That's a separate question. Here is it's a hypothetical scenario that these two words we just simulate to just measure presence of any point in the training set will leak all the way through to the predictions of the model. And we just tested this way. but it feels like your like, I don't understand why that's measuring the right thing. Right? You could instead measure how many numbers how many sixes are in the dataset. Right? that's a measure, but it doesn't measure the right thing. Why is knowing whether the data was trained on, the right thing for measuring privacy leakage. So imagine this. Imagine that there is training And an algorithm is running on that training set and and train model. Right? which will become available publicly. Now you want to know whether you should participates, and add your data to the training set or not. So your concern is whether by adding my data to the training set, do I make enough changes, distinguishable changes? to the model, such at the adversary that can reverse engineer it and point to my data. If the model with your data without your data are indistinguishable from each other, such a no adversary can differentiate between them. Then it means your data was not leaked. Right? So it's it's safe. You can contribute your data. Okay. So this is whether I decide to join some dataset or not. rather than"
  },
  {
    "startTime": "00:32:02",
    "text": "given that all these large language models exist, is my data everywhere. Yes. It's just to to to me, that's the same question. Right? So you can use it as a as a decision making for whether to participate or not, You can also use it as a way to just say, if we added how much it would leak. Right? because if it leaks, then I'm not going to participate. Because the the idea is this, If I'm running an algorithm to train a model The expectation is that it should it should extract the general patterns out of the dataset. Right? It should not extract something that only applies to my data. by social security number, that particular sequence of numbers It's not something that the model should Remember. or should produce But any random number It's okay. Right? You should be able to learn fake Social Security numbers. that's what I want the language model to learn. but not to reproduce My number, Right? Makes sense. Thank you. So the if you want to Now say, okay. This is this was a way to quantify the risk But one thing that we want to do is this We say, well, we need to preserve privacy as well. Right? So this is this was a way to quantify the risk. We showed that actually language models, leak a lot of information using this technique. We need to preserve it. first, we need to know, okay, what does it mean to preserve it? But Right? In the general context, We have one more question. Chris, did you still wanna ask a question? Or Sure. Go ahead. Yes."
  },
  {
    "startTime": "00:34:02",
    "text": "is this vulnerable to guest data? So, for example, you can imagine a dataset that was trained on medical data. Right? And so if somebody wants to know if I have a particular medical condition, can they guess does the training set contain Chris Lemons treated for acne. or whatever. is it vulnerable to guesses like that? So let me rephrase your question. ask it this way. if I want to check whether it would reveal That sensitive piece of my data is this way of evaluating and auditing privacy is relevant. Right? Because I'm just looking at whether my data is there or not there. Can you detect it? And you're saying, well, Okay. Maybe he can say to some extent that my data is there or not, but what I'm concerned about is whether the disease is xory. Right? if the disease x being xory. The is very atypical, right? So it said your record is, like, so different than any other patient's record. Then detecting whether your record is there or not is mostly due to that something that distinguishes your records from the others. Right? Right? but but but but Okay? This is answer number 1. And answer 2 is that We can develop an auditing technique just answer your question, To Instead of saying whether your record is in there or or is not there, I'm saying whether there with this property or whether it's there without property. I can still run the same game. and then That way, I can measure the uncertainty of adversary in distinguishing these 2. Within the same framework. Yeah. Tommy?"
  },
  {
    "startTime": "00:36:01",
    "text": "Tommy Poly Apple. Yeah. I I I think I was thinking along the same lines as Chris here, essentially saying, you know, 2 or to Jonathan's question about, you know, how is this actually a problem if you could use this as, like, a way to, like, brute force some exist improved existence proof of, like, okay. you know, Jonathan's SSI SSN is this. is this is this. And I just figure out which one has the higher confidence on it. Mhmm. Then I have a pretty good likelihood of extracting that data. So that seems like, legit, privacy concern. Exactly. So you can use this membership in French attacks. In terms of instead of asking it in a binary form, you can ask it with multiple questions and also quantify that risk. if that is a specific risk that you want to measure something is that But is very important is that in this question that you ask you highlighted a particular sensitive piece of information which might not be known or be identifiable in all cases. So what we want is that in a generic way, without a specific find a particular attack we want to measure leakage. But this at all, what does it mean to preserve private see given that there is all this leakage. I want to broaden the discussion a little bit and say primarily in the realm of languages is is super difficult. to define. language is a medium for communication. It reflects all our life. Okay? So it's as complex As the language itself, So the Sorry. The privacy and language is as complex as the concept of privacy in real life. So if you can define what exactly privacy is, come up with specific rules for privacy in the real life. then you would be able to extend it to to language as well. that's a very hard problem. to begin with."
  },
  {
    "startTime": "00:38:00",
    "text": "Privacy is also very contextual. something could be sensitive in one context if it's shared and not sensitive in the other context. As human, What we do is that we choose whether or not to disclose or conceal information, depending on the context. You tell me something, I sense whether that is sensitive or not, And I decide whether I share it or not to the to other party other party. But this is the The question is whether I can sense the context. This is very hard. Sometimes the context is just beyond the text itself. language model, just look at the text, First of all, it's Very hard, if not impossible to detect whether something is sensitive or not. except for very specific cases of sensitive data, And in many cases, because the context is beyond the text, it becomes impossible to know whether a certain thing is sensitive or not. So for protecting privacy first, we need to identify what is sensitive. Aresa, just a time check for maybe around 5 more minutes? Sure. Yeah. And then the Private information can take many different forms. One of you just asked about, you know, a disease the type of disease and so on. But it's way beyond that. Right? are so many different ways you can reflect something which is sensitive in your daily communications, with your colleagues, family, and and others and friends, And and there is no simple way to come up with a rule with that. Private information also has no boundaries. Alice might say something And one that we reflected in analysis data, which but it could be reflected in Bob's data."
  },
  {
    "startTime": "00:40:01",
    "text": "So user 1 information, might reveal information about user 2. Also, we cannot say if something is repeated many, many times is is is public. many cases, for example, in organizations, there are lots of secrets that everybody talks about. In but it's private. language models. We know that they would just then more something which is repeated more. So what are the existing privacy preserving methods? Scribing, removing secrets from text. You define what the secret is, Remove it before training. Differential privacy, train a model, that makes the auditing technique that we discussed before fail. such that with without your data, the algorithm, results in indistinguishable models. or use public data, crawl the Internet together all the available text. But these are all based on some assumptions. Scrubbing assumes that you can come up with a rule for removing every secret. Differential privacy assumes that the secret is entirely in one's user data. So everything sensitive about me is in my And using public data assumes that publicly data is public intended. if I put something somewhere or pause something on to each air and so on and so forth. I meant it to be used forever by Everyone. But these assumptions are incorrect. or in IT range. I mentioned that, you know, it's it's very hard to up with a rule for identifying secrets, the data the the sensitive information about one user could be in many users data, And what I share publicly I share with some intention in mind"
  },
  {
    "startTime": "00:42:00",
    "text": "which is not being used by everyone. So this is a very challenging situation we are in. We are in we are using language models. and and the existing techniques are not perfect. So what are the path forward? This is a very hard problem. So there's no solution right away. But one thing that we can definitely agree on to do in a systematic way, is the following. So we should make use of publicly intended data there are lawsuits right now against, you know, some companies that crawl the Internet to train models, So instead, it should be data, which is from the 1st day is generated 4 public use. We can use private personalization. for a user who is going to use a language model, Maybe he should be the only one that has access to the model if his data is used for training that. or for the enterprise as well. So personalizing, models, for every user or very any enterprise. But at the same time, we can also use the existing privacy preserving algorithms given Even though they have some limitations, And all the thing, privacy risk of algorithm should be definitely part of the the process of making sure that we reduce the risk of using language models on sensitive dates. Thanks a lot. And if there is any time left I'll be very happy to take more questions Yeah. And thanks a lot, Reza, especially for hanging with us. through these audio technical issues. We have a few minutes, like, 2 minutes for questions in case folks have any. Nick, go ahead. Yeah. Thank you for the presentation, I'm curious about"
  },
  {
    "startTime": "00:44:02",
    "text": "one path I don't think is in your future list here about deleting data. Privacy is often also about control the data if I if my data was out there, but I don't want it to be. I should I should be able to remove it. do you have a sense of weather large language models or others of these models have considered the possibility of how delete. and individuals' data. from the model? Or is it just gonna require retraining, a very, very expensive retraining? Mhmm. So it's a very good question. would say it's related to a couple of points that I mentioned here, which is use publicly intended data So nobody owns that. This is something which is generated like books, newspapers, and so on. and use private personalization, which is that My data only is used for the model for fine tuning the model that I'm going to use myself. Right? So if I'm using, you know, smart reply or auto completions and so on and so forth. my data is if my data is used for it, I only have access to the fine tune version that fine tune version. But but your question in general is a is a very challenge that's a very missing and but but challenging problem. in the context of smaller models with some assumptions about the, let's say, the loss functions, the optimization problems for training models. We can use some stochastic techniques to partially retrain the model and remove the the effect of somebody's data. For example, when we talk about differential privacy, differential privacy itself means that my data is not fully used. Right? So it's"
  },
  {
    "startTime": "00:46:01",
    "text": "almost forgotten. A little bit of more randomness would would would would would remove the effect, But I don't know if that would work in the context of language models with with a complex of optimization problems they have. guess we should I try to reduce the user cross contamination of private information as much as possible. such that we don't get to those situations yet. Perfect. Thanks a lot, Reza, especially for join joining us so late on a Friday night. Yeah. Thanks a lot. There was a lot of discussion in the chat. Oh, I see. I didn't see the chat, but I can follow-up, I guess. Yeah. I'll also send you a link the chat, the chat, Sure. Thank you. Thank you very much. Thanks all. Alright. Next up, we have Arthur. me just share the screen, and then authorize, hand you Slide's control. You should be able to -- Great. Thanks. -- control the site now. Yeah. you hear me okay? Yes. Perfect. Great. Okay. So, hey, everybody. My name is Arthur Edelstein. And today, I'm gonna tell you about a project I've been working on for about a year and a half, call privacy tests dotorg. It's an independent project that I that I published separately. But currently, I'm also working as a research and privacy engineer at Brave on the Brave Browser. So this talk, I'm gonna give you a little overview of what what I've been working on in the past. on browser privacy, and then how that led to privacytest.org."
  },
  {
    "startTime": "00:48:00",
    "text": "and then the high level approach to how how the project works. Then I'll give you a an overview of some of the specific privacy tests and results that that that that I've obtained. And then we can have a look also at the progress that browsers have been making in privacy, which is pretty interesting recently. And then I'll talk about what I've learned and and some future work Arthur, we can hear you fine. Would you mind just speaking up a little bit more? Sure. Let me get closer to the mic here. Yeah. Yeah. So I've been working on browser privacy since about 2014. I worked at our browser and then at Firefox and then most recently at Brave. So I just been thinking a lot about different browsers, how privacy works, how privacy is leaked, on the platform And that led to me thinking more about what can we do generally about the some And the big problem which everybody here knows about is that The web is a major target of mass surveillance. there's sort of a a a shocking thing, which is that you know, the web is being used by billions of people as one of their primary means of reading, writing, communicating, engaging in commerce, etcetera. and and yet this platform is extremely leaky, and it's sharing everybody's data in an invisible way. and it's exposing essentially everybody to mass surveillance by governments and corporations, and and you've probably all seen this recent headline, headline, that that in the US. The spy agencies are buying commercially available data, on mass, as a big part of their mass surveillance capability. So it's really this this problem is both government and commercial, and it's really it's one big one big issue because of this this data sharing so so"
  },
  {
    "startTime": "00:50:02",
    "text": "there's a a few ways that browsers are are facilitating mass surveillance The first is simply that when you visit a website, generally, browsers allow trackers that are embedded in those websites, to gather your browsing history. and they do this. by fairly pedestrian means often, such as using cookies or by logging your IP address. But there are many, many ways that browsers will leak data. So those are just 2 of the most common. And that essentially has been known for 20 years. It's kind of intentional in the sense that These issues were considered in the late nineties when cookies were first introduced. And and, basically, the decision has been gonna allow this kind of data sharing and allow tracking and it has, of course, commercial benefits for especially for advertising. So that's we could say that's sort of the intentional component. There's also a more unintentional component or maybe neglectful component, which is the browser will leak a lot of data that's not necessary. that could also be used to track users. This is probably not something that's designed, but just that's the way that that browsers are working And so these are 2 sort of categories of data that are available to websites when you visit them, and then allow you to be tracked There's also an issue with network connections. This is I I'd say higher on the priority list for a lot of browsers to try to encrypt network connections. but that's a big area where your ISP or anybody else eavesdropping on the network can Watch. Watch. you're browsing. So why why are browsers still leaky after these years. My feeling about that is There's a couple of reasons. The the first is opacity, which is to say that privacy leaks from web browsers are hidden and they're highly technical and complex"
  },
  {
    "startTime": "00:52:03",
    "text": "which means that they're basically invisible to the public. And, actually, in my experience, they're even invisible to people who are working at browser companies. because it's too complicated for anybody to keep the whole story of browser privacy in their head. It's there's a lot of different ways that things leak And then the second thing, which I mentioned before, is that privacy hasn't really been a priority for web browsers. they they they they generally been focused on other things, And, unfortunately, at the same time, most browsers will advertise themselves as protecting your privacy. There's a lot of rhetoric around that. but I think frequently, it's giving a false sense of security or false sense of privacy for people thinking they're being protected the art. And, again, that has to do with the fact that it's invisible to people that this data sharing is invisible. And then, finally, As we all know, some of the major web browsers are getting their revenue from top trackers They're not getting their revenue from users in general. So that's a very strong incentive to keep tracking going, So I was trying to look for, you know, what's a way that we could try to push back a little bit on the situation. It's these forces are very, very huge. So, you know, I'm I'm fully aware of that. there's no simple answer to that. But at least part of the issue, because we need because there's so much opacity in privacy leaks, is that we need to try to make the way that web browsers are leaking data to make that visible to the public. with the hope that that would try to apply some force in the opposite direction. so the idea of privacy tests is to spring these things to like. first by detecting privacy leaks, monitoring those leaks over time, And then Getting the results, informing the public of those results, and also informing"
  },
  {
    "startTime": "00:54:03",
    "text": "browser developers so that they're aware of what their browsers are leaking. And finally, if we can use some we can we can induce some competition between browsers over privacy when that didn't really exist before except in the realm of of marketing. So I originally proposed the project at Tor in 2018, It didn't really get off the ground, but I I started putting it together. And then in 2021, after I left Firefox, I started working on the project independently full time, and I launched in in mid October then I also introduced tests after after doing on desktop browsers. I then introduce tests for Android and iOS, and And so I I'm continuing to work on the project as kind of a side project And it's it's still work in progress, so I'm continuing to try to add new tests and new browsers. And that's a picture of what the website looks at. looks like you can you can check it out. So there are a number of challenges in trying to get something like this to to work. And so this sort of lists some of the things that I was thinking about when I tried to design, privacy tests, and, again, it was kind of an iterative design. The first thing is just that browser privacy leaks are unknowable to most people. So what I wanted to do is just do something that is objective, and that's simply revealing the facts about route and not it wasn't it's not really about trying to bring my opinion into the into the story And the second challenge is that, you know, I could test browsers once, but they're actually updating every month. and and I would like that to be something that people are aware of as things change. So what I've tried to do is publish the results every week And then I wanted the results to the test results to be actionable for users, which"
  },
  {
    "startTime": "00:56:02",
    "text": "which means that it's not it's not so much about testing what your browser is doing right now. and more about comparing different browsers side by side so that if you're a user, you can say, oh, I'd like, use a different browser because this other browser is more private, something to download this one. And then another another big problem is that privacy leaks are are too technical for most people. So if so, basically, what I decided to do was to simplify the results as much as possible by Bucketing results into either pass or fail. So that hides a lot of information, but that's very important because users need to users are not gonna be interested in the technical details, and they need to just get an idea of our is browser x making, you know, allowing a large number of leaks or not. And so Basically, yeah, by simplifying to pass fail, I then have a way that people can drill down if they're really into details. But most people are just looking at the the the simple table of pass fail scores. Another problem that I ran into is that it's hard for readers to know who to trust, which, you know, is a general problem online today that it's There's there's all kinds of fake news, false information. So isn't really any way for me to absolutely prove that what I'm saying is correct. So instead, I've just made the tests open source. and I stick only to facts so that people people can try to come to their own conclusions rather than me claiming that I have the right answer of which browser you should use or something like that. And then the other challenge that I was facing is that it's a really daunting problem because there's a large number of browsers out there and and you know, 100 of different privacy leaks And so my my solution was just to launch early without really being finished,"
  },
  {
    "startTime": "00:58:02",
    "text": "And then I've I've continued to try to add more browsers and tests as I've been able to but it's it remains a work in progress. So the basic idea is that I test browsers on real devices. I have a a Mac, that I use to test the desktop browsers, I do that because Ari is only available on Mac. And then what I what I have set up is a server that serves pages that run sort of simulations of tracking attempting to see if they can share data between two websites, for example, So I can just tell a browser, go to this page. It visits the site it records the results, sends them back to the server, and then the server can return that back to the the control program on the Mac. And it's very similar on on mobile except that I'm using the Appium, library to to control which pages that the web browsers visit So there's a simple cut data flow which is I set up a config file that determines which browsers I'm gonna test in which modes the results come back in JSON. I then I then have have another stage, which renders the the results into web pages, then it automatically publishes them And finally, I share results on social media primarily Twitter invested on to try to get public engagement so this is the And data flow I run each week. So this is what I I described before. I basically run tests on a Mac, and then on a Android phone and an iOS phone. I run the tests in regular windows and also in private windows. And I also on desktop, I run the tests on"
  },
  {
    "startTime": "01:00:00",
    "text": "some nightly builds of browsers so that we can get rapid results on the on the latest changes in browsers. So there's a variety of different privacy leaks and then testing for. Testing for. This is kind of the full list, but I'll go through some of these shortly. So the first one, guess I'm missing a slide. The first one is is state partitioning. I And that refers to basically the problem that browsers tend to leak state between different websites. So if you visit what website a.com, end it a.com has a 3rd party tracker embedded in it that stores a cookie, Then when you go to site b.com, 3rd party tracker can read back that cookie t And if that cookie is storing a user identifier, then it can track you from site a to site b. So it knows you're this this is the same person who did these to site visits. So cookies are the classic example of that, but it turns out there are many forms of state that have been able to share data between sites a very similar way. So they act as though they are cookies. And that means that any one of these individual leaks that's listed in this table is enough to uniquely track users between sites. So, ideally, you'd like to be able to partition website so they'd So this data is not shareable. This state is not shareable between the two sites. And it turns out that A number of browsers have already done that. So you can see in the in the test results here, that brave, Movad and tour browser have have completely partitioned. all this kind of these kinds of state, And Firefox, Libra Wolf, and Safari are are very close. There's just one one case, the blog Europe."
  },
  {
    "startTime": "01:02:01",
    "text": "blob URL API that that hasn't been partitioned yet. a lot of progress has been made here. So most recently, Chrome has partitioned some of the network connections and I believe that they're going to be or may have started some tests enrolling out partitioning of most of the other items here. The second one I wanted to talk about is IP address tracking, which everybody's aware of. It's it turns out to be a super important and difficult problem. it it it it's been shown that users are retaining their IP address for quite a long time in in in a large number of cases, probably the majority of users So that's a test that I do just to see do any browsers hide your IP address, server reflective address, only to our browser does that by default. it would be interesting to see if if more browsers can do that in the future. Another big area is https usage. I I do a variety of different tests on this So if you look there's been the kind of traditional idea of blocking active mixed content. That's the last row, and you can see that all browsers are doing that The most stringent test here is the one at the top, which is looking to see does a browser connect to an insecure website at all? And there are a couple of browsers which I I don't have them labeled here, but tour browser, and I believe Leberwolf both block and secure websites by default. So the user has to click through before they can connect to a site that's using HTTP only. Another area that I've been testing is fingerprinting. this is kind of a limited set of tests, but I think it's"
  },
  {
    "startTime": "01:04:02",
    "text": "a really interesting area that that will come after the state partitioning work is is finished. So I'd like to expand this, but The basic idea is fingerprinting, devices are leaking devices have many have a lot of unique data, and browsers are leaking that data about your device. and so that allows you to be tracked. But it's probably harder to track users through fingerprinting because you need a lot of different. pieces of fingerprint, and sometimes this those values will change. over time. But these are just a few examples that I I'm currently testing. that are leaking your screen size and the position of the window on your screen and fos that are installed on your device, and So some browsers have successfully been able to hide this. And so I think this is showing that it should be possible for all browsers to do this. Another area that's super important is tracking query parameters, these appear in URLs when you click from one website to another. This is just an example of an ID from Google. And if you look at the results, basically, some browsers are are doing this. They're they have a a list of query parameters that they block. and make I think this list is getting shared around because everybody's blocking basically the same list of group parameters. or they're not blocking any. So I I think this is a pretty big distinction between browsers in terms of privacy. The next area is about trackers. I I borrowed a list of the top 20 or so, trackers from who tracks dot me, I just test to see whether browsers are blocking these third party scripts, So on the right, because this shows that"
  },
  {
    "startTime": "01:06:03",
    "text": "Only a few browsers are actually doing this. This is because they have an ad blocker built in And on the left, there there are more browsers that are also blocking tracking cookies These are these are 3rd party tracking cookies. Edge is an interesting case where It has a list of trackers that it's blocking that is more limited than some of the other browsers. So you can see that some of the cookies are getting through, but but others aren't And, actually, in edge private mode, which isn't shown here, This there's the same list. So Actually, Edge seems to have the worst protection against 3rd party tracking cookies compared to Chrome, which is which is blocking offer great cookies in in incognito. windows, windows, And finally, the most recent set of tests that I introduced are testing, cross sesh cross session tracking So this is rather similar to the state partitioning tests that I showed at the beginning. but instead of testing between to different websites, it's testing to see if your browser retains state between 2 sessions of the browser. So let's say you visit a.com. then you close your browser, and then you open the browser again, does is a.com able to reidentify you by storing a cookie or storing something else. let's say, in the cash or in local storage, something like that. and it's really interesting. I didn't realize this, but browsers are doing almost perfectly in private browsing mode. So they really are deleting everything in cache is as as we would expect. So that's good to see. But I think that it would actually be really valuable to have the same behavior in in normal windows. An exception would be if I've logged into a website. Maybe I want that login state to be remembered when I I come back, in general, I think for for for short visits with websites where I don't interact with the website."
  },
  {
    "startTime": "01:08:02",
    "text": "much. Let's say, I just read a page or 2. no reason why that should that that site should be allowed to save data about me or an identifier tracks me the next time I visit Yeah. So that that's kind of an overview of the different the tests that I've I've been running Since I I launched in 2021, there have been a lot of really interesting improvements in browser privacy that that that I've been able to watch happened in in near real time, let's say, So in in December of 2021, Brave partition network state, which meant that it was then passing all of the state partitioning tests or nearly all of them. And then you may remember in in June of 2022, there was a a little scandal around duckduckgo and Bing trackers. So the mobile browser started blocking the Bing trackers at that point, and you could see see that change. Also, in 2022, tour browser introduced HPS only mode by default, which is really important for using the the Tor network. then in fall of 2022, Firefox shipped total cookie protection by default, which meant that it was now passing virtually all of the the partitioning tests And then in the spring of this year, Chrome has has rolled out the network state partitioning by default. which is part of those those that state partitioning list of tests and What was very interesting was also just to see how that propagated to all the other Chromium based browsers. So it it shows how important what's happening in Chromium is that it it it It is now the those the network state is now being partitioned in all desktop browsers and nearly all mobile browsers at this point. And Right there, just a time check for Maybe around 5 minutes, including questions."
  },
  {
    "startTime": "01:10:02",
    "text": "Okay. I'll I'll try to wrap here. And then Yeah. There've been a couple more developments what's very interesting is Safari is now announced they'll be blocking tracking group drivers in in in the next version. So just summing up what I've learned, all three browser engines have now been hardened for privacy in some web browsers and are passing most of the tests. and that's changed since I started So I think that shows there's there's really no excuse that every browser should be protecting user privacy, and we There's there's no technical reason why they can't do that. And I I've also been it's been very actually pleasant talking to different browser. engineering teams and what I what I found is that, basically, everybody where almost everybody at at least in the at the engineering level really wants to fix these things and is is very interested in in trying to improve the situation, And, also, it's been really fun talking to users on Twitter and mast it on. because they're there's actually a lot of people who are super interested and and really care about this stuff whether they're technical or not. So there's a lot more tests that I'd like to be Building, and adding more browsers. but it's gonna it's gonna take me years because it's it's a it's a big project. Yeah. So I I I'd be very happy to take any questions. if there's time. Thanks a lot, Arthur. We have a few questions already. Go ahead, Tommy. Thank you. Tommy Poly, Apple. So thank you for sharing this, and thank you for doing all the work on this. It's a great resource. Just a couple comments. 1, you know, early on, in this deck, you're talking about the reasons that browsers aren't"
  },
  {
    "startTime": "01:12:01",
    "text": "doing all the privacy things. Yep. And one I think it would be very notable to add to that list, particularly for the case of protecting from network, tracking, or also protecting IP tracking all the other things. is cost. because the cost of doing all that is very huge, and that is one of the main hurdles that collectively, the ecosystem needs to be able to figure out in order to improve this. Yeah. That's a great point. Yeah. So it's not not for lack of desire there. troop. Yep. Yep. And then just another comment for one of your earlier slides that we're talking about, you know, IP protections. You know, there's, like, a label there for being tour enabled You know, that's a very particular approach to it -- Yes. -- which, you know, I get that the Tor browser is the one that does Tor. But if we're having stuff like that, I think it'd be very interesting to include more things like what's going on with mass proxies and multi hub proxies. This is, you know, something Safari has been doing for all trackers of proxing all that traffic. and for private browsing, that's now included by default for all unencrypted traffic, and we're doing oblivious DNS for all private browsing traffic. So some of those aspects, I think, will also I I I very much hope to see them in other browsers too. So including other techniques like that the table would be useful because they are making a difference for tracking as well as well as well as well. Thanks. Yeah. I I totally agree. That's a great point. Come back. Nothing cool. Apple. Arthur? graveyard, and this really has been helpful in the lab conversations both for Safari, and I know also for Chrome and a lot of other browsers. For the cross session tests, think we need to have a conversation about what a session means. because it sounds like you would like a session to mean when the browser's open. And then once it's closed, that session's over, but"
  },
  {
    "startTime": "01:14:00",
    "text": "cookies persist within a across you know, that that lifetime is generally a session. So we should probably decide what we wanna do about that. Also, have you had any conversations with any of the standards organizations about moving some of these tests into that purview, like what WGE, you know, web web platform tests, anything like that. I haven't at this point. I I I would definitely be open to that idea. I've sort of, yeah, focused mainly on just trying to stand the tests up, but I I would be happy to to discuss that. And yeah, I've opened to different ideas around cross session and what what how to define a session. For sure. Go ahead, Nick. Hi. Thank you. for presenting this and for doing all this work. I I do think it's very useful, and part of my question was gonna be what Matthew just has. So so great. The other the comment I had was seems to me like many of these APIs are or functionalities are being used by websites, which causes loss and that maintaining functionality actually seems like the biggest barrier or or the biggest reason that browsers are providing this functionality. You didn't include functionality in in your list of of reasons why browsers might not be making enough progress here. say that's something I want us to work on at w 3 c's to make clear which functionality is being used and which is being abused. so that it is easier to block abuse. But I I was curious if you if you thought about that or if there was a reason you're not including functionality and that reasoning. Yeah. I I you're you're completely right. I guess my sense is that we're still kind of at the low hanging fruit where there's a lot of things that can be fixed"
  },
  {
    "startTime": "01:16:00",
    "text": "without really compromising web compat or compromising functionality, but you're right that there are such things And I think you know, I I my goal would be to find some ambitious, but but but realistic point where we can say, you know, the browser should limit what it's what it's leaking. you know, for example, in terms of But, state partitioning, There's a huge list of things there, but they're all partitionable as far as I I can see and some browsers are doing that now. So I I I think we should we don't wanna overemphasize web compatibility for functionality because it it I think sometimes it it can be kind of an easy out, Whereas, I think we should be trying really hard to to make sure that things aren't leaking. and and probably to the point where it's a little bit painful. Thanks a lot, Arthur. We do have to move on. Sorry. Are you the kiwist Coast. Yeah. Thanks a lot, Arthur. Again, there's a lot of discussion happening in the chat. Okay. Yeah. Thanks to you as well for joining us late on a Friday evening. Thanks. Yeah. Yvonne, will you be presenting Chris? Presenting Chris? Hello? Can you hear me? We can. Let me Yes. share. K. Okay. So since we are both Fernando and I remote, I would ask someone, the chair, to pass the slides to minimize risk. of weekends falling apart in the middle of presentation. We cover in regards to this"
  },
  {
    "startTime": "01:18:02",
    "text": "Yeah. So -- I gave you the slides Evan. I there's a lot of backruns coming up in London. I'm just gonna -- Yeah. -- revoke audio for you, and then you can request it again. Okay. No. Go ahead. So Okay. Okay. We're good. And how are we on time? Do I need to go super fast? Or what's what's I guess, yeah, like, 18 or 20 minutes would be would be great. Okay. Okay. So so okay. This presentation is about 3 draft that were just published recently. will give the intro about what is this about, and then Fernando will talk about specifically about each of them and what what they contain. So So it's about numeric tracing numeric identifiers. in the in IECF or protocols. So what do what the hell does that mean? It's a fun see 94 numeric IDs in protocols, and we call them or we define them as the data objects that can uniquely uniquely distinguish one protocol object of a given type from other protocol objects of the same time of the same type with him some context. So here, you have some examples TCP initial sequence number or the IP ID field in in an IP.datagramordns query ID, or the IP interface identifier that maybe more there are more fragmentation ID in the ipv6 potential header, the fragmentation header, or then connection ID in in quick, or the reference ID in the in NTP. So there's IDs everywhere. Tadhaar represented as usually numbers"
  },
  {
    "startTime": "01:20:03",
    "text": "and are and are transmitted as a series of key. in the in the war protocol. and these ideas usually have some interoperability requirements. like, for example, they must not duplicate during the lifetime of the packet in the network, of more a diagram in the network. or they may be it they're must be unique the within a network or or to identify a network interface uniquely things like that. And sometimes, when these interoperability requirements are not met, but things happened to the protocol. But the bad the severity of the failure is not usually very well spelled out. There may be I don't know. A duplicate ID in in in some in certain cases, may make the protocol fail and a session has to be reestablished, or in other cases, it may produce a denial of service. or in other cases, like, for example, with a a fragment ID that it's a duplicated it may insert data into an existing in the reassembly crew, or it may be silently discarded and nothing happens. Right? But these things usually are not very clearly spelled out in in practical respects. So So there's a history of problems with numerical IDs. If we define numerical IDs, this way. And and here I mean, one of your sees that got published, we have a a historical account of problems that date probably 40 years now. Like, for example, TCP, ISN issue was 1st discussed in 1985, in a paper in 1985, and a personally, I found the same problem in some implementation"
  },
  {
    "startTime": "01:22:04",
    "text": "TCP implementation with 1,000,000,000 of devices deployed last week. So it's something that it these things, these problems, security problems, and and privacy problems due to numeric identifiers that are not very well define in in terms of their properties or their requirements, keep happening for 40 years. and they happen in different protocols. in the implementations, but, also, we track down these to certain, let's say, in accuracy or lack of a of a linear definition in the spec. in the protocol spec itself. And it happened that when we see problems in one protocol, and we fix them, those fixes and the lessons learned in those protocols don't usually the travel to other protocol specs, like we see problems in the TCP initial sequence number and when when predictable those numbers are generated, and we address that. and we fix them. And we tell implementors how they should generate their their numbers. but then some other protocol in some other group get created, And the lessons learned about that were not transferred to those other groups. So the pro the the problems keep repeating. So So this is something that we observe, and that's a motivation that that prompt us to start working on this. we kept loop seeing these problems happening several times, and we said, so is there something that we can do to actually try to go to the to the root cause of this. or or at least to address this going forward"
  },
  {
    "startTime": "01:24:03",
    "text": "with some common guidance. And that's what we and then tried to do. It was originally something that we started working on 2016 when we were talking about I was talking with Fernando about some potential attack that we found in in the IPV 6 fragmentation mechanism. And, actually, Fernando was talking to me about it and explaining that to me, and I said, oh, look. Wait. This sounds like really familiar, it's, like, the same thing that happened with IP before. So how come we are repeating this this this problem? So that's how this started. You just started as a single draft that provided a sample of the programs in different protocols that try to build a taxonomy of how to categorize these problems and then to the to give some guidance and advise on on what to do to address them. And, eventually, it got separated in true 3 different drafts, which the Fernando will talk about now. So for Landau, it's for you to continue. There you go. I cannot pass this last myself right now. Right? Okay. It'll say, like, Okay. Cool. So as Ivan correctly said, So the our, you know, original document, our original draft was split into 3 different pieces. 2 of them were eventually adopted by these working group and the last of them was actually published as an AD sponsor document. So we I will briefly provide an overview about the, you know, each of the 3 RFCs that were recently published."
  },
  {
    "startTime": "01:26:04",
    "text": "First one is RFC9 9414. entitled unfortunate history of constant Numeric identifiers. Next slide, please. You should have control for Nano. give you Okay. It looks like yeah. let's say, there. There you go. the weight. Okay. Okay. So, essentially, what this document does You know, the the bottom line, it was obviously part of it was part of a single document. This document tries to performer root cause analysis. And part of that was to, you know, gather, like, a sample of different numeric ideas. We and, you know, essentially documented timeline. or including, you know, events relevant to these transfer numeric identifiers considering things such as, you know, standardization work, Bunerability advisories that were, you know, published or released. and, you know, research work. Also, we, you know, document it to the extent that is possible, like, POC or tools that we're exploiting these issues. And at the end of the day, you know, when you look at the timelines, it's like super evident that essentially, we were repeating the same problem over and over again. One particular example is the example that Evangel mentioned that of, yeah, you know, the IP identification would happen happens both in ipv4 and in ipv6. So, eventually, you know, the the the flow was found in, you know, IP before implementations. it got fixed But then when ipv6 was specified, essentially, it was specified in the same way, and the issue also affected ipv6. So it's, like, it's super easy to see, like, when when you to mind the, you know, different timelines that, actually, the lessons that we learned or that we were"
  },
  {
    "startTime": "01:28:01",
    "text": "supposed to learn from, you know, working on float transcendameric identifiers, we're not actually really learn or we're not really leveraged to actually, you know, avoid these issues from happening again. in other protocols or in new versions of the of the same protocol, like IP before and ipv6. You know, one way to summarize, you know, the the root cause of these issues is that in a lot of cases, you have specifications that employ Transcendumeric identifiers. and they don't really spell out what are the interoperability properties that are required from those IDs. So in a lot of cases, you cannot really, you know, tell, you know, what are you know, what is needed from those IDs. And, obviously, that makes it hard to whether, you know, one algorithm is doing a good job or not. because you don't really know what you have to comply with. another, you know, another of the root causes of this is that you have specifications that recommend float algorithms. Like, for example, you know, they specify that you should select identifiers from a global counter. obviously, that makes the IDs predictable. But also you have cases where you have a protocol specification that doesn't recommend, like, a way to generate the IDs in the first place. So that leaves the implementer with the decision on what to do. And normally, you know, an implementer is doing like a 100 things. So they are not going to spend a lot of time on, you know, picking you know, an algorithm for generating that is. And quite often, it happens that they pick like, the wrong algorithm or an algorithm that complies with the interoperability, you know, requirements, but that has, like, negative, you know, security and privacy implications. And, finally, you know, there's also some cases where, you know, implementation simply failed to comply with every requirement. Like, for example, we have had, like, BCPs for randomizing source ports. of, let's say, TCP connections like the ephemeral ports of TCP connections and there are still implementations that simply fail to do that."
  },
  {
    "startTime": "01:30:02",
    "text": "I believe that, you know, what these, you know, these RFC in particular, it was like part of the know, the same original document at the time. I believe that, you know, makes it clear that it's a recurring problem that we been heating over and over and over again. And know, it provides some hints as what needs to change you know, for implementations to stop using float transcendumeric identifiers. Next slide. There you go. And second of the documents is, like, more lengthy and more complex if you wish. It's Rfc 9415 on the gen entitled on the generation and numeric identifiers, we're not going to, you know, discuss all the contents. But just as an overview, it introduces economy of trans and numeric identifiers try to figure out what are the different categories for this identified, which there are not a lot at least, you know, when it comes when it comes to the analysis that we did. what we did is, you know, analyze, like, set of trans and numeric identifiers, like, from different layers, like, ipv6 interface identifiers, you know, ipv4andipv6 fragment IDs, DNS transaction IDs, and so on. and try to figure out what are the interoperability pro properties that are required from them but also the failure severity. Like, okay. If you don't comply with those properties, how bad do things get. As in addition to that, what this document does is for each of those categories that we identified, try to come up with an algorithm that is good enough for generating, you know, those IDs and do, like, you know, thread modeling for those algorithms. And finally, we also discussed some of the common algorithms that are, you know, employed by popular implementations to generate IDs. So, obviously, we are not going to cover the algorithms. It would take, like, long time, but, you know,"
  },
  {
    "startTime": "01:32:02",
    "text": "I will just, like, briefly mention, you know, you know, the the taxonomy essentially, we came up with, like, 4 different categories. Okay? So, for example, for the first one, category number 1, the interoperability requirement is that of uniqueness. and, you know, the failures verities like soft failure. So you want unique transinamerica IDs, but if for some reason there is a collection there's a collision, you can quickly recover from it. So it's not a big deal. Obviously, the idea is that for category number 1, you can just use, like, you know, a good random number generator. because if, you know, just by chance, you get the you know, you get to reuse the same number, things are not, you know, are are not bad. Like, things don't break badly. 2nd category uniqueness, but heart failure. meaning you cannot just, you know, run the chances of you know, selecting a duplicate number numeric IDs that fall in this category is like the fragment IDs, or the for example, TCP Femareports, Then we have category number 3. which is like, it has a requirement of uniqueness and stability within context. Okay? with the soft failure you know, if you happen to select the same ID. And the specific example is IPV 6 interface identifiers. Normally, what you want at least for stable addresses is that, you know, the ipv6 interface ID is stable in the same network. So as long as you are in the same prefix, network prefix, the interface ID is the same so that the address that are resulting at is stable within that you know, within that network And finally, we have category number 4 where the requirement is of you ness again, but we need monotonically monotonically increasing within some context. So what you want is numbers, again, that are unique but you want subsequent numbers that are larger than the previous number that you have selected."
  },
  {
    "startTime": "01:34:00",
    "text": "That doesn't mean that it's a linear function. Of course, that'd be, like, predictable. but you want the numbers to be increasing. And that's the case, for example, with TCP initial seconds numbers and the TCP initial tend stamp, which is the value that you use in the first an instant option that you send for a connection. You know, after we, you know, define these 4 categories, what we did in our document, is propose one algorithm or at least one algorithm for each of these bigger is, and the idea is that if you are specifying the protocol and you do the analysis of what are the interoperability proper properties and the associated failure severity that are required for your and American identifiers. And if you don't know better, you can pick 1 of these algorithms. Okay? So you don't know you don't throw the burden at the implementer. so to speak So that was the second document. This third document, was an idea of standard extract document. Okay? because the previous two elements were, you know, were adopted by this working group, but obviously being a a research group, we cannot like you do standard stack. stuff. So these RFC 9416 is security considerations for trans and American identifiers, employee network protocols, and is essentially an RSC that tries to introduce requirements or protocols specific patience. Okay? And it boils down as you know, as follows. So what we do in this RFC is introduce requirements for any specification that uses or employees transatlantic identifiers. And what will require from them is that they must specify the interoperability requirements of the associated trans and numeric IDs So if you're right in a speck, you have to tell and you have to clearly specify what's required from those ideas. This, of course, is useful so that when you see an algorithm somebody proposes an algorithm, generate these IDs, You can tell if they are just complying with the interoperability requirements"
  },
  {
    "startTime": "01:36:02",
    "text": "you know, that have those IDs or whether they are for example, overloading the IDs unnecessarily with, obviously, with the obvious security and and privacy consequences. Second requirement is that you must perform a reliability assessment transcending numeric identifiers, you are using identifiers then, or what if another care could, for example, predict that the fire. Are there information leakages? Could those be used for performing different kinds of attacks. So that's like a mandatory part if you have a speck. that uses Transcend American fires. The third one is probably the obvious one. you should not employ predictable trans and numeric identifiers, and it this is a should not because there could be some potential case where there is an interoperability requirement for the IDs to be predictable. but all other things being equal, obviously, you never used predictable France in America. It is. 4th one, is that your specification should recommend 1 algorithm for generating the IDs. Why? Well, because if you specify the interoperability requirements, you shouldn't throw the burden at the implementer to just pick an algorithm for that. So recommend what algorithm that doesn't mean that the implementer needs to follow that but at least there's a default option that is good enough. And the final one, the we state that you must follow these recommendations even when even when cryptographic techniques are employed. And this is because, you know, while we were progressing these documents, some people argued like, well, you know, if we use a TLS for this protocol, We don't really need to care about this. And, yes, you do need to care Because if you select the IDs improperly, you know, still a number of things could happen such as, for example, information information leakages. So again, this one is standard track, I believe, you know, it's like a super important part of, you know, of of this set of documents because it's the one that has formal requirements for, you know,"
  },
  {
    "startTime": "01:38:00",
    "text": "specification, you know, authors. So as a conclusion conclusions about this, well, this has been like a lot of work. work on these, like, 7 years. It's unbelievable in a way. but it is. We hope that these will have a concrete impact on, you know, how, you know, Transcendumeric identifiers are specified. At least I believe that we have, you know, provided an instrument for people for people such that they can, you know, point at at these documents when, you know, folks are introducing float transinumeric IDs in their specifications. And last but not least, like, we'd like to send a big thank you, you know, for all the that all those that provided valuable feedback on these documents, and also to no to these 2 working groups, chairs, Chris, Ivan, SADA calling are are IRTF chair, you know, Paul and Roman the security IDs because They were, you know, critical, you know, at different stages for this document actually to move forward, particularly at the last stage where we were kind of, like, running out of of so I don't know if there are any comments or questions There was some discussion in the chat. Did anyone have any questions for the mic line? Nick, go ahead. Sorry that I'm speaking up so much. thank you for this work. I intend to point lots of people to it. I wonder if you had considered or or if it would be somewhere else consider these threats where Identifiers can be linked if they are rotated at different time. it seems like your drafts don't talk about rotation. Maybe that was at a different layer or something that certainly at the application layer, that's the risk that we're seeing, and it's a risk that's compounded by identifiers changing a"
  },
  {
    "startTime": "01:40:01",
    "text": "layers in the stack, and then, you know, you you change one, but you didn't change the others, and then you didn't really hide your identity. Yeah. We did cover at least part of that. Like, for example, for interface identifiers, you have the Mac address that is in the lien layer, which then gets or used to get reused at the Internet layer because it was employed for you know, for the ipv6 interface identifiers. So what our document says is that you shouldn't reuse identifiers across, like, different context or different layers when they are not needed. And just to add a bit on that, that's one that's just one example, but there's several more of that. Like, Also, in the case of a stable addresses, across networks in ipv6oridentifiers you could track it with your with one of the original others in modes, you could track cost across networks because of the same thing. And, generally, to be to be more general. There are problems when you have a a a a 90 field in your protocol, and you add semantics to it that are not needed. Like, for example, you have an ID but you add topological information to the ID. like, in an interface identifier, or you are you have an ID, but you are some global contract which implies some order sharing property. in the embedded in the ID, or in some cases, which has happened, and this is, like, a real work example. You have a unique ID in your protocol identifier but some implementer decided to add the value of a kernel pointer into the ID. Right? So you so you start to just start breaking the requirements, you just have very is"
  },
  {
    "startTime": "01:42:01",
    "text": "trick and and feel security and privacy requirements for your ID field and by adding semantics to it or overloading the the semantics of your field, you are starting to create risks that you are not aware of, generally. So that's why we are trying to to make this assessment and this specification more mandatory for protocol designers. So they are aware of what is needed and what is not needed. Absolutely. The the Social Security Administration in the United States has has an offline case of this, where we put some intelligence in there and had many of problems. So thank you thank you for documenting it. Jonathan, Jonathan William Cloupler, I'm Have you I I have no chance to read the document. Do you describe what the difference between this and channel one dings is, as in the cryptographic version. I I I'm you mentioned in your thought privacy properties Did you do you do you describe that in more detail in the document? I don't remember about, like, you know, I'm talking about channel bindings in our documents. off off the top of my head. No. No. We don't we don't by the specific techniques. Okay. Yeah. What we mean about the cryptographic techniques is simply that you know, while this document was being progress, they were like let me give you a concrete example. So there was a protocol quick that was specified at the time. This is a concrete example. And they didn't have, like, you know, the the the the their specification of the trans and numeric IDs wasn't like what you call, like, crystal clear, so to speak. And the argument was like, well, we don't need to do that. because we use TLS, so we don't need to care about the thing."
  },
  {
    "startTime": "01:44:00",
    "text": "So what we meant with that comment about, like, crypto traffic techniques is that even if you use cryptographic techniques, but your protocol uses things such as, I don't know, board numbers, IDs of different source, you need to follow this guidance too. Just the fact that you use TLS doesn't mean that you can use predictable like this within the protocol with no problems because, for example, could have IDs that are selected from a global counter, and that global counter might leak like example, the number of sessions that that, you know, protocol has established since the last time the system was bootstrapped. That's what we meant So so they just to summarize, the the the bottom line is that even if your protocol goes over an encrypted channel, 8. that doesn't give you a free pass to generate IDs in in whatever way you want. because there may still be problems with that. Right. So channel bindings is how you cryptographically generate an ID that's guaranteed to be unique. It's it's not Right. It's not it's not about AAD or anything. it's how you generate a guaranteed unique Could we take this to the chat? Yeah. Sure. Great. Thanks. Thanks all, and thanks especially to the authors. yeah, the chairs know it's been a long time, and thanks for your work. as the documents went through, like, multiple iterations and splitting off into several documents. So thanks also to our to to Paul for sponsoring document 9416. Yeah. Yeah. It's our 1st RFCs. Cool. Matty? come up. Do you wanna request? percent. Sure."
  },
  {
    "startTime": "01:46:04",
    "text": "Hi. So I was asked to come to the privacy research group to talk about a letter that was signed by and and what sparked it. So There are lots and lots of things I could say about the current policy landscape, but we're just focusing on 2 of More than that, there there are more laws even just in France that that might be interesting for folks to learn about if you are already following it. But this just focuses on 2. that have implications for the DNS and for browser. So next slide, please. I'm gonna go over the laws first. to give you a bit of context and then I'll talk about the response. So the first law, I'm gonna talk about these are both proposed, by the way. They're in various states. of having been approved and I tried to be up to date on that. But the one this one is It's called the military planning law. With this, you can there's I I include a link if you wanna download this and follow Pointer to read it yourself. It's in French, obviously. The it's a it's a larger bill that is intended to set military planning strategy for France over the course of the decade. It it has made progress. It's it's going to be implemented rather soon. But under the first part of issues we had with it. They're is some focus on cybersecurity. As one would expect in a in a law like this. However, That's not. the problem I think the problem is that a lot of the ways The lot intends to mitigate cybersecurity threats. don't don't in general follow best practice for cybersecurity security cooperation, recognizing that it's sort of global multi stakeholder approach because the problem is so very pervasive, it requires"
  },
  {
    "startTime": "01:48:01",
    "text": "a lot of different actors to cooperate The first issue is that it would require den DNS Providers. Obviously, they're thinking about ISPs's providing DNS lookup capability. But this, unfortunately, would implicate any DNS lookup provider no matter where they're located to Block gains, and it would be the sort of the national information some security authority. that would request that block and it they could do it without a court order. It gives them the power to do that. It would also require soft vendors to disclose vulnerabilities regardless of their state. in terms of patching or mitigation. So this go this contravene's best practice where often vulnerability disclosure, you really have to take into consideration where the patching, like, where we're at in the mitigation of the vulnerability. before you disclose, you have to disclose responsibly. This does not require that consideration. The next part of the law that was raising eyebrows was that comps writers, you can take that for what it means. Would also have to disclose certain kinds of Internet traffic that that, yes, there are protections that it shouldn't be, I I I I I personally identifiable traffic, but with upon request from ANSI, anyone that's providing communications would need to re respond to a a request for that kind of thing. Tommy, do you wanna get a clarifying question in here? Yeah. This is purely clarifying. On the point of the DNS providers, kind of regardless of if they are network based or not. is there any distinction or clarity around a resolver that is running within France versus one that is running out So -- Yep. So -- Yep."
  },
  {
    "startTime": "01:50:00",
    "text": "any publicers over that see someone that they believe is coming from France. around the world would That is right. Yep. One of the one of the issues. Yep. And then the last thing that that is related to the 4th point is that this disclosure of Internet traffic and other things, there's actually data collection tools that would need to be installed at various data centers. But that and and that also would not would would not require court order to do that. So that sounds bleak. But the and it but it's not the end of story, there are things I'm not touching on that are not specific to the Internet that that that that you can read about in point b, which are more the broader civil liberties case And I would just like to point out just for perspective here that This is always going to be attention with any kind of law enforcement related or mail intelligence community related bills, there's always gonna be a tension between civil liberties and you know, powers investigatory powers like that. It's not it it happens in the all the time. Very, very common. I'm not trying to minimize the issues here, but we feel that the technical specifications around how some of these things work have specific damage proposed for the way the Internet operates, and why we're getting involved, whereas one might say, well, why aren't you involved in the 1000 other kinds of ways that that, you know, spooks and cups would maybe wanna get at data, which they do all the time and they do it through ecratic processes like building laws and so on. So I'm not up here to excuse that, I'm just trying to put in context for folks and and also to make the case for why it's important intervene on this one and maybe potentially not other ones. So there's another one. I have another slide. This one is more broadly just been referred to as the digital bill in France, there's a third one I mentioned that's not part of this suite of things that took me some time and my very modest French skills to understand the difference between"
  },
  {
    "startTime": "01:52:02",
    "text": "The digital bill, again, I've got a link for it if you want to read it yourself. This is largely around policy. You can think of it as if you've been following a little bit the conversation around the digital markets act, is at the east at the EC or EU level and also the Digital Services Act. the EU Digital Data Governance Act. This is an attempt this digital bill is an attempt to implementate To implement the those EU laws in the front. the French context. So many countries are gonna be doing this. France is not going to be the 1st or the last But I think what was interesting about this proposed implementation is it actually goes beyond what is in already the DMA and the DSA? a number of ways, also leverages In some of the provisions in the military law that we just talked about on that. And and and so the where this bill is at, It's not it's not as far as the military the military bill I think we can think of is just like a done and dusted thing. This one is is working its way through. somewhat uncontroversially, actually, despite some outcry by civil liberties groups, but hasn't yet reached the same It's not it's not quite done. There's still maybe time to influence it. So the first couple of issues are Again, requiring DNS providers to block domains, this would be done without a court order. And it additionally, for the same kinds of reasons. Again, we're also not just talking about cybersecurity here. that's the other thing to remember about the digital bill as it goes potentially beyond just like making sure there's no malware and spam and stuff like that. On the Internet, this could be for things related to you know, political like, the digital services act in the digital market tech to find those things. Also,"
  },
  {
    "startTime": "01:54:02",
    "text": "this goes into asking browsers to block certain domains. also without a court order. And in some cases and I was wasn't there wasn't a lot of documentation on that I could find, but it also sounds like not just blocking domains, but actually the browsers would need to serve user's warnings. certain cases, which I think is functionality that doesn't quite exists yet or at least isn't being used for these kinds of content, you might get for example, a user warning if certificate is Right? valid for whatever reason, but this sort of would be user warnings delivered by the browser for other things. Again, I would point you to a non Internet specific related issues folks have with this bill, and that would be in point b where you can click through and you can learn about the ways that this requires blocking of certain kinds of media, We'll recall that currently in the EU, specifically, this was championed a lot at the time by France that Our tea and sputnik are blocked. It's that kind of stuff. And the other thing that I my organization center for democracy And Technology cares a lot about at the moment because it's cropping up in a lot of different countries and a lot of different bills is this requirement to age verification. Even if you just need to toggle, like, yes or no Is this person old enough to do whatever? Not age or birth date or whatever we have really real serious can concerns with this idea that age verification is an acceptable way of buceting user experience when accessing the Internet. And then a whole slew of other sort of human rights concerns, I particularly linked to I intentionally linked to a news piece because I think they highlight In particular, a group base in France that speaks on behalf of the public interest in civil society called Liquid Richard Net"
  },
  {
    "startTime": "01:56:03",
    "text": "and they are quoted there. So you can also follow them specifically if you wanna hear about all their other So I will ask for the next slide Just before we do the next slide, someone in chat off what the name of the bill is because the link is dead. That oh, the link is dead. Oh, that's a bigger problem. I'll put the link in the chat. Actually, you know what one can do? this letter has the link to that bill in the first paragraph. And, again, like, I couldn't really find a better describing everybody's calling it the the digital bill. You mean the your active link has the Oh, the your active link. No. No. I mean, like, do you mean, like, blog post, which is the your active link over here, or did you mean some other thing. Which link is that, John? So first, the your active link. The bill. Yeah. Okay. My bad. That's links from Vince letter. So we're getting to that point. And sorry I'll hurry up. I didn't realize I was taking so long. So yeah. So there's a response. Again, we're trying to narrow this critique to just the parts that make sort of the Internet less resilient, less inclusive. Vent Surf was the pin holder on this letter, reached out to the Internet Architecture Board, and other folks. Also, you'll see ISG, at least one ISG member who signed it, and folks from ISOC and others. this is the extent of the letter that was then delivered directly to Well well. Well well. know how directly it was delivered. It was addressed to the members of French parliament who are considering and debating these these laws. Next slide, please. it makes 3 overall arguments, The first is that it's quite ineffective, unfortunately, especially on the cybercrime piece, We know that best practice in cybersecurity has been sort of collaborative, multi stakeholder global effort over the last"
  },
  {
    "startTime": "01:58:01",
    "text": "several decades. There's a lot of lessons learned and best practice and norm setting. some of those provisions contravene that well hard earned advice. And then the second argument is that this is really overreach. So there's there's risk that in a lot of different ways this could be bladed. Right? Once you start talking about, you know, blocking certain kinds of media sites, like who and how is that determination being made, even in a democracy, like France where laws can be made and laws can be unmade and decisions can go before a judge or not, etcetera. It's still a slow moving beast once you create the mechanisms to do the thing. very hard to keep that in check. last argument that is made is that obviously, These things are meant for France within its jurisdiction, national borders, nothing wrong with well, Not a lot of things wrong with sovereignty. That's the status quo, but they would actually then require compliance and changes to browsers and DNS resolvers and so on that exist outside. of Francis jurisdiction. And so that would then essentially require all of us to somewhat participate in this version of the Internet. Next slide? We just have a little bit of time left for Metairies. You're right. I don't I'm not gonna go through all of these, but it's in the slides if you wanted to time ingrained about it. could also just read the letter because this is really just a bullet bolded list of the arguments made in the letter. And I think if we go one more, It still impacts on the Internet and maybe another one. Tada, That's the letter. and I would be happy to take any questions. There's already somebody in the mic queue. Yeah. Cool. Lorenzo, the Android. I have a question and a"
  },
  {
    "startTime": "02:00:00",
    "text": "and a comment. The question is, like, I really know nothing about any of this, so please forgive me if this is like blindingly obvious. But One of the things that you said many times is blocking domains without a court order, and I I'm I don't know why that would be somehow worse or different from blocking domains with a court order. Is it, you know, because different sets of people decide or because you know, it cannot be challenged in the same ways, or is it just because, you know, in some countries, a court order is the thing you you do, and in others, it's not. I that's I have no idea. Right? So -- Exactly. So I I actually have less if issue with it on the blocking side than I do with the, like, access to traffic flow side. But nonetheless, you're precisely right. A court order is shorthand for democratic check and balance on a consequential decision. And so even within France, there wouldn't then be that check and balance. And then obviously, globally, I I don't get to vote and French elections, so that would still impact me nonetheless even if there were court orders involved. tried And the comment was we Android doesn't have a browser. Right? So it's not subject to anything that applies to browsers. One comment I do have is that the if if if the difference with the status quo or if the additional overreach is due to, like, requesting browsers as opposed to Internet service providers to form blocking. 1 point I wanna make is that if you rely on blocking by Internet service providers, then you cannot build any security technology that ob obfuscates your information from the Internet service provider. Exactly. So if I guess one way to say it is if you if you want end to end security, you have to put any sort of legal intercept or blocking at the ends. And so that is something that we we need to consider. Right? Like, you know, if we say,"
  },
  {
    "startTime": "02:02:01",
    "text": "if the endpoint say, oh, yeah. We don't wanna do any blocking or anything, we wanna, like, keep our hands clean, then I think we have to understand that the is that the blocking whatever parties are gonna request or impose blocking are gonna do that at other parts. is gonna prevent us from encrypting the tactics. You you are exactly right. I think that this law shows it's both. So it's not an either or, unfortunately, it it it requires both. And I would say that it's a plus one to your suggestion to push things the endpoints, I would take it a step further where it's opt in by the endpoint to decide whether or not it wants that. And I think if when we're talking about malware and things like that. We've already got safe browsing, so this is done. Right? But in terms of, like, do we take safe browsing and the techniques there where it's in point feature, and then present some sort of opt in to users so they also miss out on a lot of other things they might not want That's a that's a u that's an end user choice. I think the agency is really important. Victoria, you just we are at time. So please be, like, under 20 seconds. No. There's no way I can say. We're able to say in coincidence. I mean, I I found the letter. I mean, I found parts of these bills pretty bad. I also found several parts of your letter pretty bad and counterproductive, but there's no time to discuss it. So Sorry. Yeah. Cool. You have missus from physics 2. we don't have enough time to discuss, but I think this is a very, very interesting topic. I was not aware of this bill. Yeah. But it's also political debate. I would say, what's the fine line between secure and having, like, secure cyberspace? and control and blocking. I agree. No. I it's always attention. Right? This is a really important tension. And I think another thing to note that often happens in other places that have passed similar laws is books are dubious about the actual implementation. Like, It gives them power. Do they use it? I do understand the people It's hard to know. Sometimes they don't. for this. They basically don't understand 99% what's"
  },
  {
    "startTime": "02:04:02",
    "text": "allowing other people. They rely on technical people. But as technical people between us at this stage, We're not Sorry. We really do need to wrap up there. I think this is the the this the strategy the strategy was aimed at the policy not at in not at voters and end users was aimed at policymakers because I think you're right in my experience. they're trying to do the right thing. This has been told that, you know, the experts in the room, unfortunately, are often from. In the intelligence community, they're from ministries that work for the government that work on these things. So they have a perspective that you can imagine is quite, quite focused and narrow. We're just trying to offer here's the Internet resilient view here's the, like, people who care a lot about the Internet, working globally. Here's our view on this, and you might actually be inadvertently doing damage. Very interesting quote. Let's have this chat. Cool. That's all we are trying to offer. Thanks so. Thank you so much for -- Thank you. -- compressing everything in 15 minutes. all, and see you all next time. Thanks to Charen. Oh, thank you."
  }
]
