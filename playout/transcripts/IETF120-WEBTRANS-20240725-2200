[
  {
    "startTime": "00:00:01",
    "text": "So Bernard, Victor's not feeling well, so we're going to have Eric run his slides. Okay Thank you yeah uploaded the deck about an hour ago, so hopefully there have no changes have been made since then Cool, yeah, I don't think they're half. Thank you Thank you All right I think everything's been working. Good, I test the mic. I think we're good Thank you very much Oh do I have to call? Oh, perfect, okay. Thanks, that's awesome Awesome, thanks, yeah I was in this room in the previous session. Everything seemed to work perfectly, so apart from the construction next door, I'm not much we can do about that that"
  },
  {
    "startTime": "00:02:04",
    "text": "Awesome, thank you Thank you Thank you Honestly, you can take my check Thank you Thank you Thank you for this Thank you very much if you're going to be able to go Let's speak a figure of a candidate Favorite Well, I'm not a cool chair like Alex who is literally eating chocolate as we speak to I could be all right next time and you have some more for Dustin? Thank you, Dustin, for offering to take minutes minutes"
  },
  {
    "startTime": "00:04:01",
    "text": "Thank you All right, Bernard, I think we can get started. What do you think? Yep All right for folks in the back if you want to take a seat we're getting started hello and welcome everyone to the web transfer session at IETF 120 Next slide, please So if you're not familiar with Meetecho, these are the buttons and all the things As usual, especially a reminder if you're remote to play use headphones or to make sure you mute when you're not talking The echo cancellation doesn't work super well Next slide, please Since this is now a hybrid meeting, we request that if you want to say something, you join the queue by raising your hand in the tool. As a reminder, please all sign in to fill in the blue sheets That way, remote folks can join the queue just like the folks who are here. And we encourage you to turn your video on when your audio is on but to keep it muted when you're not"
  },
  {
    "startTime": "00:06:01",
    "text": "talking. Next slide Here are some links if you're looking at the PDF later. Next slide All right. This is the IETF note well. If you are in this room, whether physically or virtually, you have agreed to this by registering for the event As a reminder, it mentions many things that you have to follow, in particular our Code of Conduct and our IPR policy. If you're not familiar with these, go look up the note well and please read it Next slide And as an extended reminder we really strive to have professional and happy conversations here and everywhere at IETF and the chairs will be enforcing that If you see or hear anything that wasn't great, feel free to reach out to us the chairs or to the umbud team if you prefer We will make sure to have it handled. Thank you very much. All right. Next slide All right, some more links. Oh, right my name is david schinazi and my co-chair is Bernard aboba, who is remote. I will be looking at the chat If you want something set at the microphone and you can't get to the microphone yourself, feel free to type mic colon and your text and I will repeat it And thank you to Dustin for taking notes Next slide, please All right, so we're going to start with our tradition W3C update and then have Eric talk about what transport over HD with our traditional W3C update and then have Eric talk about what transport over HTTP 2. Victor is unfortunately not feeling well so Eric is also going to be the one talking about web transfer over HTTP 3, and that's our plan for today Does anyone want to bash this agenda?"
  },
  {
    "startTime": "00:08:01",
    "text": "All right, next slide, please Yanivar, you're up Buri from Zila you're up. I'm here to give an update on the W3C Web Transport Working Group, which I co-chair with will law from Akamai. So this is an update from our previous time we updated you back in March. We published a working draft on June 4th and our charter is expiring soon, but it was extended to end of August, and we are the we are renewing and extending by two years beyond that and that is out for AC to review yesterday so we have a timetable that we keep pushing out a little bit, so this is what we're looking at right now We're still finishing up some issues We want to get to candidate recommendation, which we're very close to and we have like only a couple of open issues, some of which we'll mention here today And a reminder that we also have the annual keyback meeting, which will be held on the 23rd to the 27th of September in Anaheim where we have a 2 p.m. to 4 p.m. slot on Tuesday, December 24th Next one so an update on major decisions and updates since our last report. We added a note about retransmissions and send order. And I've noticed that ordering of retransmissions is implementation defined but user agents are strongly encouraged to prioritize retransmissions of data with higher send order value And the other small API change is we not support relative URLs, which was the reason change to WebSocket. So we're just following suit there And also on priority a lot of it is in"
  },
  {
    "startTime": "00:10:01",
    "text": "implementation defined, but we're trying to refine it a little bit And this is language that says sending data it is implementation defined, but we're trying to refine it a little bit. And this is language that says sending datagrams should be given priority over sending streams but not to the point of blocking the stream flow completely and the definition here of not starving them is left up to the implementation It's a tricky area to be specific in this area and we're still looking to see if this is sufficient next slide slide And there's no major updates about the priorities, if that's okay, or do you want to take it at the end? Sorry? I had a question about the data grant priority thing but if you want to take it at the end, that's fine So alan frindell, I'm the chair of the MOQ working group and the MOQ Working Group had a multi-day interim last month and did a lot of thinking and has come up with the priority design, which is in the current draft 05, which published a couple of weeks ago And it's not totally compatible with the Always Give Datagrams priority. So that may be an area where we might want to get together, or you may want to take a look at what we've come up with there as input Very good, yes. Is there a brief summary? or? No Okay. That is not entirely true. I think if you rewatch yesterday's MOQ session or if you look at Ian Sweat's slide, which slide deck, which gives an update on what happened between draft three, four, and five there is a slide that summarizes how we want to prioritize things, but it is in terms of MOQ object model things which can any of those things can be a datagram So essentially, like, how if something is transported is totally in independent of how it's prioritized. So that's and that's the way, which is not compatible if you wouldn't mind if you could open that issue on a w3c issue tracker, that would be fantastic. Okay, thank you"
  },
  {
    "startTime": "00:12:02",
    "text": "Okay, thanks All right, next slide No major updates on implementation browser support. It's coming along We have two implementations and a third one coming along. Next slide And so we have some we're doing really good cleaning up our candidate recommendation issues. We have some issues and PRs with IETF dependency and the three things we're trying to do is we're trying to add support for closed initiated drain which is blocked on issue, IETF issue 166 Do we need drain web transport session? that needs debate and resolution? Basically, we're asking we started adding API We started thinking about adding APIs for client initiated drain, but the question was raised whether we actually need it. And the second one was we were trying to add us protocols or sub-proticles constructor argument to web transport, similar to WebSock sockets and the issue there is right now we have a PR, but it's link to HTTP 3 And then there's an issue open to move this to the overview doc and also have a similar issue added to HTTP 2 because we want protocols should probably work for both HTTP 2 and HTTP 3 This has been assigned to eric kinnear and has a PR, it says And also for there's a dependency on data received There's a PR that data received says all data committed, there's an existence PR 14 that needs to be merged"
  },
  {
    "startTime": "00:14:01",
    "text": "So those are the dependencies. Next slide slide and also we have a PR that is adding some stats potentially This is for helping with congestion control So we already have a stat called Estimated send rate. And so that we already have a stat called estimated send rate. And so there's some questions whether we should add additional information that a congestion control algorithm in JavaScript could benefit from or a ascending algorithm could benefit from These are quote bullions there's three of them there's a bullion to say, is sending data limited? This is basically, it's sending up application limited Which would tell you, which would start out true. And then if there's a network congestion, then the valley would flip to false The second one is sending Server Limited which is basically the server is overloaded with data and it's providing back pressure basically, for instance, through flow control. And the last one is sending slow start which is based back pressure basically for instance through flow control and the last one is sending slow start which is basically bullying that starts out true if the client congestion controller implements slow start The question is, congestion control is quite a tricky area to get right especially in JavaScript So we're not sure this is fairly new ground and we figured we'd have a lot of good experts in this room that might be asking, so it'd be useful to ask this group, is this, do we think this would be useful? Is this a good, is this a useful starting point is there anything missing"
  },
  {
    "startTime": "00:16:02",
    "text": "For instance, it was also pointed out that maybe instead of bullions something like a frame counter for blocked frames that we've seen seen sending from the client to the server, I think would that be useful? discussed this here. Yeah, it looks like you have three folks interested with questions or comments Let's find out. Magnus Yes, I wonder how you would derive the sending data limited. I mean, that's, depending on what type timescales you're looking at or saying it's, if you would poll this frequently enough, you would, even if you were sent, data, you would be partially false and partially true, depending on, on saying okay when has the when are you transmit the data, etc., how long is until, you actually have new C-win etc it's it's become interesting. I don't know how it will actually help but the others have view of this I didn't quite catch how this, how were you determined the sending server limit are you based on flow controller What are you basing that? by the way just right before you answer Magnus you come coming through a little loud in the room. Can I ask you to speak? a little softly, a little further from your microphone? Thank you But we understood what you're saying. Sorry, to you, Jennifer Yeah, so whether something is sending data limited, that would be if it's application limited, which i think would just be a formula based on whether the client send buffers are growing or whether they are remaining at a decent pace and basically and you also get an estimated send rate number back here so it's meant to be against complementary information to that so that if you get a"
  },
  {
    "startTime": "00:18:02",
    "text": "very low send rate, you might have questions as well, why is it? so low? Is it because? is it network limited? is it because I'm not sending enough data? Because that's the issue is that I think it's hard to report an estimate If you're not sending it data, it's hard to give an estimate And that could be the reason Yeah. And for the, whether the server is overloaded well, you guys tell me, I understand there's some feedback mechanisms in the protocol, and I think that's kind is so that understand there's some uh some feedback mechanisms in the protocol and i think that's kind of the server is so that i don't know if those are blocked frames exactly or how that works. But the idea would be to know if you're dealing with the server that is overloaded based on okay so it will be flow but flow control Correct. Yeah. Yeah Okay. And that should be possible to extract. Okay. Thank you. All right. Great yeah ericaneer so This seems like a lot of the transport part of the web transport And so I think this really is a room that we'd want to pay attention to this slide The thing that I, my understanding of what these are for is to say the estimated send rate is some number of how fast we think things are leaving the device And the useful thing that you could do with that, is no, should I send at the same rate? Should I? keep in queuing data more slowly than I am right now? because I'm going too fast? Or should I in queue data? should I enque even more data because that estimated send rate? is lying to me because if I were to try, it might go up And so I think these, these boolean, that we have here are essentially a signal to like you know why you might choose to make that choice rather than just being interesting for funzies So the two questions that I think we really want to be able to answer either in this room or we might even be bring some of this to like TSVWG tomorrow morning or ICCRG or something"
  },
  {
    "startTime": "00:20:02",
    "text": "CCWG would be great, but it was not it's already happened. But we can certainly ask on the list. But I think the two questions are very much what Magnus just asked, which is, is it possible to get? correct answers to these? Because there's a lot of things that we don't know, unless you try to send, we don't necessarily know if there is room to send more. And so we don't want to give people Boulin that says, hey, you should try to send What we can say is you shouldn't try to send more because we know that you are, you know, currently blocked on flow control and you, if you try to send more, it's just going to get queued. Similarly, if you are data limited or in slow start, you could try to send more and there's a reasonable chance that your estimated send rate will increase as you send more. But I think that is one of those things that like a lot of media people and a lot of other folks who are planning on using Webtra transport are going to be spending a huge amount of time because very good friends with these and our goal should be to not mislead them Thanks christian huitema, I would like that you be very clear about what information what signals you want and why I am a bit concerned that you are building an APA based on some kind of internal model you have of what congestion control does and how it operates And there are many different congestion control algorithm and they don't necessarily have the same notion For example, take something like I start plus plus, which is a variant of the of the cubic start It has a slow start phase and it also has a slower start phase and there are concepts like that So I think that the API"
  },
  {
    "startTime": "00:22:01",
    "text": "instead of being congestion-centric, should be customer centric You want to understand whether you can stand faster. You want to understand whether you should send slower. You would understand what but you should do the API best on that because basing an API on the model of the congestion control machine is making assumption about how that machine is built and those assumptions will probably never be true for all the congestion control that are being applied Apart from that, what Eric said was, yeah, I mean, if you do an API like that, you will want to pass it through the control congestion working group which is doing that All right thank you. That's great feedback matt mathis, amongst other things I'm the first author on the TCP extended statistics bib, which included exactly this kind of metric And as far as I'm concerned, we of the things that we did to make our job easier is we made the specifications for the metrics fairly general general and the RFC specifically said you had to understand the implementation to know exactly what they meant, which is sort of an out for christian hopps question. However, sending limited turned out to be super complicated and we never got it right. And the reason it's super complicated is you get into situations where you can't tell what's going on in the sender because there's a priority inversion in the sender itself And so you end up not having to it's super complicated is you get into situations where you can't tell what's going on in the sender because there's a priority inversion in the sender itself. And so you end up not having data from the application because you're doing something high priority to move data on the wire. And you get because of problems like that, it's wonderful to imagine these things working and I hope you can get them to work, but don't be surprised if they don't Thanks"
  },
  {
    "startTime": "00:24:01",
    "text": "Not to get back in line, but to Matt's point I think I'm in a similar boat of when we first started talking about this, it was like, holy crap, what are we doing here? And if we can find a tightly enough scoped, if the Venn diagram between things that are possible and real and things that would actually be useful to someone, if those are not two completely distinct circles, then I think we're trying to make that be this list. So like, is sending data limited? is arguably some variant of either just is by, are not two completely distinct circles, then I think we're trying to make that be this list. So like, is sending data limited is arguably some variant of either just is bites in flight less than the congestion window and or have you denied any bytes in the last time quantum? And if you haven't denied any bytes, then maybe now is a good time to try Yes And so like we talked about that maybe being a part of the API and then that seemed really crazy also to turn the question around let's say we didn't provide any of these would that be upsetting to anyone? Because we don't want to build this just because we can, and we are looking for feedback if people really want this or not or whether an estimated send rate by itself is useful I echo that question towards our mock friends. Hello, mock chair Jumping as chair. Chair that question towards our mock friends. Hello, mock chair. Jumping as chair, Janivar, do you know if there are requests for this information on the W3C side of things? I think, was it Victor that raised this initially? But we don't have a lot of outside input yet I will say that one of the biggest concerns that we've heard from folks who would otherwise be good adopters of web transport and mock and other quick things is that the information that you get today when you write your own UDP packets and build your own congestion controller as a completely separate thing from all the other congestion controllers is going to be missing and that one of the biggest sources of consternation is yes i know i get datagrams and yes i know they're not reliable"
  },
  {
    "startTime": "00:26:02",
    "text": "but they're still congestion control and that is of huge concern to people. So as much as I personally am like, what are we doing here? That may be a blocker for a lot of people. Thank you Yeah, our imagined use for the extent of statistics was to, as a general diagnosis facility. And to the first approximation, you'd love to know if it's the sending side, the network, or the receiving side, that's the bottle But it never it turned out to be useful for that and only a restricted class of application particularly because transactional data looks completely weird and sort of bulk, bulk indicate of which that is the bottleneck tells you nothing. It tells you who is idle thanks and as a reminder please say it your name to make our scribes life easier. That was made mathis. Question Vita, Matt I understand why the application wants to have a general idea of how fast a network can go I mean, that means, for example, how many video streams can you open and things like that? People want to know that. So, yeah there is some kind of request there In reality, the only valuable information is what you get when you try and you may get some information as in, I'm queuing data on that stream What's the ETA for them to leave? or is there already a queue building up on that particular stream? And the information will be different based on different priorities and relative priority of the stream now I'll throw you one one issue there We are going to see use of multipass at the transport layer each multi-pass, each of the multiple paths, have their own congestion control So trying to abstract that can be really hard. So we have a"
  },
  {
    "startTime": "00:28:01",
    "text": "tension there and I understand that you are building a model so that basically people can build some piece of job JavaScript and have it ported in values on Vio environment and it still works. But there is a huge tension between having precise information which depends on lot on the implementation and having useful information I think that if I was to do that, what I would try to do is provide information tied to the API tied to I am trying to write on that stream, how long would it? take, or what is the queue or something, what is that, something that can be measured, something that give you information about there underlying network Oh, did you? Sure Not to be at the mic again because I think it's potentially worth the face plant on this just given how blocked things have been The goal in my mind here isn't to say how fast could you send. I think the thing that we can say is this is how fast things are going out right now which is a little bit different from this is how fast you are in Q queuing them, right? This isn't how fast are you are you send on them. This is how faster they actually leave the device. And for a limited set of use cases, this is a place where we know that either is is bound, that this is a floor or that this is a ceiling So we're not ever going to try to say, we think you could go this fast on this network. What we're going to say instead is this is how fast you are currently going and either we know nothing you might consider trying or I happen to know for sure that if you put more data into this pipe, it's not going to leave the device any faster. So like if there's a way to scope this to just the place,"
  },
  {
    "startTime": "00:30:02",
    "text": "where we actually know for sure that it's not going to change, if you try, then that might be a tractable scoping Jonathan, yeah, looking over the link somebody sent to Victor's initial request, it sounds like what he's actually asking for is, you know, has this is the number I'm getting for the estimated send rate, is this something where I've actually you've actually bumped into congestion and thus this is probably ceiling-ish, or is this something where you've never seen it? and you're still in some exploratory phase or whatever? So that's something which is, a kind of actionable and be probably kind of usable i mean there's still a lot of questions about what timescale you're looking at and whatnot, which is would have to be very hand-wavy, but I think, you know, is this something where you hit a ceiling or is it not? Is something that is potentially useful and achievable How do all this, John? Trying to remember that the tea, model was designed for file transfer where you had plenty of time and delays didn't matter We are trying to use this stuff in cases where we were delay is the most important characteristic and where we have the chance to adapt the amount of data based on how that affects the delay Exposing this stuff, even if we can't make it perfect, at least gives us the chance to learn and iterate so that we can produce systems that are better suited for the user in the case where delay matters and we can adjust the number of bytes that we need to sound"
  },
  {
    "startTime": "00:32:04",
    "text": "Question with Emma again. I'm not going to visit what we just said, but seeing something the U.S not going to revisit what we just said, but seeing something that you are seeing anything missing One particular thing that might be very useful is an event going from the construction controller to the application saying that, hey, things are changed And in particular, we just lost the link, or we our throughput was divided by 10 or something like that which is an event to which the application needs to respond and that does happen that does happen we see that in practice, and it's probably a useful event to understand All right, looks like we've drained the queue if you want to go on to the next slide. All right, next slide, please And this is the hardest problem in API design which is byte setting So we have in the HTTP 3 draft we have web transport sub protocols available If we compare that to what WebSockets, WebSockets talks about sub protocols in the text but has the request header field sec WebSocket protocol, not sub-proc And the spec talks about the protocols field but just sub protocols in the text And in the IANA registry, we have protocol header field and then yeah there's basically a lot of mixing here between terms. So and also we're trying to add a constructor argument to new web transport, which is basically"
  },
  {
    "startTime": "00:34:01",
    "text": "an array of strings that are separate protocols you can use. So right now the current favorite name for that is sub-product to match the IETF spec the IETF terminology the alternative might be to call it pro protocols, which would match, be more consistent with web sockets, perhaps Or we could call it something else, like application level protocols protocols So right now, I think we're just going to call sub-proticles unless the ITI decides to change their terminology we're not sure that's the best idea but it's a good tIABreaker. Any thoughts on this? Well, I should also say that part of the issue here is that these specs talk about pro- protocols a lot. So, but that's perhaps more of an editorial problem rather than just an API problem problem all right we have two people in the queue nitty go ahead Can you hear me? Yes. Okay Hi, Nadi. I'm one of the W3 editors. So just to get some context, we left us of protocols to match the ITS side because it didn't seem great to call it two different things in the specs. And separate protocol kind of implies to me that it's like the underlying protocol and as Yanukhar mentioned, protocols, the very overloaded word and I think this is the application level protocol actually so of implies to me that it's like the underlying protocol, and as Yanavar mentioned, protocols are a very overloaded word. And I think this is the application level protocol actually. So, but that name itself feels very long, so not to add another option, but how do people feel about application programs? All right, does anyone have? thoughts on that one? I think I see Matt next to the"
  },
  {
    "startTime": "00:36:01",
    "text": "queue I actually just wanted to ask a clarifying question Is the Ianna language, registered language mutable? Do we know? All things are mutable? You mean the one for WebSocket? or for Web Transport? Well, O'Don would be useful if the language was uniform everywhere Well, so I mean, sorry, John jumping in, I think anything related to WebSocket, like the ship has sailed for in a lot of regards, whereas anything related to web transport, the brothers are our oyster, we can do whatever we want want Eric. That ship has already sunk sunk Yeah, I think even trying to be consistent like the existing INA registry for WebSocket is not consistent with the WebSocket spec So I would strongly like to have the W3C spec and the IETF spec match, so at least we aren't in the same bad spot Application protocols sounds fine to me. It's weird to call it a sub-protocolal when it is the protocol running closer to the application than the thing that you're running But right, I don't really want to like get deeply into which one is the top and which one isn't, but application is a nice thing that everybody agrees on where it is and what doesn't matter whether that's up or down So I would be perfectly happy with that Thanks Christian. What a question? on application level protocol Because it's also already used in TLS And there is a class of usage in which application are designed to run easy"
  },
  {
    "startTime": "00:38:01",
    "text": "class of usage in which applications are designed to run either on web transport or directly on top of quick And when they are running directly on top of quick, that application is identified by an ALPN And I am concerned that we should only use the same name if we mean exactly the same thing because there is a domain of overlap there there So, Christian, before you walk away, do you have a, like, sounds like you don't like application protocol. Is there something you do like? I don't mind application level protocols if the expectation is that we use exactly the same name if we run the application directly on top of quick That's a usage that we do see, okay? In MOQ, for example, there are the two versions supported, one of our web transport and one directly on top of Quake When running directly on top of quick, people are going to use an ALPN to identify their application So either we mean to exactly specify that or we use a different name that's that's Thanks Harold. Just a comment that just a comment that those who think that application is a well-defined place in the stack has now I really built one? I mean a browser and running a JavaScript page you will find several people with several different opinions on where the application is But in the interest, of API ergonomics, please pick a short name, toss a toss a coin, I don't care but pick one stay with it"
  },
  {
    "startTime": "00:40:01",
    "text": "All right, thanks sounds like we have drain the queue i don't think we have a specific one, but I think i've heard quite a few people talk about unifying on the same term. So I would say either reuse potentially the IETF one or we could open a issue on the IETF one to change that. Yanivar based on if you had feelings Again, bike shedding is the hardest part, so I don't know if we, I guess we can flip a coin in the room I don't know. Thank you to those good feedback Tell you what, it sounds like I've heard support for protocol and sub protocol and some folks opposed to application protocols So maybe I'm going to do a quick poll between protocol and sub protocol, or do we want to do, oh, we can do three now with the poll Let's do that. And then we're just going to to application protocol. So maybe I'm gonna do a quick poll between protocol and sub protocol or do we wanna do, oh, we can do three now with the polls, let's do that. And then we're just gonna see where that lands One second so just to be clear option one is sub protocols two is protocols, and three is application protocols and nobody wants to type level, because that's annoying I'm typing it in, because otherwise we're getting confused All right, so yes is protocol, no is such protocol, and no opinion is application protocol Don't know so if you don't vote, it doesn't get to no opinion"
  },
  {
    "startTime": "00:42:01",
    "text": "It doesn't say anything anything anything fault? No Let's do this again. Everyone has to vote again. Sorry, I don't know how that happened My mouse wasn't hovering over the button All right, I'm going to close in five seconds And of course, when I close it, it removes it from the screen. Great. For the minute Dustin, if you can write that we had 19 votes for Protocol 3 for I close it, it removes it from the screen. Great. For the minutes, Dustin, if you can write that we had 19 votes for protocol, three for sub-protocol, and nine for application protocol Yanivar, can I ask you to file an issue? on our tracker for the rename and we'll discuss it on the list to make sure we have consensus on that. But that sounds like an ob on our tracker for the rename and we'll discuss it on the list to make sure we have consensus on that? But that sounds like an ob... ideally we could just do that and move on great we'll do thank you otherwise that's my last slide all right well thank you very much um And Eric, you're next And if there's too much discussion on the issue about bike shed I will bring back my AI generated bike sheds Ah, right, I guess this isn't working the way I expected"
  },
  {
    "startTime": "00:44:01",
    "text": "expected Please do expected It's going to be easier than figuring out the next time Cool. All right Next slide and or I requested slide control. It's all good either way It's just sleeper and not doing it. It's easiest. That makes life easy All right, this is me with my hat as opposed to me with my hat Victor hat, but it's not that different, so life is good Let's talk about web transport of REH2. We did a thing in H3 where we defined key exporters because people wanted key exporters, and so we gave a label and filled in the other necessary fields to define one and landed some changes that include a key exporter for H3. I'm going to go out on a limb here and assume that nobody's like it was fine for H3 but there's some drastic problem with doing it in H2 But if there is, now is the time to speak up likefully quiet. Cool. So that's just a clarifying question Eric. Do you see any reason it should be different? No, not even slightly. Okay. I mean, the- label probably needs to be different. Like, it probably shouldn't have the three in it. We should probably just change the three to a two, but other than that that is the other challenge challenge So I believe what this means is that the W3 side of the world needs to make this failable But I don't think it changes anything for our land. We define it for where it works and different works, it works, and if it doesn't, when you ask for it, you don't get it because that's not a thing for you Yeah, say there's a question Thank you Jonathan Maddox, my comment off mic was that"
  },
  {
    "startTime": "00:46:02",
    "text": "H2 can be TLS1-2 unlike H-3, and I guess Key Explorer don't exist in TLS1-2, or do they? Let me jump in. I know the answer here So they exist in 1-2, they are just considered unsafe unless you have the extended master secret extension enabled so what we did for concealed authentication, which was an HTTP draft was to say that you cannot use it unless you have either HTTP 3 or later or TLS 1.3, sorry, or later or TLS 1.2 with the extended Master Secret enabled And our options here, I would see, are either to say that at the W3C layer, this API can fail or we can say that WebTrans transport over H2 requires these requirements, which to be fair, unless you have those, your TLS security has some pretty gaping holes So that wouldn't be a ridiculous requirement Speaking, in all this, I'm saying as individual contributor, by the way that's an interesting one so we would add another implicit requirement in addition to all the other extensions that you have to have and everything else we'd say unless you do this you shall not try web transport over h2 or any web transport really Is that a decision that we're willing to make? Uh, yeah rosomakho Scaler. One thing that is not necessarily specific to key export in HTTP 2, but key exporter available through API on the client side in general, I believe that's the first occurrence when key exporters can be available to JavaScript-based applications that run in web browsers. So previously, I don't believe there is any API for things that run inside web browsers"
  },
  {
    "startTime": "00:48:01",
    "text": "to get access to key exporter and things might break if there is a proxy in between that is performing TLS decryption, which is again perfectly fine but perhaps that needs to be noted in the spec for a application developers to be aware of this possibility Thank you, yeah Yaroslav, can I ask you to follow? of this possibility. Thank you, yeah. Yaroslav, can I ask you to file an issue on our GitHub for what you just said? That way we make sure we keep track of it. Thank you. That'd be really good All right, sounds like next slide Use some new settings. So we have an issue to bring all of the changes that we made to the settings the last time where on like round three or four or five of exactly what combinations of who sends what tells us things about web transport If we go to the next slide, I thought it might be useful if we talked a little bit about where we ended up rather than just do the same as H3, because like it's nice to do the same as H3, but in this case, it's not quite the same. So everything that is struck through, you'll notice things like settings H3 datagram That's an H3 thing. So what this means for H2 is that the server only is going to send MacSession. So at one point in our fun history meeting or three ago, we decided that both sides had to send this so that everybody was awesome opting into to using web transport and we had a discussion at the last IETF about how that required people to buffer and or delayed their ability to get started on doing things when you first open a web transfer session. You want to be able to then action open streams and open sessions and do all that kind of stuff. So we switched back In H3, you have settings web transport max sessions is now sent only by the server The proposal here is to make H2 do the same thing Except we don't have quite the same set of restrictions because in H3, things can show up out of order"
  },
  {
    "startTime": "00:50:02",
    "text": "whereas in H2 we have this very nice ordered pipe and it turns out of that makes life a lot easier. So we don't necessarily have to buffer web transport requests until the client setting show up because the client doesn't get to send those requests until it sent the settings and those requests therefore cannot arrive at the server before the settings because these things are ordered So that is the structure text there, and then we don't need to turn on settings H3 datagram to turn on HDP3D datagrams because that isn't a thing for H2 So we get completely rid of that. But what we're talking about, it, just so everybody has the whole picture in their mind, an H3 server does need to be able to handle connect requests coming into a establish a new web transport session before they know necessarily what whether web transport's even supported or if anybody's doing anything with it or anything like that. So you've got these requests coming in that you need to be able to basically sit on. Next slide please If we collapse those so we can look at the third part of this, we do still need to do settings enable connect protocol for everybody. So that's part of how you know that you're willing to speak that connect that comes in to open the web transport session and you're not just like, what the heck are these bytes, I need to buffer them. So you at least know what they are you might not talk the to of how you know that you're willing to speak that connect that comes in to open the web transport session and you're not just like, what the heck are these bytes, I need to buffer them. So you at least know what they are. You might not talk the token that is in the protocol field here So you may not speak the web transport session format or version that is trying to be established but you at least know what you're looking at and you know how to successfully discard it if it's not something that you want or you know how to reject it if it's not something that you can speak transport session format or version that is trying to be established, but you at least know what you're looking at it and you know how to successfully discard it if it's not something that you want or you know how to reject it if it's not something that you can So I think this makes sense for H2 It will end up looking exactly the same as H3 except minus the H3 datagram part which is not necessary for h2 But I thought it'd be nice if we talked a little bit about where we ended up after flip-flopping like three times All right, next slide This wall of text, you're more than welcome"
  },
  {
    "startTime": "00:52:01",
    "text": "to read. If we go to the next slide I'm going to make it a little easier to read. You can skip those parts I mean, we ryan cross it all out But fundamentally, what we're saying is because settings are sent immediately for H2 you don't have this weird ordering inversion You still are in a situation where you need to all agree about what version you're speaking for everything. So with Connect, that's really nice. We have the protocol field that you're giving a string in that has a token that represents that version of web transport and we've made some commitments about what future versions of web transport will doing, and that's why we're bothering to talk about it so much right now, is the choices we make here don't just affect this web transport. It affects how we evolve it in the future so that we don't accidentally paint ourselves into a corner Next slide, please I turned this in a bullet so you didn't actually have to read the whole thing. There's three main, things that we might change. We might change the syntax of what we put in the extended connect, or if there's the other additional headers that we include in that connect for initial settings and things for web transport. If we do that, we can't use web transport the upgrade token. We make it web transport to the upgrade token. At that point, when somebody comes to they can say hey i don't talk web transport to what the heck and then they can reject it. If we end up changing the frame format, the capsules in some way that isn't added or otherwise backwards compatible on its own, if we say, hey, this is a whole different thing, and this bite that was totally valid in the past is now completely valid and opposite in meaning, which we don't usually do. But just in case, we end up changing the Max Sessions code point that the server is going to send to the client, and because the client is the only one that gets to kick things off by initiating a web transport session, that's totally fine because the server is not able to unsolicited send you a version-specific thing as a client So the client waits for that setting to come in from the server, and if there is a different format of the thing that you would send to Kikstuff we use a different code point for that, and at that point,"
  },
  {
    "startTime": "00:54:01",
    "text": "we now have one code point for Webtransport 1, one code point for WebTransfer1 2, and we're fine. All of this is the same for H3 and HGSTF Finally, if we have the incompatible stream format, next slide, please that is only H3 specific and we have a unidirectional stream type and a signal value that we send on bidirectional streams. We will need to change both of those so that if those arrive out of order in H3, you can say huh, this isn't the version that I'm expecting. I don't know what to do with this and I should blow up and otherwise be sad But those are the three main parts that we've identified where things are might change, and we believe that all of those are fine. We can be consistent with them across H3 and H2 It's just some of them don't apply to H2 because life is easier if things don't show up in random orders. Could I get a general sense of a thumbs up, thumbs down if people are at all? like, yes, this seems reasonable or like, oh M.G, no, I'm totally lost I've got a thumbs up. I love that It's okay to say thumbs down, too, like there's no judgment. Next slide, please And you might have lost a few people, but at least it didn't say I think a thumbs up. I love that. It's okay to say thumbs down, too, like there's no judgment. Next slide, please. And you might have lost a few people, but at least it didn't sound ridiculous to anyone. I only saw thumbs up That works for me I appreciate it. That we call trust Yeah, to repeat it at the mic, Jonathan said he's lost, but he trusts. He thinks it's okay anyway. All right, next slide What's next? Oh, that was too many next slides Thank you. What's next? You might notice something about this slide. It's empty So that's the end of the issues Everything else either is, you know, has a PR, is done, doesn't need input, is ready to go So other than being snarky about the fact that there's nothing next and we're totally done, what I think is actually next is implementations We need to actually interoperate, we need to make sure that this works, we need to try it out, we need to give it a shot Exactly. So I know that we're"
  },
  {
    "startTime": "00:56:02",
    "text": "working on and close to an implementation we've interoperated in past versions and things, so we need to make sure that everything's super up to date with all these changes. Some of them are obviously in flight, so nobody's up to date on them yet I believe if Victor were here, he'd be giving an update about his side of things. Do you have any? I knew he was working on it. I don't know how far he got since last time. In the hallway before, uh, earlier this week, he mentioned that he was very close to being done Awesome. And thought that things were generally working So that gives us at least two fairly large implement that should be able to interoperate at some point soon Randall Yes, Mozilla is planning to implement web transport over H2. I think the time frame is like six months if We have not actually started implementing yet. Lovely. Thank you Awesome. Thank you for sharing. We are looking forward to interoperating. We'll do a bunch of tests and you know, to for sharing. We are looking forward to interoperating. We'll do a bunch of tests and define our fun little interrupt matrix and stuff like that, but that is a wonderful update. Thank you, Randall Yeah, we haven't done an interrupt matrix in a while So, yeah, or are you done? so in terms of what's next for this draft like I think on the key material exporters, let's have some discussion on the issue because there are two options there but I think we can easily pick one on the other ones we know what to write in the PRs so those will be reviewed by folks on GitHub and then brought to the list. But our plan, assume, this works, is that we land all the publish a new draft version, and that will then be the interrupt target. And so from a working group process perspective, then we will kind of put it on hold until we have multiple implementations Our goal is to generally anytime we interrupt with these, we either find bugs in the protocol"
  },
  {
    "startTime": "00:58:01",
    "text": "or at least find bits of the spec that are unclear and could be improved So there will be a round of changes that comes out of that And I think after that, assuming there weren't any too serious issues and they were easy to fix then we'll launch a week with a blast call. So we're in pretty good shape. I would say we probably would like I would aim to have the interrupt target before Dublin's so folks work on it before and then potentially try to interrupt at the hackathon that would be really great That would be fantastic, yeah all right then i would say next slide, and Eric, put your Victor hat on Hi, everybody. I'm Victor I'm going to put on my mask, my sideburns I'm going to put on my mask. Ha ha ha ha ha ha the slides. Let's talk about some cool slides So overview issue number 13 is about changing what data received means Please do go click that link on your copy of the slides. Essentially what we're talking about is saying in quick you have a state that says data received, but especially in H2 really we're saying all data that we're going to send has been in queued for sending. And that's about as far as we know like it might be sitting in a kernel buffer somewhere and once you've said goodbye to it it's out there. It might be retransmitted for you It might be sitting somewhere in the network We're not trying to make guarantees about whether we've sent everything or not. So the cutoff point is I'm going to declare that I have sent all of the data such that if you were to nuke the stream out from under me, I don't care And so the is basically making a generic version of that from Quick So as close as we can get to the concept of Quick's data receipts state, we're trying to define what is the commonality between quick and non-quick transports that"
  },
  {
    "startTime": "01:00:01",
    "text": "gets you the same concept of, hey, I'm done and I've sent you everything that I'm going to send, and if you were to take the stream away it's all good. But there has been some interesting discussion on that PR. So this is just a PSA to go click on that link and read the PR And if it confuses you, now is the time to speak up I'm seeing Mike jump in and out of the queue Yeah. I have questions, but I was going to go read the PR So in quick, the data received state is all acts have been received and that's not something that is necessarily exposed to the transport exposed to the application layer So is this a is this specific to H2 or is this for the H3 as well? Because this is for the overview doc So we're walking back from all the data that I've sent has been acknowledged and instead saying all of the data that I'm planning to send has been in queued and it is now somebody else responsibility to get it there. So for quick, that can still match to everything's been act, especially for unreliable data and then for h2 there is no unreliable data. And so we believe that once you've been queued it, barring the whole connection going away, there's no way to not have it get there. Right So in Quick, that state is data sent And then there's a subsequent transition to data received. Ooh, I like that so maybe if we're going to rather than claiming that we're making it analogous common version for data received, we should really just say, let's walk that back to data sent and call it a day. Perhaps So the difference is that if you were to send a reset stream in Quick, and the data received state you're not guaranteed that all data gets delivered Because it might still be sitting on the receipt"
  },
  {
    "startTime": "01:02:01",
    "text": "side and not yet delivered to the application Or it might get dropped and have to be retransmitted got it thank you okay yeah if you don't mind leaving that as a comment on the PR as well. Sure. Just make sure we don't lose it but yeah, thank you matt mathis, I sense a lot of danger there because of ambiguity between closing a connection as an abort and closing a connection as an EOF And particularly if you use the term data sent, excuse me, data reset connection as an EOF. And particularly if you use the term data sent, excuse me, data received, it will cause programmers to make incorrect assumptions about what that means and that's likely to cause bugs that will haunt people for a long time because I think something was done when it was wasn't Very much. Yeah, I think that the concrete proposal is instead of saying to that will haunt people for a long time because I think something was done when it wasn't. Very much, yeah. I think that the concrete proposal is instead of saying data received, it's to say that that all of the data has been in queued. Yes Correct cement. Committed. All data committed is the new name Is it committed? That's what the PR says. So if it's not committed, I got a comment for you Yeah, so if there is a way that a dereference on a null pointer can cause it not to be sent, it wasn't committed i think that would end up being a kernel panic for many people, but yes And the blue screen Too soon But yes, thank you. Cool All right, so we've got some comments to please also leave on the PR just to make sure that we get those back there and victory we'll also take a look at those. And next slide, please All right, we have a PR in review to update the name of sub protocol, but we just did some fun with names I think we've talked about that already, so let's keep going. Next slide, please Capabilities"
  },
  {
    "startTime": "01:04:01",
    "text": "So we're just going through all of the open Actually, just one second. Go back one slide, please Just jumping in as chair, I think one item that we did discuss is potentially switching the name to protocol. I'm making the assumption that in the overview in H2 and H2 we're going to use the same name. And someone disagreed with that please go and say something. But otherwise, we're going to strive for consistency. And i just wanted to give anyone a chance to object on that because I think we're on the same page. Okay, good. All right you can move on to the next one Thank you. Yes, please. Consistency All right, capabilities. So this is a page. OK, good. All right, can move on to the next one. Thank you, yes, please, consistency. All right, capabilities. So this is, as Victor says, it's mostly a tracking issue We've discovered a number of places where the W3C spec was referencing the H3 spec because something did not exist in the overview that needed to be common between different transports So we need to go through and do one more audit of hey, here is transport capability X that we're offering people and having the W3CC spec only referencing the overview doc has been a very nice force function to make sure that we aren't missing anything there So if anybody's aware of any of those, feel free to shout But otherwise, I think this is more of just a so everybody aware that we're doing that All right, off. This is a fun one. We could spend a long time talking about this. So we say that what? transport does not support HTTP based off. We are however running web transport over HTTP. HTTP offers authentic We are talking about resources and things like Thoughts feeling TLS client certs continue to be a thing Do we still think it's true that you shouldn't be able to do any kind of authentication? with HGP, whether those are cookies or HB? authentication schemes or TLS client certs? have a personal opinion, which is that like it's a little weird that we don't because like"
  },
  {
    "startTime": "01:06:01",
    "text": "this is just regular HTTP with extended connection it. But like, Yeah, I'm having trouble seeing why we would need that requirement or prohibition and maybe there's a good reason that we've forgotten about, but if we don't remember what we put this in, maybe some review of history would be good Jonathan says, not at the microphone and not in the queue, that it didn't always used to be a over HTTP and i think when we made it over hdp we were trying to be careful to not pull in too much extra stuff for folks who were nervous about having an HTTP stack in their midst but I think we would not necessarily require off, but I think it's a little weird to prohibit it If you're going to implement a stack that isn't actually HTTP, that's totally fine But if you're doing that, you almost certainly know who you're talking to And if you want to not support auth, then like, don't So I would personally, I agree with Mike let's use the issue to go through and do that digging into both why we added this And if we had some bright idea, then it doesn't sound like there's anybody in the room here who's particularly attached to it And so if we wanted to remove that prohibition and simply leave it open for future interpretation then we'd be in good shape. Anybody doing this in a browser context with the web API almost certainly has any HTTP stack because they're a browser and anybody who's not doing this in that space almost certainly knows who they're talking to and what they're doing it with Next slide, please Drain. So this is a fun one because I know Victor has deep held opinions on this with which I wholeheartedly disagree So I'm going to stand over here and speak as myself and I'm going to stand over here"
  },
  {
    "startTime": "01:08:01",
    "text": "and speak as Victor So we had a conversation at one point about whether or not we still needed drain web transport session and whether or not it was enforced some myself and I'm going to stand over here and speak as Victor. So we had a conversation at one point about whether or not we still needed drain web transport session and whether or not it was in force somewhere or somewhere else. And standing on this side as Victor he was saying, yes, we want to expose this because it's a signal to applications to let them know that they can start to drain things kind of similar to go away where you basically say hey, I'd like to gracefully close. Please start wrapping things up wind it all down, keep on going, and we ask the question, is this actually enforced? And Victor's response was, it's up to, it's a signal for the application to use to do whatever it wants with And so when we talk about how we define the application, did that mean the W3C? API or the actual JavaScript running in the browser or somebody else? So we have a Bernard. Yeah, I think there's two separable questions here. One is whether the protocol should include drain with transport session. And I think Alan has made the argument that one of the scenarios for this is actually more on the server side. So wouldn't, and then the second question is whether the W3C API needs it for a browser to make that available. I think those are two separate decisions. Got it So when you say, I see, well, Allen's in the queue, so we'll know him go when we get there, but yeah Alan frindell, draining, enthusiast Weird. Yeah, my, I less, I know less about the API side and whether you need this in the API, but I imagine cases similar to you if you're in HTTP and her David's wrap-up capsule presentation. I think there are cases where you have into me intermediaries, when the intermediaries go away they would like to send this someone in the middle wants to send this in one direction or the other to let, so that ultimately an end point finds out that like this thing is going away"
  },
  {
    "startTime": "01:10:01",
    "text": "and I need to wrap it up. And that's a useful signal And I even suggested in HTTP that maybe this and wrap up could become the same thing Maybe. I don't know. That could be nice I see. So what we're saying is that there is a use case for drain and we think it is a good signal that gives useful information. I think so. I don't know. If you don't think it's a useful signal Standing on this side, I agree. And I think Victor does too So Victor's not here. We can make him say whatever we want. Love it it Bernard Yeah, so so that's kind of the answer to the protocol question I guess, which is really what's in the IETF's purview I guess the other question is whether we think this is a useful thing for a browser to send I mean, I'll jump back in and say like, I Imagine that you're doing like a live stream over or some other kind of streaming thing from the server is streaming to you and the client's like going to go away and but I'd like to get to the end of something I don't know just like hey wrap it up since we have like wrap up language I'm going away soon but I want this to end cleanly and not an error because it can always just close that session right whatever session is gone but if it's like, well, I'm, I want to go away, but I got a little bit of time, let's get to the end of whatever we're doing. Like, I think that can be useful now I think the counterpoint Martin was here, you'd probably say, like, then the application can do that. And it's own. It doesn't need a generic facility I don't know, maybe there's an argument for that, but for the general, like, non like the intermediary can't do that at the end point. So having it a way to say, please don't do anything new, but like finish up what you're doing, go away style"
  },
  {
    "startTime": "01:12:01",
    "text": "I mean, we removed go away from quick. We don't have it in, but we have it in HTTP 3, I think was sort of the same set of logic. So I can sort of see why you want to remove that gotcha Gotcha all right I'm realizing we're speculating a little bit with Martin and Victor being virtually and imaginatively in the room but let's uh I can stand another third place all right let's have an idiot who's actually here virtually virtually so i think i might be understanding this wrong but go is met for like all the web transport sessions to be gracefully shut down, whereas I think drain was meant to be shutting down a single web transport session, but in the text, we don't, we just say like an end point may continue to use the session and we may open new streams and it's kind of up to the app application so i don't know especially on the client-initiated drain side it doesn't we're not really enforcing anything so I'm not really sure like what the signal is that the application is like what the browsers is really supposed to be doing with it at that point Yeah, so if I'm understanding you correctly in the W3C API, once a drain has gone, you can still open new streams So just to jump in as chair here, I because I realize we're a little confused because like the person who made the slide isn't in the room. I was reading through the W3C issue and the ATF issue on GitHub for both of these And neither of them actually talk about removing drain web transfer session They only talk about not using it in the client to server direction. I don't think is anyone actually suggesting to remove it in the server? to client direction? I mean, I did if you're not. Yeah, it's server to client is fine"
  },
  {
    "startTime": "01:14:01",
    "text": "Yeah, I don't even think necessarily the argument is against removing the protocol capability in client to server direction either because of the reasons Allen's description described Okay, so I just don't jumping in as chair. Am I here? correctly that there might be people could live with just, you know, keeping? it at the ATF layer that it could go in both directions and then at the W3C layer, they could say in the AP, there's no API to send it from client to server, or is that something that doesn't quite work for you, Eric? I don't, so I don't think the, the, that is an outcome that we could choose i think a better outcome from my perspective would be that we would have the W3C API actually enforce that you can't open new streams after you've said drain And the question became After you sent or received, sorry. Sent as a client So the client to server direction here Oh, okay. If I said, hey, drain, or potentially if one arrives from the server, we can talk about that. But fundamental the assertion was made that in the IETF spec, we wrote the application gets to choose what to do with this drain signal And there was a request for clarification of, okay, great So as we're defining the web API for this, like it seems somewhat logical that if I were to write a unit test and I say hey, I'd like to do a thing, and then a signal arrives that says, or I send a signal that says, no more new things and then I try to make new thing that should probably like fail. I'm realizing that christian amsss right in the sense that we didn't clearly define what the application was. Yes. So let's resume the request for clarification. Let's resume the cue. Yeah, yeah, yeah. So, uh, Mike So I was wanting to push a little more what Alan had said about combined this with the wrap-up capsule draft I understand that it is useful for web transport, and it is involved in the API"
  },
  {
    "startTime": "01:16:01",
    "text": "but it feels kind of silly to make two things with one of them being general and one of them being web transport specific when they mean the same thing Should we just merge this with the David's draft and let the web transport? like not deal with it in the web transport protocol? but W3C can trigger sending it Just jumping in as chair in terms of process, the wrap-up draft that I happen to be an author on in the isn't adopted by the HTTP working group and like the adding independency on an adopted draft would add significant process delays So speaking as chair, I think in the interest of finishing this it would make sense to have something in web transport, even if what we had up having over there ends up being similar. Capsule types are changed Could I suggest that we have an official request for the web transport chair? to check in with the HTTP chairs and say, hey like, we as a working group would be inter in having a dependency on this document. Are you planning on doing anything with it? And if their answer is no, then we say, great this is a subsequent capsule. If they say yes and we're you know, make it in time for your interrupt matrix to be filled in, then we say, let's talk about it Or for that matter, we rename rename this one and have the other draft say you can use it in non-web transport cases All right. I have a technical opinion as an individual contributor, but I'll join the queue later Let's see what everyone else thinks about. This is another option that we could do. So, Matt, you're next Yes, I have to admit, I'm not familiar enough for the protocols to get into the detail but I wanted to strongly encourage you to think about being functionally complete at the protocol layer, even if the recommendation"
  },
  {
    "startTime": "01:18:01",
    "text": "is at the API layer is not to choose to be complete One of the things that measurement lab supports are some measurement tools that use websockets. And one of the problems that we have with WebSockets is across all the pilots Lab supports are some measurement tools that use WebSockets. And one of the problems that we have with WebSockets is across all the platforms. There is a lot of ambiguity and non-uniformity about treatment and perhaps canceling of non-transference data. The problem is for performance measurement tool you want to shovel data as fast as you can and then when you've decided you're done measuring in order to send the report, you'd like to cancel the data you've already queued And you can't do that and the point is if the protocol isn't functionally complete, it can never be added to the API if the protocol is functionally complete, you can then later there's an open decision in the API to add the feature later. Yeah, thank you I like that. Lucas Yeah, just to echo what other people have said, I think we shouldn't arbitrarily restrict the direction of this drain message um but if you wanted to do it the API or the application, then sure, do it said, I think we shouldn't arbitrarily restrict the direction of this drain message. But if you wanted to do it the API or the application, then then sure do that if you like. On the topic of whether there should be drain web transport or wrap up capsule or something, I think even if you did it as a wrap-up capsule you still have to write sufficient text about what draining web transport means anyway so I think just do what we it as a wrap-up capsule, you still have to write sufficient text about what draining web transport means anyway. So I think just do what we're doing. And then in the future, if wrap up comes along, then we say an update document to this that says, oh, it could be either this capsule or a different capsule that's still the same draining mechanism but i'm a strong supporter of allowing intermediaries that act as clients to be able to do things that web browsers don't necessarily need to do Thanks. Yeah, thank you alan frindell, I wanted to come to what your"
  },
  {
    "startTime": "01:20:01",
    "text": "saying about whether what you can do after you sent or received one of these things. And I think when I wrote the original text for this and said, I and I had language in there that was similar to sort of what you'd happen in HTTP when you get a go away, which is like, yeah, you, after you say or receive this, you can't do streams. And then somebody made me take it out because the application is the one that knows what it means to wrap up. And maybe to wrap up, they have to open a new stream that says we're going away gracefully or something And so that's why it kind of like, had to, it just, I think, right. And I think that's where we end somebody not in the room really maybe you do that yeah but originally I had it the way. I mean, there's an intuitive reason why you, in the same thought that I did. It was like, yeah, okay, just don't make new streams. But we don't know what people are using this thing for so I don't think we can do that, which means the text is very wishy-washy but the signal can still be there and every application like, okay, I'm supposed to go away, and they get to define what that means If I'm standing on the Eric side of this line, I would be in a place where I'd say, okay, like if let's make the signal, let's define what the behavior is and if that behavior doesn't work for you because you need to open a new stream, then by all means send your own message that says timed it go away. But the intermediary can't do that So the intermediator going away needs to tell both sides or one side's like, this is going away, and then you might receive that and be like, okay, I need to do that. And the intermediate can't insert an application level message It can only insert, it can only insert a web transport level message that makes me sad on a number of levels I mean I'm been sad about this for years but we're we're getting there um I don't know coming back to whether this should should be wrap up, I don't know. It's also, I mean, I've also thought, for several years that there's a lot of overlap between what's going on in mask and what's going on in web transport and that we could have done more of the same and at the end of the day, that not what we did. And this is very small. And if there's two of them that do the same thing or I'm not going to be that sad Gotcha Hello"
  },
  {
    "startTime": "01:22:01",
    "text": "Um, Eric Orbati, Apple. Um, I just thought I threw in the opinion that if it was the case, that we would be considering any W3C API that I think it would absolutely make sense to have this state In the event that of course, like we have web transport to find over HGP, but since it's not necessarily constrained to that, if, you know, this can capability would be useful, at the API surface I imagine that it would probably make sense to have the state be expressed in a HTTP as well, kind of just for a consistent sake Thank you Individual, David David I'm going to say. It's a long week Yeah, so I think to echo some folks is have said, if we have two capsules that end up meeting similar things then like it's pretty easy in your code to parse both of them and call the same function um and so what one option that we also have is if we keep them separate for now and if the wrap-up caps magically ends up being what we need in web transport and like going through working group last call at an opportune time, then we can change or like say, oh good, this is here, we could switch to it but based on the discussions we had yesterday in HTTP that capsule could go in all sorts of ways We could be adding more, like some folks were suggesting that we focus on the use case of rate length and we might want to put stuff in there So it could go in a direction that wouldn't be the right fit for web transport So I would suggest to keep them separate like we have two to the six"
  },
  {
    "startTime": "01:24:01",
    "text": "capsule types available. And if we run out then we're really doing something wrong So anyway, and then now speaking as change again, looking through the issue Eric, what you were saying is that, if I understand you correctly, is that you would want the W3CAP are saying is that, if I understand you correctly, is that you would want the W3C API to enforce a requirement. If possible, although, Alan makes a very compelling point Always So I was going to say that's a W3C conversation that we can't really have here Yeah, the reason I mention that is because the W3C conversation hinged on the intent behind the text and the IETF spec. So the conversation for us to have here is what did we intend? Ah, or what should we intend, probably? Well, that is the hope yes. Okay All right, so we'll take some of this and go around a few more times. If there are changes that are made to wrap up in HGP because of good feedback, I would hate to have us invent this stupid version here, but if it's that it's solving a fundamental different use case, then I think that's fine. Well, I can go tell the author of the wrap-up castle a letter version here. But if it's that it's solving a fundamentally different use case, then I think that's fine. Well, I can go tell the author of the wrap-up castle to let us know if they change something. I was not super worried about the notification there, but, you know, it's late in the week All right. I take it to do for our for our last slide and one of our final issues we still have some, but we're getting close to done Let's talk about flow control we have an open poll request if we go to the next slide that brings the flow control capsules that are relevant from H2, you saw the struck through one I remember from this slide to make things a little cleaner in practice go click that link, read the poll request It's a lot of frame definitions. They're just a copy and paste from the H2 draft"
  },
  {
    "startTime": "01:26:01",
    "text": "It turns out we need two frames, and then they obviously have like their friends to say, hey, I'm blocked But we've spent many, many hours of working group time talking about them. It turns out when you actually write the text and when you actually write the implementation, what you're really talking about is handling those two things So just two little frames with an asterisk on the end So this is a, this is a call to say, hey, this topic that we've gone around on a couple of times has an open poll request. I imagine that we might consensus call it or otherwise give some time for people to go review it But please do click the link in the slide or just go to GitHub. It's one of the only open poll requests. And leave some comments, leave some feedback Turns out it's not that painful Yep, so again, to insist, please do review it I will have to decide if the depending on, because there's already been some feedback. If we get enough on the pull request, we might decide that that's enough and then we can merge it and move on. So if you care, make sure in the next week or so to review it or to at least say on there, hey, I'm going to review it. And then we'll be pestering you. Thank you Came out reasonably cleanly, so nothing too painful All right, next slide Well, look at that, three minutes after that Great time management over here Thank you very much Victor, and thank you for making the slides, and thank you very much Eric, for doing a solid Victor impersonation All right I think we did good, made some progress on quite a few issues. I think we're getting pretty close The main aspect we need right now is PRs to be written and PRs to be written"
  },
  {
    "startTime": "01:28:01",
    "text": "so that we can cut these implementations draft. What I mentioned for HTTP 2 earlier, applies equally to HTTP 3. And so I would love to see us get into a place where we have those ready before double so that we can have her for like an interim major at the hackathon Yeah, I think with that thing, everyone for coming. Bernard, any last word? Nope I think we've got a pretty clear path forward All right. Thanks, everyone, and see you on GitHub and the mailing list and otherwise in Dublin Dublin yourself"
  }
]
