[
  {
    "startTime": "00:00:10",
    "text": "Yeah. push to talk."
  },
  {
    "startTime": "00:06:09",
    "text": "we're going to start in one minute By the way, while we're trying to solve this problem, let me remind you that you need to use the queue. So it would be great if we can enforce this fairness using the queue for the questions. Thank you. Sorry. You're very quiet, Mary. Okay. Should I try again? Alright. my second attempt to remind you that we have a queue, and it will be great if you can use it for your questions. Most of the people do, but it would be great if everyone does that that we don't have unfairness. At least in terms of questions. Alright. So as I get started, Our first talk in this session will be evaluating the benefits, quantifying the effects of TCB Options, quick and CDNs on throughput. speaker is Simon Bower. Okay. Can you hear me well? Okay. Yes. device. so better this way? Yes. better now. Okay. Hi, everyone. Thanks for the kind introduction. My name is"
  },
  {
    "startTime": "00:08:00",
    "text": "Mombauer. I'm a research associate at the Technical University of Munich in particular at the chair for network architectures and services led by Professor Gillette Together with my colleagues, I conducted active Internet measurements in order to assess certain impacts by on performance, by client, and so configurations considering TCP option usage, click, click, click, click, click, click, click, click, click, click, click and also the hosting of a web service in the infrastructure of a content delivery network. Okay. Well, why are we interested in such kind of measurements I guess we can all agree that understanding and assessing performance of network connections as well as networks as a total is crucial The supplies from a provider perspective where we interested in providing optimal performance to our users as well as from a research perspective, to assess the effectiveness effectiveness of arising are already widely deployed measures. So the research question we pursue And Our paper Okay. So the research question we pursue in our paper is, which impact have TCP options, fake, and CDN hosting on the performance of Internet connections, in particular, on on throughput Next slide, please. Okay. Before we dive into our measurement approach and some measurement results, let me briefly summarize some related work regarding the measures we consider in our study, So we consider TCP window scaling, selective acknowledgments, and"
  },
  {
    "startTime": "00:10:00",
    "text": "this is a congestion notifications, which are around in 4th 4th kind of for quite a while, Yes. Next slide, please. Yeah. Next slide, please. Well, in the early 2000, researchers only found little deployment of some of the options. However, in 2 1013, Miria equivalent at all, found that selective acknowledgments as well as window scaling, is supported by nearly 90% of domains in the Alexa top 1, Milken list. 6 years later, it was reported that also ECN is now supported by the majority of Alexa Top 1,000,000 domains. Next slide, please. Regarding CDNs, I guess, we are well aware of their importance for today's Internet. in 2021, there was a study showing that giant hypergiant CDNs are maintaining more than 1000 of autonomous systems while the infrastructure is continuously growing Further, also, I guess, you're well aware. In 2021, click was finally specified. in 2022, it was already observed that click accounts for 8%, of of of Global Internet traffic. Next slide, please. So how to assess performance impacts by these measures. Well, in order to conduct active Internet measurements, we first have to identify measurement targets. So next slide, please. o Okay. So first of all, we have to identify measurement targets for our active Internet measurements. So we call this step crawling. During crawling, Recursively follow links from public from the in index page of public web servers. And we are looking for files satisfying a certain file size. In our study, we decided to rely on a minimum file size of 1 megabyte, megabyte, net"
  },
  {
    "startTime": "00:12:04",
    "text": "based on successful fully crawled domains and recording files, We then map the IP addresses to autonomous systems, which are then mapped to an organization maintaining the autonomous systems based on AS2orpplus And we do that to be able to check afterwards whether a domain is hosted in the infrastructure for CN. now have a list of files We that can be downloaded from public batch servers. And we do that numb. So we iterate over over our target files and conduct sequences of downloads while we consider different permutations of TCP option usage starting from a baseline that does not support any TCP option at all. then only a single option is supported. And, finally, all three considered DCP options r r, enabled, Afterwards, we conduct downloads with different quick implementations. So far, our pipeline includes quiche and they'll quit. For our study, we consider 3 different vantage points one physical vantage point located in our Campostatostatercenter in Munich and 2 virtual machines hosted by a digital ocean here in San Francisco. and one in Singapore. download traffic gets captured, and we then extract different packet features and calculate performance indicators However, we mainly focus on throughput in in our studies so far. So this was our measurement approach as we applied it for our 1st measurement series. However, we observed that with the baseline configuration, which always was the first download in the sequence per domain was significantly biased. due to on the edge caching, by different CDNs. So we repeated all measurements, including a warm up run before the baseline,"
  },
  {
    "startTime": "00:14:00",
    "text": "to ensure that files already cached and all downloads are conducted with the same conditions. So this presentation includes results conducted with such and warm up front. Okay. Regarding our targets, First, we generated a target set for TCP based downloads Accordingly, we crawled the top 100,000 entries of the Alexa top 1,000,000 list based on Success successfully crawl domains we choose 200 domains perconsidered hypergiant CDNs namely akamai, Amazon Cloudflare, Microsoft, And Google, And then we choose but 1000 domains that are maintained or hosted in other autonomous systems. What you see in the table are successfully conducted downloads from one measurement run conducted in July 2023. As we only find little shares, of quick support in such target set. We generated a second target set based on Google's Google Chrome's user experience data set here. We, again, consider the top one how 100,000 entries. scan such domain, this q scanner to identify domains that support quick And afterwards, we call them for files and also checked that all domains support all TCP options. However, this procedure resulted in a bit more than 500 domains that that we can refer to as the click target set in the following. So measurement results now presented rely on 3 measurement runs have one to make So first of all, we were interested how the usage of different TCP options impact performance. What you see here is the cumulative distribution function for the vantage point in Munich and in San Francisco. note that we find similar patterns"
  },
  {
    "startTime": "00:16:00",
    "text": "between the vantage point in San Francisco and Singapore. So I will only focus on San Francisco. here in this presentation. Well, the CDF shows the mean throughput observed, observed, And first, we run our warm up download and that what we can see is that the vantage point in San Francisco shows significantly long tail distribution compared to the vantage point in Munich, This can be traced back to some or domains hosted by akamai Cloudflare and partly Amazon that show a very, very little round trip time. So likely, Yeah. So point of presence of sub CDNs are quite near to our point for our vantage points. Next, we run our baseline configuration node that the warm up run and the baseline run do not support any TCP option, However, you see that the distribution of mean throughput indicates larger throughputs observed for the baseline configuration which indicates impact by edge caching. Next, we run downloads importing only this little active acknowledgments, respectively ECN. and the only final little improvement of mean throughput which can also so we explained as we only observe little retransmission rates during the baseline downloads. Next, we conduct downloads. with window scaling being enabled And as you see in the distribution, this leads to a significant right shift of mean throughputs in the CDF implying significantly increased throughput rates, Lastly, we conduct download supporting all options but only observe little increase compared to the configuration only supporting WindowsKeting. However, analyzing distributions of mean throughput does not answer the question how significant speed up by a certain option actually was. So what we did per measurement run is"
  },
  {
    "startTime": "00:18:00",
    "text": "comparing throughput of a configuration on the test to the baseline, throughput and accounted the runs accordingly to buckets of speedups. So first, we have a look on impact by selective acknowledgments and ECN. Well, as already observed, On the previous slide, such options only have little impact on the throughput of our measurements. And we also observe only small shares of samples, such as significant speed up. in contrast, Sorry? In contrast, enabling window scaling leads to a positive speed up for over 90% of our samples And we observe that nearly 40% of samples, supporting Windows scaling couples. throughput compared to the baseline, and that over 60% of samples show speed up larger than 50%. With the same approach, we surveyed the performance of quick based downloads in comparison to TCP. First, we compared downloads contacted conducted with keys to downloads conducted with Ioclave, and we found that keys outperforms vacant 70% of of our for our of our samples At the same time, we observe that over 45 percent of key downloads show a double throughput compared to electric. Next, we compare the TCP download supporting all options to downloads conducted with the audio quick. we observe that TCP with all options. results in better throughput in over 555 percent of our measurements. However, we observed that over 30% of samples indicate but are you quick? shows a double throughput compared to TCP with all options. We can now compare teach based downloads"
  },
  {
    "startTime": "00:20:03",
    "text": "to TCP all, we find a speed up for over 70% of samples and that throughput is doubled for over 40%. of of of of downloads conducted with quiche And this already brings me to the conclusion of this talk So what did we observe during our measurements? 1st of all TCP window scaling is crucial. We did not observe such significant impact by selective acknowledgments and ECN. However, as our vantage points are kind of close to the core of the Internet, these conditions might not be comparable to a user yet for user conditions. further, we observed significant differences between downloads conducted with quiche and audio click, We also observed such differences in test bed measurements further, quish modes mostly exceeds TCP with all options. However, as I mentioned before we conducted a first measurement series, and then we did not observe such an significant increase by quiche compared to TCP with all options. So this motivates to run further measurements with higher duration counts, two survey the reasoning of the observed differences. Further, we observe different impacts by VantagePoint location and edge caching which we further discussed in our paper. For future work, we consider to include further quick presentations to our pipeline as well as considering further transport layer parameters We want to integrate root cause analysis to identify throughput limitations of the downloads conducted. And as already mentioned, running long term measurements with higher iteration counts. Further note that we published the implemented pipeline as free and open source on GitHub, and there you also find the updated results presented here in the talk."
  },
  {
    "startTime": "00:22:00",
    "text": "So thank you very much for paying attention. Please join the queue for questions. Colin. Cullen first. Sorry. Hi. Sorry. I'm hiding hiding at the back, so it takes me a while to get here. Colin Perkins. So this is a really nice work. I I I I am not surprised that you are seeing the differences, although it's especially with the quick implementations, although it's it's interesting that you do. I I guess the question is why. are they implementing different variants, different sets of features, or is it just bugs in one or the other. So there are certain differences for for instance, I guess I click and keys rely on different max data values, which comparable key or not comparable, but handles the the the flow control. This might be a reason I'm not so deep in the in the comparison we did in the test bed. So what was surveyed by my colleagues there. is high speed measurements up to 10 gigabits Maybe we They could provide you more details for that question. Yeah. But as already mentioned, this was our first measurements. And in the future, we will consider further parameters to better understand the dynamics -- Yeah. -- of such patterns. Yeah. Yeah. Okay. Cool. But that I think that's that's really nice. It it would be really good to I think my my I guess my long term concern is that if we end up of doing these measurements, we end up with a race to see who can make the fastest version a quick"
  },
  {
    "startTime": "00:24:01",
    "text": "rather than who can make them with correct version of quick. And I don't know how to avoid that, but I think it's interesting that you're doing this measurement and we we should think carefully about what they mean. Yep. As I already mentioned, we are currently working on our root cause analysis approach for also quick traffic based on QLocks So maybe this adds additional values to these observations. Yep. Thanks for the feedback. Hi, Simon. Dave Blanca. way back on I think it was, like, maybe your slide 18. I see it as table 1 in the paper. see there's a huge swing between which of the Hypergiant CDNs you're on versus TCP and quick. sorry. Slide 18 Which slide? Sorry. I'm It was the one where you showed akamahi Amazon Cloud for Google Microsoft. But what we see between TCP and QUIC is tons of stuff moved from on the cloud layer if I'm reading it right. maybe It's figure 1 in your paper. It's the one that says, the the 3 different -- Okay. Yeah. Sorry. I I have to figure on the backup slides. Just let we have a look. Oh, you did show it. Anyway, it's it's table 1 in the paper. But I guess to make it a question, what Why should Why can we compare as you show TCP to quick when when you use the same 3 observation points, but the hosting platform is completely swung from akamai, Amazon, Google to Cloudflare. I'm sorry. So when we compare quick to TCP, the all measurements are only conducted based on the fixed targets Oh, okay. Thanks. So these are the same domain. Sorry. I did not make that clear. So all quick results, which were compared to TCP, are based on the same domains. Sorry, we have to drop the last question and let's thank our speaker again."
  },
  {
    "startTime": "00:26:06",
    "text": "next talk is mapping the Ukrainian refuge crisis using Internet measurements, speaker is tell miss Rocky. share the slides. Hi. Can you see me? Can you see me here? Yes. We can see I'll pass the control to you with Kale controller slides. Okay. Not yet. tell, are you able to control the slides? Yeah. I I don't see that I have control yet. But we can just try I can oh, Okay. Well, then if I stop sharing, you try sharing slides from your sites. website. Okay. Okay. Okay. They're good to go. Great. So thank you. My name is Thomas Rahin. and this paper has joined work with my colleague will see Lude. So Okay. So the conflict between Russia and Ukraine starts Sorry, Tal. We couldn't hear you. Could you be closer to the microphone or speak louder? Yeah. Sure. Can you hear me now? Still not"
  },
  {
    "startTime": "00:28:02",
    "text": "Still not No. I can try to switch microphone Yeah. Give me or not at all. Oh, little yeah. Could you say a few more again? How about now? just in microphones if you can keep talking. Can can you please keep talking? We're adjusting the volume here. Yeah. Maybe 2 some tests. now. No. Sorry. Can you hear me? No. Not yet. Can you hear me No. Still working on it. Still working on it. Can you hear me? Yeah. We've got another local talk, maybe switch the the order around. another Next one. Okay. So we can come back to this talk later and -- Okay. Let me tell please keep testing your Yeah. Yeah. device. Let me share the screen for the next one. So the next talk is not so low hang for security and privacy research opportunities for IETF Productos, speaker is Chris Wood."
  },
  {
    "startTime": "00:30:01",
    "text": "from clothier flair. Alright. Hello, everyone. This talk is I was supposed to be at the end with a bit a bit more interesting because now it's gonna be sandwiched between 2 very technical talks. This is not very technical. as you can probably surmise by the name. But my my hope here is to sort of pitched to the audience, people who are listening in, who people who may stumble upon this in the future. Some interesting problems that I think are used for this community and perhaps others outside the community to work on that would be benefit to the ITS. And the the motivation for the stock was I get asked quite a lot from a lot of people, like, you know, what are some interesting problems that, you know, I could work on as a, you know, a young PhD researcher or know, as a student who's trying to, like, learn more about this particular problem space or this particular area. And I figured rather than just repeat myself over and over again, I would put it down in words and this is an attempt to do so. this list of things may evolve over time. naturally, as we solve problems and as new things emerge, But that's that's the intent, and that's why I'm here. However, I'm actually I'm not actually going to talk about the opportunities to start, I want to take a step back and I guess, share a bit about how I think about what's a, like, an interesting research problem in the first place. So for me, when I, you know, choose to, I guess, embark on a particular problem or, you know, look at a new space. I I I'm I find myself motivated by trying to deliver value or add impact in some particular way. And in my experience, the way by which I we I I typically, like, end up delivering value is by Following an idea, through a series of steps,"
  },
  {
    "startTime": "00:32:02",
    "text": "to, like, actually getting it out there in the world, shipping software effectively. And in my experience, there's basically 3 I guess, high level pieces or components that, like, fold into the process of, like, shipping software. There's, like, the core science that, like, underlies all the stuff that we we do here in the IETF and the community at large. There's the specifications that take that science in, like, really describe how to implement it, how to actually write code to do this particular thing. And then there's a software that goes about and implements that particular specification and and does the thing. So not surprising, I guess, to maybe most people here if they're engineers, if you're a researcher, Maybe this is maybe this is new. I don't know. as a specific example, consider all of the work that went into what is now TLS 1.3. and quick. there is if you, like, open up Google Scholar and you search for, like, TLS 1.3 or quick or whatever your favorite search term is for, you know, transport security or encrypted. key exchange or whatever, gonna get a huge list of papers. I've listed some of the impactful ones here that really know, influence the current state of TLS 1.3 and quick as it's specified and implemented and shipped today, But there's a effectively a huge body of work. And what the IETF did was, like, build upon that work, by, like, writing these RFCs in a very meticulous way to, like, say, this is what Quick is going to do. This is how quick send, but it's from, you know, client server, and this is how the transport protocol works, and this is how it uses TLS and all this stuff to encrypt those bytes. and we even have a nice queue logo for it, which is really great. And then a huge army of people in the quick working group went ahead end, implemented a ton of different interoperable implementations. This is just a slight of the interoperable implementations that exist today,"
  },
  {
    "startTime": "00:34:04",
    "text": "And now, basically, Quick is, like, a household name. It's gonna be in textbook, you know, in in in in if it's not already, students are gonna learn about it, you know, basically, the future of the Internet is going to be built upon this new transport protocol, which is pretty fantastic. I think, And know, we're seeing in time, Or over time, we're seeing, like, the adoption of this particular protocol go up. there is perhaps not the most obvious trend, but, like, the green line is kinda going and to the right, and and it was further down to the left or earlier on. So I would expect as more browsers and as more operating systems enable h t 3 and quick. see the screen line go up. We'll see old versions of HTTP go down. In particular, HP 2, that would be fantastic. But at the end of the day, like, those you know, all that work that went into the core science and all that work in this community that went into the and all the the the time and passion board and the implementations led to, like, this value, which I considered to be pretty great. quick is not unique. There's a ton of different communities in the in the IETF that have, like, followed the same exact pattern for delivering value and and and impacting, you know, positively the Internet TLS, which predated quick. obviously did this. And I think in in some ways sort of was the the first working group to really kind of I guess, trailblaze like this this close working relationship between the academic and scientific community, and, like, the the actual specification that, like, shift as of that particular working group with TLS 1.3. We also have Acme, you know, sort the the protocol that is behind, let's encrypt, and all the, you know, the basically, we which got us HTTPS everywhere, which is fantastic. MLS, which is just a recently minted RFC for doing, you know, group"
  },
  {
    "startTime": "00:36:00",
    "text": "like, basically end to end encryption between groups and mask. which that's our new logo, which is pretty fantastic. Mask is like this you can think of it like Tor for built on quick. But the I guess the the takeaway is that this this this is like a, I guess, a working model for a lot of the different in the and it it seems to be something that is quite effective. And specifications are really at the heart of everything we do here at the IETF. They are the thing that allows us to take what is actually, you know, in science and transfer it to practice. Like, we want to, like, take really cool things that really smart people think about We wanna be able to, like, ship them and use them in specifications or the way we get there. And we spent a lot of time in the IETF trying to write, like, clear descriptions of specifications. so that it's easy to implement correctly and well that we can, like, reasonably implement that we can verify, analyze, and stuff like that. But I think another hallmark is that these specifications encourage, like, open, like, you know, collaboration and and and they really build communities around, like, particular problem domains or or or or what have you in And that's that's important to this process moving something from theory to practice. software at the IETF. pretty important given that our motto is rough consensus and running code. We try. the best of our abilities to actually implement the things that we specify and get them out there in the world because that's that's why we're here. We're trying to, like, deliver value by, like, you know, actually running this this and then users' devices or, you know, somehow maybe not directly on their devices, but, like, actually running in a way that affects improves someone's life. and, you know, as a result of, like, shipping this software, we, like, learn new things. Maybe we learn that, oh, doing this"
  },
  {
    "startTime": "00:38:02",
    "text": "particular thing in this way, it was hard. Maybe we need a new protocol or a new an extension to solve this this this this challenging problem that emerged from deployment. and that, like, feeds back into, you know, the process of, like, specifying something new or maybe, like, you know, uncovers, like, a new research problem that people haven't thought about before. So this is sort of, like, feedback loop between shipping software specifying things in in that actually, like, coming up with multiple ideas and stuff. And then science, while not technically, like, the the, you know, the core component of the IETF, I would argue it's at the, you know, at the at the bedrock of everything that we build upon. without all the, like, the the work that's poured into the science scientific community, wouldn't be able to, like, specify things. We wouldn't be able to ship cool things that solve interesting problems. and And, yeah, I mean, all all the the work that's poured into science, like, it, like, has an effect on other parts of the process. So, like, know, if someone comes up with a new idea that perhaps leads to a new specification that perhaps leads to an implementation that perhaps leads to another new idea that someone comes up with that perhaps leads with different specification and so on. And so there's this, like, This feedback loop is very important for solving problems. And I I guess I'm what I'm trying to say, is that if you're thinking about, you know, I guess, an interesting or or trying to, like, know, engage in the IETF from a, like, a research capacity. Each of these components is perfectly fine. to engage in from a research capacity. You could be, you know, just a computer scientists doing science in the IETF. and just spend all your time in that particular component, that would be great. You could also be, like, a research scientist who, like, really spends their time, you know, cracking or, like, writing like, super formally verified software that is, like, proven correct and you're, like,"
  },
  {
    "startTime": "00:40:03",
    "text": "You're looking at implementation protocols that we specify. That would also be super valuable the IETF it could be someone who's, like, or trying to, like, improve the ways from which we specify protocols. All of these are useful. There is no, like, I guess, preferred or ideal, like, way in which researchers engage So That's my, like, meta meta, I guess, part of the talk. And now I'm gonna, like, drill a bit deep down into, I guess, little bit deeper down into something that is a particularly interesting research opportunity in this and that is multiparty computation. So for those of you that don't know, multi party computation is it it I guess it can be really reduced to basically a way of, like, computing functions over private inputs. So you wanna compute some arbitrary function. You don't wanna learn the inputs following the output. that's what MPC is kinda good for. And you can imagine that there's a lot of different where that might be useful. The IETF is working on one of those particular applications that is privacy preserving measurement. There's also, like, work above the IETF where this particular technique is useful. like using MPC as a way of measuring and click attribution. And Recently, the IETF has embarked upon trying to standardize this type of technology. in the PPM working group, there is the distributed aggregation protocol, which is a specialized form of MPC. built upon some really cool cryptography that's being specified in the CFRG called the verifiable distributed aggregation. function, not protocol. and and This work is really exciting. It's kind of the first the ITS first attempt to, you know, do something concrete in the space. It's a significant increase in scope compared to what the IETF typically does, which is client server or 2 party stuff. And so this I would argue is perhaps, like, the hottest research area for, you know,"
  },
  {
    "startTime": "00:42:02",
    "text": "people, especially security and privacy people to engage upon, there are some really hard problems that we need to solve and and they cover all three of the different components. For example, in the, I guess, the science realm, for this particular space, there's a problem called like, private heavy hitters. How do you compute and add or solve the heavy hitter problem, which is, like, effectively learning what are the most common elements in some input set amongst a set of clients. in a way without in a way that such that you don't learn the individual inputs. there are some proposals for solving this problem. but they're not, let's say, they're not as performant as we might like. or they're they're more they're more expensive to run than other types of MPC, specialized MPC protocols. So it is, in in my opinion, still sort of an open problem to, like, solve this problem in a much more performant way. and and, I guess and also in a way that's a bit more like, ergonomic for applications, like some of the existing solutions right now are sort of rigid in how they can be used in terms of like parameter space and, like, technical details that I will lose people on talking about, but are not relevant. But There's a this is this is a very important problem in practice and this is something that, like, interested people should definitely dig into. And there's also, I guess, sort of an orthogonal problem, which is how do we like like like take all the work that was imported to differential privacy and compose it with this MPC stuff that we're standardizing in a way that can be implemented safely and correctly and that can be used a way that's meaningful for our end users. because, I guess, like, most emerging privacy enhancing technology it's not like a binary thing where you you know, you, like, turn on privacy, your turn off privacy. There's, like, there's a there's a knob you tune, and"
  },
  {
    "startTime": "00:44:02",
    "text": "and differential privacy in particular that knob turns out to be very important with respect to how much privacy you get and what the impact on the application is. So Anyways, there's a lot of space or a lot of work that potentially be done here on the composition of these 2 particular domains. On the specification side, as I alluded to earlier, there are these 2 emerging applications in the PPM working group for addressing the PPM problem. That is the distributed aggregation protocol and the the underlying VDAS specification. there's been a ton of work that's been poured into the underlying crypto protocol to sort of, you know, give us a, like, a a reusable abstraction that we know has like, very specific properties. that we can sort of plug things into and and it things should just work. But up above at the dApp layer, we're not really sure that the thing is correct as specified. which if you're, like, looking at deploying DAP in practice, may be important to your particular use case. And so we need people to sort of, like, really dig into that specification and and ask, you know, is this thing specified? Is it correct? Does it drive the underlying Vdev abstraction in a way that is required for VDA have to be secure. And for the, I guess, mechanically or are they the symbolic, you know, proof of people in the room who, like, Tamarin and ProveraF and stuff, I would I would challenge them as you think about maybe maybe we could, model dApp in in one of these particular modeling languages to to check whether or not it does indeed satisfies our notion of correctness or our notion of security or privacy. So that's work that could be done in the specifications. And then on the software side, there's definitely a lot to do. There are, you know, 2 at least to my knowledge, implementations of this open"
  },
  {
    "startTime": "00:46:00",
    "text": "specification, one that's developed by RsoG, one that's developed by Cloudflare. would be fantastic if you could take these implementations or components of them or subsets of them, and produce formally verified implementations, things that we know are correct, that are safe to run-in production and that don't regress any of the performance properties It would also be fantastic if people who are thinking about, like, the differential privacy composition how to safely use these things in practice. could provide guidance or, like, safe defaults or, like, like, API models that are just easy to use out of the box, these implementations such that users can't shoot themselves in the foot. and and So so There there are probably lots of other opportunities to be done in the MPC space that's just a few. There are a ton of, like, other more qualified people in this room things to work on. And I would be happy to dispatch you to these people if you're interested. But outside of the MPC space, there's other security and privacy things that are also useful to the IETF. this topic of anonymous credentials has come up time and time again. the IETF is working on a technology called privacy pass. is a I wouldn't call it an anonymous credential because it's very, very simple compared to what it does, but there have been, like, know, suggestions that maybe privacy pass should be extended in a particular way, or maybe you should do some other stuff. that it doesn't currently do. And sort of like getting closer and closer to, like, actually you know, specifying and implementing and shipping and then on this credential. And as part of, you know, doing that, there's some interesting research problems to address. for example, how do you how do you build these things that are post quantum secure. We currently don't really know how to do that. in contrast to, like, all the key exchange protocols that we we we have shipped today between clients and servers. post quantum security for privacy pass and related things is kind of an open question still."
  },
  {
    "startTime": "00:48:04",
    "text": "there is, again, the formal verification question of this of these implementations and then And really thinking about how you could take these know, these constructions that exist in in academia and actually specify them in a way that makes them more amenable to deployment. because there's there's huge literature a space of literature on anonymous credentials. but very few have actually made them into practice, and there's probably a reason to why this gap exists. bridging that gap. is definitely work that people in this community could do. And then I guess a bit more lower levels, you may have heard of the term as 0 knowledge proof. or Xeno Edge Group system. This is I I don't know if it's like the came out of the blockchains, like, ice or whatever, but This is a, I guess, a generally useful tool that's, you know, it does have some like, it actually is being used today for a number of IITF things. So, for example, does he pass in some sense does use 0 knowledge proofs. The PPM work does Zoos 0 knowledge proofs. But there's other there's other proposals for using these things in a way that or or There have been other, like, suggestions that, you know, Gee whiz. If we had a a way to prove this particular thing in 0 knowledge. We could solve this problem But we don't yet have, like, a, you know, a reusable extraction for such a thing that we can like, point to it in terms of a specification implementation that we could ship today and, you know, working on that and trying to provide that security could be really useful. And, like, as a concrete example, you could, in theory, build a post quantum version of privacy pass using 0 knowledge and there's been some research to do that, but it's hard to it's hard to specify. It's hard to deploy because these, like, reusable attractions don't exist. So, anyways, If I can leave you with nothing else, I would just encourage people who are thinking about research to not constrain themselves in any particular way"
  },
  {
    "startTime": "00:50:03",
    "text": "the the research can be done all over the place. It can be done, like, you know, if you're, like, person who just cares about science and writing papers and publishing papers, cool. Do science, write papers, publish papers. If you care about them, improving specifications, help us do that. If you care about software, that's also helpful as well. And if you have questions about any of the topics that are discussed, that I mentioned here that you may have already forgotten. talk to me, I will happily dispatch you to the other people who know a lot more about these things than I do because that's, like, the the goal here is to effectively get more people thinking about these problems. So With that, I will will take any questions or comments you have. if you have anything. Thanks. In the interest of time, let's just take one quick question. And there's no one in the queue, so please feel free to go to the mic, Hi, Colin Perkins. Not a question, but thank you for raising this. But the number of times I have heard people say some variants of you know, why why should I try to come to IETF if I do networking research? TCP is the same as it's always been. there's so many things that that need research input. It's good to see them starting to be so widely enumerated. So thank you. For sure. Yep. Thanks, Chris. I Let's go back to tell miss Rocky from Technion and let's see if it works this time. Yeah. Can you hear me? Can you hear me? Yes. Much better."
  },
  {
    "startTime": "00:52:00",
    "text": "Okay. Great. to try to share the slight It's not showing up. Yeah. Could you try sharing again? Yes. Alright. Yeah. Good to go. Okay. So my name is Thomas Rahi, and this paper is joint work with Yossi Luz. And in February 2022, the conflict between Russia and Ukraine started. It all also started a very large wave of refugees and If we look at statistics published by the UN. The UNHCR is the United Nations refugee Agency we can see here that shortly after the war started they started publishing on a daily basis information about refugees crossing the border to the neighboring countries around Ukraine, So we can see here that on the first few weeks of the war, millions of people crossed the border, And, actually, in addition to people crossing the border over 25 of the population were internally displaced within Ukraine, So One question that comes to mind is why is it important to know where the refugees are staying, where they're crossing the border to, And The answer is to be able to help them And that's important for large organizations like the UN, like the Red Cross,"
  },
  {
    "startTime": "00:54:00",
    "text": "And one of the challenges here is that it's not necessarily easy to know accurately how many people are staying in each country, especially within the EU, where border crossing is not necessarily monitored. the borders between EU Countries, are not necessarily monitored in terms of people crossing the border. So some of the numbers of the I'm Numbers of refugees in each country are not necessarily accurate, accurate So the goal of this work was basically to try to map The location Ukraine refugees, Based on Internet measurements, So what we did was use publicly available data, from various sources data about Internet usage, Internet performance. In order to map, where refugees are staying. So Basically, some of the data published by the UN, we can see here the rate of refugees per day who crossed the border to some of the neighboring countries around the Ukraine, And we can see that especially on the 1st 3 weeks of the war, there was a very large wave of refugees. If we take this graph And, basically, we see the same graph on the left here And next to it in the middle, we see the rates of Google Maps traffic the same period of time, And we can see that there is a very high correlation between these two graphs, and Actually, that's not surprising because We know that People had to move around a lot. They had to travel. and, obviously, they needed to use navigation apps. So that's One thing we can see which is correlated to the refugegwave"
  },
  {
    "startTime": "00:56:00",
    "text": "What we can see on the right hand side is a figure showing the ratio between between mobile device traffic volume and desktop device. Traffic volume. So, basically, we can get general feeling of the volume of mobile device usage, and we can see that during this Short period of time, there was a significant increase in mobile device usage, And, again, not very surprising given the fact that people were had to move around a lot. another aspect we looked at was was mobile device vendor, usage in Ukraine. So on the left side, we see different mobile device vendors, Over the last decade, And for example, we can see that Nokia usage basically declined over this decade But specifically, if we zoom in on 2022, and that's what we see on the right side, We can see that the usage of Nokia devices went up from 1% to around 13% in that short period of time at the beginning of into the war, and This can be explained by basically the fact that people needed more mobile device is the took out their old unused Nokia devices and started using them again. So that's another trend that we can see which is highly correlated with the refugee wave Now One thing that's going back to the previous slide, one thing to notice here in these graphs is that the most popular mobile device vendor in Ukraine, is is Xiaomi. So If we look at the mobile device usage in Poland, that's what we see on the left here. We see that in the 1st few weeks of the war, there was a very steep increase."
  },
  {
    "startTime": "00:58:01",
    "text": "in the usage of Xiaomi phones. And, again, It's I very tightly coupled to the exact same time where we saw the very large number of refugees crossing the border. And on the right side, If we look again at the mobile to desktop ratio, And we can see that in Poland, during the same period of time. the mobile device usage basically increase by a factor of 2 or something like that. So mobile devices were used a lot in Poland. So Why why are we talking about Poland, basically, more refugees cross the border to Poland than any other country around Ukraine. So And and this is significantly more people than any other. neighboring countries. So Basically, the we see here is is much more severe and much more distinct in Poland. So so if we look at Other aspects that we saw in Poland, we see on the right side here basically the traffic volume in general increased during the 1st few weeks of the conflict. And also on the left side, we already saw this graph a minute ago. mobile device usage, we can see there was a very steep increase. and then it became basically constant. from that period on. And this points to the fact that basically people continue to across the border to Poland, but from there, they continued to other countries, And this again goes back to the fact that People continue to other countries, and this is not necessarily monitored in any way. So basically, what we saw here is the traces of the refugee crisis in Poland as we can see them in Internet measurements one question that comes to mind here is"
  },
  {
    "startTime": "01:00:03",
    "text": "whether the same traces could be seen in other countries as well. and when we started looking at the same metrics in other countries What we saw was that these metrics were more affected by different factors, than by the refugee crisis. So for example, if we look at the mobile to desktop ratio, in Europe in general, we can see the the line here is the middle of January, and the middle of January, most of the COVID restrictions started being removed, in Europe, And, basically, people started traveling being more mobile, and we can see that there was a large increase in the mobile device usage. and This was more dominant than any of the effects we saw the refugee crisis. throughout Europe. So 1 of the conclusions we got from this is that we had to look at other metrics in order to try to analyze the refugee crisis. So More details about that in a couple of minutes. But Just for a second, let's zoom out and try to understand what we're doing here. Because basically on the left side, we see how the UN data, which is published basically the the number of refugees in in each country, and it's based on data collected from governments, which like we said is Not necessarily very accurate. Also data collected from humanitarian organizations. But what we're trying to do and this is the right side, the the pink side, We're trying to have an evaluation of the number of people in each country, which can be kind of way to complement the picture, And one of the important things for us was to base this analysis on publicly available data so that we don't compromise the privacy any privacy issues. and that was something that was was was was"
  },
  {
    "startTime": "01:02:01",
    "text": "very important for us. So this is what the current work work work tries to do. So generally speaking, like we said, the February 22, that refugee crisis started. We published some of our preliminary findings in May 22 on archive, And as far as we know at that time, this was the only At the time, it was the only estimate of the number of refugees in each country. other than the countries surrounding Ukraine, A few weeks after that, the UN started publishing More results about generally the number of refugees in each country in Europe. specifically in Europe, and and again, as far as we know, At the present time, There's no other source of data other than our work about the number of refugees staying in other countries beyond Europe. Okay. So The data we used in our analysis was basically from 3 sources of data. And Essentially, what we try to do is u use website analytics specifically The website visit locations, of each website so we can see the the bottom an example. We can see the 5 of the most popular Ukrainian websites And, for example, on the right side, we see google.com.ua, which is the Ukrainian version. of the Google search engine. So we can see that when this was captured, About 5% of the visits to this site were from Germany, about 2% were from France, and so on. This is data which is published for example, by Cloudflare. So What we did was"
  },
  {
    "startTime": "01:04:02",
    "text": "first of all, we extracted the 15 most popular Ukrainian websites. And for each of these websites, we extracted the percentage of accesses from each country. And we did a maximum likelihood estimation based on their data, of the number of Ukrainian people staying in each country. and each website had a different weight based on its popularity. what we can see at the bottom here are are the estimation results showing the percentage of people in each country. So, for example, I can see for Germany, About 2% of the total Ukrainian population, was was was was in Germany when this was captured, which was in July 22. and about 2% of the population was in Poland. So that's the estimation results, but the the Main problem here is that it estimates the presence of Ukrainians in each country but it doesn't doesn't take into account the fact that even before the war, Millions of Ukrainian people were staying in in countries around the world. So That's that's a major factor. So in order to consider that, What we did was to use historical data and we use historical data from a Wiki media. about the number of accesses from each country to the Ukrainian version of Wikipedia. And based on this historical data, we that. collected the maximum likelihood estimator. which is basically a combination of data about websites visits. plus the historical data."
  },
  {
    "startTime": "01:06:00",
    "text": "and we also had a second estimator which is based only on data from Wikipedia media. So 2 estimators. And at the bottom here, we can see basically the estimated number of FUGGs in each country. The blue represents the data published by the UN. And the orange and gray, are the two estimators we used here. So One major question is how do we know whether our estimators are accurate or not. We we said that UN data is not very accurate to begin with. So In order to try to assess the accuracy of our estimators, Our ground truth, analysis was based on data we isolated from the UN data, and so we specifically focused on countries which are either not part of the EU, Or they're not accessible from Ukraine by ground transfer transportation, which means that in either of these cases, the border crossing would be monitored. we expect the numbers to be more accurate. So What we can see here are the the numbers for these specific countries And we can see that in the ground tools, analysis, the ML estimator we had had a mean percentage error of 11.8 percent. which is actually lower than we expected considering the simplicity of this method, and, again, So it's important to emphasize that this method is not meant to replace the data published by the UN, but only to the complementary PCR. So to conclude, what we basically did here was first of all, try to analyze how the refugee crisis affects"
  },
  {
    "startTime": "01:08:02",
    "text": "Internet measurements, basically Internet performance, Internet usage, but we also tried to use Internet measurements to to try to map the Ukrainian refugees and to be able to potentially use that data to Help ends to protect these refugees So hopefully, this method can be something that is helpful in this refugee crisis as well as potentially in the future. Thank you. Any questions? Yeah. John Levine. I'm just wondering how you identifying the mobile users as physically where they work. And I'm just thinking, I know when I'm when I roam with my mobile phone, It shows up as being in the country where the same is firm rather than the country where I'm physically located like, if I have a British SIM and I'm in Germany, looks like I'm in I'm it looks like I'm in the UK. So I'm wondering, you know, are are you assuming that people bought local SIEMs? because cheaper or do you have some other way of locating them? Yeah. So what you're saying is one of the aspects that would probably affect the accuracy here is whether the location of The visits is accurate. Basically, it's based on geolocation and does not necessarily reflect what we were expecting. And I agree that potential factor which may affect the accuracy of this estimation And, basically, there's a whole"
  },
  {
    "startTime": "01:10:00",
    "text": "list of factors which may reduce the accuracy, and this was described in the paper, and, yes, I agree that's that's an issue. Okay. Hi. This is Eve Schuler. And I have 3 grandparents who are ostensibly our from that region of the world. So it's very interesting for me to see this information. Thank you. And I I'm wondering you listed the percentages or estimations of percentages of refugees flinked different neighboring countries, and you had, of course, a very large arrow for Poland. I wonder if you were also able to reveal to us those well, you actually didn't give it as percentages of the population. You gave it in terms of hard numbers. And so I'm curious for about the percentages for those countries because what the impact ostensibly is that the it would be harder for countries if the percentage of their population is higher for record refugees. And so that's one thing I'd love to see those numbers. It would be an interesting reveal, and I don't know if it anyway, it would be great to see that number. And then how did you take this data. Did you is there sort of do you have some partners who are trying to affect change from a policy standpoint in terms of where the UN or other agencies, Red Cross, things that are Global International, where their aid is going as a consequence of your number. So, are you somehow linked into that part of the process, and that was really why I was asking about what those numbers represent in terms of percentages. Yeah. So regarding the first question and we basically showed the numbers as hard numbers, but the maximum likelihood estimation, actually computes the percentage of people"
  },
  {
    "startTime": "01:12:02",
    "text": "out of the the total Ukrainian population, So since we know the total Ukrainian population, we can know the we can compute the hard number in each country. Oh, no. I was saying what does that number represent in terms of the destination country terms of the populations there so that you might push more aid to those countries where you know, if it's half of the you've just grown the population, but, you know, doubled it or something like that. more aid in that direction. Right. Yes. I want So that was the percentage that could be interesting to see. Right. Yeah. And regarding the second question, we used publicly available data, so we were not connected to any of these organizations or companies that publish the data. but we are open to any cooperation with, you know, organizations who would like our help to to try to get more accurate estimates of of this kind of analysis, and we would be happy to help with that. Thank you. We'll take more one more question from later. Have you taken into account, or can you measure the fact that big fraction of the refugees have Russian, actually, is their primary language Ukraine has 2 languages, And then regions most affected by the war. in fact, might be predominantly Russian. Right. Yeah. That's again, that's a good a good point, and it's discussed in the paper, and there like you said, there's a large number of rush speakers in Ukraine and Actually, a lot of the Ukrainian websites have two languages so you can pick whether you want to access in Ukrainian or in Russian. And obviously, we know that"
  },
  {
    "startTime": "01:14:01",
    "text": "since some of these sites are Russian speaking, maybe people from Russia may be using them as well. So These are factors that we took into consideration. We tried to eliminate some of these websites which were both languages and And also, obviously, there's But larger difficulty to try to assess the a number of refugees or Russian speakers because the they don't really fall into this computation so that again, is is an issue. which affects the accuracy of these estimates and it's discussed in the paper. you so much, Tal. Thank you. We're Thank thing. We're changing gears a little bit, and we're moving to our panel discussion. So panelist, please join me. Alright. So it's my pleasure welcome you to this pivotal panel discussion."
  },
  {
    "startTime": "01:16:01",
    "text": "today, we are we have the unique opportunity to take a peek into the into the future of the Internet. Completely, we have this unique opportunity to have 3 world class Internet experts with us who will be sharing their insights their predictions, there are hopes on how the Internet would be in the 20 years. So I will start with your oath, who is a seasonal veteran in the IPF community. celebrating his 30th year at the IVF, So joint being celebrating every year is in the idea. other than a veteran, the ideas, Yorg is also professor the TUM Munich. bringing his re his X-rayPs in network architecture, transport protocols, and mobile network systems. He got his speeds at the uberlin. Next, let me reintroduce, trace wood, long time no see who is a research lead at Cloud Layer research and holds the PhD from UC Irvine. Before before Cloudflare, Kris worked on transport security. He also worked in privacy, in computer Engineering at Apple. as well as Xerox p PARC. And last but certainly not least, Alexia Tsang, who is who received herpes from MIT and began her career also at Xerox Palo Alto Research Center, Now she is, of course, part of the UCLA computer science department, and she has been innovating with NDN or namedatanetworkingproject. So as we all understand, our panelists will have extremely extreme insights and that will be an amazing discussion. But as you have noticed, the actual title is is What do we want? what do we we want."
  },
  {
    "startTime": "01:18:00",
    "text": "from the Internet to look like in 20 years. This is another way of saying you have to participate. That means that we are really welcoming and your opinions, your questions, your insights, your advice, anything, your hopes, that would be super important to us. Alright. So without further ado, I just want to start the discussion. I will start from and I will ask. but do we want the Internet to look like in 20 years? Okay. I'll be the first one. So being professors for many years, you know, I learned a bunch trick. have to give a lots of talks. The easiest thing to prepare a talk is to copy from others. This morning we'll have a great k note, by professor Philip, that is if I pronounce the name correctly. I'd remember his last slides, have the title. to say that computers in 10 years they will look very different. So we ask what's wish to happen in 20 years. That's even twice as long. We how we answer that question? Fortunately, I got another quote. I look up Professor Lewis website, and this one quote I really like. beside. well, he quoted others saying that solving a problem simply means representing it so as to make the solution transparent. Therefore, I think that what we wish to see in 20 years oh, oh, oh, It's really about what the problems which you today that we wish they will be completely solved. Hopefully, before 20 years. Did I already used up my 2 minutes? So I I will stop there."
  },
  {
    "startTime": "01:20:07",
    "text": "What about your 3 minutes slot? Oh, wow. This this should get get a great start here. I would have following on the problem solving part, I was actually wondering ideally, in 20 years from now, you determine it, would have vanished. Right? don't wanna see it anymore. So following up on your transparency point here. So it would be kind of ubiquitous, and you wouldn't worry about any Right? Nowadays to people argue about download bandwidth and whatever if if something becomes a true commodity infrastructure. you don't ask how many liters of water come per minute out of your water tap. You just expect it to work. You expect Nowadays, you don't quite expect power to work, but that will be some some reasonable definition of stuff being ubiquitous, accessible, maybe reasonably uncensored you're gonna factor that in whatever that means, and globally inclusive. and in order not to make this too long, maybe sustainable or green So but we can maybe we can come back to that point later. If if we wanna make things happen at a global scale, then we could probably want to have things workable everywhere, which also means to me means efficient, and Yellow Energy. So, Chris, you want timing. Yeah. Sure. So I I think your it up pretty well. I think in 20 years, I'd like to stop thinking about the Internet. maybe everything is a solved problem at that point. I don't know. But act actually, more seriously, It's really kinda hard to see what the horizon looks like, and 20 years is a bit further beyond the horizon, I would say, I might"
  },
  {
    "startTime": "01:22:03",
    "text": "I I would say that I hope the the community of people who are, like, you know, doing things to help her propel the Internet forward. continue doing so consistent with principles that I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I find important. So know, putting privacy, you know, ahead of other potential I guess. things that run counter to privacy on the Internet would would be something I'd like continue moving forward. of course, there's, like, other things that Yorg mentioned. Like, maybe maybe we could be a bit more energy efficient with how we do things. Yeah. Sure. That's, like, that's always no one's gonna disagree with that. Right? but I I I I just I hope for sort of a continuation of a principle principle approach to how we how we continue to do things in the IT and, I which is guess, beyond the IETF kind of vague, but that's that's the best answer I can get. 20 years is a long plate down drone. we get another round? That was fantastic. Can I have a follow-up on your initial Like, I was just wondering if You know, if you transpire an Internet or, like, one that just works and nobody really thinks about it is what we're looking for, then who would actually fix it? whose job is it? Is it, like, technology that is missing? Is it policy regulation, legislation, I don't know. Did they destroy your photo or maybe? and I have my answer to your question, but let me bring up my question. So my first round, I didn't really bring up the question. Let me talk about this. There's a Winston Churchill. I hope most people can recognize the name. and he has one quote, I really appreciate There's something like"
  },
  {
    "startTime": "01:24:01",
    "text": "The farther backward, you can see the farther forward, you're likely to to understand equivalent to that degree. You can Google that So this will remind me a conversation I had with some old timers, long, long, long time ago. probably in the nineties. I remember that the few people are around. the two names I can remember at I don't think they're here. Why it's a crucial quidama? That's why it's Bob Hindon. remembered we said we joked to say, how many problems we I soft. after so many years. One will say so many years that was we thought was already many years because the IT have started in 1986. that was in the late nineties. recall in day 1, we had the 3 problems. We're worried about the 3 problems, actually, this way. not in scalability. network condition. secretly. By that time, we looked around to say that. the dalton problems scalability, always say the network routing was a scalability was a problem. Even the think in the early 2000 ish, There's a even a Latin research group, I was the one for the co chairs. for that. despite there's a persistent problem. It has never stopped or even slowed down. the Internet goes. So, eventually, I've realized that's not a load blocker. Now about the congestion control? at This is a similar thing. Right? We still have congestion Control Research Group. I stink right. Yeah. No. The the working groups as well. But the fundamentally, Since Pan Jacobsen, invented this TCE slow start congestion control. problems, problems, I think that we have that problem. On the control, people continuously improve the performance"
  },
  {
    "startTime": "01:26:02",
    "text": "resiliency, whatever, all kind of criteria is ideas applied to that problem. but the band are large. Internet is no longer like, having this congestion collapse that literally happened during the late eighties. No. I was on the network. Don't think about it. You could do anything during the day. It's entirely stopped. At the time of the graduate students, we had to work at night. So you could to FTP. Yes. That's no more no more long gone. But think about the 3rd question. Security, security, Even back in the late nineties, we realized we had not made a lot progress. on that problem. And even today, I would claim personal opinion, of course, This is a number one problem, number one challenge facing the Internet today. And what's missing is not a lack of effort. but rather A shared understanding. of the problem space. And, of course, the solution space. I heard of the mentioning about privacy. you know, I'm a kind of I'll paint this outrageous opinion speaker. So, would say that the privacy is not standing alone problem. as a part of the security. without a security system. There's nothing to talk about privacy. So I will stop there. So we have a couple of questions people waiting in the queue. maybe Chris Christopher Padum Yeah. Hi."
  },
  {
    "startTime": "01:28:00",
    "text": "Yeah. is a this is a this is a great conversation to have right now, I think. So I wanted it's kinda follows up on what professor Zhang was talking about. I want it at well, taking a step back, we want to the Internet to be invisible. but I I wonder and the analogy you used was, like, plumbing, plumbing, way plumbing isn't visible. Maybe the Internet be invisible too. I wonder if that's possible. The Internet is not just it's not just, like, It doesn't serve a single purpose. The purpose is always changing. it's very much influenced about how the how we use it there's an attacker in the network That's, you know, we're envisioning underpinning all of this communication. So I yeah. I wonder if you think if if anyone thinks that's actually possible or if we could rule that out. you can always wish for things. Right? The the question was what what we would wish the Internet to be like in 20 years. And suppose I mean, take any everything that every piece of infrastructure you need to do with needs maintenance. breaks and and right now, given the amount of effort that we spend every day on fixing little things like access or being network being down excess lines being down mobile Internet not working everywhere and and and all these kinds of things. And that is a step way before we even get to attackers and applications. So I think if we would get that part of invisibility solved. We would be a long way because then we might actually find enough time to focus on the other So that that that would be one response to that. or one thought. And our panelists need to agree with each other. but, actually, I agree with yuck on that. That he is"
  },
  {
    "startTime": "01:30:01",
    "text": "it's not so much it's disappeared entirely. Nobody takes care of that. instead, of course, plumbers are always needed. The question is where? What is the front line? challenges. don't think that I should do the low level connectivity. Yeah. We have long passed that that stage. Again, back then, 30 40 years back, the connectivity was a hard problem. This stays the problems at the much higher level. that deserve the attention. Yeah. I I I guess I would generally agree. I think that the the use cases and applications that are driving, how we think about the Internet, how we use it, are gonna constantly change. The threat models are gonna constantly change as things in society change. the Internet will have to like, aspects of the Internet will have to change to adapt to that. I and there's probably some analogy to be made by the plumbing thing, like, the way you think about plumbing might change as a result of, like, that how stuff works. Don't know. But, yeah, I I I don't think it's ever going to go away. I think the the interesting task for us would be to make sure that As as the the Internet continues to evolve. that we continues to evolve in a way that is as I was kinda saying earlier, I guess, generally good and doesn't like fall off the rails or know, implode or whatever. Right? important point to add to this is this doesn't mean that networking research becomes unimportant. Of course. Yeah. Like, I just in case funding agencies hear that. Well -- Thank you. -- it depends on which level of networking. Right? Like, you know, I'm coaching the decentralization of the Internet Mister which is published the workshop report for workshop 2 years back."
  },
  {
    "startTime": "01:32:00",
    "text": "I think that attention in this space is not about so much the network as if connecting things together. the attention and the conflict has really moved to higher level. Think about 4, has the most power controlling the Internet. People tend to say, ice fees. old days. Today? No. There's somebody else. way about the interconnect level. So Abhishek can you Bye. While you're going to the mic, Dixie, can you And kind of elaborate on, like, what do you mean by decentralization authority? Can you elaborate on that? 9 bit Sure. Given that, I'm the dean RJ culture. That's the our business. This state's centralization or otherwise decentralization is very, very cinnable. important topic, lots of people have pay attention if not everybody. But I think that we need to have a First, agree on the definition. People say the cloud services is the centralization. But On the other hand, there's this thing called economy of Scale. I don't know about other people. I used to do backup myself all the time. no more. No more. Right? upload to the clouds, that's economical because this economy of scale the cloud storage can do the backup. far, I don't know, millions of people. 1,000,000,000 of your cloud is necessary. So therefore, decentralization doesn't mean we don't need cloud. But they're rather, I think, fundamental is who is in control."
  },
  {
    "startTime": "01:34:01",
    "text": "Today, I can tell you why I feel that I lost control. Just with one simple example. plan 9. I'm a nobody. except that the Google Gomia identity called the Gmail address. That's how I And and kind of recognized it's 90 I go anywhere on the wipe, It's the gmail. as the Google. Besides, this is the person, And, you know, Google will do this authentication, when you visit other sites. That is what that caused. loads in control. And I prepared a few slides. I don't know if for Maria, you could show that. You cannot. Because, like I said -- -- 6 question people, like, going up. So since we really encourage to have people discussing. Sorry. I was like, I I interrupt. Yeah. My name is Abhishek, come from I'm gonna present a little bit of a contribute to what the panel has been saying and maybe it takes the discussion to a different direction. I don't think Internet will be a commodity in 20 years. And here's here's why I say this. there'll always be very interesting problems. Here's why I say this. Current world population is about 7 and a half to 8,000,000,000, only 3,700,000,000 of that population is connected right now to the Internet. When I say Internet, I mean, the entire till bit till the user device accesses the the entirety of it. world population in 20 years? We don't know. 16, 17,000,000,000 connectivity challenges removed, Some of them are technical challenges. Some of them are economic challenges. Let's assume that removed. experiences become more interactive and you and more immersive which means a lot more data than I mean, There's already 90% of the traffic on the Internet is video. We're talking about more immersive data. faster latency, more more interactiveness,"
  },
  {
    "startTime": "01:36:04",
    "text": "getting bandwidth delivered as well as with the latency constraints, then on top of that, let's put the the supply side, wireless spectrum will always be limited. you can't generate. There's some physics based boundaries that we have that we can't unless Nobel Prize worthy of research happen, and we'd figure new ways of sending information. So that part is limited. And then you have all this influx of more people More people come connected on the Internet, more interactive and immersive applications which demand better late much better latencies and bandwidth. So I don't think we'll get to I don't know. This is my opinion. We're not gonna get to that plumbing just plumbers, fix it, there will be hard problems to solve. Yeah. Just but but I don't think this is a disagreement. Right? The fact that if you if you grow an electricity grid, if you grow any if if you if you grow a road network, if you grow whatever, you need to overcome an engineering challenge I don't think anybody of us is arguing that we are going to have demanding challenges ahead of us that we need to face, But still, A pretty nice perspective would be to get over having to fiddle with the basics. on the on the side of the user, not necessarily on the side of the piece or whoever that might be in the future. Right? So I I I don't think there is a there's a disagreement at the point where we what you say what we have been saying. I would give you a counterargument. This is a quote I wanted to say. This is a attribute to Jeff Houston. he had a block several years back. he said that It's truly amazing. that the sum of human knowledge is at my fingertips. It's also truly frightening. that all this information is only accessible. through a single company. That is centralization. This is single company controls by measurement. over 90%."
  },
  {
    "startTime": "01:38:01",
    "text": "of the search market. Yeah. And even you look at into the infrastructure, the connectivity. There is a great infrastructure consolidation. I think there's a good talk given by Craig Leverage. diaperage. I forgot how to pronounce his name. at some reason, the Nanook. they did a study. I remember clearly back in like it. 2010 sitcom, showing the measurement of the global backbone. By that time, there's so called tier 1 IS fields. That interconnect, no big chunk of the interconnect. Today, I think he's the ladies and gentlemen in the show. like, close to 90% of the traffic, It's carried by she didn't. and private networks. That really shows where those tier ones how well they are doing. Now. Now. Now. The public Internet backbone. as far as the traffic of volume is concerned. It's a disappearing faster than you can imagine. just to quickly respond. Yeah. Absolutely. No disagreement with you. As things get bigger, we're gonna have all new engineering problems, and as I was saying earlier, like, the the answer can't be, oh, to deal with the scale, we have to turn off or whatever. That is, like, absolutely the wrong thing to do. And so we're gonna have tough times ahead to, like, enable a continuation of the services we have, but then also support new things like this interactive, like, stuff. the the I I don't know. Like, v r a r, whatever. whatever the new fancy stuff is. and it'll yeah. It'll be it'll be quite hard. Looking forward to working on that with you. let me invite West with right here."
  },
  {
    "startTime": "01:40:03",
    "text": "Okay. Great. West Hartford, USCISI, and I can word, but speaking only for myself, To answer your question of, like, where do I wanna be in 20 years, I can tell you the things I can't do now. My wife is somewhere else on the planet, I can't figure out where she is without, like, going through sun centralized service Right? I I we have to share not only, you know, where our location is, but our relationship and everything else. Many things have gotten in the way of that, And I think in part, a lot of it has come with the advent of mobile devices. Right? My wife often carries 2 devices at least. I might be able to communicate with her through. So my question is, how in 20 years can I get to the place where I wanna be? that allows me to do things like have secured communications you know, in a decentralized way I'll give you one other my my other famous thing to do is, like, if If I wanna print something, I wanna say, hey. Print something to Chris' compute to Chris' printer. Right? Chris is a very context localized sensitive name, and it's only because I'm staring at, Chris, Could anybody actually figure out that I'm talking about that, Chris? right, and his printer. That's how humans work. That is not how we have the Internet working today. So my fundamental question is, How do we get to an end to end communication situation in a dynamic and mobile environment with secured identification location discovery that is all decentralized. Good luck. Good luck. Like, one problem at a time, I would say. doing end to end encryption first would be, like, fantastic. I think we're on our way there, but those are absolutely hard problems to solve. And I I I believe we've talked about them before in the past that there wasn't there some, like, workshop where the on naming stuff. I don't remember exactly what. But yeah."
  },
  {
    "startTime": "01:42:01",
    "text": "Yes. That does seem like a, you know, very admirable goal to get to. not clear to me if the trajectory we're on will, like, head in that direction given the, like, the the forces of centralization that a play button. come. the like to see us get there. So, I really want to answer your question. So you want to communicate it with your wife directly. with the end 20 security The fundamental thing. The very first question is, How's the Wi Fi 95? Like I said, I'm just in my Gmail address. So so So, therefore, you don't have a provider independent identifier. How dare you to talk about end to end encryption? Yeah. today, we have lots of encryption work. https. Okay? as really secure communication. But there are two questions I can raise. with regarding to that, security The first of all, There are guys who actually decrypt data. It's not a Who are those guys? me the boxes. CDS, CDS, And this is There's also These are some indicators. Okay. when DDoS happens, you know, the servers had to keep had seen mitigation and the service. You can buy that. They have to decrypt their traffic and all the traffic, to sort out goods from private traffic So Let's see. so called entrant decryption doesn't really exist. in the generally speaking manner. The second thing is that is not end to end like you communicate with your wife. you actually talk to the server. gas because you are not an independent entity. you have identity on the Internet And unless until we solve this problem, There's no true end trend as user end to user end. period."
  },
  {
    "startTime": "01:44:02",
    "text": "in the end, what you wish for is actually privacy preserving globally scalable decentralized rendezvous. Right? and and that's what we would need to be working for. 10. which has A few problems on the way that probably feasible, but trying small ones. Just to reply to that. I I don't think anyone in the room's gonna disagree that is happening and that there's, like, very few parties that maintain, like, a significant market share of, like, traffic and data and whatnot. So I I think we can all just assume that that's a that's a a matter of fact, and we've maybe maybe would not like it to be that way. as a, I guess, a more technical, I guess, response to what you're saying, Lisha, you're saying that, like, you don't really have end in encryption in practice, which really surprising to me because there are, like, a number of systems that actually deploy technology today. They do have a very well defined concept of what an identity is. So, like, what's that, for examples? Right. Right. ships today and to end encryption for messaging. and they identify users by their public keys they have a system for ensuring that those public keys are consistently be bound to, like, an individual and a user. So I I there are there are, like, there are there are things we can point to that say, we we know how to do this in practice. and for end to end encryption in particular, is a separate topic from centralization that should keep those, like, very different, very separate if we can. Avish wants to, like, correct that. I choose a mental asco. West. Do you recognize your wife with her public key? But that's exactly the point of these systems, like, what the apps key transparency system. Like, you you don't have to think about, like, what is what is Wes's public key? What's what's my friend's public key? Like, the the system takes care of that binding for you."
  },
  {
    "startTime": "01:46:01",
    "text": "Like, the there we have the technology to sort of do that. And in fact, there's, like, a new working group likely going to be formed to, like, standardize that technology too and and give us that binding that we need, where you can associate a given public with an identifier like an email address, a name, phone number, whatever. Right? well on our way to shipping this stuff. and Yeah. I think Chris and you and I have a much longer conversation, but beyond this So I will not waste the time here. Yeah. Yeah. y a. Hi. Why is what that? I'm joining from Montreal, and I quick security And I have a question that what are the potential implications of quantumcomputing, on Internet security over the next 20 years. and how my quantum effects, the targets, and impact the Internet in Frost time. Thank you. Can you repeat? I'm not sure either of us any of us got that. I I got the Okay. What is the potential implications of quantum computing? on Internet security over the next 20 years. And how my quantum attacks target and impact the the I mean, incorrect infrastructure. I I didn't quite get the second piece, but I I think I got the first to echo it back, it was like, what are the implications of quantum computing on, like, the Internet for the for 20 years, and it's, like, not great for stuff that we're deploying today with class cryptography. And, thankfully, we are well on our way to deploying post quantum cryptography in a cases like at for right now, for example, Chrome is shipping post quantum encryption for TLS, and it does talk to some servers. So we're making headwinds and strides to, like, get ahead of that particular curve. There are other types of technologies that we're shipping today that we don't have easy drop in replacements for, but it's you know, ongoing research and development to, like, develop"
  },
  {
    "startTime": "01:48:00",
    "text": "post quantum solutions for those particular problems. And I'm confident that people who, again, are, like, much smarter than me, can figure them out and we can talk about them in the IETF and get them standardized and get them shipped. So I I I know there's just, like, this you know, this looming threat of, like, post quantum attack 1st, but I am very confident in the community's abilities to deliver and deal with it in a timely manner. I agree. So I need new problems, and I think we'll solve it. That's not the challenge. the challenge is really understand what are the problems. Interesting. So we have the 1st optimistic agreement. this, Don, Hi. John Todd from Quad Nine. So I think in order to understand the most optimistic future, It's also interesting to understand more of the pessimistic parts of the future. We're looking at kind of a future where it is possible actually to perfectly encrypted and secure communication between endpoints. And I think technical problems that we have to overcome, but we'll get over There are, however, very strong political oppositions to that for a number of different reasons. Security. national government that's pushed towards data sovereignty So I'd like to get the panel's opinions on what you would see, how far do you see the pendulum swinging back towards kind of a negative view of things before it swings forward again because I think we're in a good, optimistic position right now with these technologies making things possible. I think that in the next 5 to 10 years, we're gonna see some pushback on that, and some of it may quite severe. How far are we gonna go into that direction before we come back out of just to clarify quickly by, like, the negative, we actually mean, like, catering to the the the request to, like, you know, backdoor or client side scan or whatever like that. Great. I can take it. Yeah. So I I guess,"
  },
  {
    "startTime": "01:50:01",
    "text": "No surprise that I I I I believe that we should continue this path of, like, encrypting all the things and ensuring that, like, the the systems that we build we can reason about their security properties in a way that makes sense and the the proposals that I've seen for I guess, doing client side filtering for CSM purposes, for example, They don't technically make any sense in any reasonable threat model. So I'm I'm I'm not so worried about that, like, kind of bringing us back. I guess, what I am worried about is sort of our inability to come up with an actually technical. So like, solution to this problem. for which I do not know one that exists. I think that's, like, the hard problem here. Like, how do you how do you build an open system built on open standards and open protocols in a way that can't be, like, easily subverted by, like, changes to the endpoints or can't be abused by, like, service providers to, like, compromise end user privacy seeing confidentiality. don't know super hard problem. A lot of smart people thought about it. So Unclear how how how how long how much longer we can kind of stay on the path of trying to encrypt all the things before we acknowledge that can't solve this problem, and it has to come back. But I I I hope it doesn't go in that direction. that's probably the best answer I can get. But thank you for bringing your customer CPU. So we sometimes agree. Other times, we have to disagree. I think that that's what I said part of what I said earlier that by the security we really need to reach a shared understanding of what other problems Like, currently, there seem to be some kind of a conflict thing. That is there's end to end encryption. There are some other forces. trying to countermeasure that. So"
  },
  {
    "startTime": "01:52:00",
    "text": "I'll just give one analogy. You know, this is a US. we should have individual freedom. That individual freedom. is not protected by everyone carried the gun. individual freedom. That it's guaranteed or enforced. and the lock. therefore, we should all understand that. the technical solutions. do not solve societal problems. We need to understand that. So, therefore, It's not so much of thinking about fighting the government But rather, we need to make a loss. and that will govern everyone's behavior, including government. That's my personal opinion. Yep. Agree. Well, Somewhat agree, I guess. I I I I find it Difficult to say that we we would be able to reach a shared understanding of the problem. when the problem is can be defined in many different ways and interpret in many different ways, like, for example, how do I how do I how And and the end to end encryption and client side scanning particular use case. I may consider, you know, the fact that I can, like, send private information to someone else like, a feature, and then someone else considers that a bug because I'm, like, sending CSM or whatever. I I I do not have hope for us. Like, establishing any sort of consensus on what the problem is I am very confident we can agree on, like, what the technical facts of the situation are. But coming through, like, a shared you know, this is the this is the problem that the the everyone needs to solve at the same time. I do not see that happening."
  },
  {
    "startTime": "01:54:01",
    "text": "Well You have this fundamental problem that you would like to have the good strong security protection, whatever, for yourself, but not necessarily for the potentially evil And since everybody disagrees of who who oneself is and who the others are, that's an unsolved problem to begin with. and you you can either engineer for these properties to hold? Or for them not to hold, and and but that will always affect everybody. we we we we that we don't get out of. Well, I would say that, you know, from different perspective, there's always a conflict. I mentioned about the individual freedom. Yes. We have all we all have freedom. but under the constraints. That is your behavior. do not harm others. So therefore, we are living in a society. we have to help prisoners how to address resolve those conflict. So, therefore, that requires a law. requires consensus. It's not that people hold different views or different parties that should say hold different views. and we say we cannot have a shared understanding. We have to. Tobias. Muxedank Institute for informatics. Speaking Can you cross the mic, please? BSCV, Max Blank institute for informatics speaking for myself and not for my affiliation. So we had some comments about taking positive and negative views of the future. would argue that they are very positive use of the future taking at the moment. If we look at our world, we are facing a world that is heating up we are facing society is collapsing. We are facing water running out. we are facing wars around water, around land, around resources tearing apart our world. currently tearing apart Europe already. And in that world, I'm not sure whether plumbing of the Internet might be irrelevant thing to consider."
  },
  {
    "startTime": "01:56:02",
    "text": "putting it to plumbing, while plumbing might still be there in 20 years, the water running in those plants might not be a commodity anymore. So I would like to hear the perspective of the panel on how the Internet will look such a torn apart world. what that that that's actually good from another is there's obviously picture that may be beyond beyond our scope here. there's one thing when it comes to I mean, fundamentally, this is a There's there's multiple things you can say about this. One bit is that if the Internet has enabled one thing in the past, then it would be sharing of useful information besides all kinds of disinformation, which which might help assist coping with future problems. of course, the Internet itself as resource consumption. And we had this example earlier with My boss given saying that we are going to have more a higher bandwidth demands, lower latency year to year to year in past. have been pretty good at coping with Internet scale by throwing more technology, more energy, or whatever at it. in order to sustain our growth to enable growth, enable things getting faster. in in the light of your very question, that might not be very sustainable in the long ones, we might need to reconsider how we are going to organize future growth to to embrace the next whatever billion people it might be and or whoever might be left or whatever you wanna say it. But So really just adding more routers, more fibers, more processing power is not going to be along not necessarily going to be a long term sustainable approach here. So that might be one One aspect of that I I I I agree with New York that the the this is a really big problem and catch up beyond the scope of this particular panel. And your kind of question was, like, what does the Internet look like in this, you know,"
  },
  {
    "startTime": "01:58:02",
    "text": "sort of future society. My pessimistic view right now is that there may not be it, like, singular Internet, we may end up seeing, like, effectively the things be fractured and fragmented. I'm kind of already seeing that in a way in terms of, like, what information is available where. But as, like, the infrastructure that surrounds like, the Internet and like, that that like, shapes the societies that use it. I I only kinda see that that fragment thing getting worse I would like to be proven wrong, but, like, you know, that's seems to be, like, the kinda tremor going down, but So can you explain what's so called fractured? really. Mints. So there's certain world. No. No. No. Like, there's certain places in the world where places in the like, I can access certain information, and I I can't access other information. That that is That's already the fact that today, that's what I'm saying. I've already yeah. That's what I said. But the one that we talk about 20 years down the road, Yeah. It might get worse. My point is that it it will likely get worse. I'm more optimistic. I agree with the yoke. that I think in terms of getting the remaining part of the world, or interconnected. is not purely a question of fiber. definitely the fiber will be needed, more fiber will be late. we have been increasing the fiber layout. I think very fast. But at the same time, we need really need to look into Newways I've come in the kit. the you know, the wireless, the peer to peer, In academia, you see so many publications with regarding to data mills. and reviews yeah, reviewed papers that, you know, your phone can be data mule connecting disconnected parts. of the world I think the diet area I hope we'll we'll"
  },
  {
    "startTime": "02:00:02",
    "text": "become reality less than 20 years. The question is, Ace, Why can't we do it today? I always say that if we fully understand pragmatic, not what I said. I quoted the professor Lavies quote. at if you can explain the problems so clear enough, the solution will offer itself. So we really need to go down the level. Why can't we do it? this data mu model, of communication. peer to peer communication. People say, oh, we have peer to peer. Look at the parent, parent, Everyone should not be a tyrant how it works. dependent center and its source. started writing people, like, letters in the mail. That's how I communicate with them if I wanna get in touch with them and that was kind of a joke. be great. And we'll take one more question from hopefully see on Yeah. Hey, Shivan Saieb, brave browser. Just wanted to respond to something I think you were saying, Alicia, where you said like, privacy is just a subset of that, security. I guess, I disagree. the classical response to that is that, you know, you can have a very secure connection between your know, your browser and and this web server somewhere where, you know, it's fully encrypted all that. But the user or you have not consented to this information transfers, which I would think a privacy violation, but totally is secure and fulfills all our definitions of security. So so, yeah, I I guess this disagreement over there. But I and I think why it's important to keep this in mind is because, like, privacy plays really well. into, like, decentralization like, you know, like, how do you distribute keys and, like, all of that conversation becomes interesting. if you're thinking about like, classically privacy, like a like a, you know, privacy lens where you're thinking about, like, know, you know, transparency and trust and consent"
  },
  {
    "startTime": "02:02:01",
    "text": "So I think that's why it's important to like, in my opinion, like, keep in mind that privacy is, like, its own thing, which is, you know, you need security for it, but It's also like, you know, independent of that. I think that you actually agree with me. Now this agree with me. I said that the biggest problem I tested today is the shared understanding of the security problem space. I don't see TLS encryption at so called security. No. not the security itself. We need to understand what is the security the first place. And therefore, Until then, You cannot say you disagree with me. previous is not part of security. Maybe I think I should join that conversation you're gonna have with Chris later on. Alright. And with that, like, a full circle of what the CSN beginning in now. Let's thank our panelist. gives an amazing discussion, and everyone And, also, thanks, everyone, for their very insightful opinions and questions was were super helpful in having this nice discussion. And sorry, Dan, Eric, department Eduardo for not taking your questions."
  }
]
