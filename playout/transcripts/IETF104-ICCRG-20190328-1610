[
  {
    "startTime": "00:00:35",
    "text": "that was super deliberate nobody will awake let\u0027s get started it\u0027s not just my Mike Brian all right I\u0027m not repeating Brian\u0027s comments to the mic but let\u0027s get started this is IC c RG if you don\u0027t care much about condition control you are in exactly the right room you might learn a few things this is an IOT F meeting but we are going to still use the note well I\u0027m not going to read this and if you haven\u0027t read it you probably should moving along before I get into the agenda I believe we have a jab escribe do we have no sorry we have we have somebody to cover jabber do we have a note-taker you\u0027re counting jabber right yes yeah sorry sorry you thank you Tom you know I know Tom it\u0027s late in the afternoon and late in the week I need somebody to cover jabber then oh hang on I mean somebody take notes I\u0027ll get this right eventually don\u0027t worry there\u0027s such a thing called eventual consistency sorry yes that that would also work for me but it\u0027s ideal if there\u0027s one person at least who commits to doing this thank you sir who\u0027s I heard a voice I can see uh yes Brian thank you thank you thank you thank you moving on we have we have a packed agenda today we have only four presentations but all four of them interesting talks and so there\u0027s a fair bit of time for each one I\u0027m going to be I\u0027m gonna try and stick to the time that I have written out there and as always you will all go past the time so we\u0027ll see how this goes I don\u0027t have very much else here to talk about for the agenda anybody want to bash the agenda and done all right Neil you\u0027re up let\u0027s see how\u0027s this Mike that\u0027s pretty good all right [Music] so I\u0027m going to be talking about bbr v2 which we think of as a model based "
  },
  {
    "startTime": "00:03:35",
    "text": "congestion control this is joint work with my colleagues at Google you chung Sohail ianvictoria Rajan you suck Matt and Van Jacobson next slide please so I\u0027m gonna focus on the be BRB to research update I\u0027m talking about improvements between VBR v1 and v2 talking briefly about the status and recent results we\u0027re seeing in experiments and then try a sort of lightning review of the BB r v2 design so in which we talked about previously previous IT apps and then give a quick status summary in and conclusion next slide please oh yes perfect so in order to address some various issues with the initial BB our v1 version we have undertaken a couple improvements so the main issues we\u0027re addressing here with v1 included low throughput for renaming cubic flows sharing bottlenecks with bulk PPR flows in certain scenarios BB everyone was Ross agnostic which in certain scenarios with Banach buffers or a QMS keeping the queue at 1.5 be Peters shorter that could result in high packet loss rates via v1 was easy on agnostic as well did not use CCN signals and with the initial release there could be low throughput for paths with high degree of data or AK aggregation and here the the biggest issues you would see with Wi-Fi paths with no our TTS say one to ten milliseconds and then another issue with me one was that there could be pretty severe throughput variation due to quite low ceilings in when in a small period of time that they\u0027re in probe RT t mode then with BB r v2 we were trying to tackle all of these issues and I\u0027ll give some examples of the design changes in the resulting behavior so for coexistence with reno in cubic the basic approach we take with PBR v2 is to try to adopt the time scale of the bandwidth probing so that we can coexist better with reno in cubic and we talked about the details of the algorithm last July at ITF one or two and those are basically the same in the latest revision as well and here\u0027s a quick diagram to sort of illustrate the flavor of behavior that we\u0027re talking about here so here\u0027s a scenario with a 15 megabit bottom link a 40 milliseconds rip time one VDP a buffer for cubic flows one PBR be to flow and you can see the the BP RV to flow is is playing along nicely there at the bottom in the pink achieving an approximately fair share "
  },
  {
    "startTime": "00:06:35",
    "text": "the reasonably low retransmitted rate matching cubics here so the next major improvement is is using packet loss as an explicit signal with an approach that sort of has an explicit target loss rate ceiling which I\u0027ll talk a bit about a bit about later on this we also talked about it ITF 102 and here\u0027s a quick scenario just to sort of illustrate the behavior with this algorithm so here we have six VP of the two flows in a very shallow buffer that\u0027s only 5% of the BDP only 49 packets but the flows start staggered and achieve an approximately fair share and with the reason being we transmit rate given how shallow the buffers so be BRB to uses ECN as a signal now DC TCP style ecn if it\u0027s available to help keep the queues short and to illustrate the kind of behavior we\u0027re talking about here we have 20bb our v2 flows starting staggered every 100 milliseconds it\u0027s a 1 gigabit bottleneck link in a 1 millisecond round trip time the ECM marks here are from the linux coddled queue disk with the default settings except for the c e marking threshold at 242 microseconds which is about 20 pounds worth of queue okay we have 0 retransmits pretty decent fairness and our TTS are fairly low the median OTT is right around the marking the threshold and the rest are reasonably well controlled so there are also improvements for high throughput for paths with large SKUs of aggregation notably as I said Wi-Fi and - to achieve that v2 explicitly estimates the degree of aggregation that had seen recently and now maybe in the data path and maybe in the act path either way the aggregation shows up in the extreme and bbr tries to characterize that and we talked about the details of that it ITF 101 last March you can find the slides there on YouTube we\u0027re seeing be BRB to match Kubek throughput for users specifically on Wi-Fi links and overall and in control tests for seeing reasonable behavior you can see here on the right is a time sequence plot of a be BRB to transfer to my laptop at home so it started at a pop in New York City when I run a local internet to DOCSIS link which has its own aggregation behavior if their Wi-Fi link has its own aggregation behavior and still behaves reasonably well and the aggregation modeling code is available in quick in the chromium quick and now recently also in linux 5.1 the final major design "
  },
  {
    "startTime": "00:09:39",
    "text": "change is to reduce the throughput variation so when BB everyone if you had to enter probe itt perhaps because it was a bulk flow that\u0027s been going more than 10 seconds or so bbr v1 would cut the seal into 4 packets in v2 you know we can cut the c1 to 50% of the bdp to still achieve reasonable throughput even while attempting to probe the the to a propagation delay so those are the major improvements in v2 as a quick summary you can sort of consider how B to lines up with v1 and cubic as I said the it\u0027s a model based congestion control and here specifically the model parameters that we feed into the state machine for BB are we to include the throughput the RTT or men ou TT the max degree of aggregation we\u0027ve seen recently in the max amount of data we think it\u0027s reasonable to keep in flight and as I said BB our v2 has this sort of explicit loss rate target and an explicit response to congestion to eat DC TCP style congestion and the startup not surprisingly has a sort of slow start style behavior of doubling the throughput until we see the either the throughput plateau or the ecn or loss rate exceed the design parameter or threshold so where are we with d BRB - right now we\u0027re testing it with YouTube TCP traffic we have our kernel with an initial version of PBR v2 on all the YouTube machines and it\u0027s running a global experiment with a small percentage of YouTube users results so far are looking quite reasonable we see the RT T\u0027s are of course vastly lower than cubic and and even a little bit lower than BER the one the packet loss rates are considerably reduced versus vb l v1 they\u0027re about halfway down to back to cubic and while still matching cubic throughput and we\u0027re continuing to iterate using the youtube setup other production tests and lab tests and we\u0027re preparing to open source the code as soon as we can and as we do that as soon afterwards as possible we\u0027re gonna update the BB our internet drafts to [Music] reflect the latest vb r v2 code because I know there are people out there that are interested in working with the algorithm based on the draft rather than the code I would note that the code we try to we we have made a dual BSD GPL so hopefully folks in the BSD ecosystem can use it as is if if they are interested but that ain\u0027t right so I\u0027m gonna try to do a lightning over year the PBR be to design how much more time to have I guess you took a clarifying question now yes sure so so this is a job ascribe um mr "
  },
  {
    "startTime": "00:12:40",
    "text": "Johansen asks which is the which RTT is it on slide six that\u0027s the Monaro TT estimating to a propagation delay so just a quick I mean you\u0027re gonna try to do a lightning overview the BRB to design many of these slides we\u0027ve shown a previous IETF so I just thought it would be useful have a quick overview all in one spot for so at a 10,000 foot view you can think of the design as basically consisting of a system that takes as inputs measurements from the network traffic that it\u0027s controlling and it\u0027s looking at a couple different signals to the throughput or delivery rate the delay that is the RTT delay the loss events ecn mark events and those are all fed into a network path model inside PBR and then the parameters of the network path model are used to control the evolution of the state inside the state machine inside PBR and then that in turn makes adjustments in the output parameters which are the rate which you can think of as a sort of pacing rate for the sending process the maximum volume of in-flight data otherwise known as a congestion window or Sealand and and then floor at I\u0027m calling here a quantum which is just sort of a chunk size for the whatever off that mechanism might be in play whether it\u0027s TSO or gso or whatever that OS calls it and those output parameters are fed into the sending engine of the transport whether it\u0027s TCP or quick or something else which takes the data chops up into quantum sized pieces and then paces them out at the given rate until it has a maximum amount of data or or until pacing says you can\u0027t set any more right now so so that\u0027s a the big picture if we look at the drill down into the model and what\u0027s inside the model you can sort of visualize it as is depicted here in the diagram that\u0027s a time sequence diagram with a sequence on the y-axis time on the x-axis the green is showing send events the acts are showing up as blue here here I\u0027ve drawn them is as pretty bursty acts because that these days that\u0027s a pretty pretty typical behavior whether it\u0027s high speed 8 Ethernet Wi-Fi cellular DOCSIS they all have aggregation pretty significant levels and you can sort of visualize each of the parameters here the first one is the maximum bandwidth that\u0027s available recently to this flow the second one you can think of as the min RTT seeing recently which serves as an estimate of the tumor propagation delay then there\u0027s the maximum amount of inflator that we think is reasonable based on my recent loss and ecn signals and then there\u0027s the maximum measured aggregation level that we\u0027ve seen recently in the extreme and two of these in particular of course are going to "
  },
  {
    "startTime": "00:15:41",
    "text": "change quite rapidly based on the low traffic levels that is the bandwidth and maximum in flight that\u0027s reasonable to keep maximum amount of data it\u0027s reasonable to keep in flight so because of that BB r v2 maintains both a short-term bandwidth and in-flight estimate and a long-term bandwidth and in-flight estimate and you can think of this as sort of being analogous to cubic which has a sort of short term SS thrush estimate and then a longer term W max value that\u0027s a higher amount of in-flight data that it\u0027s hoping to quickly progress back up to you with if everything goes well it\u0027s sort of analogous to that so a high level the adaptation of the model basically estimates that then with an in-flight 2d set of parameters over both the short term in the long term with the goals being not surprisingly a high throughput and for bbr importantly we\u0027re trying to be able to maintain that even when experiencing some moderate particularly amount of random packet loss and then we\u0027re of course trying to keep low queue pressure and the basic approach here is sort of twofold we want to spend most of the flows time in a face where it\u0027s mostly adapting quickly to try to maintain flow balance between the extreme and the data stream while also leaving some Headroom of unused capacity either on the bottom IQ or in the bottom link and to do that we have the this short-term bandwidth an in-flight estimate tuple that basically bounds the behavior using the latest delivery process and Lawson ECM signals that we\u0027re seeing and the intuition here is basically what\u0027s the bandwidth and in-flight delivery process that we\u0027re seeing right now and we\u0027re seeing you see any lost signals that\u0027s an indication that we need to adapt quickly to those signals that we\u0027re seeing right now to maintain reasonable queuing levels and then the second part of this picture is that of course periodically we do need to probe beyond that flow balance level to probe robustly to see if higher bandwidth is available now then say it was available before the last time we probe for bandwidth and to do that to control that behavior we have that as I said that the longer-term bandwidth in and flight estimates that were have been most recently measured before we saw signals of congestion in the form of ecn or loss marks and the intuition with that part of the picture is that we\u0027re basically trying to say what\u0027s the maximum bandwidth and in-flight that can be achieved consistent with the networks desired loss rate in an ECM mark rate I solos or "
  },
  {
    "startTime": "00:18:41",
    "text": "service level objectives that\u0027s the basic high-level picture of the model so to consider the details of how it evolves I think it\u0027s useful to consider a more concrete example so it\u0027s an interesting question how do we adapt to a packet loss signal given the ambiguities involved so if we consider this this example we hear we have a shallow buffered high-speed wind would say 100 millisecond round-trip time and app does occasional 200 mm packet writes these are RPC requests RPC replies web object replies something like that and let\u0027s consider a case where the available bandwidth drops from something really high say 12 gigabits per second down to 12 megabits per second which is from a thousand packets per millisecond on a one packet or a millisecond and of course that\u0027s a big drop so in the first right after that bandwidth drops if we\u0027re continuing at the same rate there will likely be very high packet loss and there\u0027s an interesting idea though about the low delivery rate that we see there of two packets in their own trip time it was a question of to what extent is that due to with the lack of data that was sent to extent it was due to bursty traffic that may have caused that loss or or it could be a sustained reduction in the bandwidth available to that flow so the question is how do we what do we do there so the philosophy here the design philosophy here is that to fully utilize bottlenecks of all kinds including the bottlenecks with shallow buffers we want to adapt both the maximum sending rate and the maximum in-flight meal ow so in this example should we have simply cut the Sealand right down to to do to only two packets being delivered well it\u0027s interesting to consider that if we actually find a way to pace at the available bandwidth we can actually deliver far more than two packets per round trip time so the idea here is that if if this last pattern or something like it continues repeatedly we basically want to gradually reduce the sending way down to match the available bandwidth and to try to converge to an in-flight that matches the BGP so that we can send the entire response without anything being dropped that\u0027s the the basic motivating idea so what does the algorithm look like for the short term model adaptation basically this this strategy at a high level is to gradually adapt as I said to to measure delivery process which includes both the bandwidth and the volume of data and this adaptation applies generally whether we\u0027re in fast recovery RTO recovery not in recovery at all whether application limited or not and the basic idea is to maintain a very recent estimate of the delivery process both the rate and the volume of data and then upon in once per round trip time make an adjustment in those two model parameters that is a multiplicative cut by 30% "
  },
  {
    "startTime": "00:21:42",
    "text": "someone like cubic except the the twist here is that we try not to cut the model parameters below the most recently observed delivery process values so that we try not to overreact and try to match the recent or converged on to in the recent delivery process so that\u0027s a basic idea so if we so that\u0027s the the model and how that evolves if we switch over to the state machine at a higher level the state machine is quite similar to the v1 we\u0027re alternating essentially between probing for bandwidth and round-trip time when the connection is warming up we have a startup phase it\u0027s a lot like slow start once the path we think is full we try to drain the queue and then we alternate between probing for bandwidth and program for round-trip time so just to quickly go through an example life cycle of a a B B RB - flow here I\u0027ll try to highlight an in bold the stuff that\u0027s new and v2 versus v1 and this order this may seem similar if you recall the presentation from last July but I thought it would be useful just to run through it quickly again so a flow starts out in start-up which like the traditional slow start phase we\u0027re trying to rapidly discover the available bandwidth by doubling the sending radio from a trip time until we see that it looks like the path is full neither because bandwidth samples are plateauing or the loss or ecn mark rate becomes too high when we think we fill the pipe then we try to drain it draining that queue and also if if we\u0027ve seen ecn or less marks that indicate that we\u0027ve bumped up against some queue level then we try to leave some Headroom as well then we try to spend most of our time in a phase that you can think of as sort of a cruising where we\u0027re trying to maintain a low in-flight and adapting continuously every round-trip time to whatever loss and ecn signals that we see based on the algorithm I just described and then when we decide it\u0027s time to probe for bandwidth first we try to refill the the pipe by sending out exactly the estimated bandwidth so hopefully we would reform the pipe without creating an EQ for doing well and then we proceeded into a phase where we\u0027re raising in flight to a level that may indeed be likely to be creating some kind of queue and we we don\u0027t know whether that\u0027s a shallow buffer they were dealing with or a deep one so we started all cautiously with a one extra packet in flight and then progressed to exponentially higher amounts as we were able to grow and fly without causing the ECM mark rate or the loss rate to go above the design SLO thresholds and then once we do see some indication that we "
  },
  {
    "startTime": "00:24:43",
    "text": "filled the path either because we were able to we apparently created a queue by having in-flight reach some multiple of the BDP or we hit ecn or less signals then we try to drain the queue that we\u0027ve created and leave some unused Headroom again if we saw a loss or or ECM so that\u0027s the basic life cycle of a BPM flow into summarize i can sort of run through a quick comparison of reno and cubic and PRP - so reno has this sort of familiar sawtooth pattern with putting an additional packet in-flight every round-trip time until the experience is any packet loss at all and that responds to a me a level of packet loss at all means it has sort of a brittle loss response because the c1 increases are linear one packet per round trip time it needs a long time to fill a pipe so it needs a thousand times more time to reach a thousand times higher bandwidth which means for example to fill a 10 gigabit hundred millisecond path you mean more than an hour between any sort of packet loss at all which means you need a very very little packet loss rate of two times 10 to the minus 10 which is not really achievable in practice so people came up with cubic which has a more scalable growth curve using a cubic in cruise function but it\u0027s still not as scalable as we\u0027d like so you still need since it\u0027s a cubic function you still need ten times more time if you want to reach a thousand times higher bandwidth and so for example if you have a 10 gigabit link with that I thought 100 millisecond trip time you need more than 40 seconds without any packet loss whatsoever so the still you need to further illustrate of sort of one packet in 30 million so BRB to what we\u0027re trying to achieve is more a more lost tolerant design with a more scalable growth function and also to to leave more Headroom in in the steady state then cubic does so the idea here is that we try to have a scalable exponential growth so that you can utilize nearly available bandwidth in logarithmic time and you because of the last times you can fully utilize a in a big beauty piece that even with a certain amount of loss in every round determined by the design parameter that\u0027s the lost Thresh and this is a picture of a shallow buffer a case where we do run into packet loss if there is a deeper buffer then we wouldn\u0027t even be running into that Pecola\u0027s so quick status overview the BRB one is running for most traffic on Google and YouTube and backbone but "
  },
  {
    "startTime": "00:27:46",
    "text": "we are experimenting aprv - on YouTube there there\u0027s some links there to other documents that we\u0027ve talked about before and in conclusion were actively focused on PBR v2 and making improvements there including reducing pressure improving coexistence with you know in cubic there\u0027s also work going on on bbr for FreeBSD TCP at the Netflix or in close communication with their excellent team and as always we\u0027re happy to see patches here test results look at packet traces we encourage people to offer ideas about the algorithm edits for the internet drafts typos but fixes what do we have we\u0027re happy to hear it thank you very much I kill me only we have about seven minutes for questions and jabber scribe and Dave tat asks have you tried the existing RFC three one six eight compliant FQ color response we have not experimented with that and in the Google in the EB our Google version the PBR TCP or quick no we are bigger believers in a direction where ECM is more like in DC TCP or l4 s where there\u0027s an easy on response when that happens at lower Q levels and where the sender has a more graduated proportional response to the EC onward we leave in that direction is a is a good direction to go okay I have two more from jabber and then I\u0027ll drop back and Mario Hawk asks which mechanism achieves a convergence to fairness among multiple bbrt vt2 flows yes so it\u0027s it\u0027s gonna depend on whether we\u0027re running into a buffer limit or not so if it\u0027s if it\u0027s running into you an EC N or loss buffer limit and setting implied high the fairness is achieved by the multiplicative decrease which leaves that Headroom and the amount of headroom that\u0027s left is proportional to the two they imply hard the flow so bigger flows have are leaning more Headroom it\u0027s very similar to the convergence dynamics of Moreno or cubic because of that multiplicative decrease effect if there\u0027s no buffer limit that\u0027s being run into you then the convergence happens because of the dynamics of the the way that bandwidth probing works and the it\u0027s a little more subtle and we can cover it offline for anyone who\u0027s interested in details but basically smaller flows when they probe for "
  },
  {
    "startTime": "00:30:46",
    "text": "bandwidth variable they\u0027re multiplicative increase makes a larger proportional increase in their delivered bandwidth than the proportional increase that higher bandwidth flow sees when it probes I can run through the details offline or over email if people are curious okay and then John border asks is there something that can be done in a browser to always get a bbr response from YouTube no although from YouTube you\u0027re early you\u0027re always going to get a PBR of some flavor you know mostly p1 sometimes me too if you\u0027re randomly selected hidden from buffer so green face has changed now we are not draining all the way as in we are not bringing sending rate down to four packets but we are at much higher rate now so I thought the idea was to get main RTD out of it right so that\u0027s the the property T face there yeah instead of going down to four packets then on to half of the BP the intuition there is that at a high level to first approximation if you can keep exactly the bdp in flight then that will give you an empty queue so if you if you happen to know exactly the bandwidth then exactly the the RTT then if you pull it down to one bt p you\u0027ll get an empty queue since you don\u0027t always know exactly your available bandwidth and the real underlying to a propagation aliy we cut it down to half to give ourselves the ability to gradually converge down to everybody seeing that to a propagation delay but the basic answer to your question is that once the amount of data in flight is less than the bt p at any level then in theory you have the ability to see that to a propagation delay okay and I guess between 4 and 50% big the rationale to get 50% is because we are almost doubling is that the in the start of phase what\u0027s there it\u0027s not really related it\u0027s it\u0027s more question of how far do we need to pull down the in flight be able to converge given that when the flow start out there maybe there might be quite a bit of queue and people might not really have a good estimate of the tool propagation delay so when they try to estimate the BGP that estimate is not gonna be perfect either so cutting to half of something that\u0027s more than two times bigger than a water that will be is not going to give you a good answer so there\u0027s some amount you need to pull down to sort of compensate for the over estimate so that you can gradually converge downward the 50% came out to be a good based on your based on my desk yeah thanks pretty Microsoft so I have a question about their target last rates that you talked about yeah so you know mentioned the numbers are you telling them definitely for different workloads or right I think this is it\u0027s gonna be "
  },
  {
    "startTime": "00:33:46",
    "text": "subject to tuning and for their research and discussions for this particular iteration of BB r b2 we\u0027re targeting a loss rate around 1% so currently the threshold at which it backs off is a 2% loss rate in that measured round-trip time so that the overall net effect since a given flow is only going to be probing for some fraction of the time isn\u0027t much lower than 2% but that\u0027s the basic dynamic is where we\u0027re setting the max at 2% to try to achieve an overall average well below 1% oh so if I understand this correctly so there\u0027s no immediate reaction the reaction is based on the rate over a period of time there\u0027s a once per round trip time reaction to easier than or loss yeah now I have a follow-up question to the 50% so to measure the RTT it\u0027s really because we have one or two minutes left and we have this really quick ok so 50% of bdp or 50% of the current sending rate 50% of their estimate a PDP just for folks in the line I\u0027m gonna the line now and please keep it quick I\u0027m happy to talk after the ICC ochi session to mother insula I was curious about dais in the TCP style Sen mechanism are you negotiating or extradition how you\u0027re getting your feedback right now we have internal to Google we already have a private negotiation mechanism and we\u0027re happy to work with people on the public efforts to negotiate a DC TCP style mechanism such as l4s something like that right now internally we have our own negotiation mechanism and externally on the public internet since there isn\u0027t a lot of standard we\u0027re not using it yeah the next public internet okay thank you hey Jake Holland I was wondering about the probe bandwidth I think you said it responds to less and ECM version think I heard you mention our tter your source money to increase the GT yeah I forgot to mention that yeah so talking image in slide better Francis but basically as before with PBR if we essentially see an increase in our TT but more specifically if we see a what looks like an inflight that is basically one point two five times the estimated VDP then we\u0027ve estimate that we have enough of a cue that we\u0027ve probed enough and it\u0027s time to drain and and move on Andrew Berger I have I have a pretty good idea how you would do this inside Google and that you would but I on the public Internet would you be proposing to sit the ECM and loss right targets as a function of the safe code point that you\u0027re sending up I don\u0027t know about diffserv code points "
  },
  {
    "startTime": "00:36:51",
    "text": "yeah I don\u0027t know we could talk about that offline and if you haven\u0027t thought about doing them on Google\u0027s backbones you really should because that would fix some things Jonathan Morton here and I don\u0027t know if you saw the some congestion you experienced or come on Monday so but do you think an SEM being would be a useful mechanism for improving conduct the ecn fidelity in terms of PBR I think it would be better than no ecn style signal at all yeah I think anything and he my guess is any form of ECM signal that gives you a fine grain picture that sort of more than you know one bit per round trip time I think any sort of more detailed easy on signal I think would be useful and yeah okay I\u0027m sure you can discuss this to more detail later yeah absolutely thank you thank you so much meal and please find Neil outside for more discussion and hopefully we\u0027ll hear more about the results of your BRE to experiment perhaps on the list as soon as you have something to share thank you so much and now we move on to payment take it away hi everyone my name is payment area and I\u0027ll be talking about how to make easier and more useful so let\u0027s see how first I\u0027ll discuss about ASEAN you already know about this but just a short introduction about the issues that we see I\u0027m using is Ian and then the network utility maximization framework and how we use this framework and some simulation results and the model validation and the benefits that we get from this and we will discuss some applications and the advantages of using ECM in this framework and I\u0027ll conclude my presentation after that so you already know about the ASEAN and how this is TCP uses ecn based on and instantaneous cue that marks PAC is above threshold and it interprets a like in my marking probability per article and based on this probability it cuts the window congestion window it doesn\u0027t have the window as in TCP but it has a bit less than TCP and like oscillates more but in a shorter scale this is how it case its benefits but about the issues with the ECM if we interpret it as a one bit information so there\u0027s no "
  },
  {
    "startTime": "00:39:53",
    "text": "problem with normally seeing how it is defined in this RFC rather we only get one signal peralta t so it\u0027s like too little and too long but if we use DCT CPU style marking so it for example in VC TCP says that if the marking probability is low it diminishes the usefulness of ecn and also it\u0027s useful only when it deals with a packet dropping not just marking so it\u0027s better to keep it a bit higher and get a better signal out of the network about how much it is congested so it\u0027s better to use a higher marking probability in the network but the problem is that this is not an additive measure is multiplicative and also it has problems with using in the theory like the non theory because the theory works with an additive cost or additive signal with the network so has a short overview of the theory never utility maximization deals with solving a rate allocation problem based on some constraints in the network so the goal is to maximize like social welfare in the network subject to some constraints and in the simplest case these constraints are link capacities you don\u0027t want to exceed link capacities but we want to increase the send rate of each source as much as we can until everybody is happy so this is how this maximization problem is defined so in this case you are is a utility function of each sender or source are this utility function represents the happiness each source has or how much happy each source is by sending for example a trait XR and Link capacities which are denoted by CL here the crossing traffic on each link which is denoted by Y L shouldn\u0027t exceed the link capacity so this is the simplest case that you can represent this rate allocation problem but what happens if we use easy and with this theory the problem is that if we have higher marking probabilities as we have for example in data center TCP and the plot on the right side shows that the data center TCP with different VDP can it can reach to higher marking probabilities we have the problem on the plot on the left the two equations here "
  },
  {
    "startTime": "00:42:54",
    "text": "we plotted the deviation that these two have the deviation shows that if the marking probability in the network increases the deviation is larger so we have like an estimate bias that should be removed and the theory because it here it works only with Sigma but the signal that we get from the network is the first one which is multiplicative it only works if the marking probability in the network is much lower than one is for example close to point zero five point zero four or some numbers in that range not higher but we see that in data centers basically as shown in the right plot which is plotted by some equation about the theoretical marking property with one button neck link and the number of competing flows we see that is at least around find sixteen or cross the point if that\u0027s a problem that we see with a using ECM that we cannot use a higher marking probabilities although they are useful in the network so what we did is that we turn a scene into an additive signal actually with david etherion somehow was extended to adopt this and we used red because we wanted to have a deployable solution not something which is hard to be deployed and it\u0027s just a matter of configuring red in commodity hardware and as benefit that we get faster convergence because with higher marking probabilities as we will show in the rest of the presentation we see that we have a faster convergence because we get more signals from the network is a more fine-tuned signal it\u0027s not just zero zero zero and then after a while one as a signal which is which shows that the network is congested so if we get more fine-tuned signal then you can have a faster convergence and it could be used actually to have an earlier feedback with visual cues and we probably can my start we can start marking packers even below the capacity ah let\u0027s keep the theory so just as to show how it looks like the cake there are there\u0027s a theorem on a solving a maximization or optimization problems with some constraints is called KKT theorem the first condition here is the original theory theorem which is how it works it has two variables here mu I and lambda J "
  },
  {
    "startTime": "00:45:57",
    "text": "what we did was that we change these with two functions here F P I and KVJ so it has some proof that he could find in the technical report that I cited on the first slide and as the function that we incorporated in the theorem we use this function which is a logarithmic function and we we derive a new Lagrangian function which is the base for deriving three types of algorithms of primal dual and primal dual depending on how we divide the controller between sources or the network as a simplest case we used red as we talked about we use red as the dual algorithm so the algorithm is on the top it shows that the current marking probability the prop marking probability at a time step n or iteration n is the backlog the current backlog divided by some maximum threshold so so and it\u0027s a limited to the range 0 to 1 because marking probability shouldn\u0027t be larger than 1 so how we can achieved it is easy but just configuring red so we set a mean threshold to 1 max threshold to should be actually larger than some number which is derived by stability proof in the attack in the report for example how to have a notion of this number if in a scenario that we simulated we had a bdp close through 1.7 megabytes and in this case alpha is some bound on some function of the second derivative of the utility function l is the maximum path length in the network and s is the maximum number of competing flows so in that case we had max threshold larger than we or equal to 1.3 megabyte but what we did in the simulation was that we set this number to smaller value but we didn\u0027t observe any instability and also the average queue size in that case was around 75 kilobytes much lower than BBP gah so set the max p21 because we wanted to use the whole range of marking probabilities from 0 to 1 and also WQ to want to have instantaneous q marking we didn\u0027t do this like exponential smoothing and the queue size "
  },
  {
    "startTime": "00:49:00",
    "text": "so this could be achieved by just configuring reading commodity hardware and as one of the actually benefits of doing this and using that function that we showed before is that we could remove the bias that higher marking probabilities has on the actually the rate that each flow gets for example in this case we have a like a partner topology we have a number of 5 have flows competing with a number of 1/2 flows like cross traffic so N 1 and n 2 we did a simple simulation the data center TCP we actually instead of the entrant marking probability that beta Center TCP works means we use this function so we see that in the simulation the rate actually that each flows gets five hot flows divided by the rate that 1/2 flowers get actual ratio doesn\u0027t change as the number of file have flows increase this means that as the number of fellow flows increase we have a higher and higher marking probabilities but it doesn\u0027t effect the behavior of the controller so that\u0027s the good point about the theory and we derive some other optimization algorithm the first one is a primal one is similar to the one that Kelly had in his paper but as you if you remember we have a different cost here which is a logarithmic function based on the marking probability that we get into an marking probability but in Kelly\u0027s algorithm it\u0027s just cost and also to two different dual algorithm but they need actually implementation in routers probably people might say that they\u0027re not they cannot be deployed but to have a phone set of algorithms we derive these as well and also a combination of these that you could have to have primal dual algorithms so we validated all of these algorithms algorithms here with a utility function as this function as one of the properties of this function we should have actually proportional fairness so in this case five hub flows across five hops it means that the rate should be one-fifth of the one hop flows so the left plot is a numerical "
  },
  {
    "startTime": "00:52:07",
    "text": "evaluation with Mathematica we see that the line below the purple one is exactly 0.2 it shows that for every number of crossing every number of fire one of flows it means that this ratio the theory doesn\u0027t have is not affected by the number of computing flows and also the marking probability so it means that this you were successful in coping with this easy and usage problem but if we don\u0027t use that you see that as the number of flows increases we have more and more deviation with this rate ratio and the right plot is simulation with omelet and we see that they\u0027re pretty close to point to some applications we actually can obtain utility function of controllers when demarking poverty is not low so as one of the benefits of this but in the literature probably you have seen that this is approximated by some of the marking probabilities and conditioned on being the marking on the marking probability being low but in this case with this theorem we can use this to obtain marking obtain the utility function of the controllers with higher marking probabilities and also we can inflate deflate marking probability we can play with it and by marking property I mean the equilibrium marking probability because there is a base in the lock function fee and we can play with this to have different marking probabilities in equilibrium so just also useful and you are not obliged to have fixed mark in poverty depending depending on the behavior of your controller but you can have different ones and we can see in the next slide the benefits of playing with this and also the potential to deal with virtual queues they have two scenarios here they have a high you have a link with one gigabit per second bandwidth and also a limited link on the left - plucks the studio stage shorten furnace we see that as we increase the space parameter to get higher and higher marking probabilities in the network the flow "
  },
  {
    "startTime": "00:55:08",
    "text": "flows behave smoothly on a smoother and also then if new flow joins because we get more marks from the network is like a more fine-tuned and more signals received from the network we have a faster convergence rate and also on the left side we limited the bandwidth to see how much we can increase the marketing poverty and does the theory work with this or not you see that it works up to for example point nine nine marking probability but we see that because that\u0027s like in this case you get all the ones from the network at some and at some time we get one is zero then we lose some benefits from it so it\u0027s better to have a high marking poverty not so high and not so low low so that\u0027s the benefit that we can get by playing and increasing fee not too much to have a like a marking probability in the middle of this range but although it can work with low and high marking probabilities to conclude we try to improve easy aim to avoid an estranged we didn\u0027t want to limit the range to be too low or maybe too high because model controllers like it isn\u0027t a TCB and as i just about PBR v2 because they use this easy and DCT CPA style ECM like method they can have a higher marking probabilities and it\u0027s good to play with it to have faster convergence or a smoother behavior and get more signals from the network about the congestion and also the possibility of marking even below the cue the capacity and as Nexus steps we will focus on experiments it was the American evaluation of simulations and are some deployment issues of this method thank you hey Jake how could you go back to slides real quick - one more yeah what are those error bars is that like variants across experiments within a window or what 10 different line runs the lines are the average and the error was the minimum and maximum okay thank you hello again existing acnd polymers use the yellow marking probabilities as DC tcp stone marking has its own probability problems with deployment I think but what if you think about an SCE "
  },
  {
    "startTime": "00:58:11",
    "text": "some congestion experienced using the extra code point I mean using ECT one isn\u0027t of Ziyi because I think that\u0027s a way of getting the higher marking probabilities that you rely on here without breaking compatibility with existing gp\u0027s yes actually the theory just this theory just says that if you if we have higher marking probabilities so it doesn\u0027t matter if if you get it from a DCT C psi aq mignonette or if it\u0027s from something else as long as you have a higher marking policy you can use this oh we have some prototype code which produces the SCE marking using the DCT CP style of of trigger so we have we can have a ramp function over a small range of Q depth yeah for example or a threshold function that\u0027s a particular queue depth and I think that\u0027s what do you see gcp style thing it is what you have in mind hmm yeah yeah I think you can apply this theory on what you already said it doesn\u0027t matter if is only TCPS style or something else but we only test that this with PCT cps I\u0027m working but they believe that it works that case as well yeah I mean I think it\u0027s just something to bear in mind with your plant experiments yeah I Bob Brisco I just posted a mine on the list pointing to another way to get your additive property with that within the number space 0 to 1 by essentially you instead of altering them love the network you alter the end systems do you transform the number space from P to P divided by one minus P and then so it\u0027s essentially taking the instead of the number of marks divided by the total it\u0027s the number of marks divided by the number of unlocks and then it transforms the whole space and you get your additive property in a transformed number space so the marking parity is the same the same as the very sophisticated computes it is exponentially not just a function you mean you when when you\u0027re reading in the marks yes you just calculate the fraction differently you don\u0027t you don\u0027t use the total number of packets use the total number of unmarked packets and then you then you sort of compress the numbers base into 0 1 the your relative number space yes it\u0027s like something like any more questions "
  },
  {
    "startTime": "01:01:27",
    "text": "Michael is not a question of clarification I\u0027m not sure if that was a misunderstanding like that but just to be very clear this calculations done in the end system also the way it was presented is done in December so it\u0027s just a regular read with this slightly weird configuration but other than that everything is calculated in the sender already in that so I said yeah thank you well thank you so much Foreman thank you who\u0027s up next Bob you\u0027re up next this was going to be your team giving an update on place chirping but he\u0027s felt fell ill and had to go home so I\u0027m gonna give an update on the implementation status of T Prague yep thank you and just for those I actually don\u0027t think we need this given the last three tours I mean about this but essentially the DC TCP style is the where we\u0027re trying to get to with more tiniest salty so you get a high utilization and low queuing delay just a quick update on the implementation implementation status of all the bits of l4s and in fact I guess we could say that or I\u0027ve just heard about VBR v2 I was I was hoping that I wouldn\u0027t have to do the fallback part of TCP prog to deal with all the lost stuff it seems like we\u0027re very much converging on all having the same pieces with maybe I\u0027ll be to using the decent piece of East our bit for the up for the when it has got a CN and so it looks like that you could call that an implementation as well but I don\u0027t want to co-opt it without looking at it further but anyway so as now Linux implementation of the network part there\u0027s a TCP prog implementation a quick prog implementation an implementation of the real-time adaptive screen throw 4s and all the bits of creation as well which anyone can use separately from l4s so and in particular just wanted to make people aware of an announcement I made in the transport area working group that DOCSIS 3.1 will support this DC TCP style is t1 behavior as part of the new "
  },
  {
    "startTime": "01:04:29",
    "text": "specs that were released in January with an feel for s support and the there\u0027s also reduction in the requests grant lived in the Mac but that\u0027s not really why sue crg what I want to talk about here is how the internals of TCP prog and what we\u0027ve been doing on that it\u0027s based on they descent a TCP so if you know that you know in good shape and there\u0027s just a few deltas there\u0027s the URLs for testing it and how to enable it it\u0027s now ported up to the tip of the ministry and what I want to talk about today is all the parts we\u0027ve had to add to DC TCP to meet the what we call the Prague l4s requirements that were agreed in it here in Prague in July 2015 and we\u0027ve renamed them from the TCP prog requirements because they\u0027re not just for TCP so we have now an implementation of this congestion control in quick it\u0027s very early days I mean there\u0027s only built on Saturday so but it\u0027s it took a day and a half to build you know is it most of it was already there it was just too changed and very small change to the Reno algorithm which is good news for quick if you like you know but it\u0027s it\u0027s easy to do these things assuming it works that is that or assuming it works once it\u0027s been fully tested I should say it\u0027s been tested a little here these are these are all those for us prog requirements and where we are with the implementation of them I\u0027m not going to go through this right now cuz I\u0027ve got it again at the end I just wanted to say this is where we\u0027re trying to get to in this presentation and and your what you will see though is that about half of the requirements are met by altering the base TCP stack in Linux and a good half of the remainder in other words a quarter back portable if you like to DC TCP as well but that\u0027s up to the DC TTP maintain is obviously so I am gonna skip this slide I had it hidden but I guess this is the PDF set I didn\u0027t realize that was going to happen the just just because otherwise it\u0027s going to take too long and I\u0027m nearly gonna skip this slide except to say that the accurate easy an implementation there\u0027s one little extra bit on the bottom there which were added to the Linux version for testing which allows you to force the other end to give you fine-grained feedback even if it doesn\u0027t support our ATM so if it\u0027s a classic ecn receiver we "
  },
  {
    "startTime": "01:07:31",
    "text": "can we can certainly for testing I\u0027m not sure I\u0027d advise this for production but it essentially since cwr all the time to the other end which means that whenever it doesn t see any market then immediately turns itself off so you don\u0027t get a whole whole round-trip time and also it\u0027s a test for a bug in bsd as well before it does that which would otherwise give you no marks at all so moving on fall back now I need to make it clear but fall back on loss this this is what I would say would satisfy a RFC 56 81 zealot so this is this is if you want to keep absolutely through the RFC\u0027s that\u0027s what we do I\u0027d like to fall back to something more like baby are as just described and I think now you could think of the two as synonymous depending on well maybe I was lost the the answers to praveen\u0027s question I\u0027d like to hear about but anyway moving on the the full-back on lost to be 58 681 compliant what we realized is that well partly four years ago we realized that there was a bug in Linux but didn\u0027t fall back at all in the DC tcp linux unless you had a time-out but if you had a reach fast retransmit it didn\u0027t even notice it we patched that two years ago but it didn\u0027t go through so we\u0027ve patched it again and we\u0027ve certainly patched it in TCP prog but what we want to do for the future is at least not overreact by having and also having an ACN response so the idea is to as you can see in yeah as you can see here the the ecn and the loss response is a fairly independent so you\u0027ve got the sort of a WMA being calculated this thing called alpha is the calculation of the weighted moving average of the ecn signal in DC TCP and that carries all independently and if there\u0027s a loss you sort of get a round of dealing with that loss and then when it comes back alpha is carrying on without having been affected by it which is different from how that cannot code it and the other thing is we\u0027ve reduced the amount of loss so that when it\u0027s compounded with you matter reduction for the ACN they come to a half rather than doing to reductions in for both signals which is what you see there let\u0027s move on because that\u0027s more in the fifties excited one well right the next "
  },
  {
    "startTime": "01:10:32",
    "text": "one which probably more relates to what Jonathan Morton was talking about fall back to Reno friendly on classic ACN it\u0027s the next requirement and essentially you\u0027ve got this this flow diagram over here where if you get a nice en mark is it from first of all is it from a l4s aqm or a classic aqm and if it\u0027s a classic aqm is it of fq 1 or 501 it\u0027s an F Q 1 you\u0027ll it is protecting every everyone else from you anyway so you don\u0027t have to fall back to the Reno behavior because you\u0027re you know you\u0027re in your own queue so the only case we really need to worry about is this 501 here and on all the or in all these studies from the academic world including our own and I\u0027ll call that academic because I was in similar at the time the ACN marking level found on the internet seem to be virtually near zero and in fact I think the latest one that\u0027s unpublished at Brian Trammell told me the other a couple of days ago I think he found 13 C marks 16 right however I\u0027ve got a I\u0027ve got a suspicion that all those studies are wrong because because they\u0027re not they\u0027re not they\u0027re active studies they\u0027re sending traffic that isn\u0027t big enough to congested yeah to congest the links are only finding the background level when they send traffic or as the Apple data is looking at the traffic all the time it\u0027s passive and so they\u0027re much more likely to find cases when it becomes congested this is a picture of a slide from Padma boomer from March 2017 this black one which showed that they were seeing some CE Marking in certain countries and particularly the Argentine Republic 30% but when you just look at that figure that 30% marking motive but it\u0027s not it\u0027s 30% of Apple devices or at least one mark in 12 hours all right so that\u0027s that\u0027s not saying it\u0027s 30% marking and Padma says that that was that was the signature of that traffic looked like ecn marking like what it wasn\u0027t just something that was maybe diffserv mangling or s to it but it was mainly seen on the uplink and we haven\u0027t got any data yet to dig into that find out whether mainly means nearly always and possibly when it was on the downlink it may have been marked on the uplink from someone else you know like a peer-to-peer flow so because then that "
  },
  {
    "startTime": "01:13:33",
    "text": "determines whether we\u0027re in the always in the FQ side of this flow diagram here and therefore there there isn\u0027t a need to deal with this FIFO case or whether our actually maybe someone\u0027s turned on some old Cisco Reuters with a FIFO queue and ecn red or something like that so that\u0027s that one that was also a hidden slide all right this is a copy of slide I showed in the echo I\u0027m working group when it existed to show that when you remove the cue from you know when you have such a small Q because you\u0027ve dealt with it using a qm + DC tcp style congestion control you actually get a situation where you you start hitting TTP\u0027s minimum window of 2 which is built into the specs and the code because you lose the out clock if you go any lower than that and as you can see from this just five flows in this region here this this block shows the range of possible capacities in the range of possible round-trip times and typical broadband is in this range and typical datacenter is here and anything below the line between red and green there is potentially in a ceiling less than 2 so effectively what happens in those regions is when you\u0027ve we\u0027ve not got a Q TCP can\u0027t go below 2 so it\u0027s effectively become unresponsive because the aqm is turning it go lower and it\u0027s saying okay I\u0027ll go lower and now I ran back up to 2 and then it forces the Q to build because it\u0027s not going lower and so it forces a larger round-trip time by forcing a queue even though the ATM is telling it not to so we we wanted to try and solve that problem by fixing TCP so that it can go below 2 and can go below 1 and so far managed to do it for Reno we\u0027re still got a synchronization problem when we do it with DT TCP that were trying to fix but this is this is with Reno and its user spacing you sort of have to if you\u0027re lost you\u0027re out o\u0027clock you have to send a packet less often than you\u0027re getting ax back so you can\u0027t use that clock and we use a rather than as if increase of one the other so the other problem we\u0027ve got is that when you go below 2 or 1 as your congestion "
  },
  {
    "startTime": "01:16:34",
    "text": "window if you still use an additive increase of 1 you\u0027re aiming at say and 1/2 1/2 a segment that you keep adding 1 + and so the additive increase is too great for the thing you\u0027re trying to aim for so got this sort of scaling every time you change ssthresh you change the constant that you add for the for that until your next change ssthresh so it\u0027s sort of like an additive increase but not a constant additive increase you modify the constant depending on what region you\u0027re in and anyway you can see it\u0027s it\u0027s working on the right ear here you\u0027ve got all the congestion windows of a number of flows and the the right-hand side gives you the the congestion window in segments the left in bytes so you can see you\u0027re you\u0027re working below the two segments previous limit it\u0027s working very well but as I say we haven\u0027t yet got it working with DC TCP and it\u0027s something to do with the way we\u0027re doing the WMA which also has to be adjusted for it\u0027s calculated every round-trip time but we\u0027re not getting a packet every round-trip time because you\u0027re sending your aunt returned so small you\u0027re sending less packets packets less often than your round-trip time and so we have to do the calculation of the value may adjust it for the every time we get an event of a act coming in and something\u0027s something\u0027s not right about it at the moment anyway I won\u0027t go into that here because I don\u0027t need you to debug problems and what\u0027s going to jump over that right the other aspect of TCP Prague we wanted to do is make it before better this is now there\u0027s three slides on better performance first one is to be able to send a TT on the control packets particularly the syn and there\u0027s a draft in the ITF to do that and we\u0027ve implemented that if you well actually no DC TCP implements that already the issue I want to raise here is that when you do that on the public Internet we found from a measurement study that 84% of servers turn off ACN when you send them a syn with ACN on which didn\u0027t help much and what I mean there is that normally ecn is negotiators at the TCP level if it\u0027s TCP but if we set a CT in the IP header while we\u0027re trying to negotiate it in TCP it turns off ecn for the rest of the connection we traced back to a patch that was put into the Linux servers and I understand it\u0027s not in Windows which is good and that probably explains the 84 percent that patch was put in in 2012 "
  },
  {
    "startTime": "01:19:37",
    "text": "and we have submitted a patch on that patch to make it specific only to our Circe 3168 sins so if you\u0027re sending an accurate ecn sin with ECT on the code point the the install base of lyric service will not turn off ecn anymore once that patch has been back ported it\u0027s a one-line patch so I\u0027m hoping it\u0027ll back port fairly widely and that is actually the patch basically we\u0027re just checking more of the TCP flag bits than I can\u0027t be tested in the patch in the previous patch of the patch of the patch tests for the zeros as well right finally the that was a talk on paste chirping to get up to speed faster that\u0027s particularly relevant when you\u0027re using TC tcp/ip style feedback because and and decent tcp style marking when you\u0027ve got such a shallow threshold it\u0027s really difficult to get your slow start under the threshold because you know just a few packets and you can hit the threshold get a nice ian mark and then you you pop out of slow start now you could just say well let\u0027s start averaging the ecn but you don\u0027t know what sort of e Sienna is and you don\u0027t know whether the years ec n is coming from the same bottleneck as where you\u0027re getting delay from so we use only delay and one of the extra bits we\u0027ve done since last time so I\u0027ve just gone back so we got very fast startup without overshoot for the initial slow start or slow start after idle we\u0027ve been thinking well how can we implement that when you in the middle of a flow suddenly want to jump up to a new amount of capacity so say up your two flows in a link the other flow starts and normally hoobat takes quite a long time to get out and in fact typically cubic will take hundreds hundreds of round-trip times before it attains that new capacity if it\u0027s in the middle of congestion avoidance and so what we do we use the property of DT TCP that was on the previous slide and I will go back to it but you get a regular frequent amount of marking with DC TCP unlike something like cubic and so you can tell when it stops very quickly it\u0027s like you know if you\u0027re beating on the head every two years it takes many years before you "
  },
  {
    "startTime": "01:22:38",
    "text": "realize someone stopped beating on the head but if you\u0027re beating on the head every few seconds you know when it stopped within a few seconds and so in this case took off a capacity limit this was a simulation and this was because yo Keem became ill so I\u0027ve had to go back to his last simulations just took off the capacity limit as if a flow was leaving and you can see DT TTP because it\u0027s got the additive increase takes what\u0027s that 60 seconds so that\u0027s 600 round-trip times cubic takes 150 15 seconds 150 round-trip times ignore the pic and the one modified B post chirping gets up in seven minutes seven round-trip times and the queuing delay is still under one millisecond there whereas the cubic even though it takes a lot longer is 150 round-trip times but it causes 20 milliseconds of overshoot so much faster to get get up there and much less overshoot all right but this is research I\u0027m not saying this is production ready because it\u0027s got a it\u0027s got to work in Wi-Fi environment in all weather this is all fixed Ethernet so but it looks very promising all right so coming back to this slide that summarizes everything we\u0027ve got some a few bits and pieces now in the mainline kernel although most of them a large part of the parts of TCP Prague were already there and it was just really a sort of mandatory configuration of all the bits so the dark green parts if your red green caliber and so sorry if your red green colorblind the the bold ones are for the green ones there in the main line already this is Linux the the one I mentioned about scaling down to a window below low to is you know is it\u0027s in progress we out that code is not quite ready to release yet so that\u0027s in a sort of private repository the reducing rtd dependence we\u0027ve presented here before still we\u0027ve only simulated that we haven\u0027t put that all it into the framework and this is the one I mentioned where we\u0027re still wanting to see whether Apple data shows us anything that means that we may not have to do that because that\u0027s just going to make things unnecessarily complicated in the in the code if it\u0027s not actually needed "
  },
  {
    "startTime": "01:25:38",
    "text": "so that is all just a quick one on performance I showed this in TS vwg but I\u0027ll show it again here we go for time okay this is the last slide yeah rodique notes left here umm yeah what six to eight minutes left gives a bit of question time so this shows a complementary C complementary cumulative distribution function with a log scale so you can get CV very low yeah very low levels of queuing delay it at the higher percentiles so the five nines percentiles here the reason for that is we want to be able to have real time media in this in this queue it\u0027s essentially a FIFO queue so we this is showing all the traffic in that queue and what it does with the the DCTC p1 is that blue one these are all paired so like the jewel pi/2 is the blue and red one the fq coddle is the green and purple one and the pi is the and so all of this traffic isn\u0027t there at once you know it for the jewel pi/2 those two are together for the fq coddle there\u0027s those two are together and for the 2 pi ones those two are together because you can\u0027t have more than one ATM in a queue at the same time so it\u0027s overlaid just for comparison and the interesting thing is that the the classic queue of the of the l4 s the with cubic in it is roughly a similar performance as all the other what you might call second-generation ATMs in over the pi e fq coddle so we\u0027re doing no harm to existing traffic and then the l4 s queue at the median you\u0027ve got one one to two hundred microseconds which is confirmed by what Neil showed he was getting similar queuing delay but I think yours was an F Q system wasn\u0027t it yeah I was it okay Oh so when you say coddled you meant coddled not fq coddled yeah oh cool and 99 percentile one to two milliseconds and so on up to eight many seconds there but the important thing to notice about this is this isn\u0027t just a single flow this is 300 web flows per second hammering this thing it\u0027s 120 "
  },
  {
    "startTime": "01:28:38",
    "text": "make link and there\u0027s 300 per second in the L for sq and 300 per second in the classic Q plus 2 long running flows with these various traffic characteristics so this is showing exactly the same thing that Neil explained really and this is the sort of philosophy of both DC TCP and now bbr v2 that essentially you\u0027re you\u0027re keeping Headroom when you\u0027ve got a lot of traffic coming in and out and DC TCP does that by its ewm a so it sees the general background level of marking and a long-running flow will get out the way enough for all the small flows to get in but if there aren\u0027t any short flows or there aren\u0027t any slow starts the longer running flow sort of pulls itself back up and increases the utilization to 100% so I think the intuition for them both even though maybe our v2 is a model-based 1 and d2 TCP is more a deterministic algorithm well if I have to turn this tick that\u0027s not it\u0027s not a good way to it I don\u0027t quite know how to describe the difference but they\u0027re there they\u0027re essentially that I think the intuition is the same so that\u0027s cool I think we\u0027re starting to converge so summary we\u0027ve got very good performance and TCP Prague and I you know I\u0027d be happy to fall back to if I have a look at baby baby RV - if we can maybe have a bit longer to look at it maybe falling back to that is the way to go or you could look at it alternatively as BB rb2 uses DC TCP so it\u0027s pulling it forward I mean it depends which side of a fence you\u0027re on whether you\u0027re falling over one side or the other cool is for questions but Jabba FERS harab and so Delta has three points from Jabra and one is a very good assumption that the see marks from France are due to free door fr as enormous 3 million plus deployment of FQ caudal across their dsr subscriber lines - has no idea about Argentina Oakland Doherty is big there 3bb are currently caps congestion window reductions to 4 on my green oh and he says as 4.1 all someone looking at the day it has to do is pour the IP addresses relates to fruit IFRS BT pas that\u0027s it yeah yeah and that\u0027s actually the conclusion we\u0027re come to that we need to look at the CDN so I\u0027d rather meet in system side to try and get this data could you expand the status page for the implementation so directly large "
  },
  {
    "startTime": "01:31:42",
    "text": "in units of time is that suggestion that we only directed what I realized on that is that the existing RAC implementation as long as you pay space your initial window that meets the requirement we need its then effectively implicitly working in time even though it\u0027s still doing the 3do pack roll because the packets are spaced in time easy incapable TCP control packet so it seems we default on for TCP yes sorry I jumped over that slide because I forgot to make that point the reason it\u0027s it\u0027s default oh that\u0027s that\u0027s just at the moment just because we haven\u0027t written all the fallback code and so until we\u0027ve written all the full-back code we\u0027d rather have it off and you manually turn it on if you want to use it and then once we\u0027ve written the full back code we can make it default on yeah hello guess what you gotta talk about well it\u0027s it has to do a successor qcn bottleneck of course it\u0027ll say let\u0027s keep it quick though cause you have only a minute okay I\u0027m mostly looking at your data saying that\u0027s it that the ecn is there I\u0027m wondering now if that could be do because really where are we I\u0027m not saying it\u0027s wrong I\u0027m saying that the data that said it was rare is wrong uh-huh that the Apple data is the one we\u0027ve got to look at okay I\u0027m cutting the mic here and and let\u0027s let\u0027s go through this Cosby basically optimal under the next talk all right now you can ask quick if you if you just very simply I presume given that you\u0027re working with cable labs that you\u0027ve looked at aggregating max I don\u0027t see anything in the presentation about it it would be nice to see that especially for the Wi-Fi cases yes right okay that\u0027s a large element of what we\u0027ve been doing about so one question so when the van rack now and the latest iteration has that rule in there and the rate just goes high enough then you\u0027re basically reducing shrinking a reordering interval in time again not not if you place your initial window or your entre time but eventually the the rate goes high enough when are you you\u0027ve got a certain amount of packets pasted over a round-trip time when you start that\u0027s all and then later on when you get your reordering window sorted out you\u0027re out hey okay yeah all right "
  },
  {
    "startTime": "01:34:42",
    "text": "thank you so much Bob and so let\u0027s say you\u0027re up next so I\u0027m still astonished makes the research and I would like to present multi time scale branded profile its application for bursts of our furnace so what is this whole thing about we give a definition of fan as a multiple time scales based on betrayed measurements on multiple time scales and we give an implementation we build on core statuses or sharing solutions and we only update the edge marking trees like the time scales in the mark and we show potential advantages and characteristics it is based on fluid simulations assuming easier congestion control behavior so instantaneous convergence basically but it\u0027s we don\u0027t use the congestion control fairness itself itself forced by the COS theta C so sharing and we compare our results to curate 300 mark at the TCM surface so betrayed measurement and time scales bit rate is a derived measure and you have to translate this case packet arrivals to bits right and it always it always has to have a time scale associate is basically volume divided by time and there are natural time scales like oddity like one second like some kind of session duration session duration target maybe one minute ten minutes months most of the time today mainly TVs used OTT and the most again in fairness so how can we have furnace a multiple time scales when we measure a bit rate and we measure betrayed when the source is active we use it to describe performers but we can measure between during both active and inactive periods to judge the fairness of the resource sharing themselves so if you want to have some kind furnace and multiple time scales we can balance the balance all the timescales and consider all the time scales and we can allow a higher share on shorter time scales for flows know their fair share in the longer time scales so you can you can see there are two examples one we have when we have the same fairness on all time scales we have yellow flow and blue flow and the yellow flow starts beating for a longer time so naturally when the blue phone is not transmitting it to get the capacity so it it has five tomorrow so long term but even in the shortest time if they have one tomorrow so sharing so D download of the blue for flow takes quite some time but if you take into account the history of the brief blue sauce you can allocate higher capacity to the blue source temporarily and have improved stupid for the blue flow while having basic the same sweet boot for the ll flow if you if you measure it for the holder of time so "
  },
  {
    "startTime": "01:37:45",
    "text": "what the score status is so sharing there\u0027s an example for that this is per packet value place core stateless is so sharing say so it\u0027s a framework which allows a wide variety of detailed facts about policies it enforces these policies for mobile traffic mixes and scales around with the number of flows there is we have quite amount of publications about this but shortly there is a packet marker in the edge yeah so there is there is packet marker in the edge and there are resource notes in there in the cover of the network and the packet marker encodes all policy and actually flow information into a single value mark mark on each packet and the resource node can base its behavior on the packet marking only you don\u0027t have it has no need for for what what the policies were which flow is it or it doesn\u0027t need separate queues but flow maybe it might need separate queues per delay requirement but definitely not per se so you do the same thing if you are a singer flow if you have only on flows and there are very fast and simp simple implementations so the simplest course status marker is pretty valid it\u0027s a two or a three color marker there is actually a drop eligibility bid which can be set to false for four committed information rate that is that is usually something like guarantee traffic and there is an amount of excess information rate allowed it can be up to the link speed and this these packets are allowed into that word but we don\u0027t say anything about guarantees at all the only thing we do is that we hollow it into the network and then it basically depends on condition control aggressiveness for example which which flow will get which amount of capacity and of course we can drop some packets there so how can we extend it we can we can increase the number of drop residences so instead of having a single sierra and access information read rate we can have more rates and we can control the resource sharing more along these writes some corsica\u0027s proposals have here I can have like hundreds of rate and have a really well fire grained resource sharing among flows but you can also increase the number of timescales so you can have for example if you take this bucket based approach of TR TCM you can have more buckets per drop precedence and in an example I will use in the rest of the presentation we increased the number of the presidencies to four and we increase the number of times cast before also so this is the example we call it multi x cabin with popeye profile mts DVP so we have again for draw percy dances and for timescales and there is a matrix setting the rate parameters of this so we have rates associated with all of these token buckets and we have maximum packet size is calculated from time "
  },
  {
    "startTime": "01:40:45",
    "text": "scales maximum packet size around the time scale x by the rates it\u0027s actually slightly more complex but it\u0027s this is this formula is much better for understanding this and how we decide whether we can mark a packet you\u0027re given the precedence we check all of the token bucket associated with that hope to see this whether there are enough stock tokens in these buckets and decrease of course these McKay\u0027s and marks packets accordingly and we do that with our facilities so we have an example scenario for access and aggregation network we have we have five nodes we have a common bottleneck capacity of ten gigs and the long time fair share of the nodes is to give it per second there are several flows or users in one node but there is only a single bandit profiler and we want to enforce fairness in this in this case in this example among these nodes not among the sub flows of the nodes and what advantage are we looking for we want you have knows with good host history to to access high portion of the but on a capacity temporarily as long as they have good history so for example high peak rates achieved for small birth sites for do small births for those nodes which have good stories it feels like an under loaded system even when it is overloaded and at the same time we want to maintain the motif of time scale fairness V we defined so as I said its defined by my matrix I won\u0027t go into the like very details of this of the design of this matrix this is our B the token buckets this is the matrix and the time scales but I will highlight some of the design features actually there are there\u0027s a link to to our paper about this and you can find the acceptor organs there so what we can do we we actually want to guarantee some bit rate on different time scales so DP one is dropped last so by these bit rates on different time scales you can set which bit rate is guaranteed to a given node then you want to have a 3-2 target for small and medium files in those with good history and some kind of stupid targets for node we still reasonable history so DP 2 is dimensioned in a way that ODB 2 pack is going through then all but one notes are having bet history so you have a single node with good history all the others are transmitting for quite amount of time to achieve the state we call bat history and then the single notes can then the new note can reach this besides and we want to convert the first share what time scales including largest time scale so we have this fair share of seeker and "
  },
  {
    "startTime": "01:43:45",
    "text": "in different places in the mod matrix so what does the time scale mean time scales determine the bucket sizes on the same column so for example two-second determines the bucket sizes on this column and it\u0027s also means how long the bit rates on the previous column can be maintained so we have the first time scale actually set to our tea tea and the other time scales are the time determined based on five sizes and based on definition of active period one day after this amount of activity the note is considered high load so a fluid simulation example in this example we have nodes 2 \u0026 5 having a really really bad history transmitting for quite a while and the roughly realized arrives to the system so for awhile all over his buckets are full so he can reach this 6 Naga feet per second throughput so it starts on time scale 1 and in you can see in this time scale and actually the shakes here are the different personalities so you can see that in at this time scale 1 only VP 1 and DP 2 packets go through and after a pretty short time its 0.1 second here it goes 2 times here too and can reach only 4 second for 4 megabits per second and after after that its transmission stops and the different flows have actually slightly different history and are changing timescales in a different way but how actually DP free is distributed among the nodes is determined by two things the time scale of the nodes on on that the drop acid and the number flair is number of TCP flows and the and the nutria sting thing here is that the the high load notes were much more aggressive having tens of TCP flows why the normal not had a same single TCP flow so we own on the same deeply divided bandwidth on based on the number of flows so we have simulations advantages with this race tomato we had traffic model prefer person arrivals basically to five sizes small and large and the last number of flows per node is also set and we define here or something called nominal load of a node that is the load of the of the node divided by its fair share several owners note is having a nominal or smaller than one meaning it generates less than 2 meter per second traffic in this case and the high load node is having women are a lot larger than one and they also have a system load so we renamed our scenario according to the number of load notes a number of high load nodes and Weaver I the low load mode the load and the load of the high load noise so the system "
  },
  {
    "startTime": "01:46:47",
    "text": "load and we can calculate the high load load from the above to it\u0027s hard to say so some selected simulation results first I have to mention that node is measured only when the node is active so the low load nodes are not always active because they have no relative load below one so if you if you if you add up these numbers you get more than the capacity and that\u0027s that\u0027s the reason and what you can see here we we have PR TCM as the reference case so we have the scenario I I\u0027ve shown TR TCM taking care of resource sharing TFT see I\u0027m only having seer to set T to be able to second and a year can you use the rest of the capacity and we have the multi timescale bandwidth profile matrix I have just shown and let\u0027s concentrate in his to load too low free high scenario when the low load was 0.5 and we increase the system load and you can see that when the system load is below 1 or below 0.9 basically there is there is a realist okay what slots on the figure so it\u0027s again the notes we pick measured when a note is active the axes are the averages and the top and bottom is the 10% worse than 10% best capacity and and what you can see is that there is no big change when the load is small but when the system load approaches 1 or this ha is well above 1 the no load notes see much much higher average they can often reach even even the 6 secondary we said and this is the case when you have to load in the system you can have similar but smaller gains than when there are 4 high load node ads and the singular lloyd nodes and at the same time for the high load notes there is not a big change the biggest change is that actually the reversed one percent go is below the to go for second but the average is above the same actually the the best 10% is better so what you can experience here is that you have increased quality for low node nodes and almost no change for high level nodes and based on based on results in the previous time we drive them or define the measure called experienced system loads for low note notes for multi time scale bandit profile what is that measure is defined as the system load in the equivalent ear TCM scenario when the average node bandwidth for lower notes is the same in that scenario so in the TR t CM so how "
  },
  {
    "startTime": "01:49:49",
    "text": "does it look like so what does like a concrete number 0.9 means here that we have a system load of two in this again one no no more than four high load notes we have a system mode of two but but the average secret seen by low notes is as if as it was in the equivalent EFT same system wrong and as the load of that system was zero four nine so what we can see here is that in all cases basically shown here the experienced system now this verb in a1 and if the load of the landlord not is decreased then this experience is increased so an example here again it\u0027s it\u0027s basically the example I have shown in the previous slide to low note free high load no system noticed you and the experience in that overloaded system is like TRT cm below zero point eight for when for the case when the low node node is 0.5 and 0.65 for the case when the node for lower note is zero point two and like you know by emotionally if you if you increase the number of learners nodes it means that there is a single high load not generating very high system load and that the learners nodes have a smaller percentage of traffic you have even better performance so if you have small if you have smaller load for the loaner nodes then it\u0027s better performance so basically the green ones are below the blue ones that is because those with smaller node have good history much more awesome and also as you as I said as you increase the number of lower notes from a total five five notes even at very high system nerves you experience very good experience if he if you compared to the trt sermon and why is that that is that is because in in in tier TCM access to extract paucity above the fair share was basically based on on condition or aggressiveness and the high load not have a much higher number of flows and they receive much higher share of that extra capacity we actually did some preliminary packet level simulations so this was an idea of fluid model it\u0027s always a question whether it works with the video TCP so we did an s3 DCAA simulations we have five cubic TCPS per node and and was and these results are showing one one second sliding window average and I had to we had to scale down the the speeds here so it\u0027s a 100 Meg bottleneck but otherwise the site the things are the same and and not you can see here is that there are these four ability nodes having bet "
  },
  {
    "startTime": "01:52:52",
    "text": "achieving bet history after a while then a new node comes and it can reach her this perfect history state for a while and after that why it goes to the good history state and then after the 30 second period basically it will be equally it will be equally having a bad history like the others so there will be a an equally so sherry so again in summary we gave a definition of fairness a mutative time scales based on the bit rate measurement with time scales proposed an implementation the really good thing in the implementation is that you don\u0027t have to change the the core part of the course data scheduler is scheduling remains the same you only have to update the eight marking to take into account we take measurements on different time scales and we have shown potential advantages and characteristics you can find much more details about this talk there article also there is quite amount of work in in in about course data see so sharing recently you can find that it there that concludes my presentations we have a few minutes for questions I have a comment on that work we have done similar stuff called ABC activity-based congestion management and this work was motivated by region or Konex because some years ago with colleagues we wanted to achieve fair resource sharing in the stateless no stateless Network and also the effect that you have this time scale stuff so that a flow that starts gets more a larger share than long-standing flow and yeah so this work is basically how could I say pursues similar goals as all the chronic stuff and that means all the use cases that we discussed in the conex context also apply to this to this approach I just want to make sure I understand what\u0027s going on here because this is a pretty different approach to congestion and then most the ones we see so if I understand correctly this is gonna be within a domain you can deploy a thing it doesn\u0027t really need tuning "
  },
  {
    "startTime": "01:55:53",
    "text": "it\u0027s got you know you can you can pick values that are defined here and then regardless so it\u0027s it\u0027s sold for static non-responsive load kind of like like what Facebook in the IRT F open was describing the edge think did you see that one okay so forget that so but it it\u0027s sort of independent of the downstream queuing discipline so even if you\u0027re talking drop tails long we\u0027ve got the static static demand that keeps happening then it\u0027s gonna it\u0027s gonna force it to back off because it just doesn\u0027t get the capacity right understanding that\u0027s really why so I mean it\u0027s an open-loop solution but you do need new do you need to change things at the decor you need to be need to take intake of these dope residences and that you have to change and and our normal queuing these siblings today or for a single drop residence okay so why by having for example in in this work forward or presidencies in some of our articles we have actually 65,000 so by having a number of drug presidencies you can you can control resource sharing in a in a openly play you have the support and and we went down to four because we believe that this kind of for a person\u0027s behavior can be approximated with with the right configuration of unit hardware if you can figure out according to these differences that\u0027s very interesting thank you yeah but Brisco yeah I wanted to just come back on what Michael said about a PC and Clinics and so on and yes you\u0027re right Michael it it they have the same aims and I think it doesn\u0027t need comics to do what you\u0027re doing cuz you\u0027re doing this at a 1:1 node yeah it\u0027s not for a network you know it is supposed to be for a network so if you if you mark your packets according to that marking at the edge and all your audio core but it\u0027s just using local information it\u0027s not using any information from anywhere else in the network it\u0027s it realizes the use case different yeah and and so I think what you\u0027d done Michael with ABC was do it at a single node and then you don\u0027t need comics and everything you\u0027re just using the local congestion information at your local node to do the multiple time scale thing by so what you need for drop decisions is both the local the local congestion information plus the packet value so it\u0027s called here the packet value and the local congestion information that\u0027s "
  },
  {
    "startTime": "01:58:53",
    "text": "the input for the decision-making whether to drop a packet and and actually I think as well as ABC there is a commercial implementation of something like a ABC that L occurred over there traffic management as well so you might want to look I don\u0027t think they publish anything about it but I did say I could say it existed thank you so much and I\u0027ll give you a minute back folks thank you for showing up here and we\u0027ll see you again next time on trail in the meanwhile keep the list busy as you have "
  }
]