[
  {
    "startTime": "00:00:04",
    "text": "this one I see Barry right yeah it's uh pretty empty room so far yeah so it's a little awkward doing the uh the chairing when the chair is remotes but the speakers are local I don't know if I hope the speakers are local [Music] [Music] foreign foreign"
  },
  {
    "startTime": "00:02:39",
    "text": "foreign foreign questions um I see some human touches along with"
  },
  {
    "startTime": "00:04:03",
    "text": "the other presents are in the room exactly foreign"
  },
  {
    "startTime": "00:06:01",
    "text": "laughs okay so okay I don't know"
  },
  {
    "startTime": "00:08:12",
    "text": "thank you okay I could get a few minutes you get started it's uh there and if so do you want to come up and get ready for the microphone while I do the introductory slates [Music] thank you okay Tasha can you do another check I see you're there now it's a cute little camera to work foreign"
  },
  {
    "startTime": "00:10:06",
    "text": "Perkins I'm the irtf chair hopefully you cannot see it and hear me in the room I'm remote foreign slides um okay let's make her but hopefully it's not too bad uh so uh this is the irtf open meeting the the irtf follows the itf's intellectual property rights disclosure rules uh and a reminder that by participating in this meeting and by commenting on the presentations that you you agree to follow the irtf processes and procedures including disclosing any intellectual property relating to the contributions that you make uh I'm sure most of you have seen these slides before the the details are in the documents linked but uh essentially if if you have IPR on the documents you're talking about you you need to disclose that if you're commenting a microphone in addition a reminder that uh the iitf routinely makes recordings of these meetings uh available both the the online and the in-person person meetings including this one and this meeting is being streamed uh live on YouTube as well as via the usual meat echo system um if you're participating in person and you are not wearing one of the red uh do not photograph lanyards then you can send to appear in these recordings and if you speak at the microphones um then again you're consenting to being"
  },
  {
    "startTime": "00:12:00",
    "text": "recorded and as I say the recording is being made available on YouTube equally uh if you're participating online and you turn on your camera or your microphone that will make a contribution then that is being recorded and you can consent to being recorded and also the the chat is also being recorded and will be made available in the the usual jabber archives all right that's a participant in the iitf as I say you uh acknowledge that uh recordings of the meeting may be made available and that the previous that any personal information you provide will be handled in accordance with the privacy policy and you also agree to work respectfully with the other participants in the ietf and the irtf and if you have any uh issues or concerns about that speak to me or speak with the onwards team uh and the the itf's code of conduct and anti-harassment procedures uh linked on the slide also applied to the irtf for those of you participating in person um please sign in using the the mobile meter code that the meteco light Tool uh we're running the queue electronically so if you have questions then we're using the electronic queue that's accessed via the meteco tool um and keeps the audio and video off if you're using the on-site version that the meateker light tool uh remote participants uh please leave your audio and video off and unless you're you're presenting um uh or asking a question uh just to avoid feedback and also a reminder for those of you who are attending the meeting in person uh as a covet safety measure the ITF is requiring those those of you attending the meeting in person to wear an ffp2"
  },
  {
    "startTime": "00:14:02",
    "text": "and 95 mask or its equivalent uh and the only exception for that is the the chairs and the presenters who are actively speaking uh in particular participants who are making comments or asking questions from the floor microphones are expected to wear a mask at all times including while they're asking those questions as I said the only exception of for that is the the active presenter at the front of the room okay so uh as I say this is the the irtf open meeting uh the goals of the irtf are to complement the standards work being done in the ietf by focusing on some of the longer term research issues uh the iitf is very much a research organization it's not a standards development organization and while it can publish rfcs and and we we do publish both experimental and informational documents on the RFC series that the primary outputs of the irtf is research is understanding his research papers the irtf is organized as a series of research groups um hopefully you you can see them on the slide here the the crypto Forum group and the uh privacy enhance enhancements and assessments groups met earlier today um the the other groups men uh so highlighted in dark blue on the slider meeting later in this week uh so please do um look out for those groups uh this week and try and attend the sessions if you're interested in those topics a little bit of research groups news uh I'd like to welcome Curtis heimerel who's recently joined as coacher of the Gaia group the global access to the internet for all research group um Curtis will be joining uh Leandro and"
  },
  {
    "startTime": "00:16:02",
    "text": "Navarro who is um planning on stepping down from from sharing that group after this meeting and Jane coffin who is continuing so I'd like to welcome uh Curtis uh and um thank him for his service and thankfully Andrew for his his many years of service to the group I very much appreciate the efforts the Endo has put into chair in the group and I look forward to to working with Curtis going forward so thank you both thank you both did I say the irtf is primarily a research organization which have not published many rfcs we've had one RFC uh published since the last meeting um from the information Centric networking group looking at architectural considerations for using an ICN main resolution service but primarily the the iitf tends not to publish much in the RFC series and the output is more in form of interesting presentations and understanding and research papers foreign networking research price and the the goal of this prize is to recognize that some of the best recent results in applied networking research uh is to to recognize some interesting new ideas which are potentially relevant to the internet standards Community going forward is to recognize up and coming people who are likely to have an impact on the internet standards process and internet technology uh we're very grateful to the internet Society to Comcast and NBC Universal for their sponsorship of the anip that allows us to make these Awards"
  },
  {
    "startTime": "00:18:01",
    "text": "bring to bring people to give these thoughts and uh what we're doing today is uh the goal of this session is to to make some of these Awards so I would like to congratulate uh Tasha Swami and some Kumar who will be giving that there are World talks this session today um Tasha will be talking first in a couple of minutes uh talking about his work on data plane architectures lots of the line rates in for insurance um and Sam will be following later in the session talking about TCP low powerlessness uh we've got two really really good talks coming so please do uh please Instagram pay attention to those and again congratulations to Natasha and to sex going forward um look out for a little bit more more talks uh go to some current cash and Daniel Wagner will be doing the talks as it f115 and the nominations for the um nominations for this for 2023 Awards will be opening in September 2022 so to look out for those um and we'll care for the nominations okay did the audio improve trip meeting and restarting hopefully you can hear me okay okay hopefully that's better as I was saying look out for the nomination to the 2023 anrp um in September this year and um congratulations to tusha and to Sam who will be giving their their NRP talks today"
  },
  {
    "startTime": "00:20:02",
    "text": "in addition to the applied networking research prize we also host the applied networking research Workshop which we organize in conjunction with ACM sitcom this Workshop is taking place tomorrow it's co-locating with the ITF in Philadelphia so thank you to TJ Chang and Marwan fired who the chairs this year and who've been organizing that Workshop um we've got a program of uh I think there are four four really nice research papers a keynote and some Innovative talks on novel approaches to protocol specification as I say that the workshop's happening tomorrow um if you're there in in person then please do consider attending if you're attending remotely then you can register and attend um registration is free for anyone who's also registered for the ITF although we we do ask you to to register separately so we know who's attending the workshop um and the anow next year will be again co-locating with the the ATF in July 2023 um which is planned to be in San Francisco and to finish up before we get to the talks uh I'd just like to um note that we we are very pleased to offer a number of travel grants for these meetings um both to support early career academics and and PhD students from underrepresented groups to to attend the irtf research groups and a number of travel grants for the applied networking research Workshop thank you very much to the travel Grant sponsors to Akamai Comcast cloudflare and Netflix for supporting that um if you're you know please see the the travel credits page linked from the website um the details of that and if if you're"
  },
  {
    "startTime": "00:22:01",
    "text": "interested in sponsoring the travel grants in the future or if you're interested in applying for a travel Grant see that webpage or contact me for for details of the sponsorship opportunities and again thank you very much for the spices so that's uh essentially all I have to say today um the agenda for the remainder of the day um we have the the two anrp award talks uh Tasha Swami will be first talking about Taurus a data playing architecture for per packet machine learning and that will be followed by Sam Sam Kumar's talk on performance TCP for for low power wireless networks okay um I will at this point switch over to uh Tasha can you check the microphone when I get the slides up yes it's okay uh yep I can hear you remotely is it working in the room should I get started yes just one one second if you have a phone I can pass you control so you can control the slides yourself if you have the meat Echo light uh if not then um shout when you want to go to the next slide okay so I should have control over that um well uh Tisha is checking to see if that works uh I'd just like to say that uh the as I said the first talk hey it's Tasha Swami who'll be talking about Taurus a data plane architecture for per packet ml uh Tasha is a PhD candidate in the electrical engineering department at Stanford uh his research is focusing on"
  },
  {
    "startTime": "00:24:02",
    "text": "the intersection of machine learning networking and architecture and he works on the hardware software stack for data plane based machine learning infrastructure and applications uh Cheshire is due to graduate this year I understand he's on the job market so uh if if you like this work then please do uh talk to him uh he'll be around at the ATF all week and if you find this talk interesting uh I believe he's also going to be presenting in the koinagi session later this week um Tasha over to you awesome thanks Colin uh cool so um I'm going to be talking about Taurus which is a project that uh me and my colleagues have been working on and so Taurus is essentially a data plane architecture for per packet machine learning and foreign that means all right so this here is a quote from a 2015 Google Blog and at that time uh Google is already dealing with uh over one petabit per second of total bisection bandwidth um and it's only grown larger and harder to scale since so what we're essentially dealing with here is a situation where networks require more and more complex management with higher and higher performance um and so it's uh the time is ripe for finding new Solutions here and uh one of the promising Solutions in this area is machine learning so machine learning can allow us to um essentially take in data from the network and make progressively better and better decisions as we train our models and these machine learning algorithms can approximate Network functions based on the data they see and they're also going to customize their operation to the data that they're training on which in turn means that"
  },
  {
    "startTime": "00:26:02",
    "text": "these machine learning algorithms are actually customizing their models to the network itself and so we're sort of uh doing elements of this already with handwritten heuristics in the network so something like an active queue management algorithm or uh hashing and load balancing and playing with operator tune parameters so all machine learning uh is doing here is taking the next step by automating the um the search for these kind of parameters that allow to work well within your network so uh if we're okay with using machine learning we now need to examine where exactly in the network it has to happen so I'm sure many of you already familiar with software-defined networks essentially the control plane and the data plane are split and the control plane is responsible for policy creation um essentially in the form of flow rules which are installed into a data plane where that's where you're going to find your switches and they're doing packet forwarding via match action so um right off the bat there are two good candidates for where we should operate with machine learning and uh on the left here I have a diagram of the same typical defined software defined Network but on the right uh I have a software defined network with the Taurus worldview and so what we've actually done here is we've split the machine learning operation into training which is going to happen in the control plane and then inference which is going to happen in the data plane so in the control plane policy creation is going to take the form of flow rules plus ml training and when installing this information into the data plane it's going to be sending flow rules as usual but also the ml model weights and in the"
  },
  {
    "startTime": "00:28:02",
    "text": "data plane we're going to be doing our typical match action packet forwarding but we're also going to be doing decision making with ML inference and so that brings me to one of the core tenets of Taurus and that's essentially that ml inference should happen per packet in the data plane um and so the the intuition here is relatively straightforward you want to be able to do per packet operation because that is the finest granularity of traffic essentially operating on a packet scale now not every application may need per packet level operation but the there are applications that need it and so the platform should be able to support per packet operation and then the data plane that's where the packets are so if we're going to be doing decisions on packets it should happen in the data plane oh I think uh PowerPoint animations don't play well with the PDF um so that's okay uh what what's basically happening in here is that um if we were to do just a rough off the uh off the cuff math here so you have traffic at one gigapacket per second moving through your data plane now in the time it takes you to send a packet digest from the data plane up to the control plane calculate flow rules and then install it back into the data plane in this case we've assumed um a half millisecond for each step so we've now missed 1.5 million packets in our traffic stream by the time uh we had flow rules installed into the data plane so in the example here we're doing anomaly detection so we're trying to find out if incoming packets are malicious or benign and maybe if we find that it's malicious we're going to install some rule to say block that IP"
  },
  {
    "startTime": "00:30:02",
    "text": "um and by the so if we've missed 1.5 million packets during this flow rule installation time by the time we block that IP we've already let a ton of potentially malicious traffic into the network so the whole takeaway here is really just to show you why we can't let our operation for these kind of this level of application happen in the control plane and if we're committing to using machine learning we can't have inference happen in the control plane so fundamentally the conclusion here is that the robustness and performance of your network are going to be determined by the quality of your reaction and the speed of your reaction so in the machine learning worldview the quality of the reaction is going to be determined by your training data so how much do you have what kind of cases does it cover how well is it cleaned but also your speed of reaction so in the case of the anomaly detection um you want to act on a malicious packet the moment you see it you don't want to have to go to the control plane and come back and install any sort of flow rules and this is essentially the per packet operation in the data plane so zooming in on uh the control plane let's talk a little bit about the actual implementation of how you do this so I mentioned before that we're going to split our machine learning uh into training in the control plane and inference to the data plane and so the key here is that training is off the critical path if packet forward is Packet forwarding is happening in the data plane then um the control plane is not uh responsible for making uh per packet level decisions which means that we can do our machine learning training there at leisure and um essentially we can we can put in"
  },
  {
    "startTime": "00:32:00",
    "text": "whatever the latest and greatest ml accelerators are whatever your favorite ml framework is installed in a control plane server and have it training models offline the trickier part comes in The Next Step where now uh we need to deal with the actual critical path basically um tackling packets as they come so machine learning interference here is going to happen in the data plane like I mentioned and the the final outstanding question here then is if we're okay with doing uh training in the control plane we can use whatever existing Hardware we want and then what do we do about the data plane do we have say a switch that can do inference at line rate per packet operation and so this is really the the Crux of Taurus and that's what it aims to do so tortoise is an architecture for per packet machine learning inference in the data plane so uh let's jump into the actual Hardware um and how we enable this kind of machine learning inference at line rate so I have a picture here of a piece of pipeline a protocol independent switch architecture so this is the typical uh programmable structures you'll find in these kind of switches so some sort of programmable packet parser match action tables that allow you to encode your network functions and then uh maybe a programmable traffic manager so we're going to actually keep most of these elements and just make a modification of additional Hardware that'll allow us to do our machine learning inference um but the natural question is if we're committing to adding Hardware into the switch pipeline"
  },
  {
    "startTime": "00:34:00",
    "text": "um what does that look like and more specifically what is the abstraction here with which we're going to create our programmable machine learning Fabric and so in Taurus we use the mapreduce abstraction so mapreduce is really useful for machine learning because it supports a lot of the common linear algebra operations that you need for your ml algorithm so this covers everything from neural network support Vector machines k-means all these different kind of applications and just as an example I have here in the picture um an example of a single neuron from a deep neural network so you can see exactly how map and reduce her applied here in this case in the blue box um we are doing an element-wise multiplication that's our mapped with multiplication with inputs and weights and then we're applying a reduction and so this is going to essentially add all the values together and um you're going to produce a scalar value from your vector of inputs and then finally we're going to apply an activation function and so that suffices for a single neuron but you can mix and match this pattern ad nauseum uh to create a full neural network so by stacking extra of these blocks um in parallel you'll be creating a layer of neurons and then stacking them sequentially you'll be creating multiple layers and so that's how you can create say a deep neural network so the other advantage of the mapreduce pattern is uh comes from the kind of performance that it enables primarily it's a from the the simdi parallelism that same instruction multiple data so we can get a lot of performance out of the parallelism with minimal logic and this is as opposed to what you might find in a say like a a typical like"
  },
  {
    "startTime": "00:36:02",
    "text": "Tofino pipeline where they have vliw pipelines which give you much more flexibility but the cost here is that there's a lot of logic that's needed for the communication hardware and um that ends up taking up a lot of the the overall on-chip area and uh in addition simply parallelism gives us the ability to unroll the loops in our uh in our algorithms so the the idea of unrolling here if we take the example of um say a single layer of a neural network and say you have four neurons in your network you can either um execute them sequentially you're doing one neuron after the other or if you have the resources you can instantiate all four of them in parallel and so the trade-off here is that more on rolling is going to give you better performance essentially doing all four of those neurons at once while less enrolling means you only need the hardware for one single neuron's worth of operations but it's going to take you four times as long so it's less resource intensive but it's also uh uh much less um a much higher latency and uh but we get this kind of control with the CMD pattern by um essentially un adjusting unrolling factors so we went ahead and we uh essentially adjusted the switch Pipeline with a mapreduce unit that uh implements the patterns that I just described so the the a we still have our typical programmable elements we have a programmable packet parser match action tables and traffic manager but you can see in the center we have this mapreduce unit and that's essentially what's going to do our machine learning inference and"
  },
  {
    "startTime": "00:38:01",
    "text": "so there are a couple uh little idiosyncrasies about the um the arrangement of the pipeline that I want to point out and uh and that's how we use these different elements for machine learning context even if they're typically Network elements so a packet parser is normally for pulling out your headers from your packets um and doing whatever you want with your match action rules in this case packet parsing is also pulling out the features for our machine learning inference and then we have match action tables before and after the mapreduce unit and so these are doing different types of rule-based pre and post-processing on our machine learning inputs and outputs so uh the match action tables before mapreduce can be doing some sort of cleaning on the features and then the match action tables on the output on the right side of the mapreduce unit can be doing some sort of interpretation of the results and uh so when we actually went to design this mapreduce unit um there's a couple of things that came up it turns out you can't really just stick an accelerator into the switch pipeline so uh what we did was we kind of established what were the the points that we wanted our mapreduce block to fit and so most of all we wanted it to be reconfigurable so essentially you should be able to program it it can't be a an Asic for a single type of machine learning application you should be able to put in whatever or program whatever application you want oops and um it has some neat line rate with the fixed clock so this essentially uh rules out in fpga because in fpga will give you a variable clock we want it to be deterministic um and of course line rate is our performance requirement and then minimal area and power overhead we don't want to"
  },
  {
    "startTime": "00:40:00",
    "text": "blow up the entire chip area adding in like a mapreduce block it should be something that is a small but gives you access to a whole class of applications and so finally the one little thing to note here that's kind of interesting is that most of these ml accelerators are built to do uh batch processing in an effort to get high throughput but in the network pipeline you're actually processing packets as they're coming which means that you're operating on a batch size of one um which is uh turns out puts a lot of different performance demands on the hardware than a typical accelerator would see uh yeah so um I have a quick example here just to make this a little more concrete going back to anomaly detection um so you can you can uh imagine say a packet coming into the switch Pipeline and we want to see essentially whether it's malicious or benign so the packet hits the first stage and that's where we're going to um do our packet parsing so we're going to read local features say our IP whatever information we can extract from the packet itself the packet is going to move to the second stage which are the match action tables and from there maybe we're going to do some sort of uh retrieval of out of network events so these would be different kinds of uh elements of metadata that the control plane may have installed into the match action tables so something like the failed logins per IP the packet then moves to the the center block of the mapreduce unit that's where we're going to apply our learned anomaly detection so you can imagine this is maybe a binary neural network and it gives it a a score from zero to one on how anomalous it is so one is definitely anomalous zero is benign and finally the match action or the the"
  },
  {
    "startTime": "00:42:01",
    "text": "packet will move to the post-processing match action tables and that's where we do our interpretation so say we got a score of 0.8 so it's pretty anomalous and uh now we want to drop it or quarantine it this is where the match action table will set a rule for that such that when the packet now moves to the traffic manager it's going to go to the appropriate destination thank you so uh in the paper we actually did a full um Asic analysis of uh this Taurus hardware and how we can um we wanted to show essentially that it has minimal overhead and it's feasible to to build something like this and so we based our evaluation platform on a coarse grain reconfigurable architecture called plasticine and we programmed our applications in the spatial Hardware description language and so spatials is just an HDL that lets you use these kind of uh parallel patterns like map and reduce to program your um your your reconfigurable architectures at the Loop level and so the the basic architecture the mapreduce unit here is really just a grid of compute and memory tiles so easily scalable and very very straightforward in the compute units we have Cindy lanes that are operating in parallel and a reduction Network that allows us to implement the reduce operation and the memory units are just blocks of banked SRAM so uh we're doing severe pipelining within the compute unit but then we're also doing pipelining one level higher between the compute and memory units so the AED here is simdi parallelism everywhere uh and then pipeline parallelism everywhere and that's how you get your performance really"
  },
  {
    "startTime": "00:44:00",
    "text": "so uh we went through a set of real world applications and um program them onto our Asic and we ended up using a 12 by 10 grid to support all of them and um we compared it to state-of-the-art switches with four uh pipelines and um our reference which was 500 square millimeters and we found that our grid which could support these different applications was only adding a 3.8 percent overhead or 4.8 millimeters per pipeline so um again earlier I said we want minimal area overhead so 3.8 is pretty low given that you're now getting an entire class of machine learning applications um and jumping into one of these uh applications I've been using anomaly detection as a recurring example here um we tried out two different types of anomaly detection with support Vector machines and deep neural network and so uh for both models you can see in the throughput it's one giga packet per second which is the line rate for um high-end uh switch pipelines like your tofinos and broadcoms uh the latency that we added was in the hundreds of nanoseconds or less so in this case you would choose your application you can see here that the bsvm requires 83 nanoseconds while the DNN requires 221 nanoseconds so depending on your slos and what kind of requirements you have to meet you can choose your algorithm to reduce latency um and then in both cases the area and power overhead required for the hardware to implement just these applications is um single digits or uh or less a 0.6 power overhead point five percent area overhead or 0.8 and point uh and 1.0"
  },
  {
    "startTime": "00:46:02",
    "text": "respectively uh again if you don't need um say the full Suite of benchmarks you only want a reconfigurable fabric that will let you do anomaly detection you can do it with um minimal overhead here and so in the paper there's a several more applications if people are are interested such as a congestion control Network and a traffic classification Network so uh we went through this whole process of doing an Asic analysis to prove that it could be done um but as far as research goes we don't really want anyone waiting for some sort of mass-produced Taurus Asics so we've put out an open source fpga based test bed um and so this is just a rough diagram of what it looks like at the control plane we're using your typical Network OS like onos we're using a Tofino switch to to mimic the piece of pipeline elements like your program mobile packet parsers match action tables and traffic managers and then we're using an fpga to um uh to mimic the mapreduce unit so we set it up in this bump in the wire configuration um and so uh because of the limits of an fpga you're not going to be able to hit the same performance as you're going to get with the Asic core screen reconfigurable architecture but it's there to serve as a proof of concept for the functionality so just a quick demonstration of this test bed um we did an example essentially the example I mentioned earlier about anomaly detection where we're trying to do uh detection of"
  },
  {
    "startTime": "00:48:00",
    "text": "anomalous packets in the control plane or we're trying to use Taurus and do anomaly detection in the data plane and so with the test bed that I just showed you you can do um you can do either so you so in the case of Taurus we'd be uh placing our anomaly detection application on the fpga while if we're trying to do control plane based anomaly detection we would run it at the uh the controller on the CPU so uh the takeaway here is is the same sort of message on um why you really can't use the control plane for efficient um machine learning just based uh decision making and um if you take a look at the very uh last two columns the F1 Square um now this is the F1 score for the model when it was implemented on the Baseline which is control plane or Taurus which was in the data plane and in software in tensorflow uh the F1 score is 71.1 so you can see that Taurus on the far right side of the uh the the uh the table is achieving an F1 square of 71.1 so it's Faithfully recreating the model as it was in software and um we're processing packets as they're coming in whereas in the control plane we actually had two sample packets from the network and um run it through the control plane and run it through an ml framework and try to install flow rules and what ends up happening is that you miss so many packets while doing this operation that your effective F1 score drops pretty heavily so you can see on the far right column the F1 score for the Baseline ranges from 1.5 to almost almost zero so"
  },
  {
    "startTime": "00:50:00",
    "text": "you're effectively throwing away your model because of the added latency so that's just uh one example of what you know what we did with our fpga test bed um there's of course lots of other things you can do but the just to reinforce the point why you have to operate in the data plane cool so yes that's mostly it uh for me um I have my contact information here and I have at the bottom the gitlab link for the fpga test bed we hope people wanna can try it out and there's the link to the full paper in this easy to memorize URL so uh yeah I'm happy to take any questions okay uh thank you very much yeah the excellent talk um since we we have a some people remote some in the room I think if if we can manage the queue using miteko uh let me take a queuing tool I think that would be helpful uh I do see I guess it's Barry at the microphone there okay actually I'm being uh Dave Oran right now um Dave Aran asks I assume the class of anomalies you can detect are those that can be detected by header Fields within the width of the ALU of the switch things in the packet data beyond the headers won't be seen is that correct um so the in the case of anomaly detection we used uh the KDE NSL data set which had a a record of different um attacks that were calculated from like you said either header fields or um you can also actually calculate aggregate fields from across headers so you can um uh say create like a histogram using the matte checking tables across different packets um and the the packets the uh the packet"
  },
  {
    "startTime": "00:52:03",
    "text": "headers um are going to be limited by the packet header Vector size that's moving between stages in the switch pipeline but you don't necessarily need to be limited to features in the header because the control plane can install different types of metadata into the magician tables and you can do your own processing in the match action tables over time or whatever other kind of calculations you want to do on your headers so the headers are just the starting point for the the features here hi is this working yeah George Michaelson can I expect to can I sneak two questions in is that okay yeah so the first one and this is the naive attendee question I suspect the paper is very important for interpreting that last table it was really quite opaque how to understand the meaning of the columns and their impact on a comparison to the Baseline I think there's a lot of implicit knowledge in your table structure sure the paper explains it the slide where it was just a bit of juice to a naive reader so at the start of your talk that was the first point you made a case to say that the delay between doing a packet sample constructing table match rules in the controller injecting those rules down into the functional plane and applying them had a huge packet loss and mismatch interval but it seems to me the delay to perform the ml operation and tune your ml have a model that is representative of the condition you want to model and then install that has a similar cost it's not to say there's no benefit of ml I think it's huge but the component that's about the delay cost of doing an"
  },
  {
    "startTime": "00:54:00",
    "text": "instantiation of rules I don't think is a basis of doing it I think you're on stronger ground arguing it's about the ability to do complex match at line rate than the static cost of doing the rule installation right so um the installation you're right about the installing the model itself so the idea is that you could be taking sampling uh packets from the your network and be sending different kinds of metadata to the control plane and essentially be doing your training offline and you can install model weights or replace model weights as uh as needed the idea is that whatever is operating in the data plane itself has nothing to do with the installation of model weights yeah completely agnostic and I thought I thought that idea that you could do the model training asynchronously the sample exactly is very beneficial but if you consider a new class of attack attack that you have to understand it and do some form of Bayesian analysis and classification which is completely unmodeled here exactly how you do that training unknown how long that takes it's not about the speed of the chipset it's about your ability to do the good bad classification a priority to inform the model and then download it that's quite a high cost in time yeah so so this is always like kind of the uh the trouble with security rate like if you want to do an on-the-fly analysis of a brand new attack that's not really uh what we're what we're targeting at the moment here but uh but in engineering terms your case this is extremely fast that line right well made I enjoyed listening to it a lot thank you thank you so excellent work to share I have a question during machine learning and datablane will consume more energy oh sorry I can't uh using machine learning and the datablane"
  },
  {
    "startTime": "00:56:01",
    "text": "will consume more energy so and we would like to reduce the energy consumption of filters and switches so have you looked at this issue yeah so I think the the for energy consumption needs to be looked at maybe more holistically so while you are increasing by some small percentage the energy that you'd be consuming in the the switch itself you can consider that say if you're doing anomaly detection you're removing the cost of running an anomaly detection application and software on a server somewhere else so with this like specialized Hardware here you're consuming less power in the switch than you would running it in software elsewhere so on the whole you're reducing power cost but for the switch itself yeah you'd be increasing it minimally okay thank you okay thank you uh questions and I guess I'll wait a question uh I mean you know this this is a an iitf uh meeting which is which is co-locating with the ietf um and obviously you know since this kind of case with the ITF the question is then you know to what extent have you given any thought towards how the how this might change or affect the type of work the ATF does are there any implications of these types of systems for the way way we design standards or other types of protocols the ATF designs or is this just a an optimization that fits within the existing architecture yeah I think uh so one of the the things that uh actually uh hashemu who uh asked a question earlier"
  },
  {
    "startTime": "00:58:01",
    "text": "um brought to my attention was that was what kind of um standardization is needed for packet headers if we're going to be using them as features or carrying model weights or basically doing kind of this um like ml uh ml assist type operations um so I think there's probably something there as far as making a cleaner definition of what what has to happen at the uh the the packet standardization level um to support this kind of machine learning and make it uh easier for different different types of ml systems to interoperate [Music] yeah that makes a lot of um presumably there's also something in terms of the control plane and the standardized um so programming model for that in order to to to specify the the model is that right uh sorry uh um I I mean I'm thinking that your traditional programmable switch uses P4 or something like that as a programming model do do we need a similar standardized programming model for these types of ml switches oh yeah so um yeah so so is like kind of a compliment to to P4 um we went with mapreduce so we're not necessarily married to the idea of using um a map produce block or anything the the bigger idea here is just doing inference in the data plane um so but yeah it could definitely help to have some sort of standardization in the way that people works but for um the the mapreduce elements so uh you could even consider like an extra control Block in P4 as mapreduce and we actually we have another paper intermission on um what the the language level constructs here look like so yeah"
  },
  {
    "startTime": "01:00:01",
    "text": "there's it's definitely an area for standardization as well all right great thank you are there any any final questions okay uh I don't see anything thank you very much all right Sam if you can come up while I try and share the slides thank you all right can you see the slights yes we can see the slides here okay all right so with any luck you should now have control over the oh slide so they gone away that's is it working I can see it on my phone oh yeah I see I had to Click Share okay there we are just took a little while yeah okay okay great all right so the second talk today is um focusing I think on a very different problem domain um so in this talk Sam Kumar will talk about his paper on performance TCP for for low power wireless networks uh this was originally presented at the nsdi conference in 2020 if I if I remember correctly uh Sam is a PhD student at UC Berkeley"
  },
  {
    "startTime": "01:02:01",
    "text": "uh advised by David color and relax he's broadly interested in system security and networking and his research focuses on rethinking systems designed to manage the overhead of using cryptography and presumably also improving the performance of TCP for low power wireless networks so um Sam over to you okay um thanks Colin for the introduction um as you said I'm Sam and I'm going to present uh my research on performing TCP for low power wireless networks and this is a joint work with macular Arbiters at UC Berkeley uh and as you mentioned it was published in 2020 at nsdi so I'm going to begin by giving a brief overview of of history of research in low power Wireless personal area networks or low pens uh to put our research in context so Loop band research began in the late 1990s and at this point in time researchers deliberately Cast Away the internet architecture based on the idea that the load pads may have to operate in two extreme environments and two different uh from regular networks in order for the internet architecture to directly apply so many of the early protocols like s-mat dmac and so on and the early systems like tiny OS and contiki did not conform to any particular standard or architecture and this allowed the researchers to nicely explore how to tackle the challenges of Lopez chance without being hindered by having to conform to an architecture about a decade later in 2008 IP the Internet Protocol was first introduced in the space largely enabled by the six low pan adaptation layers standardized by the ietf and what happened here is that people found ways to take the lessons that were learned in the earlier systems and applied them within an ipa-based architecture and this essentially caught on in a few years by about 2012 IEP had essentially become the standard in the space but surprisingly the adoption of IEP did"
  },
  {
    "startTime": "01:04:00",
    "text": "not come with TCP for example open thread a low pan Network stack developed by nest and used in uh in the smart home space didn't even support TCP and instead the community has come to rely on protocols like co-app which are specialized low-fance protocols based on UDP also worth pointing out that during this time low Pens have not yet achieved the same kind of pervasive adoption that we've seen in other protocols like Wi-Fi at least in the context of branding internet access to devices so a natural question is whether to get that kind of pervasive adoption of low pens we should adopt not only IP but also the broader set of Ip based protocols including TCP in this context our work completes the transition of low pens to an ip-based architecture by showing how to make TCP work well in low pants and a research artifact is tcplp a performant TCP stack for low pens so what exactly do I mean when I say performant well one metric is good but and that's the amount of bandwidth that an application is able to get when operating over a TCP connection now there have been a few prior attempts to use TCP in the space typically based on a simplified embedded TCT stack like micro IP or blip and what we can see in this graph is that our work tcplp achieved significantly higher good but than prior attempts to use TCP in this space in fact we can calculate an upper bound on goodput shown by these dashed lines based on measurements of how fast the radio can send out packets and how much overhead is lost to headers and acts and so on and our work comes quite close to these upper bounds um I'd also like to share an update that's happened since we published This research which is that open thread the low power Network stack that I mentioned that's used in the smart home space we simply adopted TCP directly based on our research it uses tcplp as its TCP implementation and the research also influenced thread the network standard"
  },
  {
    "startTime": "01:06:00",
    "text": "that open thread implements so I'm delighted to have been invited to spear help spearhead this process and I am hopeful that uh that the adoption of TCP in this space will help improve the adoption of low pants more broadly in the smart home space so now I'm going to take a step back and provide some more context as to what exactly low pens are and what some of the challenges are with using low pens and I can do that by comparing low pens to other Wireless technologies that you might be more familiar with so on the left Wi-Fi provides a host with internet access via an access point in the middle Bluetooth uh doesn't really provide full internet access it's more like a cable replacement Channel a wireless USB of sorts and then on the right we have low pans which aim to provide internet connectivity at the same level as Wi-Fi would but to embedded devices and while operating within the constraints of low power for example having a transmit data over multiple Wireless hops to set up an embedded mesh Network so low pass have been used in a variety of applications for example scientific applications like environmental monitoring structural monitoring of a bridge um and it's also been deployed in the indoor environment in a smart grid context and recently there's been a push to deploy it in a smart home and iot space and the thread and open third efforts I mentioned earlier are one such attempt but despite being useful for all these applications it's difficult to use low pants because they also come with a set of challenges the first set of challenges come from the resource constraints the fact that the embedded hosts have limited CPU and memory resources uh the second set of constraints come from the link layer a low band link clear like for example IEEE 802.15.4 has a small MTU of only about 100 bytes and has low wireless range which means that in order to in order to get connectivity over a large area you need to transmit data over multiple Wireless hops and finally energy constraints are also an issue uh you typically don't have enough energy to keep your radio on and listening all the time so you do recycle"
  },
  {
    "startTime": "01:08:00",
    "text": "your radio what that means is that your radio is actually in a low power sleep state for say 99 of the time and then one percent of the time you can turn on your radio to send or receive packets and in order to provide an always-on allusion to Applications despite doing this to save power we need some careful scheduling at the link player in order to make sure the data is only sent to a node when this radio is on and ready to receive that data so to make this more concrete uh I'm going to tell you about the platform we use in our research it's called Hamilton and some of the stats of this platform are on the slide the key Point here is that this kind of device is more powerful than the devices we had when load band research first got started in the early 2000s but it's still substantially less powerful than even a Raspberry Pi you cannot run Linux on a device like this instead you have to run a specialized embedded operating system and you can understand our researchers tackling the central question of how should a device like this connect to the internet and the result of our research is that we show that tcpip works well now as I mentioned earlier the adoption of Ip in this space did not include TCP and that was no accident the reason is that researchers had doubts as to whether TCP would work well and they expected it to not work well given the challenges of low pants so here are some quotes I've taken from some research papers to show some of the concerns that the community has had about using TCP the first one is that TCP is not lightweight and may not be suitable for implementation in low-cost sensor nodes with limiting processing memory and energy resources the second one is that certain features of TCP may cause harm like for example that the connection oriented protocol aspect of TCP is a poor mesh for wireless sensor networks where actual data may only be in the order of a few bytes and finally there's the wireless TCP problem the idea that TCP may use a single packet drop to infer the network is congested which can result in extremely poor performance because Wireless links tend to exhibit"
  },
  {
    "startTime": "01:10:01",
    "text": "relatively High packet loss rates so again to summarize more simply there's a concern that TCP is too heavy that its features are necessary and that it'll perform poorly in the presence of Wireless loss so Central to where research was understanding tcp's performance and what we did is we did a study where we actually ran TCP in a low pan measured its performance and tried to draw conclusions about how well TCP really does or does not perform and what we found is that out of the box TCP indeed performs poorly but it turns out it's not due to the expected reasons that people had the actual reasons were somewhat different okay so the actual reasons are that low pants have a small L2 frame size basically a small MTU and this results in very high header overhead the second problem is that hidden terminals are a serious issue for TCP when operating over multiple Wireless hops and finally that the kind of scheduling at the link clear needed to support a low duty cycle and low energy consumption interact poorly with TCP now there's a key difference between the issues on the left and the issues on the right the issues on the left if they were to exist would be fundamental issues there's no clear way to adapt TCP or the link layer to eliminate those issues but the issues on the right it turns out are fixable within the Paradigm of TCP or a fairly straightforward techniques so in our research we show why the expected reasons don't actually apply we demonstrate techniques to address the actual issues causing poor TCP performance and our overall conclusion is that TCP can perform well in low bands after all so that's an overview of what I'm going to be telling you about uh and they're also by the way a set of techniques that we propose in order to make low pens work well which I'll go over in the course of the talk okay in the next part of the talk I'm going to focus on the expected reasons for uh why uh or why the expected uses"
  },
  {
    "startTime": "01:12:02",
    "text": "for performance don't apply um and to go back here I'll be talking about this technique in this part of the talk and the reason is that this part of the talk is more about our experiments and our observations about the expected reasons this technique has to do with their implementation which is why it's included I'll talk about the remaining techniques in the next part of the talk where I dive into how to affix the actual reasons for for performance so our methodology is based on a Hamilton platform as I mentioned earlier you can see the picture there this is a Hamilton platform connected to a Raspberry Pi and the Raspberry Pi is just there as a back channel to collect logs and so on and measurements uh the TCP stack was of course running on the Hamilton platform directly our software stack is using open thread with Riot OS and we used a wireless test Twitter collector data where each of those numbers is one of our Hamilton nodes uh the lines connecting them Show an example of a topology in reality openthair is going to generate this dynamically this is just a snapshot of what it might look like and we ran TCP where one TCP endpoint is in the wireless mesh on one of the Hamilton nodes and the other TCP endpoint is hosted on the cloud and Amazon ec2 so um one of the first things we had to do was to implement TCP uh now as I mentioned earlier there have been several prior attempts to use TCP in this space based on simplified embedded PCP Stacks but we wanted to use a full-scale TCP stack in our study now the challenge is implementing a full scale TCP stack is hard and in fact there's an entire RFC devoted to all describing all the problems that people were seeing in full scale TCP stacks back in 1999 even though these TCP sets had matured for at least a decade by this point so um our approach was not to implement a TCP stack from scratch since we felt it would be too error prone uh to do instead we started with the mature full-scale TCP implementation in FreeBSD and re-engineered key parts of it so it would work well in an embedded platform"
  },
  {
    "startTime": "01:14:01",
    "text": "and we call our resulting implementation tcplp where the lp stands for low power so now that we have our implementation of TCP we can concretely answer the question of what are the resource requirements of running TCP so what we found is that tcplp requires 32 kilobytes of code memory and about half a kilobyte of data memory per connection to store all of the TCP connection state in a full-scale TCP implementation while our platform has substantially more code and data memory than that now as an optimization we use separate structures for active sockets that are actually endpoints of a TCP connection and passive sockets that are just listening for new connections which helps to save a bunch of memory as well um but the point here is that you know at least in terms of connection State we're well within the bounds of the available memory so natural question is what about the actual buffers used to send and receive data so um the TCB buffers need to be the bandwidth delay product and size in order to be able to send at full speed of the network and we empirically determine the bandwidth delayed product as two to three kilobytes and we can see in the graph here how we experimentally did that you can see it two to three kilobytes of buffer size the available could put over TCP levels off so here is a TCP including the size of the buffers fits comfortably in memory and in fact there's another conclusion to be drawn here which is that if you notice the the size of the buffers is actually much bigger than a connection state which suggests that most of the overhead of TCP doesn't come with the complexity of the protocol is from the buffers and any performant bulk transfer protocol would need these buffers in order to transmit at the bdp so in some sense the overhead really isn't bottlenecked by tcps complexity at all um there's also some uh we also introduced a technique here in order to reduce the memory used for the buffers uh and part of this has to rely on TCP having both a receive buffer and a reassembly buffer to store in sequence data and auto sequence data for reassembly now full scale TCP Stacks"
  },
  {
    "startTime": "01:16:02",
    "text": "like FreeBSD use packet cues there's a separate queue of packets for each of these but in the embedded setting we don't want to use dynamically allocated packets because if we hold on to dynamically allocated packets in a memory constrained setting we may cause other memory allocations to fail so instead we want to use flat arrays and the naive strategy would be to have a separate flat array for your receive queue and for the reassembly queue now to optimizes what we observe is that there's an interesting relationship between the advertised window size the number of buys we currently have and the total size of that buffer which is that the number of received bytes plus the advertise window size is equal to the total size of a receive buffer now the observation we make on top of this is that all of the data we may possibly get for reassembly has to fit within the advertised window size that's the contract of TCP that if you're sending to a recipient you should not go past their advertised window so this allows us to actually store the receive buffer and the reassembly queue in a single flat array okay so the way this works is that we have our flat array and the yellow region with the start and end pointers is just a circular buffer destroyer in sequence data then as we receive Auto sequence data that needs to be reassembled we store it in the same array past the end of the circular buffer using a bitmap to keep track of which of these bytes are active corresponding to received Auto sequence data and which of them are just empty slots on the array where new data can be stored okay so in this way we can significantly reduce the memory for buffers by in some sense not having to allocate a separate buffer for a reassembly queue and just sharing that with the buffer we've allocated for the received queue okay next I'm going to talk about the wireless TCP problem and before we talk about that let me tell you about the number of in-flight segments since that affects tcp's congestion control so as I mentioned the Batman's live product is two to three kilobytes each segment is sized to about 250 to 500 bytes and this was chosen carefully it's actually based on the technique I'll tell you about later on in the talk or"
  },
  {
    "startTime": "01:18:00",
    "text": "coping with a small MTU of these networks uh so I'll come back and explain this but for now take it as a given that our segments are 250 bytes to 500 bytes and what this works out too is we have 4 to 12 in-flight TCP segments at any one point in time now this is different from other higher bandwidth networks you might imagine if you're transmitting over a higher bandwidth Network or over a longer distance you may have hundreds or thousands or tens of thousands of packets in flight and in comparison 4 to 12 is is very small and that profoundly affects how tcp's congestion control operates so here are some examples of how of TCP neureno's behavior in a low pattern and for now focus on the left graph um here uh our maximum segment size is 462 bytes um and what's going on MSA and active segment says I'm actually subtracting the space for TCP options so this is how much data is sent in htcp packet and our Bama delay product is filled by just four kcp segments so what ends up happening is that yeah our losses are very frequent but because we only need a connection window of four segments in order to fill up uh the bdp and standard line rate gcp's condition control actually is actually able to recover from losses extremely quickly and we spend most of our time actually sending at a full window uh despite the losses in the wireless medium being frequent on the right we have a more challenging scenario where we size our MSS to be smaller and we use some active Q management which induces some more lost events but we still find that TCP is able to reach a full window and operate there most of the time despite seeing treatment losses so somewhat counter-intuitively we find that because our bandwidth in these networks is so small our bandwidth delay products are small and as a result we can recover to a full bdp quickly after a loss and this means that the wireless TCP problem actually does not affect tcp's performance significantly in these networks and it's much more resilient to wireless losses in a low pan than it is in a higher bandwidth wireless network"
  },
  {
    "startTime": "01:20:01",
    "text": "so that was a surprising result but one that works well for us because it removes one of the obstacles we ordinarily would have faced in getting TCP to work so now I've talked about the expect why the expected reasons don't apply in the next part of the talk I'm going to tell you about the actual reasons for poor performance and going back to our slide with our techniques on it I'll be telling you about these three techniques now there are a couple I didn't get to the zero copy send buffer the link to your queue management and that's because I don't have the time in this talk to talk about it but if you want to chat about it afterwards I'll be around or if you or you can look in the paper to find some details about those so first dealing with the MTU problem here's a graphic showing the size of the MTU in Ethernet Wi-Fi and I've Tripoli irritated 15.4 which is an example of a load pan link layer and what we can see is that um TCP IP headers are very small compared to the ethernet and Wi-Fi mtus but there's significant compared to the IEEE inner Toyota 15.4 MTU and this is going to result in large header overhead okay normally we size TCP segments to be as large the link supports but no larger this is standard this is what's used in Ethernet and Wi-Fi but in the case of IEEE 802.15.4 it's only 104 bytes right our MTU is small and our TCP IP headers can actually take up more than half of that if you include the cost of TCP options even if you use a standard IP header compression that's part of six low pan and what that means is that if you're transmitting data in a TCP connection more than half of the data you're setting out are just these headers and your good put a severely affected by that so in order to overcome this we break this conventional wisdom and instead allow tcplp to have TCP segments that span multiple link layer frames okay what that means is that we're relying on the six low pen adaptation layer to handle fragmentation and reassembly for us which adds some overhead but it means that the overhead of our headers is now"
  },
  {
    "startTime": "01:22:00",
    "text": "amortized over multiple frames allowing us to get some good good put now there is a trade-off here um if we use too much fragmentation if we set our our MTU I mean if we set rtcp segments to be way too large what's going to end up happening is that we rely on too much fragmentation and that's bad because now one fragment gets lost we lose the entire packet so what we want to do is we want to choose a TCP segments to be as large as possible to effectively amortize the overhead without incurring more fragmentation beyond that okay and this graph was an experiment where we where we measured the maximum segment size and the good put that results and we found that the gains essentially level off around three to five frames uh so that's what we use for our future experiments and it shows that you know there's a good trade-off to be made here where we can get good good put despite the despite the header sizes now one thing that we didn't do but could potentially help in agree that's orthogonal to this is to get good TCP header compression right because six low band currently standardizes UDP hetero compression with six load pen but not TCP hetero compression and that's another opportunity to reduce these overheads further okay now I'll talk about how the link layer scheduling to support a low Julius cycle interacts poorly with TCP so recall that these devices often don't have enough energy to keep their videos on listening all the time so we Define the duty cycle as the proportion of time that the radio is listening or transmitting basically the percent of time where the radio is not in a low power sleep state okay and in order to get good energy Concentra we want the duty cycle to be as close to zero as possible uh now there are several ways in order to support this uh in the session of literature open third uses a particular Duty cycling mechanism that's called a receiver initiated duty cycle protocol which I'll now explain so in open thread you have two kinds of nodes you have battery powered nodes where we want to minimize the duty cycle and wall power nodes that are plugged into a wall outlet and have enough power to keep their videos always on"
  },
  {
    "startTime": "01:24:02",
    "text": "okay now sending a frame from B to W is easy because W3 audio is always on so we can just send the frame whenever we like more challenging is the reverse getting a frame from W to B okay so what has to happen is that W has to wait until B is Radio is listening and how does it know when boost radio is listening well this is where the protocol comes in what B does is that it never returns on S3 audio to listen for a for a frame it'll send a data request packet to W informing it that it's now listening so w has to wait until it guesses data request packet and once it does then it can go ahead and send the frame to B and B will listen and receive the frame okay so what's the key Point here the key point I want to emphasize is that these idle duty cycle is directly related to how frequently it sends data request frames B can choose to send data request frames very rarely which allow you to get very good energy consumption but doing so what uh what we're doing so will cause more of a delay in getting frames to it since W has to wait for the data request frame in order to send it one of the uh one of the data frames okay so now let me talk about what this means for TCP operation and I'll do this by comparing HTTP over TCP to co-app okay and co-app is a rest-based protocol running on top of UDP and in our setup we had bsnw we did a request frame every one second basically it basically it's in for packets every one second and that allows it to get a really low duty cycle now the key difference between HTTP and co-app here is that the HTTP requires two round trips whereas co-app only requires one round trip okay so for the first round trip right you start at a random uh phase within the 100 Milli within a thousand millisecond sleep interval so you'd expect on average a 500 millisecond delay and Co-op is consistent with that for HTTP what happens is that for the first round trip we see 500 milliseconds but the second round trip starts right at the beginning"
  },
  {
    "startTime": "01:26:01",
    "text": "of the next leap interval so the second round trip consistently sees the worst case latency when transmitting the packet from W to B okay and as a result HTTP performs more than twice as poorly as Co-op uh on this workload now I want to point out that there have been some recent extensions to TCP for example TCP fast open which you can use to eliminate the second round trip and get performance parity between coapp and http but this problem also happens for both transfers where the ACT clock nature of TCP causes it to consistently experience the worst case latency even for bulk transfers so this is an important problem to solve regardless of that and our approach to solving it is to use an Adaptive duty cycle the idea is that we can use the TCP and HTTP protocol state in order to vary how often we send data request frames the idea being when we expect a packet we want to send data request frames more frequently so for example if I'm an HTTP server one of these battery-powered devices and I just accepted a TCP connection I can be pretty sure that that I'm going to soon receive an HTTP request on that connection so I may choose to send data request frames more frequently at that point in time and doing this nearly entirely eliminates the gap between Co-op and HTTP intros or performance so if we zoom out and look at the overall Network this adapter Junior cycle technique works well for the last hop going from a wall powered node to a battery-powered node node but the overall network has the last operator over multiple Wireless hops to even get to that last hop and what we observed with the TCP performs poorly over this chain of wall powered nodes due to Hidden terminals so let me step back and go over hidden terminals to provide some background on that for those who aren't familiar with it uh we can understand the wireless range of a node is looking something like this uh the unit disk model is a simplification where we consider this to be in some sort of the perfect circle uh in practice of course it can be more"
  },
  {
    "startTime": "01:28:00",
    "text": "complex depending on the exact environment your deployment is in uh but unit just model is is going to be enough for us to capture the phenomena of Interest here so we'll go with that um so imagine you have four segments in a line I mean four four nodes in a line with their uh with their transmission ranges shown here and we want to transmit data from a to d now the nature of TCP is that we have multiple segments in flight at the same time for a single connection and that's why we have segment one being sent from C to D and segment two being sent from A to B but unfortunately this is bad because the wireless Rangers are going to overlap at B so the two packets are going to interfere there okay now in the context of Wi-Fi we typically overcome this using a protocol based on RTS and CTS frames that allow us to mitigate the hidden terminal problem in most cases but in the context of low pens the small MTU means that RDS CTS typically has too high of an overhead as a result most uses of it don't use RTS and CTS packets so as a result we're only relying on csma right so at a csma can't detect uh C's transmission because uh it's all the way because a is out of range of c and csma at C can't detect A's transmission because C is out of range of a but both of the packets end up interfering at B and the packet gets lost um this also happens because of data packets and acts going in opposite directions so for example here uh what we'll ultimately see is that um you get the same problem with b and d both setting at the same time to C because each of their csmas can't hear the other so to mitigate this our approach is to add a new random back off delay between link layer rate price okay so the idea is if you transmit a frame and it fails but you know because you don't get a link layer acknowledgment for it then you wait a random amount and retry the transmission and this is different from"
  },
  {
    "startTime": "01:30:00",
    "text": "csma in two respects the first respect is that in csma you do this randomized uh delay with exponential back off if the channel appears busy in this case even the channel appears clear if your transmission fails we still do the back off so it's different in regards of what triggers the transmission and second it's a much longer delay right because in csma you can rely on hearing a concurrent transmission you can transmit immediately if a channel appears clearer in this new delay that we're adding this link free trial delay what we're seeing is that we want to have a delay that's chosen between 0 and 10 times the time to transmitter frame the idea being even if there are two concurrent Transmissions that can't hear each other with high probability they won't overlap in time okay so um the way this would work is that uh each of these two nodes would send its data once in order to go and they will collide but then when they retry the transmit a second time at hopefully different intervals and they won't overlap in time and the transmission will succeed Okay so um we did a measurements for you to understand what kind of Link delays would be appropriate and what would work what we observe is that there's a huge reduction in packet loss even from a small delay and as we increase the delay too much it starts to iterate your good bud because now you're beating a lot when transmitting your packets so we found that there's a sweet spot here at around 40 milliseconds which is about 10 times the time to transmit a single frame and I have Triple A root 2.15.4 uh so that's what we used in our study um and this reduced a packet cluster from six percent to one percent which was we should consider a significant Improvement so finally I'm going to summarize our evaluation and and conclusions so first I previewed this result at the beginning we're able to achieve significantly higher good but than prior attempts at using TCP and we're very close to a reasonable upper bound that we computed based on measurements of how fast the radio can send out packets and the overhead loss to headers and acts"
  },
  {
    "startTime": "01:32:03",
    "text": "um we also did a measurement study to study the Energy Efficiency so we use TCP and Co-op for a sentence and task and measure the radio dutious cycle over a 24-hour period and you can see the radio duty cycle here the key point is that TCP is not significantly worse than co-op in fact they perform comparably for the duration of the experiment at about a two percent duty cycle and be considered this a success because TCP is able to perform essentially on par as a protocol over UDP developed specifically for low pens so now the TCP is a viable option what does this mean well first we should reconsider the use of lightweight protocols that emulate part of tcp's functionality um in the sense that you know if you have a protocol that's specialized and performs just as well as a general protocol that's more interoperable and used more broadly we should perhaps prefer the one that's used more broadly and is more interoperable second we think that TCP May influence the design of low-pad network systems in the sense that you know for a long time it's been the case that many smart home devices that you buy on the market require a specialized gateway to get internet connectivity um and TCP gives us the opportunity to allow these devices to connect end to end to any uh Services externally that they may depend on and finally I just want to mention the UDP based protocols I think will still be used in low pens but just in the same sense that they're used broadly in the internet for applications with specialized protocols substantially outperform TCP in cases where TCP performs on par with the specialized protocols using TCP is now a viable option so just to talk a little more about the about the middle point about how TCB May influence the design of low-pad network systems when I say Gateway architecture I mean a setup like this where you have your devices these smartphone devices you bought on the market and in order to allow them to communicate with an application server in a data center somewhere you have to install some specific Gateway in your home that is some protocol translation and application logic in order to bring connectivity to those devices"
  },
  {
    "startTime": "01:34:00",
    "text": "uh what this means is it's often the case for some of you may have experienced this is that if you go buy a smart devices from a new vendor uh now all of a sudden you need another Gateway for those new devices or even maybe the newer versions of devices from the same vendor like for example uh for a long time it was a case that for life that if you have bulbs from say lifx and Bulbs from Philips you would need separate gateways for both of those devices um so uh the the introduction of Ip in this space didn't really change this in the sense that now your application protocol on the left is now implemented over IP but you still need the application layer Gateway and the missing piece I think that would allow an end-to-end uh a connection here would be to have a transfer protocol that's supported on both sides namely TCP and once you do this your application layer again please become regular border routers and you could potentially consolidate these together into a single border router so um in conclusion uh we implemented tcplp a full scale TCP stack for low pan devices uh we explained why the expected reasons for Port TCP performance don't apply uh we show how to address the actual reasons for poor TCP performance and we show that once the issues are resolved TCP can perform comparably to low band specialized protocols that's all I have prepared I'm happy to take any questions now foreign excellent talk um I see we have a couple of people in the online queue and a couple of people at the microphone um which we do the uh I guess that we'll do the microphone first so uh I can see who that is but if if you can go ahead and see your name in your question so hi I'm Matthias I'm one of the co-founders of white great work thanks a lot um one remark and two questions a question first"
  },
  {
    "startTime": "01:36:01",
    "text": "um so you argued that supporting TCP is important because it's popular now Creek becomes popular did you work on any comparison from a system point of view um sorry I didn't quite hear what they said becomes popular uh you said that TCP is quite popular but quick also becomes popular in the internet quick you know uh so we didn't do a comparison against quick but I like to comment on that because that's a good point that other transports are becoming popular many of the issues that we addressed aren't specific to TCP they apply broadly to TCP and other protocols needed for bulk transfer like for example um the main issues getting it to work with hidden terminals getting it to play well with link clear scheduling and so on apply broadly to any protocol that's transmitting a lot of data and wants a significant amount of bandwidth therefore I think that many of our conclusions would actually apply equally well to quick as they do to TCP okay um and another question I mean in your paper you note that you also have an implementation for gnac the default networks they can write do you also plan to submit the Pierre to upstream's implementation um at some point we did have plans for that but what happened is that Riot OS already adopted a different TCP stack and it seemed a bit redundant to contribute a second one uh recently what we've done is we have we must have contributed our code to open thread which now uses it as its default TCP stack okay first did I highly encourage you to submit the PM and final remark um you said that the fragment needs to be a packet needs to be is lost when the fragment is lost I mean this depends a little bit on the fragmentation screen right if you consider for example selective fragment recovery um it doesn't matter too much whether the fragment is lost or not for the whole packet yeah so um my understanding about the basics load pan work at least the way it was implemented in the operating systems we"
  },
  {
    "startTime": "01:38:00",
    "text": "looked at was indeed that if a fragment is lost you lose the whole packet but I do agree that there are protocols you can use to recover a loss for Apple without losing the entire packet and those could also help with the problem allowing you to make the packet bigger and amortize TCP IP headers even better cool all right hello um Tommy Pauley from Apple thank you for doing this talk I'm very interesting I'm super happy to see the use of TCP here um I just had a couple questions from the presentation um way earlier and you don't have to go back when you're talking about um the memory saving aspects and the ability to have the flat buffer you had the diagram there of you know essentially here's kind of what's in flight and then there's the out of order bits and there are gaps in there as well um when you're doing this are you able to essentially guarantee 100 of the time that you'll never need to allocate memory or is it like just most of the time and then there would be a failover case where you do need to have Dynamic allocation uh that's a great question uh we ensure that you never have your dynamically allocated memory cool and the way we do it is that you store the data there you have a bitmap to keep track of which bits contain the out-of-order data but the bitmap can also be sized statically because it depends only on the array size which is also static got it okay cool um and then the other question is more about kind of what you're ending with talking about how you can use this to get to internet hosts end to end um and I believe in your tests you were testing against um end-to-end internet connections for that do you need to modify anything on the TCP implementation on the internet servers like as we were mentioning things like timing the retransmit timing schedule so you want to add Randomness so you're not colliding um is there something that needs tuning on the internet hosts to make sure that"
  },
  {
    "startTime": "01:40:01",
    "text": "they are friendly to the low pan devices or can you use completely unmodified um internet hosts to talk to yeah that's an excellent question and the short answer is that the hosts on Linux side were completely unmodified great um I mean that's to say a little bit more about that uh the timing that we adjusted for like the randomized delay was none of the TCP level it was at the link layer so as a result the the other side actually doesn't see any of that got it um there's also one of the advantages to us using a full scale TCP stack like the one from FreeBSD because it's been battle tested in the real world and it's interoperable with all the major TCP Stacks that are out there and I just want to say that uh interoperability is actually a problem in the embedded space many of the ability CPS tags you find are have inter have interoperability problems in pretty subtle ways with the real TCP Stacks that are used and that's something we managed to sidestep by using a battle test the TCP implementation as the basis of our study cool thank you [Music] so hello this is Thomas also from the riot Community thanks again for this work thanks for using Riot uh there's uh another encouragement using Junior C because you have a generic packet buffer here which you could reuse that even reduces your memory over it even further uh just just a remark one question about uh about your multi-hop experiments you showed us nicely how by jittering the the TCP forwarding how you could avoid the hidden terminal problem was that in a cleaner environment without cross traffic with only a single TCP connection yeah so uh the hidden terminal problem affects even a single TCP connection in isolation um and we verified that our randomized back off fixes the problem in that case yeah but only in this case I mean the normal cases that you have background traffic"
  },
  {
    "startTime": "01:42:00",
    "text": "right so yeah yeah so I mean if you have background traffic this is also why we used randomized delays instead of fixed delays because if we had a randomized back off it doesn't matter the interference is coming from the same stream or a different stream right in both cases you'll back up a random amount and hopefully transmit again without colliding um this is also why we did it without because I mean there are several protocols you could use that look at TCP state in some way um and having it just be a randomized related link there gives us some confidence that it would work across TCP extremes and regardless of the source of traffic whether it's TCP different TCP streams or even something else in this context did you also consider experimenting with more flexible uh um link layer Mech layers then just a csma CA for instance the dsme Mac layer which is also supported by riot no we didn't experiment with that uh we looked at csme because that was the most common one supported across all the operating systems and networking protocols that we tried across tiny OS Riot and open thread so it's the most natural to focus on that okay thank you thank you all right I think I can't I think we have a remote question am I unmuted finally so uh this is following up on the multi-hop case uh so in these environments the uh forwarding devices are in fact also very low power low um resource devices um did you see or could you speculate on what you might see as to whether TCP traffic would have more stress on the buffers of the forwarding multi-hop Wireless nodes us so that's a great question um first I want to I mean so first I just want to clarify that the buffers use of the intermediate routers these aren't TCP layer buffers is just like the general packet buffers used for"
  },
  {
    "startTime": "01:44:00",
    "text": "forwarding because you know an end-to-end TCP connection there's no TCP State sure sure but it made for the different may put a different low aggregate load um on those buffers then say Co-op traffic or something that's more you know simple request response related yeah so I mean of course that's the case that when you're transmitting at higher bandwidth you're going to place some more stress on the on the buffers of the intermediate uh all of the intermediate routers and there are a couple things that that we do in that we actually did in our study in order to help mitigate that the first one is that we added some active queue management functionality to those intermediate routers where you mark packets as congested using explicit using explicit condition notification and so on in order to prevent TCP from filling up the entire buffer and keeping your cues short um the primary reason we did this was to improve fairness of different TCP flows that are competing for buffer space of these intermediate routers and also to reduce the and also reduce the latency of traffic but it also has a side effect of limiting the amount of buffer space that's being used by a single TCP flow to address some of the concerns that you brought up thanks I was looking for the aqm uh angle on that all right uh so I have a question um does the uh I I I very much like the idea of the head of multiple link their frames um does this put any constraints on the link there or or does the six Loop Handler um so handle all of that um that's a great question so some of these uh can potentially be handled at the sixth low pan layer but others do indeed have to do with with the uh with the link layer directly like for example the randomized delayed that we added to avoid hidden terminals is something that would operate at the link layer right because of the six little band layer uh you don't have or at least you don't"
  },
  {
    "startTime": "01:46:00",
    "text": "naturally have the same kind of visibility into you know when your link layer acts are coming in and so on whereas you would need that to determine uh that a transmission failed and how much to back off on the re-transmission and so on so some of them do indeed affect the link layer yeah other requirements that the link layer delivers packet in order um to avoid um damaging the headers or os6 weapon handling uh sorry I didn't understand your question is there a requirement that the links packets in order in order because of the way you've sent the head of split across multiple link their frames or is that all that the reordering handles by sixfloper oh yeah so the reordering and reassembly is handled by six low pan and there's no strict requirement that you have to transmit the frames of a packet directly one after the other consecutively in fact one of the things that I skipped because of the time limit was another set of techniques we have uh at the level of managing how to deal with concurrent frames basically how to schedule frames when you're some of them are going through other wall power devices some of the battery powered devices and in effect what we do is if you if you receive a data requesting from a battery powered device then you prioritize sending frames to Tech in order to reduce its duty cycle and let it go to sleep as fast as possible and that's one case where we specifically might interrupt another transmission and not send its frames concurrently I mean nothing streams consecutively oh okay yeah that makes sense uh Gabrielle me one okay um yeah thank you very much for for this work this is great stuff um I did have a comment on um the comparison with Co-op I think the justification for Co-Op was not entirely"
  },
  {
    "startTime": "01:48:01",
    "text": "based on we can't use TCP type type thing it was more based on we can't use HTTP because of the justification for it was for folks who wanted to use a restful interface for the application layer not every application of the year in iot wishes to do that but there's certainly a lot of a lot of incentive to use restful so when when the restful folks started uh to become interested in iot the only alternative was http11 which uh I completely agree is terrible uh it's my Sardar it's textual based protocol you cannot compress it it's it's very verbose Etc it's it's terrible um we subsequently had HTTP 2 which became a binary protocol and we actually had a paper uh three years ago in an RW about you know how to use that over something like six low pen for example uh some just initial scratching the surface but now we have HTTP 3 and quick and it's all binary so um and I um I understand you you guys haven't had a chance to go after the excellent work you've done to look at those layers but I would highly encourage you to do that because that would that would address um a significant portion of the of the application layer incentives um to um for iot as well yeah so uh so thanks for for clarifying that um I do acknowledge that Co-op has has evolved quite a bit in a few years some of those Evolutions happen after after we published this work uh but I do want to uh to clarify my position on Co-op a little bit uh based on what you said it's that uh indeed I think that Co-op is useful and it has its uses and it's very flexible it's been evolving a lot over the years and that's great um I do I mean I have noticed that Co-op has been evolving in some sense more and"
  },
  {
    "startTime": "01:50:00",
    "text": "more towards the same kind of abstraction that TCP provides right in some sense uh the ability like for example with some of the recent work on on streaming on streaming block transfers and so on um all I'm saying here is that I think that an application that's built on co-app in these kinds of networks with all the latest features like for example the ability to have multiple blocks in Flight uh concurrently and so on would also be wise to potentially consider using TCP directly itself given that TCP is also a viable option in these Networks but thanks for that comment all right thank you uh one more question uh Benjamin uh hi thank you so much for the presentation I really appreciate it um I was just wondering you talked mainly about the applications of this in um in lands do you see any application for longer range networks like um like mobile ad hoc networks or anything of that sort uh that's a great question so all of our experimentation was uh was done using iFly 82.15-4 which is a personal area network protocol and that was motivated by the recent interest in it in adopting some of that technology uh to work in the smart home and iot space um some of the I mean some of these lessons might carry you over to the mobile and and ad hoc network space like LP Vans and so on um I'm not sure I'll be able to tell you any specifics given that I don't have much experience with those networks um but I mean by first gut would be there's probably a way to make TCP work well given that it's been adapted to work in so many different kinds of networks uh in all kinds of different environments but other than that I'm not sure if any of this which of the specific techniques would directly carry over there thank you so much all right uh thank you very much"
  },
  {
    "startTime": "01:52:02",
    "text": "excellent talk [Applause] uh and thank you again to to both of the speakers I think that there were two two really great talks there um both uh Sam and Tasha will be around all week uh I'm sure they'll be very happy to to talk with people more about their work uh so please do do you find them have a chat chat about their work make them welcome to the the ietf and to the irtf uh congratulations both to uh Simon satosha for the award of the anrp this time um as I said earlier looking up for um more anrp award talks um at the the uh itf15 in London in November uh the nominations for the uh 2023 uh nip Awards will be opening in September so if you know any good work please think about nominating that work and look out for the uh applied networking research Workshop which is taking place uh co-locating with the ITF in Philadelphia tomorrow um thank you again everybody um hopefully I will see some oral of you in London or at the nrw tomorrow or later this week thanks everyone"
  }
]
