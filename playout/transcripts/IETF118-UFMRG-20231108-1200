[
  {
    "startTime": "00:00:16",
    "text": "For our time. So we'll get started. I'm Stephen Faroe, one of the co chairs, like, other coach here, Jonathan, is approaching. We have a kind of a short session today. I guess this is the universal formal methods research group if you're in the wrong a good time to not be in Roarum. There's a note well, the IRTF, part of the internet research task force, not the ATF, but we followed the same processes, to the large extent. So, hopefully, you're familiar with the note well. If you're remote, please mute and don't send video or audio unless you really need to and our agenda for today is we have a bunch of presentations coming up. And and the agenda bashing So we have a short session. So let's kinda move along. Okay. With us, Nobody wants to pass the agenda. Chris. Chris on Sebastian. Yes, Chris. Oh, just a reminder, blue sheets. Gotta do the blue sheets. Oh, there's a yes. Use the QR code, sign into the room, do all the good things. Are you you're sliding me straight. Oh, yeah. Great. So we have there's a kind of a timer that would appear on being shorty. Okay. So you can see that you can see the clock ticking away. I can see the clock. My already started. Yeah. Alright. Well, afternoon, everybody. And thank you so much for having me speak. It's it's really lovely to be here. So today, I'm gonna talk about using formal methods at Google and specifically with regards to help you get security guarantees,"
  },
  {
    "startTime": "00:02:02",
    "text": "our critical software, hardware, and protocols. I specifically want to talk about the usability aspects of some of the tools and the tool change that use and particularly how these may impede what we want to do, going forward. Right. So the structure of this talk is pretty simple. I'll start by saying what we currently do at Google when it comes to formal methods then I'll talk a little bit about what we would like to do going forward. And then I'll talk about what would help to make those things happen. Right. So this is a sample of some projects and efforts that we have going on at Google, and these are projects that make use of formal methods. And they span both hardware and software projects. I'm not gonna go deeply into many of these, but I will tell you what they are. So Open Titan is an open source, a silicon based root of trust. So it's Chip. Project Oak is a trusted execution environment that aims to provide provably private computation on the service side. Protected KVM is a Linux based hypervisor for Android. And then the end to formal project makes use of machine learning before mathematical reasoning and mathematical computations. And of course, we also put, verified cryptographic code into crypto libraries like warring us as well. I've also included a publication from Google about, you know, our static analysis tools that we build and that we use. Some of them make use of formal methods or have a foundation in formal methods, but they're not very cases. So I'd say they actually maybe you belong in a different class compared to the other if it's that I've listed here. Okay. Doesn't seem to be Right. Okay. Of course, we also have some internal efforts and efforts that are in the infancy and, and, and just starting up. Right. So some of these efforts have an intersection with photography and specifically surrounding the security guarantees"
  },
  {
    "startTime": "00:04:02",
    "text": "that we want from the photography that is used in these projects. But for today, I'm really gonna talk about getting formally verified code into our cryptographic libraries and also Avan cryptographic verticals. Right. So we have a small team at Google that focuses on doing this. And we are a fairly young team. We've been running for about a year, and we're part of a bigger cryptography team, at Google. Where the information security engineering formal team And we have Andres, who is really the it's magicians. That does a lot of the heavy lifting with regard getting formally verified code into our libraries. And then Bill, Brian, Jade, and Lucas are very kind individuals. Who help us where they can, on top of the other Google jobs and responsibility keys, but they're either working on some of the other projects that I've mentioned, or they're just very it in former methods and are ramping up with the tools that we use. We also get some input from senior engineers. Care about how we use form Right. So basically, the team wants to We'll formally verify security critical software and systems at Google. And we want to do this because we want to mitigate common and subtle photography vulnerabilities proactively. If there are guarantees to be had, we want to make sure that we have them. Right. The team aims to cover 3 pillars, specifically way hardware and and protocols. Software is really about getting guarantees for cryptographic libraries. Hardware is performing verification on the crypto that are used in Silicon, roots of trusts or or chips. But we really are more in an advisory capacity when when it comes to this pillar. We don't do that much there, and sort of we're also trying to grow and expand in this area. And then protocols are really about security guarantees for all the cryptographic protocols that we would use. Be they standardize popular ubiquitous protocols or some internal protocols. Of course, we try to mandate standardized, well designed, well tested protocols, where we can and wherever possible."
  },
  {
    "startTime": "00:06:02",
    "text": "Okay. So the first two pillar software and hardware really are about producing formally verified code. And then the protocols pillar is about verifying the logical flow of cryptographic protocols and seeing if there are any logical flaws that an attacker it exploits. Now I mentioned that we are a very sort of small team So currently our focus really is getting verify cryptographic code into our crypto libraries and specifically boring is, which is one of our very important cryptographic libraries. And we make use of the feared cryptography framework. And this makes use of the cock theory improver to provide functional correctness guarantees cryptographic operations. And then the cryptopwork, this helps to produce very fast, highly optimized assembly code, that we can Now this is the workflow that you would need to follow to get verified code into into boring to sell, I'm not gonna run through any of the components in detail. But you do have to be quite familiar with the cochlear improver and how it works. As part of fear cryptography. Then the crypto work does some very careless stuff with assembly to produce highly optimized assembly. I borrowed this figure from the crypto paper and extended it, a little bit, so thanks to the cryptopals. Okay. As I mentioned, Andreas is really the engineer who's responsible getting a lot of our verify code. Into boring SSL. And what started as an interim project in 2017 has now led us to having formally verified elliptic curve operations in boring SSL. And as an example, we have, formally verified curve 25519. And when we landed the we saw a 20% performance improvement when we measured it across the Google fleet. So this was really a nice win for us. But we would also really like to do more would like to cover more libraries and and for libraries that we use."
  },
  {
    "startTime": "00:08:00",
    "text": "We'd also like to cover new algorithms, and this is particularly important when we think about the PQC transition, that is just around the corner. With regards to protocols, we wanna look at Google critical protocols, we also wonder if these protocol twos will be useful in our security review process. When teams come to us and interesting designs and and new protocols that they might want to use. So we would definitely like to use more of these tools, but here's the problem. They are not that easy to use. Right? And, I spoke to some of the contributors who work on our team who are ramping up with the tools to give me some feedback. And this is mostly about producing formally verified code that goes into our libraries, from personal experience, I can say that a lot of this also holds true for the protocol verification tools. So to quote, the initial learning is quite tricky and the documentation is not all that great. In terms of readability, documentation, and debunkability of proof checkers, these lag behind most of the other code that I interact with. Non backwards compatible updates to fear and privacy in common. Bit necessary, but they can still make working with even only mildly out of date forks. A pain. Alright. So this is feedback from people who have had to ramp up, and these are engineers, at Google. It's super helpful talking to someone that already knows their way around the various tricks and pitfalls this really speaks to having access to an expert you're using these tools, there's also a lot of infrastructure that's needed projects at scale and the issues are compounded, with regards to documentation because they're quite project specific. Under development. There's probably room for standardizing this infrastructure a bit course, this is a challenging problem. In its own right. I don't know if there's a just a quick clarification question, if I may. In the previous slide, you're, someone is complaining about a non backward compatible changes."
  },
  {
    "startTime": "00:10:02",
    "text": "To the theorem, proven. Mhmm. Does that also imply or cause changes to production code. That influences production code. Yes. And I'm gonna talk a little bit about that. In the future, but it it does make it difficult have production level code. Thank you. Yes. Right. Okay. Then there's also more feedback, and this is really about saying, if you're an expert, maybe you make very quick progress. If you're not an good, maybe some automation within the tools would help save some time. When it comes to formal methods tooling, I think automation is a topic all on its own and and probably deserves, its own talk. Right. So we would like to make more use of tools and 2 chains. It would be great if they were more accessible to more engineers. But as the feedback suggests, these tools and toolchains are very complicated. To use. They are still largely academic proof of concepts. You need to be highly skilled or have access to someone who's highly skilled to be able to use them. And perhaps the situation is understandable because there hasn't been much corporate level investment in producing polished students. Now that's not true in all case There are some companies in the in the formal method space. But largely, these tools and toolchains are still academic. In nature. So easy to follow ramp up documentation is difficult come by or it's lacking and documentation policy seems to vary across many tools. Sometimes the documentation seems to be very good very simple examples, but more complicated, when it comes to elaborate use cases. So something that I come up against as well is when we're talking about using more formal methods is that the benefits are not always that easy to sell, and and this links back production level code. Don't wanna use code that we don't understand, don't wanna use code that we can't maintain. Not in the game of producing one offs that maybe you need for a publication. Right? The proofs and the code need to be maintainable for the long term."
  },
  {
    "startTime": "00:12:02",
    "text": "And also when it comes to sort of building models and and the protocols tools, we often get the question while do you know that a model is appropriate and and correct? Of course, this applies more generally to provably approval security, but, you know, we do get this question, particularly with regards to the security guarantees, when you've modeled something. Of course, when you find a big attack in a big protocol, you can see the benefits but you have to be able to motivate why doing this stuff at scale and at a large company is worthwhile. Right. So we think that there are things that will definitely help. A perhaps more usability research, in the space. We have work in the area of cryptographic libraries and cryptographic APIs. Maybe it can extend to the specific formal methods case. Also maybe some more systemization of knowledge, some SOK type work in the area, covering the pros and cons of the different tools and the toolchains. There is already some work here, but perhaps there could be more. Also improved documentation and debugging. You know, descriptive use for error logging is very, very important to a user's experience when they're using these tools. And also honestly documented limitations are much better than surprises when you're using a tool. So be that with regards to functionality or even the security guarantees that you get when using a specific tool. And we've we've spoken about this. That's, you know, the this needs to be maintainable over the long term, so stable and well maintained releases. Are really important when you wanna start using this code at the production level. Right. Now we acknowledge that none of this is easy. Right? And this talk isn't meant to be a laundry list of of complaints. It's also about asking the question. How can we help? How can we help tool builders and maintainers, right, to make sure that we can use tools at scale for production level code. Perhaps this is an area where industry in academia need to work even more closely together."
  },
  {
    "startTime": "00:14:01",
    "text": "Whatever that may look like, and we are very happy, to take suggestions. So, thank you very much, and I must thank the people who are doing all the really hard work. So, that that is the team. So thanks again. Thank you. So this is the the RTF, so we were allowed for people. It's not it's not the horrible, cold, I use the effort on. So, again, if you have questions, please join the queue if you can, in Midaka too, and But if you can, that's not great. Press your action Thank you. Thanks very much for the talk, Tyler. I'm wondering if, Google is honing in on a small number of different, different tools. By tool, I mean things like Provera for Tamarin, blah blah blah. Lean, I guess, is the cool cool kids do lean now. Are you honing in on, like, like a small number of tools, or are you, are you planning on kind of Having expertise in many, many tools. Right. That's a good question. We're a very small team right now, so we'll probably start with a small subset tools, but we wanna have the space to explore the different tools. And this is where sort of clearly documented limitations or pros and cons of all of the tools will be incredibly useful when we make these types of decisions. So long term, the world, like, in 10 years, you expect we'll be using a lots of different tools still. Maybe. Interest. Hi, I'm Alexander. Could you provide some more details about the 20% speedup? Was it just because it was rewritten in assembly? Yes. Or Oh, I I can tell you briefly what happened. So there was some sort of gnarly assembly that we had that that wasn't that great. So then we used the cocktail improvement. We got some nice homey verified c And then the cryptop framework helped to, produce very, like, take that and make it into very fast assembly codes. So"
  },
  {
    "startTime": "00:16:02",
    "text": "So that's that's sort of what happened. We basically didn't start with something that was ideal. And we weren't expecting such a big performance improvement, actually. Thanks. Great. Thank you, Fred. Groups. Everyone. No problem. Goodness. So our our next speaker is remote, all going well. It may work. This so Joshua, we can pass you control of the slides. Do you know how to do that? And if you flip up the list of attendees, Can you all hear me? Yes. Yeah. We can see you and hear you. You can hear us like this. In the just in the process of giving you control of the slides. Sounds great. Thank you. On his icon there yet. That's like control. So you Joshua, you should see the, kinda slide numbers at the bottom of your screen that you can kinda click through them yourself. Yep. That works great. Thank you. Execute. That's that's great. And I'll start a timer. You've you've probably see it also on screen, just to keep us on schedule as well. Okay. Thank you. Good. Thank you. Hi, everyone. So, yeah, I'm Joshua Gancher. I'm a postdoc currently at CMU. I work with Brian Pardo, and our group does all kinds of you know, cool applications on, verifying relate, you know, various parts of the Internet infrastructure, and I'll be talking about, what's mostly my work today, which is verifying security protocols end to end with owl. So, as the title of this talk suggests all of the new language developed by a group here at CMU. Developing security protocols. So think of the same ones that, like, Cameron targets, like TLS, wireguard signal, these kinds of things. One of the big differences, though, with Al, is that we're really optimized towards verified implementations."
  },
  {
    "startTime": "00:18:04",
    "text": "So we envision Al not only to be a tool for analyzing protocols, but also a development environment for building and protocol implementations that you can deploy. So indeed, our vision is to develop formally verified implementations of protocol that can serve as drop in replacements, like a verified version of Wireguard. That can simultaneously guarantee among other things, Functional correctness, meaning the code does the right thing, memory safety, meaning the code does not introduce memory errors that could later become a tax And last but not least, the protocol is designed securely. Which means that the protocol has a cryptographic guarantee. In a bit more detail, the L framework looks a little bit like this that, user provides a protocol written in our programming language, and Al first performs a novel type based analysis to make sure that it's secure. And after the protocol is checked for security, We run it through our extraction pipeline, which produces verified rust code. That's not only functionally correct and memory safe, but also resistance of certain side channel attacks and will have competitive performance. Additionally, the out framework not only gives excellent benefits for your bare patient, but it's also fully automated. After you write the protocol in the out language, you're basically done. Because the security analysis and extraction steps are both fully automatic. So to see how Alberts, let's first take a look at this type of security analysis. Is the basis of a paper that we presented at, IEEE S And P this year. So our protocol security analysis guarantees a very strong property called computational security. To So to understand what computational security is, let's first compare it to symbolic security. So symbolic security doesn't model the crypto directly, but instead considered as a somewhat simplified mathematical model using a term algebra. So for example, symmetric encryption is fully specified"
  },
  {
    "startTime": "00:20:03",
    "text": "by these functions for encryption and decryption and the equation that says that decrypt is correct given the ciphertext. And then any security properties that you get out of the symbolic encryption, will fall out of these equations and these functions because you assume the attacker can only manipulate the cryptographic data through these functions and equations. For example, these actions imply already that encryption is perfectly hiding and cyber techs are unforgible. However, these axioms do not allow the attacker to, for example, learn the length of messages through cyber texts. Which happens easily in practice. On the other hand, computational security models the crypto in a very low level way using bytes. And instead of simplifying the way the mathematical details we specify the cryptography directly via their security properties. So for example, we specify encryption by saying that messages remain secret, cybertech are enforceable and so on. And consequently, computational security considers a very strong attacker model. And allows any possible instantiation of the crypto primitives as long as they don't violate these assumed security properties. So in our we strive for computation security. Not only because it leads to more realistic implementations, since there's no symbolic obstruction, unfortunately, computational security is somewhat less popular today. Because it's thought of as being harder to obtain. And to change the story, biggest novelty of owl, is that we introduce a new paradigm for guaranteeing computational security by using a type system. So our type system is actually rather simple. So for example, we have typing rules that say if K is an encryption of things of type t and m has type t. That encrypting m under k results in a public value that you're allowed to put on the network."
  },
  {
    "startTime": "00:22:02",
    "text": "So using typing rules that look like this, we guarantee you security via type checking. This means that we prove ahead of time that for any protocol p, if p is well typed, then P is secure. Can't get into detail exactly what security means. But it's essentially like an information flow property. Confidential data should not get leaked to the attacker. And high integrity data actually contains what it should contain. Most importantly, this proof of security via type checking is a one time upfront proof effort. And because we've already proven this fact, the protocol developer only needs to use the type system to get security. This is great because writing well typed code is much easier than proving your call Secur. Directly, And in particular, using a type system gives you a new automatic modular proof strategy that did not exist before for computational security. The great thing about type systems is that they're naturally modular. So using AL, you can do modular proofs of protocols using tech type based obstruction, just like you've seen in ordinary language, like go or rust. So for example, suppose you have a protocol that first establishes keys, and then uses these keys to perform a data transfer. Since these are logically 2 different phases of a it'd be better to prove them independently. And indeed, you can do this in owl, by having the data transfer and handshake protocols live in different program modules. So that the data transfer module uses the handshake module. Crucially, Our tag system allows the data transfer module to type check only using the patient of the handshake and not any underlying implementation. This is great because it allows you to instantiate the handshake in any way you want as long as it not just the specification, For example, you can do key infrastructure"
  },
  {
    "startTime": "00:24:03",
    "text": "or you can do it with pre shared keys or maybe even a combination of the 2. Now switching gears a little bit, I'll tell you about our verified extraction. And this is all ongoing work. So to obtain verified secure implementations, We're currently working on leveraging Verus, which is a new verifier for Russ programs. Developed here as CMU that takes advantage of rust ownership type system, like the bottle checker and so on. For easy proofs. So to obtain verified implementations, We're writing, what we're calling a proof producing compiler. So what this means is that given a level type protocol in AL, we all put a few things. First, we also had a human readable mathematical functional specification of the program, shown here on the top. And on on the bottom here, we have an executable safe rust implementation of the protocol on the bottom, that we prove in an automated way using Verus that the low level implementation faithfully matches this functional specification. The functional specification is meant to be manually auditable. Indeed, it's almost exactly equal to the code, you put into Al in the first place. While the executable implementation will have all of, like, the gory low level details. Like, verified parser, that we're also working on separately in Verus. 0 copy ciphers which to do this, we'll kind of leverage other verified work for crypto at the Evercrypt framework and things like this. So to relate these 2, worlds, Veris also, what we also give to Veris is an auto generated proof script that's kind of embedded inside of this implementation. This proof scripts will ensure that the code is memory safe. That's kind of given to you for free by rust. Functionally equivalent to the specification and side channel resistance."
  },
  {
    "startTime": "00:26:01",
    "text": "Given the proof script which our compiler generates, Verris then goes on and verifies that a code is correct fully automatically. So that's the tool chain that we've been scaling up in recent months. Let me just tell you about our what we're doing currently, which is, working on wire garden. So our first la the wire guard is our first, like, large scale application. For Al, we've done other case studies before. Especially in their S and P paper, but Wireguard is kind of the biggest kind of realistic when we're doing, right now, So Wirecard is a modern if you don't know about Wirecard, modern secure VPN protocol, that uses state of the art crypto and is designed to be extremely fast. Is very widely used as it's now, like, example, Embedded of the Linux kernel, but at the same time, the core protocol is incredibly lean Indeed, it's implementable in about 4 thousand lines of code. And because it's so important and so widely used as a perfect candidate for formal verification. However, despite this, there does not yet exist a usable, like, you know, drop in replacement for a verified wireguard. And that's what we're hoping to, to do without is to create a, version of the Wireguard handshake that's formally verified that, you know, anybody can just go in and use directly. So that's out. It's a new end to end verification framework for security protocols that uses our type system to guarantee security, our type system guarantees modularity and automation by fault guaranteeing computational security. And we're currently working on a verified extraction pipeline for real world protocols like irregard and the future, we'll work on, you know, bigger investments as well. The project page is here. You can download it out and you know, play with it yourself if you want. It's like written in a, and a type checker written in Haskell, and my email address is here if you have any questions offline. Thank you."
  },
  {
    "startTime": "00:28:00",
    "text": "Thank you very much. Do we have any questions? Hi. My name is Felix. Thanks for the intro. I have three questions. I hope all of them easily answerable. So if I got it right, that users don't specify security properties. Yeah? The security properties are kind of born out of the types. So the so the Texas mint owl is of a combination of information flow and refinement type checking, if you know what that is. So that means that it's kind of an alternative way to specify types compared to like Cameron or, crypto beareth, but, kind of, I would argue it's kind of an equally expressive way to write them down. Okay. And and so do you, assume symmetric clients, and how do you handle sort of like What I mean by that is do you assume your codes speaks with another instance of your code, you know, because you you might write a code for a client and for a server. So these could be completely separate code bases. So I and I would expect a connection. There's some Yeah. That's a new question. And maybe connected to that, how do you handle the network layer where someone reorders messages or the likes? That's a really great question. I should have a backup site for this, but, basically what happens is, similar to other, protocol verifiers, you just kind of specify the entire protocol in one proof development, so you specify the client and the server together. And indeed, you actually might these, like, could do gothic invariance that tie the 2 together and then ex the extraction pipeline gives you implementations of each party separately So you kind of write a group global protocol. And to answer your second question about the network, So I'll assume a kind of a generic fully untrusted network set up where the adversary is kind of"
  },
  {
    "startTime": "00:30:04",
    "text": "communic, you know, serving as the network. So, like, message reorder, for example, is like already built in because you assume no properties at all about the messages coming from the network other than the other than implicitly they're coming from this adversary. Great. Thanks. Chris? Thanks very much. This is, this strikes me as, like, everything that I want in a tool like this. So I'm very skeptical But, but, like, yeah, I mean, really, really cool work. Here's my main question about I really like the the idea of expressing site, you know, sort of like, what you wanna prove about the thing in as a type system, like, as a as like a programmer that's really intuitive. But as like a cryptographer, so how do I connect Like, I I I'm wondering in one sense is this computational security. So how would I how would I express, like, in the CPA for symmetric encryption. In owl. That's a yeah. That's a great question. So, So Al kind of takes a different perspective related to other kind of computational provers. So we kind of bake crypto assumptions like NCPA or cyber tax integrity, that kind of stuff, that's kind of baked into the type system. The type system kind of relies on a certain crypto primitive being in CPA And from that, you get out security, assumptions. For example, you get out that, you know, when I decrypt a ciphertext as long as the key remains secret, the ciphertext contains what it should contain, and this is a site, you know, coming from cyber tax integrity."
  },
  {
    "startTime": "00:32:00",
    "text": "So you don't specify crypto assumptions directly. But Al comes with crypto assumptions, and the time system is relatively extensible so you can add new wins as well. With some effort. Very interesting. Thank you so much. Alright. Alright. Ori Steele. Thanks. Fascinating talk. I acute for essentially the same reason. I just want to understand, con how computational security is achieved. And it does sound like It is baked into the assumption that the types are implemented consistently in binary and that that that it's not just the type system, but it's the compiler for the specific type systems. So there's sort of this other layer underneath the type system that's ensuring that property. And then that raises the question of, like, for what different classes of type system with their unique compiler versions is the property maintained. So, I know, I don't I'm not a great Rust programmer, but, you know, can I use other type systems for this? At what point do I pick a type system like type scripts type system? And then I, like, lose some of these properties, perhaps. That's my question. So, I would say that, like, this property is So first, let me explain a little bit about how it works. So how it works is We have, like, a very specific type system that has, like, certain features that you don't find in type systems like Rust or TypeScript, for example, if it refinement types, and which means basically that you can prove arbitrary predicates about programs, in the tight system. So it's a very expressive one. And the second thing I wanna say is that the the way that we guarantee security is a type system's proof. Which means that we kind of do a crypto proof about the protocol, but kind of on page birth. That's kind of generic for every protocol and the thing that you, like, induct over is the typing judgment."
  },
  {
    "startTime": "00:34:00",
    "text": "What this means basically is that the type system and kind of the proof that the type system guarantees certain cryptographic security guarantee is kind of like fully intertwined So there's really no notion in which you could like replace it with like another type of system. More of an ocean of like you extend the type system in certain ways to get new properties. Thanks. Great. Thanks again, Joshua. I'm sorry for the messing around. Getting in. Projection working and so on? Yeah. Of course. Thank you. Thank you. We're trying to hand to control. Okay. Good to go. Great. Hello, everyone. I'm Lucas Bali. I coach out of that quick working group. Some of you know that, that's a transport protocol. But, actually, I care more about application protocols, so I'm kind of a moon lighter. But really what I care about is more about the the, the interface between application protocols and transport protocols is kind of weird thing that maybe causes interactions that aren't necessarily easy to predict. I'm not gonna talk specifically about that, but it will come up in the slide. So when it does, I'll try and remember to mention it. If I don't, I've got that out of the way. So HTTP 2 is second version of HTTP. The click is not working. Sorry. Thank you. And this record reset thing is is new. It was then between the last I ETF and this IETF hence why I'm a bit tired. Because it affected various big cloud operators around the world. Cloudfire, for instance, Mind Warrior, but others that you may have known about We've talked about this, this up to but I wanted to focus on the protocol aspects of this and to really ask for room researchers and folks here. Is that something we can do to think about analysis of this protocol. This is a protocol feature It's not a bug. It's a feature."
  },
  {
    "startTime": "00:36:02",
    "text": "That caused some issues in some implementations or deployments, not all, And this is something that was discussed during standardization. But evidently not changed. There were other things in 2019, that were identified again, not bugs in the protocol, but facets of how implementations work. Could lead them to over commitment of states or overuse the CPU given certain, protocol traffic patterns. In the HD. 2 layer. So I wanna explain a bit about HTTP 2 just to get a baseline requires talking a bit about HTTP and then we'll go into the yeah, this is like the biggest layer 7 loss we've ever seen three times our previous and it's a really simple attack. Why didn't this come to light earlier? Because this protocol was standardized many moons ago. So just very briefly, yeah, HP request results protocol. We have these drafts that talk about the semantics. Like messages, get requests, responses, catch sharing, etcetera, etcetera. Those things that come in across all versions of HTTP. And then we have these different wire formats, HTTP 1, 1.12 and then 3. There's not much difference between what they can do. Feature wise, but how they look on the wire and how they behave how the control plane of those protocols operates is is subtly different. There's more in common between HB2 and HB3. Only real difference is that connections over which these things run vary by the the transport layer underneath either TCP and TLS or quick I'm ignoring plain text because It's Lane. You have these message exchanges. Right? You've sent a request which is a composition of some metadata that includes the method get or post, which mean different things, a path host"
  },
  {
    "startTime": "00:38:00",
    "text": "there's a much optional content in the old days, this is for features payload, but now it's called content. This is data, and the meaning of that data is described by other parts of the it showed, you might post, an image of the puppy app and can include a content, encoding header that says it's gzip or the the the content type is a JPEG or something like that. Similarly in a reverse direction is the server's gonna receive that request. Do something with it. What that is. I don't know. HP is built around intermediaries a lot. So even if the server that receives that request, could respond, it would need to pass it to some back end to present that. And do something with it. So just bear that in mind as we go through. Again, you can respond with headers and optional content, etcetera, etcetera. So HP 1.1 would use a TCP connection. The cylindrical pipeline in, we're just gonna ignore that today. Effectively a single HTTP 111 connection could be used for multiple requests and responses. But they need to be strictly serial due to the way that those things a serialized and written to the wire. There's no way doing to leave them safely. We could go into the details that are boring, but, effectively, you have to do a whole request. And then wait for the whole response to come back then you can issue the next request. And you keep repeating that. Web browsers do clever things by making more connections. Again, nothing to do with that Yeah. Yeah. Hb2, it's different. It's better. A single connection can can be used multiple requests and responses, but there's multiplex and concurrency. This is the the reason for the design is to address the problems we had with 1.1, driven mainly our own performance. And the way this works, a nutshell is that you just divide the connection into into these logical streams. Each one has an numerical ID. Know we divide those messages requests or response into different parts. That carried in frames that are then conveyed over strings. So the metadata bits I mentioned, those carried in a header frame, And then the optional data parts are in Take time."
  },
  {
    "startTime": "00:40:00",
    "text": "Premise. And you can multiplex then. You have identifiers that link specifically the parts or sub parts of these messages, the streams that they belong with I'm not gonna make that out sequence diagram faster. The other sequence diagram is BS. Because actually when you look at the the TCP or TLS layer. Client's gonna generate a bunch of these things, and then it's gonna omit them. And the receiver's gonna collect them into a buffer or into a TLS record, it can actually decrypt in whole. And it's gonna read all of these frames all at once. In this example, the client has, is is showing 3 requests 3 had this frames, to the server, and the server will respond to them. It could respond to them in any order. Like I said, these could be interleaved, but in this example, so the sending back headers and data for the the three streams that were requested. And this comes into, you know, concurrency and parallelization up in kind of hinting at, but, This is cool, but it's also an immediate. Anyone well, you can do much more work in one connection. What's, like, that's good. But it's also bad because it's much easier to generate a lot of work and allocate more resources on the server that's doing this thing. Concurrencies isn't new. You you can just create loads of TCP connections and hit a server with hp 1.1, but traditional dust protection can look at certain floods or various other things. So HB2 kind of changed the game a bit. Lots of folks were able to adapt to that and modify their systems. Or we build protections into the protocol itself. So we have this thing called map concurrent streams, which allows a server or the client for the sun serving this case to declare how many concurrent streams that they're willing to support if you try to open a stream, and, you know, that you're that string will counter to the limit If you close it, it will not count towards the limit. Hit the limit and you're trying to in the stream,"
  },
  {
    "startTime": "00:42:01",
    "text": "the server will reject it. It's not a connection level error. We'll just say Sorry. Can't do that right now. Try again. Try again. So just part that a moment, and we're gonna think about cancelling requests you might have all something where you're doing a large file download and you decide you don't need it anymore. You just wanna turn it off. And then you can do that with a 1.1, but it's very turn the the only way you can really do is tilt kill a TCP connection. In H2, that's that's even worse. You've got all of these concurrent things happening if you just want stop one transfer of beta. You don't want to take down everything. So HP 2 defined as other frame called reset stream, which allows cancelling just one. The climb If you go back to our previous example, could send these we had these frames and then decide even before the service responded, actually, I don't need for this request. An example of this is just doing a page navigation. You know, you you've loaded one tab, and now you switch the other tab. Try and reuse our connection and make the the best use of the bandwidth for good throughput. And you can see that there's service only responding to those things, took that opening and closing streams. Every stream has its own life time model. It's even more complicated than this. I don't have the time to go into it in detail. But but There's various states that streams belong to and the, the sending and receiving of frames drive the state machine. Through various processes. There's this flag called end stream. Again, congo into that right now, but what you can see in this example is very quickly, the client and the server are synchronized in their view. This this this because it's a reliable transport underneath. Since the client sends a frame, it will know that the server will at some point receive it. And then the state has transitioned from idle. Effectively rapidly through closed. And that stream credit gets consumed and then returned immediately against the settings concurrent stream limit So as long as the limit's bit bigger than 0,"
  },
  {
    "startTime": "00:44:03",
    "text": "clients are immediately doing this, the concurrency limits effectively useless. And this is the crux of the record reset issue. Is that part of the protocol protections didn't help. But also that Well, from a protocol perspective, just a HD TP 2 server, the act of creating request date and tearing it down. Not really that hard. Yet. Higher plan on multiple other services, sought some kind of denial of service this is a big spike. It's cool. It's on the on the blog. Go and read that So the question is why why was this happening actually? I said This affected some, but not all implementations. From HP intermediation, like I say, that that there's a general kind of design pattern that you'll have a TLS decryption proxy in front of all of the business logic. This does more than just TLS decryption. It it takes hp2. And and converts that wire format into the common semantic that it can then pass upwards. Everyone talks HP, the wire vision doesn't necessarily matter. And what we have here is bunch of incoming connections that all have a bunch of strings on them. And the connection between that decryption proxy that's turning them into messages. Each message is then conveyed over a single units domain socket to the business logic proxy. And, effectively, what we saw was that the act of destroying that domain socket had some kind of lag to it. And there's a finite number of them. Although the TLS decryption proxy could keep up, the, the, the communications between the two couldn't, and effectively, we just ran out sockets and start over to errors. For my decryption proxy. And that's it. The high level summary is that the protocol feature is working. It's designed and that was published in 2015. It can be abused. So exploit characteristics of some implementations on the deployments of those implementations. Resulted in some musculotides in 2023"
  },
  {
    "startTime": "00:46:01",
    "text": "the potential of this is already discussed, but you know, we thought it'd be okay. I don't know. I wasn't part of the discussion. It's all on the mailing lists. I just wanna discuss, you know, is there anything about formal methods that can maybe provide an opportunity to do better? Not to text something we didn't think about, but maybe model it, is the modeling and the tuning that we haven't discussed which is beyond my comprehension most of the time. Does it connect and model this kind of, wider system effect? If there's any questions or comments, I'd love to hear. Thank you. Hi, Phil Humph Baker here. I'm I'm very I like this presentation a lot not least because it wasn't my fault. To answer your question and I'm having to dredge set for 30 years ago. Now, now I believe that you could model this using CSP. And I think that what you would look for is to prove that the, traces are bounded in certain ways I think that you could probably arrive at a proof I basically what you need here is to have better feedback in the back end system. And that's what it comes down to the the the the the problem is one of system composition rather than the protocol itself. But, yeah, it can be modeled. Yep. And to respond to that. I just forgot what point I was gonna make. Sorry. Like, Yes. This is we feedback is very important. Delay in control loops results and bad effects, I think."
  },
  {
    "startTime": "00:48:00",
    "text": "Yeah. Yeah. The problem there is, getting into timed models, Yeah. Like I say, the the this isn't a problem until it it becomes a problem. You know, you know, people were already resetting requests a lot. They could it's fine. But at what point is too much, too much. And and that depends on actual limits of real things, simulating that as possible. But To me it seems quite hard So So, I'm not gonna stand up at the line. I'm sorry. I'm very glad, Beatrice said, CSP because that was exactly what I thought would be how your model does decide. What what is CSP? Communicating sequential processes, I was gonna say late sixties. Yeah. Tony Hall, late Well, certainties maybe. That's cool. Thanks. There there are probably more modern tools now. But but but but I don't know that. Right? Okay. If there are no more questions, then we can thank the speaker again. Okay. They want them to stop sharing slides. Okay. And, our last speaker is gonna be Corey. You know, husband George if you don't mind doing that. Good afternoon. Name is Corey Myers, and I'm here just to give an update, on the the last meeting about sample problems in this working group for for for recent review, excuse me,"
  },
  {
    "startTime": "00:50:02",
    "text": "one of the things that we talked about at, 117 was, defining sample problems that can bridge work of this research group. With what working groups on the IETF for doing. For the goal of both bringing, non parole methods people, which would include me at this point. Into this work and, also providing basis for comparison of what different tools, can be used for in a in a tractable way. Steven had introduced a draft, proposing, modeling. I'm now search, and in discussion that was interested in finding protocol self contained enough that it could be modeled in its entirety. For rigor, other than just pulling out, my command to a larger protocol. Hannah volunteered to do OAuth Depop. And I'm looking to sort of apprentice myself to a learning task here I'll enter a top out with that. I could have the next slide. Yeah. Thanks. We wound up doing something entirely different based on had us this familiarity, with the TIP protocol for trusted execution of our provisioning, we decided that it was a good candidate for modeling for several reasons. And we into those reasons in more detail on the draft, but the 2 that I'll pull out here are wanted to protocol still under active development, And so, modeling it is a way of testing the design decisions that are still being made and standardized, and also a way to use the process of modeling it as a way of getting more people familiar with it within the IOTF. And the second is that it's expressly concerned with security. And so, can be modeled not just for its liveness and correctness, but also for the security piece that is designed to achieve Again, to tell in the draft, but those are sort of the 2 to highlight for now. Thanks."
  },
  {
    "startTime": "00:52:03",
    "text": "So we've got 2 revisions of the draft, currently submitted, and completely in parallel, for the teeth working group has actually, proposed 2, draft models of Deep, 1 in Prover, 1 in Tamarone and, kind of by analogy actually to the purpose of defining these sample problems there's a nice cyclical relationship there too and that we can use the sample models to test the draft of actually specifying that as a problem as well as, examine how the models, demonstrate or don't demonstrate we suggest the, the other protocol itself is useful, for proving we'd welcome feedback, on the draft otherwise defer to the chairs and the reship group. For next steps. Thanks. K. Thanks very much. Any questions for Corey for a Yeah. Excellent. And, since the I'm up search. Example was mentioned as there, there has been no progress. I'm afraid. Sorry. But I will I just I will get back to which case. Nothing. You're looking for feedback. Yeah. We have just dip So, yeah, 6 So, one of the things we want to use the, last few minutes of this session to discuss was how people failed the training on Sunday. Went. Was there anybody who attended or who didn't attend? Who thought it was good, bad, could be improved, any feedback is much appreciated. So"
  },
  {
    "startTime": "00:54:02",
    "text": "Just how many people here were at part of the training and something Alright. Right? So it's it's entirely fine to send us your feedback as an email to the chairs. Our our coffee, plus been appreciated if, if anybody now had the chance to offer That's right. Chris. There are two questions I'm sorry. We'll take Chris and then Chris. Life I did attend training. I thought it was actually excellent. But I will say that from a recording point of view, I'm not sure it would have too much value because the real value in my mind was the fact that did exercises and there were experts in the room to help us. And so I don't know that it's repeatable in any way other than to get people in a room again. And so that was the one downside for me. You have really had to be there to get the value out of Yeah. Crispy. Yeah. I agree with Chris. Basically completely. Let's do it at the next IETF. Maybe I mean, I think getting deeper into Tamarine would be great. But, I'd also like to get exposure to other things that people think are about you. So I don't know if we're pitting pro verif against I mean, our first draft is gonna pit pro verif against hammering, it sounds like. Which is great. So maybe we learn programming next time. So, before you despair would you be interested in, tutorial on something more general like cock or what Tayo was mentioning yeah, court side. I mean, Fiat is not a general tool. As I understand, it's for generating, like, finite field arithmetic, basically. In different languages, which is, like, useful to know for sure. I would say cock a hard note just because it's too general. I think I think directing, like,"
  },
  {
    "startTime": "00:56:01",
    "text": "protocol designer like myself towards the thing that is most successful is is the most useful thing. Thank you. Dougley. Felix, thank you for all the training. But, so it was more, a hands on training to try it out Tamarim but I could have enjoyed, more of a lecture That's what my feedback is. Okay. So I think we've drained our queue. In which case I I wouldn't ask Do people agree in general with Chris Pete that they would rather security tool focused training, or do we think next time we could do cock or some other more general tool or sign up for security focused. As I think that is in our charter. Okay. Or you've you've you've got people in here. Yeah. Oh, we've got people in here again. Excellent. Ah, yeah. We we we had one call from the room for general. The record. I very much appreciated the presentation at 117 about Isabelle. And that seems like a good somewhat more general tool that could be, accounting for both as well. Okay. Thank you. Phoenix. Yeah. I might be a bit biased for that. I find, active theory improvements to be very intimidating. And I think especially in an audio in, like, in a research group that's called usable format methods. I don't think that interactive your improvers are the tools we should try target to make more usable in the beginning. Like, they are the pinnacle of what humanity can prove, kinda, right? And I mean, they are great tools, but they are so specialized. I would, for example, allow a session on Provery because I don't know the tool very well and I think these tools"
  },
  {
    "startTime": "00:58:01",
    "text": "can also be reasonably taught in a time frame such as the one we had on Sunday, which I don't think to be true for theorem proofers. K. Thank you. Alright. Hi, Ori Steele. I think Proveriv can be easily taught in a shorter period of time. So I don't claim to be an expert on it at all, but I've learned I picked up enough of it to use it in a weekend for something. So I think it it that that one can become compressed. My the reason I came to the the queue was to act actually I'm following, work in the rats working group and in other working groups the deal with serialization formats and verifying the, the structure of binary formats that are based on seaborne, cozy, those kinds of things. And I've noticed some very interesting commits to those repos that are generating those drafts that are doing interesting things where all of their examples get piped back through all of their data definition languages And I'm wondering if there's interesting GitHub, magic, in any of the repos that you're aware of that's doing that kind of thing for protocols yet. So just I'm looking for stuff to to look at that's like that, but not at just the schema validation layer. But that's a protocol error. I'm not aware of any, but if anyone is, that'd be super cool. Mohammed. Yeah. Hi, Salma from T U. Dreston. So my feeling from the training is the first thanks for putting in all the efforts to at short notice, like, he had to go from 2 hours to 4 hours. It was very nice. But my feeling from the from the training was that the server that was initially done, which was that we should target intermediate trainings the initial training material already exists is already done in RWC or whatever is online available I would agree with somebody who said that. Okay. So the the real value is going down there. And sitting there and see and talk to the talk to the, developers or let's"
  },
  {
    "startTime": "01:00:00",
    "text": "trainers. So in my understanding, all the people who were attending were really beginners, and had no idea whatsoever how tempering works so on. So so that that that's my recollection of the thing. So what I want to highlight is that it's that the initial survey that was done which came up the thing that we should target the intermediate is not probably the right way to do it. It's it's the basic which people at IDF will appreciate more like, the reps working group that he mentioned, I presented today our work which we have done for the verification with Arm and Lynaro. And, for the tip at the hackathon, we found a problem in the the protocol verification that was done, the initial birthday that they had done for the deep imposed imposed imposed which Corey just presented as the sample problem. So we had a look at that in the hackathon. We found that the way Proveri was understood. So there are 2 different levels of reachability there without going into the details. I mean, the reachability that the thought is the reachability was not really the way. So so it was at the horn crosses level and not at the applied by a close level. So it was completely misunderstood the people. The point I I think I'm making is that people at idea will appreciate more taking your tools rather than going to cork And even at the beginning, the the the tools, even also at the very basic level, the starting from without any knowledge of the formal method. So that's that I think will be successful like the one training that we had. And last comment that I would make is I'm not a developer of Proveri but I have been using it. I'm, for last 4 years, I will be very happy to support, Cartig, Vincent, or Bruno, if they are here, so I will be very happy to volunteer I'm not in Canada, or I think maybe in the next, if possible in the Europe training, I I would be very happy to give it here. Okay. Thank you. Yep. It's great. Thanks. And I guess for a time, Thanks for coming. Thanks for our notetakers, Chris and Joe. I look forward to the wonderful notes."
  },
  {
    "startTime": "01:02:01",
    "text": "And we'll we'll discuss on this, I guess, next step. And further to meet the next IETF and all those kind of things. So thank you, and enjoy the rest of your day."
  }
]
