[
  {
    "startTime": "00:00:48",
    "text": "okay have you everybody we\u0027re ready to start the session so people would need to sit down and be quiet so I guess like micro just volunteer to be note-taker network well great thank you very much Aaron is a JavaScript and Spencer is unfortunately not here but I already beaten into the room so he will co-chair from the meteorite day I guess yeah and we\u0027re ready to start the session and this to tease the area session and ietf 102 it is working do you have to eat it or no so welcome again this is a note well it also applies to t3 area even so there might be less document discussions but probably you\u0027ve seen it already this week this is the agenda for today yet again thank you for airing to scribe for Java thank you for taking notes Michael we have 15 minutes for the t\u0027s VAV overview because we have the overview and we also want to talk a little bit about m80 experience and what like what we do in our daily life so you can get an impression and then Kenny date for the next eighty position that\u0027s available in March and after that part we have three presentations and we kind of cluster them all into like deployment issues and considerations the first one is from Christopher ash on magic our CCP and like the difficulties to bring this into the Linux kernel then we have actually a presentation from Ian who which has a different title now and I didn\u0027t update this light so this is on quick deployment challenges and then the cert presentation is on wire images and passing else on from Brian and tete which considers two documents which are under discussion in the IAB and then if everything runs well we still have ten minutes open mic at the end any agenda bashing okay um as always a big thank you to our review team and we\u0027re still kind of trying to get everything up to speed but we see more views and we see those videos in a "
  },
  {
    "startTime": "00:03:48",
    "text": "timely manner which is really important for us so thank you for that but we also still try to figure out how to optimize the workflow and kind of the videos so in case you\u0027re interested to become a transport area reviewer you can talk to us or you might talk to you at some point Spencer if you have any comments interrupt me right yeah sure and this is the useless light we have on the working group status from like the 80 per spective it\u0027s a very brief high-level view so if you\u0027re chairing you disagree don\u0027t worry or talk to us from my side I have a couple of working groups which which are wrapping up their Charter which is very positive I think so we get some work done there and then I have also some more long lasting working groups which make good progress the only thing I would like to mention here is that we change responsibilities for I ppm so that was Spencer\u0027s group grooving now it\u0027s my group and Spencer do you want to comment on your groups just to say that the connection ID for quick is really hard and I think the other thing to mention is you know we are looking at several groups that are you know finishing up milestones so we\u0027re kind of getting close to recharter or conclude for several working groups so just kind of keeping that the back of our minds are you saying we\u0027re looking for more work I\u0027m not sure what I well okay I think well I think I think what I\u0027m saying is that though then it would be good if we did not get it my replacement starting in March and then all the buffer quest requests are start arriving in April very true any questions on this part um yeah we have also done some work I think this is actually less documents than for example last time but that\u0027s like I think in the normal variation also there are a couple of documents which are under I used evaluation currently so that\u0027s the progress we make since the last meeting um yes Spencer do you want to go on this one yeah so we were contracted by Kemp to talk basically the BBF has been relying on TLS keepalive mechanisms in the net comp server and everybody post heartbleed is removing support for that as fast as they can tithe they were the suggestion was made over around in the that cough world about using TCP keep lives and the sec hey these were not "
  },
  {
    "startTime": "00:06:51",
    "text": "happy about Playtex mechanism for keep alive so we were not happy about encouraging transport only keep alive for several reasons so we had a couple of couple of proposals there one was you know could they could they actually talk people into not removing RC 6520 support which is humanly possible but I wouldn\u0027t bet on it the other one is just basically saying that they need to do application level keep lives anyway because that\u0027s what matters more broadly we\u0027ve been asked about providing some kind of consensus guidance about this that\u0027s not just you know me and Maria and a couple of second ease opinions so you know basically a statement that would recommend against using plain text keep alive mechanisms for secure transport sessions recommend using application level keep lives to actually test liveness there\u0027s a there\u0027s a thread ESP aerialist call statement regarding and keep lives and we would you know there\u0027s minutes of discussion under that thread already we would be interested in hearing more so I think Ken Watson who asserted this reticence room do you want to stand up for a second can see you and if they want to talk to you so one question we also have is what to do with this statement right is this something which we just need to write down and send an email somewhere is this a BCP is something the IAB should figure out what to do with like any feedback on that is welcome yeah thank you all I can\u0027t from juniper just added that it\u0027s not just a statement about not using the T speaking level keep a lives but also a recommendation for protocol designers to include keepalive mechanisms inside their protocols which is many times an oversight right thank you okay I go to the next slide yeah this is also related work we want to point the transport area to so that\u0027s a congestion control mechanism that was or is developed for cope and mainly discussed in the core working group and but be given given this is a congestion control scheme there will be presentation in ICC oh geez so a transport feedback is highly appreciated in ICC Archie on tomorrow afternoon I thoughts and Friday okay tomorrow good yeah thank you in advance so I\u0027m still the outgoing transport area director my term ends in March Martin and Mary and I have been working since 2013 to make the job doable and there\u0027s a link in the presentation for what we told Malcolm which is actually pretty close to what Nam Kham is using for their position descriptions "
  },
  {
    "startTime": "00:09:51",
    "text": "if you just look this up on the NomCom page please nominate freely and please give lots of feedback on willing nominees because this really matters if you have any questions about the position please ask you could ask here now Marius in the hallways asked the saw the TSV area list that would be fine or in private email and again thanks for thanks in advance yeah and I also saw ty talk a little bit more about what doable means you should be a conscious people to my Pecha Kucha but in this case maybe more information might be interesting for you so I mean what does the the daily life or from 80 nowadays or transport 80 look like and I think those spent and I will spend like 15 to 20 hours a week might be sometimes more might be less the only problem I sometimes have is that\u0027s really 20 15 to 20 hours every week so if I go to a conference I will be more hours the next week but yeah it\u0027s still a reasonable amount most of this time is really reviewing in I use G evaluations so it\u0027s reading drafts of all different areas trying to understand as much as possible but and it\u0027s about it like we have now a limit of 400 pages for each telecheck so there\u0027s a telecheck every two weeks which is about like 10 to 20 drafts usually but I mean also you know drafts right if you have a 10-page draft is there any kind of five pages you actually have to read because it\u0027s front and pacinator and whatever and then sometimes yang models where you also don\u0027t have to read every line so 400 pages is actually realistic here if the draft is short enough and like not not very related to transport I can do like a draft in 30 minutes but like if there actually is a problem and I have to understand thing and it of course takes longer still um then there\u0027s also of course a little bit load that is related to the working groups but we in transport only have 12 working groups so it\u0027s like six working groups per ad and I actually looked at a published 25 are seized last year that\u0027s also a reasonable load and many of them was actually on Spencer because a big working group which was wrapping up a lot of work so it\u0027s actually even a little bit less than that some steering is involved from time to time but we also have very good experience chairs who know what they\u0027re doing so it\u0027s some steering coordination some feedback but it\u0027s not like a huge amount of time there are other things the IHG is doing like everything related to the IETF process itself and we need people doing this in the I use qi but usually it really depends a little bit on who\u0027s interested in the topic who has time to do it so like I myself don\u0027t take up a lot of these additional responsibilities it really depends on your Co time commitment there\u0027s one thing transport is responsible for in that support registry and usually that works on it earrings we have very good reviews there but sometimes there is a question and then you have to care about "
  },
  {
    "startTime": "00:12:52",
    "text": "it but it\u0027s like in terms of time commitment is it\u0027s also not a huge thing right yes so that\u0027s my my day any question Erin Faulk for a brief unhappy period I was on the iesg mailing list fortunately somebody fixed that how much of your time you spend reading is Gmail so I actually try to manage to keep my I 80 work on like two days a week so it\u0027s usually Monday Thursday and sometimes I don\u0027t read those emails the other days or I have a very quick look but I mean after all it\u0027s also it\u0027s not that you have to read all the emails because it\u0027s like it\u0027s it reviews its comments to reviews on oh I don\u0027t know everything is weatherman for you so figuring out what\u0027s relevant for you takes a little bit of time but still it\u0027s like you don\u0027t have to read everything right so it\u0027s I guess it\u0027s an hour every morning I don\u0027t know if I could yeah so what I ended up doing was bullying my ISG email into documents that I was responsible for documents that I was not responsible for on intelligence and kind of everything else so that allows that allows me to prioritize a bit the other thing I would say you know Maria Maria\u0027s prefer referred to this a couple of times but there are things that the isg needs to do things that the ice tree needs to worry about but we don\u0027t need 15 people worrying about them so you know that would not actually help so you know basically if you find some topic that you\u0027re the right person to work on or there are three people that need to come up with a proposal for something you know you spend time on those if if they date you usually don\u0027t so I have to say like at the beginning and freak me out a little bit to see all this park mail is your friend yes so that\u0027s this pretty I mean like now would be a chance to ask more questions or you can just talk to me in the hallway I guess if you catch me and seriously there\u0027s there\u0027s we we are we have limitless interest in talking to people and answering questions so please don\u0027t be shy okay then we start with our first talk Christophe is coming great and just as "
  },
  {
    "startTime": "00:15:53",
    "text": "we\u0027re talking as we\u0027re starting this I\u0027m gonna go ahead and beam off here so that people can use the mic line if anybody needs to talk to me just yell and I\u0027ll raise my hand hello so this is a modified version of a talk that we have been giving last week at the Linux Network Developers Conference this is work where we are working together with a bunch of people there is Matt Martino and Peter crushed-up from Intel much about from casares and myself and so the bow met and matured they presented that last week at Linux Network developers and I\u0027m presenting here a slightly modified version that shows more the IETF and protocol design impact on up streaming MB TCP as most of you know MB TSP has been our experimental standard for I think maybe a few years and we since beginning we always had linings implementation that was implemented in a standard however it is still not upstream inlets from Linux kernel and some are surprised by that and there are various reasons for that and I will talk to one of the some of those now so the question is one is wise up string MPGs be actually so complicated well first of all for those of you who know the Linux kernel the TCP stack it is highly optimized right they can go up to almost now I don\u0027t know I finger Hahnel Giga bits per seconds for a TCP stream any addition of additional fields in structures incurs additional cache misses which basically kills performance and if statement will introduce additional branching they are very sensitive to all of this the maintainer of the Linux kernel networking stack right and because of this the stack has become extremely optimized so every change in the Linux TCP fast path is being scrutinized heavily then there\u0027s also the original implementation the one where everything started off with that we created back in the days when that Sebastia body actually began in days for those who know him and that I took over then and this implementation the goal was a bit different our goal was not to upstream it immediately the goal was basically to have a way to quickly iterate and implement the standard as the standard was evolving as the draft was evolving we wanted to be able to experiment to get quickly numbers out of this currency empties the implementation to see how certain decisions in the protocol would affect the performance of MP DCP so our "
  },
  {
    "startTime": "00:18:53",
    "text": "goal was basically to have a non-generic stack and be quickly at iterating at the time and also we were researchers so our goal was also to write papers and not necessarily to upstream code to the Linux kernel now over time this implementation has evolved into a more stable version and actually nowadays there I think millions of devices that are using it people have heard of Samsung deploying it in Korea and all of this is based on this particular stack but all of those deployments that are currently out there there are still very special purpose deployments where the one the system administrator was deploying it has tight control over how MPTP is being used now we want to evolve and the MPSP implementation and Alana\u0027s kernels so that we can actually even more easily be integrated into the Linux kernel and that means we have a few limitations first of all that means they are not there can\u0027t be any performance regressions in a regular TCP stack that\u0027s the non plus ultra condition the certain second one is we want it to be maintainable and configurable the current like special-purpose way of deploying MPSP can\u0027t be done in a generic implementation that might be used by android and many other systems and so we won\u0027t so want it to be deployable in a variety of deployments so an ideal implementation would basically look like that right you have the socket layer below that you have an MP TCP socket and then you have to TCP sub flows over this that would be we have a clean interface between the different layers and we would just basically send data either over one or the other TCP sub flow and receive it as well that that is the ideal empties p.m. temptation however the reality looks rather like this we have data structures sitting at each layer we have callbacks from up and down from one sub floor to the other sub flow and all of those different interactions data structures and calls are making the upstream very complex and I will show you how some of the protocol design decisions that were made have basically influenced this design and have led us to end up in this situation so that brings me to the protocol challenges and I must say this is not to like kind of put the blame on one or the other it\u0027s just like well because I mean I was also part of the protocol design so it\u0027s kind of we all we all are kind of part of this and also this is just how one part of the protocol specification kind of influenced influences certain decisions that need to be made inside an implementation so first protocol challenge is that the data sequence "
  },
  {
    "startTime": "00:21:53",
    "text": "numbers and the mappings right for those of you who know MP TCP right we have the two sequence number spaces there\u0027s one sequence number space for the data that is being sent and the sequence number spaces in the individual TCP sub flows and for example the data sequence space let\u0027s say here in this example goes from one two three four five and we are sending segments and let\u0027s say we send the gray part of the data we send it on the left sub flow and yellow part of the data we send an on the right part of the subfloor in the right sub flow so this means for every segment that we send we need to specify the data sequence mapping in side as part of the segment the MPT protocol says that this kind of DSS mapping is to fight inside a TCP option that is seems like a very obvious design decision and it was at the time when we were designing the protocols seems seemed like a good approach now however if we want to implement this it becomes a little bit tricky one of the problems is so we have the empty space socket who\u0027s basically holding data and then we are pushing data down on once up flow now if we have a clean interface between both right we are basically pushing data down this just memory allocation now we need to kind of add to this tell the TCP stack a I would like you to write was this segment this particular TCP option and so the TCP stack would need kind of to ask call back into the empty speech tag and ask hey what kind of TSS option right should I write but this introduces a lot of peg and force between the layers so the other solution that we would want to do is well why not just simply add this information well to DSS option as part of the metadata that comes with the data and if you know the Linux kernel it has basically the memory region that is holding the data and then you have what is called the SK buff which is holding the metadata so the obvious approach would be well let\u0027s simply add the DSS mapping inside the SK buff that is problematic for few reasons the first reason is that we can\u0027t just simply increase the size of this structure because the TCP stack is so highly optimized adding fields into a structure means potentially more cache misses and a cache miss can go anywhere from up to two knots CPU cycles and that will simply kill the performance so it is basically forbidden to add anything inside this kind of a structure except if you managed to do it in a cache neutral way the second reason why even if we would manage to somehow add this information into the sk buff such that "
  },
  {
    "startTime": "00:24:54",
    "text": "there are no additional cash line misses there\u0027s still a problem is because the TCP stack can at any point decide to split the segment and split them in multiple parts merge it with others when he has always used and it receives the sack the Lennox TSP stag is going to split those segments in different pieces and might merge them back with others so what that means anywhere in the TCP code where we are splitting and merging segments we need to make sure that the DSS mapping is propagated through the metadata as well again meaning more if statements into the TCP stack another issue of challenge that we are facing is that we are transmitting signals on TCP options and there\u0027s no clean interface in the TCP stack to basically propagate TCP options up to other layers usually TCP options have only a notification locally inside the TCP stack right let\u0027s take the example of sack the sack option will be treated will be taken care of by the TCP stack only same for these same for the timestamp option same for all the other option all most options that we can think about now let me give you an example of how the use of TCP options for cross layer signaling as a problem let\u0027s say we have an MPD speak connection with to TCP sub flows and one of them has basically notification my interface goes down at this point the TCP socket will be destroyed and we now need to send a remove address on the other sub flow so that means first there is a notification coming up to the MP TCP stack and then we need to push an information down on another TCP stack and telling the other TCP stack I want you to send an acknowledgment with a very particular TCP option right currently there is no no clean interface for those kind of things usually an application that is sitting on top of the TCP stack never tells this piece like hey send an act with this option right so that is something that is new and needs to be built and added into the TSP into the Linux kernel another problem known you can see this on the receivers site when we are receiving TCP options right we for example we receive the remove address option which means you need to kill the other TCP sub flow so receive a TCP egg with a particular TCP option first of all this TSP Ike looks like it like a duplicate ACK and usually gets simply dropped ibid TSP stack so we need to already cut down cut this behavior and in the stack then we need to pass the remove address option and "
  },
  {
    "startTime": "00:27:54",
    "text": "propagate it up to the MPSP layer so that ultimately then the MPSP layer can kill the other TCP connection so again we need cross layer signaling between the layers that are is originating from a TCP option so there are many of those signals in inside and M TCP inside a MDM PT specific specification and each of them need needs different kind of different kinds of behaviors and a different layers in the stack and so it\u0027s kind of tricky to make sure that all of this gets consolidated into a single point all those cross layer signaling are basically major pain point for the NP TSP implementation so how we\u0027ll have we are we trying to fit the amputees piece back into the networking stack in Linux so first of all having this layered approach is very clean it fits very nicely into the existing stack we can create a socket with a certain type that basically is sitting like a shim layer between the TCP flows and the application there are some internal interfaces that allow to send data together with the meta meta data with the SK buff and also to read it as well this kind of design fits very well the challenge comes later on nor for the for the cross layer interactions one more obviously obvious obviously think that adding this like indirect list for indirect costs because that allows to have a generic approach it allows to make the TCP stack not MPGs be specific which is something an operating system always tries to avoid however since spectra and meltdown indirect costs have become increasingly extremely costly and so the Linux kernel developers and maintainer are basically avoiding indirect costs so annoyed calls are no more solution to have an generic implementation and the other problem that I already mentioned is the sk bath non extensibility if we want to add any kind of metadata to to a packet or segment that is being sent it needs to be done in such a way without increasing the size of the structures which is very tricky so what are the next steps for us to target up streaming so together with the people from Intel and Tessa rice we have been now been working roughly for one year on it and last week we presented our plan and our challenges that we are facing in a more detailed presentation than the one here today we received very supportive feedback from the Native community telling us that the "
  },
  {
    "startTime": "00:30:56",
    "text": "TSP maintainer said that he actually really wants MP TSP to be upstream so that was very positive feedback for us and he gave us also some suggestions on how to handle for example certain issues like the escape of non extensibility we started to reduce MPT speed to the least minimal viable implementation moving all the features just making em participe bringing it all the way down to the bones so that it can just interoperate with another implementation but without supporting any feature the cross layer interactions we are trying very hard to consolidate together in one single coil so that reduce those scars layer interactions we hope that we can make them in an asynchronous way so that we don\u0027t need to basically block on those kind of calls and we received a suggest on how to do the sk buff extension in a cache line agnostic way so this project there are a few links are out here on the on this slide and you can check it out yeah go ahead I don\u0027t let you finish do you have more oh just one slide for lessons alone I mean okay okay so in terms of lessons learned in specific specifically with regard to standardization here\u0027s what in my opinion is one of the things that we should maybe keep in mind in the future one is obviously the protocol design has a direct impact on the implementation right any decision that is being made can impact the upstream ability or the widespread deployment of the protocol one thing that is very tricky in my opinion is that TCP options should only be useful let\u0027s call it unreliable signals and not for signals that are linked to the payload because in an implementation it is extremely difficult to make this metadata move along with the payload because the payload in the end inside the DSP stagette can get splayed it can only get merged with other with other segments and so many TCP stacks don\u0027t have a real notion of fragments or segments right and so it\u0027s best the TCP options are best used for just unreliable signals like a sec option right it there\u0027s no relation to the data itself it\u0027s just purely related to the TCP header cross layer interactions should at best be asynchronous so that it the signal that is coming in through the TCP option can basically be queued and can be read out of the TCP stack later on if it\u0027s asynchronous it simplifies the layer separation between the different stacks also one lesson that also I learned "
  },
  {
    "startTime": "00:33:56",
    "text": "I was very much involved in the prototyping at the beginning of the linings and empties the implementation is that the prototyping is very different from widespread deployment and integration in an upstream car product during prototyping you try to be quick you try to iterate fast you do short cuts you do sometimes nasty things because you want to finish paper before the deadline and you want to get some numbers out of out of the implementation and so all of this is extremely different from the deployment in a real in a large-scale and in a real generic way and now I thank you very interesting I learned quite a bit I would I don\u0027t know if I would replace your first bullet but I would certainly add a second bullet that says the the converse which is implementations directly impact protocol design it seems to me that what you learned what your the the story that you\u0027re telling is that there are a bunch of constraints design constraints on the protocol that you didn\u0027t know when you are designing MPT CP that you discovered late on and that caused you have to do a bunch of rework so would you agree with that so you mean that in the end the implementation shape the protocol as well I\u0027m saying that you learn things like that like the the buffer the cache access efficiency was a constraint and you needed to change your design to accommodate that and maybe if you\u0027ve known that earlier on and what you might have made different design decisions so I guess let me ask it differently if you had known then what you know now about the constraints would you have designed the protocol differently yes absolutely okay so is there is that are those constraints written down anywhere and do you think they could be written down and do you think you could write them down not that I am aware of they are mostly on mailing lists on feedback coming from the maintainer right so I\u0027m not aware of any like written down list of constraints might be worth to write them down I agree seems for people who want to get the technology I mean we spend a lot of time in the ITF these days talking about making our stuff deployable right that\u0027s one of the reasons we\u0027re talking about github right we want to sort of reduce the distance between us and actually rolling things out and I think you\u0027ve identified a pretty big issue here which is like we\u0027re designing stuff that you know that the maintainer may look at and say yeah you can\u0027t do that and you know that\u0027s a real impediment getting it out there so it\u0027s a good point one question I would have Dennis then like those kind of constraints are very olynyk\u0027s but specific other implementations like like in iOS we don\u0027t have that problem we don\u0027t because we kind of don\u0027t care right so well there are certain things where we can say okay the benefit of adding empties be over weights the cost that it is introducing hi Brian Trammell I guess we\u0027re just gonna go down these bullet points oh I have I want to make a "
  },
  {
    "startTime": "00:36:56",
    "text": "point on point too I expect two more people to get in line behind me so this is a really interesting a really interesting lesson learnt easy options best used for on reliable signals right like so the further away you are from the core specification the less you can rely on it working everywhere and I think we kind of know that in you know I mean like we have you know the hops and the map party stuff we\u0027ve dug into that these things we\u0027re measuring more and more I think sort of a lot of middle box measurement work was actually um spurred on by MP TCP right because that\u0027s what it turned it started as a let\u0027s extend the protocol and it turned into a whole bunch of work on how metal boxes break the protocol there seems to be kind of a there\u0027s needs to be like a protocol architecture truth hiding behind this and it\u0027s something we we should probably consider moving forward when we\u0027re talking about extensibility and quick for example so thanks I\u0027ve learned a lot too you know one question is like if when we start adding multipath into quake and there will already be a bunch of implementations out there and will they will need to remain stable and performant will we be able to add multipath into quick easily that\u0027s a question I think so because we can actually design you know I mean there\u0027s been a lot of thought now in what we found out this morning is probably gonna be called version two so that when we go into version three that we were Lance on the ground work now and see IDs make it easier for example trait so possibly difficult question radically so um I how much how much of the stuff would you say is is specific to Escobar Farsi you and all the stuff that\u0027s specific to the Linux kernel like how like if you if you were to take these learnings and you take them to the iOS user space networking team how different are they I mean how directly applicable are they is this something that we do so there could be I could see two things first of all you write all this down and you come here and people don\u0027t believe you unlike that they don\u0027t believe you anyway so ok so but at least you\u0027re right but if you write it down but but if you write it down and he says look dude like in my implementation it\u0027s entirely different like the shed um constraints I have a completely different I don\u0027t have this locality problem because my architectures I have something else right have you compared notes with the people working on the use of space implementation to see because if you even if you like if you take this and you take it into DP D K maybe it\u0027s entirely different again or if use a FX DP to do it in user space which you maybe can do in 4:17 actually have you consider actually agree with you that like different implementations have different contexts and it\u0027s not that easy to to generalize all of that right Eric Kinnear Apple very briefly yes we "
  },
  {
    "startTime": "00:39:57",
    "text": "have compared notes we\u0027re all the same team but I think your point still stands in general is it would be really nice if we could take some of the stuff that we\u0027ve learned and seen and kind of put that out there as more of a as we go forward in the future and look at MP quick or other areas of protocol extension or even just first time design where is that kind of things exactly how can that apply the yeah we wish if we write something like this down I think we should try to make it implementation agnostic and try to yeah not make it specific to Linux iOS or whichever static yes right Google I had one comment on DMP quick thing we actually tried to take a whack at that early on and it turns out if you have your software structured well it\u0027s it\u0027s not overly complex and you may be at max introducing won\u0027t like one extra pointer drf in the worst case scenario but like overall the fact that in quick you\u0027re acknowledging packets with packet numbers instead of segments in sequence pace means that the type of data structure that is conducive to multi path quick more looks more like two entire sets of basket buffs and so you don\u0027t need to add things the sk box you just need two parallel data structures each indexed by packet number no no no no not from TCP for quick I\u0027m saying employ it\u0027s easier to implement and quick because of the natural packet number versus a sequence number difference that\u0027s and Lorenz could be a difficult question now that you\u0027re here how much of this do you think is due to the fact that you know decisions are right in the past that were absolutely in the best interests of the Linux TCP stack and it\u0027s gone really really really far down the path of being highly optimized and all those decisions like if you\u0027d had like multi path in mind at each of the decisions points like how much do you think you know would have it would have cost more because like I can imagine that at some point they were faced for the trade off and if they\u0027d had multi path in mind they would have picked a instead of B and they you know but they pick B because it\u0027s like well and how much you think that is and yeah it\u0027s a difficult question like you said yeah it\u0027s hard to know how much this has been involved yeah influenced those decisions but yeah would have been interesting if we would have gone directly upstream and and yeah taking all those things into account I guess should does this mean that we should if we want it to be deployable should this mean should we be saying oh that should we should we not ever or try ever not ever to say oh this is just an extension or an existing protocol so it\u0027s gonna be easy to implement right because because we might have said like you know well it\u0027s already there right the code is already there but if you if you have the structural problem then you can\u0027t get into a real-world implementation because you\u0027re trying to you have a chicken-and-egg problem and the people who maintain that "
  },
  {
    "startTime": "00:42:57",
    "text": "implementation is like no you can\u0027t make me 3% slower because you know because it cost me a lot of blood sweat and tears to get that 3% and took me a year right so yeah my name is Tim Sheppard I actually just reading this I know you mentioned Linux a little bit but just reading the slide I was actually thinking this is more about decisions that were made in the NP TCP working group which might have been made differently and I think you said something along those lines yes this is about D like for example defect that the TCP options that we are currently using right are linked to the payload itself makes things hard so that\u0027s was a working group decision to design the protocol that way and that makes the implementation very difficult I remember there was a MPT CD working group meeting which I was sitting there listening to where they were deciding to either use TCP options or put stuff in the data stream of the sub flows and it was the people in the room it was overwhelming use options but there was one person I can\u0027t remember who was I think Michael Shaw okay thank you for remembering that and I thought he had compelling arguments but essentially very few people I mean a lot of people shrugged I think I was the one other ones shrugging even though I found his arguments compelling and but there was a very large number of people who are like oh we\u0027re definitely going to use options because that\u0027s because it\u0027s a because it\u0027s multipath TCP and so it should use him TCP options so that\u0027s and the decision was made and and here we are and I\u0027m wondering could we think about MPT zbb - that doesn\u0027t try to do it all in options I think we have something to do in your working group date but the thing is like the option story is not the only problem right it\u0027s not like business like has doomed MPT CP or anything it\u0027s just one of those things that makes it kind of tricky for up streaming using option yeah because performance is not only focus of the film we designed the protocol in my understanding so yeah Thomas is not when we make on MPT we think other factors as well that\u0027s a result right and then that\u0027s the one common and then the other comment is then a new design MPCP from scratch you want to put the information in the payroll that\u0027s your conclusion I would put a DSS sequence number in the payload the data arc in the option right "
  },
  {
    "startTime": "00:46:02",
    "text": "redesigning mbcp to the energy working group at this point a move on because we\u0027re running a little bit out of time but I\u0027m happy that people are interested thank you very much so in your slides are perfect okay nope as this says I\u0027m slide I am Ian sweat from Google so this is actually taken also from Annette dev talk but this is much more truncated the net dev talk talked quite a bit about the history of deploying quick including a few more performance numbers and some of the other data but that all all that information pretty much has risen presented at the ITF either in the form of a bath or some other kind of working group activities quick working group or otherwise so here I\u0027m actually going to focus mostly on kind of deployment in terms of like CPU and software architecture and interacting with Linux and kind of some of the similar issues that that Christophe alluded you so at the beginning as an overview quic is a protocol for HTTP transport at least that\u0027s how Google had it deployed starting in 2014 between Google services and Chrome and mobile apps there\u0027s some rebuffering improvements some search latency movements it\u0027s about a third of Google\u0027s traffic as of sigcomm in 2017 7% of the internet and obviously there is a quick working group that\u0027s very active you know so all of this is good motivation for us to plan this more widely but there are challenges with that so here\u0027s our ramp up over time from the sitcom paper but um the thing I want to call out here is during the March to August time frame where it was all flat and there was no increase in rollout we were just furiously working on CPU improvements so some of these were kernel related many of them were just internally in our own software and that\u0027s why you keep kind of see a long period of a flat and then a huge jump read afterwards where it goes from you know about 15% of our traffic to 30 in the course of a month or so and so I\u0027m going to talk a little bit about how we how we did that and how we\u0027re continuing to work on new things to make things even better so initially the major sources of CPU crypto is a fairly large one although certainly not as large assume the other ones but cha-cha 20 at the time was was fairly slow originally and we did not have the avx2 assembly and that seems to make a ton of difference so you know just FYI like cha-cha doesn\u0027t have to be slow but you probably do need a relatively modern "
  },
  {
    "startTime": "00:49:03",
    "text": "Intel processor and so that\u0027s exactly what we did the other thing we did is actually we\u0027re using in-place encryption at least for the sending pad that you know consumes a little bit less memory and bandwidth and appears to actually have a few percent CPU gain so a bit surprised that it\u0027s true but but it turns out to be true we haven\u0027t tried it on the received path yet but I suspect it would work equally well we thought about doing scatter gather and you know kind of copy and encrypt all in one operation it turns out the api\u0027s for that are more complicated than doing in place so we stuck with in place just FYI so sending and receiving this is where like the vast majority of the cost goes so on Linux it\u0027s not uncommon for this to be sending to be something on the order of 25% of CPU on the machine receiving maybe like 5% 10% at the worst case scenario on Android I\u0027ve seen numbers over 50% presenting I\u0027ve also seen that on iOS so it can be pretty huge currently our two biggest kind of one is new and one is older the older one is packet rx ring which is a receive side optimization that allows us to share a memory buffer with the kernel packets get very efficiently read into that buffer and then we just get an up call and says like here\u0027s a ton of data like go at it and then when we\u0027re done of course we give it back to the kernel and rinse and repeat so at least in terms of profiles this is not quite free but shockingly cheap I don\u0027t know why but it is super so when it\u0027s available it\u0027s it\u0027s quite appealing UDP receive in general is actually not that expensive it was more expensive when we started the quick project but Willem and Eric doomsday made some really nice optimizations on the UDP received path over the last few years and so when we\u0027ve done recent benchmarks it was only a few percent difference between our x-raying and UDP received where is probably more like a 10 percent difference when we first launched it the other thing that has been added more recently about May time frame is something called UDP GSO and that allows the kernel to do segmentation not fragmentation but segmentation where you give it a very large UDP packet and it basically segments it for you now you have to do everything correctly because it\u0027s very important that the quic packet boundaries line up exactly with where it\u0027s going to segment it otherwise you end up with like Internet garbage but it turns out this isn\u0027t super hard to implement you just you know you have a mass packet size and you put that in and like everything works and of course you know we we haven\u0027t really been exploring it too much but kernel bypass is still a potential option especially on the receive side and I know that like we\u0027ve been talking with Apple about some of their newer api\u0027s particularly for iOS "
  },
  {
    "startTime": "00:52:05",
    "text": "11 or is it 12 12 sorry and some of those look very promising from a performance perspective in terms of client-side performance so a journalist take all of the issues that were highlighted before about about multipath and the challenges of getting multi path in our data structures are not nearly as optimized is the nice kernel as you might expect because we have been around for four or five years and gone through over 40 quick versions and they\u0027ve been around for 40 years or so you know maybe not quite that long but a very long time and had a lot of people working on it for a long time so we\u0027re still working on improving cache efficiency improving your data structures and minimizing copies and allocations we\u0027re pretty good on copies and allocations at this moment cache efficiency is actually still I made your problem like there\u0027s still some pointer following and visitors and such and I think we pretty much gotten rid of all the linked lists or anything else that\u0027s you know of that nature in our code because pretty much you want as much as possible to be in you know contingent our coherent memory that\u0027s like you know in large blocks and you do not want to be like following pointers to pointers to pointers okay so the last quirk is quick has encrypted acknowledgments unlike TCP that does defeat some of TCP is kind of receive side aggregation that I can do I believe that\u0027s our the giro mechanism if I remember correctly or at the very least some middle boxes do it for you whether they you ask them to or not and so do some Wi-Fi access points promoting here and so you know in general it\u0027s possible and quick to get to fewer acknowledgments but also more importantly you don\u0027t have to do decryption and they\u0027re relatively simple to process so our solution for that is we send ax less often so by default right now we are sending ax every quarter RTT or every 10 packets whichever comes first so at least with PBR that turns out to work rather nicely it doesn\u0027t actually give you better congestion control by itself but it saves enough CPU in the client-side that you end up actually with a slight increase in bandwidth it\u0027s kind of crazy but it\u0027s true it turns out that if you if you send one act per packet or run it for two packets and you\u0027re receiving very fast on Wi-Fi you run out of transmit opportunities and so estimation actually really helps even though you\u0027re doing nothing else wrong so and the other side of the coin is you know people have tried to get that into upstream Linux as well as like a we don\u0027t have this problem data centers go away and so we need something that order Tunes so I guess that\u0027s one of the advantages of doing it out of the curl yeah so ever want to talk a little bit "
  },
  {
    "startTime": "00:55:05",
    "text": "about how we\u0027re using sockets we\u0027ve gone back and forth with our kernel team about kind of what the recommended approach is and a server side application it\u0027s not clear that this should be the recommended approach for everyone but it\u0027s it\u0027s sort of what they came up with so the approach is to use the socket per thread sorry I should clarify a receive socket and a sense of separate lis per thread with s eries port for the receive socket so it splits all the traffic kind of stabili for you into you the number of threads obviously you can\u0027t change the number of threads after you start up the server but that\u0027s usually not a huge constraint we do use our X ring but you still and think end up having actually well with our exit ring I guess breezing a PPF but I think I say reuse port works with our extreme - you can\u0027t remember because our experiencers packet sockets so the app dispatcher is based on quick connection ID so you know it gets all these random packets for various different connections dispatches the correct connection you know hash table instead red stuff and you know at least initially what we did is we just had used straight as a respawn and if a packet landed on the wrong thread we would just toss it to another thread so that adds a fair amount of contention but now rebinding is in connection migration together are much much less than 1% of all connections so if you\u0027re doing this for you know 0.5 or 0.4 percent of all your packets it\u0027s it\u0027s somewhat acceptable in the world if we if we thought we were mostly going to have connections that were longer lived or a lot more connection migration then that birch might be a little less viable so and then the other thing I kind of that renzo got out of me is that yes you can also use a PPF for connection ID based earring and that\u0027s pretty simple yes so reason the socket bird thread for sending a sending socket per connection is mostly impractical from what we\u0027ve tried together we actually been going back and forth this week on kind of what the right API for that would be but at least my current understanding is that in Linux none of the API is do quite what you would like so you can\u0027t kind of follow the standard TCP accept and then create a new socket sort of like pattern that that pattern doesn\u0027t quite work for sort of connection oriented UDP sockets if I\u0027m wrong then then please correct me but this was certainly true when we kind of started this project three or four years ago also it turns out like a socket / connection is not necessarily that beneficial there\u0027s a fair amount of like state overhead in such per socket and we\u0027re not really using it for anything at the moment we\u0027re not using like pacing offload or anything like that and so it\u0027s just a basically we\u0027re just "
  },
  {
    "startTime": "00:58:05",
    "text": "looking for the cheapest way to get a packet onto the Nick as quickly as humanly possible oh I should mention since we\u0027re sharing socket among all of these threads and all of these connections we do use application layer pacing and we only paste 1 millisecond into the future which is fairly comparable to what Linux does internally in the FQ pacing acutest and that allows us to not have an insanely large socket buffer otherwise if we send you know 10 or 20 milliseconds in the future we would need multiple megabytes probably in the order of 10 to 20 megabytes per thread of socket buffer just to hold all the packets but bouquets were only sending one millisecond in the feature it\u0027s more on the order of like 256 K to a Meg depending on the circumstance Spence Minich yes but we still do have to use a larger than like default socket buffer one issue that we do have with the setup is the fqq disk creates some unfairness between quick and tcp so if you do get into a situation where your NIC limited quick will sort of suffer relative to TCP because all the quic flows get as much egress out of the box as every each individual TCP flows flow so you know you\u0027re basically getting in the worst case scenario maybe I 100x less throughput then then needs to be in practice you have to run your your box stupid or your neck rather stupidly hot for this to ever become a practical issue in any kind of longer-term perspective but something would be aware so packet sockets packet sockets with chaired membered ring Eric\u0027s ring as I said are a nice improvement over standard UDP sockets we tried out TX ring which is a memory act transmitted version it\u0027s kind of very very similar to our experiment just the opposite and we couldn\u0027t get really large CPU went out of them it\u0027s not really clear why they should work quite well but a more important point is that they are very very difficult to deploy so packet sockets on the receive side basically only require you to have cap net raw and then you\u0027re you\u0027re good to go you don\u0027t really need a whole lot of you know intelligence and software complexity on the send side you are bypassing an awful lot of awesome cool stuff that the kernel is doing for you and you will hate yourself yeah I mean it\u0027s just shocking like you\u0027re like oh I can do this and you just it will be a bad like here so we spent a lot of time trying to one-by-one fix all these issues before realizing that there\u0027s a foolish idea and that we never should have gone there and we gave up there was a very small CPU benefit as well the difference between like raw sockets and packet sockets but it was not that big and it was not worth the fact that every once in a while users had like terrible "
  },
  {
    "startTime": "01:01:05",
    "text": "terrible things happen to them UDP GSO so you know at least according to Williams recent benchmarks UDP gso achieves performance that\u0027s around the same as TCP for send performance it\u0027s around 3x faster than what UDP is today so the the one quark is it does release all the data grams that innocent call at once and so in order to get the full CPU savings assuming you really wanted a one millisecond pace and granularity you have to have bandwidth the 512 mega bits which is a bit of a bummer so you kind of have to trade off between like how bursty and lumpy am I going to go versus how much CPU savings but it does definitely save a substantial amount of CPU so ideally the segment could be split up in some way to really reduce loss but you know it\u0027s it\u0027s still a great addition and especially on high bandwidth connections either you know in the future maybe client-side uploads that are high bandwidth or in data center of sort of applications it\u0027s pretty easy to get to a point where like this is a big CPU when and you\u0027re getting either all of the majority of the to benefit packet pacing so for those who are at net dev which is not too many people van spent a nice hour talking van Jacobson spent a nice hour talking about released time-based packet pacing and how he was a big fan of this I am also the kind of this it really makes it easy to integrate it with your congestion control it\u0027s easy to reason about when the packet actually it\u0027s the wire admittedly the packet might go out a little bit late but you know you can basically consider Nick queuing delay is just part of the the path RT to you for that purpose it would allow us to you know use our shared socket approach but also control pacing in theory so like if the fqq disc you know I had a release time based pacing module which I I\u0027ve been told it should in the nourish feature then it would actually allow us to share that socket and also pace to the socket at the same time unlike a rate based pacer where if you try to share a single socket and you use a rate based pacer there\u0027s just no correct rate that you can set for multiple flows so disabling pacing can save us up to 30% of our CPU which is a pretty insane number the actual pacing CPU cost in a profile is like 1% you know in terms of timers and all that junk so it\u0027s not the direct cost it\u0027s actually the indirect cost it\u0027s the fact that you\u0027re bringing in a huge amount of connections day you\u0027re sending one packet and then you\u0027re tossing it all out and you just like trashed your entire CPU cache and so you know by sending say like eight packets at once or like some larger number you\u0027re bringing connections in less often you "
  },
  {
    "startTime": "01:04:05",
    "text": "you\u0027re getting better cache locality when they are kind of in cash because you just sent a packet so you\u0027re kind of doing the same thing over and over again it\u0027s fairly likely that if your men copying from data that your men copying from the adjacent chunk of data like a moment later all these things kind of start working for you instead of against you so yeah but it yeah it also does retrain increased retransmit rates about 50% which does cost CPU but not as much as the cache locality stuff so I added some links to some patches there\u0027s reason sport for TX time which is released time-based pacing it was added to a non a different cue desk I can\u0027t remember which one but you take a look and there\u0027s now code in Chrome for pacing off load said the chromium quick implementation will will do pacing off load if we have a release time based pacer available all right so this is kind of my dream of sending and it\u0027s a very approximate dream because there are a lot of details that I\u0027m I\u0027m completely leaving out but it kind of gives you an idea of where I think things are heading in the future so you have quick as an application there\u0027s some set of shared memory pages it\u0027s handing over to the networking stack a symmetric key to potentially do crypto offload it\u0027s sending over a release time so we can you know allow the packet to be released and encrypted later and some efficient data structure says there\u0027s a timing wheel is inside the networking stack and potentially even interacts with you know a hardware sort of like pacer depending on kind of how the the roles are split and then you can offload crypto as well to the rack if if things were working out so the the thing that\u0027s hand-wavy about this is there are an awful lot of details about exactly what these API should look at look like rather and I think that\u0027s what I and a variety of other people are trying to figure out in the next few months did you reduce your sentence than that suppose I know I was just wondering what your thoughts are bound so currently when you do the pacing and chromium you delay the time at which you create the packet as a result of facing versus this would change to a model where you the time you send the packet so for going away those two different models are effacing and which one would you lose some efficiencies as a result in applications where you think things like cancellation are going to be very likely then you you could lose a fair amount or cases where you\u0027re extremely sensitive to recovery time and so the in fact that like no I can\u0027t send out a retransmission until where I\u0027ve like cleared the buffer of the things that I already send out so there are circumstances where it actually could be measurably costly so you have to kind of balance like how far in the future you want to allow yourself to pace for CPU performance with these other application metrics but I think for so far in our initial testing for YouTube we haven\u0027t seen any QE changes when we we\u0027ve done "
  },
  {
    "startTime": "01:07:05",
    "text": "it well as long as we haven\u0027t screwed up paying out of curiosity you see time stamping happening on the left or on the right or it doesn\u0027t matter I see the time coming from quick and then the net stack actually being responsible for releasing it yes okay so the congestion for a while then the time stamps inaccurate and that that doesn\u0027t matter right sorry what if the nest egg sits on it for a while as it releases it and you know paste the timestamp gets stale but presumably yet doesn\u0027t matter these cuz you generate the time time on the left if the wheel if the pacing wheel like sits there it\u0027s the release time so usually it\u0027s in the future right so like it\u0027s intended to be okay it\u0027s into the net stack is intended to sit on it for a little bit so there are quite a few things that still remain to be to be done there\u0027s a possibility of doing some receive side optimizations we\u0027ve gone back and forth on UDP gr o net dev people in that dev had differing opinions as to whether that was a good idea at all there\u0027s the question of what actually is the API for you know crypto offload when we whenever we do that that\u0027s pretty wide open at this point and they\u0027re pretty expensive topic I think KD K TLS hopefully at least gives us an architecture for where to start in that process but I don\u0027t know if we\u0027re gonna actually adopt you know extremely similar API or not and yeah some way to to do multi Datagram UDP sins and actually have them split out this sort of a critical issue at the moment whether it\u0027s euro or some other mechanism I think that\u0027s it oh yeah and thank you I want to thank all the people who have contributed to making the kernel better for for quick and for other UDP applications just one comment on the previous slide I do so for receive pass depending on whether crypto is the is the bottleneck or kernel stack time is the bottleneck with with the right flags in the Nick you\u0027re supposed to be able to get using AF x DP you\u0027re supposed to be able to get zero zero copy receive pass so basically like packet rx ring but packet rx ring is one copy yeah so you\u0027re suppose so it\u0027d be interesting to see like how what benefit like do you get by that kind of a floating versus the benefit of a floating crypt islands a bit curious if you have an idea of the answer yeah I don\u0027t think I have any idea of what the answer is at the moment except I can say that as long as the acceleration on a given platform is working well whether it\u0027s a s and I or otherwise for crypto crypto it\u0027s pretty cheap so it\u0027s it doesn\u0027t seem to be the worst part about cocoa is you have to touch the memory so like you know in all of this like if you can just not touch the memory that you\u0027re trying to move around like that\u0027s that\u0027s golden that\u0027s that\u0027s worth a lot thank you "
  },
  {
    "startTime": "01:10:17",
    "text": "yeah yeah so hi I\u0027m Brian and here comes Ted and we are going to spend the next 15 to 20 minutes talking about a bunch of things that are blindingly obvious so we\u0027ll go fast coming up this is Ted I Ted that\u0027s Ryan I\u0027m Ted and this is the worst episode of Family Guy ever all right hey at least it\u0027s not one of the musical episodes you don\u0027t want to hear us sing alright so everyone here should remember the 1990s this is what transport protocols looked like back then you had transport headers for end operation it turns out that we built a lot of stuff that also used the transport headers like you know in-network inspection forwarding I know that we didn\u0027t like to say the word NAT in the ietf in the 1990s but it happened anyway you know filtering measure oh there\u0027s not there\u0027s a modification you know various sorts of nasty and acceleration things like delaying acts and spoofing acts and doubling packets and doing other sets of medical stuff and then because there was no crypto there was a lot of deep packet inspection and random payload modification and everyone was sad so then we invented security but we put what we called transport layer security around the application layer headers and payloads just to confuse everyone I still have the transport headers for antenna operation you can still do all that wonderful in network inspection modification on the transport headers but now you have to do pre-shared keys or man-in-the-middle or other things that will get you yelled down the tls working group in order to actually do deep packet inspection this is where we are today you know this is what an encrypted transport protocol design looks like so quick is an example of this it\u0027s like the example of this right now but it\u0027s a general pattern you have kind of the function of the transport headers is now split between outer transport headers and inner transport headers right like so in the outer transport headers in quick are as little information as you can possibly expose we\u0027ve spent a lot of time talking about how little that should be and then the inter transport headers are all of you know sort of the acknowledgement so on and so forth and those are all encrypted so you get the end end operation on the inner transport headers you can do in network inspection on the outer transport headers but no modification because the transport layer security can also be used to do Integra in DES and integrity protection of those headers this is the wire image right it\u0027s just this blue box everything inside the blue box is static this is this is all of that stuff that we used to have in the 90s this is the gripping surface what\u0027s there the obvious part is like any information that is carried in the unencrypted bits in the protocol "
  },
  {
    "startTime": "01:13:17",
    "text": "headers there are a few other things here right lakes are the length and the entropy of all of the bits in the packet that essentially provides an upper bound on information content even for the encrypted bits um it does not provide a lower bound right so much of sort of traffic analysis resistance goes into adding length and entropy to all of the bits and all of these packets so that the upper bound is is um not driven by the traffic dynamics it\u0027s driven by the dynamics that thing doing the obfuscation um you can also do timing of packet and observation so transmission arrival and these things that gives you information about what the sender is doing and you can maybe fingerprint the center based on that why am I going through all of this review why does this matter um we are used to how the protocol operates being what you see on the wire and when you have a wire image that you can design explicitly the protocols and end operation is separate from its appearance on the wire and it\u0027s separate from how the intermediate devices interact with it and this is new and this so Ted said this at a plenary at one point and I was like this is new this is different this is novel you\u0027ve got to go in the other direction cuz I want to go back to timing so I will point out that timing a packet observation may not depend solely on the protocol but may depend on something else derived from the sender\u0027s behavior you may have seen that there was a study a while back where somebody was able to demonstrate that they could determine what somebody was looking at from the Netflix of a select number of Netflix series or movies because there was a characteristic packet inter arrival time and packets size so it isn\u0027t just the packet where image from a protocol perspective that has to be examined here but really some of the wire image is derived from the senders behavior or the recipients behavior an acknowledgment that\u0027s still visible on the wire and that turns out to be important because of path signals we talked about this at a ball field shall not be named but in essence when transports used to transmit clear text data on path devices read it and use it to create state manage resources and in for permission that is we had Nats we had firewalls and a whole bunch of there and if the cousins consumed the metadata as if it was intended to a signal to them so what are explicit path signals so when transports use encryption for metadata like packet numbers those inferences fail Nats firewalls and their kins fall back to default parameters or and this is where it gets even worse they start to use the other characteristics of the wire image like packet inter arrival time which may actually be content or sender behavior specific and that has some scary consequences because if I\u0027m "
  },
  {
    "startTime": "01:16:18",
    "text": "willing to give you one set of content fast but because I can tell that you\u0027re looking at a different set of content based on packet inter arrival time I give that to you slow and paste it out differently there are some interesting policy issues around that we don\u0027t want to go to what we\u0027d rather see then people go out and build very expensive inferences boxes that are based on things that we don\u0027t really want them to infer from is that we switch to explicit pass signals where we send data that you intend for the path to consume now where does that signal go in the information to the path well you could use internet layer facilities to send the signals you could send these signals onto each transport or you could do nothing wait given where we are you may know what our intention is hint not internet area facility okay so the example we talked about quick before is the example of an encrypted right image the latency spin bit is a quick example of an experimental piece of explicit signaling the bit is set by the client and echoed by the server the client changes the bit once per a round-trip time and integrity is protected by each side this exposes the round-trip time to on path observers without exposing session state when the client chooses to that\u0027s what makes it explicit it isn\u0027t a default in the protocol that this is always done but the client chooses whether to start sending the spin bit and the server chooses whether or not to echo it back so there is unfortunately not linked to here but some work that was recently shared over in the quick working group on some tests would show that this is relatively effective except in the case of pretty extreme reordering and even in the case of pretty extreme reordering there are some heuristics that you can use to to determine that the the cause of the the failure is reordering rather than a failure of the spin bit itself I direct you over to the quick working group I think Oh Marcus oh sorry I didn\u0027t see you Marcus guest-starring on today\u0027s image with the family guy so this exposes the round-trip time to own path observers without exposing session state note every single bit has to be designed and considered this is explicitly added to the protocol by the designers of the protocol and the use of it has to be explicitly considered when you\u0027re using the protocol there\u0027s no default for what signals to send it needs to be determined per transport and so the use cases of specific transports may never use this if you\u0027re inside a data center and you\u0027re using quick and you turn on the latency spin bit everything is going to be so quick it\u0027s not going to change your behavior note the extra K on the last quick and it needs to be optional if a client or server don\u0027t want to send that signal it can\u0027t be needed for session state remember the whole point of this is this is going in the outer part that that "
  },
  {
    "startTime": "01:19:19",
    "text": "blue bit here so that it can\u0027t affect the actual functioning of the transport headers as seen from the internal it has to be purely optional for this to work so there are two IAB drafts on this topic draft IV pass signals and they soon to be renamed draft IAB Tremmel and slash wire image currently tremolo soon to be draft by EB we were looking for comments on both of those drafts or at the mic line today do we have a specific meeting do you want to send these comments to architecture discuss or stack EVO at IAB org or sorry-sorry stack evo discuss that IB dot org would also be fun yes they have already put the cookies out Ron even just to note that one of the issue about doing this explicit is to verify that there are no security or privacy issue related to that and that\u0027s important part of this explicit signal yeah that actually should be a third point on the future version of this slide is you actually have to think not only about how the signal is to be used but how it could be misused right because you\u0027re basically you\u0027re putting you\u0027re putting information out there the nice thing is because of this separation here you\u0027re not forced to leak inner information out to the path however if you\u0027re not careful about what you put out here it might be just as bad right so in the case of the round trip time discussion for example we actually had a design team and quick that went off and looked at what what could possibly be leaked by a spin bit and spent a good bit of time most of it Brian\u0027s trying to work out whether it could leak geolocation information and determined that the the fidelity of the geolocation that it could potentially leak was so poor compared to geolocation information you could get in many other ways that it was not an effective leak you know one of the difficult things about any leakage is there\u0027s always the possibility somebody will be able to put something together with other data and make make no inferences that you weren\u0027t expecting if they\u0027re still in the mode of making inferences which is one of the reasons the use of it has to be optional if in the future somebody discovered some fingerprinting aspect of it that we were not aware of when we ran the design team devices could not send could decide not to set it and the result would still be correct protocol in operation from the point of view of the endpoints then I really like the fact that we\u0027re taking this pin bit as an example for the explicit signaling and and and I really "
  },
  {
    "startTime": "01:22:23",
    "text": "hope that we can accelerate for other bits in the future my only concern is that if we start doing one bit per use case we can end up with pretty fragmented frameworks I don\u0027t want to rush the design to something that potentially we can we can think of more extensible framework that doesn\u0027t need to be you know one use case per bird so it turns out that one bit per use case is actually difficult because when you\u0027re talking about doing the analysis to figure out what yeah so basically when you\u0027re when you\u0027re doing this you\u0027re explicitly taking stuff that could be down here and you\u0027re moving it up here into the obvious part right which means the information is more accessible and when you\u0027re when you\u0027re looking at how that information is exposed in the wire image explicitly you also have to consider how it interacts with other information that is exposed explicitly in the wire image right so so this spin bit like the amount of effort that we went to on the spin bit that\u0027s been bit looking at that was easy because there were no other explicit bits that we had to consider interactions with right so when you throw the larger the surface that you\u0027re adding the more sort of degrees of free I just feel like we can do the analysis on a1 one by one the security and the privacy analysis but at some point we can we can figure out a better way to to put it into a common framework that\u0027s that\u0027s all my name is Tim Sheppard and I listen to some of the discussion about the spin bed at the London IETF both in the quick working group and in the hallway afterwards and I what I hadn\u0027t thought of had learned about there was that people had privacy concerns about the existence of this been bit because it reveals the round trip time to be passive observers of the traffic on the net can the can discover the round trip time of a user more easily I guess they could probably do it anyway but what I just realized sitting here and you must have thought of this already is that this design leaves open the possibility for a client to be deceptive in its setting of the spend vet to make it appear that you have a longer or shorter round-trip time you could set it every half round-trip timer a third round trip time for every two round-trip times if for some reason you thought that was useful I don\u0027t know if we should be concerned or happy so there\u0027s so like in you know and we did that work and there\u0027s a bunch of there\u0027s some text in the drafts about okay about sort of the model and I won\u0027t really go into it here because we don\u0027t really want to talk about the spin bit we do want to talk about is sort of the general principle there is that yes so when you when you separate the information here from the operation here you now have like you don\u0027t even have to have an encrypted side channel you have an encrypted channel that everyone can see and you can coordinate here to lie "
  },
  {
    "startTime": "01:25:23",
    "text": "about out there yeah that\u0027s a general Ben Schwartz so I want to ask about how you generally think clients will go about making the decision to use optional explicit wire image features those sort of reasons they might choose to enable those features and and any result in pressures or ecosystem interactions so I think the decision by the client will depend on what the bid is doing right and so one of the things I think you have to consider is whether a particular bit that you\u0027re proposing to send is explicit signal to the path is valuable for the client to expose or not and so one of the questions you might ask yourself for example and setting a bit about round trip time is whether the the client cares that the network on path observers know the round-trip time and in this particular case one of the reasons a client might care is it turns out that that that is being used in state maintenance by devices on path which are doing net for example so if you\u0027re running ipv4 you may want to expose round-trip time in order that the state in that\u0027s not aged rapidly without you having to send heartbeat packets to keep the stat fresh so um so there there\u0027s a variety of reasons you might actually design a client to enable somebody in the network to have access to the state but it\u0027s going to be different depending on what it is you\u0027re exposing and you may find that there are cases where you\u0027re like hey I\u0027m running ipv6 there should be no net in the state between me and and the rest of the world so that reason doesn\u0027t exist so you might set it and won\u0027t address family and not in another again the whole design here is that with any of these explicit past signals the client has to make the decision to set it and it can\u0027t affect the the correct operation of the protocol from the in systems perspective if it is not set so let me add to that I think you know so the work that I do in this is explicitly focused on measurability and a lot of what\u0027s interesting about measurability is the ability to do diagnosis Freight so I would see in a lot of cases there\u0027s a checkbox in the browser that says hey I\u0027m having trouble you know you go you call the whoever\u0027s looking at your path and they say click this box on for your right now or open a window that\u0027s in in debug mode and then try to write or run it and then the device is along the path can look at the information that deep right like so for things that are explicitly looking at Diagnostics you can essentially have sort of like in been Diagnostics targeted and then in that case you you you would expect in that case you\u0027re adding a little bit more information than just one bit and then you would usually leave it all right so they\u0027re different different bits different complexes of bits different use cases you know the he "
  },
  {
    "startTime": "01:28:25",
    "text": "sometimes the decision goes up to UI some sentence decision is a system model decision sometimes decision is an implementation decision that you just after case by case thank you both I want to correct my question slightly I should have said end point probably yeah I know you said you don\u0027t want to talk about the spin bit I\u0027m sure there are long queues at mics and lots of text but the thing that\u0027s from the benefit of somebody who knows nothing about this have having that perspective seems to me the the spin that\u0027s actually very opaque and hard to understand and so because like if nobody if you don\u0027t look into like the exact details of why that\u0027s been that gives you these properties it\u0027s kind of hard to understand why and I think so I guess the question is if you wanted to expose the rtt presumably there was a reason why you could have said okay the spin bits one bit if I want to if I want to express the RTT as whatever twelve bits I can\u0027t do it once every 12 packets as a 12 bit number and I guess that kind of goes I\u0027m not interested in that particular engineering trade-off but what it means is like how clear does that blue have to be right in the outer layer right and if it was really more explicit like this is the RTT would people making for example policy decisions around that like do I expose this or not would they understand better and what that better map to the policy outcomes that you want to achieve it it could indeed um but we have so far not been successful in coming up with a framework that would allow us to express that in a way that you could actually have a vocabulary of those things work continues the other thing to recognize is if if you wanted to send an explicit RTT as seen by the by the endpoint to the path and simply Express at every end packets you have different engineering trade-offs to make there and some of those engineering trade-offs is now the packet format has to deal with that amount of variability which is something that may not be easy for certain packets and formats to deal with and you have to deal with loss if that particular packet gets lost or suppressed and so there\u0027s a a set of issues around this where when you\u0027re doing the design that the protocol you have to think about what the again what the the the client is going to get from this knowledge being available in the in the path and from the paths devices perspective how easy it\u0027s going to be to read and from both sides perspective what the parsing characteristics are so there\u0027s a good bit of work to go into to the he about the design and you\u0027re probably right there\u0027s also probably some marketing and we\u0027re probably not great for having chosen to market it as spin bit instead of RTT bit where the spinning characteristic is how you derive the RTT but don\u0027t consider that probably derive something else from it "
  },
  {
    "startTime": "01:31:25",
    "text": "as well and if it was just the RTT maybe you couldn\u0027t I don\u0027t know also it seems to me the client gets actually nothing out of this only the network does but yeah well so the client does get something out of this in the case where the network is maintaining state because if the network stops failing to maintain state the clients experience is better or if the client no longer has to send packets to maintain network state the clients experience gets better Network state doesn\u0027t doesn\u0027t get cleared on the order of milliseconds it gets it gets cleared on the order of tens of seconds you you said that this is what you said about the net time out earlier well so it depends on what and actually there\u0027s a good bit of stuff in in in the analysis we did for the spin bed on what the particular application characteristics are and if you\u0027d like to talk about that offline I\u0027m happy to do that but one of the things is is there are application characteristics where you don\u0027t expect the packet trained to be milliseconds but maybe many seconds before one particular side sends and that\u0027s actually something you have to consider in in this is are there application forms what you would get out of what you are analyzing is hiding something about the network in behavior that\u0027s actually coming from an application what - two minutes you get to be very negatively quick it\u0027s Christine Hutchison just say I\u0027m supportive of this I where an operator I\u0027d rather have a few bits of information explicitly and optionally stated but that I have confidence in rather than doing something relatively expensive and computationally heuristic ly to infer things which you\u0027re never quite sure about so it\u0027s better to I mean it actually going back to Lorenzo\u0027s point what\u0027s in it for the end clients is is a better managed cheaper network thank you very much everybody for coming I believe there are snacks and drinks outside so thank you for my site we don\u0027t have an open mic anymore I mean if you want to ask me questions you can still come in are some questions but like other than that enjoy your cookies thank you everybody "
  }
]