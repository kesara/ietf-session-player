[
  {
    "startTime": "00:00:19",
    "text": "That's right So it is the box for us to get started in the interest of making the best use of everyone's time who's here, let's do it so welcome to the seabor working group that I IETF 120. It's correct on the screen. If you had meant to be somewhere else, go somewhere else. If you want to be here, welcome So, Chris hopps controlling the slides? I guess you are So next slide, please I'm, ah, okay, my screen is not show the new slide so note the note well well that's all i'm going to say about it you see seen this many times keep looking at it Have some time to read it. It's important Talks about your responsibilities and participants And next slide, yeah So this is basically say, treat each other well Don't be nasty, don't harass people If you feel that you've been missing in a way that suits the ombuds team, go talk to the ombuds team. And in fact, he's in the back of the room right now. So you can have a very short trip to talk to him Next slide As we've been for many meetings now,"
  },
  {
    "startTime": "00:02:01",
    "text": "please use Meetecho, even if you're in the room. There's an on-site tool that you can use if you use the full client tool, please make sure that your sound is off and you don't unmute And please get in the queue on Meetecho if you want to talk so that you are intermixed proper with Carston and Christian and all the other people so here's our agenda we're going to talk very quickly about a whole bunch of dollars and where things are. And Carson will, I guess, go through all of those. As we did on the last interim call. And then next slide And then we got we're going to have a little bit along longer discussion on a few other documents they're probably not going to be in this order they'll be in whatever order they are in Karsten slides which I reviewed the afternoon, but don't remember And then we'll just verify the interim meeting schedule between now and Dublin Okay, so Carston you're on So I just made slides for number three on your list. If you want to go through the maximum status, you may should stay on the first. Oh, okay So, yeah, maybe we should go back to the agenda slide so we can do that Okay, so, Karsen, you want to just run through as you did on the call? Sure So I'm hearing some echo interestingly that seems to come out of the room Yeah. The microphones in the room are very hot. So let me see if turning off my mic helps Yeah, it's always a bit"
  },
  {
    "startTime": "00:04:01",
    "text": "confusing to hear echo. Oh, it's gone So that made a difference? Yeah he wants me to turn down the game, which I can do Okay, I'll just use the floor mic up here for when I need to talk But then you are a very dark figure because the lighting in this meeting is pretty bad Okay, so we have two documents with the RFC editor the time tag document, which we started in 2007 if you don't remember That actually has made it to OTH48 time tag document, which we started in 2017, if you don't remember. That actually has made it to author 48. All but one author has approved and we are waiting for a son of life from the third author. So maybe we need to do some emergency shuffling here But this should be done within a couple of weeks. Update, Gwema is in the edit state. So when this is done, we will have a consistent CDG specification again, because that addresses the ERAP errata and a few more details EDN literal has left the working group and is currently under 8 the errata and a few more details. EDN literal has left the working group and is currently under AD review and we will have discussion for that More control is has completed working plus call has a happen report so we could send it on we can discuss this later Cibor Pact I must admit I didn't prepare anything because I had enough drafts to look at So if anybody wants to bring up something that would be great, I have a few slides to join your mind, but I haven't prepared any questions So the second item where we probably want to spend some time is"
  },
  {
    "startTime": "00:06:01",
    "text": "CDE. Common deterministic encoding, and there are two drafts that are in individual drafts, G and H we could discuss as well, but I mean, there is no rush in deciding we should really for on getting CDE out of the door and then we can look at the other documents Well, DC ball is not on the working group plate we have decided And I think indeed there's seems to be a pretty good consensus that it's also not an end run around the working groups so it should be able to go to the independent stream stream modules has one item that we still have to solve, which I probably can briefly report about It's not yet done, so it's not yet in the implementation and in the text, but maybe we can discuss this briefly and then ed and eREF i'm quickly going to touch on ED&EF when we discuss EDDN literal and yeah, there we need to decide whether we can go ahead with that, but that's meaningless if we can get progress on EDM literal And then there is the draft numbers document, which we probably don't have time and this is this is interesting because it's something that is in a lot of documents, but it's so supposed to be removed before the document is operated on by the RFC editor So it's a normative reference that vanishes"
  },
  {
    "startTime": "00:08:01",
    "text": "while the document is being edited Okay, I think that is that part and then I will just switch to the other You already have slide control. I'd like to jump in briefly here because there was to the other. You already have slide control. I'd like to jump in briefly here because there wasn't, because we still had the agenda slide on um you sent me a link recently on um you might want to have discussed here on ab and f tooling I think I'll just move that down to the aob and we'll address that then, or do you want to talk about that in some earlier time? Let's do this under AOB. It's really interesting and it was on the working group bailing list as well So most people should have seen it And maybe we can talk about in the context of modules as well because it's highly related to modules Okay, so I hope this time it's not going to switch to the slides I need to waste one minute here So we had this reds meeting on Tuesday, and thomas fossati was commenting in the chat about the escaping health you get into when you nest Jason into Jason and so on. I occasionally read in zibo there is no escaping And AJ j said oh that that's a good t-shirt so maybe we will use this little combination, yeah, a little bit more in the future. Anyway so just quickly to give some order to this huge number of documents, we have three formats we are working on, one interchange format"
  },
  {
    "startTime": "00:10:01",
    "text": "Cibor itself, which we sometimes visualize in hex, but it's a binary format and two languages that we use to talk about CBO, one is for CBO instances, diagnostic notation, or EDA where you can describe a single CBO talk about. Cibor, one is for Cibor instances, diagnostic notation, or EDN, where you can describe a single Cibor document in text form on a whiteboard in a configuration file, wherever you need text-based representation And then we have CDDL, which is something that some people would call a schema, which I would probably call a data definition. So it's a data model language and grammar And that is very close related to A, B, and N that's why the ABNF issue there came in today, that Kirstian just a little to is so interesting Okay, and so we will go through this is just showing zero pretty we will go through the agenda in the sequence that was on the agenda, so we will do the two EDN related items first. Then we go to CDL more control maybe because that's essentially decision to ship that that should be quick then we can talk about CBOPact, but again, as I said, I haven't really prepared much there then we go over to Seabor and look at the determination encoding and two related explainer documents and find deterministic encoding and two related explainer documents. And finally we go back to CDL with a CDL modules item so I just so go back to CDL with the CDL modules item. So I just used the sequence here that was on the agenda. Okay, so that was the overview. Now, diagnostic notation notation Diagnostic notation is meant to be used in tools"
  },
  {
    "startTime": "00:12:01",
    "text": "on whiteboards and specifications for diagnostic output It's not meant to be an interchange form So we were very, very adamant to make this clear in the original C-WROW C-WRC-7049 specification, RFC-749. And we were adamant enough that we decided we are not even going to put in ABNF for that So people really notice that is not meant to be for interchange So that has that magic has worked for some 11 years and maybe it now time to do the abien thing EDN is our diagnosis mutation is just plus some more functional So every JSON text, actually every interoperable JSON text is an EDN text So you can just push in your JSON into CIVA tools There are the three main things we need to add is binary binary strings. We not only have text strings, like we also have byte strings. The concept of tags, the extension point of Sibor and we need to generalize map keys. In Jason, they are restricted to text strings and Cibor, they can be anything So the small principle you see on the bottom right corner of this slide actually is an example for using diagnostic notation in an RFC One. Yeah. Just a quick clarifying question regarding map keys So it doesn't appear that there was any documentation prior about what was allowed, but that integer, that positive is"
  },
  {
    "startTime": "00:14:01",
    "text": "integers and negative integers and text strings were allowed, and that's what's allowed in in various seabor-related registries like the seabor web tag Do we have in the watch? anyone using map keys that are not integers or strings? Suddenly, the suit manifest specification, for instance uses arrays as map keys Thank you It's a bit weird to use a map as a map key because maps have this not so great equivalence relationship. And since we want to avoid duplicate entries in maps, it's probably not a good idea to use a map as a map. But anything else can go there Yeah, Chris just notes on the chat that he's using arrays as map keys. So I think that's that's pretty normal Okay so that's the background. That's where we are coming from and in the 11 years since we have added a few things, so the update with 8610 actually gave us embedded Cibor a notation to put a byte string of encoded Cibor into an EDN notation and instead of having to use the hex a1 A10101, which of course every CBO user can read as a map with a key of one and a value of one, we can actually write that down in the annotation again So that's what the double angle brackets do here in the example. We have worked on readability, so we have added comments, which is"
  },
  {
    "startTime": "00:16:01",
    "text": "probably the most requested feature that JSON doesn't have We have added a new common syntax as well end-of-line comments So when this says news, we are talking about the new EDN literal document. We now allow a final comma in a map or array The commas are actually completely option everywhere. So if you just want to do a vertical listing of things, you can do that no commas needed. And the big new thing that comes in this document are application-oriented literates and let me talk a little bit about what these are We have one extension point in Seabor that not many representation formats have That's called tags And these are essentially plug-ins you can put into C-BOR to express some semantics using a structure that you can write down in C-BOR, but you design additional semantics to that structure using the tag So for instance, if you have a byte string, that might be actually an arbitrary position number, but you don't know, but if you put tag two on top of that, then you know, oh, yeah, this is an arbitrary position integer that uses bytes in a specific way. So that's that was pretty much an innovation we put into seabor and that seems to have worked pretty well The problem here really is that when you write EDN, you see that seems to have worked pretty well. The problem here really is that when you write EDIN, you still have to write the repress pretty well. The problem here really is that when you write EDN, you still have to write the representation that the tag uses in its tag"
  },
  {
    "startTime": "00:18:01",
    "text": "content instead of being able to write something semantic. And on the previous slide, we had an example. So date time DT, 2024 or 32 and so on that becomes a number, in this case an integer number And so far, EDN, you only could write down the one set case an integer number. And so far, in EDN, you only could write down the 1711 and so on and had to put the actual semantics into a comet. But that, of course, is error problem so we were looking at a way to actually allow the syntax that is on the left hand side of these arrows. So daytime and IP addresses are two obvious things, but we have quite a few more application-oriented tutorials that are lined up just not in this document So we have one piece of syntax that is almost extensible already, which is the by- string syntax, so we decided to simply use the byte string syntax. If you look at the example here, that's okay I mean, that's maybe not the most beautiful syntax record have come up with, but I think one can get used to this very quickly and it works. So we are using the byte string syntax and each time a new kind of application or literal is being defined we register a prefix with a specification that says how you turn the text in the single quotes in the information you want to have And by the way, these not only generate tags but they can also generate the unwrap"
  },
  {
    "startTime": "00:20:01",
    "text": "tags or the naked, untag information that is within these tags So that is really the innovation That is how EDN literals started and we just thought, okay, if you are touching EDN, we might as well do some general clean-up, do the usability enhancements I've talked about. And finally answer the frequent request for an ABNF of the language So that's essentially where we came from And examples for further plug-ins are, for instance, E prefix is used to introduce some concepts into EDN EDN is not a programming language, it's just a data format So if you have something that needs to be 17, but don't really want to put 17 in your example in the spec, you can use the E mechanism to read the constant out of a CDL module that you use as input to this So this is an extremely simple mechanism, but it's actually quite useful and it has already been picked up in a few specifications One of the benefits of using this is that when you are using code points that haven't been assigned yet you can throw in some private use code points or some random squatting code points, which I don't recommend and get your examples actually machine processable instead of what we do today"
  },
  {
    "startTime": "00:22:01",
    "text": "is we write the code points into the EDN and then we really have to be very careful when the RFC edit comes and tries to understand what we have been doing Okay, somebody should look at the chat I cannot both talk and answer the question Go ahead I think it's sufficient to keep that in the chat Part of it is in the minutes. You can pick it from there later Yeah, so if somebody Naming those things. Reads the questions in the chat and answers in the chat, then I'm fine. And I can also send corrections the next time So another example that we had is the concise resource identifiers that we are defining in the core working group group There we have a format which you can see after the arrow on this slide, that essentially contains a parsed to which you can see after the arrow on this slide, that essentially contains a parsed UI. But of course people are not used to writing parsed UIs they want to use the UIs they write into their browsers. So having an application oriented literal here, which is called CI, allows us to notate a URI or actually IRI, and have the tool generate the CIS CI representation of this UI OK, so these are just some examples, a few more are coming up. And I think we, will use this a lot Okay. So, um"
  },
  {
    "startTime": "00:24:01",
    "text": "want to have these application extension in ABNF, but by definition, because this is an extension point, the ABNF, cannot describe yet what is not known. So what the ABNF in EDM literals does, is define the overalls syntax. So how does an abstract? actually look like? It's an app prefix followed by a single quoted string and the app prefix has a certain syntax and a single quoted string is a single quote and a sequence of zero more characters that satisfy the rule single quoted and then another single quote so the is generic. It definitely will cover all the application already literals we might be defining in the future but it doesn't actually define them So the question is, how do we do that? And the idea was to make the application extensions plug-able So the overall syntax stays the same for all application-oriented literals And we describe each application-oriented literal with an additional piece of ABNF that is unrelated, not connected to the ABNF that is used as the base center but describes what the text in the single quotes after the escaping and what you do in such a text, literal in a representation format what the text actually has the syntax of the text is And one reason why this makes a lot of sense is that we have"
  },
  {
    "startTime": "00:26:01",
    "text": "foreseen the situation that an idiot and literature parser may not an EAN parser, excuse me, may not have been upgraded with the most recent prefixes So you may want to use an existing parser that simply doesn't know what say, CRI is. And in that case, what you actually get is a tag 999 with the word CIA as the first text string and the DSC descape decoded content of the text in the single quotes as the second element So you can have something later in the processing that actually implements the part that the parser, the EDIN parser you have been using for some reason, didn't provide yet Okay, so in this case, for CIA, the actually different is extremely simple because we just have a single line ABNF here Absentee CRI is IRI reference and what IIA reference is we can copy paste out of 3986 37. So this is really useful if you have existing ABNF for the format that you want to use. Otherwise, yeah, you of course have to define your own ABNF as we always like to do. So this is the idea And this is, of course, means we have multiple grammars in the document. We have the grammar for the base syntax and we have per prefix another grammar"
  },
  {
    "startTime": "00:28:01",
    "text": "that tells us what the content of the single quoted string actually is supposed to be per application extension prefix So an alternative idea, of course, would be to define a way to add the application extension syntax to the base ABNF in the process where we define new application extension prefixes so that means that the the syntax that goes into the base ABNF that is added to the base ABNF you're probably using the equal slash syntax of ABNF that needs to define its own string parsing and escaping. And it also means that from an implementation, point of view, the base ABNF changes each time you adopt a new application extension So they're actually much less pluggable And also, it's really hard to do the tag 99 stuff without kind of falling back to the other way of doing things, which would probably motivate doing it that way in every case So that's really the discussion the specification that we came up in the world group uses the the pluggable architecture as I described I have described that as a two-layer approach because that's exactly what it is Many people who have been working with ABNF for a while know that this has been used in other places as well, for instance, in the web linking"
  },
  {
    "startTime": "00:30:01",
    "text": "specification because there are simply a problems with trying to put all definitions that describe both the way something is encapsulated embedded into a grammar and the grammar of the thing itself on the same ABNF layer. So that's something that the web link people learned the hard way, and in the code co-working group, we unfortunately have used the old version of that specification so we have exactly the same problems and special case handling because of course the mapping was not perfect and so on. So PR 49 is a proposal to replace this with a single layer approach, at least for the four previous that we have defined in the ideal digital document so each of these prefixes defines its own single-coded string syntax, and of course when we get a new one, we will have to do this in a compatible way Yeah, there has been a lot of discussion in the mailing list and a lot of arguments I didn't really feel like I should put all of them on a slide because we will hear them I think. So let me just summarize the pros and cons As I see them, of course so the pros of the one one-layer approach is that this is, of course, much more familiar to people who are used to monolithic ABNF spec So when ABNF was designed in 1977, they're probably wasn't that much of an application"
  },
  {
    "startTime": "00:32:01",
    "text": "that would have benefited from using ABNF to describe something that comes out of a parsing process that is described by ABNF And also people who have implemented JSON or something like that also will have been able to do this with us ABNF. And also people who have implemented JSON or something like that also will have been able to do this with this one layer syntax. So they also will be more familiar with a one layer representation and one other thing is that it's simply easier to share ABNF rules between the base syntax and the syntax for specific application extensions. So if you want to define what a comment is, you might be able to use the almost mostly the same LBNFWRT So on the two-layer side, I explained Yes, on the two-layer slide, side, I explained why it's actually useful to have true plugables, so you don't have to modify ABNF in flight while using a tool But I think more important actually is the ISO isolation between the plugables and between the plugable and the base. And that means that we, for instance, can simply use existing ABNF directly so we don't have to do a new UI syntax that is robust enough that it can be used in a scene single-quoted string. We can just use the existing syntax for existing grammars I should say um and we we don't have to create a telephone process Yeah and we all should be used to using layer"
  },
  {
    "startTime": "00:34:01",
    "text": "as separation of concern, so it really took me by surprise that we were having the discussion But Horan has had a comment on slide 13, so I'm going back to that Hi Oh, I think, no, sorry, it was actually slide 12 It was this, the tag 9999 So so it seems that if somebody has support for a new tag, sorry, for a new app prefect, app string prefix Another implementation does not, then they're going to, they're going to encode to different seabor and they're not going to be compatible so I didn't understand what the advantage would be of adding this tag over giving a hard error during the encoding if a particular app string prefix is not known The advantage comes in when you have software that actually evolves So if you do a static photo of your system at any one point in time, there is no advantage. But if you are right some code that is using a diagnostic puzzle, library and it turns out that library has not yet been updated with a new efficacy, literal, then with and it turns out that library has not yet been updated with a new application literal. Then, well, you can fix the library, maybe if it's open source or wait for that to be fixed I mean, this is a real deployment hindrance that is coming up here So, like, the example that I'm, that I'm, envisioning here is somebody has, you know, a test rig and they have some EDN and they use the EDN to generate some seabor documents"
  },
  {
    "startTime": "00:36:01",
    "text": "If the encoder, if the conveyor to seabor doesn't fail, then they won't notice this and they'll just be generating CBO seabor that is not the intended encoding Right. So the base control they won't notice this and they'll just be generating Seabor that is not the intended encoding. Right. So the base configuration of any such pausing library should be not to do that but there also should be a configuration where an application can say yes, I know what Teg99 is and when you, the parsing library, hand me up some parsed ed EDN, I can handle the prefixes that you didn't know about So this would definitely be a flag that needs to be said It would not be the base configuration of the puzzle to the speech of things. First, I have no dog in this fight as far as the seabor stuff goes EDNs are just fine. Nothing couple of things. First, I have no dog in this fight as far as the seabor stuff goes. EDNs are just fine, not the EDN literals just fine, not my problem Second of all, the only reason I came into this to discussion was because Rowan asked me to take a look at the ABNF that he proposed, and I actually proposed something different So if you'd go back to slide 14, the pros and cons, the basic problem I've got, well, let me start with all of this confuses implementation with documentation. There is absolutely nothing about the ABNF that I proposed that cannot be implemented two-layer. It can be implemented exactly the same as what Carson is describing And the documentation will have subsection"
  },
  {
    "startTime": "00:38:01",
    "text": "which has the ABNF for the specific subtypes of EDN literals. And when you introduce a new one, you introduce existing what the documentation currently looks like in the draft with a new literal The only thing you add is that app string or equals the new thing, which allows you to update the base should you want to, but you can completely imagine the flat AB and F with the same plugable format that is not an advantage to changing the way ABNF work So I think we have to get rid of that. The disadvantage, the con on the doing the tool layer is that things that expect AB and F to be a single, complete description of the grammar will fail What is currently in the draft is not valid ABNF because those EDN literal sub subtypes don't attach to anything in the overall syntax. They're just free floating Dangling productions. They're dangling productions And a again, what we are doing, is coming up with a way to go from a B and F to an implementation, that is not impacted by the way the ABNF is written. More importantly, Karsten pointed out back in 19 an implementation, that is not impacted by the way the AB&F is written. More importantly, Carston pointed out back in 1977, but this is not the 1977 ABNF This is the newer one that came out when we did RFC 2234. The old ABNF was a real parser description and it had multiple layers of token White Space was dealt with outside of the whole thing"
  },
  {
    "startTime": "00:40:01",
    "text": "You had to tokenize and then you had to interpret after the tokenization. For better for worse, I would argue for worse and I was one of the people who pushed this back when we did 2234 we've got a grammar, this new ABNF grammar is just lousy for really going from instantly grammar to implementation It has limitations. It has those limitations because we wanted simplification in the grammar itself. That's a bummer. We wanted more specificity but that's what we end up with. So what I'm object to here is don't start playing games with A, B, and F to create something that looks like your particular implementation desires when the implementation is not a affected by what you're doing with the ABNF Use the AB&F the way it's designed and you'll get the same thing. As far as the other stuff that has done this, Kirsten pointed me to the RFC I'd not actually looked at the link state stuff before It's got unparsable A, B, and F in there, completely unparsable. It doesn't even have left-hand sides in some of the production It's really weird-looking And it was written down for convenience because it was a documentation mode, again, not an implementation description. So overall, what I'm telling you is if you use the AB and F that I showed you, you can do exactly what you want to do but you'll have a complete ABNF What's currently in the document is not a complete ABNF description of the grammar"
  },
  {
    "startTime": "00:42:01",
    "text": "Yeah so this is certainly not interesting philosophical point of view. The reality is that the tools that we're using today don't really work very well when you are modifying your ABNF in flight So you expect these tools expect to get the complete ABNF of what they are supposed to parse and then that gets converted into a program that actually uses various mechanisms these there's usually peg parsing to actually do the parsing Kasten, can you clarify something for me? Yes, sir When you say it, modify it in flight, are you saying that it these don't support things like or equals very well? Is that what you mean or do you mean something? else? I mean that you have a tool that uses a compiled piece of a BN and then you add a plug-in that provides a specific application-oriented literal You cannot do this in AB&F implementations I'm familiar with You essentially have to recompile the whole thing, make sure that that attributes and annotate and so on still fit in the right way and so on and so on so that's one reality that makes me a little bit averse against this philosophical model The other thing is that the missing link that you have been pointing out actually is a pretty simple relationship so the the thing is that the missing link that you have been pointing out actually is a pretty simple relationship. So the name, the prefix"
  },
  {
    "startTime": "00:44:01",
    "text": "actually goes into the name of the top production of the application oriented grammar. So that's where the link actually is done. And you already have to do this with any AB&F So whenever you introduce an ABNF in your system, you have to say where you want to start so this kind of missing link is completely normal when you are using ABNF And your criticism of 82, 88, yeah, that's maybe a little bit too, none non-traditional for me, but the important part here is that they found out that a one-layer approach really really, really doesn't work for them And so they came up with the crisis that they're using to do a two layer approach. It would be nice if AB and S would at some point support this with syntax. Right now we're supporting it with English I guess my reaction to what you just said is why don't we just fix the tool which is perfectly plausible instead of fixing the A, B, and F in something that it's not designed? for. It's just as easy to tell the tool, use the or equals the same way you would just add a plugable piece It makes sense to me. Doesn't say like hard code to write You still have to recompire the whole thing So that's a bit The practicalities don't work so what you describe as a really complicated thing is absolutely trivial in a"
  },
  {
    "startTime": "00:46:01",
    "text": "real-world ABNF implementation just extracting the prefix out of the base path result and finding the right stuff production to actually look at the content of the text That's absolutely trivial Recompiling the ABNF each time new application already literal is added, that's way more complex And that's not one something I want to support. So the, I have implemented the whole thing based on existing ABNF parser generator So it's not like our tools will break when doing that. But I'm not going to write a new parser generator just because I want to make a particular philosophical model of how ABNF should be used to work. So let's go through the queue because we've got now Rowan and then Christian Christian Okay, so three quick points here So I'm surprised when you talk about recompiling versus not recompiling because I expect that you have to compile whatever it is that processes the second layer ABNF So if you're compiling that, I don't see why it's a problem to compile, you know, you have to compile one way or the other, it appears The second thing is that for me, like, how I stumbled upon this was I read the AB and F and I was like wait a minute, this doesn't make sense. There are characters in here I mean, I sent a long email to Carson I'm sure he remembers the comment about, you could win the obfuscated the obfuscated seabor contest or something. I don't remember something"
  },
  {
    "startTime": "00:48:01",
    "text": "like that. But the reason that I came up with those examples, was because I assumed that this was a single layer, AB&F because that's how other IETF specifications work I don't think that I would be the last person to make that mistake even if there was some text note in the in the document Finally, the third thing I will would be the last person to make that mistake, even if there was some text note in the document. Finally, the third thing I wanted to say was about the so the PR gives the PR number four it gives a BNF that has all of this combined, but in a between the comments on PR 49 and the mailing lists, I gave BNF that is compatible with that single pass model for all of the other app string prefixes that I'm aware of other than CRI And if the working group said, okay, like, yeah, we can go do this this way, this PR is accepted, I can, I, you know, it wouldn't take me very long, probably 30 minutes to go right up not even, to go right up this for CRA Yeah, so essentially you describe an editorial problem. We apparently haven't described this well enough, maybe because it's so never for some people, which I think we can address by putting in more text Yes, if you write a plug-in, you have to compile the plug-in, but you don't have to compile your base tool. So that that is a very, very big operational difference because those plug-ins are like five lines of code in many cases So it's really trivial to make them available as individual modules"
  },
  {
    "startTime": "00:50:01",
    "text": "But if they need to integrate into the base AB, then essentially you have to make the whole tooling that is attached to the base ABNF accepting of this new stuff The other observation it's interesting that you didn't do the CRI because that is actually interesting Let me try to find this slide I was unaware of it at the time Yeah, I mean, that's the point The point about having an extension point is that people can use it So we will always have unknown users of our extension points. And we have to build systems in such a way that the unknown users work. But this is really interesting because the abstract CI can use the single production AI reference from RFC 3987 But yes what the comment says copy and fix more stuff from RFC 3986 Fix, right? so there are things that are broken in many of these old in this old ab and s that we reference So you really do, you can't just go and blindly paste something from a document that is, you know, sub sub five, you know, RFC 4,500 or 5,000, right? You really do have to, like, pay close attention to it you have to understand what it and you have to you may have to rewrite it Yeah, so in this case, we had to make sure that the, missing productions that were presumed to be imported from RFC-2-3-4 are there But, I mean, that's something that everybody who does anything with AB and F does all all day So it's not a risky thing. So I'm glad you said that, something anybody that does ABNF has to do"
  },
  {
    "startTime": "00:52:01",
    "text": "So that's that like what the IETF has done IETF has done in the past with a bnf is it uses it the way frankly the way Pete described which is, it's a, you know, it's a single layer thing And so if we're going to go off, off P piece here, I, you know, we need to have, like, you can't do it kind of an answer, I feel, instead of just it was you know, we need to have like, you can't do it kind of an answer, I feel, instead of just it would be more convenient to us if, because we're doing this for the entire community and for people outside the IETF that reference IETF specs It's not just for the working group A question? Basically, my question was just in the right? of what we've been talking to. Maybe he's going to Carson's point. So, um maybe we have to do some fixes for 390 But for new documents where we have, for example, the data time, the update broker of the date, those should have upto's back ABNF output If that is turned into a two layer, so in a single layer case, I can just take that paste it there, and it's fine. For two layers, approach, wouldn't I have to start adding the escaping rules? Because some of that part might be escapable and can legitimately be escaped because it can be produced by a tag 999 processing generator. So something could generate a single quote literal with some escape in there. So all of us sudden there those one layer approaches would have to take that IRA reference or date time thingy And for every case where there can be a say, a single quote, replace that with a double quote and for every place where there is a whatever else we can escape and we can escape a lot of things um it would start having to take that into account. And all of a sudden, I'm taking"
  },
  {
    "startTime": "00:54:01",
    "text": "a single APNF that should may or may not be copy pasteable but in many cases it will be copy pasteable into something that needs a lot of pre-processing to really represent what that string would look like before before escaping Pete I said in the chat room, I want to repeat here the documentation should not be driven by a particular implementation model. And Roan is exactly right the reason Rowan came to me in the first place is because he was planning on doing a single package implementation This harms his implementation to do it this way It makes it harder to figure out. You have to do some hand stuff someone is going to have to do a little extra work when Karsten implements it, if you do the syntax, my way, he's going to have to put in a little bit of special extra code going, I'm not going to define app stringing at the top level. I'm only going to use the plug-in app strings. And that how it's going to parse, such that if there's a missing plug-in, it's going to fail in the apprainer way. So he'll cut out that one line of ABN to feed to his automatic ABNF parsing If people do it single, pass and Carston's syntax is used, they're going to have to add a little bit of something Someone's going to have to do a little bit of work My concern is we should make the little bit of work B to change the ABNF into"
  },
  {
    "startTime": "00:56:01",
    "text": "something that it isn't if the implementation it finds that more convenient. We shouldn't I'm sorry, I swam that sentence. We should make the people who want to change ABNF into something that is non-standard ABN do the little bit of work, and it's only a little bit of work whereas for the other case, we get to leave standard ABNF loan not implementation dependent. Everybody knows how the ABNF works and the documentation is correct So I just don't buy this argument that some the ABNF prevents Carson from doing his particular implementation Yes, it means he can't just cut and paste blindly, but you couldn't do that in the first place It's a one-line change that he's got to make to feed into his particular parcel Yeah, I don't know where to start First of all, I'm really annoyed by you asserting that I'm changing AB&F. Sorry about that ABNF is used as designed here. It's just used in ways where we combine multiple ABNF parsers, which you may not have been done in your life before, which is fine, but it actually fits the problem here But the most important thing here is that when you design something, you think about deploying You think about how is it actually going to be used. How are people going to solve their problem? Are they going to solve? the problem in the way that was prepared? Are they going to do some horrible hack because it's faster?"
  },
  {
    "startTime": "00:58:01",
    "text": "for them? So doing deployment right means that you do have to think about ways of doing limitation And the whole point about the application extension mechanism is that it is easy to add stuff to And there are two aspects of this. One is the implementation aspect so it's really easy to add another parser that the stuff to. And there are two aspects of this. One is the implementation aspect. So it's really easy to add another parser that just processes the 10 lines or 5 lines of ABNF that is needed to operate on an extracted text string. And you also have to make it easy for the people who write the spec No sane person will take apart the 40 lines of ABN for AIRF and try to make them compatible with the single path approach. That just doesn't make sense So of course people will hack something and it will work in various ways, it will not be consistent, and so on. By not creating the problem that you need to intercept the string escaping with the action syntax of what you're trying to describe here You are mauch more likely to get a plug-in that actually makes sense. A plugin specification is excuse me, an application or a little specification that makes sense well yeah um so i'm was going to comment on what you just said and what christian said about having to do work on ADNFs that you include so i'm going to give an example so Christian specifically mentioned if there's a character that's escaped, that would normally need to be escaped in the generic BNF but isn't escaped in the thing that you wanted to include then you might have to go through and add that as"
  },
  {
    "startTime": "01:00:01",
    "text": "escaping. And I say, good, that's a feature because the chance of somebody going, and trying to do a single pass, maybe an end they miss that subtlety, is very high. And it means that you know, if, for example, there is somebody writes, an AppString that has an unescaped backspace or sorry back back slash then converting that to a single pass AB and F someone needs to realize, oh, here is a Kate character that would otherwise be escaped, whereas if you are just if you were, if you're doing a single pass extension, then if you use the productions that already exist, those productions already have sets of characters that you can reuse, those building blocks that you can reuse that don't fall into those into those, you know, that don't fall into that quicksand and this is how you know we had dozens of specimens dozens of specifications related to SIP that added things on to the BNF that didn't cause that didn't cause massive problems for implementers or for spec writers because you reuse these building blocks. You write things in terms of the building blocks that already exist May I just ask you quickly a reply question? So I'm if if i understood you correctly, that would to me imply that someone would build a J- question. So if I understood you correctly, that would to me imply that someone would build a JSON parcel where they basically parse the JSON and then at some point start plugging in a UI parser into that because that's the syntax is inspired by Jason so is that what you think will happen in this case? Jason is portable Jason is in extensible, right? The rules. People build accessible"
  },
  {
    "startTime": "01:02:01",
    "text": "applications based on Jason. They just put it into string literals because they have nothing else I'm not sure I understand what the issue the issue is here. We're talking about ADNF the way that it's being used at IETF for the last 45 years years And what I'm saying is if you want to add a new upstring prefix, whose content contains a backslash that you should make sure that the grammar says backslash back, backslash, backslash, back slash instead of just a bare backslash backslash And if you use the, if you use the productions that are already present, I think that's a relatively straightforward thing to do. And lots of other protocols of have done that and have done it where the number of things to be added and the variety of new grammars that were added has been been, you know has been substantial So I don't, this hasn't been, this hasn't been a huge problem for the rest of the internet community, so I don't I don't see why this is a problem for this working group Yeah, but following the example that Christian gave you would argue that any application that uses Jason to keep UIs should be integrating the U.S parser into the JSON parser No. I'm sorry, that doesn't make sense. And that's exactly the idea that created the problem for standards like RFC 5988 where they try to do this and they busomily failed I mean, if you, Jason is not extensible, so you have to treat it like"
  },
  {
    "startTime": "01:04:01",
    "text": "a fixed black box now if what i'm going to point out here is that if you do this, if you have a code coloring, you know, syntax coloring in your edit for Jason, and then you go and do something that has you know, a bunch of escaped back, back double quotes in it The syntax color is going to do the right thing with receipts to the JSON What you're describing for a ABNF, it's going to do something where it's not really going to be correct for somebody who implements a single pass ADNF, which is what the rest of the community is used to doing and what the spec basically defines Okay, we have to move on to the rest of the topics so I had hoped that we could resolve this during this session. I think we have not yet So let's take it back to the list and put a time limit on this I'll post something to the list about what our next steps should be. But we do need to resolve this issue to where I can judge some consensus. And I can't right now So, Carson, move on to the rest of your presentation please Thank you Yeah, I think we have discussed EDN literal The next document is the CDDR control I think we have discussed EDN literal. The next document is the CDDL more control document CDL also has an extension point, which is very different from the extension point that we have in EDN, where we can"
  },
  {
    "startTime": "01:06:01",
    "text": "add control operators to the CDDA processors and we have had an original set of control operators, batteries included in 8610, and then we have had one in 91 and okay occasionally people give us hints that they could write better specs if they had a few more control operators and this is exactly what happened here So the main thrust here came from the fact that we sometimes have to include this 64 encoded, text coded, based 32 encoded or even base 45 encoded bytes string in particular if the city is actually used to describe JSON and not C-WR, we wouldn't use any kind of base encoding in CW SeaWorld, but we do in JSON So this is a much needed support to enable standards like Eat which have both jason and SIBA representation, to share the CDDA code that is describes the structure So I'm going to skip looking at the details here, but it's really useful to be able to say something as basics for encoded And in this particular variant, URL, classic, sloppy, not sloppy, hex lowercase upper case and so on So that's why these names may seem slightly unfamiliar. And while we were doing these, let's encode things in text, additions, we found that often people need decimal numbers for something. So we put in"
  },
  {
    "startTime": "01:08:01",
    "text": "control operator to generate a village decimal number. And then people said, oh, I need a four digit hex number with upper hex. And we said, we are not going to do 10,000 control operators We will just do one, which is called printf and we can steal most of the specific out of ISO IAC 9899 Then we also did it.J .jason control operator and finding a dot join that can be used to assemble longer pieces of text or even longer pieces of bytes string out of components so this has received quite some features For instance, we had something about deterministic that it is not in a different document so i think it's pretty well analyzed by this point We had a working with us call in March And throughout deterministic encoding, clarified the strictness requirements fixed some very unclear text about join and finally we now have a nicer table one that actually gives you the data types. We also have a shepherd report. Thanks to you. Thank you, AJ And to me, it seems we should ship it Thank you Jason Escape, Unescape, DoubleG? quotes? It does everything to represent the data structure that is on the right? side into"
  },
  {
    "startTime": "01:10:01",
    "text": "valid JSON text. And usually you have to do some the data structure that is on the right-hand side into valid JSON text. And usually you have to do something with your quotes. So yes is probably the answer Be reopening Oh, um yeah, I've just been yep, thanks, question So the cue is open Maybe I should show the table so we can see the whole picture Okay, if there are no other questions, I think I would like to hear from the chairs what they plan to do next Yeah, well, if If nobody expresses unhappiness with where we are, given the state of given the maturity of the document, then we're done here Yeah, this is a relatively simple draft I actually timed implementing it. It took me two hours and 45 minutes And this took so long mainly because the base 32 library I was trying to use didn't work"
  },
  {
    "startTime": "01:12:01",
    "text": "so that's what you get from using library All right. Okay, so then, well, let's hear what Lawrence says Yeah, I used this to very all. I'm sorry, Lawrence, I barely here you. Can you please speak closer to the mic? or is that better yeah I use this to verify the CDBL in Eat which you know the and the descent through the base 64 and into the jason and all all over the place it worked really well It caught a bunch of errors in the CDDL in Eat that was really valuable. But it did take me more than two hours to do that Two and a half It's good for me. I'm fine with it thank you okay so um so moves ahead then. Thank you So the next one is packed and I'm not entirely Oh, that ABNF is in RFC 9160 is that the right number? Probably So the previous kit of control operators that we published Okay, Zbore packed There is some pretty interesting thing going on here, which I'm not going to describe in detail but the point here is if you need data compression, you know where to find it. That's not something that this working group has to work on. However in a constrained system it's often difficult to do data compression because compression decompression means copying data and you may not have the storage to do the copying. So it's interesting to have something that is a little bit like compression but operates"
  },
  {
    "startTime": "01:14:01",
    "text": "on Cibor data that actually are sitting in your packet buffer and that makes it much easier to actually use compressed data in a constraint system. So that's what the Cibor Pact is. The draft, defines three things a reference set, number of tags, and also similar values that are used in place as stand-ins for reference data. So if you are used some string twice, you might want to put this string into a table and use something from the reference set to reference that string Then the second thing is we have defined reference semantics. So what I just described is the shared semantics, but there are also arguing argument-based reference semantics, so you can actually conclude things like a curry would do. So curries are kind of covered by placebo and we have something called function tags that actually provide an extension point for the reference semantics. And then the third thing is table building tags that build tables that you want use in those references that I have described. So the table building tags are likely to be application specific They are a natural extension point of ZioPact, why the reference set and the reference sima semantics, with the exception of the function tag extension are probably going to stay at base base. So we had one remaining issue and we need to get around to deciding what to do there. So you might have an unpacking error and how do you hand an unpacking error to the application? Of course you can"
  },
  {
    "startTime": "01:16:01",
    "text": "do it using an API in some form or there might be a standard representation that you can easily if you want to. So that we have to look at that. There also is a another draft, draft, AMZUS, C-wall Pact by reference, which provides another form of table setup But we can progress this draft independently We don't have to wait for the draft to be ready to integrate it because we have the table setup tag as an extension pointer So that's the current situation We all have to polish our implementations a little bit more. We may want to do a little bit more text polishing, but this is essentially done at this point in time So the part that I did prepare properly here is I didn't look at the working group last call and whether we made too many changes since. I didn't is I didn't look at the working group last call and whether we made too many changes since. I did, I had the look there We wanted to go for another working group last call, but there were in one in the latest interim we talked about this still hands on like the updated is not yet ready for for and later working group last call. But I didn't all, when later asking, I didn't get any feedback on which parts need up updating. So I'd like you to ask you to like, hey um is there someone who has like off their head who has a comment immediately or on on the maturity of this not being ready for a plus call otherwise I would should start a very short show of hands on whether the remaining issue has been flashed out or not I think the timeline here is this summer"
  },
  {
    "startTime": "01:18:01",
    "text": "Yes So if like I don't all I also don't see anyone lining up for the queue. So if you could just if especially if you have any objections to this going to work in blueblast. If you know of anything that would stop that makes this unsuitable for what's call, please tell me otherwise we'll start a work new blast call and then continue from there So this is also the implicit, how many people have read the draft? question All people trust others who are afraid? it. AJ? I haven't read it recently so I'm just going to click there before I embarrassed myself. When I read it, I was excited and enthused and we'll reread it. I don't oppose it work with last call and think that's a good idea. That being said don't remember the last time I read it in the last two months so concretely. So maybe I'm the worst kind of opinion in support of that, so I'll let others speak up to Yeah, and we do have the last course so if we find something, we can fix it there Good okay whoever raised that no last time is either not in the room, or it has been resolved. So let's go for working group last call and ship it once that still. Thank you yeah we do one revision, right? Yeah Yeah, but that's like that was that wasn't the title of the of the show of hands so unpack error has to be fixed but that can be probably even fixed in parallel with the others. Good So let's use the last 10 minutes for CDE Deterministic encoding"
  },
  {
    "startTime": "01:20:01",
    "text": "When we defined CBOA in 2000, we were really not that interested in supporting deterministic encoding because so much nonsense has been done with that in other representation formats But we put it in because it's a feature that people want to have And 8949 tightened this down a little bit It's now defined in section 4.2 of 89949. And it actually employs preferred series serialization which is just something with that we recommend Cibur encoder does And now we no longer recommend it we require it, plus add a few things and one of the things is that you actually have to sort the entries into a map to actually get a determination encoding result but all this happens at the serialization level and of course you have to do the whole detail deterministic thing at the application level as well So when we define the section in 4.2, we actually left something open for applications to decide how would level as well. So when we defined the section in 4.2, we actually left something open for applications to decide how they would want to have their more esoteric data encoded. For instance, Flood how they would want to have their more esoteric data encoded. For instance, floating point numbers have some interesting things like nan payloads and so on, which yeah. So at this point in time, looking at the documents, generic deterministic encoder is not fully standardized So that's the background situation we were in And what we try to do and what the DC bore activity essentially pushed us to do was to define a common deterministic encoding that would actually push all the remaining leeway that was needed up one layer"
  },
  {
    "startTime": "01:22:01",
    "text": "So I'm sorry, I'm talking about layers again Essentially an application has to decide whether it sends time stamps as integer seconds or as fronting point numbers with fractional seconds That's not something that C-BOR can solve for you. So we will always have these kinds of decisions at the application there, and we mentioned to squeeze out the somewhat overextensive variability provided by 4-22 fixed nail some things down and put the rest into the application or an application profile So we actually defined a term called application profile. This is a essentially a defensive term So why? do you need something defensive in your architecture? Well, because people are going to do something that is worse if you don't provide it in the architecture So this doesn't mean that we want to see application provides left and right but if people are somehow unhappy with CDE, because of something that happens the layer above, at the application, they can define an application profile. And actually it turns out that DC bore is exactly one such application profile So that's why we at some point found out we have to crystallize that and put on top of CDE to a arrive at a single common CDE But I can understand that some people in this room don't really want to give that much space to the context of application profiles I just think that the reason we are doing this which is to really encourage people to come up"
  },
  {
    "startTime": "01:24:01",
    "text": "with a single, to implement the single common genetic encoding that is damaged if we don't provide this emergency valve called application profiles so this would be the point where we're goes to the microphone Yeah, so I'm going to, I'm going to see back a little bit of background here about the way I think of this. So there's there's Seabor where you're not no, there's just seabor where you're not specifying in kind of serialization of or anything like that It's the, the core rules of, of Seabor, you know, for example, you know, no duplicate map keys So on top of that, you have preferred serialization which is I think something and now we're, and we're discussing also about whether this is going to be called base serialization or what, but for the point, the discussion here I think preferred serialization is actually really important I mean, I, you know, if you, if you, if designing a protocol like CWT, or Centimel or suit or TEP or something like that, and you want to make sure it interoperates, you're going to say, let's use, let's all use preferred serialization and you're going to do all right. So that's, to me, that's like 90 out of 100 use cases that are going to want that. So that leaves like 1% of use cases that need determinism And I mean, I can think of only two examples of determinism that I know of. I mean, maybe I just there's others, but, you know, one of those examples of determinism is DC war the other is the little bitty part in the internal of Cozai where you derive the structures that you need to feed into the algorithms"
  },
  {
    "startTime": "01:26:01",
    "text": "Not the payloads, right So, if we're doing, if we were to do profiles, so, so it's me CDE is hardly ever used and something like DC board is the incredibly specific use case of you know, so much so that we're really kind of pushing it off out of this working group. Hey, I'm not questioning the use case I just think it's a very specific use case that is out there. So if we're going to do profiles, if we're going to enter introduce the notion of profiles in Seabor, why are we doing it for 1% of the use cases and not like all of the use cases? So, and why isn't CDE itself? a profile? You mean you're building profiles on top of something isn't CDE itself a profile? You mean, you're building profiles on top of, on top of something that probably should be a profile itself I think answer that question. It's not a profile because you don't want other profiles to exist at this level of the layering Otherwise, we would have called it a common detail of the layering. Otherwise, we would have called it a common definition with the coding profile. Right, so you're making the defensive argument that you don't want people to invent all sorts of stuff And I'm not really too worried about that because we've only really seen the only real example so far is DC Board. And it's been actually to me, it's a reasonable example it's a reasonable thing to do for a very specific use case. But we don't, it's not like we have you know, in all the years of 10 years of Sea War and CDE, it's not like we've had a bunch of dumb proposals that were we need this defensive mechanism here to keep them at bed It just, and the other I guess the other thing for me is that section four and Section 5 of RFC 88949 are there's a lot of concepts in there"
  },
  {
    "startTime": "01:28:01",
    "text": "and there, that's the hard, those sections are hard for me to read even parts of those things are kind of confusing, like preferred serialization is defined in forward except for in 3.4.1, I think, on big numbers. And so you get caught out on this and there's generic decoder. There's so much concepts here of things that it's hard for me to get caught out on this, and there's generic decoder. There's so many concepts here of things that it's hard for me to... We have a minute and a half left, so please wrap up so i i really am not uh enthusiastic about introducing another concept in Sea War, this whole notion of profiles, especially one that is just narrowly Yeah, I think Lawrence just explained my name slide. So CDE has been pulled out of the head by some people to say, we can use this for variability reduction which is a good thing if you have partial implementations, constraint implementations that don't support the full CBO specification, then you want to know in your encoder what you should generate to satisfy these partial implementations and vice versa If encoders do that, then decoders know which parts they can leave off So, CDE does this perfectly because it reduces variability to exist one encoding But that comes at a price so we decided we wanted to have something that is just on this variability reduction scale which is at the bottom of my slide but it turns out that preferred serialization is a little too weak because we probably want to exclude indefinite the length encoding from this very consistent constrained variant. So we now have a hierarchy of general CBO"
  },
  {
    "startTime": "01:30:01",
    "text": "CBO with preferred serialization, which restricts the end a hierarchy of general CBO, CBO with preferred serialization, which restricts the encoder a bit, basic serialization, which we restricts it some more, but at a relatively low cost unless we are talking about streaming and coding, but for the normal encoder, this will exactly be the same effort during the preferred serialization and the basic series civilization and then we have the deterministic end encoding, which incurs the additional effort of map ordering. And then you can add application provides on top of that but that doesn't change anything about the city layer here OK, we are out of time Let's Chris very quickly say what you wanted to say I dropped it into chat, but basically I'm doing basically exactly what you're saying here I'm using CDE for variability reduction and um i am modifying the CDE in exactly the way you are proposed to not require map ordering So I have at least one specific use case where this is valuable. I don't know that it necessarily justifies that application profiles, but I'll bet we can find out on the list Okay so we're a minute over over I think we just need to take these this to the next side Can you quickly put up the agenda again? So I just want to go through the interim dates quick before we shut down And one more, okay So we start on the 21st of August with our first, and then we have two dates in September, two dates in October before the Dublin meeting in early November We've already cleared these days with the core working group"
  },
  {
    "startTime": "01:32:01",
    "text": "so we're doing our usual alternation And so let's continue any discussion from here on the mailing list and at the August 21st interim Thank you, everybody, for coming Karsten, thanks for being a remote participant And same with the question And hope we'll see everybody in Dublin and on the interim meetings Thanks everyone, see you, have a good trip home home Thank you is the evening literal or the event person? Barry, I just items that I'll probably want for the minutes to check, double check with recording. I'll leave this open my side and then upload it when I'm through it"
  }
]
