[
  {
    "startTime": "00:00:04",
    "text": "a good morning good people can you hear me I can hear myself just not a good thing anyways good morning and welcome to the IETF and welcome to IC crg if you are here if you don\u0027t believe you\u0027re here for IC crg stay on anyways this might be more interesting than anything else you are going to go to before we start I need the standard things I need jabber scribe and I need a minute taker volunteers come on I can start naming names Gouri you\u0027re looking at me like you want to do some Janna I\u0027ll take the minutes but I don\u0027t have a jarek on what do you say Andrew I see it I\u0027ll take the minutes but I don\u0027t have a jabber client so someone needs to JavaScript Thank You Andrews gonna do minutes jabber thank you sir so we\u0027ve got a jabber scribe and we\u0027ve got minute minute taker so we can move on so we have a pretty packed agenda let me see how do I do this but before we start we will just briefly I will flash you the note well very very briefly we follow the IETF IPR disclosure rules and if you do not know what the note well is you should go read it before you come up to the mic and say anything at all there\u0027s also privacy and code of conduct which again I will not go into right now but you\u0027re welcome to read it their slides are available online and the code of conduct is available online at that URL and the goal of the IRT F just to be very clear we do research we don\u0027t do standards development so if you are here to push a standard Strack RFC or on the wrong room but if you are here to hear about research ideas and not just sound let\u0027s work then you\u0027re on the right rope and there\u0027s our agenda for today so we\u0027re gonna basically we have a fairly packed agenda and I\u0027m not going to walk through the entire agenda but we\u0027re gonna try to get done on time here so to the speakers please come up just as your talk is about thought and please make sure to stick to the allotted time and that "
  },
  {
    "startTime": "00:03:07",
    "text": "includes me so I\u0027m going to go through this fairly quickly the first thing to note here is we have to site meetings we couldn\u0027t really bring them into this meeting but they are very relevant and you may be interested in them so do go to them if you think you are interested the first one is on data center condition control and Paul Congdon is organizing this Paul\u0027s there raising his hand and please ask him speak with him if you have any questions otherwise the those details are sill correct Paul okay lovely thank you so that\u0027s tomorrow morning in room VIP a early in the morning the side meeting time slot and then you have l4s on Thursday in this very room at the same time at 8:30 in the morning Bob are those details still correct so this will basically be an update on l4 s yes o l4 s and okay TCP Prague it\u0027s about TCP Prague not a bottle for s not just sell for us yeah okay but itself for SNP CP Prague euro the details are that the l4s is mostly in the th vwg meeting on 3rd on Thursday this side meeting is mostly TCP Prague fair enough thank you for that clarification David and that\u0027s happening Thursday in the morning so again these are very relevant to ICC RG so we just don\u0027t have enough time in here to do do all of these conversations so these side meetings are you\u0027re encouraged to go to them I will be there and with that I\u0027m gonna start off on just a one topic I\u0027m gonna spend at most five minutes on this we had started a conversation last time how to do this beautiful fit the page maybe okay that works we had I had broached the topic of doing documents in IC crg publishing documents through ICC RG adopting them in ICC RG I\u0027ve had a number of conversations with various folks since then and I\u0027m basically here to say now that we will start adopting documents in this research group I intend to use discretion and the RG is guidance in proposing what documents makes sense but that\u0027s the the goal is to adopt and publish documents here we\u0027ve done this way back in the past I\u0027m gonna start doing it again what does it mean to adopt documents well broadly the document is the reasonable fit for what we do in this room and by what I what I mean by what we do in this room is not just if it fits the letter of the Charter but if there\u0027s energy in the "
  },
  {
    "startTime": "00:06:07",
    "text": "room and if this is the right sort of audience to do this work or look at these documents typically any condition control other fits in internet-like environments is an absolutely wonderful fit for example but we need to see author engagement we need to see author engagement with the research group and to show that there\u0027s interested in continuing that engagement to be adopted in this research group and again I intend to use discussion and research group guidance in determining relevance here what does it mean to publish a document it just means that the research of things the document is in good enough shape yes this doesn\u0027t seem like a very firm process but that\u0027s by design this was not intended to be a heavy process this is not the ietf again I expect to call upon the RG folks in the RG for reviews and for other decisions and we will figure this out as we go there\u0027s I have faith in the community here that we can make this work I feel great saying this by the way this is a good medium to say this because I feel like I\u0027m I\u0027m handing off this I\u0027m doing this thing from upon high looking down at all of you I will decide but this is this is the plan going forward is to publish documents in ICC RG and starting off I\u0027m gonna propose adopting the following documents we will hear about them today the first one is led back plus plus the second one is our LED bat if you haven\u0027t read these documents please do I think they are fairly good and I think laid-back + + s is actually I\u0027d say it\u0027s it\u0027s almost an update to let that and it\u0027s it\u0027s a very important update to like that itself so I\u0027m going to propose this I will also send this out on the mailing list and we\u0027ll adopt it when unless I hear somebody yelling and saying this is a terrible idea that\u0027s the plan going forward with that well any any quick thoughts I give you 30 seconds and your 30 seconds are up Thank You Corey sorry go on Corey first I\u0027m cold account real quick III think this is a good idea I like this and these two these two documents fit well because they are less they are more conservative in other transports therefore they definitely fall within something that could be done in a research group other things we don\u0027t have to tread carefully on and I guess we\u0027ll figure it out as we go on but these two documents seem like good candidates for here to me yeah I think we will have to figure this out as we go so question is the Martin Duke f5 is the desired outcome of these documents to produce informational RFC s or something else I yes I would think so I don\u0027t know can we publish experimental a lot of C\u0027s here anybody no we can\u0027t do that here right who says we can Andrew says we can "
  },
  {
    "startTime": "00:09:09",
    "text": "I come on Perkins yes you can publish experimental or informational RFC Thank You Colin that\u0027s the voice of authority there so yeah I would I\u0027d be interested in experimental but that\u0027s what I\u0027m gonna be pushing folks for anyways thank you for that with that I am going to move on with the agenda again this is the agenda pravin Europe first you\u0027re gonna talk about that bed plus plus and I\u0027m gonna give the slides up morning everyone can you hear me morning everyone I\u0027m here to talk about led by plus-plus this is now a draft if you haven\u0027t read the draft there\u0027s version zero one that was published recently so please read it LED bed plus plus is a less than best effort congestion control so the goal is to use unused capacity bus but quickly yield to for ground flows example workloads include operating system application after it updates or backup replicas sort of burglars next slide please slide is a summary of led by plasmus the original RFC 68 17 has a bunch of problems which were found in prior research as well as in our efforts to implement the RFC in the Windows operating system I\u0027ll quickly walk through the problems and the mechanisms that had bad plus plus proposes to fix those problems one way delay measurements are hard in practice for example in TCP the clock frequency is not known of the pier as well as there is a clock drift problem let my plus plus users RTT there\u0027s a downside which means on the downlink you may measure congestion and react to it but because this is less than best effort in practice this doesn\u0027t cause problems there\u0027s a late comer advantage problem if there are multiple flows already in the bottleneck link a late comer or let bad flow would end up getting more than its fair share led by plus plus introduces two mechanisms to address this problem one is the multiplicative decrease instead of additive degrees giving congestion avoidance as well as initial and periodic slowdowns the original RFC said that natural gaps in application traffic would allow led by to measure the base delay but that doesn\u0027t happen in practice so led by plus plus immediately after slow start would enter a slowdown period and then would enter subsequent slow long periods with a goal to sacrifice throughput of at most to be at least 90% of standard tcp when there is no competing traffic "
  },
  {
    "startTime": "00:12:11",
    "text": "there\u0027s a internet but fairness problem as well this is related to the late comer advantage problem and you can measure the base delay but although the queue is stable the flow is bound fair sure the led by plus plus introduces multiplicative decrease to solve this problem the original RFC does not specify slow start in very great detail there\u0027s two things that led by plus plus does it does slower than we know ramp up during slow start and it also does a variant of high start to exit slow start when it senses delay increases on the bottleneck link there\u0027s also a latency drift problem when you let connections run for a long time there\u0027s a ratchet ratcheting effect where the measurement of the basically key keeps increasing and that causes us to keep increasing latency from the link initial and periodic slowdowns help address this problem there\u0027s also a low latency computation problem if you have a bottleneck link where you can\u0027t reach target delay for example if the buffer is too small that causes LED bad to basically just compete with standard TCP and the bottleneck link so slower than Reno increase during condition avoidance addresses this problem there was on the mailing list there was a problem that was brought up I think need card will brought this problem up there what happens when there are multiple flows or large number of flows on the bottleneck link do we see a ratcheting effect problem or not so we actually did this experiment recently so this is a hundred millisecond or titi link we actually changed the target in later hundred millisecond for this experiment but yeah the results are similar with the default of 60 milliseconds and this is a 10x 10x bdp bottleneck buffer so even with the increase in number of connections we see that the latency remains near target at or near target so when we look at traces there\u0027s two reasons one is when the new flow starts even though the slow start is slower than Reno it still causes a deal increase and that causes other flows to back off and then the newcomer flow would immediately enter it slow down phase allowing all the flows to measure phase doing we find that all the flows are able to successfully measured by anyway there is another experiment this is with both 1x + 10 X B DPS again with large number of flows as we increase the number of flows we find that things mostly stabilize around target so this is 100 millisecond or TTN and the D for 60 millisecond target so we would like to experiment more there was a suggestion that we use B be our probe our TT phase and see how that doesn\u0027t comparison that would be a very interesting experiment to do in the future but currently in practice we don\u0027t see a problem what are the changes since the first draft so I read and rearranged some sections to make readability easier and thanks to Marshalls review so I fixed all the feedback that he gave to make the text better the multiplicative decrease cap was not specified very well so the intention is to basically ensure that we never react more than what Reno would do in if it sees a last "
  },
  {
    "startTime": "00:15:13",
    "text": "signal so the delay increased signal the worst-case decrease even with multiplicative decrease would be half the condition window removed the reduction factor just to simplify so if the reader has read that bad we just use the gain parameter now one of the requests has been to make the document stand alone currently it\u0027s a addendum on top of the original RFC so the goal is to make the document stand alone so that any implementer could just read this RFC and be able to implement the algorithm we also want to add pseudocode so that it makes it easy for implementers so I request to folks in the room would be please review this draft I think like less than best-effort congestion control is extremely important so please review this we provide feedback if you could implement this and do some experiments on your own that would be great any questions Jake on I was curious whether you had a chance to examine the impact of a queuing of FQ systems on on this strategy we have not so I call that out in one of the prior presentations it\u0027s also an active area of research I think like both testing with a QM as well as newer congestion controls like VB are they currently are testing is limited to cubic and you know so yeah testing with other delay ways congestion control algorithms which are aggressive would also be I think part of that future work thank you [Music] other questions we have a couple of minutes all right all right thank you thank you a presentation Marcelo morning hey I\u0027m Marcelo Angelo going to talk about the receiver driven led but so I represented this in the last IDF so basically this is an update of the changes that I\u0027d done in the draft so if you remember LED body is a set of mekinese that allow to run a less than best-effort congestion control algorithm at the receiver in order to throttle the control centers rate right so in the original document I mean the in the "
  },
  {
    "startTime": "00:18:13",
    "text": "version zero document that I represented last time the document contained both the mechanism to implement this in the receiver and the congestion and congestion control algorithm controller is now move to to the level plus plus a document that Praveen just presented so in this document we only kept the set of mechanics that allow the receiver to exert control over the sender\u0027s rate right so in particular in this document we kept how to use the receive window how to manage it safely in order to avoid window shrinking handling the window scaled option and this type of things in order to control the sender\u0027s rate right in addition we work out the different dock mechanics in order to feed the information that the congestion control economy needs including the Artic measurements in order to estimate queuing lay and the detection of retransmissions at the receiver so basically the the input that that the congestion control algorithm form of RTD measurements delay measurement and some form of loss transmission detection so the changes since the zero zero ssin is we remove all the kind of the part of the congestion control algorithm we defined so that elaborate is compatible with any less than best F for congestion control as long as it uses some form of delay estimation and all packet losses in order to work and we currently are referring to labelled plus plus or any other a congestion control that uses this type of signaling we could point to LED but if we consider that using LED but with RTT instead of one-way delay make sense assume I mean consuming that leopard plus plus is essentially an upgrade on on lead but we believe that this may not make a lot of sense but we could do it if people feel strongly about LED but and regarding our LED what implementation we\u0027re currently working I mean we haven\u0027t an implementation we presented some results in the in the last IDF we\u0027re now working in order to make it perfectly aligned with the level plus plus latest version that Praveen just presented this is this is ongoing work we hope to be able to present a results in the next meeting and we also became aware that a apple have a receiver based less than best effort implementation that is available in this in the it\u0027s open source and available here it seems somehow aligned in the sense that the use RTT use timestamps and receive window basically the tools that we are that we are describing the draft but it would be interesting to actually have some input from from from apple to see if they actually are aligned to to what we present here and "
  },
  {
    "startTime": "00:21:15",
    "text": "if not to be able to align properly the less than best effort control congestion control agon that they use clearly it seems different then LED but misplaced but as as I said are LED but is compatible with different congestion control I\u0027ll go and examine xed steps will be will be very useful to have reviews comments from from the from the research group and feel free to contact me if you have further questions or comments and with that I available for questions or I can leave questions folks come on it\u0027s Monday morning do that the mic is all I\u0027m saying some engagement people well I actually do have one question for you myself have you considered expanding this to the mechanisms would just be slightly different but I think the concepts still hold for quick yes do you plan to write that in the draft or do you think in this particular draft yeah yeah I\u0027m asking ah I was thinking to I mean once I\u0027m done with this do it in a different document but if you love define do I mean I mean if you want me to do this talk mean I\u0027m happy to it it seems reasonable to do it if you\u0027re gonna cover both TCP and quick in the same if it\u0027s possible to write I don\u0027t think the differences should be that much okay but yeah if it becomes big enough it might be worth separating it out okay yeah sure thank you gory Fair has just asking about what you actually said at the mic were you asking about strange Eckles were you asking about whether this document could describe a method that could be applied to quake or were you saying the document would include a mechanism described how it applied to quick I see the two is quite different that\u0027s fair I was asking how you would apply it so the mechanism itself is not an underwire change it\u0027s entirely a receiver side it\u0027s its management of the receiving door in the world of quick it would be how you advertise flow control credit and so it would be slightly different in the sense that you\u0027re not just talking about modulating the window but you\u0027re talking about modulating the size of the credit that even advertised and there is no equivalent I mean you can actually shrink effectively so that gives you a degree of freedom that is available in quit because what I mean by shrinking is you advertise lesser credit in the future right because you can actually shrink the window so if that I don\u0027t need that\u0027s that\u0027s what I was asking me this right the way it applies to quick would be slightly different so if I "
  },
  {
    "startTime": "00:24:15",
    "text": "understand that that means this is a mechanism and you\u0027re not going to describe the protocol but you\u0027re going to describe the mechanism we saw what you were talking about yeah so so maybe maybe then what I\u0027m asking for thank you for for for teasing this out Cory what I\u0027m asking for is how would this apply to like a credit-based flow controller as against there in the window waste one okay that\u0027s the difference I\u0027m looking at clear thank you Robi Praveen from Microsoft I do think that both of these RFC\u0027s are kind sort of transport agnostic there are certain like transport mechanisms were using to communicate information as you point out but yeah I think they could be written aware that they could be applied to both TCP so that\u0027s true but does the avoiding of the risk of the window shrinking right which is very tcp specific it\u0027s it\u0027s very ecosystem specific to now that\u0027s not applicable to quick possibly not but III don\u0027t think it\u0027s illegal to shrink it\u0027s what\u0027s not illegal it\u0027s it\u0027s not illegal to avoid it certainly but I\u0027m saying it you have the degree of freedom if you want it okay Nokia versa I\u0027m wondering about the windows shrinking thing is that like what I mean I understand what it is but I don\u0027t know what what would current TCP sender applications to if you if you put you know if you would submit the window that is too small and you would actually shrink it they might just ignore it right because they have to have a way of handling it on the sender side anyway if that happens what did send a reset or sorry buddy no so so if you so as far as I can tell there are some situations where the window shrinking is general when you\u0027re using very large window scale for instance that the granularity of the window that you are able to announce will will result in window shrinking so I understand that implementations usually can handle window shrinking without losing packets but that\u0027s that\u0027s what I think Praveen Tony so gory Fairhurst not answering the question directly but saying that will because I\u0027m going to talk in a side meeting on quick for satellite later in the week and one of the things I\u0027m going to talk about is the way tcp windows are managed in bsd using receiver window advertisements and how you grow and shrink the receiver versions and how that relates to what quick does so this is an area where yes you can\u0027t change the receiver window and it doesn\u0027t will impact maybe that causes you to send more than you\u0027re expecting it ends that we ditched or whatever but this is a mechanism that does work in real implementations and and talk more "
  },
  {
    "startTime": "00:27:15",
    "text": "to me if you wanted to find out what I was saying thanks Maya okay thank you so these are the two drafts that we are planning to adopt if you have any thoughts please send them to me or send them on the list like I said I will send out an email I enter the list after this meeting and I see news already there and are there miles on you guys can you guys hear me yes we can hear you okay and your slides are up gonio all right great hi my name is Neel Cardwell and I\u0027m gonna talk today but with a brief update VBR version to work at google we think of EVR to as a model based congestion control and today I\u0027m going to focus on some recent performance optimization work that we\u0027ve done this is joint work with my colleagues at Google you Chong and Sohail Puri Ranjan you suck in Kevin the quick folks in Victor and bin a summer intern I had this year Luke and Matt Mathis and magic I\u0027m hearing substantial echo in my hand are you guys okay yeah we\u0027re fine go but that says the function of the room okay great all right next slide please yeah so just a quick outline I\u0027m gonna talk for a while about the performance optimizations I mentioned and then I\u0027ll give a quick rundown of the status of the VBR version to code and the deployment at Google and then wrap up next slide please so yeah performance optimizations next slide so what are we doing here what\u0027s the goal here well we basically want to ensure that bbr version 2 gets to a point where it\u0027s doing a good job as a general-purpose congestion control for both TCP and quick for the environments where those transport protocols are used today so that means LAN and when networks data center networks VM guests and we want to get to a point where it\u0027s a good drop-in replacement for the predominant algorithms used today so reno Kubik DC TCP and we want to make sure that it\u0027s providing in performance improvements across all of those environments and has acceptable coexistence properties when it shares with common congestion control algorithms like renown cubic and of course as a stepping stone to number one we are deploying bbr version 2 for all the TCP and quick traffic at Google and we are working on that as as we speak and as part of that of course that involves a fair amount of testing and that so we basically want to ensure that PBR is doing a good job on both the "
  },
  {
    "startTime": "00:30:16",
    "text": "production where clothes we see at Google and a wide range of test matrices in synthetic or lab tests and we want to make sure it\u0027s doing at least as well as we know and cubic and DC TCP next slide please so I mentioned some tests of production workloads a cool so so what\u0027s that so at at Google we do a fair amount of testing on our production kernel changes this includes TCP loss recovery and congestion control including VBR changes and there\u0027s a fairly rigorous suite of application benchmarks that we apply obviously web search is included but also databases and storage on and we look at a fair number of scenarios and not just sort of simple bulk transfer dumbell tests and not just looking at only traditional congestion control metrics like loss rate or throughput fairness queuing delays and so forth we also look a fair amount at CPU usage and median and tail RPC latency and why we do that well you know it when you operate these things at scale in production environments of course the details matter a lot and in particular one of the important details that doesn\u0027t always get as much attention as it should is is CPU usage and the sort of contributing factors including data packet send rates acknowledgement rates offload birth sizes interrupt rates all the things that feed into CPU usage and in addition there are a lot of sort of tricky and stressful scenarios that are important to test like when you have thousands of flows coming from a single sending host or sharing a small BDP in a data center like environment those are also important to test and as we\u0027ve been running bbr version 2 through these kinds of tests at Google we\u0027ve developed a number of performance improvements to tackle various issues that we\u0027ve seen in these tests and so I\u0027ll go through these in a little bit of detail but just to give a quick summary I could say that basically the the first two improvements are in order to match the CPU usage for Reno or a cubic DC TCP and number one we developed a fast path for PBR on number two we did some improvements in the TSO auto sizing calculation and then the third improvement was to fix in a sort of interesting issue we uncovered with the Linux TCP receive code path and for that we\u0027ve developed a mechanism that generates faster acts and then finally the fourth one is to enhance PBR version to you so that it gets better performance than the widely deployed algorithms like Reno and cubic and DC TCP and the sort of interesting case where there are more flows sharing a bottleneck than the bdp of the path expressed in packets "
  },
  {
    "startTime": "00:33:16",
    "text": "all right next slide so the first improvement that we worked on was a fast path for bbr so why did we do this well much of the traffic in the real world is application limited so web transfers RPC traffic adaptive bitrate video and if you think about it cubic and Reno and DC TCP when they\u0027re not receiving ecn or lost signals they take a essentially a fast path when they have application limited traffic the the first couple lines of most of them basically say if this if we\u0027re if the flow is not currently see when limited and then let\u0027s return and not do any of the other processing that we might normally do for the congestion control the problem here is that bbr when run on these very simple application limited workloads I ran into CPU usage issues and even through progressions and some of these simple application limited workloads and why is this well thus far the BB our code base has tried to prioritize simplicity and basically ran the entire algorithm on every act so that means updating the entire model of the path updating the probing state machine and then adjusting all of the control parameters the pasting rate congestion window offload chunk size and this was causing up to sort of 2 to 5% CPU and throughput regressions on some of these synthetic tests in these workloads and to tackle this we basically constructed a fast path for bbr where the idea is is that we only run the portions of the algorithm that are strictly needed based on the properties of the information being conveyed by a given acknowledgment and this resolved those cpu and through progressions without sacrificing throughput or latency an excellent please so we also worked on some improvements in TSO auto sizing so what is TSO auto sizing so in high-performance transport stacks often they achieve a very big performance improvement by bundling together consecutive segments into a single unit when passing them down to lower layers like the IP layer and the the NIC driver and in particular the the core Linux TCP stack has for about seven years or so used a TSO auto sizing algorithm that adapts the offload chunk size largely based on the pacing rate and it goes something like this so the the core TCP stack computes a pacing rate and whether that\u0027s used for pacing or not it still uses that pacing rate for the offload sizing decision so it computes the pacing rate as some constant scale factor times the congestion window divided by the smooth "
  },
  {
    "startTime": "00:36:17",
    "text": "round-trip time and then it takes that pacing rate and it calculates how much data it thinks would be paste out in one millisecond and then it applies a floor of two segments and a ceiling of 64 kilobytes that\u0027s determined by the the offload mechanism itself and an interesting thing happens if you have a workload where the sender host is the bottleneck for a large number of flows and in that kind of scenario because the sender host itself is the bottleneck there tends to not be any easy on or lost signals that the sender\u0027s get and so with an algorithm like DC TCP or a cube occur we the Seawind can be surprisingly high in these cases which then generates a large pacing rate and then a large offload chunk size and that large offload chunk size allows very low a very efficient CPU usage and then the problem is if we if we take a b b our scenario where we have a large number of BB r senders and they try to use reuse this auto sizing algorithm now BB r is computing its pacing rate based on its estimate of the flows fair share of the bandwidth coming out of that sender host so if you\u0027ve got thousands of flows the fair share for a given flow is quite small so PBR was choosing small offload chunks and using a lot of CPU and so to solve this issue we developed a mechanism that adds a term into the auto sizing calculation that is a function of the minimum round-trip time so that as the minimum round-trip time goes the saw the size of the offload budget that is added for the min ITT falls off rapidly but for very small men are TT\u0027s there is a considerable budget that\u0027s allocated for the the TSO offload size and you can see the exact formula there and you can read the the code for the the details on the constant next slide please so the the third issue that we tackled was and it\u0027s sort of an interesting issue involving the specifics of the Linux TCP receive code path and the delay dock logic and DC TCP and BP are v2 both have a nice property where they emit an immediate act when the incoming stream of congestion experience gets change but that mechanism doesn\u0027t always save you because if you\u0027re under heavy enough congestion then there\u0027s continuous C II marking and that mechanism doesn\u0027t kick in so it can happen is that you can actually get into trouble as a Linux TCP receiver because the the details of the code for the traditional TCP every other Paquette mechanism are actually a little surprising and they\u0027re actually two conditions the first one is what you "
  },
  {
    "startTime": "00:39:18",
    "text": "might expect which is to say if more than one MSS has been received since the last time we sent an AK then that condition needs to be met but it also has to be the case that the next received window that we\u0027re about to offer is at least as big as the previous one and the problem you get into is if the receive window stops growing this causes check number two to fail which causes the receiver to wait until the application actually reads some data out of the receive buffer to allow the receive window to advance and then trigger and acknowledgment and this was actually this sort of surprising detail was actually causing 2x higher p99 RPC latency x\u0027 was sustained congestion and some of the tests we were running and as as a solution what we\u0027ve done at least thus far is to just remove truck number two and this is taking care of those RPC latency issues and we should say that if this works well in practice as we deploy I think will propose this as a general fix for the Linux TCP receive code path next slide please so this is just a quick picture to give a better sense of what\u0027s going on there because I know the the text doesn\u0027t make it entirely clear so here we have a picture that\u0027s a time sequence diagram of one of these scenarios and you can see at the beginning of the connection the receive window is is opening up you can see that as the received window the the yellow line is growing further away from the act line the green line and during that phase the receiver is allowing itself to acknowledge immediately just giving a high throughput which is reflected in the lot and the high slope of that line but then as soon as the receive window stops growing the yellow line sort of plateaus you can see that the acts are all of a sudden very delayed because it the stack has suddenly waiting for the application to read some data before it sends an acknowledgement so that\u0027s just an illustration of an example of this next slide please so finally the final mechanism I\u0027m going to talk about is is some work to some work in progress that we have to reduce the queuing that we see when we have lots of flows so next slide so what\u0027s the scenario we\u0027re talking about here so this is a scenario that\u0027s been documented and and talked about for a quite a long time and I think it goes at least as far back as Robert Morris\u0027s paper on TCP behavior with many flows in 1997 and maybe there are earlier papers as well but the basic issue is that when you\u0027ve got mini flows and al-obeidi P then most existing widely deployed congestion control algorithms like DC TCP Kubik you know and BB are thus far tend to run window limited and they\u0027re gonna have a standing queue when the number of flows times the minimum congestion window of those flows is bigger than the PDP and the problem that "
  },
  {
    "startTime": "00:42:20",
    "text": "you then get is that there\u0027s a standing queue that\u0027s basically that budget of the aggregate in flight the number of flows times minimum Sealand minus the bandwidth delay product and that can be fairly substantial and just as an aside here the to give some sense of scale the the B be our minimum C wins aside from RTO recovery is for segments and that\u0027s to avoid stop-and-wait behavior with TCP s every other packet policy so the diagram here sort of gives you an intuition about what\u0027s going on you can sort of see that the Q of excess packets here is building up at the bottleneck due to the aggregate in-flight being larger than the BGP of the path and all of these flows running when they limited next slide please so the solution that we\u0027ve been investigating is to improve PBRs use of ecn and loss signals to make sure that we do a better job of adapting the pacing rate to match the available bandwidth and so there\u0027s sort of two core pieces of this the first is a multiplicative decrease in the pacing rate and in-flight data when we see ecn marks as well as loss and that allows us to sort of quickly match the available bandwidth and then drain the queue and also helps in converting converging toward a fair allocation bandwidth and then when the cue seems to be low enough based on the EC on signals we\u0027re getting we then do a small additive increase and then continue in the BB our bandwidth Pro beam growth curve and this is a paper this is a sort of approach that\u0027s been discussed before you know there\u0027s the the google timely paper from 2015 discusses this kind of approach and I also believe Bob Brisco has done some work in this area for TCP Prague if I\u0027m not mistaken and the picture here just sort of gives you an intuition about the the dynamics that we\u0027d like to see where all of the flows are matching their pacing rate to the available bandwidth of the ebon link so you can try to reduce the queuing there at the bottleneck next slide please so this slide has some details about the the mechanisms that we are experimenting with in the PBR version two code base and the at the top there\u0027s a sort of summary of the core mechanism as ecn marks are arriving once per round trip time bbr v2 has thus far been reducing it\u0027s in fly cap or in-flight low multiplicatively based on an exponentially weighted moving average of the ECM mark rate and then for this experimental variation we are also capping the the pacing rate or bandwidth low as as we call it in the code base and decreasing that likewise and the "
  },
  {
    "startTime": "00:45:23",
    "text": "other sort of pieces of the puzzle here as mentioned before we\u0027re cutting the the bandwidth low multiplicatively and we\u0027re as as another piece of this we\u0027re increasing the the ecn factor which is the sort of magnitude or scaling factor for scaling the ecn response into the pacing and in flight response and then we\u0027re also decoupling the bandwidth high parameter which was already in the in the BB rv2 model from the maximum bandwidth sample that we\u0027ve seen so that we\u0027re sort of refining the notion of this bandwidth high parameter whereas before it was just the maximum bandwidth that we\u0027d seen recently and now we\u0027re turning it into a more refined notion that says it\u0027s tracking the maximum bandwidth that seems consistent with the tolerable level of EC on and loss marks and if we\u0027re not seeing a tolerable level of those signals then we do a multiplicative cut in the parameter and then finally when it seems like the level of queuing is low enough based on the EC on mark rate then we go ahead and do a very small additive increase and then continue on in the bandwidth probing state machine so those are just to give you a sense of the details and next slide please so I\u0027ll present some synthetic lab test results to give a sense of the properties of the experimental algorithm here here we have two Center machines and one receiver all on the same switch all with a 50 gigabit Nick and the switch here is configured to use DC TCP style ecn marking where the congestion experienced bit is that if and only if the instantaneous Q is greater than 80 kilobytes in this particular case and we\u0027ve got 60 second bulk knepper transfers here and we\u0027ve collected a number of typical metrics which I\u0027ll go through as they talk about the results and then when we\u0027re comparing three different congestion control variants here one is the Linux DC TCP code the very latest and then one is the VBR version 2 baseline which is the algorithm without the changes I just described and then BB are two new is the algorithm with the changes I just described next slide please all right so here the results next slide so the most interesting metric here in these experiments is that if we look at the queueing pressure as reflected in the retransmission rate as we scale up the number of flows with notices we\u0027re sort of Inc doubling the number of flows in each experiment you can see with a small "
  },
  {
    "startTime": "00:48:23",
    "text": "number of flows all of the algorithms are doing quite good at keeping retransmission reloj and as more flows are added there\u0027s sort of varying degrees of retransmission rates with the largest number of flows you can see d c-- tcp because it\u0027s necessarily running window limited tends to have a fairly high retransmission rate around sixteen percent in this experiment experiment and the newer version of EVR is able to keep the queuing pressure lower and has a lower retransmission rate of around 1.6 percent and a next slide please similarly if we look at other metrics that reflect the queueing pressure we can see that on the newer version of BiBi ours is doing a slightly better job at keeping the queues low this is the average RT t next slide and here\u0027s the 95th percentile RT t it\u0027s sort of a similar pattern next slide please and then the throughputs are relatively similar for the for the variance the older version of e b r2 is not quite as high as the others but otherwise they\u0027re fairly similar next slide please and then you can look at sort of the C e mark rate is sort of an interesting way to look at how good the algorithms are doing at avoiding saturating the signal they\u0027re using and you can see the PVR the newer version of PBR is doing reasonably well here next slide please and then finally we also look at the fairness the jain fairness index for lips small number of flows that are all doing quite well with larger number of flows they\u0027re all not quite ideal and there\u0027s definitely some room for improvement and we\u0027ll be looking at proving that but it looks to me like their use ibly fair and all you know in the same ballpark but there\u0027s some work that we\u0027ll be doing in that area next slide please so wrapping up next slide so where are we in terms of the status of the code so the performance improvements that I was just discussing we pushed those to the Google will be our github repo you can follow the link there for instructions on how to check out and play with the code as we mentioned in July and there\u0027s also a bbr aversion to implementation for quick and you can follow the URL there and we again we encourage folks to take the code for a spin try it out share any test results they have our packet traces share any ideas about issues they run into or ways to improve the code we\u0027re always looking for a feedback from the community and then we "
  },
  {
    "startTime": "00:51:25",
    "text": "have links to the slides and video from a previous IDF\u0027s on next slide please so where are we in terms of the deployment status we\u0027ve got PBR version two running for a small percentage of users on YouTube where we see lower queuing delays than both PBR version one and cubic and considerably reduced packet loss closer to cubic than VB R version one and then we\u0027re in the process of a large-scale test pilot program pushing BB r v2 out as the default congestion control for TCP within and between Google Data Centers and we\u0027re seeing some nice results there and we\u0027re continuing to iterate both in production and lab tests next slide please so finally in summary we\u0027re actively working on VB our version 2 at Google and doing some tuning of performance especially in data center contexts high-speed data center networks so that we can enable VB our version 2 globally at Google for internal traffic and we\u0027re working as you saw on improving the algorithm to scale to larger numbers of flows and as always we invite the community to share any test results or issues or patches or ideas they have we also wanted to give a shout out to the freebsd tcp community and the team at netflix which has been working on bbr and they actually released a an ability shinobi be our version one for FreeBSD a couple of weeks ago and we wanted to finally leave you with some food for thought it\u0027s sort of an interesting question of if you have to design a KPI or key performance indicator for congestion control what would that look like basically the question is how would you tell if your congestion control is doing well in a production environment given that in a production environment the traffic is dynamic the routing is dynamic topologies are dynamic and so it\u0027s it\u0027s it\u0027s non-trivial as far as we can tell to to develop a good key performance indicator and we encourage folks to tackle that as a research problem if they\u0027re interested we think it\u0027s a valuable area for contributions so thank you very much any questions or comments I I new mardukas thanks for giving up your Sunday night I have two questions one is is there a plan to update the BB r Draft at some point we\u0027re trying to track with an implementation I know obviously Netflix as well we really nice them a fixed target yeah an update of the internet "
  },
  {
    "startTime": "00:54:25",
    "text": "draft to reflect version two is absolutely on the to-do list and I apologize we weren\u0027t able to get it done by this ITF but rest assured it\u0027s definitely on the to-do list yeah okay thanks the second can you return to the sequence number graph you had yeah so um maybe I\u0027m missing something here so this strikes me is a for certainly for legacy t speak gesture controls this is a feature not a bug like you\u0027re not getting pulling more data out of the system when the app is not able to consume it is this a problem in bbr just because the act pacing messes up your bbr or your bandwidth estimation no I don\u0027t think so the it it\u0027s it turned out to be a problem in practice in these RPC workloads and it\u0027s you know it showed up in the detail Layton sees for one of their storage applications that we\u0027re benching benchmarking so we could definitely go into detail offline if would be helpful but yeah not sure what else to add it at this point okay well I mean I guess III think you were proposing that maybe this should just be a blanket changed TCP to remove that second condition about the window increase and I I don\u0027t have a lot of experiment remember why exactly that was put in but it strikes me as it as a fairly obvious thing for for the reason I just said which is to not just pile more stuff on on a backlog client on a backlog receiver right I mean I think there are probably a number of trade offs here and I guess it you know at the end of the day maybe it\u0027s something that is going to be context dependent whether it\u0027s a an improvement or not and so that\u0027s you know why we haven\u0027t offered it immediately as a fix for upstream we want to get some experience with it in production workloads and see if it is you know see if it is a net win we think that you have good reason to believe that it will be an at least acceptable if not an improvement in every case because we the the previous congestion control that was deployed in Google actually had a de facto behavior that in congested cases actually did essentially bypass check number two so we do have a fair amount of experience at least in the congested case with it helping but you know well reserve final judgment until we\u0027ve pushed this for all of our workloads and and we don\u0027t see any regressions thanks Neal right there just "
  },
  {
    "startTime": "00:57:29",
    "text": "to ask a follow-up question do you know why that second condition was was instead in the kernel right now we don\u0027t know for sure I mean as Martin said there there is an argument that you can make about trying not to overload the receiver another argument that Eric dooms a pointed out was that this check works nicely with the underside fast path which has only taken if the receive window is is constant and so this can save some CPU on the sender side if they received window stays constant yeah but where this this check predates they get history in Linux so it\u0027s a little a little tough to talk Wow all right well thank you so I\u0027m I\u0027m very amused of course because my question was what was condition to expected to help solve Roberto who are you hi I\u0027m Roberto and I\u0027m with Facebook right now and III you know one comment is say hi to everybody the other comment I love the data-driven approach and in cases where there is no clear answer why I\u0027m really hoping that you will be able to drive changes in behavior this particular one with number two here seems exceptionally odd to me because it it it provides a cliff of behavior and we know cliffs and in control theory tends to be bad for being able to understand the systemic properties so I think it\u0027s super awesome to try and either eliminate to or prove that there\u0027s a good reason for it to exist given that nobody seems to know why so anyway thank you thank you a couple of questions so the minimum congestion window for so TCP traditionally had that value one matter standing is quick is using two because of the problem that you mentioned in the slide I\u0027m just curious why you would pick four instead of two so the details have to do with the dynamics that you get into given the delayed act behavior so the basic idea is that you want to have four packets in flight so that two of those packets well the simplest scenario to imagine is basically that two of those packets are in flight in the data transmission direction and because you have two of them that\u0027s going to force out quickly quickly force out a delayed ACK or quickly force out in a community rather than delaying the ACK rather so then you\u0027ve got also two a two packets worth of budget coming back in the reverse direction in the form of acknowledgments so that those packets when the windows "
  },
  {
    "startTime": "01:00:29",
    "text": "acts when they arrive at the sender they can immediately release the next two data packets and you basically want to have all all four packets worth of budget in flight so that there\u0027s always data going in the transmission direction and always acts flowing in the reverse direction and the only way you can sort of ensure that that happens is if if the receiver is acknowledging every other packet is to have 4x or sorry 4 packets worth of budget in-flight on and yeah so obviously there\u0027s that does increase the aggregate in-flight in these sorts of scenarios but the I think architectural II my sense would be that a better solution is ensuring that the flows tends to run pacing limited because that\u0027s a more complete solution and you\u0027re always going to have there\u0027s no matter what your minimum sea wind is there\u0027s always going to be if you\u0027re running window there\u0027s always going to be some number of flows and some B DPS at which you get into trouble so it seems like a more general fix is to to run pacing limited with a C a minimum C wind that\u0027s robust ok I\u0027m gonna ask folks to be super quick because we are well over time at this point and I\u0027m closing the lines one more question was the target loss rate so currently how do you determine that for various workloads to do pickup constant value or do you tune it based on some parameters right so far we would like well so far we are using a constant value in our deployment but we are leaving the door open to making that something that could be conceivably tuned by deployments for example if you have a background file transfer like a less than best-effort transfer you could that is isolated to its own cue in your quality of service implementation in your switches you might want to tolerate a higher loss rate for example but yeah the we the final loss threshold that we\u0027d like to target for the public Internet sort of yet to be determined and we\u0027re open to ideas from the community about that yeah thank you I came up it when you put up the thing about key performance indicators which was quite a time ago now just that just to say to people go and have a look at there\u0027s a hot Nets paper in on its this year yeah called beyond Jane\u0027s fairness index which is quite interesting sort of thoughts on how to do that being hot Nets obviously it\u0027s not completed work "
  },
  {
    "startTime": "01:03:30",
    "text": "but it\u0027s more about measuring the harm one flow causes others rather than equality and they\u0027ll be on that conversation about packet windows and things like that I\u0027m gonna just forward reference to the slot about TCP Prague Assad\u0027s finished his and defended his thesis in September on that work and the code will be available hopefully before Thursday and well it I mean it\u0027s it\u0027s completed but just chickened out on everything and it I\u0027ll give a link to his thesis and things that we we did manage to get it working with a window based algorithm inside the base TCP stack in Linux quite quite nicely but there are other ways you can think about it so I\u0027d like to hear when we talk about it on Thursday more about it okay tests on that particular paper but you\u0027re gonna have that presentation in Vancouver I think the Hartman speaker initiative I have very simple question so you mentioned about the documenting baby I\u0027ll be too but I might be wrong but my impression is birria bytes consists of lots of small hello complicated Rosic and then some sounds like our implementation tricks and then so I\u0027m wondering how do you are you going to document spec over the b-b-b our beta I yes we as we mentioned with when Martin Duke came to the mic we definitely intend to document the algorithm in an internet draft yeah how you know strike a hospice think something like Fast Pass seems to be an implementation trick from my sure right so that is not something that would necessarily need to be in the draft because as you say that\u0027s an implementation detail I think the much as the you know the traditional TCP fast path is also not in the the RFC 568 Lauren\u0027s back I don\u0027t think that the VR fast path would be in the internet draft but I do think it\u0027s you know we definitely intend to to document the the high level algorithm in the internet draft yeah if the implementation Yoshi we we have to move on I\u0027m sorry Jake we\u0027ve closed I close the lines earlier okay last question if you have you are still in the line before it closed it if you have you have ten seconds okay yeah John\u0027s Norton I\u0027m just wanting to ask whether you cope well with our cc-16 eight style ecn for "
  },
  {
    "startTime": "01:06:30",
    "text": "example if a TC and is not negotiated uh that\u0027s far we\u0027re not planning on integrating bbr with RFC three 168 style ecn we expect that thus far people using VBR would not negotiate three 168 ecn that\u0027s that\u0027s the current plan thank you so much I am going to thank mean for staying up late and doing this and I am hoping that we\u0027ll see the draft sooner than later but we should talk offline about that especially now that we\u0027re talking about adoption of documents in this in his research group and yeah thank you knew thank you I\u0027m going to now handoff to Nathan Gert from Facebook who\u0027s going to talk about their experiences with Coppa hello everyone can you guys hear me should I move closer or further away close is better okay so yeah little bit about myself my name is Nathan GUG as you can see it\u0027s my first ITF here and I\u0027m super excited to present some of the work we did at Facebook comparing Coppa with cubic and VBR for live video upload use-case I work in the videos infrastructure team at Facebook this is the rough outline of my presentation I\u0027ll start with the motivation for our work then I\u0027ll go over the experiment setup we used and the results we saw on the application side then I\u0027ll do a brief overview of Coppa how that works although that is not the focus of this presentation but I think it\u0027s important to go over our understanding of Coppa and how we applied it and then Indian I\u0027ll go over the conclusion and the future work that we apply so motivation in videos infrastructure team we work with a lot of product teams who work on videos and give realized the different type of video experiences require different quality and latency trade-off so on one end of the spectrum you have applications such as video calling where the end-to-end glass to glass latency is super important and these applications are usually willing to tolerate some quality loss in order to get that desired latency on the other end of the spectrum you have applications which don\u0027t care as much about latency but they require extremely high quality playback and then there are applications which falling between which are kind of like yeah we need low latency but we also need high quality playback so normally you would need different type of condition control algorithms compatible with different trade of scenarios what we wanted to find out was is it possible for a single congestion control algorithm to offer a dial to the application and the application could use that dial to just make it compatible "
  },
  {
    "startTime": "01:09:30",
    "text": "with the desired quality versus latency trade-off this is where we evaluated Coppa which is a delay based condition control algorithm it has a dial which is in the form of parameter called Delta which you can use to control the delay sensitivity of the algorithm so in our experiment setup we wanted to start with something so we started with the big one extreme where we are optimizing for throughput at the expense of delay so we tune Copa to optimize for throughput and we did some comparisons with bbr and cubic which were which are two popular algorithms available and a for testing we use the Facebook live streaming application so in this application any user with their mobile phone they can go live they can broadcast their live feed to their friends and followers and one thing to note here is that like this is a very this is very different scenario as compared to HTTP style traffic HTTP traffic would be short and burst years well could be app limited a lot of times as well in this case we have long-running flows over a single quick connection no multiplexing with the mean duration of around 3 minutes so you always have a constant flow of data like which you are pushing with the wire other a different thing about this application is there is an adaptive bit rate algorithm running in the application which would change the encoded bit rate in response to the network conditions such an example if it sees that the network queues are building up and the throughput or the goodput it\u0027s seeing is lower then it could change the encoder to produce less bytes and hence that which that will result in less bytes being written on the wire and we implemented a coupon in the Facebook quick library like quick because the transport is in user space it allowed us to implement Coppa quickly and experiment with it quickly and we use the bbr in cubic implementations which were already implemented and deployed for several other use cases with quick so for conducting the experiment we used facebook\u0027s eb testing framework so this framework allowed us to conduct this experiment all over the world this framework helped us divide users into three random groups a one random group got a copa the other group got a b BR and the third group got a cubic we conducted this experiment for three two weeks and we collected roughly four million samples in each group so before diving into the results I want to spend a little bit of time explaining the metrics and how how we define them so we mainly focused on two metrics on the application side to evaluate the condition general algorithm the one metric is average good word so the way we define it as as you guys might already know number of application by its sixth sense successfully over time so because they do not cast could be long-running we calculated the total number of audio and video bytes that were sent during the entire cast and then we divided that by the duration of the broadcast in seconds that gives us "
  },
  {
    "startTime": "01:12:31",
    "text": "average code word the second metric we used is average application observed oddity this is our proxy for video ingest latency to explain how this is calculated the there\u0027s a diagram here to calculate this the live streaming application inserts a pink frame with basically no payload just a bunch of headers in between the audio and video data now a dark pink frame travels with the rest of the audio and video data through the transport send buffer through the kernel through the same network through the proxies and it reaches the Facebook live server which is processing the audio and video data then when the server receives the ping frame it immediately responds back with an acknowledgement frame for that ping frame which goes all the way back to the live streaming application now the application uses a timing of sending the frame ping frame and getting the ACK it calculates an RTT which is its picture of what is the round-trip time for the application so the application does this measurement every second and it gets roughly one sample every second and to calculate average application observed RTD we take the average of all of these samples now let\u0027s look at the results clarification question method so these are basically measured on the upload this is site yeah so looking at the results for goodput we saw that both coppa and VB are provided better goodput as compared to cubic however we saw that coppa provided much better good word as compared to BB re not to be specific of the p50 good port was increased by 5% for bbr and it increased by 16% for Coppa and similar trend followed even if you look at the look the verse connections which have a lower quality and as you can see like at p90 it maxes out because our encoded bit rate is capped at 3 Mbps because that\u0027s pretty much like a reasonable max quality that we want to have from from our our users and this this improvement was massive for us and we also saw positive impact on some of the top line video metrics that we observe which tell us how much people are watching those videos and how much people are engaging with those videos the second metric we used is the video ingest latency or application observe data team in this a metric what we saw was a bbr was able to improve it slightly more for the best connections so as you can see in the left graph bbr reduced it the bbr a bar length is smaller for p50 BBI reduced it by 8% Coppa reduced it by 4% but but those connections already had like quite low RTD and the reduction was not as visible for our users if you look at the verse connections P 75 P 90 and above are Coppa reductions were highest for P 90 Copa reduced the application "
  },
  {
    "startTime": "01:15:31",
    "text": "ITT by 27 percent whereas BB our application RTT was same as cubic in our tests um so this told us so in the previous slide you saw that Coppa had better good put so that meant that Coppa was able to help like it helped transport to send more bytes successfully over the wire and it also impacted the application ABR to produce more bytes in response to a better better transport but it was able to do it at the same time as keeping the latency slow especially for the users like that had high latencies and where it mattered the most so to be honest like this result was quite surprising for us we did not expect both goodput and latency to be better so we spent some time trying to under and more like why that might be happening so in in the process we found some interesting observations on the transport side but before going into those I want to spend some time explaining how Cooper works I\u0027m sure a lot of you are already already now aware but I want to explain like how we applied it and how we understood it so it\u0027s a tunable delay based congestion control algorithm the tunability kappahd comes from the delta parameter which varies from 0 to 1 the closer it is to 0 the more the algorithm will optimize for throughput the closer it is to 1 the more it will optimize for delay and it\u0027s a delay based algorithm which means it uses RTT variation and signal of congestion and not loss or something else in order to do that it maintains two key variables one is RT t minimum which is the minimum RTD the flow has observed over a period of 10 seconds it\u0027s its estimate of the two-way propagation delay which is a property of the network path the second metric the second variable it maintains is RTD standing this is a minimum RT t observed over a much smaller period s RT T by 2 to be precise this is a copepod estimate of what is the current round-trip time including any queuing delays in deep water like links so the reason it does it chooses RT t by 2 is to make sure like the measurement is correct if there are some jitters or there are this compression on the on the transport which might mess up which might make the algorithm think that there is queueing even when there is not then it uses these two to calculate a queuing delay which is RTD standing - Artie T minimum so on every ACK the algorithm the main controller calculates the queuing delay and then it calculates a target rate using this formula the formula is 1 divided by delta x queuing delay i am not going to go in details of this there is a long mathematical justification given in the paper they show that like you can critically reach Nash equilibrium if you have certain modeling assumptions on the packet arrival pattern so please do read more about it if you are interested and then it compares the target rate with the current rate and accordingly it adjusts the condition window to what said by using an additive increase additive decrease a variant "
  },
  {
    "startTime": "01:18:31",
    "text": "so it moves it by a V divided by Delta times condition window so condition window factor in the denominator helps ensure that the change in one RT t is going to be at max V divided by Delta packets V here is a velocity parameter it is 1 by default and this a parameter comes in handy when Co pi detects that it has been trying to converge towards a target rate for far too long and it needs to do a better job at converging at it faster in that case it starts doubling the velocity parameter I think every RTT the other unique thing or different thing about this algorithm is that it has a competitive mode normally deliveries condition control algorithms we lose too will lose in the presence of buffer filling flows in the owner in the battle in the bottleneck link and because if the buffers are filling up for no fault of the deliberate sender then the deliberate sender will think that this congestion or it\u0027s the delays are building up and it\u0027s going to back off before the lost beast flows are going to back off hence it loses some of the throughput coppa gets around this problem by using something called a competitive mode so in competitive mode it uses a heuristic which I will explain a little bit more in the next slide to detect the presence of buffer filling flows and if it detects a buffer filling flow then it adjust the Delta to be more aggressive basically optimize more for throughput and perform better in the as compared to fulfilling flows however in our experimentation we did not implement competitive mode we wanted our experiment to be like smaller in scope so we just tested like what will happen if you use aggressive value of Delta without using competitive mode so this is the steady state dynamics of cope algorithm this is what the bottleneck you looks like in steady state so the bottleneck you goes oscillates between having zero packets up to 2.5 times Delta inverts packets and this the entire pattern repeats itself every 5 RT so what I was saying before was that the competitive mode and computing flow detection so Coppa actually exploits this property that the queue is going to empty every five RTD and if it\u0027s not doing that then it it thinks that there is a buffer filling flow which is preventing this from happening so hence it concludes that there is a buffer filling flowing in sharing the bottleneck link so here like if you see let\u0027s say at T equal to zero the queue starts to increase for the first time so it will take about half our TT for the RTD standing measurement to be available and the target rate to reflect this change and the target rate will be lower than the current rate at that time because of the inherent RT t delay in the network it will take one oddity for that change to be effective basically the change of reducing the target rate and then in another oddity "
  },
  {
    "startTime": "01:21:32",
    "text": "the queues will start to drain again and then the entire cycle repeats itself like half our TT for the architects timing measurement to reflect the change and then another oddity for the congestion window increase or the target rate increase to result in the queue length to start increasing in our experiment V and then the results that I\u0027m presenting they used a delta value of point zero four which means roughly twenty-five packets in the equilibrium twenty five packets equilibrium queue length and the maximum queue length is like two point five times twenty five now let\u0027s look at some of the transport level stuff we observed so one metric we looked at is how the how is the transport Arcadia varying so we saw so this is measured as just taking the average of all the quake RTD measurements over the duration of the broadcast so the trend we saw was a very similar kind of similar to application architecture meant bbr for the best connections which already had pretty low RTT bbr was able to reduce it slightly more as compared to a copan cubic however if you look at the tale cases 75 1995 Copa reductions were very higher as compared to VB are so for a p90 for example Copa reduced the RTD by 38 percent whereas bbr reduced it by eight point eight percent so this was pretty good to see and this told us little bit like where the application IDT reductions might be coming from and it also showed us that like it Koopa flow is not just improving the application latency but it\u0027s also improving the RTT for the network so if there are any other flows which are also sharing the bottleneck length like they will also start seeing a lower equities the other metric we looked at is the retransmission overhead so this we defined it as the total number of bytes retransmitted by the transport during the course of the broadcast divided by total number of bytes acknowledged during the course of the broadcast I think it\u0027s a pretty important metric to look at because it tells you how efficient the transport is if you are wasting resources and bandwidth rewriting the same bytes over the wire again and again it\u0027s just not good for anyone so what we found here was that for 90% of users coppa retransmission overhead was available as compared to both be brn cubic it was about half of what bbr had and around 1/4 of what sorry cubic hard so this gave us an idea like why application goodput might have been better like this might be one of the reasons why we were able to send mode by it successfully for the application however for the last 10% of users we saw a different trend that coppa retransmission overhead grew very rapidly and it became three to four times as compared to cope and cubic and VBR which was concerning as well as surprising because we did not see a corresponding proportional degradation in the application metrics for any other tail users so we spend some time debugging into it and the first thing we "
  },
  {
    "startTime": "01:24:32",
    "text": "did was we sample a few cases some some broadcasts which had a very high loss rate for coppa so we notice two things one thing was that these flows had very constant or throughput as you can see by the red line here the number of bytes act over time it\u0027s it\u0027s a it\u0027s a straight line and the second thing you observe is that these flows actually have very low arteries so in this sample except the one spike the RTD stays constant around 75 milliseconds so this did not look like congestion or loss is happening because of buffers filling up this look more like network policing to us because if there\u0027s a token bucket a police are in effect which is identifying your traffic and and limiting it as a bit at a bit rate it\u0027s not going to result in any increase in queuing delays or equities but it will look more like this we also grouped our retransmission overhead numbers by ESN and we found that they do vary greatly depending on which s and you\u0027re looking at some essence which are known to police Facebook traffic they had very higher retransmission overhead for tail cases for Koopa and there was some other a essence where the retransmission overheads look very similar so to generalize our finding we did some more aggregated analysis we looked at the relationship between artery transmission overhead and our TT queuing delays and we found that for cubic they are highly correlated as the retransmission overhead increases our TT and queuing delays also increase for cubic which indicates that all the losses are happening because of congestion instead for but for Coppa we saw similar trend for the first part of the graph but later on we see that the retransmission as the retransmission overhead increases the arc titties and queuing delays actually start to come down so this kind of solidified our hypothesis that this net the network policing is a big factor being playing a role here in the high retransmissions although it\u0027s also possible that there are some other reasons at play here and network policing is not the only one for example short buffers if there\u0027s a short buffer and Coppa sender is trying to maintain an equilibrium queue length of 25 packets then the buffer is smaller than that then like definitely going to run into losses so definitely there are improvements are possible in Coppa to handle this case better competitive mode as I described before could help because it could adjust the target Delta based on whether it sees loss or success we could also add a heuristic to change congestion window based on something simple like multiplicative decrease based on target loss just like cubic or Renault or even BB are now is doing other option is to have an explicit network policy detection similar to how BB R by V one had it so in conclusion the aggregated results showed us that Coppa provided better and lower latencies in artists these tests were for mobile broadcasts for uploads and we compared with cubic and "
  },
  {
    "startTime": "01:27:32",
    "text": "VBR with quake but one thing to note is VB are like via tuning VB are in our internal implementation and also like Beiber veto is happening so the results may differ in future one of the big future work items is a better understanding of the reasons behind these improvements could are these improvements because of lower retransmission overhead or is it because Kupa is just doing better target rate estimation by sending fewer packets and it\u0027s converging to that faster or it could be something else in the beginning of presentation I spoke about our goal of having a tunable delay based condition enroll algorithm which could be compatible with all the video experiences and so far we only ran experiments on one extreme where we are optimizing the throughput we would also like to run some experiments with other with goobie condition control as a comparison where the end-to-end ultra low latency super important and we would also like to test for more use cases like playback and all the traffic this is these are a few links where you can read and learn more my email is here and I\u0027m happy to answer any questions now or offline thank you so much nothing I\u0027ll intake myself in the front of the queue so I\u0027m asking this question from the floor basically the scenario that using this under right now is when the client is brought is uploading video up to the server and this is almost a textbook example of a situation where there is no cross traffic or there\u0027s no other traffic that is going to be competing with Copa at a bottleneck because I imagine that when people are uploading video they\u0027re not doing anything else on the phone so it seems to make sense that the that even without the competitive mode which I like to think of as TCP friendly mode because that\u0027s what it is no that\u0027s what cubicles it but we don\u0027t get there the the point is that that\u0027s that\u0027s simply a it makes sense that it works without that so I\u0027d be interested in seeing more evaluation on on what happens when you when you have competition with cubic with PBR with other things in the network because that\u0027s going to be useful for downloading download streams and in the common case that\u0027s gonna be an important factor definitely like that\u0027s like I said that\u0027s one of the future work items although then to be honest when we started working on this like we did not really know that there is going to be no competing traffic and like even now like I\u0027m not totally sure like if there is any evidence like a real life evidence of that I would love to read more about it I know from textbook that it is the case but like yeah this is one of the future work items thank you Michael Mike reverse begin by saying that I greatly appreciate Facebook coming here and bringing this here and some some good results and I guess the mechanism "
  },
  {
    "startTime": "01:30:33",
    "text": "is not altogether unreasonable I have to say sorry for that I have to say that the base idea or that this you know it\u0027s based upon of a delta that that reflects a trade-off between having high delay and good throughput or low delay and not so much support is fundamentally wrong just because well I mean if you look at the Intuit the base intuition between mechanisms like bbr or alpha s or things like that you know they aim at having high throughput and having a low q and having a large q just isn\u0027t good for anybody I think there is such a trade-off when you\u0027re trying to compete with TCP so there is you know you had this in your in your competitive mode you can try to be a bit more or less aggressive which will produce more or less queuing so I guess there you have to trade off but whenever you are able to detect that there isn\u0027t only any other traffic competing with you I don\u0027t think the idea of have of including such a trade-off in the design as a base idea is good at all so my answer to that is like maybe you\u0027re right like I don\u0027t know for sure but the trade-off does exist on the product side because on the other hand we have video calling applications which have not been able to use cubic or bbr so far as far as I know they have to use something like Google condition control where they have to really optimize for end to end delays right on the transport side like we did run some limited experiments by using like a very high value of Delta right and I did see that queuing delays decreased even more so I mean I don\u0027t know like fundamentally if like it\u0027s a good idea or not but from the application point of view this definitely a trade-off and if the condition control could also kind of like give you a varying performance depending on like what like it could also be expressed in a different way for example prop 8 exposes a target delay to the application and it will just try to optimize and limit the queue the bottleneck you delay it to that like that could be an alternative way for application to benefit from it like I honestly don\u0027t know I can maybe point out some other work offline yeah Stuart Cheshire from Apple you presented some results showing marginally better application throughput which is interesting I think it would be interesting to understand where that\u0027s coming from because either the other congestion control algorithms were letting the line go idle and wasting capacity or they were retransmitting unnecessary and wasting capacity with duplicates or something else and it would be really interesting to know where that extra is coming from where these tests done on "
  },
  {
    "startTime": "01:33:39",
    "text": "Wi-Fi links or LTE good questions so we ran these tests globally and when I looked at the data like about 3/4 of the users were using cellular I did not I don\u0027t know like LTE versus 3G or something else and about 1/4 were using Wi-Fi but in the Wi-Fi case there would be competing traffic at the bottleneck because it would be probably the first or probably and when I looked at the results and compared them like the trend was similar for Wi-Fi and cellular although we saw more wins on cellular as compared to Wi-Fi and what is the Delta value for these experiments no 2.04 Thank You Roberta they own I just wanted to push back on the labor sis bandwidth not making sense I also want to agree with delay versus bandwidth not making sense I\u0027m holding both of these opinions simultaneously because it\u0027s hard to define what the network is does the network diff\u0027ent is the boundary of the network between the application and the operating system or is it between the nic sending packets and the fabric to another NIC right the you know depending on where we are in the stack we tend to think of either of these as the network and try to exclude one of the others generally the internal to the host part so we know from previous presentation by Neil Cardwell for instance that there are parts of the stack that are trying to reduce the amount of overhead that we are experiencing within the host and the effort by which we do this and of course there\u0027s Nagle\u0027s etc right the effort by which we do this is really a trade-off of how expensive it is in CPU versus how important it is to get the information through so there you go sorry thank you thank you so much folks and I want to thank mitten for showing up here and presenting this please make him feel welcome it\u0027s his first IETF /i r TF / ICC RG meeting and for everybody else watching I love to see more presentations like this if you are doing in experiments in your house or in your lab or at your company come share them with us and we now move on to Marcus and get this up mc/dc CP there you go take it away my name is Marcus Armand from Deutsche Telekom and today I want to present some joint work with Karcher at University and City University London on multipass multi-party ccp that we intend to to use "
  },
  {
    "startTime": "01:36:42",
    "text": "for providing multiple capabilities for UDP and IP traffic why I want to presented today here at ICC RG is that we see a challenge there in respect to congestion control and I think you you are the right group to become aware of this and maybe work together with us on a solution B and we also think that this is not a limited issue to the multipass tcp maybe that is also something which affects in future the community we come to this during the presentation first of all I want to give you a short introduction why are we working on multi pass DCP what is the reason behind I want to make you aware about the development we did so far and then coming quickly to the point where we see the issue of congestion control the multipath TCP as such is usually pushed at TS vwg where we also have some updates in the afternoon session okay starting with them motivation why are we working on on a solution for providing multiple capabilities to two UDP or IP traffic so as you may know there are specifications ongoing in the 3gpp area and also in the BBF proppant forum on multi connectivity architectures to provide multi-part usage to either mobile phones though that is on the upper part that is directly happening at 3gpp released 16 so within the 5g standardization and they\u0027re discussed in in the area of 80 s SS XS traffic steering switching splitting it\u0027s an operator controlled approach so that\u0027s why we are so telecom are very interested in that that means multi connectivity is terminated in the operator network the same is also happening for the hybrid access use case where multi connectivity is applied between a multi path capable CPE using cellular and fixed access and again the operator network in future the IP Texas will most totally also rely on the 3gpp a TSSs specification and currently in release sixteen multiple CCP is a protocol which is defined for making simultaneous use of multiple X\u0027s if you start now with having a look on "
  },
  {
    "startTime": "01:39:42",
    "text": "how the traffic mix has changed over the last years I I think I don\u0027t tell tell you something you we see that the quick traffic is coming up more and more though that is some statistic from our fixed network in 2018 where we already see more or less 12% of quick traffic and we think with HTTP 3 deployment this will become more and more and maybe it comes to the point where Creek or UDP traffic in future dominate the TCP so that is totally different to what we have seen maybe three four or five years ago where TCP where the dominating protocol with almost 100 percent with all almost 100 percent share but coming now back to to these multi connectivity architectures we have seen before within the a TSSs we\u0027re multipass TCP the protocol for providing multipath transmission this means that it cannot cover anymore all the traffic which is today generated between customers and and the internet the Gothic week is coming so our finding so far as we have a long lasting experience was not a past TCP it\u0027s a very well working protocol it\u0027s efficient it\u0027s a good candidate to use or to provide yeah multi connectivity services and there\u0027s also finding that the congestion control should buy TCP and also used with a multi pass TCP for scheduling decisions and so on it\u0027s very beneficial another finding is so far if you look at IKEA for example there is no multi part of a call for UDP or IP traffic and an encapsulation into multiple TCP of such protocols it\u0027s not an option at least that is our opinion and that was also something discussed years ago in the multiple TCP mailing list that this does not make sense because if you encapsulate UDP or IP traffic into multiple TCP then you impose the reliable nature of TCP so a potential multipath solution for a UDP or even IP traffic must not impose TCP like reliability additional high latency packets cramping or head-of-line blocking otherwise it Prague the UDP and IP principles on transportation and service expectations before I come now to the solution and talk about the "
  },
  {
    "startTime": "01:42:42",
    "text": "multipath DCP itself I want to present what what are the key components for a multipath transmission sure for sure you have a generator and a receiver part in between you have multiple paths or multiple X so at least more than one path to provide multi-multi connectivity and you you need a you need a scheduling unit to distribute traffic over a multiple part and it is very beneficial to have a path estimation for the scheduling unit which gives you information about the path characteristic latency is data rate available capacities so that the sketching unit can make proper decisions efficient decisions and that\u0027s not overload the path may be according to two policies preferring paths with low latencies and it needs this information which which latencies are applied to the individual path and also in most scenarios and real scenarios you need some we or a queue to compensate and the different paths or icterus takes to compensate the latency differences and so on for the three ordering unit or poor for making this reordering working for sure you need also some some sequencing but that is that is very simple as such so the the the most critical components I see are the scheduling the path estimation in the reordering so all of this has worked together and to make efficient multi-party which possible now coming to the solution multipass DCP so at the ITF which which types of protocols are available which can support us in finding a solution to transport UDP IP traffic over multiple paths and there we came up with the DCP protocol which is a mix between TCP and UDP it\u0027s it\u0027s of our unreliable nature but it has a congestion control and that we think we can exploit for our purpose purposes so what we imagine is we have the multipath DC CP as such which by itself is only possible to to distribute a TCP traffic over multiple paths but if you combine it with a multipath framework which more or less means we have virtual network interfaces which can consume any kind of IP traffic "
  },
  {
    "startTime": "01:45:43",
    "text": "and the the traffic which is is incoming into this virtual network interface then encapsulated into the multi pass TCP distributed and then on receiver side again may be reordered and then stripped all the DCP information and send out of the outgoing virtual network interface so that is a rough description of how we imagining a solution you can you can find more details in all the draft I have mentioned here on this slide and that is our prototype and also our simulation test that we have available so we have a Linux kernel implementation extending the the available DCC keep of a call by multi part capability and the same we have available in nf3 for simulation purposes and it\u0027s the descender side you can see on the on the left side the multiple CCP home gateway as we have called it and that is very similar to to multi pass tcp we have a modular scapula which can load different kind of scheduling algorithms round-robin for example a cheapest pipe first which is a path prioritization scheduler but also more sophisticated ODS for example out of order for inorder arrival schedule on receiver side and that is something you compare to to multiple CCP we have reordering modules so we can apply different kind of reordering mechanisms we have a passive one which more or less does nothing we have a adaptive fixed algorithm which waits for a fixed time when when packets are missed and then skips possible gaps in the reordering queue we have a depth effective so that is a dynamic approach using the RTT informality the latency information from the congestion control put a ten-time and last but not least we have delay equalization which is more or less no reordering approach it just tries to delay the packet on the faster path by the latency difference we have a path manager we have this virtual network interfaces from the multi path framework implemented so it\u0027s more or less complete a set up which we can use for testing presenting now some results in "
  },
  {
    "startTime": "01:48:47",
    "text": "short that is really a short sort of results just to make you aware what is already working and then coming to the challenge we seen so that is that is our na 3 setup where you see that we have plenty of possibilities we can simulate different you ease we we can simulate you know tea or cellular axis we can simulate Wi-Fi access and also and we can simulate moving scenarios where we move you ease from one point to the other which maybe lead to the effect and therefore I have results in the next slide that we lose connectivity or that we enter you access nodes and we can get connectivity so that that our results from last ITF but again just to remember you so switching and aggregation is it\u0027s possible with our multi-party ccp setup so on the top you see when we move and you e out of a Wi-Fi access so that the the Wi-Fi access is not available anymore then the ue automatically connects to the LTE and the multi-party Suzuki is capable of doing this in a in a seamless way and with the fewest switching approach you see between switching from by factory suddenly you have a get with aggregation mode which was also supported by multiple CCP you see a more or less as seamless hand over without having a gap between switching from Wi-Fi to LTE we think that is a very beneficial scenario the same test we also did and MP think that is very realistic maybe I have to switch back so that what was pure UDP traffic it was some some iperf UDP traffic we generated but that is not very realistic so we did the same test with UDP combined with Nara congestion control most of the UDP traffic we see in the field is to some extent congestion controlled and if you talk about quick then it\u0027s it\u0027s pretty clear that this is congestion control and you see here in the same scenario you\u0027re switching or aggregation it also works to some extent but you see some "
  },
  {
    "startTime": "01:51:51",
    "text": "some interact between the Nara congestion control and also the D CCP congestion control you have this black reference line so that is from the previous slide the results where you have seen more or less yeah maybe where you have seen that after switching from one access to the other a direct use of the full capacity was possible whereas whereas now with the nadir controlled UDP traffic you see it takes on time to to adapt this one I I will skip for time reasons and that is now also exploring a little bit what\u0027s going on in in terms of delay that is also some result from from last our ITF where will be proved that reordering makes sense we need reordering in a in a multi pass set up supporting UDP traffic and the cream bar here that is proving that reordering is beneficial over scenarios without reordering and you can find the details in the slides from from last ITF what is new we have some real world results with YouTube traffic based on the call now and you see here the set up we have the multi Party CCP client and multi pass TCP server that is a local set up and but connected to the internet connected to the multiple TCP client we have a laptop running the Chrome browser and between the both multiple TCP boxes we have two links to ethernet links going through so-called TC boxes but which we can shape the links and where we can change the path characteristics as I said we use the Chrome browser with that we are able to request YouTube videos using the quick protocol we requested a static video with a fixed resolution and we skipped this video at ten thirty fifty 70 90 and 110 seconds and to force a buffering the total curation of the test were 120 seconds and the network condition chained at 60 seconds we see on the next "
  },
  {
    "startTime": "01:54:51",
    "text": "slide again so we had to part both at one megabit per second and a latency of 10 on the first path and 50 milliseconds on on the second path of the 60 second be changed from 10 to 90 milliseconds on the first path and on the right side view you can see the playback ratio but is a data rate where you can see that the green circles that are the one using multipath in combination with reordering promise promises the highest gain whereas other scenarios like single pass usage or Multi multi Party CCP without reordering ya have have some have some lower playback ratio compared to this multi-party which with with reordering however that is not part of this slide the detailed evaluation revealed and the imperfect parts use it even in the best performing use case which means even if you have reordering and multi-part usage combined there are still some room to improve the customer experience and now coming to an intermediate conclusion before coming later to the final conclusion so our multiple CCCP prototype and also the the the standards or the the troughs we have available at TS vwg making very clear that this is something which can brought a work scheduling profits from the TCP congestion control and can therefore perform similar to multipath TCP the reordering on receiver side is is proof mandatory for efficient multipath transmission of unreliable traffic smart algorithm to keep a traffic flow smoothly ongoing are required and can make use of congestion control information head-of-line blocking is not purposeful and should always assess if it is worth to wait for missing out of other information which means encapsulation into multiple TCP doesn\u0027t make sense for for UDP or IP traffic you have also seen that we have plenty of scheduling and and reordering algorithms in our prototype and all of this together benefits from from the "
  },
  {
    "startTime": "01:57:51",
    "text": "congestion control the ship with TCP so from this perspective it was a very clever decision to use the TCP and also with the congestion control we can make sure that we do not overload paths though that we do not force a packet loss however the main target protocols transmitted over the multipath TCP framework like quick employee and own congestion control in combination with multiple TCP this leads to a kind of congestion control over congestion control scenario with well known issues from literature so now the point where I think it becomes very interesting for ICC at ICC Archie so in all of our tests we used so far the CCI d2 which is a TCP like a contest in control and we asked ourselves if it makes sense to use CC ID - what could be the alternative and how intra neural we tackle the problem of congestion control over congestion control and such scenarios if we assume that combustion control for multipath purposes with the right way to go okay so as I said we have this issue of congestion control congestion control identified we think also that affect the the community so there are troughs regarding quick tunneling where at the end you you have the same question depending on which type of traffic you want to tunnel through through Creek my main question today is ICC actually the right place and dealing with this question of congestion control over congestion control and who is interested in elaborating this topic further not dedicated to this multi parties yeah I think it\u0027s a general question but for sure it would help us in pushing the multi processes EP further thank you very pleasant Dacian Marcus I think just very quickly to answer the question about is this ICC has the right place it can be is the Shorin said that I have I think the question I would have is other other folks who are interested in in this problem and I\u0027m not just speaking academically is this a real problem folks interested in deploying our face are they facing this problem in production that sort of question if you can you know you could you could have this conversation on the mailing list and see there are people who are interested in working with you on on establishing relevance any other questions we have - Anita a "
  },
  {
    "startTime": "02:00:54",
    "text": "quick question have you ever experiment with that pass is not disjoint no for example with some savvy multipass DCC subfloors share the same bottleneck have you ever experiment with that kind of environment the question was will be experimented with couple congestion control like multiple TCP uses no that is on our to do this but I\u0027m not sure at the end it also ends up in congestion control over congestion control for sure we have to do this we should do the such test what we have done but what was not able to to show up that we have implemented PBR 40 CCP and we did some tests with that that was very promising but copy congestion control is on our to-do list we have the issue that we have to report everything into the DCP world to make such tests I think that\u0027s important make sure it\u0027s on our to-do list but again the problem stays at the end then we have inner congestion control of the piggyback traffic then over couple congestion control where also think that this end ends up in some challenges or conflicts thank you so much Marcus and with that we end the session I\u0027m going to quickly note that in Vancouver I\u0027m trying to get together there\u0027s been a recent spate of academic work on EPR analysis so I\u0027m gonna try and get those authors and even need to show up or BBF folks to show up in Vancouver so I\u0027m hoping to do two ICC re sessions in Vancouver four of those panel this is tentative of course one of them will be around bbr specifically and the other one will be more generic but if you have any work around that if you have any analysis that you\u0027ve been doing with DVR or vrv - please write to me I\u0027d love to have that be a part of the program and that we will end thank you so much specifically tank Marcus for winding down his kind of his presentation quickly and we\u0027ll see you in Vancouver thank you folks "
  }
]