[
  {
    "startTime": "00:00:20",
    "text": "Okay. We can go ahead and get started in just a minute. It's getting that folks frequent Folks are fill filtering in the remote chairs asked me to stare and make eye contact into the crowd. DKG is taking notes remotely and he couldn't use some help. So if someone could be a backup minute taker, that would be very excellent. With It it and there's a a prize of a button Thank you. Can you identify yourself? Thank you, Devin. Thanks, Evan. Go get you your button. And someone please get a button DKG as well. Thanks. Okay?"
  },
  {
    "startTime": "00:02:09",
    "text": "Sir, administratively, before we kick off, I just as, this is Roman Daniel. I'm the cognizant AD of this for a couple more hours. I wanted to note that you see a some changes in who are the working group chairs. So I wanted to give a big shout out and thank you to Ori who helped who helped us kick off this working group and I wanted to say a big thank you to you, Praachi. You're way ahead of me. Thank you so much, Prashy, for be willing to serve here in the working group. And Shivan, thanks for being the steady hand for all of this. Of course. Thanks a lot. Absolutely. Thank you so much. And congratulations. Or if I don't know if you're in the room with on your new role, and I wanted to also administratively announce the good news for the working group Deb Cooley is gonna be the responsible AD. So you are in much better hands than you are with me. She knows so much more about all of this all of this technology, sir. She's gonna be great. Yeah. On that note, I think we can begin. Y'all, welcome to Keytrance. Like Robin mentioned, your co chairs are joining you remotely. Thank and thanks, Roman, for helping us get things started. And, yeah, Prashi is a new co share. And, thanks to her and thanks again to Ori. For his help getting this group off the ground. And good on the 0. luck And, yeah, moving on to the note bell. Please go through this. It's important. If a reminder about IETF Policies on various topics as patents or code of conduct, and, applies to both in person and remote participants. So please do give it a read. And also just a friendly reminder that if you're using to please use please use the meet echo app for joining the MyQUE, especially it helps with team management, and is fair to"
  },
  {
    "startTime": "00:04:00",
    "text": "mode attendees and also just, it's a high hybrid model, like, It is. It just makes it a lot Sayner, manage And Yeah. Moving on. Our agenda today is pretty simple. We have 1 adopt adopted working group document, which is the key trans architecture document. And that'll be the focus of our discussion today. And Brendan, will be talking about that for the next little while. I'll go ahead and share the slides and, you'll be presenting. Cool. Hello, everyone. So this is progress on draft at ATF Keytrains Architecture. Next slide. And there's the cool thing is that this is a draft IETF because it was adopted. That happened right at the beginning of the year. So Thank you sincerely to everybody who helped with that. I'm incredibly grateful to you. Next slide. So the feedback that we got prior to adoption was not so much that anything was wrong with the draft. Just the draft was lacking in quite a lot of content that people wanted to see. So one big question from from before, in IETF 118. Was was was was how would seal cinder work? So that's seal cinder is basically an anonymous type of into an encryption we didn't describe how KT would work at all in that context. We also didn't say anything about Federation if you have, like, several key T servers that wanted to work together, we had no recommendations for that. Life cycle management is also very important. We didn't say anything about If you ever create a new log, how do you spin down the old log and then move to the new log? And also if you run a log for a very long time, how do you deal with the the issues that come with with with having a log, which is extremely long lived. And then the last two are are frankly the hardest compliance with privacy laws, so GDPR,"
  },
  {
    "startTime": "00:06:02",
    "text": "GDPR, in particular, has a a provision where service providers can be required to delete user data. And QT is a transparency log. So it's not immediately super compatible with the idea of deleting the data that's in it, and also there were some disagreements among participants about the exact privacy properties that, we provide to users and also that we provide in terms of what is leaked to different parties in the protocol. Next slide. So the changes since IETF 118 is added content about all of this, which is wonderful. I'm pretty sure we have consents on all of it, but I'm going to go through all of the changes, that we did for each of these points today. So if there are any comments or questions, but you should then please interrupt me as I go because kind of just a bunch of little points. You don't have to wait till the end. Next slide. So in terms of the first one, which is a sealed sender or anonymous communication, next slide. I thought it would be helpful to initially give a bit of background on how synonymous into an encryption works in the first place because it is kind of, weird and also kind of unusual The way that you start is that the center has to get the credential of the receiver they wanna talk to this first step is actually kind of the hardest part because Getting the credential from the server, you wanna authenticate the request before you give someone a user's credential, because you need to You wanna authenticate that endpoint to prevent abuse. In particular, you wanna prevent people from scraping all of your users. That would be a pretty bad privacy property. But you have to authenticate it in a way that is anonymous. Because if it's not anonymous,"
  },
  {
    "startTime": "00:08:01",
    "text": "then the service provider sees, oh, the sender wants to talk to this receiver. I can that they're talking to each other. So it's not an honest anymore. It doesn't really work. Signal is probably the best example of an application that actually does this. And what they do is they have an SGX enclave, that you can encrypt your Contact list to and this SGX enclave. To has a little list intersection with signals user base. And you use the the remote attestation feature of SGX to guarantee that the code running monthly, it actually does just that and then delete your contact list. And so have kind of this train of custody where you know your contact list is use for this one thing and then it's thrown away. So that's really important. So that's how you get the the credential, the the receiver that you wanna talk to in the face next slide. Once you have that credential, you can encrypt your message to the receiver. Part, part other thing that encrypts you encrypted the receiver is your own your own credential. You you encrypt your own credential combined with message that you wanna send, and then you can authenticate that message with signature or something else, and you can send that to the server over a much cheaper, anonymous channel in particular, like, tour or something. If you think about this from the server's perspective with the server sees as it gets a little encrypted envelope that is addressed to a specific receiver like Alice, and there is no There's no sender information because it came over to her. There's nothing in the envelope that identifies it. So that's all the server sees is I guess this envelopes just to a user. Which it will give to that user when it when they come online next slide. So when that user does come online, they can download all of their envelopes. They can remove it from the server when they decrypted with credential, they get the sender's credential, and also they get the message from the sender. They can authenticate with, the credential, which they received. Notably, you don't need an an anonymous channel to to download your own messages because there's not really any value to that. The the server already knows who you are, but the fact that you're getting the messages for that person next slide."
  },
  {
    "startTime": "00:10:02",
    "text": "Actually, what's cool is after the sort of initial handshake where I get your credential over some weird, expensive channel, and then you get my credential because I encrypted send it to you. After that, we both have each other's credential. So it gets a lot more efficient. I can just encrypt messages to you. I can send them to the server over in anonymous channel like tour, the server, again, doesn't see who it was from. It just sees that it's to this person, can download it, and then we can talk back and forth, basically as efficiently as as normal into an encryption. So, yeah, next slide. There is there is one slight problem with adjusting KT to work in this environment, And that's the idea of having credentials, It's actually when we did the BAF for KT, the very first question that we got from someone was from from Richard Barnes. To picked up on a word, where I was describing KT as a way to distribute publicies, and he immediately pointed out I don't wanna disturb you publicly because I want to authenticate them. At the time, I kind of didn't understand the difference, but this is actually the difference. So the problem here is that KT is an interactive protocol. Where you have to interact with the server to look up the specific data that you want. And that doesn't really work in the context of our credential, of a credential because credentials are supposed to be provided out of band and then can be verified in a noninteractive way. But the way the solution that we have for this is that we can basically turned users into and anonymizing proxy for looking up their own keys. So the way this would work is if Alice wants to join some group anonymous thing. She can look up her own public key and the transparency log, then she can take the the transcript of that interaction and serialize it and then send it to the anonymous group. So everybody in the group can see that transcript. They can evaluate it themselves. They can verify the correct. They can verify Alice's key is what she says it is."
  },
  {
    "startTime": "00:12:01",
    "text": "And then we can all move on with our business. So essentially saying that serializing a transcript of a an interaction with the KT server, is equivalent to a credential. Although there is one difference normal, like X Five 1 and credentials, between And, and rushing with the KT log, which is that sometimes you look things up in a KT log, it creates what's called a monitoring burden. So the fact that you've looked this up in the log it creates a burden 30 checks to ensure that the law is still behaving, honestly. So if you if you look something up in a log, there's always the risk that, like, did the log could remove that data and pretend it didn't exist. The monitoring, one of the important things it does, is it ensures that once they does add a tool log, it's never removed. Of course, you have the same problem with monitoring. That you do with the engine the initial lookup, which is as the u as some user in an anonymous group, Hello. Got, you know, a credential, credential, from Alice, I can't just go to the transparency log myself and Say, Uh-uh. Can I do the monitoring for this specific user? Because it has the exact same leakage. Where the transparency log knows who I am, I knew who Alice is and the conclusion there is that we're talking to each other. So it's not anonymous anymore. So we have to do is basically the same thing which is Alice will in the future whenever monitoring queries would be needed by their users, can do that herself and she can serialize it and she can send it to the group. Everyone has the data. That they need. Are there any questions there? I can't see the Q There's no one there Okay. Cool. I'm assuming that makes sense to everybody. There's also one extra point, which is If Alice is malicious, she does the initial lookup she provides. That's the group. Butts,"
  },
  {
    "startTime": "00:14:02",
    "text": "later, she disappears and she doesn't wanna provide them monitoring for her own key. Users in the anonymous group can still do it themselves. They just have to do it an anonymous channel like tour. And the reason they can do that is because monitoring doesn't require indication If you wanna look up a user's key, That would reasonably require authentication because there's sensitive user data that you were providing but monitoring doesn't provide any sensitive data, so you don't have to worry about that as much. Yeah. That's how how that works. Next slide. So the the next point that we had to talk about was Federation. So next slide. Generally, federation with KT is pretty simple. You generally just have one rule. Is that you need to ensure that lookup keys are proper are properly namespace. So if you have some set of KT servers that all want to work together What you need to do is ensure that every key is uniquely is uniquely mapped to a single, Kt log, that that key belongs to. And the easy way to do this is just to pretend like a domain key. So you have, like, user at example.com, user example2.com, then you can immediately tell where Each of this keys can immediately tell which log each of those keys is supposed to be routed to. And there's a secondary point, which is If you have a federated set of KT servers and you wanna and one of the service providers in this federation wants to protect the privacy of their users. The recommendation that we would give them is to provide an anonymizing proxy sort of similar to oblivious HTTP. The reason that you would do that as opposed to something like mirror another federation another service provider's log If one of your users wants to do a lookup, for a key,"
  },
  {
    "startTime": "00:16:01",
    "text": "on a different service provider's log, you can be no HTTP proxy for that request so that other federation member, can't, like, learn the IP of your user or something, but it's still a direct request from that user. What you would not want is for one service provider to start surreptitiously surreptitiously storing a bunch of other service providers' users' data in a centralized place because that creates a lot of privacy concerns. It also creates consistency concerns or if you're trying to to answer requests yourself you're not gonna have the most up to date, not the most up to date data to answer those queries. Okay. Next slide. So moving on to to life cycle management. Lifecycle management breaks down breaks down into two parts. The first part is Next slide. So the first part is migration where when you have a new log and you transition from your old log to a new log either because there are cool new algorithms, or you don't like your old log. You don't like the code or whatever. You wanna move to a new the the recommendation that we have for you there is to create your new log to prepopulate it, all of the data from the old log that you wanna move over, and then store the final root cash of the old dog in the new transparency log. And the reason that you would do that is when users come up and they want to start using the new log What they will do is they will look up the final root hash of the old log in the new log And they will complete their monitoring of the old log up to this distinguished root hash. So that essentially says that in the old log, we guarantee consistency all up to this fixed point, and then we sort of can can continue for with the new log. Way that I think of it is kind of like a this and cake eater approach, if you know what that is."
  },
  {
    "startTime": "00:18:02",
    "text": "Where all of the monitoring burden for all of the keys in the old log I'll get transferred to this one key in the new log. And you you can move forward like that. And you your users have a guarantee that data was lost or no inconsistent views were provided to anybody. Provided that the the monitoring for the new key is successful. Next slide. That's migration. The second point is pruning. So if you run a transparency look for a long time, thing about transparency logs is that they kind of increase in size infinitely, usually. The way that applications like certificate transparency deal with this is they will share logs annually. So, basically, each year, you have a new certificate transparency log, to replace the old one, and then you can throw away the old one that necessarily doesn't work as well in a messaging sort of context. Because, it creates a lot of, operational overhead, the idea and is this would ideally be something pretty easy, a pretty a pretty minor infrastructure of a messaging says service. The way that we handle pruning is we separate the data and a transparency log into 2 separate classes. The 1st class is what we call user data. And user data is essentially anything that The application would know what it is. Like, it's, the content of a specific version of a key. And then cryptographic data is everything else. All of the commitment values, all of the commitment openings, all of the intermediate tree hashes, all what we'd call cryptographic data. And so if you have that that separation between Wyatt user data is and what cryptographic data is. We can handle the pruning concerns for those separately. So with user data, If we don't need, some piece of user data, the application can tell us that really easily. We can just delete that. So an application might have a policy that we only ever allow users to look up the most recent version of a key. Which makes sense."
  },
  {
    "startTime": "00:20:03",
    "text": "So the the conclusion there is that all the prior versions of any that are not the most recent, we can all delete those and we can be done with them. And that actually saves a lot of data. So next slide. So what's nice about this is that If we know what user data we don't need anymore, That's actually pretty easy to feed into a key transparency implementation. They can go through and do kind of a a tree pruning and remove all of the cryptographic data. That it doesn't need as well. So here, you can see if we remove these 2 user data blocks at the end. Then the the implementation can go through and see, like, oh, we don't need this whole subject anymore. So we can delete this. And you can think about the sort of logical conclusion, You know, if you have a log, which is running for a really long time, and the data in it has a reasonable degree of turnover then, At some point, all of the data in, like, the left half of the log is not gonna be necessary anymore. So you could just delete the whole, you know, less halfhalfhalfhalf of your transparency tree because nobody's ever gonna go into it again because it only has data too old to actually be accessed. That's a pretty cool Property. That could significantly extend the lifespan of a log? There's a quest there's a question in the chat DKG is asking Are there cadence requirements here for monitors to detect the turnover actually happened turnover for pruning or turnover for I can I can ask the question, try to be tier if that's okay, Are are do you wanna take questions now? Yeah. Absolutely. So, seems to me like once you've deleted that stuff, you're not gonna be able to see So if the a user's key has been published, and then changed And then published again and change back maybe."
  },
  {
    "startTime": "00:22:01",
    "text": "And you deleted stuff because you decided that it's pruneable because we only allow people to look up the current key. Then how am I as a user gonna know that there wasn't some It's Jeff in the old versions that are now deleted. Or will I just know that there's mischief because I know that the date that my key was added is more recent than the date that it should have been even material is the same. And I just don't know what it was in the in the interim. Does that make sense as a question? Yeah. That does make sense. It's hard to answer that question without specific implementation or, like, specific protocol document that we can prove security properties against? But I think the the general, like, architecture document answer to that question is If you still need data for users to complete monitoring a key that they looked up in the past. Then you still need that. So you can't delete it yet. Like, we're not gonna delete data that we will need to answer monitoring queries. Because then we won't be able to answer the monitoring query and users won't know if they're, like, secure or not. Kevin, Hey. Yeah, I guess I'm not it's the same question or kind of like a related question about the printing, but question I have is, so when we have, like, whoever needs to check, like, the append only mess of the tree help with that work with pruning? Like, would there be like, because it seems like if we pruned off an entire subtree, that basically lets us or, like, that seems to, like, buy or get penalone properties. I don't know. Metery, Alright. Haven't haven't a design online for that, but Yeah. If you can So specifically with regard to the appendectomy property, you never really lose the only property if you delete these subtrees. Because they're you know, if"
  },
  {
    "startTime": "00:24:01",
    "text": "If an auditor needs to download this data to check that these are valid changes to the log or something. Kind of goes back to the answer to the last question, which is if an auditor needs this data, Then we still need this data, so we need to hold on to it. We can't delete it yet. But, you know, even if we delete Oh, sorry. I see. I see. What were you saying is you just remove the, like, subtree, but you don't change, like, that in your example, like, that Ancestor note that's not deleted, you still, like, keep the hash value as is. Not, recognizing it or exchanging anything. Got it. Sorry. That answered the question. Thanks. like, Okay. Okay. Yeah. The root hash of the log still includes the hash of all of the stuff that was deleted. We just don't have it on desks to, like, do anything with it anymore. And, again, I I understand, like, the whole pruning idea of, like, deleting user data and then kind of hoping you can go through and delete some some of the cryptographic data in the tree as well. That's very hand wavy. Again, without a specific protocol, we don't know how well it's gonna work. But it does it does generally make sense. I think that's something that we can move forward with for now. AM. Next slide. Privacy law, this is one of the hard ones. Next slide. Compliance with privacy law, luckily kind of relies on the same, classification of data that we did before where we're separating data into user data and to cryptographic data, and I can also say that the approach to privacy law that we're gonna take is what's called anonymity. So Because this is a transparency log, and it's gonna grow infinitely, and we can never remove data from it. What we can do is we can delete data that is necessary to understand what is in the log. So there's some, like because if auxiliary data, to the side, like maybe a commitment opening We can delete that. And without the opening to the commitment,"
  },
  {
    "startTime": "00:26:01",
    "text": "and the associated user data, then the commitment that's actually stored in the log contains no information. It's basically essentially random from from the perspective of from anyone's perspective going forward. So Yeah. The approach raising is anonymity. We kind of classify data into user user data, or this is, So in QT, you have lookup keys and you have the value associated with the lookup key. What we're talking about here is the values associated with the lookup key. So, Deleting privacy law compliance with respect to The values associated with lookup keys is actually really easy because I'll be stored in the log essentially an EKTconstruction, is a commitment to the value Of I can move into the the value of a lookup key. Like I said before, you can just delete the associated data. You can delete the opening to the commitment then all that's left is in the log is the commitment, and that contains no information. So That's completely anonymized. Going forward, nothing about the user that is, is is still stored in the log that they could that would Nothing about the user is still stored in the log that would constitute PII. So like I said, those were the values associated with lookup keys or user data, The other thing that we kind of have to contend with is usernames or the lookup keys. So lookup keys, these are typically typically going to be things like Usernames or phone numbers, in a KAT construction, these are probably going to be stored in some kind prefix tree, and I've got a picture of a prefix tree here. This can kind of be pruned as well where you can go to the point in the prefix tree where the the key that you wanna delete is uniquely specified, and you can remove that. That just has the problem that what you leave behind is the common prefix."
  },
  {
    "startTime": "00:28:03",
    "text": "So everything up to the point where it's uniquely defined, do you have to keep that is that's necessary for answering queries about the other keys. But you can delete that. And the reason that deleting just that unique portion of the tree seems to be okay. Is because lookup keys are always processed through a VRF or verifiable random function. So essentially every key is a completely random, uniform choice out of, like, 32 bytes. And when you do that, the common prefix that's left the prefix tree is essentially always going to be too short to uniquely identify anyone. You'd expect, the prefix length to be proportional to the number of users that you have in your system, or the logarithm of the number of users that you have in your system. So if you have the 32 users, and you need to delete a user, you'd expect the prefix that's left in the tree to be about 32 bits. And that's essentially always gonna be too short to uniquely identify someone. Depending on what your keys are. That's, Yes. Sorry. Alright. Oh, Rory Steel. Is this on? Can you hear me? Remote. Well, Alright. So, I I'm trying to wrap my head around the the privacy properties we get from the VRF. And then privacy properties we get from know, for example, you know, salting the, the messages that are going into the log or adding context binding, at that layer. And it it it It feels like, you know, For an architecture document, maybe that's too much detail. So, like, again, need a protocol document to actually make sense of any of these security considerations, but Can you, like, maybe just sort of shoot from the hip on, like, what's the relationship between the VRF and it's"
  },
  {
    "startTime": "00:30:03",
    "text": "you know, role in providing privacy and and the like like like like like like like like context binding or domain binding that you might do with the message layer before you add anything the lock, Does that question make any sense? I'm not sure I understand the second part as well, but what I can do talk about the security properties of the VRF for a second. So, a VRF, what it does is it maps input bytes to output bytes. But these output bytes are essentially completely random, and nobody can distinguish the output from random unless they have what's called the VRF. Proofs, So if you think about it, like, if you put your phone number into a Vira, what you get out is 32 random bytes, I give you just these 32 random bytes, then It's completely indistinguishable from, random. Unless they give you what's called the VRF proof. And the VRF proof is, essentially a proof that shows that this mapping is correct. So I can give you my phone number. I can give you these 32 bytes. I can give you the proof. And you can verify. That these 32 bytes are the unique correct output of the VRF on my input phone number But in the specific case of removing things to comply with privacy law. What we're deleting we're deleting the input key. And we're also deleting the proof that we store with the VRF. So What that does is it turns it into essentially a a a password cracking kind of problem. Where I have these 32 output bytes, and I need to go through and guess what input key in the VRF could possibly produce this output, again, if we're only storing know, a common prefix, which is relatively short Then there's gonna be a lot of input keys, which potentially could be put into the into the VRF. Produce this, like, short 32 bit"
  },
  {
    "startTime": "00:32:02",
    "text": "prefix that's stored left in the tree. So that's where the the ambiguity comes from that allows us to credibly say that's that's This is anonymizing you know, because, One possible approach to this is that we You don't delete the common prefix that's stored in the tray, so we store full length of the VRF output, That's that's less credibly anonymizing because in the future, if our VRF private key is ever compromised, people can go through and do pass cracking so they can try it, I guess, you know, iterate every phone number pretty easily. And see viewer of output matches my phone number. And then, you know, I was in you know, I was in this KT log even though I requested my data to be deleted. And that would not be good. Yeah. So I think that's all for privacy law compliance. Are there any more questions on that? Okay. Excellent. And then privacy. Next slide. In the early version of a a protocol document that I wrote as an individual draft and also in the architecture document that we've adopted. There's sort of a trade off that I was struggling with. So before, we used to con before, in the in the individual draft that I wrote, I used a construction that had no padding or storage overhead and provided the guarantee that when outside observers interact with the transparency log, the best inference they can make from interacting with the transparency log in terms of estimating the number of users. That a service has. Is they would see it's less than some absurd bound, like, 2 to the 2 56. Like, if I interact with this log, I have truly no clue how many keys are actually stored in that. And that's something that I originally expected to be relatively important"
  },
  {
    "startTime": "00:34:01",
    "text": "to service providers is if we deploy KT, I don't want to just leak the number of users that I have. It seems pretty dramatic to me. But when consequence of this construction, where the number of users in the system is very well hidden. Is that if you have a 3rd party auditor, then the 3rd party auditor going through changes to the log, it can tell whether a certain change creates a new lookup key or updates an existing lookup key. And sort of, a consequence of that that third party auditors can also tell whether a change is real in terms of it actually has it actually affects a real user or it's like a fake padding change. I've just added to basically pad the changes to my log to make it seem like I'm making more changes than I am. So the 3rd party auditor can tell that. Know, outside users, maybe cannot tell them. Maybe they can. But, there was a lot of a lot of concern expressed about this leakage to the 3rd party auditor, about the 3rd party auditor, able to tell whether an update creates a new key or if it's an existing lookup key, whether those changes are real or fake. So what we did was we moved to a new system where the 3rd party auditor can't tell whether an update creates or changes a lookup key, and similarly can't tell whether in a page is real or fake. And, actually, one of the really kind of harsh predictable consequences of that decision is that if you want to pad your log with fake changes so that people can't many users your system actually has you have to make actual changes to the log. So the Kind of the upper reasonable limit for the number of changes that you can make to a log is probably gonna to the 32. People can infer that you have a number of users proportional to about that size. And, again, because you're actually making changes to the log, have to store a bunch of random data corresponding to a bunch of random changes, and and that's gonna be maybe a 100 gigabytes, probably at the low level."
  },
  {
    "startTime": "00:36:03",
    "text": "Realistically, it's probably gonna be several 100 gigabytes of of random data that you have to store, which kind of hard. To grapple with. It was hard for me to grapple with at first, but talking to people especially if you were in a large messaging service, like a couple 100 gigabytes of overhead, as maybe not a huge price to pay. For this, And specifically the way this was explained to me, that made it make a lot more sense. Was that before the privacy leakage we had was better for the service operator. Better for the service operator in the sense that we're not leaking anything about the number of users that the service has. Whereas the current approach that we've changed to is better for individual users. Where we are leaking less about, leaking less about individual users because, again, or Sorry. I don't wanna jump in, like, right in the middle of finishing your thought, but that that the the I guess the this This only applies in the case that you're padding the law with noise, like in trying to obfuscate the total size of the Right? Like, and if you're, protocol doesn't need that kind of, like, hide my size property, then you don't need to do this, and you never have to pay that 100 gigabyte v. Set. Yeah. That's actually that's a good point. Is that if you don't care about leaking the Right? of users that you have. Then you can just link the number of users that you have and not store all of this fake data. It's hard for me to imagine a service provider choosing to do that unless they're just particularly I don't know very open. Actually, it was said to me recently that WhatsApp publishes the number of users they have. In their quarterly reports for, like, the SEC. So leaking it in the protocol would Nope. This doesn't really be the biggest problem either. But, yeah, really interesting. And there was point about the 3rd party another auditor. Okay."
  },
  {
    "startTime": "00:38:01",
    "text": "Going back to the thread of The current approach being better for individual users and the previous approach being better for the service operator, With the previous approach, because the 3rd party auditor can tell, Whether an update creates a new lookup key or updates an existing lookup key, If the user submits updates in a particularly no possible way. So if they update every Tuesday at 2 PM or something, editor can actually pick up on that. They can identify which key belongs to this user that's updating it weird by. With a new approach, that's not true anymore. Every update looks all the same. Next slide. Yeah. It's all for privacy, and that's actually all for everything. Which was great. My sense right now is that See, architecture document is mostly done for now, and I think that people mostly agree with it given the fact that there had not been a lot of questions or comments on this presentation so far? So next step for the working group is to start writing a protocol document that actually matches the architecture document. So if you are interested in working on that. Please reach out to me because it's going to be quite a lot of work. Bam. Anything else? Thanks, Brennan. I guess one question that I was gonna ask you which you actually doping to right away was, yeah, just like noticing that they're aren't that many issues open on GitHub. When you mentioned that, I guess, from your point of view, things are looking pretty complete. Do folks in the room have Awesome. That, like, is there stuff that The thing should live in the document and doesn't and it doesn't exist right now. Guess now would be a good time to raise those issues, and put them up on GitHub or discuss them. And,"
  },
  {
    "startTime": "00:40:01",
    "text": "Yeah. I think we definitely would want to see a protocol document kinda start pretty soon ish, before we send the architecture document forward, for publication or whatever, because it'd be nice if If those documents can kind of evolve together because, you know, as as you pointed out in your presentation, Brandon, like, there's tons of things that are kind of, hand baby, which really will take more shape. When we have a protocol document, So, yeah, I guess, like, to repeat I'm writing this question Do you folks think that we are ready to work on a protocol document? And if yes, are you willing to contribute and review? Yes, Felix. Go ahead. Yeah. I'm ready, and I would be happy to help. No. I'm not. I'm ready. I think We ready. are To work on the protocol design, and I'm happy to help. So this is Roman as the eyeballs in the room for the chair. You can probably tell from the queue no one in the room is jumping up to the mic. Right. There's some parts shifting in their chairs in a response, but still no one at the mic. If you don't think we are ready to work on a protocol document, would also be really extremely helpful if you if you if you if you if you if you if you if you if you can say why not? We have a few plus ones for the protocol doc in the chat, though. Yeah. And Paolo, it's just It's it's not really working well. Sorry. But just wanna bring up a point, speaking as individual only that, because of both the time zone and, the cost of"
  },
  {
    "startTime": "00:42:03",
    "text": "travel and going to Australia that a lot of the people who are interested in privacy, and our part of nonprofits are not here. Might also not be awake at this time. So this should definitely, like, ask this question on the list where everybody has some time to respond. E. Thanks, Paul. Cool. Actually, the The chat team's pretty encouraging. If folks the folks who said plus 1, if you wouldn't mind solving the same, if you particularly interested in, in contributing versus reviewing, that would also be pretty helpful. I'm I'm here at the mic to say. I'm excited to review the protocol document, if we could get started on it, I'm particularly interested in the data structure part of it. So, you know, some of this contact monitoring the some of the protocol pieces for the, you know, network interface or transport layer connections, like, I'm less excited about that, but I'm really excited about the VRF construction the data structures, like that part of the protocol document, I'm eager to review it And, Yeah. I think I think The sooner we can get into that, the sooner some of these privacy and security issues that we think we've got, you know, A framework for thinking about in the architecture, we can actually start to analyze. Did we get any of properties are not So Let's do it. I'm sorry. Go ahead, Kim. Yeah, I was actually about to also kind of just say what were we already signed? I just thought it was an awkward answer. But I guess just to repeat, you know, like, I think that a lot of the questions that we were having was about the architecture and privacy specifically it's just a lot easier to talk about when we have, like, the product in front of us because I think there's a lot of, like, little low things in terms of, like, okay, how did you pruning what what, but it's actually hidden when when you're going to display it. Like, if we're doing a prefix tree or"
  },
  {
    "startTime": "00:44:00",
    "text": "or supplementary or whatever. Like, it actually meet the privacy guarantee that we're trying to get and warn the trade offs if we try to do, like, more versus some 35s. So yeah, definitely forward looking to getting started with the protocol doc. Cool. Good evening. Hi, Steven Farrell. I I'm also happy to review, and I think having a protocol document would be helpful to do that. I'm just here because I think there's time. So forgive me if you're short of time for some weird reason. On the list, I was kind of asking, earlier about How How One recovers if a log kinda goes bad. Out of incompetence or are Yeah. Or, you know, things just tend to disappear. And I I think there was some discussion on the list about that. I haven't read the recent architecture 1. Sorry. Where did that kind of land? Can you dismarized. So I think the the answer to that is So that falls under the the migration that I talked about, where you have the old log and the new log, and what you do is you put the final tree hash of the old log into the new log so that people know where the old log terminated. I know that you said in response to that was that you wanted to handle case where the old lock kind of just disappears. That is actually a lot harder to do because if the old log just disappears, there's like, monitoring monitoring and stuff that still needs to happen, where if you don't do that monitoring, and it could potentially adversely impact the the security of the full log where users don't necessarily know that all of their data was properly stored and managed Does that make sense? Essentially, I'm saying that, like, if the old log just disappears and there's nothing like, we can do about it. That's"
  },
  {
    "startTime": "00:46:00",
    "text": "a very bad place to be that I don't know how to get out of. Sure. Yeah. Okay. If they if if they're holding us vaporized and all the storage is gone, then Yeah. But if you know, if there's some remnant there, but nobody operating it anymore, Is there Does this migration strategy work in that case? Yeah. It still works. It doesn't require any changes to the old log. You just need the the final root hash of the old log so that you so that everybody agrees on where the old log terminated. Right? Cause if you if you still have the old dog, and you can still make read queries to it. You don't have to change the log. Yes. But you can still make read queries to it, then you can finish your monitoring, and you can use that approach that I described. Okay. And just so I think I had a map studio, what history that happened with PGP key servers and things like that, and it's I don't know if it would kind of have helped. With those kind of failures that we saw. Maybe it would What kind of failures? Yeah. Just, yeah, at some point, the the the You were at yoke. I'm trying to think. I'd have to, I'd have to go look. But, basically, I think at some point, you would kind of get, It wasn't that just that the key server stopped? Was become became unresponsive. Remember? I didn't remember. I don't I I think I think There's a there's 2 different categories of failure here. There's like, availability is completely destroyed. You can't even read anything anymore. And then there's, like, rights won't go rail, I can't write to it. And so, like, in the event that, like, you still have availability, but rights are turned off. You can recover with a migration strategy. That's And and as I understand it, that's, a difference between, like, failures that seen with PGPE servers is like you you couldn't actually do reads on them. Anymore Yeah."
  },
  {
    "startTime": "00:48:03",
    "text": "Right. Not being able to do reads is the thing that I don't know how to texts. Okay. No. I mean, I just think it's, you know, it's Given we're thinking about that there are, you know, the the assumption is there'll be many such Services for to different messaging platforms, I guess. So it seems quite likely that some of them will stop operating At a reasonably regular cadence, I guess. Right. Right. Right. Right. Right. Every few years or something. Yeah. But the idea is that when they do stop operating, they will hopefully stay redone leave for long enough? Right. So maybe so is there something in the architecture document that's kinda this if you wanna operate one of these things, in order to handle end of life in our know, problems in operation, you, what you want to have is a plan for being readable even if you're not doing updates anymore. I bad. I think we might say that somewhere. I will definitely go double check if this is I mean, that's, know, we can't go what happens in the rear work, but, you setting that expectation might be good. Thanks. Thank you. 7 Hey, Devin O'Brien, Google. In practice and other transparency systems, no matter what you write in a document, you can't stop something from disappearing if they want to. And so as we know from, like, strategic transparency logs are a little more disposable than we may have liked. And just to confirm here, The use of things like lead only mirrors is not a solution in KT, the similar way is to say, mirroring a CT log where it preserves that data, right, because of the mapping so a read only A read only mayor of a KT log. Would definitely solve this problem. Or if you have your read only mirror, can use that to complete your monitoring and then move to the new log. The issue is structural. Yeah. I think it's less likely that service providers would be open to allowing a third party to make a complete mirror of their luck? Well, I mean, actually,"
  },
  {
    "startTime": "00:50:02",
    "text": "that's not true. Maybe they would be. So that would actually be a good solution. Yep. Yep. Yep. Thanks. Thank you. Cool. I guess there seems to be a lot of consensus for working or starting work on our protocol document. I guess we can use the individual draft you had? Brent to start off with Not really. No. No. The the requirements that have come up and working on that architecture draft, have substantially obsoleted the protocol document that I wrote. So I would need to start essentially from scratch. That's why I said it was quite a lot of work. But, yeah, Again, please reach out to me if you've now worked on that together. It looks like folks are pretty interested at at least from the chat. Cool. We can Sean has is going to propose, let's put a, like, as Paul suggested, Let's put on the mailing list, as well. Since there are a ton of people who are missing. And maybe farm a small group and Brendan, if you want, we can probably from Chairside, we can share those first technical sync or something where a few people can come together and start prepping the first draft. If that works, Yeah. That would be good. Thank you so much. Okay. Oh, hi. Hi, Praachi. This Roman. to is this is I'm just trying checking in here, are you proposing an interim meeting to scrum on how to come up with the first version, or are you proposing that there's gonna be a design team, or maybe something can be week. Well, I was not going to the design team idea yet Roman, I was thinking based on the interest we get on the meeting list, like, if there are a few people I know people have shown interest to be reviewers. But who wants to come in as an author and actually write the draft what Brendan"
  },
  {
    "startTime": "00:52:03",
    "text": "So maybe if we get interest on the meeting list, we can just get them together in a room and get started or get a recurring meeting on the calendar for the others. Maybe I'm looking at incorrectly, Ruben. Correct me if there's a better version of doing this more than happy. To go that route? Not at all. I'm just kind of trying to clarify, clarify what you were saying. Working group chair decision here. I'm not in in your right here. Okay. Yeah. Yeah. I guess we can see depending on just just Okay. DKG says that it's probably, design team patent. Okay. So how about we take it to the mailing list with people chime in if there are others who are interested in coming and contributing Felix just showed interest in helping writing. So we have at least one person along with Brendan then we can take it from there, Shivan. Yeah. That sounds great to me. Okay. Cool. Just Anyone have anything else to discuss or talk about? If not, Thanks, Brendan. Prajji. Anything else? To discuss about the meeting. I'm good. Thank you Alright. Okay. Thanks and see you all in English. all, Alright. And, yeah, thanks again to to Ori and, and Roman and everyone else. I'm Jed. I'm looking forward to work to working with everyone. CEO. 55 Shivan Prochi. Thanks for joining us from a bad time zone. Yeah. Absolutely. This"
  },
  {
    "startTime": "00:54:07",
    "text": "right now, I'm sure."
  }
]
