[
  {
    "startTime": "00:00:31",
    "text": "test one two we\u0027re just working on getting the projector that\u0027s what so people can\u0027t see but we can kind of start with some "
  },
  {
    "startTime": "00:04:00",
    "text": "it\u0027s there it\u0027s gonna be won\u0027t comfortable eternal but it\u0027ll be right on the jet so okay it\u0027s Troy Johnson thank you all right um everyone take their seats we\u0027re gonna get started we\u0027re running a little bit late already so okay um Jason Jason who\u0027s up first can you be ready to go all right um we\u0027re gonna get started so welcome everyone this is the measurement and an analysis for protocols research group I\u0027m Dave Wonka and this is Maria who Lewin where you\u0027re co-chairs we have a packed agenda in our to an F hour time slot eleven presentations this morning so I\u0027m gonna ask the the next speakers get ready to to come up right after when the persons closing up and we\u0027ll have you up here you\u0027ll have a clicker to change your slides with the slides on the on the common laptop here will spin through the introductions so the IRT F uses the same note well and intellectual property process that they IETF does that\u0027s their here\u0027s a bunch of links to our Charter the mailing lists there\u0027s an either pad for note-taking and the slides are all posted so yeah thanks to the note takers Matt and Lars yeah and Lars thank you so the way we put together the agenda for this meeting is again the call for contributions on the mailing list that we put out on February 1st we received about 15 proposals of which 11 you\u0027ll see today just a couple were out of scope from FRG and the others we\u0027ve asked the people to present at a subsequent meeting when it might be more convenient and also to fit the times we have thanks for sending that stuff again we\u0027re soliciting for for future meetings you can advertise your projects here if you don\u0027t have measurement results yet we\u0027ll put them in the agenda and there\u0027ll be a couple advertisements at the end of this again I\u0027m not gonna go through these right now just to save time but 15 presentations and we\u0027ll introduce them as we go along two announcements the applied network research prize has been awarded for I think to six people for 2018 two of which we\u0027ll be presenting tomorrow morning in the IRT F open meeting so please join us for that really need opportunity to see some of the best work from the academic conferences "
  },
  {
    "startTime": "00:07:01",
    "text": "presented it in this forum the other thing that\u0027s going on in IRT F right now is the applied network research workshop the call is out now the paper submission deadline it can be existing work is April 20th I mean previously published work April 20th and that will be happening at the next IETF in July in Montreal so when come up or start with our first presentation here and we\u0027re starting off with a couple of presentations that are just heads up talks that we think are important for this this audience to hear something about so this is Jason Livengood great thanks very much Jason Livengood that I work at Comcast in North America and ISP and I thought I would give a quick talk about is there way to get quick talk about measurement challenges in the gigabit era there we go great thanks so emeasure Minh challenges in the gigabit era so a view of some of the impacts that measurement have on things like network operators so his background I certainly recommend this paper from some folks at MIT a few years ago there are in essence a wide range of existing systems they\u0027ve really fall into two primary camps one which run automated measurements from homogeneous gear that set customer homes most of these are national systems installed by regulators for example Sam knows which is based in the UK used by Ofcom FCC and a bunch of other regulators around the world of course they\u0027re also user facing systems usually web-based likes to test net and both of those you know from my standpoint you\u0027re starting to show some you know interesting behaviors as we approach a gigabit per second at the edge and the access network we\u0027re starting to see some issues which we\u0027ll talk about now and they sort of multi hundred megabit per second era but in our network at least many of our customers are 1 to 500 Meg\u0027s and we offer one gig now but the bulk of our customers will start to move to that 2019 and lastly you know measurement systems today focused primarily on speed but and user expectations about their internet services are changing a lot measurement systems that are sort of customer facing haven\u0027t caught up yet to that they\u0027re really still focused on speed customers are caring more about reliability availability and other performance of app services so what are some of the existing issues that we\u0027re seeing in the "
  },
  {
    "startTime": "00:10:01",
    "text": "systems that are out there today one most of those systems were designed with an assumption that the bottleneck link is in the access network or on the home network and the entire design of the system is oriented towards trying to measure that unfortunately as we enter the sort of gigabit era the home network Wi-Fi is usually becoming pretty good with a mesh Wi-Fi in the house and the access network is usually one gig that means that the bottleneck link has moved off to primarily an interconnection link or to the application servers where the tests are destined or something in that data center network but the tests really the designs of those tests really haven\u0027t evolved yet and so in many cases we\u0027re finding that the bottleneck link that people believe is being measured is a different link and so in the multi hundred megabit era that were in today we\u0027re seeing a 20 to 30 percent negative impact on occasion on some of these tests which is significant and of course why does that matter you know a lot of regulators and States Attorney General and others have financial consequences if operators aren\u0027t meeting these tests users certainly use them for troubleshooting so there are significant financial impacts to negative results in these kinds of things so in terms of key questions that we\u0027ve been asking some of our measurement partners and measurement researchers is you know does it make sense to continue this sort of test where you\u0027re running from you know one one sort of test or multiple TCP connections to one destination site there\u0027s no resemblance whatsoever to actually use your behavior anymore in that test and there really no application servers services at the edge that use one gig so in a way what\u0027s the point of a single task to a single destination of one gig maybe a more appropriate is to spray hundreds of tests simultaneously across hundreds of destinations but it also prompts questions about you know the underlying you know design and how your report question yeah your response is out and so on you know and of course even today you know how can a lot of that server infrastructure be better and more independently transparently monitored for example in the u.s. we\u0027ve used rape Alice probes to measure the Sam knows infrastructure and you know they sort of outsource that to some other providers like M Labs and I found really interesting variations in availability of time they\u0027re not really apparent to even regulators or or operators so there\u0027s a lot of variation that the server side that can affect the results as well and of course a key question that we always have asked in an operator environment is you know when will the test away from simply testing throughput tests move and start looking at latency and reliability and other things bearing in mind of course that the things that are measured pervasively are the things that network operator is designed to the things that they market to and then the customers to pay attention to buy so in a way things that start getting instrumented in measurement may eventually make their way out to the way the services are "
  },
  {
    "startTime": "00:13:01",
    "text": "marketed and so on and of course the last couple points here qoe is a as much impacted by sort of speed as if you will and network operator sort of access network performance but also application or edge performance and so you know we\u0027ve contemplated measuring those things as well and then lastly you know as I mentioned before the things that you measure are you\u0027re going to incentivize so even in the beginning of things like the Sam nose testing in the US and the UK it fundamentally changed the way that operators were provisioning capacity in the network and the way we were you know offering services it wasn\u0027t just up to and you would hope that you would get a speed once a day but an expectation that you\u0027d get your speed at you know peak hour and off-peak and so whatever gets measured is what you\u0027ll incentivize so you know my question of course is you know BC p38 and other kinds of things might be interested in the measure and well here are some of these things today about ipv6 and DNS SEC you know things that are more nuanced about services um thanks Jason thank you we probably have time for one question or comment anyone okay thanks a lot Jason Luke you can come up oops all right next up we have Luke Hendricks talking to us about some visualization stuff you did in a hackathon right yeah this that\u0027s correct good morning all I\u0027m Luke I\u0027m with University of Twente I like to measure things v-6 this is running automatically right now and spoiling all of my surprised you thank you see I\u0027ve been attending the the derived v6 hackathon I think four or five months ago and this is kind of like a spinoff from that so yeah this will work so a problem with visualizing v6 stuff we cannot really use what we have done for fee for right which is often true for things v6 you probably have seen one or both of these visualizations are the xkcd or some of the work at eyes I buy by John Hyneman and his colleagues we can visualize the entire free force base in something that\u0027s you know still readable if you will this is a hilbert curve a space-filling curve showing the the entire fee for aerospace if you want to do the same thing for fee six it\u0027s quite useless it\u0027s like you know something like this you will see you will see a small dot representing the 2000 slash three space this is useful to convince people that indeed we do have enough address space in fee six other "
  },
  {
    "startTime": "00:16:02",
    "text": "than that it\u0027s it\u0027s I think it\u0027s quite useless right so I have created this tool called says blots s means six in touch this is based on square Phi 3 maps you can you can look it up in this footnote which basically means is that we will always fill the same space for gardening of our of our input set I will show you some pictures but we use the size to depict the size of the prefix and we use call us to depict how many addresses of our input set are in this specific prefix and instead of you know plotting all the v6 address space we only plot what we feed it right this way we can we can look for outliers in for example measurement data or HTTP exit logs or whatever right so um extra time this is this is a visualization from says blots based on all the announced prefixes so this is 45 46,000 prefixes as seen by right few rah-rah fuse and for the addresses I used a hit list that the guys at the TU invention make right so what you see here is that the bigger prefixes are in the top left corner and we fill up the entire plot with all the smaller more precise prefixes we then color all the squares according to the number of addresses that we\u0027ve seen in our input set all right so what we can see here is that for the entire announced prefixes there are still some white space right which means that on this hit list there are no addresses within these announced prefixes so now you can say something about how complete is this hit list yes or no there is some some white space although there\u0027s a lot of a lot of stuff is there if you want to look at what is only in prefixes that are represented by this hit list we can filter it out how will you get a I think a more more fancier picture and now you can for example spots for outliers right you see a big red square on top which is a bigger a s with a lot of representation so you can reason that this might be a over-represented prefix in your data sets well those are these are more things to spot but because of time I will skip from things another thing measurement I did we could go on open memcache D insists on v6 I found 361 and the interesting part here is that you can spot some smaller prefixes that have a lot of them relatively right that\u0027s the the bright red square on the lower right corner so one of the features I\u0027ve built into this thing is that we outputs to HTML with some interactive stuff you can zoom in on the plot you kind of have some extra information in tooltips which you can see here on this on the screenshot so that way you you can get more of plots that are very dense and I mean "
  },
  {
    "startTime": "00:19:03",
    "text": "they still become very dense right if you plot a lot of prefixes with a lot of addresses and that\u0027s that\u0027s basically it this tool is now in a very generic state if you will you feed it a list of prefixes you feed it a list of addresses and that\u0027s it and I\u0027m I\u0027m wondering where to go from here right there\u0027s probably a lot of use cases that I didn\u0027t think about and I want to continue working on this thing because I think it\u0027s quite nice but I I would like some input to see where this can go it\u0027s it\u0027s it\u0027s under an MIT license on get up it\u0027s written in risk because I like risk but this is this is basically the most important sheet maybe please try it out if you see any you know nice use cases in this please let me know I will be here all week so so please reach out to me either in person or via email if you want to see any features tell me or make a get up issue or whatever thank you thanks so much Luke well while there\u0027s time for one question or comment while we setup Roland if you what I think is one of the things that\u0027s interesting about that is a transitioning from the tool to in a useful measurement and I think you got a good start on that by being able to show for instance where memcache stuff is mm-hmm Thanks thank you and here\u0027s Roland so the next two presentations are going to be updates on things that are at map RG before so one of our goals and this is to keep bringing you the data back again and again and again and Roland it\u0027s been one of the one of those that\u0027s visited us a number of times and can keep us up-to-date on what\u0027s going on within a sec yeah okay thank you yes so this will be a quick update on measuring domestic deployment with the focus on the quality of DNS deployment and its shared work with Northeastern University s IDN labs and Cerf map so the goals we\u0027ve been measuring the NSTIC deployment for a long time and one of the things that we observed that everybody observes is that in a general population DNS SEC deployment generally remains low right it\u0027s around 1% income that an org and then there are some CCT of these to do much better for instance Datsun L and OTC both have almost half of all their domains signed with the in a sec and this is likely because they incentivize the NSTIC deployment by giving registrar\u0027s some reduction in the price for registration now what we wanted to study is if organizations that do deploy DNS a get it right both in the general population but also in these CC 2ds that have a much larger at the intersect deployment and this the graphs in this presentation are based on two papers and the references around the one of the final slides if you want to read papers we used longitudinal data from the open Intel platform go and have a look at our website if you\u0027re interested in that we measure almost 60% of the entire name space on a daily basis and we record a data and we now have some over three "
  },
  {
    "startTime": "00:22:04",
    "text": "years of data collected that you can use to analyze the state of the DNS from day to day and we have a new website coming soon now for the comb Network study we use 21 months of data and for Delta C and Delta nel we used about one and a half years of data and we had a number of challenges when we were doing these measurements because what we wanted to do if you want to see if the NSTIC is done right you need to validate all the signatures and we really had to validate millions and millions of signatures and also we wanted to see if people do the more complex stuff in the NSX such as key rollover so we had to track the NSA key rollovers over time and that was actually quite challenging to do for such a large data set so for that I\u0027m not going to go into detail but we used some say quote-unquote Big Data technologies a Hadoop cluster with spark on it so we could validate all the signatures and if you want to learn more about that read the papers now if we look at the general population ComNet org this shows you a graph over over almost two years of the development of the number of signed domains in in dominant and the takeaway from this is its low white dot org is is the highest run about 1% of domains in the doric are deploying the in a sec and for net common net it\u0027s a little bit lower but it\u0027s also sort of going towards 1% mark now this data is almost a year old and I can tell you that since then not a lot has changed it\u0027s still around this mark but this isn\u0027t the whole picture if you look at it the previous graph shows you domains that have signed their zones but up to 30% of those don\u0027t have a secure delegation in their parents own so that means that they\u0027ve gone through all the trouble of deploying DNS SEC and then they didn\u0027t create a secure delegation so nobody can validate their signatures so that\u0027s just plain stupid why would you do that and we have another paper that I realized that I forgot to cite in this that actually looks at that in a little bit more detail but it turns out that many of these signed domains in combat and org are actually side effects of the incentives in the dot NL and all to see ccTLD so these are people that deployed in a sec for all of their domains and they also do it for combinatoric domains that they have but then they don\u0027t bother to create a secure delegation in the parent zone now we looked at errors in these deployment because we wanted to see if people duty and I said do they get it right and the most common problem that we found is actually missing signatures so in as you can see income data and org up until the end of 2016 up to 2% of science were missing signatures and basically that means that you break the zone and it turned out that this was a it was mostly one operator that was responsible for this and what they were doing was that some of their name servers if you had sent them a query they will give you back a signed response including the signatures and other name servers would give you back just plain DNS response and and "
  },
  {
    "startTime": "00:25:06",
    "text": "this really broke stuff we crawled some logs and this led to validation failures for people so that was really stupid they fixed that at the end of 2016 and that\u0027s why this huge drop occurs at the end of the graph and then there\u0027s also a small minority of time domains that have signed some records but then the signature over the DNS key set is missing and that basically also means you break stuff actually broken signature so signatures that are either expired or in some way invalid so we can\u0027t validate them because the content of the signature is somehow incorrect are very very rare expired signatures less than 0.6 percent of all signatures were expired at at any given point in time and invalid signatures are extremely rare and the other thing that we found that was where we expected to see quite a lot but actually didn\u0027t occur so much was mismatches between the parents owns a home network and the child where there would be a secure delegation but the secure delegation would not match the key or there would be a security Legation but the zone turned out to be unsigned and that\u0027s actually also very rare so that\u0027s that\u0027s good news because it means that at least people are not breaking that now we also looked at the ccTLDs because they have such large DNS SEC deployments right 50% of their domains are signed and we we did the other checks that I showed you before and and the situation is much the same so if zones are signed they\u0027re usually signed well in the CCT of these the number of missing secure delegations is much lower because you only get in the financial incentive if you have a fully working DNS SEC deployment so what we wanted to check there is if they deployed in a sec do they follow best practices and we took the NIST guidelines for that as a starting point and the guidelines are up here on the slide and and things that we look like was do they use to write key size do to use one is recommended algorithms and more importantly do they perform key will overs if the if the guidelines say that you should do so at a certain frequency tracking key role over I\u0027m not gonna explain the picture in detail because I don\u0027t think there\u0027s time for that but tracking here will over turned out to be a pretty tricky and because you you actually have to look at all of the keys that you have in your data set and see which ones changed but this is not one day to the next day changes keys are used simultaneously so you really need to work out when a key is discontinued to compute what the lifetime of that key was and had a student do that the his master\u0027s ESA\u0027s has most of the explanation in and it\u0027s cited in one of the papers now this might be a little bit hard to read on that screen but on the left hand side you see dot NL on the right hand side you see Delta C and list is on listed on "
  },
  {
    "startTime": "00:28:08",
    "text": "this slide are the top DNS operators that are responsible for 80% of the signed domains in those two cctlds and the takeaway from this slide is that for algorithm choice and KSK size choice of size of size of the key signing key most operators do well so at the top left in dot n luc1 operator that has a few red crosses for algorithm choice and that\u0027s because they use RSA sha-1 which this does not recommend you use because it has a sha-1 over in a minute it\u0027s debatable whether it that is insecure or not but that\u0027s just what the best practice States more importantly if you look at zones any key size you see a whole line of little triangles with exclamation marks in them and that means that that\u0027s it satis case size would be okay if the key is rolled regularly because these roll all of these operators are using 1024 bit zone signing keys and the in the next column you see whether they actually do a rollover and there you see red Xs all across the board so people are using 1024 bit keys and they\u0027re never rolling them and actually the the oldest keys that are in use have been in use for over two years and they\u0027ve never been changed so that means that while these CCT of these really have a high uptake of the NSTIC deployment nobody is following best practices and you can debate whether using 1024 bit keys and never changing them is really a good practice so to conclude while the NSTIC deployment generally may arrange low there are exceptions among this cctlds and there are other CCT of these other than dalton Elendil to see that have good results I think for instance in Norway there are over 50 percent of the zones are signed real mistakes where people break stuff be so signatures missing or invalid signatures or expired signatures are actually extremely rare which is good news because that means that people are at least automating that properly but other important best practices are seldomly followed so regular key willow verse for wikis they simply do not occur now one thing that you don\u0027t see in the graphs yet is that there is there is an increasing uptake of elliptic curve signing algorithms I didn\u0027t have time to make a graph for that but that there is uptake of that and that is slightly changing the picture so finally for the financial incentives that the CCC of these have seemed to work right they get really high up take off the NSX but we think that those incentives should really include mandatory quality requirements right you only get the incentive if you follow best practices "
  },
  {
    "startTime": "00:31:09",
    "text": "and otherwise you don\u0027t do that and we talked to both folks at Lawton Elendil to see and the good news is both of them are considering updating their incentives and hopefully that will not just change the picture in their CCTs sealed deal these but also in other t all these that have signed domains as a side effect of these incentives that\u0027s it all right thanks so much Roland I do apologize for having an interrupt you but we\u0027ve got a lot of things on there and I really appreciate you squeezing into the timeslot and you got it right on the money is there anyone in here who hasn\u0027t seen it oh I\u0027m sorry yeah yeah if you haven\u0027t seen the attendance sheets please raise your hand and we can run it over by you next up we\u0027re gonna switch to another update and don\u0027t think there are questions but we would have time for one question okay no good um next up we\u0027ll have Tommy Pauly give us an update on he visited us I think last year possibly the year before with some v6 measurements from their vantage point uh an apple equipment alright hello I\u0027m Tommy Pauly from Apple we\u0027ve previously spoken to map Reggie about measurements we\u0027ve done for six adoption rates that we\u0027ve seen from the client we\u0027ve also done some presentations about how we see happy eyeballs working of when we have both before in v6 how often are the races favouring v6 over v4 one of the comments that had previously come up was that we didn\u0027t have quite as much our TT data actual performance of how v6 is doing compared to v4 and so we wanted to rectify that so we added more to our measurements so I just wanted to share that with everyone today thank you all right so what is new in the data that we have now so previously we actually had presented some RTT data about the TCP connection establishment time latency but we weren\u0027t gathering essentially the performance of the connections overall once they ended up connecting over v4 v6 so the new data is actually tracking the smooth RTT values over the lifetime of an entire TCP connection up until the point where the application closes the socket essentially and the data that I\u0027m going to be presenting here was collected over the course of one month that was just February of 2018 and it\u0027s a random sample of 0.1 percent of all connections that have opted in to doing metrics collections so first just overall to give an update on where we\u0027re seeing ipv6 availability on networks from the client side so this percentage shows the number of times in which a client device is on a network that seems to offer v6 that it even has a chance of connecting to a dual stack or v6 only host so globally we see 29 percent of the Wi-Fi networks we connect to offer v6 44 percent of the cell networks do in the US we see that figure go up for both "
  },
  {
    "startTime": "00:34:11",
    "text": "cases 39 percent of Wi-Fi networks to do offer v6 and quite nice 87% of all cellular network connectivity does offer v6 just because we are in London I also grab the data to share for the UK Wi-Fi is at a pretty decent 32 percent and cellular just completely abysmal at barely measurable but at a point one two percent so let\u0027s work on that so the rest of the data I\u0027m going to be presenting kind of in this format so to look at how we\u0027re reading this the pie chart is just saying for this sampling what percentage of the connections were actually using v6 so globally we\u0027re seeing about thirteen of the thirteen percent of the connections that we\u0027re making are actually using v6 this is overall including all networks the salted lines are a CDF of what we see for the overall connection smooth RTT and the dotted lines that are tracking those are what we see for the just the handshake latency time so we\u0027re comparing when we\u0027re measuring for happy eyeballs how long it takes to actually bring up the connection how well does that translate into the connection actually being faster overall so the story for v6 here is really quite good overall we do see that the cdf is it has a nice little bump for v6 so that on the whole there are a lot more faster v6 connections and we see that in general across all networks the handshake is a good predictor of the actual connection RTT time but when we break it down there are some interesting observations we can make so here is the data for us Wi-Fi values so here we see 14% of connections are using v6 slightly higher than the global average again the v6 performance looks quite good overall one of the interesting things we noted is that in this case on Wi-Fi the TCP handshake latency is generally a little bit slower than what we end up with on the actual RTT the best guesses we have we\u0027ve discussed this with several people would be that often times the handshake may correlate to when you\u0027re having to bring up the radio and so there are more things that you need to bring up at the beginning of the connection that could impact its overall latency but in general looks quite good now when we take a look at that same chart for cellular connections in the US we actually see a quite different story so first of all we have a lot more v6 connections which is great we have 40% of all connections in the u.s. being made over cellular using v6 however the when we look across all the carriers we "
  },
  {
    "startTime": "00:37:11",
    "text": "are seeing that the observed RTT for v6 is actually trending slower than v4 and we also see an interesting thing that the handshakes are actually often quite quite a bit faster this was a bit surprising a bit odd we looked closer at the data and we recognized that actually one of the carriers in the US was actually accounting for a lot of the difference here and so when we removed just one one of the carrier values we actually see that the trend shows this in which we have v6 being again better performance overall but we still see the fact that the handshakes are generally faster than the overall connection RTT so this is a very interesting point I\u0027d be curious to hear anyone\u0027s thoughts on this our best guess at this point is that in the case in which we see certain carriers being slower for v6 if you actually go back and forth between these you see that the main difference is that v4 was made a lot faster and so this is probably artificially being done by essentially TCP being terminated somewhere closer to the device and actually not doing relent and TCP so that the values are being skewed but that seems to not be done for v6 in the same way or at least if it is being done it\u0027s not having the same beneficial effects so we\u0027d love to see that be improved in the future as far as why the handshakes are on cell often faster than the connection which is the opposite of Wi-Fi some of the speculations could be that although there is still a radio bring up time that we have to worry about the cell radio technologies may involve more batching of delivery such that the large connections would actually be slowed down slightly so just because we\u0027re in London let\u0027s look at UK as well over Wi-Fi we have 11% using v6 the performance actually quite good interesting curve for the v4 numbers here don\u0027t quite understand that but we do see the same trend that we saw in the US and we do see this actually kind of globally that over Wi-Fi we do expect that the handshake is going to be slightly slower than the overall observed RTT and then just to make us sad here\u0027s the cellular RTT values again there\u0027s almost an almost no data for the v6 on cell here one thing I will point out is that for both v6 and v4 we see the same trend over cellular that we saw in the u.s. of the handshakes being faster than the overall RTT even when we have so tragically little data for v6 here I\u0027ll leave it to you tell me if you want to take a quick question during the session or sure this is my last slide so I\u0027ll just kind of summarize and then we can go to the comments so essentially the observations are we see on Wi-Fi "
  },
  {
    "startTime": "00:40:13",
    "text": "generally we have better hench large slower handshake our tt\u0027s than we do for the phone connection it\u0027s reversed on cell in general the RT T\u0027s for cell is better for v6 is better all across the board cell is worse for some carriers that seem to be doing proxying and the UK cellular network has very little v6 adoption so yeah okay yeah there must be a switch thanks Tim yeah so I\u0027m a bit where are you getting your cellular measurements for for the UK position I\u0027m just curious so this is just sampling from Apple devices so this would be essentially iPhones and stuff okay so there certainly may be other devices that have been configured differently but at least this is the view that we have of UK carriers okay so I I believe II maybe there\u0027s someone in the room have about a million handsets with v6 but maybe not on Apple devices sounds like it okay so tell me do you have the order of number of measurements it\u0027s like 10 or a couple thousand so in general the measurements that we have for all of these are quite high for this particular v6 this is order of just thousands of connections so very very very variable okay let\u0027s cut the line now and let\u0027s try to do this quickly learn zouk annuity I think I bowling the grass and the pie charts I get the impression that pretty much across the board the v6 percentage is that the you you end up using v6 half the time of the total connections right let\u0027s so let\u0027s say you say in the u.s. I think the number is 32% you use v6 about you know half of that right which would mainly based on a happy eyeballs data that would mainly be from hosts that are not dual stacked right like I guess the question is if you used let\u0027s say that you had a large head start from v6 well how would those numbers change so when we look at our happy eyeballs data when we have v6 available in the network and we have a dual stack host we are using v6 95% of the time okay so maybe I\u0027m just miss reading across sure so if you go to the Wi-Fi one like the one before this right so I remember from the other graph that you know v6 and u6 and 32 percent of Wi-Fi wife of us wife I had right 39 yeah so why is it you\u0027d being used only 14 percent of the time I\u0027m imagining that\u0027s because a lot of our connections are two things that don\u0027t have dual "
  },
  {
    "startTime": "00:43:15",
    "text": "stacked servers oh right right okay so it\u0027s about so that 50 percent is the content factor then it\u0027s multiplying down yeah thanks mycotoxin clarification for our question what is the RTT of a TCP connection I do understand what the RTT of measurement for the header is for the handshake is but it\u0027s the SRT T at the end of the TCP connection or this is kind of like the smooth average that it\u0027s it sees first life and that essentially the socket layer is calculating that so at the end you call TCP info and getting it out right at the end of the connection we look at simply the smooth average of what TCP saw for the lifetime of that connection yep okay thank you Jeff hi I\u0027m having a slightly hard time actually interpreting these profiles because what is going on is that you\u0027re not measuring v4 and v6 to the same endpoint that\u0027s exactly so what you are measuring is happy eyeballs selected six and here\u0027s a profile of what happened as a result and happy eyeballs selected four and here\u0027s a profile or the server was v4 oh that\u0027s correct so yes lots of biases in here right so I\u0027m just sort of trying to understand that profile that you get and what is the precise meaning of the differences in the profile because I get certainly a wildly different answer when I look at the one endpoint and look at v4 and v6 to the same endpoint dual-stack endpoint deeper in the network so then this profile as I said I\u0027m still trying to wrap my head around exactly what you\u0027re measuring right guys happy eyeballs if you change the timers would you change that profile I\u0027m like are you measuring the v4 when the v6 is kind of okay to within 300 milliseconds or whatever time you\u0027re using right so I think it\u0027s important to consider that because we\u0027re including all v4 connections here a lot of these are ones that weren\u0027t even eligible for happy eyeballs right so it\u0027s just trying to get the overall picture that the VC is considering that there are very few six only sides of these six sites we\u0027re happy eyeballs the race for v6 one that\u0027s correct and we\u0027re essentially we are often favored oranges mandarins yeah okay cool thanks so much Tommy for bring that to us all right next up is my colleague Eric nygren talking to us about an update on TLS sni and ipv6 usage by clients on the web hi Jimmy hello my name is Eric Nygren and I\u0027m gonna be talking a little about some measurements we\u0027ve been doing on both TLS S\u0026I and client IP v6 client adoption um the motivate one of the main "
  },
  {
    "startTime": "00:46:15",
    "text": "motivations for why I\u0027ve been looking at this is looking at HTTP growth where when we have HTTP growing rapidly yay but without TLS S\u0026I you have no I P multi-tenancy so the client in the in the handshake if it doesn\u0027t actually send the HL SS and I the only way the server would know would know which certificate to serve back we based upon IP addresses but ipv4 is exhausted at the our our our IRS and ipv6 dell has a ways to go and if he went back a few years TLS S\u0026I adoption wasn\u0027t really was wasn\u0027t high enough to be a good general solution um however there\u0027s been a lot of movement on at TLS S\u0026I adoption recently making it much more viable than it used to be so we look at HTTP growth even though the certificate side this graph comes from let\u0027s encrypt and you see that we\u0027re let\u0027s encrypt now has about 50 million active certificates and if you want to try to have an ipv4 address per certificate to do this without TLS and i that would be about 3/8 worth of ipv4 addresses even now just for this one use case um so that\u0027s not going to be such that going forwards just using ipv4 addresses 2d MUX isn\u0027t going to be at all viable if you also look at HTTP growth and Akamai even in the past three years we\u0027ve seen a lot of growth of customers going and taking sites that used to be HTTP only and moving them over to HTTPS where if you went back into mid 2015 we had around 30 37 38 percent of customer host names that were delivering over a hundred million age P requests per day we\u0027re using I\u0027m cut their own TLS certificates with their own names on them too now when that\u0027s somewhere around the 57 58 percent range and if you add on top of this customer using wildcards ERPs and you also kind of mix in the fact that some of these host names do have a mix of HTTP and HTTPS traffic we\u0027re seeing around seventy five percent of requests we serve now or over HTTPS which is a huge improvement over what it was a few years ago if we look at TLS S\u0026I if you even go back a few years ago to to 2014 you start the s and I story was pretty bleak this is the percentage of requests that would come in to Akamai over HTTPS it sent TLS s and I and back in 2014 that was in that that eighty to eighty-five percent range which is something where if you go to a customer and you say hey go go turn on HTTPS but the s and I only it only break 15 to 20 percent of your end users that\u0027s just not going to fly so well but if you go back to even on where we were at the start of last year in 2017 that was starting to get up into that ninety eight percent range and still two percent is in that that there "
  },
  {
    "startTime": "00:49:15",
    "text": "are a lot of large sites that will not be who may have a three or four nines availability goal we\u0027re telling them hey we\u0027ll only break two percent of your end users is still not great but it\u0027s still a lot better than the 15 to 20 percent that it used to be but if we now look at where it is in the past and how this has changed in the past year or so even the past year we\u0027ve seen substantial improvements in terms of clients that are sending TLS s and I as some of the remaining light on things that have gotten fixed so this is this graph this is showing here a CDF across Akamai customers or Akamai customer certificate slots which and for just the HTTP requests on those with customer certificates lots and bucketing them by what percentage of HTTP requests came in that sent TLS and I and if and each of the the curves is a CDF at a different point in time so the yellow one is from February of this year and then you look at that and we\u0027re now at that point where we\u0027re 31% of of customer configurations of slots have sni adoption that\u0027s over 99.9% so and if you look even at at the median the median is starting to get into that case where we\u0027re talking more about the median later the medians also well over 99% um however there are still plenty of customers we see who have sni usage that are still low so for example 20 no 20 about 1% of our customer configurations have sni usage below 97% and a lot of that is because people will have custom apps custom devices customer appliances going and doing it doing HTTP but not sending us and I so if you go back and look into some studies that were done a few years ago it for example CloudFlare had one it showed a big variation in the global in the global medians we\u0027d see some countries like China that had much lower SMI adoption than others if we go and look at that now we\u0027ll see that that globally it\u0027s actually leveled out a lot of that global variation between customers are chemi between countries has settled out so for us we see the our median customer usage has around ninety nine point seven percent of HTTP requests sending TLS S\u0026I on their TS handshake in almost all countries in geo regions globally that medians around ninety nine point seven six percent but we however there are a lot of countries around the world we were to actually where that median has gone over ninety nine point nine and the the lowest that we\u0027d seen that we\u0027ve seen even kind of middle of last year with China and turned out that a lot of that lower s and I with remaining in China was due to one search engine and having a conversation with them they started sending TLS s and I and Paul "
  },
  {
    "startTime": "00:52:16",
    "text": "shot and pulled China\u0027s median backup to be the rest similar to the rest of the world at least from our perspective and if you compare this over to two TLS one two there\u0027s actually less usage of TLS one two then there is of TLS S\u0026I and there\u0027s not always a strong correlation here there are a lot of TLS 100 clients that do send us and I and some cheela want to clients that don\u0027t but sni usage is actually ahead of where Kilis what one two is if people who are considerate at this point considering turning off TLS one one and one oh that\u0027s going to be more impactful than switching to something that is s and I only and requires s and I to work the Z so what does it send to us s and I there\u0027s it there\u0027s a a lot of what we\u0027re saying it\u0027s we see custom clients apps gaming consoles antivirus things there are things that are have spoofed spoofed user agents or man-in-the-middle devices like antivirus and secure web gateways Windows XP is now it is it used to be the thing that was was the Bugaboo but of the non TLS S\u0026I traffic only about 6% of that it\u0027s not sinning it\u0027s not sending us and I and and PI and also older versions of Python Java and Apache Clyde are also around 4% of the non S\u0026I hits we have and there used to be a big issue where last or even last year a lot of search bots didn\u0027t send us and I but all but one in China have fixed that now but then there\u0027s a long tail of other things and anecdotally some of that\u0027s getting fixed like I found Apache bench on there is one thing that didn\u0027t but there are commits in the non released version that that address and I support Eric you were right yet you seven more minutes okay I look I looked at our own agenda okay the so for ipv6 trends the methodology we\u0027re looking at here is looking at 24 hour snapshots of data it\u0027s the same data that we\u0027ve been sending to I socks for world ipv6 launch org for the past few years and we\u0027re looking there at a few hundred billion HTTP requests occurrence dual-stack sites and looking for what percentage of those requests are over ipv6 relative to the the total hits and i\u0027ve been focusing in the past on what are the top ipv6 leaders here but as we start getting as we start getting that that global average in ipv6 moving up depending upon what the mix of content is we can see that global average being anywhere in the 17% to 31 percent range and you\u0027ll see outliers like websites that are heavily mobile may see something closer to 31 percent ice or global average you may see some enterprise software downloads that are only a few percent "
  },
  {
    "startTime": "00:55:17",
    "text": "ipv6 um but if we look at how do we get that I that global average to keep moving how do we get it to not stall out at 50 percent the I\u0027ve started looking at what are the top areas with residual ipv4 where are the things that we can do the most work to get that residual ipv4 to go away and there tend to be two clusters there there are places where ipv6 deployments are already in progress and then there are places where IP where there\u0027s little to no ipv6 yet and both of those are areas which have may have different strategies for what can help move the needle in ipv6 and a lot of this ends up being heavily influenced both by the content and Ice peanut works that are deployed b6 but there is a lot of content mixed sensitivity here as I mentioned earlier so if we looks at what are the top countries with residual ipv6 and that breaks down into two buckets so those countries that already have ipv6 deployment in progress but we there\u0027s still a lot of opportunity and those are actually the the top ones at least from what we\u0027re observing and everyone will observe different slices but from our observations the top the top opportunities for residual ipv4 are in those countries that are already moving in ipv6 the u.s. is at the top of the list already at 441 % ipv6 by on hits but then you but below that you have the UK and Japan and and Germany and India which are also already deploying v6 but then you go over to the other side there\u0027s a set of countries that are further down the list none of them are in that top 10 of residual v4 but they have less 3% ipv6 in some cases it\u0027s close to zero when observed globally and those are Russia China Italy Spain Indonesia Turkey and South Korea up at the top of that list and there are a few of those that may have ipv6 in country but they\u0027re ipv6 connectivity you\u0027re peering to the rest of the world is sufficiently bad that it\u0027s hard to actually measure it and that happy eyeballs is flipping over to not use v6 and therefore it looks low so on the device side on the high side if there\u0027s a lot of the modern devices do ipv6 well and sent use up quite a bit of the time um oddly went there\u0027s the ipv6 preference across the board we see four four older versions of Windows seem lower that may be because it is a heavier enterprise deployment or heavier deployment in some countries um but there\u0027s a lot of opportunity on the very limited ipv6 side among especially set-top boxes and some streaming set-top boxes and some custom apps if you look at that there are some vent there are some vendors of consumer electronics of set-top boxes that do v6 and have averages in that 30 ish percent category but then there are others that are 0% that are actually fairly high in the residual ipv4 bucket the same goes for custom applications where people have may have written some app and just don\u0027t send v6 from it if if we look at things on the network side one of the things is how do we and one of the questions that comes up and in "
  },
  {
    "startTime": "00:58:17",
    "text": "some discussion forums is how do we not get ipv6 stuck at 50 ish percent and when looking at what that distribution of networks it\u0027s like is like we are gonna it\u0027s not just the the big networks that we can solve the problem with by themselves like if you look at the top 55 networks by residual ipv4 was about top 55 Networks account for half of the residual ipv4 and over half of those have ipv6 greater than 2% so so in that that front-end of networks you have a lot that have started deploying v6 already but if you start if you want to get out and in handle the next 50% of residual ipv4 traffic that\u0027s going to be it that\u0027s going to require networks that haven\u0027t been deploying to be sixth deploying it so in the if you want to get out to the 90th percentile of residual ipv4 that\u0027s going to take there\u0027s 1200 networks there and only around eighteen percent of those have ipv6 greater than two percent but one thing that is a positive sign is there are a lot of networks out there that have v6 configured but haven\u0027t actually turned it on for their end user eyeballs so for example our networks team has has been able to get v6 working on servers in around 840 networks and 114 countries around the world which is more which is a significantly more than you necessarily are seeing a high ipv6 eyeball usage from um so that\u0027s it those these let the links have some more steps are also on the support slides some graphs on various networks and countries thanks Eric question your comment okay so so next up um we invited Yann Ruth to come and talk to us about two two different topics that they\u0027ve been studying lately both active measurements in the v4 space just recently got me sick so that you mean the the University just yeah yeah okay take it away yan you got a 25 minutes total and you get to decide between your two presentations how much you want to spend on each okay so this is work we\u0027ve been presenting at imc last year and some new stuff that hasn\u0027t been published yet so basically this is about TCP neutral congestion window so why would you actually borrow so basically TCPS initial window is the amount of bytes that you\u0027re allowed to send in the first round trip of a new connection so basically boot steps the congestion window install start so with every round trip you will effectively double your congestion window so which will lead to as you can see in the plot on the left your congestion window ramped up faster and if you take a closer look at some performance measurements you\u0027ll see that you can actually have faster flow completion time for example so you might think well this is nice so why aren\u0027t we just "
  },
  {
    "startTime": "01:01:19",
    "text": "making it very much so well of course at the start of the connection you have actually no clue about your network so you\u0027ll be bursting or at least this Energy\u0027s people burst the initial window in the nonprofit work so you only know one sample of the RTT at start well and as it turns out depending on the available queue sizes that are in your bottleneck link this will lead to a lot of losses so we\u0027ll have a lot of retransmissions with isn\u0027t good so the question is basically what is it what is the initial window so well good thing we have standards for that if you take a look at the standards you\u0027ll see that well they\u0027re various things in the standard so at the start there wasn\u0027t really a lot of congestion control only in 88 fannia Krypton proposed well use an initial window of one this was some standardized then was an experiment standard that serie used to - for then people started measuring it then it was actually standardized and most recent standard says well you couldn\u0027t use an initial window of ten segments and this is also already in the Linux kernel since 2011 but basically what I would say yeah we don\u0027t know because well do people actually follow the standards so what we try to do is to figure out how it looks like an ipv4 to do so we actually wanted to contact all available ipv4 hosts and so the question is actually how would you measure the initial window and we do that in the following manner so on the left is our scanner on the right is the host that we are going to probe what we are going to do is we are establishing a regular TCP connection by doing a three-way handshake but we\u0027re in announcing a very small segment size because in many implementations the initial window is a multiple of say segment size so we won\u0027t need a lot of data later on and we\u0027ll be announcing a very large receive window for the flow control because effectively a TCP will send the minimum of both the congestion window and the flow control window and to be never limited by flow control we just announced a very large one well after we\u0027ve done the three-way handshake we\u0027ll send a request to that host in hope to trigger a lot of data in response when the host will start sending segments we won\u0027t acknowledge any of those because when we acknowledge them it will effectively increase the congestion window which we don\u0027t want to do so we\u0027ll just not acknowledge anything and after a certain time the O\u0027s will stop sending any more frames because it\u0027s just the initial window is full and we\u0027ll have a timeout and a retransmission of for example first segment at that point can actually start establishing the initial window by observing the sequence number space of the packets that we got however you might see that there are some problems in doing this for example yeah what happens if some segments are lost this is especially problematic if the last segment is lost because we can\u0027t detect "
  },
  {
    "startTime": "01:04:20",
    "text": "that so what we actually do is we just scan multiple times and hope for that over ten scans we won\u0027t have a lot of tailors and what you should also do is you shouldn\u0027t enable sex or the Baron or tailors probes or something like that but there are some other problems when you do it because how would you actually know that for example yeah but n segments were actually the initial window and maybe the house just did only have enough data for n segments so the next thing that we actually do is we actually start acknowledging data and if more data is coming then we actually know that the epoch that the other end was an application limited but was in fact limited by the initial window yeah so we basically implemented that in zmapp to scan all of ipv4 on with HTTP requests and with GLS client hellos and well when you do this the following picture appears as you can see all of I P V force basically dominated by four values it is initial window of one two four and ten the first thing that you\u0027re probably noticing is that TLS and HTTP distributions do not agree there are more hosts using TLS on an initial window for them compared to http where you\u0027re seeing more on 10 as it turns out after we ran these scans our inboxes ran over because people were complaining that our scanner were appearing in the access locks this finally only happens with HTTP not with TLS because people don\u0027t lock that for some reason so we actually took a look at do we actually need to scan all of ipv4 to derive these values and well as it turns out for at least our measurement was enough to scan one percent of ipv4 to get the same distribution well now you might say okay it\u0027s performance parameter why would I care how is some host in ipv4 is configured so let\u0027s look at the services we are actually connecting to if you take a look at the Alexa list of popular domains so here now the plot is with a logarithmic scale so don\u0027t be confused you can still see that two four and ten are present but now the share of 10 is a lot higher so it\u0027s around 80 to 85 percent depending on HTTP and TLS you can also see that both distributions start to agree at least on the RFC recommended values you can also see that there are some custom or larger values generally we saw that actually these initial windows that were standardized a couple of decades ago are actually present and access networks and if you connect to these apiece manually you see well this is some gateway in some ice B or something but the next thing we were asking us ok how does it actually look with the infrastructures that we\u0027re contacting the most if we are getting "
  },
  {
    "startTime": "01:07:21",
    "text": "data today and so we took a look at content delivery networks and to do so we use the HTTP archive and got a lot of list of URLs that are hosted on CD ends and as we have now a lot of data and we know about that we can use regular sized segment sizes and we use again HTTP and also window scaling option because well some of them send a lot of data in the neutral window so what you can see here basically is an overview of what we got so I know my CCD ends basically the blue box on the lower left is what the current standard says and as you can see for example C D and B has a ten times higher window than the current standard however most of the CD ends are below an initial window of 50 further you can see that if you take a closer look there are some Syrians in there that actually customize the initial window depending on which customer or what services they are actually using so there are some that use 16 and this for other customers they use 32 for example ok this brings me to my last set of this talk so what he basically saw is that the distributions are generally dominated by the RFC recommended values there is still a lot of initial window 2 \u0026 4 in the all of ipv4 popular hosts seem to have updated to initially we also find some customization we find some very large initial windows and it seems that Syrians are far beyond current standards and when you look more closely unless you can even see that some customized depending on the access network that you\u0027re using to contact the source you can get data on our website and well I am happy to take questions I bet there was a presentation - I have TFS to go about it was actually about measuring ACN but it also instantly measured - iw 10 on and it got personal up the results but didn\u0027t go into as much detail as you however it did also measure from about 50% of its vantage points with our mobile networks and I think all the all this fixed isn\u0027t it this is all fixed so for our university network so you the data is available if you I can post the URL on the less often and almost bitten and the papers and things if you want to compare Thanks okay let\u0027s take the one more Stephens tries ripe ncc I will be looking forward to your ipv6 results I\u0027m part of the reason I\u0027m asking that question well ipv6 enumeration a bit tricky like we can talk about scanning the v6 Facebook since you\u0027re talking specifically about CD ends a bunch of "
  },
  {
    "startTime": "01:10:23",
    "text": "the CD ends do do pretty aggressive MSS clomping and I\u0027m wondering how that affects your results does that\u0027s why I asked the question a bunch of the CDNs do fairly aggressive MSS clamping so I\u0027d be interested to see how that effects your results yeah well so we further looked at the CD ends and we were wondering how they were actually delivering the initial window and well we looked at them and some of them seem to pace at it and not all if but yeah okay so I\u0027m gonna pull up your next presentation which is a first look at quick in the wild and I think we\u0027ve got one minutes total including questions only 15 minutes ago oh well so I just start so basically we would only be presenting this next we got the Pam conference but as I\u0027m here today I thought it might be of interest to all of you because quick is the hot thing right now yeah so this is joint work with colleagues from binocs and to Berlin and well I don\u0027t need to tell you how awesome quic is because you all know that basically there\u0027s Google quick currently out there and you\u0027re standardizing the other version and we were asking ourselves how much grew quick is actually out there to answer this question we basically want to answer these three questions so what infrastructure is actually out there supporting quick is it practically used by any website today and how much traffic is quick today in the Internet to tackle the questions yeah we again look at ipv4 and performs image scans further we also look in zone files of the matter in arc zone as well as on this Alexa lists and further we take a look into traffic shares in a university network in a major European to one in its mobile network as well as in an IXP I\u0027ll start with the first question so well - the foundation of that question is basically how I going to find quick capable servers in the Internet and luckily for us quic is a version negotiation feature so to be evolvable in the future it basically worked like this so a client sends in a first packet only client hello or in the initial in ITF quick it\u0027s version and if the server supports it it continuously handshake however if it doesn\u0027t it will send a version "
  },
  {
    "startTime": "01:13:23",
    "text": "negotiation packet and that version ago she ation packet will include a list of versions that the server supports so what we basically did is we wrote a Z map module the test for that so we will send developed client hello however we are going to include a version that is very unlikely to be supported by the other end so if the server doesn\u0027t support the burden he should if he is implemented correctly sent a version negotiation packet so we can from packet structure deduce that they end house is capable of doing quick and further we also get a picture of the version that it actually supports well so we\u0027ve been doing these observations ipv4 since August 2016 and I\u0027m going to show you now data that covers roughly a bit more than a year so until September October last year so what you can see is starting from August 2016 to one year later you can see that the number of quick capable I piece more than tripled to roughly 600,000 that piece as I said quick is now virgin eyes so we can further take a look at which versions were supported when and how vibrant all this quick landscape actually is and if you color the plot depending on the versions that the host actually announced you\u0027ll get the following picture so basically what you can see is that it\u0027s very colourful and furthermore that there are some versions that fade away over the time but if you take a very close look you will see that they\u0027re actually a version that are quite stable all over the period that we observed so for example version 35 was available in August and is still available in October of last year yet given the colorfulness and this a lot of changes the question is how will the future quick internet actually look like how often will we change versions and as you all know updating systems in the internet tends to be quite challenging so the question would be if versions are deprecated if you\u0027ll be creating islands in the internet of versions of quick that well your wouldn\u0027t be talking today anymore so the next question is that we ask ourselves who\u0027s actually operating these IPS to answer the question we took a look at the a SS in which these IPS are hosted at certificate data and reverse DNS entries and what you\u0027ll find is that for roughly 53 percent of the IPS you can actually Bute them to Google well that\u0027s another large surprise I guess for the rest as of October last year we were able to attribute them to Akamai and you can actually also see that the growth if "
  },
  {
    "startTime": "01:16:23",
    "text": "you see in that plot is due to Akamai so in August of 2006 in there only roughly at a thousand visible a piece to us and as of October as of November the same year it\u0027s already 44,000 and then continues to rise so that still leaves us with around 6 to 7 percent of hosts that we couldn\u0027t classify that way so we did HTTP request via TCP to find out what the remaining ones are and first thing that we notice we get many timeouts so they\u0027re hose that they are exclusively running quick further we\u0027ve found more Akamai and Google servers that we couldn\u0027t attribute using the classification from above and we also found roughly 7,000 Lightspeed web servers and this is web server that announced or included quick support in August last year and also roughly 350 Kenny web servers which is a web server that\u0027s based on the quick go library well if you\u0027re interested to see how that continues to evolve you can check out our website and there\u0027s more data and we also publish all the data on a weekly basis yes so this continues well now we have infrastructure so is that infrastructure actually used by websites to investigate we look at the dot-com end up not in the arc zone which roughly contains 150 million domains and unluckily for us there are no tools out that actually investigate quick usage so we had to build some that are efficient enough to look at this number of hosts and we built in on top of the quicker like I just mentioned and we but if I did slightly so that we can basically trace the handshake in a very fine-grained manner to dump all the connection parameters actually established when you are doing a connection you can also get that well basically now we have an efficient scanner for these domain lists and we can then further analyze the connection parameters so for example do these house actually live a valid certificates for the domains that we are requesting again this is stater as of october last year it\u0027s just for completeness so let\u0027s focus on some of the data so from all these 150 million domains you basically see that there 160,000 domains that are hosted on quick capable infrastructure and well today when you would build them with your browser a lot of them wouldn\u0027t work because they don\u0027t also valid certificates for these domains so roughly only 3000 certificates were given to us and well for example if you have a Chrome browser it works like this you will be first visiting the HTTP or HTTPS variant of the website wired TCP and if you find a certain header in the HTTP header that will tell you that this "
  },
  {
    "startTime": "01:19:23",
    "text": "host actually also supports quick and you will be using it however even for the valid certificates we only found a very small fraction that actually announced this header so in practice you wouldn\u0027t really be using probably a lot of these hosts using quick today or as well October so the next question basically is given that there is well not a lot of practical quick out there how much quick traffic is there in the Internet to answer this question we have to get a step back so how do we classify quick traffic classifying quick traffic is a bit hard because everything is encrypted and yeah I mean you\u0027re just saying garbage apart from some packet numbers is something well so we basically relied on the port based classification so everything that we saw basically on UDP port 443 it\u0027s quick so and we are going to classify that as quick which will give us an upper bound on the quick usage so that might be a bit lower the same TCP port for HTTP and poor lady for HTTP and depending on the data so that I\u0027ll be showing you we have a s level information available to see who\u0027s actually calling that traffic okay we start with the so-called nabhi trace which is an open data set which basically provides fifteen minutes each day at 14 o\u0027clock of CAP packets and the source and destination has been anonymized however we can still analyze and this is what you get when you do that for the first 10 months of last year and what you will see is here in real quick in red HTTP and in yellow HTTP and what you can see is that there\u0027s no Google there\u0027s no quick traffic as of January last year even though we found that Google said well we activated it in January for most customers so these don\u0027t seem to be most customers well but as of March we already see on average four percent of quick traffic and as of September roughly seven percent yes this is an open data set we can continue doing so so there\u0027s more data again on the website so and if you look closer into that small picture you will hopefully see that it still continues to rise and some Peaks around 30 percent of quick traffic in that trace okay that still doesn\u0027t answer the question who\u0027s calling that and for that we took a look in a major European ice P and we will basically lucky to get an in my sled flows of all of their border routers and we got them for one full day in August of last year and it basically contains all of the upstream and downstream traffic so including their each customer\u0027s cell your customers and the backbone traffic and the IPS and that "
  },
  {
    "startTime": "01:22:25",
    "text": "trace have been replaced by the area\u0027s numbers for us and here you can already see the daily pattern and what you can see now again the same coloring as before however we hatched the plot depending on who caused that traffic so we saw that Akamai and Google have capable infrastructure so we specifically looked for those two if you take a closer look so here on the left is a relative plot of this what you can basically see is there is quite a stable clickshare all over the day and it is roughly at 7.8 percent plus minus one all over the day further HTTP and HTTPS are still dominating with roughly forty percent each and most of the traffic that we see also you\u0027ll see stars in that plot and that means it\u0027s Google traffic or it\u0027s coming from what to Google in fact actually Google is capable of pushing up to 42 percent of their own traffic with quick which averages at roughly 39 percent but we see close to no Akamai traffic in that trace so point 1 percent still if you take a look at the very HTTP and HTTPS traffic Akamai cause a lot of the traffic so given that they have a capable infrastructure this potential future quick traffic when we take a look at the mobile network of that ice P the first thing that you probably notice is the daily pattern looks different so people seem to be using a smartphone all over the day starting in the morning and you can again see that there is quick in it and here the quick share slightly later than for the whole network it\u0027s at around nine point one percent with a slightly higher deviation and Google again dominates the share and they can push up to thirty four percent of their own traffic with quick in that network to run that up we had a look at a major European IXP and we got the same quality of data for the same day in August and so we got the flaws annotated by the customer port and you can basically get the same image again the first thing that you is that there\u0027s a lot less blue here so quick is only roughly at 2.6 percent furthermore if you would now start zooming in on that plot you\u0027ll see that actually Akamai accounts for 60% of the quick traffic here and Google only for roughly 33% honestly we have no real idea why this is the case we just assume that both companies seem to have some different traffic engineering strategies so it seems to be more available for Akamai to use the XP or less available for Google to use it we don\u0027t know so to summarize quick is on the rise "
  },
  {
    "startTime": "01:25:26",
    "text": "there\u0027s a zoo of versions which still at least my point of view questions future compatibility more and more infrastructure is actually enabled but there\u0027s only a small set of domains that actually maps to this infrastructure that can practically use it even though we see nevertheless we see that the fraction of Google traffic is already quite high it\u0027s very vantage point specific and there are singing companies which have quite potential to increase the quick shares given their infrastructure and it also challenges the question how quick traffic actually impacts Internet traffic as a whole ok thank you very much yes ok we got we\u0027ve got a few minutes for questions and comments if you have them one of the things I particularly appreciated about yawn bringing this work to us is he and his research teams one of those from IMC last year the initial window talk but this is this is upcoming publication next week at Pam so you you\u0027re one of the First\u0027s to see it yeah admit retake enough of lights be just a light speed is built l ite any other typos all right thanks wait in the job I think there was a question about the availability of the of the data I guess your own measurements might be available or terrible yeah so our measurements basically on we cannot make available the I expand the ISP traces obviously it will be the Nabi Tracy can get on that website it\u0027s not too hard to detect quick in there we cannot publish the TLD stuff because the lists are unknown NDA but the ipv4 stuff we publish on a weekly basis so every Friday I guess the scan is running and on Saturday the data should be on the website okay thank you so next because Torsten talking about the server push thanks for you traduction yeah so I would like to talk about the adoption the human perception and performance of HTTP to server push so just a quick reminder what are the major changes of HTTP 2 in comparison to HTTP 1 it\u0027s a binary representation not an ASCII representation anymore we should use only a single TCP connection and we\u0027re kind of screams on that single TCP connection that can be multiplexed and prioritized and we have had a compression this list is incomplete there\u0027s flow control and as a stuff going on and there\u0027s one particular feature this code that caught our interest and that is server push and among web developers is often regarded "
  },
  {
    "startTime": "01:28:27",
    "text": "as a key feature for h2 so just to if you a quick overview what pushed us remember how your brows are back in the old HP one day\u0027s request at a website so you\u0027ll request the base HTML document the browser starts parsing the document it will detect a stylesheet it will detect your JavaScript and then it will issue new requests to the server so we serve a push the server has the ability that upon a client request it can push these resources to the client without an explicit request for that so these this technique then saves requests and ultimately round trips and so this relief should improve the performance so ever the standard itself does not provide any strategy to what resource should be pushed and when they should be pushed and moreover in comparison to the other features offered by h2 this feature needs active configuration so you get all the stuff on the on the top for free by just upgrading your server software but but server push itself has to be actively configured and this is why we say that if you see so I push in the wild this is an indicator for true adoption to a new protocol because this features that could be configured and this act then motivated our study to look into who\u0027s using server push and what is pushed so first of all before we can look into what\u0027s being pushed we need to identify HTTP to a capable websites and for that we use the Alexa 1 million list and the complete set of dot-com dot Network or domains and again because I work at the same Institute as young we are huge fans of Z map and a built-in scanner for the ipv4 space and we explicitly look for TLS and application layer protocol negotiation and its predecessor next protocol negotiation announcing HTTP 2 and given these two datasets we then utilize the HCP to capable library it\u0027s the ng HTTP 2 library and try to see is actual content delivered via a cheapy tool so we define this as full HCP who supports because for one for some web sites you see that they say I speak HTTP 2 and then you\u0027re redirected to HTTP 1 so we define full HTTP to support if the web site actually delivers content I our HP 2 and we instruct the library to visit the landing page and follow-up to tell redirects and we distributed form ultimate to merge the workers in our network and also publish this data on a regular basis on our website so for small domain let\u0027s we do this on a daily basis and for the larger two main sets we use on a weekly basis so to give you a quick look at the HTTP to adoption at a glance this is just in here for completeness I would like you to focus on the top right right now so over the last year from January 17 to this year\u0027s general we see that the HTTP 2 adoption on the Alexa list has doubled which is quite nice and the users of server push head has also increased by a large factor however if you have a look at the absolute number out of 220,000 remains on the Alexa list only thousand used server push and we "
  },
  {
    "startTime": "01:31:29",
    "text": "see the same on the dot-com Metro Court set so they are also the adoption of 8gb to has nearly doubled we have over alpha 11 million websites speaking hep-2 and five thousand websites using server push back in general at 17 we saw that 7,000 sides we\u0027re using server push however there were six thousand five hundred websites registered by a domain partner they were all using the same template and if you take them out of the equation you have only roughly 560 websites using server potion so given given this data we ask ourselves so we see okay hup-two is out there server pushes out there it\u0027s not that much use than HP to itself so who\u0027s driving the adoption there and we do a case study for the Alexa 1 million list and we see that out of all websites that speak HTTP 2 on the Alexa 1 million list most of them are hosted by CloudFlare and when looking into the web sites that use server push we again see that most of them are hosted by Claudia and then we see a drastic increase that we also Akamai\u0027s on the list so if you have a look at this over time you get the following picture on the lower left you see what I just said we see a rising adoption of h-e-b 2 on the Alexa 1 million list and and also an adoption of HCP 2 and what a server push and I would like to focus on two special things in their lower right plot you see a drop in last year September so what happened here so we looked into that and saw that unfortunately Claude Lee had to temporarily deactivate server push for our customers because a lot of customers were for example using WebSockets over HTTP to and most as you might know is currently not possible so there to take down this feature for maintenance and if you have a look at this rise of in this year\u0027s February we see that it\u0027s also caused by CloudFlare because it seems that the law of users that hosts their web site hopefully I use a certain content management system and this content management method system called HubSpot had has recently done a software update and they introduced pre-law Harris so preload preload is used to tell the browser this is some content it might be you want to fetch very early because he will need it later on but 12-layer uses this preload header to identify what to push to the client so we see a rising adoption here so we next focused on how much through the website\u0027s push to the client and and this is the picture for last year so in general we saw nearly 600 web sites utilizing push and we see that around for example if you take a look at the Left plot 80% of them push 20 resources to the client if you take a look at the share of resources plot that are the resources set up push your bill to the client and these are the resources that are on the same origin then the landing page we see that for some websites they push everything they can push and we see that the top 3 content types are JavaScript style sheets and images and here\u0027s an updated "
  },
  {
    "startTime": "01:34:31",
    "text": "view on that from this year we see further for the first part not much has changed there however on the for the mime types we still see that javascript insurgents dominate dominate the the top list and there are less images in comparison being pushed to the client so now that we see pushes out there is it it\u0027s used and actually there\u0027s some stuff pushed to the client how does this impact the overall performance because in the beginning I said on papers this should improve the performance so what we what we did was we configured a Chrome browser to automatically repeat with you visit these websites and recorded performance metrics first of all we recorded the page load time and we take the time between the connect ant event and the load event start and have you look on the Left plot there you see actually compare the HTTP to version of the web sites using server push to the HTTP one version normed to the HTTP one case so the red line on the left is the portion of web sites that improved in their web page load time will annoy upper right in this plot you see the web sites that suffer from using hep-2 with server push and to identify if this caused by server push we looked into please have a look at the right picture where we compared the websites they are HTTP - version without server push because the client can deactivate this compared to the version with server push and again it seems that some websites improve using server push but some websites although they might improve only using other HTTP - features like screen multiplexing or header compression they suffer from using server push yeah you can argue ok page load time is a very technical metric because after the onload event there might be a lot of other stuff going on and therefore we also looked into the speed index the speed index is a metric defined by Google and it basically measures how quickly the page contents originally populated and I won\u0027t go into much detail here but it draws a same picture so we then try to map these results to the results we saw before so how much\u0027s were pushed by the websites what content what content is actually pushed and how large the content is but we could not attribute this to simpler reasons so to it to the number or share of objects so we next ask yourselves yeah these are all technical metrics and we saw inconclusive results but in the end it\u0027s people visiting websites so do people even notice what happened that there is a change going on and for that we conducted a user study and basically we created an online questionnaire where users had to view side by side comparison of the loading process of a website and I want you to have a close look at the baby that looked like this user visit the websites they see set-aside videos of the loading process of different versions so the left version here is actually the HTTP 1 version and the right version is the website version using HTTP 2 web server push and we ask the user to say which "
  },
  {
    "startTime": "01:37:33",
    "text": "were loaded faster and I won\u0027t go into all the details of the results here but what we saw was there\u0027s a strong agreement among the users so in nearly 60 percent of our tested scenarios more than 75 percent were in favor of one version of our website so they people could clearly tell what caught the interest in what what was they perceived or perceived faster for over 36% of the websites more than 80 percent said I liked the push very and more because it was was faster than any age to variant without Saba push however for a non-material amount of websites we saw that some of the users were even in favor for the version without server push so the key takeaway here is that server push can lead to human perceivable and negative performance and after we did this questionnaire we talked to the people and try to identify reasons and also looking into the rendering behavior in the browser of these websites and there are various reasons that may have caused the decision in the end so for example like I said we see that a lot of websites even even benefit from other hep-2 features like multiplexed streams which lead to less head-of-line blocking we see that if you push too much resources before the debates document the browser is delayed in processing the base document and that\u0027s the overall performance suffers like and we also saw saw some real effects there were some websites configured that they push resources that are not actually referenced in the web sites or they are not used but as we always use a cold connection to a server pushing more and larger congestion window and then ultimately the actual page was loaded faster so as a takeaway here is if you want to use server push for a website you should really know the website you should know how the browser behaves and you should know how the server behaves and I would like to before I come to and quickly show you what we are currently looking into based on these results we saw in the technical analysis and in a user study we are currently analyzing server push in our control test bed so to remove the variability we have been measuring it in the internet and we using we web websites and be using the Mahamaya tool for that and what\u0027s cool about that is that it tries to replicate the reword website structure so it will spawn multiple namespaces for third party resources and for digital resource so you\u0027re not just saving a website so local server tries to replicate how the websites look in the internet and we testing various strategies for server push there and some early results are you shouldn\u0027t push everything you could push on the website don\u0027t push images at least not large images and you should just push in a push enough to fill the initial Network idle time because we see that after the browser receives the pushed file and then the initial bytes of the HTML most browsers at least in our tests were fast enough to then issue all requests for resources what we\u0027re also currently "
  },
  {
    "startTime": "01:40:35",
    "text": "looking is in to an alternative scheduler for an HTTP to a little bit web server and we try to push the right resources at the right time and what we do there is that we identify resources that contribute to above default viewport so the stuff you see if you open the web site and we try to interleave these pushes with the base HTML document so we instruct a scheduler to just push the first bytes of the website stop pushing the HTML and then push resources like style sheets I mean we already see that this can lead to promising results for some websites and our test but however still this depends on the overall structure of the websites the number of serve party continental and ultimately the browser behavior so takeaway here is unfortunately we cannot give you a single generic guideline for server push requires web site specific tuning and configuration and so to conclude my talk what\u0027s cool is we see a rising adoption of HTTP 2 and a lot of features they are like the multiplex stream stream prioritization you can really improve your website and we see that every now and then there can be a drastically increased by just updating server because all major browsers already supported and regarding this server push feature I still think that it\u0027s a very cool feature however you shouldn\u0027t just switch it on because it\u0027s not no silver bullet to improve the website performance we\u0027ve seen that it can lead to you receivable negative performance and up to now I would say it requires a lot of deep understanding of the page loading rendering process in the browser so I think we should all agree on that we need best practices and guidelines for the use of server push and with that I would like to thank you for attention and compute my talk thank you so we have some time for some question faith yeah I have one chair related but in the portion about analyzing the Alexa top million when you\u0027re trying to determine what who\u0027s hosting the content there what is the strategy you\u0027re using you go from the Alexa domains to site names yeah so we started off by a I piece and then try to map into a SS and we are also doing on a regular basis DNS measurements and that we simply try to analyze the whole DNS chain and follow the C names and identify for example hosted by Akamai or Microsoft or whatsoever okay so I I mean this can be an incomplete list I don\u0027t think we have 100% coverage there but yeah um maybe um I guess I think I want to offer not just for your study but using passive DNS can get you the collection of fully qualified domain names underneath a domain and what I find a lot when I\u0027m studying it is they\u0027re very many Alexa listed services that use a many CDN simultaneously so I\u0027m curious to know what we\u0027re you know what subdomains they might be using server pushing or something that you wouldn\u0027t see just at the TLD I\u0027m Eric tiger now my fault actually follow-on to Dave\u0027s is there\u0027s probably a general thing from Google doing this class of measurements to "
  },
  {
    "startTime": "01:43:35",
    "text": "think about is that that especially as more sites move to s and I the kind of zmapp ipv4 scan let\u0027s go try it isn\u0027t gonna start becoming decreasingly useful because many things may switch their behavior based upon which they S and I they see and along with also as the ipv6 stuff is becoming more and more relevant there but also I think on David one about the Alexa one is one thing you\u0027ll also see is that the Alexa very much focuses on dub dub dub style sites there ends up being an increasing amount of content that is on separate domains for images videos etc that don\u0027t show up as well in that list the the other question side of this is for the when you\u0027re looking at the push performance behavior is you you showed that there are a number of different providers who are providing push did you break that out by them to see if that was a common across the board or was it the case that that some of those providers had behaviors that on general were negative and other ones had we\u0027re taking strategies that tend to be to be more generally positive so we couldn\u0027t attribute this to the provider itself it\u0027s more like figuration of the website so the user hosting the website at the provider so for example I said in beginning that push requires manual configuration and but there are also some websites where we see that plugins do this configuration like WordPress plugins they scan your your file tree and then CA there are some static files why not push them and so it\u0027s more like the configuration of these features then instrumenting the infrastructure to say push this and that influence of performance not so much the infrastructures itself right but I think there actually are starting to be a number of product features by various vendors that will go and analyze website behavior and then feed that feed that back into the bush behavior so you may see cases where where that where that kind of closed-loop let an out analyze I decide what to push has different very different characteristics of behaviors and stuff that is more astatically configured things here yes what Google first I want to thank you for publishing this work I\u0027ve repeatedly asked for good public evidence that push actually works well or poorly and it\u0027s really hard to get dated this is as good as I\u0027ve seen and I really thank you for that and I think it emphasizes the challenges of making push work in the wild which are real and we\u0027ve seen them at Google I mean it\u0027s possible to make it work it\u0027s a lot of work the second the actual question portion was have you considered evaluating cache digest and whether that makes things much better yeah so we will look into that as you might know today at the HTTP HCP this meeting there will be a talk for case arises and really look into it because there are still a lot of stuff that can go wrong with server push and it\u0027s the I think it\u0027s a cool feature but maybe it\u0027s it\u0027s yeah it\u0027s well it was standardized too early so maybe it\u0027s it\u0027s yeah it\u0027s understand that it\u0027s being used but I think without cache digest shouldn\u0027t be used for example so we\u0027re not having look into that but we\u0027re planning to do so Thanks ok thanks "
  },
  {
    "startTime": "01:46:35",
    "text": "so much so next up George smart dock is is he\u0027s gonna join us to present some to present some work that was it IMC in fall of last year about passively detecting black holing by listening in on the BGP hold on this once just one second we get this up for you okay okay thank you yeah let me start by saying that the knell of service attacks are still illustrate in every year becoming more and more dangerous either for profit or for fan or supported by national regimes and other countries so when a network is under attack even if a small number of the eyepiece and servers that host it in the network are under attack the network provider has to react because if it doesn\u0027t do so there is a problem that the overall performance of the network may suffer so one very cheap and available solution is to just drop all the traffic that goes to this targeted host and this called platforming and of course you can do better because you carry all the traffic all the way to your network only to to drop it when has already and Loonette and to achieve this you use something called bt people are holding so you trigger the network is a send in traffic to you stop sending the traffic because anyway I\u0027m going to drop it and what we do what we did in this study is to see by collecting data from different sources if we can actually infer this in network data and we can have some insights about who is using that how often how popular is this technique how popular is this technique in general so for the beauty people are calling when you trigger your network that sends with the traffic the network can also drop the traffic earlier and to do this one way which actually I would say the default way is to use PGP communities so when we advertise a prefix that is under attack you add the community field a value which is a 32-bit where the first 16-bit are notate the network that has to drop the traffic and the second 16 bits is a number that is used to allocate that this that all the traffic to this essence has to be dropped unfortunately this is not satirized there are other solutions and there is not a common dictionary that you can use for this now some "
  },
  {
    "startTime": "01:49:35",
    "text": "terminology the the prefix that will be black hole the black hole prefix the community is the black hole community and essentially by doing this you try to drop the traffic material including the legitimate traffic now the same thing you can do actually if you are in the next pin in the in this case the XP provides the black hole community to the users the members of the egg spin and by announcing the prefix to the route server with this community the members that participate the rhetoric and heard about this announcement and then essentially the axpy null dump that drops all the traffic in this case the XP is the black column provider for the black hole okay now let\u0027s see what is the difficult problem that we have to tackle in order to do this in reality as I said there is no common dictionary of communities I want with very smart people we try to find ways to create a dictionary for communities is almost intractable so the only thing you can do is you can go in case by case for example for black falling or for location or for other traffic engineering properties and use some of the keywords in order to search list in order to infer blah blah to infer commutes for example here I saw the level 3 being paid or did they explain it and then you can see that actually with simple data among things you can find what\u0027s the black hole for them so who did this have we found approximately 400 black hole communities 50 of them in ixps and then we use that to analyze passive BGP measurements in order to infer blocking activity so what we do is that we have a list of communities when we see an announcement with this community in some of the pits be collectors where we have access to we tackle this and with that also the starting time and then we wait until there is a withdrawal implicit or explicit in order to identify a platform event and then we do this everywhere in they didn\u0027t we have access to this type of data now let\u0027s see the trends in the black hole connectivity here I saw a signal the resources that we have used for this study of course as many other researchers academic research mainly we use driver out use but we also use PC aids which is the route server ATP feeds "
  },
  {
    "startTime": "01:52:36",
    "text": "as well as the bit species from alerts see the N and for the case of platform we were able to find three times more blood volume events from the CDN and appreciate the set compared to the other two right around use in total we use something like 13,000 PRI peace and approximately 3000 pas now one of the big observe essence is that indeed the use of the beauty people are falling on the rise so we expect that as the number of the DDoS attacks increases the number of providers are willing to offer this as a service increases and indeed the during the less than three years of this study we saw more than 250 percent increase in the number of providers that provide blackhole more striking is increasing the number of users by users I mean the networks that actually ask their providers to drop traffic this is a more than 400% increase and in terms of prefixes there was more than six kind of percent increase so the last years we have seen a steady increase with peaks that I will explain later and on average we have three to four thousand events per day at least from what we see from our data and these pigs are actually correlate well with high profile attacks for example attacks during the Russian coupe attempt during the Olympic Games and you see also a shift in the summer fall 2016 that correlates well with the Marais attack okay now the next question is how efficiently how efficient is the black falling in other words if I want to drop the traffic to a destination how many hops can I save by doing this using p2p platform so what we do is that we get a signal that the pressure is blat fault and then we do tracer out there and then when we get the signal that the blocking activity event is over then we repeat the trace route and I\u0027m going to see how many hopes in terms of five pitch or aliases we have saved and for this we use the right buttons so in this plot I saw that on average we save five IP hops and three AAS hopes which means that s seemed simple they\u0027re getting with BGP blah holy can save the three networks to carry traffic that has only to be dropped later and this is also on academic at least one of you a very "
  },
  {
    "startTime": "01:55:36",
    "text": "nice way to show how net we can collaborate with each other when there is an attack at the garden we\u0027ll spend maybe the last minute right have two minutes the last few minutes in order to show you some insight about who is using a block falling so it\u0027s not the big users it\u0027s not it\u0027s not very popular in many countries of course it\u0027s popular in the years because it has a huge ID but this particularly popular in Rosia and to become even more and more popular in ukrainian and in other ASEAN countries we have seen also that there is a huge difference between how many prefixes it\u0027s provide their black holes so there are some of them that just black hole a few tens but there are some the black hole thousands or tens of thousands and also in terms of blackballing uses again we see a concentration in Rosia who crania also Latin America and most of the users are cotton providers or cloud providers and this is somehow expected because running the big firms of servers so when we did the in-depth analysis of what is running behind that and who is actually affected by black hole we see that mostly our HTTP services but there are also other services that have to do with mail servers etc but most of them are in cloud providers and most of them most of the domains are Lauren domains so it seems that black holing still is a medication technique for the poor probably others have used other medication techniques for example scrubbing that is not visible in our data now I will conclude just with one interesting observation something that we saw many times is that some of the black hole events lasts for hours days we have seen that month but most of them last for a few minutes and this was a striking generation when we talk with people that did security they told us that yes indeed most of the attacks are not certain lasted they last for a few minutes and another thing that we found is that many times will be round like falling because you are not sure when an attack is over or they are not sure that you want to penalize this IP forever you just do an unlock so you just activated activate lock on all the time and this actually was very visible nor didn\u0027t okay I will conclude here and I\u0027m open for places thank you thank you because you\u0027ve been quick we have time for a few questions thank you hi Alexander zoom of Kershaw let\u0027s I just "
  },
  {
    "startTime": "01:58:39",
    "text": "when you get a little buzz in Mike please one more time Alexander zoom of curator let\u0027s I just want to highlight one thing that are unfortunately black hauling becomes a service it\u0027s not only at the dose mitigation it\u0027s also a service for censorship and especially in Russia the Collins is used to block some resources and it is quite popular so you see the platforms also used for censorship or for other yes yes yes so I do not want to say that you said that\u0027s fine some of the long plastic ones and these for censorships we have enough indications for that but you don\u0027t have a ground truth so I\u0027m with what they can say right now is that like if you see days of life falling and also we use the NHD and other tools but we can actually find out what this IP runs some of them are website of websites of political content yeah and it just just well you know it looks like like holding plus hijacks in this way okay all right thanks a lot George so now we\u0027re getting into the part of the session where we talked a bit more about tooling and less about measurement data there\u0027s some measurements without as well say first it\u0027s Kevin talking about how to use multipathing or how to measure multi passing with tracer is there a everybody can hear me okay so I\u0027m Kevin Vermillion a PhD student Sorbonne University and I\u0027m I\u0027m gonna present ongoing work in collaboration with ripe ncc and Stephen present in the room about the burning MJ traceroute and rabbit as probes so this is presentation that we\u0027ve made at kata for aims and so first of all I will give you a short recall about what is multi pairs detection algorithm and what its limits and today I will mainly focus on so the goal here is to provide a better nga and for that we have done and we are currently it is a an ongoing survey that "
  },
  {
    "startTime": "02:01:43",
    "text": "and I will present the first results that we have here under load balancers and the data that we found in our survey so for those with which are not familiar with what is multipath detection algorithm so it allows a user to discover all the paths between a source and a destination it in it is based on Paris trace routes so it is an extension to Paris trace routes it provides some strong statistical guarantees on the discovers ology but it potentially sends tens of thousands in packets to discover all the topology and for that it makes the worst case hypothesis that every discovered interface could be part of the doctor answer so what we\u0027ve done here to answer the problem of sending too much packets so the first step is to provide a survey and for that we have will send source 350,000 trace routes or destinations from impact IP at least and we\u0027ve divided this work between 3,500 lab notes so today approximately one Drennen thousand trace routes are computed and by computing I mean extract all the patterns that we can found in in the trace route in terms of how what diamonds to say contain in oh it looks like so so what we found so we define some metrics first of all the diamond lengths so here a very easy example where the max and equals the mainland\u0027s was 2 and the right distribution of the next lengths so we can see that mainly these diamonds are max trains 1 and also you can see that the power of 2 are are more current that not the total ends and that is one of the diamond that we found in the data so with a maximum of 17 so with the source and the destination corresponding and for us it is important to understand what\u0027s behind this so if anybody could explain us what happens in the network in Cyprus network operators here we\u0027re pleased to talk with you so it\u0027s like second Matrix that we\u0027ve defined this diamond width so the width and we provide also the distribution that we found it data so we "
  },
  {
    "startTime": "02:04:46",
    "text": "see that there is diamonds with very large max width and in particular this one that we\u0027ve found so yes so here is max width is 96 so in the destination and sources are provided so this is totally IP level and not without an alias resolution and the last one that is important for us because it helps us to define the heuristics to provide in better MVA it\u0027s a rigid symmetry so here in the short X on top of it two symmetries - mmm and we provides a distribution and what is important to see here is that the reduce image file / - where so for us it\u0027s very it\u0027s a good thing to take into account and here also so here\u0027s a max with symmetry is 39 and you see in fact that there is so in the red there is a source in green there is a destination and there is completely two divergent paths going into so one going into two clustered area and the other one going into a simpler path so here also will please to discuss with network operators to understand better what is going on here and the last matrix that defended the machine so and we are defining this on more meshing metrics - to also improve our heuristics so this is an example of so the this the the diamond on the right as a percentage of Michigan of 95% so it\u0027s what we\u0027ve what we have found in the data and the last one just to to give an example of when you enroll a topology of my hope the MBA uses eight thousand and fifteen five hundred packets to discover all this topology so that\u0027s why it\u0027s important for us in in their constrained environment such as ripe Atlas to to make this figure uh lower okay so do you have any questions yeah look it looks like well we\u0027ve got a couple minutes for questions if anyone has one anyone other than me thanks Kevin I think I I don\u0027t know the literature here I think it\u0027s interesting that to contrast what you found with this breadth of paths versus what we usually call the diameter of the internet right we usually say the diameter across the length basically the "
  },
  {
    "startTime": "02:07:47",
    "text": "trace we see a maximum length what what do you know what can you tell us about is this terminology with diamonds and things something new that you\u0027re creating with your project or is there an existing literature about it and can you compare so you said you used 80 well that\u0027s the first question just just is it your terminology and yes so there is a paper from aristocracy ah in the translation the networking which has done a survey also almost ten years seven or eight years ago and it defines some metrics that are present here like Lenten whiz but we have added the asymmetry and machine because for our eristic s-- these metrics are important all right Carlos do you think that this information or these techniques could be used by an attacker to discover weak points in the network in particular if there\u0027s a wide node you know that that could mean something or if there\u0027s a non wide node that could mean something and could you as an attacker try to target a specific load balance node and take it down because you got identified it as a bottleneck for example so well I know the security at all but well maybe I suppose so the goal here was to make a nice a nice map so by merging all the trace for an MDI trace roads that we get indeed we see some we see where as where is the Corazon network and maybe yes yeah Tim Chen thanks very much it\u0027s very interesting I regularly use a package called persona I don\u0027t know whether you\u0027ve heard of that for measuring loss latency in throughput between a large number of quite a large number of sites and one of the issues there is its trace task has no idea when the path changes whether it\u0027s in the local site the far site where the modes measurement nodes are hosted or somewhere in between and I think the interesting now is to understand just simply from an algorithm is it something in the local site is it something in the far site with ecmp or whatever or is it something in the middle then actually having just a classification of which of those three it is would be very useful in itself can you repeat the question I think one of the interesting things you could get from this is to be able to have some heuristics to determine when you\u0027re doing a measurement between measurement points in two sites using whatever package it is that does some kind of trace is it a change in the path in the local site is it a change in the path in "
  },
  {
    "startTime": "02:10:48",
    "text": "the remote site where the device is hosted or somewhere between those two sites if you\u0027re hearest it\u0027s going to be applied to that that would be a very useful thing for people use tools such as persona I think yeah so there is the heuristics at the moment don\u0027t take into account where is the load balancing is taking place Chaz is it at the beginning of the path or are the ended path but it could help it\u0027s a question that we\u0027ve that we\u0027ve without in any try to email you an example of what person orders and how it visualizes changes in parts and how your approach might help it okay thanks thanks Tim thanks Kevin for joining us alright or up to the last of our presentations for this morning and we\u0027ve got Olivier Timmons up will take us through our lunchtime or up to urlan okay hello everyone or just to start but by giving just a quick recap of what we have you know campus network because that\u0027s main motivation for this work so or operators so that\u0027s University campus never have two ways to understand what\u0027s happening net florets and some locks in the DPI or the forward that\u0027s about it know there\u0027s this thing that\u0027s been popping up a lot over the last few year and if you don\u0027t recognize the santa needs comes from there basically if we start to encrypt everything looking at net fruit racists or DPI locks or subtract this from be useful at all in order to understand what\u0027s happening with the network do I have a transmission diagnosis we are doing these kind of things seem to debates that\u0027s been happening the creep working group for just one bit in the clear does not make it look like it will be easy to understand about anything else about it and so basically we want to be able to travel through the network at our scale and we have no visibility of the traffic so the main inside we have is that the arrows know everything about the connection beads for TCP as of now but for quick answer your browser sees everything the clip before in cryptic and transmitting so if we had a way to instrument the host to get information out of it get exactly what we want I love it we would be able to troubleshoot anything the ratio if that is that the width this is that so far we have MIPS so the TCP MIPS for example to create contours to SNMP and again that\u0027s about it so we have an issue there we want to get visibility insights over the traffic and work its own habits what "
  },
  {
    "startTime": "02:13:51",
    "text": "we\u0027ve been working so far on well here the thing I\u0027m presenting here is a tool on which we have been working which tries to pro to instrument dynamically and host so computers and look at for example the TCP stack on the Linux kernel or DNS resolution routines in the Lipsy and do this at runtime by injecting some code that will just feedback measurements towards some user space demon once we have this data that gets extracted you can just explore it again over generic civilization format society such as IP fix you don\u0027t want to export anything you want to export surf that we\u0027re able to understand and that should be a bit that you should be able to apply to about any protocol so we extract transition for state machine surfing statistics about what has been happening in the TCP handshake and then what\u0027s when the connection has been established what happened over there we did we see less retransmission this kind of thing and we look for very specific specific bits of information that we get that we can extract directly from there so that\u0027s a high-level overview the tool and originally we wanted to stop there but then we deployed it and it turns out that doing a deployment is quite interesting because your own stuff about your network that you know did not expect there what so we push this on every single machine that the computers that students views in our labs because the computers the student mini boroughs Facebook Google and this kind of stuff we have three C\u0027s toward these services these are mainly TCP and DNS traces because well quick is still a bit hard to instrument so far that\u0027s a work in progress and I\u0027ve collected here data from the past month and maybe just as a final comment we have a network that\u0027s vastly of a project provision so we don\u0027t really see a lot of big issues but we still have a few so this is a plot that shows the amount of flows we got over pv6 and ipv4 and so as I said earlier students rows Facebook and Google and so these are dual stack services that\u0027s the delegate we get from there and the ipv6 peak for all the next graphs the colors should match for pv6 on ipv4 and you will well get forms every 12 hours over the month I care about the people for an ipv6 in this particular example because in our network these are routed over different epochs so these may have exhibit differences and so the first performance spawn that I tried to look into us did we have connection experiencing word gnosis so in this particular case where that connection that we\u0027re losing some TCP "
  },
  {
    "startTime": "02:16:53",
    "text": "scenes for example when we\u0027re trying to establish a connection and how is it comparing course across IP versions well that\u0027s a result here so we have actually some connection so that\u0027s a tiny amount where it\u0027s distributed evenly over ipv4 and when you look at the destination addresses that were targeted by this particular connection these were uniform so without any particular bias towards a given destination nothing was happening over v6 if we filter the data and check only connections that had more than one loss in the number of scenarios so we have something in our own network that appears to be I don\u0027t know overloaded or dropping some things we don\u0027t really know and it\u0027s low in small in small enough that when we query our network administrator\u0027s back we don\u0027t even see it because that doesn\u0027t appear in there up there well it\u0027s too small to be flagged in their data logs so that just one of the corner between kind of no ties it that we did not expect that\u0027s a super simple parent once you are able to get data directly from the annals how\u0027s this one second Spencer did you have something that I need in interrupt here I\u0027m Spencer dawkins I\u0027m the responsible area director for quick among other things and I just wanted to say that I find this presentation fascinating thank you just make sure we also looked at well connection that which was failing and there well the work the network works and that\u0027s what\u0027s happening here we have a small spike there that we investigated and actually all of these connections were linked to some weather applets that was running on every student\u0027s computer and that was do me that basically we didn\u0027t we didn\u0027t know what\u0027s actually over there and doing queries so I have to clean up a bit the network if we move from just a little less initial TCP since we can look at TC PRTG so these are media smooth our duties as extracted from the Linux kernel directly and compare it or over for example ipv4 ipv6 and across from providers so students use microsoft services to get emails to use a la the online office who so we know we\u0027ll get data over there well Google that\u0027s how they work that Google depression but if you look at your TT over there well we actually again have no clue as to why do we have such a vast difference between two services that are used to be presenting Parvati versions so that\u0027s yet another data point we need 20 BC I don\u0027t know you know or network "
  },
  {
    "startTime": "02:19:55",
    "text": "and especially because we are starting to speak here but latency about RTT that\u0027s something that we well at this or opera Torah had no knowledge about because I could not track it tires bike we don\u0027t know why but it\u0027s fairly loud if we start to compare jitter across TCP jitter across address families so remember and before in v6 our rotate over dedicated separate up links again we observe surf we actually really need to take a deeper look at what\u0027s happening now in our own network so far all have presented our results that are tiny that currently have no widespread impacts because because people are not complaining that much but looking at all of the data we analyzed these were kind of spikes so the track that expected and if for network was to be more more used I don\u0027t know this could start to raise issues I conclude the talk we just recap us something we actually had in February and so for once we got complaints so students were having issues to reach the online course platform so Moodle and varied wrestling while it\u0027s slow it takes way Malick\u0027s wave slower than usual and so how do you get run troubleshooting this problem if you just have daylight but that you can pro sue and that\u0027s very simple you know that well do DNS query so value use our own DNS resolver so we just we can just look at for example the time to establish well the time it gets students the time it took students who get a DNS reply furnace from the servers that we know are co-located in the same data center that web servers and compare it to the time to establish at that time to get the cynic firm the TCP load balance over there the few ties were in this part of the data the data center was 200 meters away from the post instrumenting and so I guess at 30 milliseconds for just 200 meters Prairie not that great once the program get noticed well that was a load balancer that was over row there etc so that\u0027s upset but ethically disappeared so we know it must need to redress that because the Pearl is fixed and so just as final thoughts we build a genetic approach which actually did not get into detail here because there\u0027s that\u0027s academic formal stuff before state machine reduction these kind of things we have a prototype that instruments tax that students currently use on or network because that\u0027s what you care about we have a prototype for quick but there are technical challenges to deploy it over there but the main question and asking is is there interest for a kind of an even base reporting monitoring mechanism for transport protocols "
  },
  {
    "startTime": "02:22:55",
    "text": "especially if we start to encourage because we don\u0027t we can\u0027t have a middle box sitting in the network decoding everything that\u0027s visible so how do we get around extracting the data and analyzing it that concludes the talks if you have any comment or questions or maybe hints as to why we have a natural Goethe\u0027s crazy behavior I don\u0027t think so let me um yeah there\u0027s a question back there we\u0027ve got about seven minutes so we may be able to answer those and then I\u0027ll close and I think that Mike might be off back there Tim do you mind if we have him so I\u0027m just a quick question is the code available openly on get helpful something was it private at the moment if you want I\u0027d rather keep what we have but I\u0027m very open to just debris elsewhere and compare data data data post all right thanks for maybe shoes we\u0027ve showing the data set up because there\u0027s service implication for the students their insecurity you what are you using the C group EPF filters I\u0027m sorry what are you using you said EB PFA is if the C group filter so that\u0027s that that part some targeting so far Linux machine and so I\u0027m putting get Brooke so probe tracing mechanism in the kernel attached to whatever socket goals CC socket so you see socket calls no no I don\u0027t use well I can how to make it simple you don\u0027t have to make it simple for learn so just say what it is so I\u0027m I\u0027m intercepting any code doing in the kernel on any function and I mean able to Denver an arbitrary good sure okay so yeah so then my question was my guys actually is what does this give you four quick it shouldn\u0027t give you anything you can do that for uses this process as well and so that\u0027s what we did for an RTP library from a new point where as well as long as you get the symbols we thank you so basically you\u0027re saying that you you have to do something like either LD preload or whatever to attach not even actually so we can set up the kernel such that any which user space or kernel space call matching some symbol we will have a soft interred attached to it so and so your interpretation depend "
  },
  {
    "startTime": "02:25:55",
    "text": "like only works for a given version of the code is actually a change we have a creek which is that we have 12 implementation and so if people start at well it\u0027s literally a target for the for the owner or or network so we know that students for example here in our infrastructure have chromium we have the symbol we can instrument that one if you want to move to another implementation we need to make it compatible and the strict struggle here we need to define what would be a better way that to perky\u0027s basis but have a generic way to instrument it but like it could break between chromium 65 and 66 right I\u0027m sorry what it could break between chromium version 50 65 and version 66 yeah but since we\u0027re on the machine we own the version that\u0027s there when we control everything so again alright okay thank you let\u0027s throw out the last two questions Neal Cardwell Google I just wanted to offer a conjecture on the mysterious delay spikes in some of the graphs I noticed that um and some of the TCP RT T\u0027s you\u0027re seeing whereas suspiciously similar to delayed ACK values on common operating systems of 200 milliseconds and 40 milliseconds so that would be one thing to look at that one was 40 and that one\u0027s 200 that\u0027s one I don\u0027t know if he looked yeah I think you said you wouldn\u0027t offer some insider commentary about the spikes right yeah that they might be delayed ACK timers firing at 200 milliseconds in this trace and then 40 milliseconds on the on the next page would be something to look at designing hurt you Berlin thanks for the very interesting talk I\u0027m sorry if I miss it but where can I find your code so if you want to run it and do it on your own infrastructure I can give what give you links privately but so far it\u0027s not finished I can\u0027t make it available ready but I\u0027m free to what I\u0027m fairly open actually to just collaborate because use more data more start to reason about thank you love it so just in closing you might have noticed the last two talks were a little different than the earlier ones if you didn\u0027t notice the way they were different is they\u0027re a little bit more about the measurement tool or the strategy and in map RT we specifically say we want to bring measurements that that provide insights to the engineering of the protocols or the operation of them so if you have thoughts about that you can share them on the list or with us but we basically suggest that you can only come and talk about a tool if you brought a novel measurement for us thanks thanks for joining us for the meeting and we\u0027ll see you in Montreal we expect yeah so thank you for being so much on time and if if you have any feedback on the agenda in general what you would like to see what you didn\u0027t like please let us know or send it to the main Eunice thank you "
  }
]