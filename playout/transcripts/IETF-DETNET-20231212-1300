[
  {
    "startTime": "00:00:12",
    "text": "okay it's the top of the hour so let's get started uh welcome everyone good day I'm yanos farkash and co-chair the that networking group with Lou Berger uh unfortunately L couldn't make it uh this time so I will be the one uh leading this meeting I'm affiliated with Ericson and we have our secretary Eve Schuler many thanks to you for doing a great job uh since you have found uh your way here uh you figur the logistics part of the meeting I would like to remind remind everyone of the ietf not well uh these are the rules and policies under which we conduct our activities in the it PF please check the details like there are a lot of references here if uh you are not familiar with them I would like to draw attention specific attention to the part that all the contributions that is all what you say here for example become part of our permanent records and by participating you acknowledge this uh I would also like to remind everyone I mean the the group is great just to mention that uh of our remind you our contact guidelines so please behave respectful and professional uh to others uh please uh uh mute yourself when you are not speaking and um headsets are recommended like we are I guess all familiar with the virtual meetings uh by now and um"
  },
  {
    "startTime": "00:02:00",
    "text": "as I mentioned uh I suppose everybody is familiar with the logistics since you have found your ways here but I would like to uh remind everyone that uh note taking is is a joint effort uh in these meetings and let me give the link to the chat window so please join uh The Joint note taking uh you can find the link in the chat as well uh otherwise the usual uh uh places for the meeting materials and so on and um a few words about the agenda uh we have uh an Norm uh giving us a presentation on uh ongoing work and developments in the it PL to1 TS and T group about projects uh that I think the most relevant to our work in the de networking group uh that's the main subject of uh this interim today and we will have some time for discussion I think I would uh say verbally also what um we said before that um at the last ITF uh originally we planned this meeting also to start discussion about taxonomy aspects for the enhan data plane proposals and the offline preparations did not uh go as we expected uh by this time so we need to reschedule uh that part of our next steps to another uh uh meeting that's not the subject of the meeting uh today any comments on the agenda I have one more slide to say uh but uh Tas yes please go ahead"
  },
  {
    "startTime": "00:04:00",
    "text": "yeah just a quick question on uh that thing that had to be dropped from the agenda um would would be nice to see discussions about that on the mailing list what the next steps are so that we don't have surprises uh like like like this one um only surprises if we're not getting active on the mailing list right so that's fine yes understood like we are we are continuing offline discussions actually on on on on like how to have an effective meeting so that we are prepared or there are some preparations ahead that we can meaningfully discuss that's the that's what I meant actually and now we are facing the holidays honestly like uh many people start vacation next week like myself and come back like uh 8th of January or so so please don't expect anything uh uh radical in that that time frame but um uh hopefully something in January uh you may also remember that we discussed another potential interim on like the third week of January I think that was at the time was about the row aspects but if we cannot progress that much with the row architecture yes we need to craft plans on on how to make next steps with the taxonomy yeah and remember right so I mean mid end of February is uh the the big Chinese day then again right so it's a always for us from the West kind of easily forget about that no I mean I think the the primary thing that I wanted to bring up which would be good to maybe discuss is that it's not only about selecting specific uh mechanisms but to think about also broader topics resulting from what we've been looking into like you know what extensions to the overall dead net architecture do we actually want to have for large scale networks I think those are questions easy iier answered um with less I think"
  },
  {
    "startTime": "00:06:03",
    "text": "contention than the individual mechanism selection based on the work that we've done for with the requirements document sure sure yes and again I think also the one reason for not being able to be well prepared for this meeting is already the holidays like many people some people have started vacation earlier and we are now facing that everybody's visit to for wrapping up the okay uh moving on a bit and then we have some time at the end for discussion uh one more slide on the intro part is that actually we have received two leas on statements we have haven't sent any uh I would mention the one the coming in later the one from it study group 13 about their efforts on deterministic networking uh there are the links and here here and you can uh check the details uh the both leaz on statements are for information and I would like to mention the one from 802.1 which is actually about uh uh their more relevant projects and uh this is what Norm will be presenting these two projects and many thanks uh for Norm accepting the invitation and and and willing to present and with that I would uh switch uh uh to Norm's presentation I have changed the slide that can let me give uh okay somehow it's I got a strange message I'm trying to press the control anyway if you don't get it you can just tell me when to flip okay well I'll start talking and you can work"
  },
  {
    "startTime": "00:08:01",
    "text": "on giving me control um if anyone has any questions please speak up um I think most of you know me um I'm uh funded by Huawei Technologies um I was uh in detet for uh at the beginning I I actually applied for the birds of a feather session that started detet and started the uh uh architecture draft um I've been working in 802.1 mostly for a very long time and uh right now I'm the editor of two documents that I think uh I'm hoping are of relevance to debet now first of all this is the opinions of these are the opinions of the author this has not been been approved by I 802.1 or for that matter by Huawei so this is what I think is going on in 802 uh there we are um so there are two Works in progress in the time sensitive networking task group of 802.1 of which yanos is the chair and one is 802 project 802.1 DC quality of service provision by Network systems and the other is the 802.1 qdv enhancements to cyclic queuing and forwarding now the first one is in standards Association ballot which is the very last ballot and comment cycle before publication uh and what this uh I'll be talking about this one first and it allows a system that's not a bridge complain to claim conformance"
  },
  {
    "startTime": "00:10:01",
    "text": "to all of the TSN quality of service standards uh and I'll get into what those are along with Yang modules to support them so as qos wise you can do anything a bridge does without being a bridge and caring about Mac addresses and uh certainly not forwarding based on Mac addresses the other document is qdv this is an amendment to the 802.1q bridge standard and it a and it's adds a new um queuing mechanism uh transmission selection mechanism to 802.1q uh which is a new form of cyclic queuing and forwarding we'll talk about what that is this one is in task group ballot which is the very first part of the process so this is work is just starting it's had a lot of presentations on what it is and why it's helpful uh but uh it's just starting the standardization effort and it provides as you can read multi-level nested transmission Cycles with output bins selected according to either time of arrival perlow bite count or potentially a field in the frame so let's start with quality of service provision by Network systems 802.1 DC in our terminology those capital letters mean this is a standalone document it's not an addendum or an amendment to 802.1q bridging it's a a document all that stand all on its own uh there is a big list of features for right now that are defined for bridges in"
  },
  {
    "startTime": "00:12:01",
    "text": "802.1q uh you're you have run across them uh but basically you've got strict trans you've got strict priority um weighted Fair queuing uh per priority credit-based shaper you've got scheduled output where you can schedule each of your cues and you're on for this time you three cues are on uh uh you can fight out during this time period uh this queue is closed at that time period and so on um we have the Primitive form of cyclic queuing and forwarding which is the whole network advances all the TSN traffic one hop per clock tick all at the same time um that's the original cqf we've got the newer asynchronous traffic shaping which is a time-based shaper which uh is fully analyzed and uh gives you full TSN capabilities uh at some cost in computation and potentially this new one that we've started working on qdb the enhanced cqf there are a lot of related mechanisms um we've got token bucket yellow red green marking we've got VLAN tagging that you know about uh with eight levels of priority and a two Lev green yellow drop eligibility tag on the frames we've got priority control which is better than xon xof it's a similar idea but it's uh a little better a little um gentler than that uh to pre that's mainly to pre uh to be used at very low level to prevent input buffer overflow we've got the"
  },
  {
    "startTime": "00:14:00",
    "text": "frame replication and elimination for reliability uh you know this uh uh this is in the uh debt net architecture document we've got the per stream filtering and policing which involves um uh that can include uh timed input Gates uh scheduled when certain flows are allowed to come in on this port and when those flows are not allowed to come in those port at other times uh frame filtering uh and flow identification for qos purposes and we know you know about frame preemption where you can interrupt a a frame and transmit more time critical Transmissions now all of those are included in the feature list of1 DC um and I want to stress that more important than that list of features is the fact that 1 Q integrates all these into a structure that defines the interactions among all of these things and of course they have strong interactions uh in a sense any standard is a definition uh that relates managed objects control parameters with observed behavior and it does that via a model it's got the model it's got the managed objects and it can produce the behavior it produces predictable behavior for anything that conforms to the standard so any combination of all of those things can be configured and the result is well Define you've got the Yang modules and in particular frame preemption is integrated into the mix and uh TSN time critical streams can be"
  },
  {
    "startTime": "00:16:03",
    "text": "pre preempted by even more time critical streams that's all part of the model now we all know this feature set is implemented on devices that are not just 802.1q Bridges and people Implement them on devices that have no Bridge capability at all of course because they're ubiquitous they're in the chips now uh many of them are so uh it's interesting to see that none of these qos features in oneq actually require that forwarding frames based on Mac addresses is present at all uh and to my delight when I started working on this project I found that the qos features are rather well isolated from the rest of the document that the bridgy stuff and the protocol stuff are really separated from the qos stuff in the text so we decided to start work on DC to make all of the qos features ref referenceable by standards that Define hosts or routers or firewalls or load sharing appliances or security appliances or tarat switches or Nicks or any other kind of device that wants to do uh these kinds of qos functions has a path now to specify how to do that um this is the last slide on DC what is in DC uh there's a guide book to point the reader to to the parts of1 q and other"
  },
  {
    "startTime": "00:18:01",
    "text": "documents there are uh a number of documents that make up1 Q uh amendments and so on and there are also related documents uh that are relevant uh so there's a guide book uh a reader guide if you will there is a list of all the TSN features and a trail of requirements for each feature saying this is exactly what part of oneq you have to pay attention to to know what this feature comprises and to be able to implement it and say that you are uh conformant uh and it's got the Yang modules that you need so that you can control these features and you don't have to import uh spanning tree state or any of that kind of stuff into your device because it's not needed for qos so that is1 Q I've got a pointer uh slide at the end that says where you can look at where you can see this document if you want to see it uh it is in its final stages uh any questions about uh1 DC Florian please uh go ahead yeah thanks um so yeah thanks a lot for this um I think it's really great to have this opportunity to apply this these DS features to to routers and not I mean in the past I always tried okay this is some kind of a bridge but it was really hacky so it's really great to have this um so I actually tried to apply this DC Yang model um for DEET router configuration and I ran into two minor issues with that and um the first one was uh that I was not able to find a"
  },
  {
    "startTime": "00:20:01",
    "text": "way to actually configure the priority to traffic class table and the other thing was um it was not completely clear to me how to really apply the stream identification before the uh flow classification and metering um can you comment on that or should we rather discuss these details offline well I can I can give you a a very brief thing um uh the configuring priority mapping I cons uh I considered a bridge feature as opposed to a router feature okay okay so uh priority mapping uh mapping the priority implies it has a priority a layer two priority when it comes in and you're putting a layer to Priority when it goes out and uh I think technically speaking that's just not true for a router it doesn't really it's not a router's job to carry layer 2 priority through but um yeah I so that would be my answer to the first part um to yeah agree with with with the argument the problem that I had is for example the psfp also outputs these uh these internal priority values so I have problem to map them then to to the actual cues yes um uh again how you map things like dscp to AQ um that would be in that has to be in the router spec um okay right uh from from my point of view right now and the other question was about uh the the identification oh the stream identification stream"
  },
  {
    "startTime": "00:22:01",
    "text": "identification is in another document that's in 802.1 CB yes the the the question was just because I think I I have the the I think I implemented the way it is supposed to be but it was just a little bit confusing because that there is a diagram in the DC document where basically you have the reception port and then directly the the flow classification and I was just expecting some ification in between uh yes I I've seen that com uh there's a comment on that in the ballot um uh I I I ask you to watch the ballot resolution on DC um and we'll get into that yes uh that's confused many people in the past it will continue to confuse people in the future there is an answer and and I hope to clarify that better uh I hope that that a comment on DC will Pro will get some text into DC to clarify that thank you okay thanks a lot thank you thank you toas yes how would you um how would I mean it would be interesting to see an example how you know a uh dead net flow classification uh in a Yang model would be brought together with any of the TSN DC actions that that should happen for for that flow right so I can't quite well imagine that uh well it's it's actually in DC so there there are explicit references to to ITF dead net Yang models no no it's to uh uh the CB Yang model"
  },
  {
    "startTime": "00:24:00",
    "text": "modules the1 CB Yang modules remind me which which one is CB CB is the frame replication and elimination which includes flow identification uh would it be fair to say that it's it's not not not quite clear to me now you know um what the actual proof would be whether um what what you folks are doing in DC is sufficient um to to actually achieve the goal that you have um in you know a context of a router a dead net router it's yeah it's not a router spec right BC is not a router spec but it seems it it it seems here that's why I'm no no but if it's if it's not meant to be able to work in conjunction with already defined Deb net um uh Yang models to actually establish TSN functions for a de net router then um the question is where and how should the missing bits be done and if the missing bits figure out that there are issues in DC how would that work so that that's why I'm presenting this to encourage people to talk about exactly that I don't have the answers we didn't do this project in order to meet a specific debt net need we got this to separate out the qos in so far as we Define it but let me give an example uh uh it would be nice to have"
  },
  {
    "startTime": "00:26:04",
    "text": "a uh token bucket shaper integrated into this model because in fact people Implement a lot of token bucket Shapers and it would be nice to have a token bucket shaper inent in in this model1 Q doesn't have a token bucket shaper it's got a token bucket Red Green uh yellow uh marker but it doesn't have a token bucket shaper if debt net needs a token bucket shaper let's talk about that how we can do that uh and make a model that works for everything okay uh it takes interest on both sides to cooperate to make that sort of thing happen and so this is a step in that Direction I hope as as I said I I'm I'm currently working to try to integrate the deet Yang or have some very simple example of using the deet Yang model together with DC I will share it to the mailing list and then we can maybe disc about it good thank you so we ready then yes please go on nor Okay so um TSN where it stands now doesn't scale to large networks and the two most common issues are that adding or removing a single flow can require recomputing all the parameters for all the flows in the network that you've gotten so far uh and that sort of computation load is not I ideal for a big huge"
  },
  {
    "startTime": "00:28:00",
    "text": "Network also adding or removing a new flow can require configuring parameters or a state machine in every relay node along the path so qdv offers uh this amendment offers a basis for a variety of related queuing strategies that can reduce these issues particularly the recomputation of all the parameters so I'm going to talk about uh three aspects of this um the basic idea is that you've got several cues each Q is divided into bins and each Q outputs its bins in rotation or makes its bins eligible for Transmission in rotation at a regular frequency but each Q is at a different frequency so in that context we'll talk about uh the buffer allocation and output timing and so on uh the things you see there on the read and then there's the input side which is how do you assign received frames uh it says Reed cues how do you assign received frames to bins uh and we have three methods to talk about uh time of arrival per flow bite counters and Frames marked with a bin number and we'll talk about scaling tradeoffs and time synchronization options and so on and then some consequences of all this so I'm going to use this diagram for several slides as a basis for several slides and I want to point out it's not in the text of the slides but I want to point out this is drawn as the"
  },
  {
    "startTime": "00:30:04",
    "text": "cues that are feeding a physical Port this could just as easily be cues feeding the uh an aggregated flow these could be aggregating Flows at different qos levels into a single aggregate flow this can be merging uh data in several different qos levels into a single physical uh link uh obviously if you're merging this stuff into an aggregate flow uh you wouldn't be having any best effort traffic down there it would be just the red boxes just the cqf stuff would be merging into an aggregate flow but this this principle of applies hierarchically uh whether you're talking a a physical Port is just an instance of another kind of aggregate flow so in this example I've got three cqf Qs and one of them is operating rotating at one millisecond one's op one is rotating at 3 milliseconds one is rotating at 6 milliseconds the one that rotates the fastest is given the highest priority all of these cues in the1 Q model wind up feeding into a simple priority selection everything feeds into simple priority what happens is that Q's can uh feel that they're eligible to present something to priority or not uh in this case uh we've got all of these things feed in priority the short period is the highest priority the"
  },
  {
    "startTime": "00:32:01",
    "text": "longer periods are lower priority and of course all the cqf traffic all the critical traffic is ahead of um any traffic that has unbounded bandwidth so this shows the cues uh this picture shows how you allocate bandwidth um I'm assuming that approximately 10% of your bandwidth is taken up by the uh priority 71 millisecond traffic uh 30% by the priority 6 3 millisecond traffic uh 30% by the priority five uh six millisecond traffic now these these windows uh are very close together you would never configure that in reality but this is an example and I can only draw so many boxes on a picture um and of course this is an extremely loaded uh Network I'm not sure you're allocating 70% of your bandwidth what this is bringing out is that allocating 40% of priority six allocates 40% of your total bandwidth and really when doing your admission control when deciding what you're uh allowing to pass through the network to a first approximation um all you have to do is add up the total bandwidth of the cqf traffic per link and make sure it doesn't exceed your uh configured limits uh which is all done out outside the box can all be done outside the box yeah"
  },
  {
    "startTime": "00:34:02",
    "text": "there's a limit uh you can allocate a higher percentage of a slower cycle than you can a really fast cycle uh so it's not quite just uh that simple but it can be as simple as uh I don't care what different levels the different flows are at how many are at the fast flows how many are slow flows uh it's the total bandwidth all I have to do is not over allocate my total bandwidth on my link Norm Norm do you want to take questions on the fly or any time yes you interrupt me so is is for for this multi priority is that actually standardization wise new or new explanatory text because it seemed to me as if this was already possible with the you know well cqf being just a configuration method of the pre-existing TSN components right so you had the Yes except except the way cqf is uh described you need a separate Q for each bin so it's not practical to have more than one frequency running so so you're saying that that uh so what what exactly could not have been done so diff different frequencies of of well if you look at the next slide this is typical buffer utilization um in this example um the priority seven I'm using some extra buffers to allow for delay variation uh on my inputs the second uh the priority six is the typical case where you have three buffers running uh and the third case is because it's"
  },
  {
    "startTime": "00:36:02",
    "text": "nice long uh time periods I'm able to synchronize them and just use double buffering so I minimize the buffering um in this example in cqf I would require four four separate cues for the blue three for the green and two for the red which adds up to nine qes and I would have to have nine class of service cues and I only have eight and that leaves nothing for best effort so you're saying that this this enhanced cqf would a ask for Hardware to have more than these eight uh priority cues so to speak no I'm saying that you have a new way of doing a queue which is very similar which is actually uh essentially the same as the the asynchronous uh the ATS thing where uh when a frame is put into the queue these four packets in this priority 7 Q these four packets are in there but each is marked with its bin number and you cannot output the stuff in the second bin until the time comes up for the second bin to be output wait a second these these timed cues already existed that's how cqf original cqf no no no no cqf operates by turning by having a whole q a big fifo q that it turns on and off okay okay and it works by selecting which queue you send a frame to based on not just priority but also whether you're use transmitting from the left one or the right one so you you steal two qes and so in cqf these four packets in the"
  },
  {
    "startTime": "00:38:00",
    "text": "priority 7 would be four Q's all at priority seven which are enabled at different times so it doesn't matter what priority they're at because they're enabled all at different times all it matters is that there're a higher priority than the green okay wait a second let's let's stick to the most simple comparison for the hardware implication of old cqf versus new cqf let's not we'll take too long to explain how cqf works today let's just talk about how the enhance cqf is intended to work in this document the way it works in this document is and you can't do this with the existing cqf we can't go into it now it'll take too much time trust me you can't do it um the you have here three cues that of the eight dedicated for cqf each Q is divided into bins conceptually you wouldn't implement it that way exactly uh and the bins come up are eligible for transmission and rotation so a given bin on this que on this first the topmost Q one bin is actually transmitting that would be the rightmost bin one bin is actually transmitting uh the next two bins are full of data and the fourth bin from the right would be filling if you look at the green cues there's a good example the rightmost bin is emptying right is about to start emptying the next bin in is sitting there full of data waiting for its opportunity and the third bin is right now uh being being filled up okay and in a"
  },
  {
    "startTime": "00:40:02",
    "text": "moment this rightmost bin will start transmitting and when that third bin gets filled up the next bin will start and 3 milliseconds after this first bit starts transmitting 3 milliseconds later the second bin will transmit and we'll get to a slide that shows that so this shows the buff typical buffer utilization the fact that this top one has a whole lot of of uh buffer shown does not mean that it has a whole lot of data allocated it's actually the buffer space is consumed by the slow Cycles this would be the output timing now this looks similar diagram but this is time um what happens is that because it's priority every time a blue frame every time the top bin rotates the top Q rotates its bin it gets to Output its frame so the blue frame comes out first um the green frame is delayed by trans it can't transmit when it comes up when it becomes eligible all three of these cues become eligible at the same moment but priority 7 the 1 millisecond frame has priority so it goes out followed by frames from the 3 millisecond bin uh those are not exhausted before we come up again to another cycle on the fast bin so the green gets interrupted while that blue frame comes out when the green frames have been exhausted now now I can start working on the red frames in their slower bin the red"
  },
  {
    "startTime": "00:42:01",
    "text": "frames get interrupted by the blue frames and that continues when I've exhausted the blue and the green and the red frames I've got nothing left in my cqf stuff so now the best effort can be transmitted except the best effort gets interrupted for that blue frame right here and then we start another cycle of red and green so that's what the data looks like when it comes out on the wire um if I were implementing this I would want to I look and see how much it would cost me [Music] to do something to spread the cqf data out a little and give the best make the best effort transmission opportunities less lumpy so that's the basic idea of cqf nor we have a we have a question we have a question from shafu shafu please go ahead please hello thank you and thank you I have a simp question uh uh for this multi for instance uh it seems that the you should uh combined with uh frame preemption function right yes uh you can use preemption uh preemption preemption of the best best effort traffic um allows you to reduce the amount of interference um with the cqf traffic and for that matter you can preempt lower priority cqf traffic in favor of"
  },
  {
    "startTime": "00:44:02",
    "text": "higher priority cqf traffic I have a paper that is available uh I will there's a pointer to it at the end of the deck that's on the 802 public website that talks about that specifically oh okay preemption fits so what is that value uh for one thing admission control is Trivial um uh you mustn't exceed the bandwidth of Any Given link or some configured fraction whatever you think is the maximum you want a cqf you want to do compared to the best effort traffic um uh so mission control is trific uh the per flow worst case delay computation is uh trivial or very near trivial because the number of buffers used for hop is either the number of of bins number of Bin Cycles you spend at each hop is either constant or it varies by one depending on how you assign traffic to bins uh I think there may be cases where the it varies by two but we'll talk about those things um and the time per buffer the time spent uh per per bin is fixed to some accuracy um physical link delays changes slowly and we'll talk about that um so the per flow worst case delay com computation is just a matter of adding the per hop Del lays"
  },
  {
    "startTime": "00:46:01",
    "text": "up um and importantly adding or deleting a flow does not change the delay of any other flow that's critical because if adding a flow changes the timing on other flows then every time you add a flow you have to recompute the whole world to make sure you haven't uh spoiled somebody's day uh of some existing flow um but of course that comes at a cost so unlike uh a uh uh token bucket shaper or the unlike the time based shaper uh the ATS shaper we have in one uh where you're you can have absolutely any flow with any requirement and uh you can bring them all together and get deterministic results no matter how you combine things uh the D you know the downside of that is the computation gets tough so um what you have to do is have a certain number of traffic classes each with a characteristic period every flow is defined with a characteristic period and a number of btes it can output during that period that's its share of the bandwidth um and you can't have an infinite number of those um if you have a flow that matches the priority cycle time then cqf is as"
  },
  {
    "startTime": "00:48:04",
    "text": "efficient as you're going to get it averages you buffer one frame uh you have one frame buffered in every hop along the path uh things move along nice and smoothly uh and uh per hop delay is inversely proportional to uh uh your speed and that is exactly what um the physics demands of flows that can interfere uh arbitrarily if you have multiple cycle times at multiple priority that allows you to fit a flow to approximately the right priority level if you give a flow too high a priority then you don't use all of your transmission opportunities but you have good delivery time uh when you assign a very slow flow to a very fast cycle time uh that's basically wasting a lot of allocable bandwidth but that allows you to do alarm flows that go zipping through the network really fast even though they're low bandwidth and so that's one direction of violating the Rel reltionship between bandwidth And Delay um if you have if you assign a flow to two slow a time to too low a priority then you're buffering multiple uh frames multiple packets for that flow in one buffer which means you're eating up Network buffer space uh to deliver it more slowly uh um so pick your poison and that's the"
  },
  {
    "startTime": "00:50:00",
    "text": "downside it's not infinitely uh flexible on the other hand I'll point out that if you're a service provider having a fixed repertoire of service choices is not an unfamiliar situation um and we'll get to time synchronization that's important um any questions on this slide I think this is a a fairly important slide we got the point across jino go ahead yeah jino please okay thank you thank you Norman I am jinu Jang I think that uh the very important question here is how to determine the duration of the cycle time it it affects the yeah it affects the performance the endut latency bound and so on that's right how many cycles I will point out a couple of things one is that um you're going to have a repertoire of cycle times um the fastest cycle times may be available only in the parts of the network that have very highspeed links the parts of the network that have slower speed links are going to have to use uh um uh will not be able to use the really fast cycle times because the fast cycle time may be shorter than one packet um yeah so the whole network can have a larger repertoire than any given link um but picking these cycle times is a situation that's really quite familiar uh uh to the telephone people um there is nothing in this slide that wasn't invented 50 years ago by the"
  },
  {
    "startTime": "00:52:01",
    "text": "telephone folks okay yeah um if I yeah if I continue my question please um so yeah we as you have said uh the ideal cycle time is very small maybe one frame one um One Transmission time for a frame right but in that case if there are very many flows we have to allow that many number of Cycles as many as the flows then the whole thing becomes uh recursive again uh no what what you do is if you have um if you have a number of flows they don't each get their own cycle uh let me go back to uh this uh uh um if you have five Flows at priority 7 then you have to have room in each cell in each bin for the frame one FR for a frame from each of the five flows oh I see oh so each each Bean has to accommodate all the flows pecket or all the flows frames right this blue this blue doesn't isn't just one frame the green is not just one frame it is all of the frames at that priority level okay thank you uh my another question is so do you assume a bursted traffic so uh if a single flow transmit a burst of packets or frames at once then do you have to accommodate all the burst in a single beIN um that's handled at the edge and we'll get to that oh you mean Edge you cannot you no"
  },
  {
    "startTime": "00:54:03",
    "text": "you cannot have a burst every one of these Cycles um the rules for these Cycles are that any given flow has a maximum number of bytes per cycle that's allocated to that flow it can use it or not if it doesn't use it those bites are available for best effort okay I see they've been allocated and they're gone in terms of allocation yeah but I think that that is the case uh and the source wants to send the bursty traffic such as the video streaming right then the whole um yeah whole the whole burst has to wait at the edge of the network and that has to be counted as endtoend latency well you have you have Alternatives one alternative is to hold it uh you're going to allocate some amount of bandwidth for this flow okay one alternative is to allocate the something that's slightly larger than the average bandwidth in which case when a burst comes in it has to be spread out in time which depending on your application simply may not work the alternative is to allocate at more bandwidth allocate it 90 uh uh equal to uh some percentage of the burst rate in which case the burst has to slow up just a little bit at the edge which may be susceptable or allocate it a bandwidth equal to its maximum burst rate in which case the network uh in which case you've used up a lot of your networks available cqf bandwidth and that's not available for other flows I see okay so um yeah because the question becomes too long I may summarize like"
  },
  {
    "startTime": "00:56:00",
    "text": "this so we have to have some solutions to come in the future about the burity traffic I will think about it myself thank you very much thank you okay please do um I I I wish you luck okay thank you so um just one one one more one one more point it's it's less a question but have you have you compared the you know um scheduling mechanisms that you described with ATS because it seems to me that it's not really all that I I think you've got the same problem in setting up the policies selecting parameters as an ATS the big difference is just that um the the arrival time the earliest maybe maybe short in ATS when you add and remove flows but the guaranteed one uh that that that you have will not change either uh you you you can argue and I think it's a valid argument that if you use uh a grainy a a a large grain clock to drive your ATS scheduler that uh and play some games when you're allocating bandwidth um uh there is a Continuum between ATS and this multi cqf okay and that's brought out in the the draft document well but I was I was thinking even if you don't do that even if you compare just you know what percentage of bandwidth do you give to each ATS priority um what the maximum buffer size is you give to them that ends you up with a very simil comparable questions of of policies okay"
  },
  {
    "startTime": "00:58:01",
    "text": "you have to um if you're if if the problem you're trying to solve is is more or less continuous flows that are that can be that are uh that can reasonably be characterized solely by their bandwidth and if there's no coordination among the transmitters uh to um avoid each other's bursts and so on the best you can do the best you can possibly do is uh that the per hop delay the the per hop forwarding delay the time to get through each forwarding device is inversely proportional to your bandwidth and you're buffering one packet per hop uh that's just physics and both ATS and cqf offer that ATS offers it for all the Flows at the expense of having to compute it which is simply an intractable Pro untractable problem for a large network cqf does it g uh is Trivial computation but it's only optimal for flows that EXA ly match your available classes of service and it's suboptimal for any flow that doesn't match a class of service so there's no such thing as a free lunch okay this is uh another point on the uh I think this is a uh at one end of of of the scale ATS is at the other end of the scale but they are fundament Ally the same algorithm yes I just I mean"
  },
  {
    "startTime": "01:00:00",
    "text": "thanks thanks for the detailed Insight I was just thinking that um this multi- priority doesn't open up any new policy questions that you didn't have to deal with uh in multi priority ATS either um I'm not sure what you mean by that but uh you're a smart guy I'll believe you so I want to stress that's what's Critical with this whole thing is we we're talking about um you know TDM over best effort okay uh we're talking about phone circuits over best effort um what's critical is that every hop all of the cqf frames are arranged in a transmission timeline that adheres to these rules that the bins rotate regularly that bins at different priorities on the same port are integrally aligned and that no bin is ever overallocated so that every bin is guaranteed to empty before its time expires that ensures that interference on all levels is is bounded and easily calculated um so let's talk about how you fill the bins uh now I've taken an hour so I'm going to have to hurry up a little uh unless you guys are comfortable so I'm assuming for a moment we're looking just at one priority level not a bunch of priority levels I'm just"
  },
  {
    "startTime": "01:02:00",
    "text": "looking at one of them the rotation frequencies are approximately the same frequency at every hop along the path and that the phase of the rotation cycle varies along the path that means it's not required that all bridges rotate their bins at the same moment it's not required we've got three clear obvious methods for assigning an incoming frame to a particular bin on an output que one is uh based on what bin what bin it was in in the previous hop the basic idea is that for this for all this to work you want uh one technique is for frames that are in the same bin at one hop if they go to the next top to the same output Port they'll all be in the same bin again that's what enables you to not have to look at the frames and not have any per Flow State machines no perlow State at all so you've got to know what bin number the frame was in at the last top you can get that from the time of arrival of the frame that's in1 qdv right now you could get that from some frame from field in the frame now I know this has been discussed in debet and in ietf uh it is not in the current qdv draft and I will just mention to you uh so that uh uh this uh I have an assumption in my head that that's somewhat different from the presentations I've seen and I don't want that to cause Fusion I'm assuming the frame is marked with what bin it was transmitted from I'm"
  },
  {
    "startTime": "01:04:02",
    "text": "um uh without going into why I like to think of it as marking what bin the frame was transmitted from not what not I don't I don't think of the marker as telling me exactly what frame it's going into if for no other reason than there are multicasts to deal with so one way to assign it to a B is I know what bin it came from therefore I know uh whether it should go into the same bin as the last frame I passed on or whether it should go into a new one the other way is to select the bin based on Counting bytes how many bites I've stored in each output bin per flow that also works um I invite you to look at this offline nor Norm no don't no don't worry about time we have time we have booked this meeting for two hours so feel free to explain I think it's a complex figure and it's helpful and it's good discussion so again thanks for doing it we have time so this is two nodes uh we've got node a and node B node a is transmitting to node B and I've marked out three times lines or four timelines here um so the first timeline is transmitting bins I'm transmitting frames from bin a uh the the frames in b a are transmitted between time zero and time one I think time is reversed here from the other diagram it doesn't matter uh you'll figure that out um they're transmitted between time zero and time time one and they received sometime later I'm assuming three of a cycle TC is the"
  },
  {
    "startTime": "01:06:01",
    "text": "cycle time uh they reive sometime later so time uh timeline two is when the frames are received offset when the first bit of each frame is received offset by the link delay it takes a certain amount of time to figure out what I'm going to do with the frame what port I'm going to forward it to what kind of qos I've got to process the frame I've got to do something with it somehow um arbitrarily I've chosen a fairly long time here 08 ticks um so there is a worst case delay and by the end of that time I must must have assigned this Frame this incoming frame to uh this incoming packet to a bin so the third timeline is bin assignment time and this is the critical line that I was talking about in this Slide the timeline paragram there must be a schedule okay that meets all the criteria that you never put too much data in a bin there's always going to be time to empty the bin every flow gets its share of the bin that is enforced at some point and that is timeline three when I'm assigning frames to bins having assigned it to a bin at the moment I close and this is the end of the time to be able to assign something to bin a Ben C is still transmitting so at some point after"
  },
  {
    "startTime": "01:08:00",
    "text": "that time uh that closing time I'm no longer to uh I'm no longer able to um store stuff into b a b a then becomes enabled for transmission and it gets transmitted so the time from timeline two the start of cycle a and timeline two to the start of cycle a and timeline four is basically the the delay across this bridge does this diagram make sense to people so this is the timeline I use if I'm basing mys bin selection for this hop on the bin that it was in in the previous hop um when the frame arrives or while I'm forwarding it I have to identify what bin it was in so that I can apply an offset and assign it to the bin it was transmitted now I will point out that the frames that come in from in bin a into from node a to node B those frames go to different ports so this another Port may be have its timeline for in a different phase not all my ports may be in Phase although they're operating at the same frequency so the relationship between timeline three and timeline four will vary from port to port uh and"
  },
  {
    "startTime": "01:10:02",
    "text": "so this gives you a picture of what's going on when I'm trying to keep everything in the bin all the way through now what does that require the advantage of doing this is that there are no perlow State machines there are no per flow configur duration the delivery time end to end delivery time is constant modulo one bin rotation cycle this is as deterministic as things are going to get and this is the minimum possible um penalty uh when you want to recompute add a flow you can't get better than none um so except for the fact that not all flows are going to exactly fit your available classes of service and therefore either have to waste bandwidth by going into too fast of into too high a priority or excuse me waste allocable bandwidth by going into too high a priority or waste buffer Space by going into too low a priority it's ideal uh because it's simple for the network to do and so that scales my goodness that scales however it requires if you're going to keep everything at the same bin all the way through the network that requires that every single node in the network rotate its bins at exactly the same frequency what do I mean by exactly the same frequency that means that if you're operating at at a at a rotation period of 1 millisecond that you have exactly 86.4"
  },
  {
    "startTime": "01:12:01",
    "text": "million cycles per day um now uh so meaning that the the difference in the number of Ro of Bin rotations between two boxes the difference in how many I've counted over an arbitrarily long period of time with the number that you've counted has to be bounded but this is not a trivial problem but it's way easier than synchronizing time to a high prision okay if the bins are rotating at 1 millisecond then it costs you one extra buffer one extra bin to have a phase inaccuracy that wanders around over a millisecond range okay if I keep the same frequency but I get a little off in the absolute if I get a a little off from you and I wander a little here and there as long as that wandering is on the order of uh a bin then I only need on the order of of a Bin's extra buffering to accommodate that and that's a different problem than trying to get subnanosecond [Music] uh time synchronization that makes it I think possible to have reasonable sized frequency lock regions I'm calling them frequency lock regions um I will mention that you do have to"
  },
  {
    "startTime": "01:14:04",
    "text": "track the phase relationship between my bin rotation and the previous hops bin rotation if I'm going to do this by timing rather than labeling in the frames I have to track that uh to a high precision but that's a one-way thing it's a trivial protocol and it's in the draft uh that allows the phase between my bin rotation and your bin rotation to vary slowly which in fact is what it does it doesn't jump around so uh for example if a long link stores two cycles of data and that long link can vary in length over uh sign or or for that matter a long link might store a dozen cycles and if that link can vary by a large fraction of a cycle over the course of a day that's okay you need a little bit of extra buffering you need however much extra buffering corresponds to the time that the link delay can vary and you can accommodate link delay variation in the same way that you can accommodate uh Vari uh uh limited variations in frequency over short period of time as long as at the end of the day you've added up the same number of buffers so you don't miss a gear so the gears don't uh fail to mesh no we have questions please yeah or or differences of of opinion so I mean we've been uh in um in the dead net proposals that we've been having up"
  },
  {
    "startTime": "01:16:01",
    "text": "for since uh well five years um the um you know Tech CYCC queuing looked into that as well so I don't think you can say it's simply frequency utilization because you can't have the bins drift apart arbitrarily you need to have an upper bound which in uh PTP um fashion is called the maximum time interval but yeah that maximum time interval can now be in the order of you know a few um Cycles um whereas it had to be I think in the clock synchronization in cqf in the order of less than 1% of the cycle time right so right exactly it's it's a humongous order of difference uh less uh clock and chronization accuracy needed the extent to which this results in easier to implement clock synchronization I think is still something to evaluate when I was talking to some of the PTP expert they were saying that obviously the cost for clock synchronization goes up quite dramatically the closer you get into the nanc area so yeah I'll be very helpful hopeful that this will be very helpful to uh further proliferation of of this mechanism um that's that's why we've also been proposing that into Deb net good I'm I'm hopeful that that's true I'm glad we're in the same uh on the same page here now um another question yeah uh Norman uh this is Jin Jang again I actually wrote my question in the chat box so you can take a look but if I repeat um so you said that the duration of cycle is determined by the number of flows and their packet sides so some of the packet sides of all the flows uh that is a deter determinant fact of the factor of the cycle duration"
  },
  {
    "startTime": "01:18:00",
    "text": "right even the highest priy cycle can be very long you you want to make it short but there are a lot of flows of high flow that is the case and in that case the low priority cycle which does not have many flows that duration becomes necessarily larger I think um um no uh I I don't think so the reason is that the reason those flows are in a lower priority is not because they're less important they're in a lower priority because they have less bandwidth um the high priority are the highest bandwidth the low priority are the lowest bandwidth um when I say that a flow fits the class of service that's assigned to you assign a flow to the class of service ideally you assign it so that it gets one packet per cycle um I'm not sure if I understood correctly but I'm uh what do you mean by High bands I don't understand the high priority traffic has high band how many uh okay let's talk about uh it's it's bytes per second but you know the industrial control package have only small bandies but has high priority and um yeah video streams can be lower priority but has more bandies I would say so that okay we're getting we're okay we're getting into the basics and let's let's talk about that briefly"
  },
  {
    "startTime": "01:20:04",
    "text": "um if [Music] a if you want to deliver frames if you want to deliver packets with zero congestion loss and and that's our goal is zero congestion loss um you can't you don't want to buffer things uh you don't want to have things sitting around in buffers that's uh but flows can interfere with each other you have to have enough buffering to take care of the interfering between flows if everybody happens to transmit a frame at the same time you have to be able to deal with it so if you think of a of one bin has to be able to accommodate all the data from all the flows that operate at that speed that way it's guaranteed you've got enough room if they all transmit at the same time the if you're using the minimum amount of buffering in the network then it's the casee that for every flow let's assume for a moment that all flows have equal sized frames okay that every frame of a flow is the same size for a moment that makes thinking about it easier then the ideal situation uh is that every flow spends has one frame buffered in every node along the path and that every frame spins one uh that the number of frames per"
  },
  {
    "startTime": "01:22:04",
    "text": "second that you're transmitting is equal to the number of hops per second that you get through the network that ensures that the network will always be able to transmit when it needs to transmit that ensures that the network never misses an opportunity to transmit it ensures that all of your alloc allocated bandwidth is available so your time your per hop delay is inversely proportional to your packets per second that's just the law of physics for the kind of flows we're talking about that's a critical thing that uh everybody that you need to understand understand that the per hop delay is inversely per that the per getting through a node I forget about link delay for a minute if all link delays are zero then the per hop delay is inversely proportional to your packet rate you can't do better than that unless the flows cooperate and stay out of each other's way somehow which we're assuming they're not right thirsty if they want to transmit more data occasionally then you have to allocate them more bandwidth right yeah this this the uh the same yeah that's the point uh so getting back to your question now now that we've established that getting back to your question if you've got a whole bunch of high of of these flows that are high priority uh yes they take up that that requires a certain cycle size to match that that implies you have a a whole bunch of flows that have the"
  },
  {
    "startTime": "01:24:01",
    "text": "same bandwidth and that's fine and you're right uh that means the next level has to be somewhat bigger than that so um you can't have a whole lot of flows that are slower than that because if you're uh small if your short frame size is just big enough to hold your fast flows then you've allocated 100% of your bandwidth so uh your small flow is going your small bin is going to be considerably larger than the amount of data that it transmits per cycle because otherwise there's no bandwidth left for any other traffic uh and yes the next larger bin has to be at least twice that size but okay if you've got a lot of flows that need that next larger cycle then okay you could allocate a bin that's twice that size um it's it's whatever kind of mix of flows you have uh I as I said it's not your your observation is correct this is not infinitely flexible you could have um um 25 classes of service I'm I'm serious you could have 25 classes of service and each class of"
  },
  {
    "startTime": "01:26:00",
    "text": "service has two bins compared to the adjacent class of service okay that gives you a great deal of flexibility and that is possible if that's what you want to do yeah I yeah thank you for your kind answer uh one way to solve it I'm not so sure but um we can we can allocate a flow a priority maybe so according to your um hierarchical definition of the classes and the the size of the the duration of the cycle we can only allow small number of flows to the highest priority beans and we can allow more traffic to become a low priority traffic and so on there can be I think that can be a solution well or not maybe we'll talk about that at some point okay yeah let's talk about later okay thank you thank you so the next big thing is to select based on bite counts if I'm not frequency locked sorry nor Tong Tong has a comment question ahead sorry nor to interrup I I just want to join the discussion I think in your diagram of M multiple cues with different priority I think in this yes in this kind of uh cqf uh scheme the priority is for the for the ques but not for the flows because you know because all of flow has its own latency uh requirement so we alloc all uh allocate this flows to different bins thank you by the requirement of latency so the the the flow priority is not so important in the in the schedule on on each"
  },
  {
    "startTime": "01:28:00",
    "text": "note so we don't need to mix the uh priority of cues and priority of Flows In in discussion we need to uh allocate uh the flows into different cues that's what I want to add and if uh in my understand if you know some high speeds flows or oh low latency low latency flow I mean those those traffic need to be forwarded in short time it's should be uh put into the one millisecond bins to to achieve the low lowest latency and also the highr bway if they don't have a high speeed traffic then some bway Wate uh and yes that's my uh understanding of this scheme thank you Tong Tong that's exactly right uh I agree with you um and that is this slide um the optimal world for the provider is that you equipment's used efficiently and so on um these priorities are strictly bandwidth classes okay uh and you can assign something to a higher priority and that eats up the bandwidth of a higher if you want to give it a higher priority that gives it lower latency but that eats up the bandwidth of a higher bandwidth flow um that's bullet 2 a or 2 B and that's exactly this yes exactly right thank you for making that point okay thank you so now we can have per flow per output Port per Q"
  },
  {
    "startTime": "01:30:00",
    "text": "we can have a bite counting state machine uh this is Mick Sean's pattern uster algorithm and you can spool the the data out into each bin and I can say that when a frame comes in I put it in the bin that's closest to the output bin that is not yet full of data from this flow so each flow can be potentially being assigned to a different bin at different stages so that um I don't care at what rate the data comes in I assign it to the bins ensuring that no flow gets more than its share No flow gets more than its fixed number of bytes per bin um and there are lots of ways that I can use this technique um I can use this as my main mechanism uh and I give uh a little bit of over provisioning propor to my clock inaccuracies so that um uh if the previous hop is faster than me that's okay because occasionally um uh a bin won't be full and I'll be able to catch up uh so basically you uh you don't provision you don't uh allocate all of your bandwidth you reserve uh some fraction to handle the for the clock inaccuracies that way you can still run as if you were frequency locked and you"
  },
  {
    "startTime": "01:32:00",
    "text": "still get most of the benefits the catch is you have to have these purport machines to spool out the data uh so I can use them by themselves I can use them at flow Ingress or at the boundary between frequency lock regions to recondition the flows and get them back into frequency locked mode that way I uh minimi that way I've got compatible uh mechanisms all through the network I just have to count bytes at the boundaries I can if I have a place where I have to change the rotation fre frequency uh if I have my bins operating at 1 millisecond and I want to shift to uh I want to shift this flow to a thing that's running at uh 1.67 milliseconds then I'll have to use the bite counters to make that transition uh one method of supporting flow aggregation and disaggregation is to use bik counters there are other ways to support flow aggregation and disaggregation and even within a frequency locked region if I don't trust my neighbors to work properly I may want to implement the flow counters the bite counters occasionally at certain places in my network to make sure I haven't screwed up and uh uh have some somebody uh somehow with extra bandwidth that they shouldn't have so I can use the bite counts the"
  },
  {
    "startTime": "01:34:02",
    "text": "pattern uster algorithm in a lot of ways it can be the basis of my network if I don't want to do any kind of frequency locking I can use it uh and it's compatible one thing that's in QD no no we have a question from Shuffle please uh hi R thank you my question is that if we use account based being assignment uh uh uh the the the per Lance may not uh be same as the T based being assignment right uh say that again uh my question is that if we use uh counter Bas being assignment then uh the performance equation uh maybe not the same as the time based method uh the which equation uh yes the latency the the latency the latency is um the latency is a little harder to calculate yes uh and I will refer you to mix paper at the end of the I I've got pointers at the end of the paper of this presentation they point you to mixed paper he talks about uh the latency and more detail okay okay there's a paper at the point at the end of this thing I'll point you to it that talks about that in detail uh one thing I'd like to point out is that at each node along a flows path can use a different number of bins per"
  },
  {
    "startTime": "01:36:00",
    "text": "output q and each one can use a different bin selection mechanism as I go from Hop to hop I can use a label on this hop I can use time on this hop I can use a bite counter on this other hop one output bin can be fed by input ports or even individual flows using any of the three methods my whole network doesn't have to be time based my whole network doesn't have to be label based or count based in fact I can have on Port six I can have data coming in on Port one that's using time to determine which bin in Port six it goes into the data coming in on Port two can use a label to determine which bin in Port six I'm going to and a third one can use labels for most of it but for some flows I use a counter to determine which bin it goes into a multicast flow one input can be sent to different output ports using different bin selection methods um so there's nothing that restricts you in that sense except the physics and configuring the parameters if you're using a method that configures the parameters you can use the bin Counting even though your frequency locked you can use the bik counting I should say even though your frequency locked if that's what you need to do for"
  },
  {
    "startTime": "01:38:02",
    "text": "safety okay you can use the bend counting to enter a frequency locked region um you can only use time or label to assign a frame if your if the port to which you're sending the frame is frequency locked to the port that the frame was transmitted from the last top otherwise it doesn't make sense but you can mix and match these things to uh an arbitrary degree and one thing that I don't think I mentioned here um you can even assign a frame you can add an arbitrary additional number of bins to the bin assignment for certain flows certain ports or all flows whatever so that you can do uh delay matching um we've run into the issue in a ring for example if I'm transmitting both directions on the ring for redundancy and the exit from the ring is two hops to the left and 12 hops to the right on the short path I may want to delay all that data for 10 extra hops so that it's delivered it's it arrives at the same it it goes out at matching times to the next ring so you can actually easily use this scheme to do delay matching and arbitrary delay flows obviously at the cost of buffering"
  },
  {
    "startTime": "01:40:01",
    "text": "them finally hello Norman um yes I have I have another question I I also wrote it on the chat box my question is if you have to count the bites of flows then should you maintain the Flow State what do you mean the Flow State well um you have to I think you have to count the bites that is already allocated to a beans well you're you're counting you're counting the btes that have been allocated you're counting the btes that have been placed in the bin for this flow you have a separate counter you you have to remember some uh records of flow then can we say say that's the Flow State well that is a part of a that is a kind of Flow State yes how many uh how many btes you've uh you've put into this bin and when the bin advances you know it it clears it so but but you know the ATS the ATS has a problem of of uh maintaining the Flow State and that is the reason it doesn't scale well of course that's right I think the B the b b bite counting cqf has the same problem of course it does of course it does that's why okay that's why I suggest that while you can use it everywhere um it's best used at the boundaries between frequency lock regions for example if you have a region if you have a boundary where you simply cannot frequency lock then you have noo you know at the yeah at the netor boundary there can be large number of flows you well ideally you reduce those flows to a"
  },
  {
    "startTime": "01:42:04",
    "text": "small number of aggregated flows okay okay I see so you try to solve that problem by flow aggregation okay that's all you can do that's the physics okay thank you okay it's physics I can't change that um if you're time locked you don't have to do that if you are time if you're not time locked then you've got to count bytes so you better count bytes on a few aggregated flows rather than thousands of flows and I and I should say aggregated uh we'll get into flow aggregation this is my last slide uh flow aggregation um obviously flow aggregation is critical and I've not talked about it very much um I did mention at the beginning that this whole Paradigm here is applicable both at a port level and at the entrance to an aggregation you can be doing this in order to combine flows into an Aggregate and I'm convinced that it's possible uh for example I've got a box I've got a high-speed aggregate flow coming in from Port one to port and going out on Port two I have another high-speed aggregate flow going from port 11 to Port 12 I want to separate out one or two component flows from the first aggregate and transfer them and insert them into the second aggregate I'm convinced I can do that without slowing down either of the"
  },
  {
    "startTime": "01:44:01",
    "text": "Aggregates the aggregate data will still move through quickly those flows that I separate out will not move through the box at the same speed as their aggregate that they used to be members of and are joining they have to find their place in the new aggregate that takes a bin time uh so I'm convinced that there are no unsolvable issues with flow aggregation that flow aggregation from a qos standpoint can um do what we need done um now let's get into some details here and uh and say what details we don't want to get into and what 82 has not done so obviously flow aggregation is critical um we have not no one has proposed an 802.1 a multi-layer flow wrapper protocol to wrap flows into an aggregate uh no one has proposed one I'm rather hoping they don't uh to be honest but that's a personal opinion I will point out that the flow identification methods that we have defined in 802 do allow do support implicit flow aggregation that is I can recognize 25 different flows and assign them all the same handle uh the same stream handle we call it and then I treat them all as being belonging to a single flow so I can do flow aggregation by recognizing all the individual flows and assigning them to"
  },
  {
    "startTime": "01:46:02",
    "text": "an Aggregate and that is sufficient in a small Network for [Music] um uh obtaining the value of flow aggregation and the big value of flow aggregation is that after after you've aggregated the flows they go flashing through the network with lower per hop delay um I've got another slide deck that I talk about that uh it's publicly available and it's got a lot of wrong stuff in so I don't highly recommend it and I didn't point to it in this but flow aggregation gets you a huge speed Advantage uh it greatly reduces your end to lend end to end DeLay So flow aggregation is critical for several reasons one is reducing state but the other is getting data through faster aggregation speed everything uh greatly reduce your delay I I want to stress that um and so we can get that in one with implicit aggregation now I've mentioned this before for you can use this bin rotation mechanism to aggregate your flows and I'll say that qdv is a good fit for aggregation because it delivers the aggregated flow at a steady rate and that's important if the aggregated flow uh doesn't move at a steady rate if it varies in speed that increases greatly the amount of buffering you need when you disassemble the aggregation uh and in a frequency locked region I believe that in a free Cony"
  },
  {
    "startTime": "01:48:01",
    "text": "locked region you can do flow aggregation disaggregation splitting merging mixing with the minimum possible number of buffers uh uh meaning uh that delay at every hop is proportional to inversely proportional to bandwidth okay and that uh uh same for buffer space so uh I believe you can uh if your frequency locked you can do an optimal job now one thing I have not talked about a great deal is labeling the bins I'll just do this uh orally that's the end of the presentation um B uh tagging or marking the frames with bin numbers versus using time um I've convinced myself that a combination of protocol or marking frames or running a protocol separately and timing um locking my input clock to your transmit clock uh in some fashion are both necessary to make any of this work at all and you can shift around you can use primarily labeling but you have to have some time checks there periodically you can use primarily time but you have to have some protocol checks period I odically to make the frequency lock system work when you do"
  },
  {
    "startTime": "01:50:01",
    "text": "aggregation and you go to split out that aggregation either to pull out one flow to send to another aggregate or to just end terminate the aggregation and split up everything um you've got the same problem at the end of that aggregation that you have from one end of a link to another in terms of identifying which bin the frame came from and every frame has as many bin IDs as it is deep in an aggregate so if I've got an aggregate of Aggregates of flows then those inlows have three bin IDs for the outer aggregate the inner Aggregate and the flow itself they have three characteristic cycle times three bin IDs that have to be identified when that flow gets out to the edge and has to be in some sense synchronized with where it went into the aggregate or the link that it went into um and I'm convinced there's no trivial answer to that there's no one answer I know that at the outermost level the link level I can get away with just looking at arrival times of the frames using the three timestamp that doesn't really apply to separating out the components of an aggregate so I'm convinced that some kind of rapper protocol or something is required but there's a lot of room whether you do that"
  },
  {
    "startTime": "01:52:01",
    "text": "with bin numbers whether you have time markers whether you have Sonet framing information uh there are a lot of ways to do that but however you do it I believe that this is the basis for how you do it that this timeline number three assigning everything to a schedule where you meet all the requirements for bins is the heart of how you do TDM at all and I think that's the only TDM is the only thing that scales to a large Network and I think qdv is a good foundation for that right now there's none of the higher level protocols and um I'm pleased about that because um that is where cooperation with ietf is what makes it happen so any final questions um can I uh yes please yeah I will question it is not directly related to your presentation today but um the success of the internet was based on the statistical multiplexing and it's efficiency but somehow I uh um T TSN and denet is trying to go back to the TDM era right telephones"
  },
  {
    "startTime": "01:54:00",
    "text": "yeah I'm not so sure if that is the right direction actually in that networking group uh we are still uh one one way of our focus is uh based on the statistical multiplexing and work conserving schedulers um I think that is one way of going and I think that is the right direction well yeah I thank you I couldn't agree with you more now um so what is happening um what we observe in TSN is that people who have always done things with a big wiring harness and lots and lots of individual copper wires and I push a button here and that at the speed of light turns on something at the other end have gone to special purpose networks and now they're thinking about well the special purpose networks I have to rethink my fi every five years and I'm getting tired of that why don't I piggyback on ethernet and let them do the hard job um but they still want it to work as if I push a button and at the speed of light the current flows at the other end I think the way you do things is you use different algorithms where uh uh you have smarts at both ends and you have a model and both ends have the model and I tell you I give give you frequent updates as to what I want to see happen at your end and you make that happen rather than telling you right now put in an extra two10 of an amp to the"
  },
  {
    "startTime": "01:56:02",
    "text": "motor uh and expect that to stop the train at the right place uh but that's a different way of thinking about your problem um you you've got Engineers that are really good Engineers that have figured out their problems and now they want to translate that problem to an Ethernet Network you can't just tell them you're thinking about your problem the wrong way then they don't buy your equipment okay so I suspect there are problems I know there are problems in the small scale where the best way to go about it is to make the network behave more like a push button operating a light bulb um if I look at the service provider thing when I when I look at what we're talking about being able to offer someone a guaranteed bandwidth and say Here's a pipe I guarantee this is as good as an Ethernet link you will never lose a frame you have this much bandwidth and you will never lose the frame due to congestion okay I see that as having some value okay um nor this is the only way I know to do that so J I'm sorry to interrupt but actually we have three minutes left and in the queue so maybe we could give some time for them to make a very thank thank you very much nman I I want to discuss with you more about this thing later thank you"
  },
  {
    "startTime": "01:58:00",
    "text": "okay toas please go ahead yeah I wanted to just on this point I think the the model of a deterministic bounded latency just has in the design of many uh control application a lot of benefits of trying to haggle around with stochastics right because when you're trying to build something reliable and you're using a stochastic model um you're also in in a big amount of you know probabilistic um complexities and uh you I haven't seen anybody who's who's managed to you know allow people who want to deploy a reliable deterministically reliable system to to get away from from not having to to hire math experts on that whereas I think when we do have you know a well-written deterministic bounded latency uh controller plane system that uh you can really um deploy that uh without having to understand complex math well I feel very strongly that this what I've just presented does not require any complex math it gives you the answer it supplies what you want and it doesn't take the sophisticated math it takes Edition okay Tong T please okay thank you I just have want to add a comment that uh to me TS is not only a TDM scheme it has uh time based uh uh methods and also asynchronized methods that's why I I think in normal um standards there are a Time based cqf and counter based based the cqf uh so either way can uh have a bonded latency but have different uh uh formula maybe but uh I think both uh approach can uh"
  },
  {
    "startTime": "02:00:02",
    "text": "give us a b latency performance that's uh just what I want to comment thank you thank you if I if I can say one last word uh why did I give this presentation the reason I gave this presentation is that I'm hoping Deb net will look at this combination of these two documents assuming this is going to go ahead assuming this is going to go into 1dc that uh Deb net would consider making that as the basis for whatever you're doing in order to scale up because I think the physics are going to tell you that the only way to scale up is to do TDM and I think this is a place to start thank you very much uh norm and really appreciate it many thanks for you allocating the time to give the presentation make the preparations and join us in this early hour of yours yeah and thank you for letting me pontificate and many thanks for everyone for the good discussion time is up David if you want to say a few words please I just wanted to Echo your remarks Charles sorry j thank thank you very much Norm very useful excellent use of two hours indeed great thank you thanks everyone Let's uh call it the end uh we are adjourned and uh keep uh continuing the discussion on the list on how to proceed thank you thank you bye happy holiday if we don't talk to you then"
  }
]
