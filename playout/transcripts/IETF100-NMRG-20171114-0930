[
  {
    "startTime": "00:01:35",
    "text": "hello good morning everyone welcome to this Anna Margie meeting this is the second session that we have this week yesterday we have the first session yesterday we were talking about mainly on intent-based networking in the some kind of a report about an MRG and the future of the group itself today we are going to concentrate the presentations on the use out of artificial intelligence for network management this session has been organized after call for participation that we send along of course the NMR gmail is but also together with out our other medalists in other communities as well the approach behind this session is that we wanted to revisit let\u0027s say artificial intelligence techniques and solutions to observe whether they can be used for network management this is not something new because network management has been already observed from the artificial intelligence perspective but the area itself major along the years and then it\u0027s a good type to check whether artificial intelligence techniques and solutions could be used for network management in this as a new context so of course this is the note well bushes are already being circulated so we have some online sources of course then we have the agenda materials for the presentations that we have today as well as the remote participation we have already volunteered to guys and the audience to be a minute takers and as well as the jabber room so as I mentioned before this is the special session on the user intelligence for network management and then we have organized the presentation in let\u0027s say to internal sessions first we\u0027re going to see a set of presentations then we have some questions and answers so this is to be and then to see with additional presentations and then in 2d we\u0027re gonna have some discussions conclusions and plans for the future as I mentioned before we wanted to revisit the use of artificial intelligence this statement has been collected from the open call for British patience so he visiting AI "
  },
  {
    "startTime": "00:04:35",
    "text": "for network management seems to be timely and more appropriate for this time so we\u0027re gonna see now use cases and he searched his outs so far and then emerging landscape for AI networks it does work in the US so thank you so I will start by motivating a little bit this work I don\u0027t think they need to explain why machine learning is relevant for networking I think that more or less everybody can agree on that but the thing is that it is working ok now it\u0027s better but I would like to in this in this talk I would like to talk about ok we can agree that machine learning techniques are useful for networking but then we have several questions for instance which kind of machine learning techniques which are the applications or how we can apply and in our work I will try to show at least one of these applications quite specific so the idea is to use these deeper informal learning techniques so deeper informal learning is let\u0027s say somehow our very recent breakthrough that we can claim that it was invented by Google through this paper in 2015 where they managed to train an agent to play a video game so the idea in this case was that you have a nation which is we are trying to teach it how to play a video game the agent will see the video game as the state ok and it will act upon this state through several actions in this case it has three different actions right go left right and straight and the idea is that the agent will at the beginning try the actions let\u0027s say randomly like a human that does not understand anything and it will understand how the actions impact environment ok so if I move right what happens if I move less what happens and so on and after a while it will understand which is the relation between the actions and the state and the idea is that the asian has a goal which is optimize the reward ok so what Google showed in this paper is that you can actually train an agent to understand which is the right set of actions so that in the long term it maximizes the reward which means that learns how to play this video game so we\u0027re trying to do exactly the same to networking which is it\u0027s a little bit straightforward in our case the network is the environment ok and then the act Shen\u0027s is change something upon the network we need to change like the "
  },
  {
    "startTime": "00:07:36",
    "text": "routing configuration or you need to change the serving fraction path or you need to change something outdoor network then out of this system when you change something you will receive two signals the first one is the state this means that when you change something on the network something will change right so you can you can understand the status and this is a very broad statement many many different answers are valid but let\u0027s say that you can think as the state of the network ask the traffic which is the current traffic load and which is the performance that my network is providing me out of this network network configuration and state and then you have the reward the reward is the target performance of the network right so the reward is how well more than the target performance how far away I am from the target performance at the administrator has set okay so you can train an agent to do that and let\u0027s say it should work so I\u0027m going to skip this because I don\u0027t have too much time but let me explain then what kind which which was our experiment which I have to say that it is a work in progress we have a paper and it will be updated with new results so the idea is that you have one of these deeper informal learning agents and we can change the routing configuration and for this and in this scenario we have assumed that the routing configuration is as simple as the weights of the links pretty much like in OSPF not a strictly speaking OSPF as the IDF protocol but as a routing protocol that you set the weights and routing happens then which is the state okay when you the state is the traffic matrix okay so in each step we have a different traffic matrix the traffic matrix can be actually the same as the on in Russia step by the idea is that in each step the agent will see one traffic matrix and which is the reward the delay okay depending for a particular traffic matrix and for a set of weights what you will get is a certain reward delay out of the network then which is the river the river function so we train we want to train the Asian to minimize delay so the goal of the Asian is find the right set of weights for routing such that for that particular traffic matrix the the average delay is minimum so we want to teach the agent how to route autonomously there is a quite interesting discussion at least I hope that you find it interesting which is that the River function is actually the network policy is exactly the same so what we understand as the network policy and yesterday we have a very interesting session about intent so the river function is the mathematical representation of the intent that you need to install on the Asian so that the agent will operate the network following that policy I\u0027m going to jump a little bit over the methodology it\u0027s also in the paper but pretty much what we did was we compared the performance of this agent with a very quite sophisticated optimization technique a traditional "
  },
  {
    "startTime": "00:10:37",
    "text": "atomization technique which is called [Music] simulated annealing and the idea was that so in in RL what we do is we train the system with 100 thousand steps random steps after training we for each traffic matrix we asked the agent okay give me which is the optimal weight for this particular traffic matrix and then we compare that with a similar tunneling with it which is a traditional imaging technique I which is a search base and for in this case for annealing we allow it to run for 1,000 steps so we asked similar tunneling okay for this particular metric which is the optimal weights and you have one you can iterate 1,000 times so you can find you can test 1,000 different weights so it\u0027s not a very it\u0027s a fair comparison with TRL but well it\u0027s it\u0027s enough as a benchmark and in this slide you can see how the Asian learns this is the traffic intensity which goes well beyond 100 because we wanted to test also what happens when you have a network which is very separated that if he\u0027s losing a lot of packets and here you have that the average delay and each box plot represents how many training steps we allowed the agent to have so when you have two 2000 you get worse performance and when you have 1000 so it shows that the Asian is learning and you can see this very common exponential decay that is very common in machine learning so the more training you get the better results you have until you hit a flat a flat curve and this is the performant result with respect to simulated annealing so it\u0027s the same but here we see for each traffic matrix which is the delay that each technique achieves and as you can see both results are quite comparable they achieve similar performance still annealing works a little bit better and we want to tune a little bit the agent to perform better but it is also true that for annealing so for each traffic meeting annealing has 1,000 steps to optimize while the agent has only one okay so from that point onwards on the presentation what I would like to discuss is at least what I have learned and which are my conclusions regarding the use of deep brain formal learning techniques in networking so I have tried to make this as light as general as possible and not strictly related to my to my work DRL they have amazing advantage in in our area that\u0027s at least my opinion it\u0027s a quite quite an amazing technology so on the one hand it does not require any kind of prior knowledge of the network you don\u0027t have to explain it I have this amount of routers the capacity of the links is this one nothing because it understands the system as a black box where you have actions and you get a reward that\u0027s it so that\u0027s quite quite amazing it works online and in real time so you can actually run it and expect real-time optimization of the system "
  },
  {
    "startTime": "00:13:38",
    "text": "something which is extremely hard to do with other kind of techniques and it is autonomously not just for optimizing but also for learning it\u0027s not like super vise learning where you have to generate a dataset the agent will generate its own data set you don\u0027t have to do anything it will play around with your infrastructure for a while until it understands how it works and then it will optimize it so with respect to traditional optimization techniques which is what we have today deployed it provides a constant optimization time which means that any kind of optimization technique typically has to iterate I\u0027m trying to find which is the best one in in DRL after training it\u0027s just one step that\u0027s why you can achieve a constant time optimization so it\u0027s model free typically for optimism our typical I know always in an optimization technique you need to run it on top of an analytical model of a simulator right and then the analytical technique will search over many configurations using these analytical model or simulator to try to find which is the optimal one that\u0027s not the case for DRL you can make it learn directly on your infrastructure which means that you don\u0027t need to simplify anything it doesn\u0027t matter how complex your net worries and with enough layers although this has to be confirmed with enough with a deep network you should be able to learn it and it\u0027s a black box optimization so any kind of optimization is tailored to the problem that it is trying to optimize so that\u0027s not the case for general you only so if you want to change your optimization function you just need to change the reward function but not the algorithm you have the same algorithm for different reward functions and in a duration optimization technique any typically what you do is that you tailor your algorithm to your optimization goals then of course it has many challenges which I will also like to discuss the first one is training okay so yeah it\u0027s it\u0027s very cool so but it\u0027s very hard to think that I will allow an agent to run my network during the exploration phase where it has to learn because it will hit actions randomly to learn what is happening with infrastructure like a kid right so it has to understand how things work so at the beginning it will be a little bit harsh on your infrastructure and probably you are not going to allow training to happen online and that\u0027s not a come that\u0027s a common issue not for us but for anyone willing to apply these kind of techniques so what other people are doing is well you can train it on a simulator that\u0027s something that you can do and people are doing that in other areas then you lose some advantages and then once you have the system trained you put it online to work in the real network another approach which many people are doing is to train it with us with an expert so you take the agent and you show it how an expert is which are the actions that an expert is performing in order to optimize the network then the Asian will learn okay those are the right the right set of "
  },
  {
    "startTime": "00:16:39",
    "text": "actions and when it has learned these actions we could put put it online and that\u0027s not quite common approach and the expert can be either a human or can be another algorithm that you rely on and and you believe it\u0027s it\u0027s it\u0027s it\u0027s an expert for instance a tradition optimization technique then there is a second challenge and that\u0027s I will say a little bit more conceptual but I will say it\u0027s it\u0027s bigger which is the lack of explained ability so yeah deep neural networks are very cool my students are very happy working with it but at the end what you get is a black box it\u0027s something that you don\u0027t know how it works you cannot open it or if you can open it you cannot understand it and you don\u0027t know when it will work and you know when it will fail and you don\u0027t know why it is working and also academically that\u0027s not an issue I understand that from the industry perspective that\u0027s a big issue because you cannot it won\u0027t be able to offer you anywhere auntie\u0027s and it won\u0027t be able to offer you any kind of turret shooting if it breaks you don\u0027t know why why it is not working and you don\u0027t know how to fix it beyond trained it more and that\u0027s I understand for the industry perspective a big big issue also in terms of liabilities and so on and again that\u0027s not an issue just for us it\u0027s an issue for anyone using these kind of techniques so what a possible solution well you can always train more and that\u0027s it at the solution an another solution is that many people are starting to do something which is called explainable artificial intelligence and they realize that they use a second neural network that they connect to the first one and the second neural network is used to try to understand how the first one is working and why it is working that way and once you start understanding it and you can start relying on it and you have a very nice paper here which if you are interested you can check ok then the River function so I was discussing this at the beginning so they were function is like literally the network policy so yesterday we were discussing the intent language so in in this case what you need to do is so let\u0027s say that you have an intent language you compile it and you render it into a reward function which is the mathematical representation of your intent language once you have done that you install it on your agent and then the agent should be able to operate following that policy so I believe that what hopefully you find it relevant for for this working group for this research group but this makes me ask myself some questions for which I don\u0027t have an answer that why we call it research working group I guess the first one is can we actually represent any Network policy with these functions I don\u0027t know because those functions have some constraints for instance they are continuous and so on I don\u0027t have an answer for that but this is an important question if if we are willing to go that path we need to understand if this "
  },
  {
    "startTime": "00:19:39",
    "text": "reward function can represent any kind of network policy and then okay how we can compile this intent language into these River functions so as a summary and I\u0027m very quick so I really believe that this is an amazing technology it is to me at least it really represents the full realization of an autonomous intelligent network which is something that we have been discussing out in the past and to me this is what and how I understand this kind of autonomous intelligent system but it and it has many many advantages real-time operation plug-and-play no configuration just pick your reward function but it comes with important challenges so the first one is training that online training it\u0027s challenging and the second one is that it doesn\u0027t offer any warranties and here you have the paper the code the data set any everything it\u0027s it\u0027s up there thank you questions very quickly questions there is a question from the Java room the question is have you tested the or DRL model against virtual agents against the virtual agents no so far we just have tested it against a very traditional demolition technique hey this is Alex come I have a question as well I couldn\u0027t explain a little bit basically the odd function how it relates to the policy I did not quite get that part because a reward function but presumably have to be something that is quantifiable yeah so I\u0027m wondering with you how is policy 105 oh yeah so that we were functioning is a continuous function which you can understand it that the output is a scalar value so the higher is the reward so the agent is trying to find or is trying to act to find to increase all always this value okay so the higher is is the better so the reward function should be which is the distance between in a mathematical terms which I don\u0027t know how to do that but the distance between the current performance of your network with the one that you are trying to achieve and and the distance is somehow the reward I don\u0027t know if I was very clear we can discuss out of like presentation or I want to ask do you consider it on virtual agent because if we simulate the top there are multiple or entity in multiple our motif or server so I guess they you think about a single agent so it is more proper to SVN cases but if we reflect or your network infrastructure is involve error to consider multiple age on vertical Asian yeah but for agent yes or will 12 vertical or will 12 wait for vertical yeah so we only did "
  },
  {
    "startTime": "00:22:40",
    "text": "one Asian on top of an SDN controller finding the optical optimal configuration for a single network no and no vertical asian anything else with just one and I don\u0027t I don\u0027t for this scenario I don\u0027t see why we need more than one token regarding their looting routing algorithm is that what kind of routing a you so what we\u0027re doing is like a deterministic or it\u0027s done realistic so okay that\u0027s a let me elaborate so another limitation of these kind of agents is that the action is space cannot be very large so the act the output of the agent cannot be okay give me the CLL configuration formal all my notes it has to represent like levers like in a video game right so a few levers that represent how you steal your network so in this case the Asian was choosing the weights of the links similar to our or SPF I guess my question is that actually is that link to any kind of routing algorithm or is that working for any kind of routing algorithm so it\u0027s we are trying to express an algorithm where you pick the weights of the links and then each router will pick the path with the land with the last wave which is to me somehow OSPF it\u0027s okay okay we need to move on to the next presentation but after oh I will have time to come back to this discussion thank you hello good morning everyone Shinya from poly today I will share our research program you see BAE in u.s. management that a CVS one of the civil one of the popular generative models and they have achieved a great success in a an area the the character is it can extract a hidin figure from the training site data and then reconstruct the distribution model of the focused object because this this 24 faces this face is are not real they are generated by the Civic models if we input the label such as we want a female or male we want adult or children and then the CV can generate the virtual phases for us and why we and here we want to introduce this can conditional aberration or auto encoder into network management to provide inference ability for the qsr "
  },
  {
    "startTime": "00:25:41",
    "text": "performance this is our basic sort we think that the network is a complex system and the curious parameters have some Haydn\u0027s that is statistics feeder which is hard to express by the simple distribution formulas or the combination of the distribution formulas therefore we can use the CVA II to model the network us and then we can use the Train model to generate the new samples and finally we construct reconstruct the curious distribution according to their generated samples that is always truth be a pointer that is we input a condition like we want a female if you want a busy busy tank us description to the CVA II models and then it can generate a distribution of the QoS parameters and then we can do some actions such as and such as we can implement the proactive operations to reserve the bandwidth or the priority sightings or migrate the flow of we focused or other other application is we can evaluate the actions that if we migrate a flow or VPN sighting to a new path we perform well enough for our for our SLA okay and this this is our plan but I prefer to close the the the black box as this looks more more simple here we focus on the icarus variation that caused by the traffic such as we pay more attention on the queuing delay than the transmission delay we so here we use the traffic metrics as the label or or say it the condition of the CVAG model and then we use the Q s as the other value like this the end to end delay may be ten point two and we we set the label as one and then we can got the training training process and then we go we obtain the CV model and after we we got model we can infer we can infer the Q s distribution by into the traffic metrics as a label or as the condition and then it will generate the qsr metric samples for us and then we can use these samples to create the Q s distribution in any conditions here we have a simple simple cases is a demo experiment that we regenerate we set up a tent an eye a traffic label from one to nine and we set a Hyden rules that the Q s label is the the Q s label is a normal "
  },
  {
    "startTime": "00:28:44",
    "text": "distribution obey the normal normal distribution by the mean the mean value of label multiple 10 and the very earth and the variance is three like this we have this distribution of the proper probability distribution of the Q s parameters it is virtual regenerated and then we use 1,000 sample to Train the CVAG models and we took it will take about 200 seconds for per training time after we got the CV model and we will test whether the model can reboot the the probability a distribution for our training set so we input the training label from 1 to 9 such as you will input the label to whether we expect the CBA model can output Q as distribution with the min value of 20 and then the VAR the variance of 3 that\u0027s a result for the non label we can obtain the accurate distribution the gray is not very clear the gray bar the great curve and the gray bar here is the cases or the samples that we generated and the red one is the the cases the samples the CV model generated for us we can do you can see that this it looks similar they the arrow of mean standard and the 19 point like this so that is not a very important one because many many AI models can do that so the interesting one is if we here we input a label that never appear in the training set can see BAE build the same distribution or our expected distribution for us such as here we we have got the Civic model and then we input the label 11 which has never appeared in the training set and then whether we can got the distribution of the mean value with 110 and the variance is 3 the answer is yes for the non label we can also obtain accurate distribution so here this label 11 to 14 that\u0027s very important somebody may say hyeon\u0027s in your distribution is too simple and you have only two parameters and one of the 1 now is a fixed but for "
  },
  {
    "startTime": "00:31:45",
    "text": "the it is based on the the knowledge of human bring that because at the Machine can not know what is what is the concept of the normal distribution we know that but the Machine do not know that we can build up the the model and recover the distribution from the training set okay this model required required new maybe new merriment technologies to feed to feed such as we need more high frequency and accurate accuracy data and then to see the feed out this model and also we will face to the data expression and the transmission problems now we have two moles for the CAE training the first one is as part mode which means that we trained the model severe model for each pass as a unit such like this one so in this case is we need to build up the path set based on the route that what is that is we are working on now and our our destination is we want to build up the we want to build up the severe mode for each of the three in mode for each note like this it says still in a challenge if we can do that we can combine each of the note as a past as the path q s distribution but now we we cannot simply add two or multiple note q s distribution into one because it\u0027s not reasonable in mass so why we use severe mode when why not use other mode we have some advantages the first one is it performed quite well for known distribution and better than the competitor competitors one of the competitors in the area is against this is also a powerful tools for the generative model but after many many experiment we we we found that the against performance not very good in other scenarios so we choose CV and the second one and and the most one a important one is the the CV model can infer the the unknown cases for the milk mode the few of the two existing tools can do this and so one is the the CV can cover some complex problem we will meet in the in the network now we are using this CV mode to predict and infer the Q s in given conditions and such as we can proactively proactively avoid the performance one the first when the burst "
  },
  {
    "startTime": "00:34:46",
    "text": "calm we can predict the the curious by the input given conditions and on another uses is we use the Q s model to infer how and there are if for the simulation before we deploying te policies and in the future we will try to use the the real Q s data to train other mode and we will try to explore the solution of the the node model then the node mode which is more missing is it is more powerful and and useful in our problem so here\u0027s a conclusion that CV can can use to model the network Q s and the facility has been approved and sir and second one is severe has many advantage especially it can infer the unknown cases and third one is pass mode is easier we have implements that we will try to explore the solution for the node mode it is still a challenge and said and and finally we can we need new measurement technologies to support our data capturing and here are some information for you and welcome software I do not think it well professor I have a question for you about you you can actually a severe emote can propose a potential solutions for your deep learn tip refund reinforcement learning for to solve the start start problem the training yeah you can use this kind of things to drain yeah the question my question is more clarification what you do is you predict what will be the Q s given the current traffic matrix yeah so it\u0027s like a network simulator somehow yeah okay okay we need to move on to the next one thank you I know we have the discussions that forced general uh you have question no my jabber is down I\u0027m trying to get back does anyone is anyone in the jabbering is a dog there\u0027s any questions there yes [Music] "
  },
  {
    "startTime": "00:37:48",
    "text": "okay so I\u0027m sure we\u0027ll present some work that we have done on traffic analysis I will not give any very few details regarding the techniques I mean give more an overview of what we have done and what has the problem you are facing now so as you can see I put in parent it is encrypted because we are not only working on encrypted traffic but you will see that are some nice result and challenges that remain okay so yes we\u0027re getting the challenge okay so traffic is very important for a lot of stuff of course for security as mentioned in my title you want to do some traffic energies for detecting bad behaviors for enforcing some access control you may want to use for doing some Q s as well and particularly as there is one topic which is traffic classification when you aim to label your traffic or traffic flows with some of course letters and profiles which can be different depending on the context should target it can be for example can be the type of application which is used may be the user may be the type of the attack if you are in security context and so on so it\u0027s a very diverse topic and it really depends on the context so and you look with us has been known I mean in terms of let\u0027s say legacy technique of course for if you want to classify some application or type of application part number could be used at least in the past for if you want to track some user you can maybe use IP address if you want to try some service and you can use maybe DNS name and so on so this kind of standout technique in order to to know what type of traffic or what equal its to and of course if you can you can maybe do some deep packet inspection or looking at the content of the packet in order to add a bit of your father is inside of course it has a wallet change and no a mini application rely on same framework so it\u0027s very hard to distinguish them many of them are web-based meaning that the almost all use the same protocol I mean with some HTTP HTTPS and so on of course we are all using now cloud CDN and so on so IP addresses are not so very valid and Iquitos of for example original traffic and of course there is the problem is the encryption technique okay with privacy and concern that weighs more and more traffic is encrypted so meaning that mainly today it would be maybe a big carry tacular a lot of traffic is web-based and encrypted so basically HTTPS is that we have applied a lot on HTTPS all right so yes of course that\u0027s good to have encryption I mean for user protecting in particular you your privacy but on some "
  },
  {
    "startTime": "00:40:48",
    "text": "who it\u0027s also legitimate if you do some network operation to know what type of try think is going on in order to do some operation so there is this kind of need in particular for security if you want to apply some security policy you need a bit to know what happens and then somehow you mean don\u0027t need to break and you should not break the user privacy a lot of solution today just rely on an HTTP proxy in the middle in many I mean many companies ensign which i think is not good and so the question is can we do some directly in first work I\u0027ve done can we do some monitoring and fht PPS without of cross decrypting whether it inside so this is a first example and very rapidly I will show you because I present you this example in a previous and MLG session so that is that ok so different kind of things you can do you can try to if you look at where we can try to look at what is a web page which web page is loaded which is called basically a website fingerprinting a lot of record don\u0027t try to categorize traffic is it fine Apple web is it superior is it a voice over IP and so on and you are being citizen either way we try basically to know what is a service which is used so basically we need to know the service provider we have lay your classifier we first wait to know what is a service provider like Google is it Dropbox is a book and Sun and then the different services that the heart behind so we will want a z4 for each service provider we use simple it\u0027s a simple or Revere it\u0027s a regular decision tree algorithm to do that and one one main I mean when main contribution was looking so digital right here most of the features that we use our pretty much the same and we extended a bit rather than look he can for example packets I we just extract the encrypted payload and we apply some statistic on the encrypted pill and actually this gives better results here yeah very rapidly some some result and as you can show here we look for each service provider in which a range of co-education we are and mainly for 50 168 service Friday we have a classification so we are really able to know what is the service I use for this provider without the encryption what is the traffic with an accuracy around 95 percent okay I mean not in too much into the detail so then the question also that can resist is there is no encryption is it so hazy things that are easy looking at this example so basically I we use our dark net or dark space well where we have the 20/20 network that only used for that we should not you receive any traffic on that and as you can see on the Left pictures here all the packets we receive in a short interval of time or on you have to to square one is origin with IP address and port and user is the destination or Zoe\u0027s IP address and pons and what we know is that all these traffic it\u0027s alleged he made them left "
  },
  {
    "startTime": "00:43:49",
    "text": "but we don\u0027t know what there is a really inside we know that there is probably thios there is a scanning and so on and let me to want we want to achieve is to have the right side figure right we basically have the different kind of slices which represent the scanning a TCP scan we have on the docket and to do that we try with some I\u0027d say uh no sure to like share ricotta and we were not really satisfied with the result you have to still mainly our components and we use another technique so it\u0027s traffic is not on fifty by the primes we should not of course in analyze individual packet but we should correct them in order to find the right pattern so even if it\u0027s not so simple so we use G da which is topological the tanneries we only use one part of the TDI here we are no bit further what was the mapper which is R is a first step which is basically you can see that as a partition basically stirring so we have a big space of data multi-dimensional data and then we compose few let\u0027s say a per cube in that and we are this intermediate representation that we built in the middle which is based on kind of clustering and then we measure all the result together maybe you can discuss more if you want more detail about this technique but with this technique we have been able of course to be able to to level a bit more they\u0027d help you have in the document okay going further of course I will give some and some feelings about there\u0027s a problem we try to address of course the first question is always the same we should target the right problem and when you do some traffic a nice traffic classification so either you say that I want to classify everything which is would be very hard because I think you should be able to learn almost everything and it\u0027s not very good because it means that you want to to know what is everything as well or are you better for me which the better second guess it may be simpler because you prefer to look to build signature for particular for example use I mean use cases or profiles you want to be able to detect for instance if you if you think about security and this is more compliant if you are a bit against mass surveillance because you don\u0027t want to try for example in individual user but maybe you want are any service usage but only ones have issues that you want to attract of course you have to think if it\u0027s you can do your honor isn\u0027t a single let\u0027s say that instant so multiple that instance like if you look at a single flow if you want to correlate different flow together I think the signal technique is usually a more efficient but is more difficult to apply I will so then we have the methodology which is very not really going to this it\u0027s very classical so we straining and so on one thing that I want to point out is already point to it and in the mailing list as well is that a future engineering I think isn\u0027t the core half as a problem choosing the right feature I mean you can you can use a different algorithm we "
  },
  {
    "startTime": "00:46:49",
    "text": "need people just just try a lot of figures on without thinking first about maybe the second slide maybe thinking about what also algorithm really works I think it\u0027s really important to know use of course algorithm like black box but also features so one one thing which I think is very a representative is a when you do some a distance metric many people just choose algorithm on the shelf they use distance metric on the Shelf for example you can use a given distance but most of the time there is nonsense I\u0027m just looking for example at port number if you look what number as numerical values of course you can do it you can apply distance can compute the distance but there is no R meaning to compute a numerical or akkadian distance between port numbers for instance we need something more advanced just to show you some some work we\u0027ve done so far here is we look again the darknet and of course there is some semantics in sport and the idea was not to automatically we have also done that the semantics of ports from knowledge but to build our knowledge based on SciTech her behavior so we look at what an attacker basically scan consecutively in terms of ports and we constrict this kind of graph and then with some community detection algorithm we were able to see what our port number which seems to be close I mean in the mind of an attacker which then we can use this information to into our model for the techniques and attacks so I had a question for the jabber just a one slide in the rest on so and just to conclude I really think that regarding the encrypted use case based on our experience on what you can win literature I think if you target well precise problem you can easily find a solution can find algorithm features that could give you some reasonable and good results I don\u0027t think this opera of course it suppose that you are adult Amin in very precise database for different environment so there is of course some things that you have to fit and it\u0027s not so evident like that what are remaining issues that most of the case we don\u0027t consider as adverse behaviors and what I want to do to mention is that the fingers of course is always evolving we have new protocols have kind of optimized protocol if you look at HTTP and even if you have a GPS of course on top but you have HTTP to so know things are multiplexed are compressed and so on so it\u0027s very difficult today that we can easily apply so technique because you before basically we have one flow and we try to label let\u0027s go if I can do an analogy today we have the big Chanel where we put everything in a generic it\u0027s all the same application all encrypted it\u0027s mixing stationary use and so on it\u0027s very more far more difficult that you can read in most of paper actually if you look with the new context and I\u0027m that question from the jabber room about datasets how balance or imbalance is your target class okay so it depends on because I represent a different use "
  },
  {
    "startTime": "00:49:49",
    "text": "cases in first-year HTTP use case it\u0027s quite well balanced because we control or we collected that I said of course was G as if example for the darknet it\u0027s not so well balanced because I of course it\u0027s real data that we gather so we cannot really control the different proportion Shin from Paulie I have a question that you said that you need to collect a data post from from then from the network and out of the network and my question is how you how you align this to two types two types of data because we need to build up the relationship between the in internal internal and thank you you sure I mentioned that I didn\u0027t really focus on that but that\u0027s true that in cases we went to analyze traffic we need also some knowledge which are which is out of the network and then III deepen the case maybe to cite an example for instance we have this dark net space and also we have also only pots so there are two different I mean both on it well but two difference Network space for example we are collecting what possible which I use by the attackers when they try to connect to ssh to on me but and we can fight stance if you if you look in our database we can see that that some hoes are some changes is a password which i use because i try together if i get some other devices vio two devices at that time and so you can see also the same change in the port which are targeted or in the in the document and of course we it\u0027s really depends on the context it\u0027s it\u0027s a very complex and very difficult question and of course it has to be taken as individual context question each time in please wait we are just giving the mic to show your online hello yeah please go to the page 9 yes we can hear you can it go to the page in line by July sorry can you repeat this so in again yeah because it\u0027s table so I can understand exactly which which is the which is the young master is not a day which cologne is their own method is the selector feature and what\u0027s the meaning of the of the lamb of your feature for the accuracy so if I understand value what question was regarding the features so we have we have difference at a feature I was very rapid and future so classical feature "
  },
  {
    "startTime": "00:52:51",
    "text": "has features that you extract from state-of-the-art that has been used for I should GPS traffic and then we have selected set of features that then you have full features which is state of just features plus our new features on encrypted data and then we have features which is a selection among them if I go back to once okay here you can see what other the selected feature which is the one are pouring in Pola the ones that we had it is user as one from state-of-the-art which are the selected feature which are basically based on an information gain that we selection and then so what means the tables at for each line we look at the number so for example the first line is the number of service provider for which we were able to classify between 95 and Android percent of the service which have been used and so on and then we have the different bins and so for so the first thing is the one most from penalty for the best classification because it means that for if you use silicate feature 51 of service provider I\u0027ve been steep services of 59 of service provider have been classified as I carry higher than 95 percent the underground Norwich what a skewing won\u0027t leak spitted what sample rates can you work with if you try you know when you were doing traffic sampling at certain rates how does your algorithm work or you have to be on a live tap and see all the traffic so we can you means for a time of execution well the question is regarding uh if you can apply online right well yes that\u0027s the idea because I think if you look at this particular gene algorithm it\u0027s I would call near we had time because our initial goal was to build HTTP firewall so it\u0027s I think in terms of classification it\u0027s very fast so but the problem is that we need to have the full HTTP session so it means that if you do a firewall you cannot block the connection you need to observe entirely and then block so it\u0027s not really a real-time so you call any real - ok so it\u0027s worth for Zenzi of forensic tool yeah in that case yes and then yeah we did some experiment on if you can already profile it during the middle of the connection it works a bit but of course not not that this is best okay thank you jemelle do orange just thank you for this presentation I did not understand whether you use supervised supervised learning unsupervised learning or both of them so for this one is supervised because we need to be signature fuzzy those are work I represent and TDA is "
  },
  {
    "startTime": "00:55:53",
    "text": "totally unsupervised Thank You curatu compeller I have a comment for you which is as you\u0027re doing this stuff and saying how well you can classify based on not just classical features but all these other features you\u0027re suggesting to people how they can hide their traffic better so I can pad my packet so that they all look roughly the same size I can adjust the inter packet gap so that that information goes away just a comment okay thank you on the explanation will be online Armin if you hear us please connect can you hear me yeah we can hear you and see you okay hi so I\u0027ll be presenting yearly research we are doing Kindle apps nokia Moodle free resource management with response to enforcement learning so the motivation is the following so in the future hyper-connected society we believe that we will have a major challenge in controlling and managing large networks and you can have in mind the AI enabled cloud hosted applications connected through 5g and interconnecting other thousands of IOT devices until end-users so in such a large network traditional approaches to management will be challenged because they require and the link of a model in advance and then acting and the reasoning contact model a while in such a large and complex systems it would be hard to learn in advance the model that maintain it update it because it\u0027s dynamics of this large system and we are after a noble approach the concept of learning in interaction introduced and derived from reinforcement learning so the idea in years has been already presented by the first presenter so let me go work quickly through the high-level concept here so instead of building an advanced "
  },
  {
    "startTime": "00:58:55",
    "text": "model the management agent will learn generate actions on the environment on all the system and will collect the feedback and this feedback will be separated into two for two flows the first one is the interpretation of a state so it will we define notion for state of the environment and it should also identify what is called a reward so the goodness of the state how good is the state from the Givens the objective of the management so let me go to the next chart and to show how we Nastasia is this general schema and we apply it to an elastic cloud bed application so the the a level objective is protein automatically loan optimal resource management policies with limited priority formation and adaptive this due to the changing environment so we are experimenting with the cloud-based application which receives a very low and it can scale up and down basically it can add or remove virtual machines to adapt to to the current load so the reinforcement learning module will generate action so what we what are our actions here it\u0027s adding or removing capacity adding or removing virtual machines and then by the beginning it will generate and offer random actions and then receive the moratorium input and will we identify here the state as being a combination of the current workload and the current capacity and the reward is provided as the desirability desirability of this current state and it in fact it reflects compromise between the the capacity more you are using capacity this is your reward and in case you are violating necessarily in terms of the response time you\u0027re paying a penalty so you are you are in good shape if you are using low capacity and you are not violating the wrestling and to to to reason and to build the the optimal policies the algorithm will duel the so called Q values and update these two values which which you find on loan pairs of state inductions and basically these two values will be used afterwards to to who identify see see the highest Q value for a given state so which will identify the best action to be executive in the given state so let me go to some results on the next chart and it\u0027s chart please "
  },
  {
    "startTime": "01:02:02",
    "text": "so some some results we will present here are basically reflecting the the the ways you we can derive automatically the control decisions by maximizing the system efficiency and yeah we need to mention here that the system efficiency is defined as the sum of discounted rewards over the entire time origin of your system and here we are using minimal system information no no system model the only thing we know about the system is its state and reward as defined previously so we we are observing the capacity current capacity we observe the car\u0027s response time under the workload and since the iterations of this closed loop iterations of the learning agent shows that we have converges of convergence of the average system efficiency and why this is achieved by identifying this is the best trade-off between minimizing they are located capacity and maximizing the customer responsiveness as as a reward functional indicating and on the next chart then we will show how the system it can adapt to the changing environment and to do that we first fit system model and derive kind of an optimal policies which maximize the current so we we this time we we build as as precisely as possible and model which connects the inputs on the outputs of the system basically connects the workload and the response time and as soon as far as this model is precise as the system doesn\u0027t change we can see that this model based approach outperforms the reinforcement learning approach but then we introduce a system change and which which can happen in a cloud environment because of many reasons because of adding additional tenant to cloud and sharing the resources or because of variations in network bandwidth capacity etc which makes this precise model invalid or imprecise and what we observe is that reinforcement learning continues to learn exchange and strives to maintain the return at higher level while the model-based approach is loose it is losing its performance indicator this splits me to the conclusion slide the "
  },
  {
    "startTime": "01:05:04",
    "text": "next one please so what we just initial research the done is this initial research is that we elaborated some principles for state and the reward definition in some prior works the state definitions I\u0027ve been done in ways that do not respect the separation of causes from effects so if you look the way we define Z state it it includes all the only kind of independent attributes workload and capacity which lead to the response time which causes the status is the value of the response time so the first time some other words were including the response time is the notion of the state so it\u0027s important to define a state in a way that it captures some independent dimensions of the system and then the design of a minimal reward is very important so it makes sense to the engineer and makes sense to the user of the system and here we we build the reward fraction which has a meaning it is a revenue minus the cost and in which case the return which was the discounted some of that reward also has a meaning it has a long-term profit then we have illustrated in with some experimentation so the policies can be automatically derived I didn\u0027t tell or elaborate on that but we we have paid with a real cloud system as well as with a simulator and working with the simulator allows to work on the real-time adverb and accelerates easy tests and have a quicker evaluation of algorithms and then we confirm validate the results within a real cloud infrastructure and we have obtained this original English and when demonstrate is that the reinforcement learning the right policies beat the system model-based approach under the changing infrastructure conditions I will end this short presentation with by enumerating a few open challenges with where we reach this area so the first one is the unknown reward latency because when you execute an action of the system you cannot be sure that this the effect of this action is seen immediately in the next iteration of jaragu so you do not know how long time how is the inertia of the system is is operating so how long time it will take until the effect of your action it will be upset this can be annoying that issue because you\u0027re lowering kanji "
  },
  {
    "startTime": "01:08:04",
    "text": "is immediate reward another problem we have been faced is if faint actions these feeling actions you execute on your system can be really executed so your algorithm should be adapted in a way to differentiate between successful actions from which you observe your reward and the argon should learn whether these actions were improving or not the reward and the the actions which didn\u0027t succeed just from system perspective they just didn\u0027t manage to execute it within the system so they will introduce the noise in that in that case of course the general problem will known is a slow convergence the pole start so some of the similar approaches should should be put in place to to accelerated by probably introducing some domain knowledge and the last point I would like to precise here is the abnormal environmental changes so I patient here that when the environment changes your postman learning algorithm can can do miss that and can adapt but what if these changes are situations which are happening very rare so you don\u0027t really want to learn anything because they will not have a sufficient time horizontal to lead your auger into convergence so you would like basically to ignore us these situations which are failure situations so that your your policies are not kind of too much conservative they can differentiate is the normal operational changes from from anomalies and failures so let me stop here thank you Seidenberg Dunwich apologist on what apologies did you test your model we test it with the relatively simple web applications where you have load balancers and many workers beyond the load balancers and working in parallel so you hadn\u0027t remove the virtual machines by simply adding and removing capacity I haven\u0027t read the draft so I don\u0027t know but I would be curious to see more more information about that because the abnormal environmental changes are you know really depending on what topologies you have and then what actions were able to take this is the point in fact abnormal changes like if you are running OpenStack cloud you can have OpenStack problems right here your your cloud environment you can be in abnormal situation you don\u0027t want to learn which policies to apply from from scaling perspective when you are in this abnormal situation you want to tackle "
  },
  {
    "startTime": "01:11:04",
    "text": "this abnormal situation by some specific actions you you know and not learn it what you do you want what you want to learn is when the system is operating normally it is maybe it will have variable conditions so you want to adapt but you\u0027re not not learning directories default situations I don\u0027t know if I managed to answer the question but Angela Eriksson two questions you mentioned so convergence can you give us a feeling what slow means seconds minutes what and how do you know that the traffic situation doesn\u0027t change faster than your convergence actually and that might lead to no converges at all you have no guarantee about that but if you go to this letter as reproduce right I can go mental before and there\u0027s a one before please yeah so the vault Road we\u0027ve considered is a kind of arrival workload if indeed you can challenge the system by providing really work up which changes the patterns of the time so if you assume you should assume that you will converge to a stationarity should converge to two to some optimal policies and there\u0027s some stationarity assumptions if your input load is is not stationary doesn\u0027t have any repeated pattern it will no no you don\u0027t have any current e2 to converge to anything may be another answer or so is that the underlying assumption is that your system is behaving like a Markovian model so your workload variations your state definition should be done in a way that your workload variations are still preserving this Markovian property if they do not they may you you cannot claim any convergence so in this particular case we we we had a pattern of daily patterns in fact what you see on the left is daily patterns of arrival workload and it so you have I think one week of data or a bit more and the convergence took about one or two day we to converge to the right policy thank "
  },
  {
    "startTime": "01:14:04",
    "text": "you okay there are no more questions Thank You Armen for presenting in the middle of the night thank you bye so this is I mean it\u0027s a summary of a project that is about to finish that is a cognate only these the idea was to build a network management environments that should be based on in general model machine learning weights use machine learning but ideas to use any kind of data and intensive techniques to apply machine learning basically it\u0027s as being since it has been funded by the European Commission and the 5g PPP program is very much oriented to fire use cases so the idea that we wanted to experiment with different potential techniques not only not to apply one but it is was to experiment with the different potential techniques and being able to build a to build a framework that would allow us to select the best learned or trained approach and apply well these two and we have applied it to some cases like traffic identification precisely resource management and end-to-end service management and in general apart from that there has been another applications related to automated car some an IOT well for sure they are it\u0027s based on open source and is about that to contribute to several communities this should be a little bit more colorful but well this was I can tell you this was light blue and the dark blue and is difficult to see but basically what we have here is the usual closed-loop environment for controlling which what you have is a set of policies and a data stream that is generated by monitoring danger phase and the functions that are running well cognize that they would come as a cognate das is precisely try to add a second closed loop in which what you have is a data stream and a set of the score relate related precisely to the policy engine you should read here policy engine try try have because it\u0027s say so and I hope that in the in the version that is on the in the portal it would be better if not we have to talk every tie to see this is the automatic I\u0027ll send you a better PDF so so the DSA "
  },
  {
    "startTime": "01:17:05",
    "text": "adding this it is that that you have we have in the position of making some selection of the different trained results so in more detail basically this is the same diagram at the beginning but with a little bit more of detail and abs that precisely that song over there is where several different mechanisms for machine learning several different mechanisms for training the the policy and Jing are applied with datasets that are generated by collecting real-time data there to two important note is that what we have is as I said before is that to to control loops in which we have intersection at what we call the the coordinate smart in genie that is where we are applying the machine learning mechanisms and second is that we are applying we are assuming that the in underlying infrastructure is based on any V+ as being a and a particularly texture in which what you have is a double double is the end controller one acting as the vnf level the other one acting at the at the infrastructure level so we you can understand apply Sdn rules for the to control the behavior of the different functions this grows in more detail again so you can see here over there where we are assuming that there are the two the two Sdn controllers well it will be here the other one will be here this would be controlling how the different ENS are interconnected the other one is controlling the they behavior of the vnf themself both of them are collecting data together with data related to the status of the infrastructure and data really collected by the by the VNS themselves that are processed and input to the upper loop where the trains the trains behavior is is evaluated and then selected and fed into the the the the policy engine that takes the decisions on network management the current implementation that is available in in several cloud deployments use it open source use use a set of databases to forward the different streams is uses several tools for applying the machine learning processes is using a policing Jing is using open daylight for a stallion controller open source mono "
  },
  {
    "startTime": "01:20:06",
    "text": "foreign if your castration and and well we are using up in AV Brahmaputra that is a little bit back because we started two years ago and we decided not to make any many changes to the infrastructure itself itself I am well just as a note we are using super for policy definitions for the obvious changes apart from this and this is something that is running and we will in a couple of weeks we are going to go to Paris to to set up the final demo precisely I mean not to part two part two the real Paris not the orange Gardens we are going to have the meeting and apart from that as a side byproduct we found that training training and models and using those data sets during initial data sets for training the models were extremely complex because they were a horrible lack of datasets and and and getting data set is extremely complex because first data is considered asset and is considered more and more an asset and getting significant data is almost impossible even internally in the same organization can tell you in an operator like telefnica getting real operational data is it\u0027s a real nightmare because nobody wants to this closer second there are important privacy concerns there is a lot of concerns about that you will be exposing and well with the new C GDP are here in Europe they are in Europe so it will be completely almost completely impossible because any single data can be considered as person and third even when you have the data when you manage to get the data and you get to manage to anonymize it etc in many cases is unusual for certain real scenarios because you don\u0027t have any clue about what what\u0027s going on it\u0027s true that with a tippler the deeper learning processes and you don\u0027t know alike you don\u0027t need to rely on them but then making validation and bring in the testing of the of the Train datasets unless you are bold enough to put them on the real network is very complicated and again if you\u0027re running in an operator environment telling the guys were running the operations they you know I\u0027m gonna put managing a piece of things that have learned from some data whatever controlling the network try it try it and wait a couple of hundred years till you get the the permission so we started to work in the production of sound synthetic datasets we started something that we call the mouse world the idea basically is that it\u0027s a laboratory for generating synthetic data they\u0027ve used to generate traffic samples in a controlled way and we\u0027re not allowing that we can mix real and synthesize data take for example is injecting real data "
  },
  {
    "startTime": "01:23:06",
    "text": "mix with a with a security attack real data mix with a a viral event on the internet things like that so we are in the position of generating realistic scenarios and use them to train and validate the training this is something that I told you it is a byproduct of the connect project right now but we are really willing to in the future with the VL of producing something that is a thing is very important for all these to happen that these are open open datasets and even open synthesizing tools so experiments and results can be replicated and just to finish please this is something that well is a conclusion about that the data driving in general not necessarily machine learning a whatever anything that is based on evidence from data is that we believe that the next natural step in the natural management but as I said is not only machine learning or AI we have to have we have to apply for example a sensible way of managing data sets and well still there are challenges before we see this happening in the real in the real in a real environment operational environments we have first of all that the networks are different in the behavior that from the fields that AI has been applied so far it\u0027s not so static as I don\u0027t know dynamic image recognition or or she stays this is the Diagnostics on this like that we desperately need data data sets with a whether meaningful and alberto saying before we in these environments do need to have always someone to blame you have to understand what we know is is say in a real environment is completely and applicable to have something that you have just dropped and you don\u0027t know what\u0027s going to happen and why any any problem happen well that\u0027s all relying on a question for a java room how big are those data sets how big are these data sets oh well the in general what we register is a couple or three days of traffic big in terms of terabytes we\u0027re talking about an average one two terabytes the ones we are generating right now so I have one question in your architecture you did you decide that training is something that happens offline and then you pick the model and you install it on your architecture or its online initial is the initial training it happens offline and "
  },
  {
    "startTime": "01:26:07",
    "text": "afterwards it is that you feed you keep feeding the data that you\u0027re generating and to the and you make it online so it\u0027s a let\u0027s say it\u0027s a double double loop before going on to the next part of the session to the audience I have comments about the presentations that we have so far as a group okay please go ahead I am Truphone of Italian telecom you have mentioned several cases can you can you speak up because okay I think is that troubleshooting especially for a virtualized Network function surety is very important so that we also you close even in this use case for Carbonite how about shorty yeah troubleshooting you no no definitely and this is one of the cases that has been demonstrated precisely by the people from the Nokia team in Israel something that they called oh yeah - try not to identify noisy neighbours and during the during the execution on all natural functions okay thank you so are we gonna go to the second part of this session where we gonna have presentations that somehow summarize other AI related efforts on for example sterilization berries or associations like the first one it\u0027s gonna be on I to buoy then on at sea and then at the ITF itself thanks ji-sun row so this talk is quite different from what we have seen before it\u0027s not use cases or techniques it\u0027s more I would say a new initiative in I Triple E Comstock which are not called emerging technology initiative before that it was called subcommittee or committee so this one is called Network intelligence so as I say it\u0027s kind of trying to build a new community initiative inside Comstock which is focused on network intelligence it\u0027s still not officially approved this is in the process but we are pending approval by the Comstock et Cie it\u0027s not completely new for those that are used to that or aware that it\u0027s inherits from the Chi Kok Technical Committee on atomic communication which was established we said several years ago that was investigating atomic communication so there are kind of legacy around that kind of technologies in the network intelligence currently the the mission that is defined for this ETI is essentially to support and endorse the research in the domain of artificial intelligence for software networks and towards future networks so "
  },
  {
    "startTime": "01:29:09",
    "text": "this is really not the goal of the each I is not to do the research itself but it\u0027s to provide a framework to support the researchers in the publication of the research emergence of community providing avenues for for discussions so the also the evolution of networks towards more software wise networks as we say will be also an opportunity for application of cognitive techniques and network intelligence what we summarized as some enabling capabilities that we introduced with Network intelligence is of course to increase the overall network operations towards faster deployment or faster processes essentially going towards more minutes or seconds or for those processes also to better and the continuous provisioning I mean for legacy Network we used to have okay you plan your design you install you deploy then you operate you have initial configuration then you have reconfiguration but more and more with the advent of satellite networks you want to go into a more continuous life cycle where configuration and reconfiguration is just the same same approach and you want to go into continuous provisioning also enter an orchestration to have current deployment of IT and network infrastructure together such as with a service change for nfe and a Sdn and also the aspect of more reliable and available T of the network\u0027s okay this is some of the topics that are listed as topic of interest for the network intelligence ETI I will not go through that it\u0027s available should not be available on the web page of the ETI but probably we are targeting a different kind of artificial intelligence techniques machine learning techniques but also the realm of autonomic networking and data analytics so currently the the officials of this ETI are listed here the chair is a mind reader Yaya from Orange Labs in France then Raisa Mohammed patents on E which is from Maria Lin in Kannada technical program generally mom which is also from Canada Sonali is on I\u0027m just on a liaison officer so that\u0027s why I\u0027m doing this presentation here in IETF RDF we are trying also to create lesson with other groups in other standardization bodies and secretary revert on quad area so participation to this ETI for those that are not aware you don\u0027t need to be actually member to "
  },
  {
    "startTime": "01:32:10",
    "text": "join the sea tides I would say community of researchers so anyone which is willing to participate to this community can join you just ask your chairs you will subscribe to the mailing list and then you can even proposed activities or events in the CGI and you will be aware of what\u0027s happening there already some activity that are it being planned or that will happen soon organized by the people involved in this in this in this committee so there will be a first international workshop on network intelligence co-located with the I CIN conference in February in Paris you have to link to the Twitter workshop the deadline is already over so there will be a third international workshop on management of 5g networks 5g man which is a dinner organized usually with the norms and I am conferences and the next one is in April in Taipei the Aeon alienates conference next year which is organized by our side which would be co-located with the MPLS Sdn energy conference in April in Paris and we have some people attending IETF meetings that will go to that conference so you will see some of those of those guys there is also a special track on autonomic network management organized regularly with the IMS conferences so this is also a topic where you can find researches on this aspect some standards area where members we try to get involve and active so this very meeting ihe F an ITF but also you will see later presentation on the HC and I should police a lot committees and what we also have in the plan is to organize in the future especially issues for various Comstock journals and magazines so please stay aware of that in the for the future and that concludes my presentation so this is what just really to give you information about this new committee that is setting up we are trying to also invite members to join this committee and I mean to bridge the different communities and have more events more time to discuss and exchange on this on this question of artificial intelligence for networking thank you so it\u0027s not this one hello everyone this is will do from "
  },
  {
    "startTime": "01:35:11",
    "text": "Huawei so today I will actually I guess in last year or before last year I presented some I mean topics about this work at that moment that we just started to this work research on the network inheritance and I mean in last I mean from last year to this year we have made some progress one of the biggest one is that we create a new is G in SC called Eni experiential network to intelligence this is this is G or this is this group is focusing on the network inherent improvement and the migration so actually today\u0027s transition is a joint presentation drones Dresner should be here to present some part of this transition but due to his house a issue it cannot be here so I will present that part on behalf of him and next so in my presentation I will basically present three parts first part I will talk about the progress of uni and the second part actually this is from drawn it\u0027s about the MAF work it\u0027s about the policy working M yeah we will discuss at the progress and the third one is from one of my colleague as we are doing some technical work on how to development and how to implement it such technologies to the real network so we have a new use case and the proposed that has a draft in IETF and this one yeah is something we are doing our optical network and helped the optical network to collect data and to predict the healthy status of optical network so this will be presented by my colleague online so now please so this is the basic progress of our I mean of the a is G so this was established in the February of this year and they are more than 15 companies as joined as members already and so the core idea of this work is uh I listed on the top here so when we were proposing this work to SC we meet a lot of challenges because the network intelligence is a very big idea so it\u0027s hard to I mean limited the scope and the to explain was the detailed idea of this concept so at that moment we proposed "
  },
  {
    "startTime": "01:38:13",
    "text": "these three k terms in this is g to make people easier to understand what we are doing here so first one is network perception a knife and a lysis so this one is that we basically we collect the data from the data plan as well as the management plane and we analyze and predict based on this data and the second one is data driven policy meaning that actually we already have policy work in IGF as well as other s do such as an M Y F but that policy is about predefined the policies that operators or the administrative management people predefined this policy and the crudest policy into for example the management function controller as this theta trillion policy is a little bit different its meaning that we after we collect the data we change or we create the policy based on this data based on what we learned what is happening in a network and the third one is about AI based a close group so the goal is to make a use of the AI technology techniques to enable the close group so that we reduce the manual work in the management and the control of network so there are this is this more words are what we are doing actually it\u0027s a detailed explanation of the what I said just now and so in the first phase we will specify a set of use cases and the architecture in the second phase and about how to make this I mean the three key elements in the future and so the the figure here shows the progress so after the phone I mean after this IC was founded in February we set up with two phases the first phase we were focusing about the use case requirement and gap analysis and the second phase will focusing on the framework and the techniques and the proto requirement and other stuff that enable this I mean techniques so basically in this year we have four meetings planned we already have three of them so the first two are held in the headquarter of se and the third one was held in Beijing I guess some of attendees gear have drawn that one that was a house I mean hosted by China Telecom and a third and the fourth one will be held by son some in UK close to London so next year the the the meetings for next year are also planned I will show you in the neck next page and this figure and the other figure "
  },
  {
    "startTime": "01:41:13",
    "text": "here shows they the rows from different companies so we have chairman by vice-chairman from far away from China Telecom and Verizon and we also have some main players such as Samsung but there are tech comes SKT Intel and Chunghwa Telecom bla bla so basically because this is G was founded this year so they are also still many operators and vendors are on in their progress to join us so we also we also hope that you will be interesting this work and join us so this listed the ongoing work in this is G so basically we started we already started for work they are our use cases requirement gap analysis and the terminology and there is a upcoming work about the architecture which were proposed by Verizon and will be started soon and this this actually this is the pager I just added it because it joins not here so I want to talk a little bit more about the first part so the use case is already received more than fourteen use cases for both the wireline network as well as the mobile I mean the wireless network so basically they can I\u0027m not going to go deep into this so basically they can be summarized into three categories so one is the resource management and optimization the second is the service experience optimization and assurance and the third one is a for the detection and prediction so basically people are we see that the operators they want this working group to help them to resort to absorb these issues that it\u0027s not I mean deal with very well in the existing network for example they still need a many menu configuration and many menu decisions on this work so really they really I mean wanted this such a solutions come out from the industry so this this listed the the you know ecosystem because as we discussed that there are many organizations are working on these directions actually we are just one of them and in se actually we are we set up this is G and we discussed with the Board of SC we wanted to make this is G as the core as the core group to make people "
  },
  {
    "startTime": "01:44:15",
    "text": "I mean aligned and make people get the consensus on the what is the concept to offer the networking hydrants and what we are doing and what is the scope so basically we you know I will generate the concept the mainstream use cases and the requirement as well as the high level framework and the requirement for the interface and the protocol and the data models will be filled with other abdullah by other organizations such as ITF 3gpp so we also have some colleagues working there to create a some related work in 1:1 and run for and in the ITF we have the data models here working and for the itu-t we have the Big Data project in sg-12 working on the data analysis and the KPI KPI issue and again muf joins leading the policy work there and for the pbrn jsm a we also discussed with them and we want to make more corporations with these organizations and for the Sdn IAA actually this is located in in China its meaning the main bodies in China but it\u0027s also regarded as a sure organization on the Sdn and NFV aliens so they just founded a new group called AI applied again applied for network basically this can be regarded as an industry group for how operators can better use of these technologies so they so we wanted to cooperate with the mainstream players as well as organizations and we already send out many lies and including IDF MAF and many are seized in SE so that that\u0027s poorly the existent and this is what we see for the evolution of the near future about these networking hydrants so basically the the content here was a draw on the PI was drawn from the keynote speech from mr. won\u0027t how in you PBF in this year so mr. Montoya actually he is the boss of Bobby\u0027s development and management department and so he has a a speech on the network of origins this year in December and so basically they have figured out they improve I mean they did the state stages for the networking hydrants the first "
  },
  {
    "startTime": "01:47:15",
    "text": "one is Adam automatic meaning that you have the automation or for the service distribution and for the with the joint of the integration for the control plane and the second step is adaptive meaning that you can collect the data and you can meaning that you can analyze the data and you can improve the management based on that and analyze reads out actually this concept is similar to what Thiago just presented so they shared a similar idea and for the third steps actually we think it\u0027s more like a future work so we make it like a Chinese Angie so so that we can I mean well use the the AI technology to to largely reduce the menu and configuration and the many decisions in a network meaning as a network can be run by itself just like the self-driving cars meaning that you have your intent input and in system in network can generate the policy and as well as the configurations its and make batter dismay can even better decision than the human beings so this is our next steps and as well as the agenda for the meetings on upcoming meetings so all day as I said we welcome all they are companies join us and so actually we have online meetings almost every week so we already have 20-plus meetings in this year online and offline so offline actually we have four per year and online we have ones from a week so this listed there is a link here you can click and find all the meetings information so I just listed the the the rest meetings I mean for the for the rest of these years so from this month we have this online meetings on each work items and the in December we have the fourth meetings and also we have a workshop joined by other organizations and in your next year we already planned two meetings in the headquarter of SC so next as I said next meeting will be in UK okay so this is the first part if there is no more question I would go to the second part second part is about the progress of EMF the because the joint is not here I will briefly present this part on behalf of him so as you might know that amia started policy work since "
  },
  {
    "startTime": "01:50:17",
    "text": "last year so they have so this is a basically a summary of what they are doing actually they are doing a extension of I have super framework and they defined the declarative because as supe was founded in I have the declarative or I mean enchant party is not in the scope so actually they extended this and they do the declarative and intended policies you know Asian to these imperative policies in yeah and they already have the information on the I mean model used as a grammar about how to define these api\u0027s and yeah else DSL and physically they define that these three DSL for and with mappings to each other are imperative policy the clarity policy as well as intent policy okay okay first so I will speak this once and let\u0027s go to the third part so third part will be present if you have any questions on this and the second part please contact to draw a stress on it directly so the third part is about the use case on the vision service so could you let hello George and okay oh wait yeah I ugh okay yeah you can put pasta I will give it a bank bond yeah we can hear you public DM is semester of combining multiple signals are not the beams are at variously Imperator web Lance is for transmission a lot of fiber-optic media and a WDM system use multiplexer as a transmitter to join the several signals together Andrea did Tim multiplexer address to server choose split them apart another during the web last division service running the network data is consistently generated from the webblock division device and it can reflect the first process of the service learning and is the motivation of this chapter has two meanings the first one is the inner case of traditional web last division service custom is a custom that you can know the network affair after the service interruption such a service very as the inhibitions and easily DS to the large service interruption Anna secondly is a the net will be data can help I\u0027m ready to change the tan point as which service a llama llama also this is risky or here see under the girl has the distri point but why is the illustrator of Connor\u0027s of the little data use that you evaluated the performance of weblog TV service and secondly is demonstrated "
  },
  {
    "startTime": "01:53:19",
    "text": "defined the application scenarios of narubu data in web last Davinia\u0027s service and the last is the present the exists existing problem of many network data next nice place okay we summarized the network data this has three attributes the subject is a object to be made and it has multiple important properties from dependent image dimensions and stop yet may have one or more negative values and each m area corresponds to a spell specific indicator and the itchy itchy border of the main value will have the timestamp attributed to indicate its time next place okay we are cablecast there\u0027s no be curious case and the fact is the Alana medicate detection an army today is the identification of items events all of these observations which do not conform to a expected pattern or other items in data typically the learning must attend we are just translate to some kind of problem such as the optical a problem okay and for a web last video services scenario Network API data include the a VC B D F this means forward error correction coding before the election and the input optical power the last step bias current and the other key factors this test statistic data can be further used to do today last division service a lamb burning all you know the aku-aku accuracy rich of for the web last division KPI Alana meaty texture and the second is the risk assessment so this is a component aiming aiming at provide providing a estimation of the overall Network investor condition so this is way similar to the pre previous use case but risk assessment modules go is do and anticipate the network EBIT forecast as a shortened change and the risk in the network it is a it is based on the trend of little bit data under there were two ways to access the network is risk the single KPI recording another module Modi KPI scoring the single KPI is calling is the usage score of a single KPI to access the network risk also "
  },
  {
    "startTime": "01:56:19",
    "text": "if the device or the service is the molecule by a broker several key KPI the risk should be analysis the by the integration of this KPI scores next this okay okay because it is this single KPI yeah we can we say different affinity machines of our work API for example the parakeet ratio the trend the special issues the testicle index of which wise and force Shaklee over a period of time and the indicates which whether the state of the network at the animal is stable on a signal the trend a KPI data is connected according to the equivalent time another trainer component can be obtained by decomposing the stock data when the state of the network animal is in the process of the gate violation as news quiz process of a change resulting in rising or falling trend the stretcho that when we know the network an amended attribute itself defines a capias we showed that it can maximums spotted so once the comparing value is the smaller than the statute the closer of the this value the newer the reliability of the clang so we can see the the the time the the town left the town bigger okay this is user a vetting process to to wet the sweetie machine choose the single one then we can see the right right the right part is done motor your KPI we use we use the two dimensions of a KPI our nanny system so the course is corresponding let\u0027s go Kara didn\u0027t next place okay this is a open issue so so how to make matches the data from defender time time period we know in the in the process of data connection the connection period of the same KPI may be different from each atom for example from multi-team don\u0027t domain deployment service there may be many different connection carry the phone network that effective artists such as set here seeking all the five per minute from 15 "
  },
  {
    "startTime": "01:59:22",
    "text": "minutes and to our sometimes they come comparing is co-founder a modest among this dependent to miss some so we need to modulus terror cell from the banner period into creditor-debtor said using the metric in their period such as the mean value antique value or media value so we we are very well well welcome you everyone to join this research topic thank you okay that\u0027s it that\u0027s all about our transition any questions Shinya just for for comment will you please tend to pay the priest one yeah maybe maybe I have a have the answer about your open issues that that is the question I just asked the Germans how to align the data from different scope I think that it is depends on your frequency of operations for example if you do operation once per per second that the the higher frequency data will be helpful will be unhelpful it will be a noise it useless so is the answer should be your depends on your frequency of your operation that one you need a lower operation frequency you do not need a more higher frequency data to feed your system that\u0027s my my private opinions so thank you and another and none of comments I stand for SC n GP a walk item six idea and said at and a little bit surprised that there\u0027s somebody talk about SC works and we are we are welcome to have any corporations based on of working scope and we are focusing on the architecture the protocols the communication between different devices and we I think we have a lot of walking point that can be culprit and exchange other useful information thank you yeah thank you "
  },
  {
    "startTime": "02:02:23",
    "text": "very much so we rather our last presentation for this part of the meeting and then we will try to save time to have a overall discussion and assessment of what we have learned today thank you yes machine Howie I would try to be a little bit short we have enough time for discussing you summary for what we already have the efforts in idea and I are here actually people that we did have the mark M Arg the machinery the sort group as purpose research group for almost 1/2 years we did have some Co discus lirikrekt in yo Stata use cases but given the there are so many useless cases in various error of network with we have real full converging to certain use cases and reach the equipment what maybe you know standardized so here we\u0027re forming another a fault as most of you may already join the menaced s\u0027 ID net meanest way actually hands-on training is to start from this April under IDF and up now we already have more than 300 350 and we actually have more manliness that discussing there the Mao RG before there are 45 active participants this is it Allah it is what may is their night yeah okay this is a little bit analysis what\u0027s the who is up participants in the Middle East we actually saw a lot of people from universities so far we also "
  },
  {
    "startTime": "02:05:26",
    "text": "have the wonders and SPS so far we have very few participants from SPS but actually we are packing to post the ASP Network and the ice P networks and we saw a lot of participant from European and is also most American who\u0027s in dealing in AI error this gives a little bit reason everybody knows how important to apply AI into the network it can makes the network be less human and dependent but actually there\u0027s already traditional way to make the network autonomic by using the affairs way but in that way that means actually the network devices itself may become more complicated because whatever they autonomic function you want put in you need to carry that machine learning actually actually give all us another opportunity which may able to you know have both Hottentot normals and to reduce the complicity of the network devices that means you know we may be able to have some kind of unified algorithm machine learning algorithm to be able to handle multiple use cases because the the algorithm itself can train itself to add adoptee into different scenarios in different conditions so that give us the possibility to reduce the complicity of network devices for in do not to you know put too much efforts on the use cases this is the adding an architecture we already have abstract in this we have three layers infrastructure layer control layer and intelligence layer we put all the machinery and AI efforts to become a platform which can trainee from the worst data we got from post user devices and through the measurement function from the network devices they provides real-time data to the air platform but actually the data itself is for now is in very different format so but why we with each data refiner here "
  },
  {
    "startTime": "02:08:29",
    "text": "ideally in the future if more standardized work taker take on the data from the network could be unified or somehow the formal formally handled before it can become the input data into the air platform unlit and stream the air platform will generates policies or the executable Spartacus than the control function will translate that into the very detail very detailed indicating the network function could take on and that whatever the networking changed that will influence to the network status which will catch by the measurement function I\u0027ll skip the use case because say the use case already being introduced several times or mentioned by others there are two use cases we already have have the prototype and it\u0027s ask successful without traffic repel violence and it is a proactive social labor agreement maintain this as I said there are some many high values and errors has been proposed and there is some one document or the Jerry Lewis case together but I would like to spend a little bit more time on how you know what may be the standards point in the architecture actually from this actual architecture we can clearly say there are two aspects of it why is the data aspect from that\u0027s how the we could get the data from different scenarios according to different targets and pre handling process them before as the input for the AI platform and at the control part also requests the policy to be formalized and to be able to understand by the controlling function and by the network devices so all of those actually requests the data and the policy the output of the a a platform to be standardized otherwise the devices a technology devices from different vendors would not be able to function "
  },
  {
    "startTime": "02:11:30",
    "text": "and without modified okay I already acted yes is actually my last slides in this I\u0027m not going to go into the details part in this and you can see there are a lot of use cases and each use cases have different data requests and for the control aspects the output to for the specific scenarios tasks is actually also different so that actually come to the discussion point I would like to you know Co attention is you know we actually still struggling you know what could be standardized in this areas there are two approaches for you know how we do the AI or machinery in the net network errors first of all the machine learning algorithm itself is algorithm which happening in whatever in one was in one network devices so that\u0027s not be able to you know us become standardized a standardized is is what happened underwear for you know deterrent devices of all at least patrolling functions so there are there are two approaches for you know the data would be used by the machinery functions why is the first one years you know we could use the data whatever they can\u0027t make any them provide to us that means collects the data from the current network devices but those data\u0027s are in a very different for the their original purpose and the format of them are very very different from each other and given the different measurement mechanism and is a different measurement fragrance the data may not be short before for machine learning at all particularly for those real-time machine learning tasks then there\u0027s another approach you know we maybe we examine the whole data set from the aspect of machine learning algorithm what the tasks we would like to complete and what the task lead for data as input then we can work "
  },
  {
    "startTime": "02:14:34",
    "text": "back what\u0027s the what\u0027s the new data we did what\u0027s the new measurement algorithm all we need and where the data collection and where the data should be translated into a centralized all or maybe this distributed place to do the a calculation yeah although still in discussing I don\u0027t have answer for now and I believe most of you who don\u0027t have answer for now but I would like stream or discussing in both Network emotionally research group Menenius and the Ida net meninist yeah there\u0027s one last one points I would like to mention years you know we\u0027re always thinking you know where is the possibility possible violence between the very specific use case and you know some something could be abstracted into calm along multiple use cases because a standardization we would like to get you know as much as possible a value from the standard format to be able to serve multiple tasks rather than you know go too deep for the very specific very narrow use cases but as you can see from this figure is different scenarios different tasks a curry requests very different data so we\u0027re in the middle to found the the best point for standardization that\u0027s that\u0027s also another point where we need to discuss and the result of discussing we have become very variable input for ITF standardization and we we have to be ready with some level without to be able to start another efforts for ITF working group we have comments on this presentation Soyoung from my university so sanction I think was the question you raised that is quite important because as we know there were a lot of state of our research under machine-like unemotional in the so so in addition to the natural "
  },
  {
    "startTime": "02:17:36",
    "text": "network management there are also studies on the TCP and multimedia and the even the the new English placement some and some other areas so we needed the data right for the machine learning and we also needed some kind of or control message to communicate that between each other to sighted the parameters and also the result to the network and it is so I think at least how it is quite important de thank you I agree we we would like the data and we would like the low data as well but from more than that we would like the data with right format to be able to use us as input for the machinery yeah just some comment about standardization and MRG is a research group and then sterilization is a nice outcome but it\u0027s not the goal of the search group itself so identify which can be standardized but also identify which cannot be standardized or should not be Stern rise is also an important outcome so because we become aware of it so these were the presentations that we have today the idea of this session is to bring together people that were working with AI for network management as I mentioned in the beginning we wanted to revisit it because the use of artificial intelligence for network management is not new Percy probably more than 10 years people have been working on that but the AI re has matured along and then we as her research group we are looking for directions for future work to be done in the context of the of the research group itself does the audience has been you know about it what to move on how to do next and is it an area that is worth while doing that anyone willing to share these are her thoughts about what we have run today I mean general comments about the use cases what could be the requirement we need to tackle what will be research items who would like to investigate in the research group just share your fault it\u0027s no it\u0027s a general discussion we want to try to build a kind of roadmap for the research group or maybe beyond the research group of what we can achieve what we need to work on "
  },
  {
    "startTime": "02:20:45",
    "text": "they\u0027re a lot better I said just to insisting what I was trying to save you in the presentation is that we an essential tool for research is precisely to have data value I think that and especially in the case of these technologies that are so dependent on on what is the data you fit to them and I think that having having a set of open data sets that are well-known that are properly I mean I\u0027m proper metadata so so whenever you report a result on a particular algorithm or a particular technique you can refer to the edge of the data set you have been used and these results can be reproduced and analyzed it\u0027s essential if what we want is something apart from a I have this magic here that has these nice results and then try to repeat it but if you don\u0027t put the right ingredients inside the concoction it would not work this is something else I mean I have a question with respect to that because I mean it\u0027s a recurring comment or problem we are facing I\u0027m I\u0027m trying to sketch I mean in other domains for instance image recognition they are I would say reference a standardized database or medicine etc can we imagine a similar approach of similar databases for network of a specific function or problems in networks this is is it the right way to think or is it completely different and we should not seek to have reference databases but to have maybe focus on on use cases and what are the properties of the data we expect to solve specific programs of networks I think I think that we have to I mean the goal will should be similar maybe you similar to what you have right now for Biosciences for example G normal things like that probably different because doesn\u0027t make any sense the data sets I\u0027m aware of have more than 20 years the publicly available and the traffic while the traffic that is reflected there is I mean IP traffic on the other hand they are not following the current patterns they don\u0027t have for example video flows just an aside in our example so probably they\u0027d be able to be that we should have to keep those data data banks or whatever we call them because it should be a set of data sets and keep them updated according to to the evolution of the of the natural search buttons I know I know that challenging I mean it\u0027s a and again data is an asset and is complicated that is but that is big that they become really widely shared but at least having a set "
  },
  {
    "startTime": "02:23:49",
    "text": "of not necessarily not necessarily complete sets at least something that is usable and helping in reproducing results [Music] and this is Jefferson Aubrey I think that uh learning for our our past experience we fee with a NEMA working group maybe are a good way to push this forward is to have use case aimed meeting and as we can have and how this use case should be in a as an ideal world they should have a draft so you could discuss using the drafts previous read by the attendees so we can screw Sarah I\u0027m a minimal set of use cases and try to produce from the use cases written on the draft some more some more material in order to push for the directions for the the AI and the intelligence Defined Networking King oh yeah the use case are very important actually we already can you know collect various cases post from the previous email Archie VD ed also to the air Falcone for network management I think the group should go ahead for to do the analysis research for those use cases try to found out you know how the ce o--\u0027s cases could be come together and just to do some kind of neck gap analysis you know what the use cases lead it and the current network cannot provide and what is missing to be better AI ready network or network management saying based on without we can you know somehow fake where maybe the direction for future potential standardization work she\u0027s not gone from ten oh I second the the ago mentioned data is important but also the methodology is important as well I mean the traffic classification different vendors have different implementation and use a different classification and "
  },
  {
    "startTime": "02:26:50",
    "text": "that lead to a different insight but very often we don\u0027t have the standards came to a classified traffic like a type of traffic and also a protocol and sub protocol what what kind of level we need to obtain the insight I don\u0027t need for the London mint which is very important basis which I say is kind of missing I saw some proposal to have a standard scheme but when if I do it be a would be a very good thing if all people using the same classification and then we all have the same insight which summarize what we have so far you know also I\u0027m trying to remember what we have on the NMR G in the past to because Jo mention about the datasets and for example in the past we were able to say prove that get message from as SNMP was not to use it given a set of data sets that were shared and collected from important backbones around the world that same data set shown that SNP v3 was not used that much although everybody was expecting that so we have on the working group itself some experience on that but we never use it data sets for these artificial intelligence supports so maybe we could try to do that again and and the problem with data sets is actually having people willing to provide the data so that we can collect them and share them somewhere so the first is infrastructure for sharing second is having the willingness to share data set another aspect that Jefferson was that we have used study areas in the past so that we could evolve in the group and then the fact that we need not only to check the case studies but also we need to analyze them so we should go one step further and analyze them and then all of it we should consider assuming more specific methodologies instead of having everybody doing the wrong way maybe come up with some methodology that could be reproducible please go ahead I think we\u0027re all focusing on the right point on the data but I I don\u0027t see there is particular AI data so we got out the data you mentioned SNMP and today s episode we still have to use it and you knew is all implemented there\u0027s a young model and there\u0027s open config we actually prototype early model it demo late it "
  },
  {
    "startTime": "02:29:51",
    "text": "can get our real-time streaming that\u0027s what you need if you don\u0027t have real time data forget about it right whatever when you use is 15 minutes ago some time ago so I believe how these are foundation work it doesn\u0027t directly to do AI but it\u0027s tightly coupled without them you cannot do it so on top of that you can do your AI in centralized approach in distributed approach one is it that good data set that I think we all agreed and two is the how to handle the massive data sometime you can see you dump them into t SD be pulling out is also very difficult it does the right way to do it there\u0027s a \u0027memory processing thing and all things you need to skill it and to be able to process in time for some maybe for long term study training some maybe just for immediate troubleshooting so I think we made my mind I don\u0027t think there\u0027s a particular AI data but a I depend on all these data if we have enough power like a machine we can do that ourselves we don\u0027t so we need the AI we need all the algorithms and things and each the complexity in networking or say the full stack all the applications e-commerce and so on it\u0027s really different than like natural language like image processing speech recognition all these are fixed set these are distributed all over the place so it under it\u0027s unsupervised learning a lot of time because we don\u0027t have label the data Michael thank you thank you this is Jefferson Aubrey just to add it to my suggestion suggestion before I think it\u0027s something that we should agree before is to have some kind of a minimum set of agreement on the use cases or the data sets in order to have some minimum requirements for the for the drafts and for the public data sets and I think this we could do in the mailing list no just changing an opinion using for example the use cases that change mentioned before that are sent to the intelligent define a network in Middle East so and this is something that also worked for for the autonomic networking use case because the use cases are at some and some sense they are comparable because there is this you know this fixed set of sections and minimal information that is required to understand use gates so this is Albert I "
  },
  {
    "startTime": "02:32:53",
    "text": "haha I would like to share my opinion on the data sets I agree that the desert important thing when were training machine learning algorithm but I also agree with Diego they are an asset so although that in any idea world people will share them maybe this is not going to happen because I will be my algorithms will be as good as my data so why share it and have my competitors take advantage of that and I\u0027m from the Academy so I don\u0027t know but I\u0027m I don\u0027t have too much hopes on that but in any case if you take a look to other areas like computer vision public datasets typically are not used for training that\u0027s not true they are used for benchmark sorry for testing for benchmark so I train my computer vision algorithm with my own private data and then there are some public datasets where everybody goes and benchmarks they are going so it\u0027s like a public benchmark I am as good as I don\u0027t know I have a 92% accuracy with that public benchmark but this is just to for comparison from competition which isn\u0027t a bad thing maybe we can do that and it\u0027s a way for people to understand how good is algorithm that by vendor is selling me but public datasets for training I don\u0027t see it happening I just want to say I agree with his point commercial data production data no way that does for sure I mean that\u0027s something that sharing sharing sharing day timing data that has a real commercial values and value is something that we won\u0027t see happening but I was claiming is precisely for the way of validating the results and reproducing them which is benchmarking if you if you like and we one might make a reflection about on separate base unsupervised methods I\u0027m a little bit concerned that again deploying unsupervised methods in a running Network is something that I I don\u0027t see I mean frankly I cannot imagine telling my operations guys these will take control of the network and we\u0027ll learn by reinforcement another likable complicate that\u0027s really I mean on the in the lab you can do you can do this kind of experiments unless you can to some extent prove that the thing is going to behave I think when I talk about and supervise the other thing it just small for a particular problem in our domain that a is very you know this "
  },
  {
    "startTime": "02:35:54",
    "text": "distributed right so you don\u0027t have one methodology solving our problem you take one problem that may be fraud detection you develop certain methodology it works well it goes into production we do have them in production call it whatever surprised not supervisor is just you have to target one problem at the time there is no one big blanket cover-up yeah Jabbar Stan you\u0027re asking Santa Fernandez its beauty its value of your data stats targeting to machine learning techniques it\u0027s the most viable thing to the community he said is impossible to train models without good data and another suggest will be organized competition similar to cattle like the schedule competition online yeah another one on a research topic Eric from moly one area of research topic that we want to pursue is networking tent use cases was mentioning actual cook actual customary use cases would be useful ok we need to close the session because we have some already use additional minutes but I want to emphasize one thing we have nice discussions here we would like to have these discussions in the mailing list as well because the mailing list is quite quiet of course we try to provoke these discussions on the mailing list but your opinions are quite important over there not only because the discussion itself but it help us to define the the roadmaps thank you all and we keep on discussing this on the meaning list of the idea net and energy please thank you you "
  }
]