[
  {
    "startTime": "00:00:20",
    "text": "Please take a seat and remember to sign into the session via link the agenda or by scanning the QR code displayed in a room. Alright. Welcome to the first meeting of the CCWG. We've got a very packed agenda today. So as you can see, we've given ourselves a time or 2. This session is being recorded. Make sure that you scan the QR code that's up on the screen over there. And when you want to speak at the microphone, You can press the raise hand button to join the queue. Press it again to leave And, similarly, if you are remote You can do exactly the same thing, but also use the microphone icon to send audio and video you choose. This is the IETF note well. These are the terms under which we participate here at the IETF. Please make that you've taken time to read it and we'll make it speedy on through here. Also note that in this working group and in the rest of the IETF, we participate under a code of conduct which we take very seriously. Here's some helpful links, which you can click on later if you feel like. And this is our agenda for today. So as you can see, we've got a lot of awesome things to talk about. And first things first, we're gonna talk through a little bit of how our charter is going and all of that. as a new working group, what are we actually going to be working on?"
  },
  {
    "startTime": "00:02:01",
    "text": "Okay. So we had Richford volunteer to be our notetaker, but I'm not seeing him maybe. If we could have a backup notetaker, anybody willing to be our notetaker, please. Yep. Thank you. Alright. So I So we wanted to quickly let's recap or charter which you can parts of which you can see on screen here just to make sure we once on a same page on what we're doing in this working group. So our congestion control working group will analyze some of the impediment to congestive control work. and standardization in the IETF, and we will be general general to all transport protocols, not TCP or any other particular transport protocol. and this work will inform a revision of RFC5033, which is also gonna be our first agenda item after the share slides. So that is one thing we're doing and then also CCW is a natural venue to take on other work specifically congestion control algorithms and we are looking for 2 things to determine whether work item is in scope, mainly empirical evidence of safety, and stated intent to deploy by major implementations. So we are looking for proposals in this space, and we're interested in adopting proposals in this space before or after we complete our primary deliverable 5033 biz. and then we will coordinate closely with other works during relevant work in the space."
  },
  {
    "startTime": "00:04:00",
    "text": "such as ICC or GE TCP, I'm Quake, and VWG, and as I already said, we're gonna be as transport diagnostic protocol agnostic as possible. We do also take algorithms proposed for experimental status and we are also potentially able to adopt publish information at RCs, analyzing the published standard, control, algorithms And, yeah, and these are our 2 milestones as already a charter as I mentioned, we are open to more work Does anybody have any questions or comments on the current shorter Could I ask a question? I haven't get my laptop yet. of either different document I would like to update in the BCPs on congestion control. Do you think that might also possibly fall within charter? It might. Let's talk more after your presentation. Thank you, Corey. And please remember to sign in to the working group session. We've heard anecdotal evidence off rooms being fuller and not everybody signed in virtually by the blue sheet. So please make you do so. And please make sure you use the queue feature to improve fairness among, you know, participants. Just a few quick thoughts on GitHub. We did create a GitHub organization already, and we already put a few of our working group materials, in on GitHub, we're interested in using it for drafts as well. And once we get substantial discussion, on GitHub, we will make sure that a, we always confirm consensus on the mailing list and, b, we will keep the mailing list updated on our GitHub activity using an automated updating"
  },
  {
    "startTime": "00:06:02",
    "text": "mechanism, for example, sending a summary. So people who are not following GitHub the time, we'll still see what is going on, but we are into a it in using GitHub to collaborate on documents So just as a heads up. And if we don't have any other thoughts, we will be happy to already go into our next agenda item. If Martin Wood to come out please? you wanna drive slides, you I'm not really sure why not. goggles. So I'll hot off the presses on your new 5033 biz editor. I'm gonna attempt to use Ben with freed up from not being area director come March. which is a reminder for somebody. Please run for transport area director. Alright. So as as I well, let's just go right into the slide So this is the main deliverable I am not the responsible AD for this working group to be clear as I had is. I had his here today, so I will be standing in should that become important, but that is rife with conflicts because here I am is the Men, men, men, men, liverable editor. Okay. So I think the first question is lucky. Before I talk about issues, let's talk about adoption. Richard, who was the original editor of this basically, copy 5033 into a new GitHub draft. and and a 00 draft. There's, like, the boilerplate is updated. But other than that, there's, like, no substantive difference. and and this a good basis for adoption, or do people want to blow away the whole thing and start with a whole different Is framework the stock in Or does anyone really desperately need to see something in here before we adopt And I'm happy to stop right now and and see people have With Christian. Bye, Kristen. Thank you."
  },
  {
    "startTime": "00:08:07",
    "text": "Question, would you come to Okay. I I am trying to put my mouse on the screen because I'm our guy and it goes away, and then I can't find it. anyhow, Martin, I think we we had discussions before that. and and you you know that that we have had lot of opinions exchange 1 of the big issue in 5033, is that some of the recommendation in 5843 are having negative effect on the Internet. Specifically, bufferblocked. And so that makes me leeway of just taking a carbon copy. I think both of the exercises to find out which of those recommendations were long. And effectively collect that. Oh, I mean, so to be clear, if we adopted 5033 as biz as the baseline document, like, you would clearly, like, examine every line of the text, and, like, see if we wanted to change it. The question, is this like a Is this is this document format, like, correct? I I mean, I think it's the real question, or do we just really need a completely different editorial approach to what the stock given is doing. that would that would imply, like, a completely new draft that started is it better to start from 0, or is it better to start with this? I mean, the the good point of starting with this is that we have something to start with. Yes. Why is this so so, like, Right. There's sort of 3 alternatives. 1 is we start with this and we edit this. Number 2 is"
  },
  {
    "startTime": "00:10:00",
    "text": "start with an empty Internet draft minus boilerplate that we start writing text for. option 3 is that somebody comes up with some other beginning starting point, which I'm certainly not going to do. So Yeah. That that's that's the question that that I'm doing. I I I I think I I think I'm at my point to let those of people speak. Yeah. Okay. n, in Square Google. Yeah. I I think you're not gonna find someone else who said, like, gonna produce another internet drop that's a better starting point of this. I think that seems fantastical. I mean, if they want to, then we can consider that. I think at starting with something empty seems a little bit to scorched earth. I have a difficult time believing that there isn't substance of value NRRC, that admittedly has some flaws. So I would to start with this because it's a thing. and then we can start actually making progress and until we adopt something we can't make any progress. Gory. Gory did press the button, but never mind. I agree with you. Okay. So chairs, I don't know if you wanna, like, take hands now or just go to the list or what what Alright. Let's do a quick show of hands to see who's actually in reach of their machine and awake. I think the question that we're gonna ask here is should we take the existing 5033 this, which we've essentially just made this copy, and is there interest in adopting this as the starting point for the work we're going to do as a working group, which we will then confirm. can I interject before we do that? Can we do an actual show of hands? Oh, no. No. Wait. But when you finish, can we do an actual show physical show of hands and, like, And people and people skimmed 5033 and, like, know what it's about. there. Okay. So they know they know what they're getting into. press. Thanks. Thanks. Excellent. Let's note that in the minutes too. Alright. Let's see. 5033 is a good starting point. to adopt"
  },
  {
    "startTime": "00:12:16",
    "text": "Excellent. Alright. We'll leave this open for another second. While we're waiting, does the does the decentral want to articulate a a viewpoint? Alright. We will close this momentarily, and we are Cool. Alright. That appears to be 30 hands up for one hand not raised. Okay. Thank you. We want to confirm then the but I'm gonna assume that this gets adopted and, therefore, the rest of these slides are relevant. Alright. So there are a few issues from Richard GitHub that Uh-uh. What'd I just do? Right? Okay. Great. So as as probably figured out, Richard, to step down as editor. Thank you for getting this this ball rolling, and Corey and I have have stepped up to take it over. There's a question of authorship here. So, you know, in in In keeping with general policy, we're gonna try have some editors and then have some authors listed in the back. you know, nobody's really written anything yet. So but the way it seems like it's a sketch out as Goriya are gonna edit this and and judging from some commitments, it looks like at Mathis and Christian. We're gonna write a bunch of text and probably recognize authors in the back of the document. the somewhat more Sensitive discussion is the original authors of this document, Sally Filietamar. Alman. there's some ISG stuff that I could talk about here, but, like, I will just make the the case that I think is presumptuous to"
  },
  {
    "startTime": "00:14:01",
    "text": "attached Sally Floyd's name to this work, and since she's not a the late Sally Ford is not able to endorse it or not endorse it. So Certainly, she deserves to be acknowledged. Mark is very much around. I do not from my Experian limited experience working with them, I don't expect to be particularly responsive or even necessarily supportive of some of the things we're doing here. So And we'll see what the difference of this document at the end, I think it would be substantial. So I am going to propose that barring further developments that they both acknowledge and no longer listed as authors of this document. And does anyone have a problem with that? Say so now. Seeing general nods, but nobody's diving for the microphone. Super. Thank you. Okay. the the other I mean, there's some relatively simple editorial things I'm not gonna bring up, but current multipath is one one of the issues that is not addressed in 5033 as it is today. And, like, multipath is a terrible word. is a little overloaded in the transport space, but here we're talking about, like, molt not address migration to fail over actual, like, data simultaneously moving in multiple paths, which has this bottleneck sharing problem. And I as I think most of you know, there's some experimental work in this area. I don't know that we have anything like the idea of consensus on how to solve some of these problems. scheduling is a related thing, like, on what pathway send this piece of data to to actually improve performance. This is another thing where I think there are no tracks or IRCs of note unless I'm mistaken. Do we think they should be in scope for this document? Do we know enough to say anything about what congestion control should do here. Or should we leave it out? Christian, I am all forwarding it art."
  },
  {
    "startTime": "00:16:02",
    "text": "Basically, because we have not We don't have enough plug these still. and the aid. I'm not sure we can say something useful at this point that is good enough to make a BCP. Okay? Thank you, Christian. Whoever's next in the queue. Gory. issues with summary. Gary. And I I kind of preface this by saying I'm only jumping up because nobody else did. So when you hear what I say, if you don't like it, come and jump up. I think we should have it on her radar. I think we should be thinking about this as we write it. That doesn't mean we can make the recommendations now. This is kinda like guidance for how people analyze this. So If we don't keep this in mind as we write this, then we're gonna cover crop people actually have those supplements. So do you think we know enough to tell people how to think about these issues? we might know some of the points which they should look up not enough to give a formula for how to get one adopted. I do wanna emphasize that this problem is in scope for the working group. I'm asking specifically should 53 biz address the subject off? Yeah. The the the short answer is I don't know, but we should think about it. Okay. PMs? My inclination is we don't have enough information to really make a solid recommendation given this is Yeah. Yeah. So, we can't I was in the net? Yeah. My I have a similar inclination. I would like to point out that the difference the impact of the network is no worse than browsers opening multiple connections. And so it's sort of bound by that problem. I I used to refer to this as small end cheating. Thank you. He was clear. Alright. Here we go."
  },
  {
    "startTime": "00:18:06",
    "text": "Is that really high latency or did not press the button? There we go. Okay. I think I skipped the slide. Did I? this thing. Alright. Thank you. Yeah. Did I did I get alright. Right. So you can see what this says. You could see I copied and pasted it rather than trying to summarize it. you know, there's the old textbook definition of fairness, which I think there's been a lot of interesting work recently about this? Do we wanna reopen this and like, redefine fairness in some way. God. I think fairness in some form will be in this document. one would imagine. So yes, open this, or are we happy with the old definition of fairness? Matt. You guys could sit closer to the mic if you're gonna be standing up a lot. Yeah. Clearly, I planned bad badly. So I would rather Replace fairness by a notion of freedom from starvation. And fairness at low bandwidth makes sense, and it doesn't make sense at high bandwidth. Eliminating the concept k. And I had to close the queue because we're all together is is a 3rd option. Thank you. short on time. Pretty much what Matt says. I I get glooze mutts. We should not be focused entirely on fairness, and we should not have a goal to make sure that and players have the same bandwidth, etcetera. The non starvation goal is fine. So, sorry, suggesting it goes in the other draft, I'm gonna present what's and and This is not really part of this because I agree with what Matt said on this Okay. I think that I think the"
  },
  {
    "startTime": "00:20:01",
    "text": "since these comments is just get rid of it entirely. Yeah. in this group. So But we we might get a chance to talk more about fairness And by the way, Eric is trying to summarize this stuff in GitHub, which is very helpful. Thank you, Eric. In real time, I mean. I'm impressed. So you're technically out of time is the last slide. Go. Okay. Alright. And then there are just three one of these well, I I don't know if they're new concepts. The things that weren't addressed in 5033 bps, buffer bloat, AQM. And then, obviously, still still start existed back then, but know, everyone kinda did the same slow start. Uh-uh. are these things that we should address and just open up the comments at any of the three? Yeah. Please comment on the GitHub. Okay. And now -- Are we Sorry. We're we're out of time for this. One's a right question. I shouldn't shouldn't have reopened the key at this time. On my other slides, I don't remember. Okay. Great. Thank you. on shares lens. Okay. Okay. So next we have Yoshi Thank you. Can you hear me? Okay. Okay. My name is Yossi. Today, I would like to talk about analysis for the difference between standard condition control scheme. No. No. Okay. Just the next slide. Apparently, the thing isn't working. Thank you. let me start background of this document. So today, we have several conversation control standards, such as GCP, you know, barcicurino, acubic. And then I personally believe that conditional control standard should provide a consistent guideline, and this should not contradict each other."
  },
  {
    "startTime": "00:22:00",
    "text": "and Also, I personally believe that when we provide 1 condition control standard on a different transport protocol such as TCP a click or a CDP, we should expect they behave more or it's equally because now they should behave based on the same condition control principles. That's But as far as I check these standards, there are some differences. in the diff standards, That's the one motivation I load this kind of draft. And then another motivation for me to write this kind of document is basically analyzing impact of the congestion control on the Internet is not a very easy task. because Internet is very huge. diverse, and also deploying a condition control standard takes sometimes user takes years. And then but having, you know, describing the difference between transition control standards may be good information for huge analysis of the impact of the concession control on the Internet. that's another motivation for me to write this kind of draft. And then so what is in this rough? So, basically, my draft describe a risk of difference on the certain topics in conjunction control standards. that draft base mainly focusing on dcpdino defining RFC 5681, and quickly, no. defining RFC 90 auto, and CUIC RSC defining RSC 9438. And So one, point in this draft eights and not difference between tcbrenoandthequickreno. because, ideally, discipline on the piclino So not diff different with in terms of aggressiveness."
  },
  {
    "startTime": "00:24:02",
    "text": "But as far as that's the standard. There seems to be some differences. and then I try to, you know, list of some differences. then another point of this draft is focusing on different splitting genome and the cubic. in terms of fairness. And, of course, you know, this doesn't mean cubic on the linear behavior. equory, Of course, cubic can better path from a down window. That's why we developed QOB. and that's why we are going to develop 2 because a standard. But at the same time, renew is also a congestive control standards. So cubic is not pushed away, you know. So it it should be have some balance. between cubicandino. And but and we standardized a cubic. in TCPM working group, there are many discussions about Fiannis speaking, cubic and. So probably, you know, this kind of analysis takes, you know, require long term analysis but this drive to basically capture some discussion point on this point. from based on the previous discussion. k. and then So let me describe some difference between DCP on the quick reno. the first diff differences in showwindow in RFC5681, The initial window is defined as up to segment were 43.80 bytes. in case of 90 2, the initial window is defined is up to as up to 10 segment or around 15,000 byte. That means quick can utilize bigger in some inside windows. compared to TCP. And then also another difference is being in our RTO, minimal RTO is defined in RFC 6922 in case of TCP, But in this case,"
  },
  {
    "startTime": "00:26:00",
    "text": "minimal audio is defined as One second. on the other hands, case of quick, there is no minimum RTO, so you can choose any small meat. RTO if we want. then lost window, And window guys away is a windows condition window size after, you know, transmission time out happens. And in 5681, Windows 1 segment. But in RC 90 auto lost window is 2 segment. this might be, you know, right difference. But because we have derived algorithms, this has some performance impact. Are you willing to take hands now? Oh, okay. I got two people in the queue. Okay. Maybe let's hear what really has to say. Okay. It's good to Hi. Sorry. BD Apple. It's good to highlight the differences. But what are we trying are we trying to tell quick developers what changed your condition control? to match 5681 or the or TCP developers to match 9 1002. Okay. So, basically, these drafts try to clarify the defense. I don't say this is bad or this is good. So But that goal of this document is to initially discuss whether, you know, this whether this is acceptable, you know, we can give it or we we shouldn't fix it in some way. Martin, do Google. No hats. Okay, Martin. I think this president this analysis actually shows that we should update 5681 because I don't think anything in 9002 was Controversial is just just like newer. Mhmm. And like, quite frankly, editor energy to just do that and TCPM would be better spent than, like, trying to push to do that. I mean, this I'm glad you did the analysis, but I could continue to progress those documents. They're just fix these fixing 5681 is probably I just fixed 5681. only somebody from TCM could, like, do that. That'd be great. Rex."
  },
  {
    "startTime": "00:28:02",
    "text": "Yeah. I am also same feeling. That's no. 5681 getting sales because no Internet is absorbing. But beforehand, this kind of discussion may take time. So beforehand, I would like to just defines other reference. That's my intention. Okay, Vinny. Did you wanna say something more? quickly reply to what Martin said about that that that 5681 is outdated, and 9002 is It doesn't have any issues. I think there is a small issue with when we do last when we basically reduce congestion window after loss, it uses Stephen Divided by You know? Basically, does math on c 1 instead of bytes implied, I think there's a little consideration there to basically look at You're not using c wind when you're at the end of the tail. So you cannot just use Steven directly. I think 9 1002 does miss some of the things that the window validation draft, 7661 talks about. So it's not a clean, answer that you should just do 9 1002. There are some details that are missing in 9002. Thanks. And then Michael? Oh. Okay. Yeah. I I I I I'm not arguing that 9002 is perfect. And if, like, this next thing, like, is is further iteration 9002, that's great. just just just like, there's just legacy stuff because 5681 is old, and let's just fix it rather than writing a lot about Thanks. Okay, Michael. Michael Jackson, if we get if we try to synchronize TCP and CAC, which should also synchronize SCDP to the Fair enough. Okay. So we We have a minute and a half the 10 minutes left. We're already in an excellent discussion. Okay. Yep. Next"
  },
  {
    "startTime": "00:30:00",
    "text": "Okay. Then I will make it quick. So are some other differences, window growth in slow start. in case our now it's observed too, and the congestion window can increase amount of an occupied. So if with the AC node 10 segment, this window cam increase by 10 segment. By in case of DCP, you just you can increase just one segment. So that's the device. And so we start after targets through slot threshold after packet loss is also different case of RFC 90 total, it's half a value of a condition with open packet loss detected, But in case of 5681, it's half a body of fried sides instead of a consistent window And then, also, there are some text inside 5681, it's basically prohibited to use congestion window here. So it's somehow these two standards contractics feature that to some extent on this point. And then Also, I'll just would like to talk about difference between dinoandthecubic And so the big difference between Dina and the QD it's a multipurricular windows window decrease factor. And so this defines how much on just a window. reduced, and we find packet loss. In case of 5681, this bar is 0.5. which means can we find packet loss we should reduce concession with the by half. But in case of cubic, can we find bucket loss, which is reduced congestion without just 30% on the And then this might not be very aggressive, but it could be read some kind of analysis. Now we have some discussion previously on the TCPM working group, So this draft the basic I captured at some discussion point. Yeah. So we're actually, unfortunately, out of time, I will take Martin, and then we should move on -- Mhmm."
  },
  {
    "startTime": "00:32:00",
    "text": "After after 30 seconds of reflection of VDs and Michael's comments. Like, another way to go instead of 50 say 681 biz is actually just you know you know, do a trans a trans prognostic renal congestion control document in CCWG since we're here. So, like, I don't know. Whatever here what is gonna step up and do this, like, I think I I'd personally be open to any of those avenues. especially if they're problems with 9002 and, you know, given SVTV, etcetera. So lots of space here for somebody to to step up and and and be a hero. Thanks. Thanks. Okay. Uh-uh. Let's talk more about this. At a later point, we're gonna ask for 2 hours next time. Sorry for the 90 minutes. We didn't know how much we would be getting. But now we're getting Gory next with another PCP Hi. I'm Gorrie. Then what do I do? Press this. Okay. I'm Corey. This one's been presented in TSBWG before. I promise never present it again unless somebody offered to help me. Michael Veltzel For some strange reasons, he said he would read it, which I counted as enthusiasm, and And -- Yeah. gonna talk about it here because I think it refer refers to this group. This is bcp41, which congestion control principles. The problem is it when we offer congestion control kind of worms. We end up with a lot of documents. This one is also quite old, and I've got some outrageous opinions to start with. So let's see. Number 1, much of what we learned in the nineties is still relevant. I think much of it is. not all of it is. and number 2 links, Holston Ruches are very different. They're much faster than they do a lot more things. We run a lot more protocols. So I think that probably is also true. maybe it's not outrageous. TCP provides safe operation for decades. What do we do next? Well, I think we back."
  },
  {
    "startTime": "00:34:02",
    "text": "Yes. Well, they're different from each other, and they're different from the nineties. It's they're all different. Okay. Thank you. Okay. So, seriously, my target is to try and replace or update, 2914. which is the principles of congestion control. This might sound silly, but it might also good because it will then give us language that we could use with the document that Martin presented. So I'm gonna motivate this a little bit further as to why we need to update. atatatat Things have changed here. changes in the network. At one time, it was common that the serialization delay of a packet at the bottom that formed a large proportion of the round trip time of a path. And I think that was common for many of these early specs from the nineties, and that really changes when we don't have that as the issue. So There's also changes in practice. We design things differently. We do congestion, controls slightly differently. We certainly designed implementations very differently. So things are different. Right. the Internet continues to be heterogenous, are parts of the Internet, which are very slow, there are parts which have very weird links. They are part of the Internet because people who come to this meeting from various places around the world will experience these as the norm. just because we see a certain type of network as norm If we live in a particular area, it doesn't mean everyone else does. And somehow, the IETF has to care about this And this is probably a document in which we should do it that care visible. So, There's a need to react to congest I think most people would hopefully agree with that. We need to be tolerant to a diversity of path characteristics we need to kind of somehow tease out what that is because the original document talked about this, but not in the way."
  },
  {
    "startTime": "00:36:00",
    "text": "We mentioned And, also, there's an protocol mechanisms, which really wasn't there before. It was always a assume that the protocol did the thing and normally would try and be the bad person to try and make your protocol break. that that isn't the way we think anymore at the IETF. So when we think about congestion control, we have to think about bad players and actors also. Well, hopefully, all that was relatively easy. So what's the big change I'm hoping for This one. No. It's gone. This one. I I think the congestion control isn't one thing. I think congestion control is 2 things. Hopefully, this is not contentious. I think congestion is a consequential side effect of multiplexing things. We need to adjust rate. We need to match the available capacity. I think all this is absolutely normal. we should see congestion in the Internet is good to have congestion from time to time, and we adapt our rates accordingly. The thing we want to avoid is persistent congestion. and that takes multiple forms. It might just be full cues It might be full queues where you start dropping control packets, and the control plane starts going away. It could be a regenerative retransmission, are many ways in which that persistent congestion is something we should avoid. But previously, when we looked at this at the IETF, we haven't split these two things apart when we talk but guidelines for congestive control. So this I think is my position I would like to put forward on this. If people hate this statement, that's fine. Please throw something now. in my direction. If people like it, please put your hand up. If people wanna talk, please just come on top. And please make sure you enter the queue. by -- Oh. -- the data tracker. bob talk quickly on the limited time. Okay. I'm both pretty good. independent ish. funded by Apple. So The the point you made about"
  },
  {
    "startTime": "00:38:01",
    "text": "the effectively policing the situation you know, enforcing things a bit I would to be very careful on that because I think we need some evidence that there's a problem. before we do that because we could really break stuff. Really break stuff. if we try and fix things that aren't broken. Good. like to I like comments. It's fine. Question. COVID. I am a bit worried that if we do two documents at the same time, 50, 33 b's, and this one You end up having 2 simultaneous ways of saying almost the same thing, but not quite. And I don't see the super.com. There could be one recipe for that. because I seem to be volunteered potentially on both editorial teams. And I could purposefully avoid that if that is what the working group wants to do. Yeah. I mean, I frankly, I would also see just one document, but I mean, we should find out what the two documents mean. I take that as an important input. we really should be very clear about what difference between these two documents is apart from the written and different decade. Thanks, Stewart. I wanna throw out a crazy idea, which I don't expect people to agree with right away, but I'm putting out this idea of people to think about and and they've slept on a bit, maybe we can discuss it again. I think we should stop using the term congestion control. because it creates the wrong impression in many people's minds. And particularly, the decision makers at companies, who are the managers, who are making decisions about how build products. we need a better term. So one idea I have is the rate optimizer. because When I talk to engineers who are building their own protocols over UDP,"
  },
  {
    "startTime": "00:40:03",
    "text": "because they're smarter than Van Jacobs, and obviously, can do best than TCP in the weekend. They're building these protocols And when I ask, what's your congestion control algorithm? they laugh at me like I'm irrelevant. Like, the Internet doesn't have congestion anymore. They fix the congestion and Their mental model is congestion is like cars on the road, It's a rare thing. It happens at rush hour. You fix it by making the road bigger. and a well operated network should never have congestion So congestion control is a thing from the eighties. Right? We don't need that anymore. And And I don't know. We we in this room know that that stupid congestion is normal, congestion when we use it in the sense, we mean the network is operating correctly. Yep. Device -- -- point 1. Yeah. Yeah. Right. And and point 2 is the thing that they they were thinking of, which Yep. Having the same term for both, I get it. So I If I send you one photograph, on iMessage, on my iPhone, should fill the network to 100%. Right? My bottleneck might be my Wi Fi hub. It might be my cable modem up screen. But whatever that hope is, I should be using 100% of my share of that bottleneck. because if I'm not, I could be going faster, and then you could file a bug report because we're not doing our job So it doesn't take a 1000 football fans at the stadium to make the network congested. it takes one person sending one photograph. or to be enough to saturate the network to a 100%. And I feel like if we ask these people, Is it the same? What congestion control you have? You say, what rate optimizes you have. Suddenly, they can't be quite coke so consident condescending and say, it would be stupid to optimize my design, and nobody says that. Right? So you obviously want a rate optimizer because that's what we're doing. We're working at the fastest we can send to get the job done, fastest, to get the video comms with the highest visual quality, But no more than that because you can't go faster than"
  },
  {
    "startTime": "00:42:02",
    "text": "the fastest, but you want to get as close to that as you can. So We need to reframe the discussion so people have a better intuition about it. I got it. Thanks. Next answer. Spencer Dalkins. I would invite the working group to think about names and Stewart suggested a very reasonable one thing I would like for the working group to think about first is The what he said that he was suggesting a name for I think that is an important thing to do for the reason that he said, I think it's also an important thing to do because when we are talking about at least some of the media related protocols that that I'm spending time with. The We often talk about patient and rate control and things like that. And the characteristics of what you're shooting for are reasonably different, but I hope that that those kinds of protocols also end up in scope for this working group. especially since RMCAT has just closed down, and all of their protocols that they had put out were experimental, I believe they still are. 1. Thanks, Christian. Okay. Like, 2 a half minutes. So Follow Steel's point, Yes. People will optimize selfishly. But there is one part of optimizing selfish that makes life harder for everybody. that if you are creating queues in joint resources and things like that, And That kind of congestion is not desirable. It's not desirable to"
  },
  {
    "startTime": "00:44:03",
    "text": "keep somebody else's packet out of the network. It's not desirable to impose a 2 second QQ to another connection. So there's a limit to any kind of self optimizing system. Okay. Matcherous. metros Meta. don't rename things to not congestion control. people that are laughing at you for suggesting that can you should they should be congestion control will laugh at you no matter what you say. So I would not optimize for those people and throughout, you know, decades of a what is relatively a good name. to So I wanted it to actually you echo Christian's point from earlier that I feel that more documents in this space is probably a bad thing. I would prefer to have fewer documents so that when it's not You don't have to be deeply involved to know what the different documents are saying. I think that's the state we're in right now is that we have a very long list of documents that all say different things, and it causes a lot of confusion when people are working in this space, and so I would encourage people like Corey, who are involved in both efforts to consider consolidation where ever possible and bias towards single documents rather than many numerous documents for guidelines, guidelines, Thanks, Matt. Yes. I've heard Van Jacobsons say in his own words, he regretted using the word congestion control. because it's ambiguous. It's it's used in different ways in different communities. So I I give a double thumbs up for finding better language Thanks. I know we're running of time. do you wanna close us out, Corey? I wanna try and go through let's let's go quickly. I've got through quite a few slides here in one These are the things that are in the draft that has got my name against it talking about these things because these things were mentioned in the principles"
  },
  {
    "startTime": "00:46:02",
    "text": "of congestion control. We're a different sort of list to the set of Martin's lists because these are principles for what to how this works underneath, what you want to try and cheese, and have a look at the draft if you want to understand what these are about. I have a quick show of hands whose record is draft. and have one hand. 2. Alright. Let's move on to our next presentation. So I guess I stopped that thing because I don't see people coming to me. So And talk to me afterwards if you want to change it because I made the threat last time, but, really, I'm not gonna carry on editing something that people don't want. And we can channel some of these accident discussions hopefully into a 50, basically. this. Okay. We got Neil next with a BBR. date. Hi. I'm Neil Cardwell, and I'm gonna give a quick update on work on the BBR algorithm at Google. This is with my colleagues here at Google. was done on those slides. And, yeah, So today, I'd like to give just a quick update on some algorithm changes, and then give an overview of the current deployment status of BBR at Google. talk about the status of the code and open source release plans. as as usual, we are we're interested in any kind of collaboration or feedback that people have time to provide. So today, one thing I wanna talk about is the fact that We're making a number of changes in the BBR algorithm and mostly bug fixes."
  },
  {
    "startTime": "00:48:01",
    "text": "and a few performance tuning changes. And as part of that, we are updating the version number from version 2 to version 3. That's largely because the recent bug fixes change the bandwidth and fairness Convergence properties. And so a lot of the test results from previous versions from version 2 won't really apply to version 3. So we Mainly, we just wanted to raise the visibility of this and make sure that folks rerun and test before carrying over assertions about the version 2 behavior to the version 3 algorithm. So that leaves it set a point where I'd like to just give a quick summary of the various version BBR versions at this point. just to sort of clarify. So DDRvone is is the version that we first talked about in sort of 2016, 2017, and it it uses bandwidth and RTT as the primary signals and uses loss a little bit over short timescales, and we can think of this as sort of obsolete or or deprecated at this point. There's VBR version 2. which is evolved from bbrvone, and but also is able to use ECN and loss as explicit signals. And we talked we've talked about that at a previous IETF. including some of the known bugs with that. So bvrv3 is what I'll be talking about today, which is basically a small evolution of v 2 with some bug fixes and performance tuning that I'll I'll talk about today. And then there's a variant call that we call BBR Swift, which we talked about, I believe it was 20 21, which you can think of as basically DBR version 3. But with using an estimated net RTT as the primary congestion signal. And this is sort of for data center environments where you know a reasonable target RT that you can use to decide if you think you're going too fast."
  },
  {
    "startTime": "00:50:07",
    "text": "Why is it not advancing? Yep. Alright. Here we go. So BBR version 3 The first bug fix I'll talk about today is a it's basically a bug fix with the bandwidth conversion with when you've got a loss or ECN signal that you've seen. And the specific bug is that you can run into cases where after you see loss or ECN signals, You set the the parameter or variable that we call in flight high. your estimated maximum volume of data that's sensible to have in the network. And then after you set that, your your bandwidth probing thereafter can can stop early. And and this can cause you to stop your probing before your flow reaches its fair share of the the bandwidth on the bottleneck link. And the root cause here is is basically a somewhat subtle circular dependence that you have between the maximum bandwidth that you're willing to send at. and then the maximum volume of in flight data that you're willing to place in the network. And, of course, Those can both limit what you see from the network in in a sort of circular way. And as a result, this issue caused PBRV two flows, in some cases, to not reach their fair share competing either with themselves or with Reno or Cubic. And then, also, it could cause flows to take a long time to to reach full utilization And the fix is basically to keep probing for bandwidth. until either 1 or 2 things happens. Either 1, you your loss rate or your ECN mark rate exceeds your your tolerance threshold, or you estimate that the the bandwidth of the bottleneck link has been saturated. And, specifically, we mean here that"
  },
  {
    "startTime": "00:52:04",
    "text": "in flight high has not limited your sending behavior recently. and you estimate that the bandwidth has saturated using the same algorithm that BBR uses for deciding that the path is saturated. in in startup mode, which is that if if you've gone for more than 3 round trips without the bandwidth increasing by 20 5%, then you estimate that you fill the pipe. And, of course, because this is a bug fix in prevents you from starving yourself in some cases it means that in some cases, you're using more bandwidth than you used to when you compete with cubic and Reno. And so it necessarily affects the the bandwidth share. existing with those algorithms. So to just give a quick example of what what the behavior looked like before the bug fix You can see it on the top where you got 4 BBR flows to starting at t equals 0 seconds and 2 starting 2 seconds later. You can see they never really converge. After the bug fix, you get some you know, reasonable a level of convergence or at least it's vastly better. And then the second bug of fix I wanted to to talk about today was was a case was a bug where you could see Bandwidth convergence problems when there is no loss ECN signal. And that can happen in when you've got a sort of moderate a deep buffer where there is no loss and you You know? No. There's no ECN signal available. and the and the root causes of this failure to converge are are that the the approach of using a fixed c 1 gain to calculate your c 1, could prevent slow flows from raising their sending rate to actually effectively probe for more bandwidth. And then the second aspect of the cause was that slow flows when they're using a lower gain"
  },
  {
    "startTime": "00:54:01",
    "text": "to potentially drain packets from the bottleneck queue. The the gain was actually too low, and and many cases caused flows to yield. Too much bandwidth to the fast flows. so that the fast flows kept believing that they had a higher fair share. And the end result to this was that you could could fail to reach fairness. even within a set of BBRV 2 flows, in in deeper buffers. And the fix here has 2 parts. Part 1 is to is to actually increase your c one gain to make when you're probing for bandwidth. to make sure that you can put more packets in the network and and effectively probe for more bandwidth. And then the second part is to not quite slow down not slow down quite so much. So instead of a a lower pacing gain of 0.75ease.9. And that basically arrives from a sort of if you do the algebra with a simple model of of what sort of value you need to make sure you can converge. That's kind of the number that that falls out. the question from -- Yeah. -- Emily. Hi. I'm Boragasny. question, you know, you talk about the convergence of different flows, adding a different setting points Can you share the assumptions on the fabric? because I think concise experience, you know, it really depends on how you, for example, manage the buffer. in your fabric that may impact your convergence Is there any consideration about it Can you say what is kind of there? queuing algorithm you are using or buffering algorithm you are using in the So all of these results that we're testing here are just with simple FIFO queues with droptail behavior. Alright. And so for the second bug fix, just to give you a sort of idea of what the behavior looked like before. And after the bug fix,"
  },
  {
    "startTime": "00:56:01",
    "text": "On the top, we have an example of a sim a similar test we had 2 flows starting and then 2 flows entering later on. And you can see that they've sort of failed to to converge to their fair shares. After the bug fix, there's sort of the more reasonable conversions to toward an approximate fair share Alright. So those are the bug fixes I wanted to to talk about briefly. And then The other aspects of bbrv3 are couple minor performance tuning changes that we've made. The first two are are changing the gain values used in startup, both of the see one gain, reducing that one, and then reducing the pacing gain. And then we link to some sort of quick algebraic derivations that sort of explain the the motivation for those changes. And the the nice thing is that they're both smaller values, and so it's it's less you know, it's it's It's gentler on the network, reducing queuing and loss. but also turns out to help performance as well. The the third changes to tweak the algorithm, the or mechanism that we use when we exit start up based on a congestion signal. And that one basically is saying instead of setting in flight high, based on the estimated p VDP also takes the max of that estimated BDP with the maximum number of packet that were delivered in the last round trip. And then finally, the the 4th change is that the exiting startup based on packet loss is triggered with a smaller number of loss events in a round trip And, basically, if you put all this together, then the the nice thing is the the impact of all these changes is that you get lower q and delays. lower packet loss rates on and that turns out to help latency for web like traffic, for example, or RBC, the traffic."
  },
  {
    "startTime": "00:58:02",
    "text": "So where are we in terms of deploying this stuff? and our team has basically deployed BBRB 3 for all of our internal WAN traffic within Google. as we mentioned Recently, BBR Swift is is used within a data center. And then for external traffic, that is traffic over the public Internet. We are using BBRV3 as the congestion control for all the TCP traffic for google.com. which we you can think of as as all of our sort of 1st party web traffic that's not video streaming, essentially. And then we're continuing to do AB experiments comparing v3andvoneforasmallpercentageofusers for TCP traffic for YouTube and then for quick traffic. ongoogle.comandandandYouTube So we're iterating to try to launch BBRV 3 for those traffic types as well. So what is the the sort of net impact of of the 3 versus is v 1 for these kinds of traffic. We're seeing a slight reduction in retransmit rates. we're seeing a slight latency improvement for web search and also the start of video playbacks. And as a whole, it seems like the latency wins stem from the the reduction in loss rates, meaning that there's less loss recovery and and faster loss recoveries. So earlier today, we pushed VBRV 3, the TCP implementation to GitHub. And it's mainly the changes that we discussed here in the presentation. And this is again, this is dual licenses, GPL or BSD. You can use it under either 1."
  },
  {
    "startTime": "01:00:00",
    "text": "And then we plan to email these patches and try to get them in the mainline Linux in the next month or so. in the strategy here is that we're gonna replace BBRV1 with v three in place. So if you use the BBR module, you're gonna get the newer version of the algorithm. And the motivation here is that We feel that this makes sense since V Three has better coexistence with Reno and Cubic. It has lower loss rates. It has lower latency for for shorter web requests and RPCs, all the while maintaining throughput that's pretty similar r to v 1 sort of within the 1% realm on YouTube testing. Alright. So, yeah, in summary, we we we're open we've open sourced a new version of BBR, and we're using it atscale@google. and we are planning on trying to get this into mainline Linux. And then after that, update the the Internet drafts to to match that version of the algorithm. And as always, we're or we'd love to get people's feedback and test results and and so forth. Thank you very much. Thank you, Neil. We have 6 minutes for questions and videos first. you, Neil. Midi, Thank I have two questions. Actually, the second question, you already answered that you updated all the GitHub soft Right? My question is in the first two box that you fixed, The general trend that I noticed is it's more aggressive terms of the conduction window gain and the pacing or maybe During the probing bandwidth time, it's more aggressive than VBRV2. Did you measure latency for those changes, and did it look like it went higher for those two flows that converge better. Yeah. We did measure latency, and latency seems to be sort of in this same ballpark because the the overall latency is sort of"
  },
  {
    "startTime": "01:02:05",
    "text": "determined by that, the average c 1 gain of all the flows. And So the changes on on the whole sort of mainly shifted the bandwidth distribution among the flows rather than changing the the overall amounts of queuing. So you took away from the other two flows to give it to the flows that were slower. It was I didn't see the yx's, Yeah. Exactly. The you're basically with taking bandwidth from the big guys to to give them to the small guys. Yeah. Okay. I can try to dig up the RTT numbers if you're if you're interested. Yeah. Mhmm. Thank you. Thank you. Thank you. Thank you. Yeah. Thank you. Okay, Corey. Hi, Gory First. Great presentation. Thanks ever so much. Are you continuing this is an IRTF activity of presentations? Are you planning to actually submit the draft for publication. We're open to discussion, I mean, or other suggestions, we we don't have any particular plans that are concrete at this time. Okay. It'd be useful to know your intentions in this we know about other drafts that might depend upon this. So and that'd be a useful conversation at some moment. Thank you. Okay. Thank you. Roy. Hi, Ruipolo Apple. does DVRV3 have any impact on L4S Networks. I noticed that on GitHub, there were some change it related to big gathering tree tree tree. bbrv2andalfourus yeah, let's see. So at a high level, There are no sort of conceptual changes. We do have some implementation changes in terms of some proposed patches for how the algorithm is"
  },
  {
    "startTime": "01:04:02",
    "text": "enabled. With bvrv2, there was sort of a temporary module parameter that we encourage people to use when they We're doing experimentation with ECN. with VRV3, our proposed model for deploying this is kind of in this in the spirit of DCTCP. So with DCTCP, there was a facility where in Linux, you could say, okay. With for this particular route, I would like to use this particular control algorithm. And so that would give you a way to say, over this route, let's use DCTCP, which would give you away. For example, if you have a machine, where on one side of the machine, it's facing the public Internet. And on the other side, it's sort of facing your internal network that is RFC 1918 private, for example. you could basically create a route that says, internally, I wanna use dctcp So we're thinking that for VRV3, big we need something similar in spirit that's different in the details because we would like the algorithm to be usable for both the public Internet and internally So the proposal here is to say that if you're deploying bbrv3, you would say you would create you would use a a new per route feature that we're offering where you could say, over this route, I know that this is a low latency ECN environment. And than The BBRV 3 code basically looks for a, does this TCP connection, didn't negotiate ECN support. And then b, does this route that it's using have a feature attribute that says this is low latency ECN. And if so, it'll use the ECM functionality in bbrv3. So that's kind of the proposed model. we're open to other suggestions if people have other ideas. but that that's what the patch set does now. And there's a brief discussion of that in the read me. document."
  },
  {
    "startTime": "01:06:03",
    "text": "We have a minute. Christian is next. So simple question. I looked at your modification of startup. What I would serve with my own measurements is that the BBR startup tends to be less efficient Then a I start And the reason for that is that I start exit start up based on the observation of the architect while BBR tends to exit later. So I was wondering whether you have considered using measurement of the RTT to exit startups sooner. We have. Yeah. I think that's something that could absolutely make sense. We just haven't had the development resources to to experiment with that yet. But I think you know, high start plus plus makes a lot of sense, and I I would love see people, you know, experiment with using that kind of approach with with DBR as well. agree that. can make a lot of sense. Okay. And Avishak is last. Hi, Neil. Appreciate from Meta. I saw the analytic derivation for the startup phase, even, and pacing gains. but but but in the bug fix, then the the Steven gain was 2.25 Is that backed by any analytic deformation or experimental Yeah. That one is experimental. I think, basically, probably any value bigger than the than sort of steady state value would work, and it's probably just sort of the, you know, the trade off you would imagine where the bigger the value the faster you converge, but the more acute pressure you have. Yeah. Right. Thank you. And then we will"
  },
  {
    "startTime": "01:08:00",
    "text": "hand it over to our next speaker. Thank you, Neil. Natalie, is next. Okay. Well, afternoon. My name is Nathalie Romo. I work for Dacha Telecom, and this will be share presentation about congestion control for DCCP Okay. So, basically, during the last years, I've been done some some work related to congestion control for this CCP. whose main application has been the multipath DCCP protocol. to give some context about a how condition control is 5 for the CCP. basically every congestion control algorithms. supported in this CCP has a congestion control identifier ccID. And the specification of these condition control algorithms as RFC are also called CCRD Profacts. Basically, each CCRD profiles is based on existing TCP, a standardized congestion control algorithms And at the moment, there are a 3 congestion control ID profiles for this CP. So, basically, in these profiles. What is done is a reference to the existing TCP congestion control algorithm and all some description of what what things have to be modified or adapted to work with DCC be. As I said, there are 3 condition control identifies currently standardizes for DCCP. They are the condition control ID 234, The CCID 2 is a TCP like congestion control algorithm. the CCID 3 is based on the TCP friendly rate condition control."
  },
  {
    "startTime": "01:10:00",
    "text": "And the CCAD 4 is Our experimental are received based on the TCP friendly rate for a small pack. 6. Now as I said at the beginning, the main application of has been the multiple DCP. So under this content we saw the need of having additional congestion control algorithms implemented. So we did implementations of BBR version 1, BBR version 2 and cubic implemented the CCIT 5, 6, and 7, respectively. further enough kernel, and these implementations are based on the the implementations in the next gap. For the case of the bigger version 1, I even presented some results in 90 CRG like 2 years ago if I'm gonna suck. Good. So what I would like to do with this is to actually have new CCRD profiles. which covered the newly implemented congestion control algorithms for visibility, that means to have an additional CCID profile for cubic additional CID profile for PBI I started also 2 years ago, an individual draft for the CCID profile 5. which is our implementation preview for a BBR version 1 for DCCP. I submitted this draft in ICRG, but it is currently expired mainly because I didn't know how to continue this work because there is no standard for BBR itself. can be referenced. So at this point, and the reason of this presentation is to actually get some feedback and how could I move forward with this work. So The first question is whether a CCD profile that is VVR Basel for this a good requirement, Saturday, and we we are a standardized at RFC, If not, what are the alternatives and also whether it will be possible have a different CCIT profiles for BBR version 1 and BBR version to and"
  },
  {
    "startTime": "01:12:03",
    "text": "related to the profile of the cubic a. of the given condition control algorithm. So the current RFC for much much much and I don't know if having CCID profile that references this informational Hubic RFC is also possible. basically. That's Okay. We have Spencer in the queue. Dispenser Dalkins. I would think that Having a Staple reference. would matter more than having an RFC RFCs are good. And we have talked about not We have we have talked about not standardizing, congestion control, mechanisms because you know, their work in progress. have been for decades. So for us to have an information of our see. They describe some it sounds perfectly fine to me and it's kind of what we've been doing unless something has changed dramatically recently. Michael. microtricks and having hurt his presentation, I would say to to something from a BBRB3, but not 1 or 2. regarding your last point. In TCPM, we do"
  },
  {
    "startTime": "01:14:00",
    "text": "this document of cubic, which is proposed then. So that should be out. sooner. Sooner. It's in the RFC editor. Okay. Thanks. Do we have any more fonts on this. Oh, we have Gary. Gory First, TSBWG. Tcher hat on. I think this is a TSBWG decision. be specification for DCCP, so standard track required specification of the protocol, which is a CCID. So I think we require standards to track publication of the protocol. speck, for the cease for the congestion controller. and have a top one of that. that answer your question? Does it make anything? Then I cut Does another question. So that means that if I want to n do this CCD profiles, I will have to submit the draft to the TSBWG. We could talk to our area director about whether he whether he believes that bit of work is transferred to another working group, but maintenance of DCCP certainly in TSBWG, and the CCID is a I honor registry that's controlled by TSBWG. So At the moment, yes. But the rules will be the same anyway because it the rules are specified in the RFC. And we have Martin. Since K. you put out the bat signal, as responsible AD for TSCWG and not for this working group. Yeah. My it's, like, registering DCCP code points is, like, a DCP thing and probably should stay in TSB at ABG. rather than be here. I would that would be my interpretation. Okay. Vaxel. Okay. Thank you. Well, if we're already clear on that, we can move on to our last presentation by Barak I believe."
  },
  {
    "startTime": "01:16:08",
    "text": "Hello? So I'm here on behalf of quite a big group of people contributing Sorry? Just get closer to the mic. Can you hear me now better? Alright. Alright. Alright. Thank you. Thank you. So I'm here on behalf of quite a big group of people working on this topic for quite some time. And, basically, we have come together from both kind of operators, datacenter operators mostly perspective, and vendors, and we wanted to collaborate on this work. So And intention here is came from the need for speed of cloud bidders in this, And, basically, what we see is that We have way more data that we need to we needed to reverse the network That means we need to have a high performance networks, networks, and it serves storage. It serves compute. especially if we consider these days and we have those we had sorry. Also, aside meeting about it, the growth of HPC N AI Networks, and they're requirements, requirements, as well as a resource segregation trend in the data center. So all of these contribute to the need for higher higher speed networking And, apparently, what I think we see is that as we need higher and higher speeds in the network we need to offload some of the things we use to do in software into hardware. It's not a new trend, but I think this trend evolves and adding more and more layers of the stack"
  },
  {
    "startTime": "01:18:01",
    "text": "if in the past, it used to be kind of a simple mathematical operation offloads. Now we see more and more portions of the stack going towards how they're offloads. is the only way to really scale the the capacity of the networks today. And because of that, we actually need faster or better congestion control. because everything reacts to much faster. The order of magnitude of the direction is is much lower. So it puts more pressure in the network, and we need to react to that. a better way. Some changes part of some connection control suites that are currently using or being used in high speed networks, is convergence. So I think it was kind of discussed earlier. if everything is smooth, then everything is okay. But then sometimes different flows, starting different times, and we want to converge to a to a certain state. and it's not that easy, and we need it to happen very quickly. We run multiple applications over converged networks. that by itself put some new challenges. And as most of you are well aware, especially in the centers, queues and buffers of scarce, As we scale out, we need bigger adhes and packing all of these together with bigger buffers and queues is a very big change. One more thing is to tune parameters. I think it was discussed in the BBR talk as well. especially as we have different applications, different environments, tuned parameters of some contextual tools, which is quite hard. Okay. So I think what we wanted to discuss here because of all of that is is"
  },
  {
    "startTime": "01:20:00",
    "text": "kind of a new approach or new input, into the conjunction control scheme, contextual suites. which is based on inbound telemetry. What we see in the market today is that many of the new networking ASICs have the capability to support inbound telemetry which means basically as packet traverse, the networking devices, The limit we can we put into the packets themselves and that can actually allow us a much more a much richer and much more accurate signal. from the network to be considered by the entity that we'll have or we implement the congestion control algorithm. There are various efforts in the industry that define these kind of inventory models and operations One of them is sitting here in the ITS, part of ITTM, You may be familiar with the IOAM term and it's being included in different types of protocols. Are there other efforts such as as in the p4.org organization, which called INT, and IFA also has some graphs as part of IPPM here in ITS. And, again, the idea is to utilize that this new capability coming from the network you have a more precise in which your feedback be congestion control. And what we have done, we have currently two drafts that are available for review. And, basically, we have the first left, that defines the algorithm of using telemetry information, and in particular, defines what kind of the telemetry. do we need from the network to be provided to the entity that control the congestion such as key links,"
  },
  {
    "startTime": "01:22:00",
    "text": "smitted bytes, Thanks, teams, link capacity and so on. So this is part of the first draft and the second draft. Accordingly, the process we took is not to decide on what is the actual format and what is the actual way to push the telemetry into the packet servers in the network? because as we discussed, the other efforts on that in including in the IETF. We want to build on top of that. We want to utilize this effort. With that, we put a draft more informational, that will allow implementors, to have some examples references to how this can be done. based on, again, the current efforts across the industry. I'm gonna take questions now. Yeah. Sure. We have Craig in the queue. Florida. Hi, Greg Mursky Ericsson. So I have a question. How critical you see that telemetry information being collected in a data packet itself. How would you define a data packet? Excuse me. How would you define a data packet? the packet that really triggers the collection telemetry. not sure that they fully understand the question. Okay. I'll try to explain it. So in IAM, define the several method of collecting information. the packet that we can characterize as a trigger packet includes descriptor of information that's being requested Right? So some method in I'm allows to put this information in the trigger packet itself. Mhmm. while another method referred as I am direct expert,"
  },
  {
    "startTime": "01:24:02",
    "text": "basically leaves it to the local policy. And that can either exporting raw data, or aggregating data and exporting that all over the management plane. Mhmm. So what you envision that your proposal requires and depends on exporting data, raw data in a packet that trigger a collection or aggregation of some sorts and then analytics of their situation in the network. multiple flows. Yeah. Thank you for the clarification. And what we assumed here so far? is that the packet that triggers the collection will carry the delivery data, Forward, towards their receiver But, you know, this is I would guess initial approach. Well, what we had the graph so far. And I guess we would be more than glad to discuss more ideas around that. think the key contribution we want to make here. is to introduce this new scope this new input into condition control algorithms and see how we we can we together as a community build on top of these new capabilities that are available And how can we build better networks this way? Mhmm. Yes. Because that sort of a Not in intuitive that collecting information in one place for the particular flow gives you a good understanding of overall situation in the network because obviously, congestion a particular place probably is caused by multiple flows, not one flow. right right right Would you agree? It could be it could could be yes and could be no. Yeah. Right. So probably aggregating this information for analytic when you can"
  },
  {
    "startTime": "01:26:00",
    "text": "see the bigger picture would be beneficial and preferable. Right. I think I think the trade off here is about the action time. So as we discussed, a lot of the high speed networks today assume how that offloads for some of their operations. If you will take this data into some collect remote collector, and then try to push it back into the network it may take long time. But, again, these are trade offs. Right? Yeah. But -- One But I don't think I don't think this is the right time. with the time that we have. focus on that, Yeah. We only have -- To can definitely be happy to further discuss with you. Okay. Thank you. We actually have four more people in the queue. We have -- So Are you Wanted to take some Yeah. That's that's all quickly for a slice. Yeah. Okay. basically, I think we addressed So, we try to address all the challenges we just discussed. So providing faster convergence, since, again, both the sun the center can have a very accurate understanding of the path and the current state of the past 4 given flow, that can allow for a very precise rate adjustments. We can get to a a near0q Again, because we don't rely only on the queue itself, bytes, if one rely on the queue, It means q is already has already been built up. and we may want to prevent it. And, again, because of these rich telemetry, we can reduce the amount of parameters needed to be tuned. I think this is a very important one regarding one of the initial talks here today. The way we look on HPC plusplus architecture is this is a service It's not"
  },
  {
    "startTime": "01:28:02",
    "text": "out of a specific transport. and it comes to potentially serve different transports. as well as potentially routing engines. So Some other routing engines may also be interested in the state of the network. So that might affect them as well. Additional work that is currently being discussed is multiqconsideration currently, we have been very focused on single queue case. consider additional residual feedback, and extend on encoding examples. With that, I would be very glad to discuss. Yeah. We have 2 a half minutes. and Dimitri is first then Jeff. Yeah. Sounds. And it's Most question, and I wanted to comment on previous question by Greg. It's really about your action time. Because in technical data center environment, you want to react on sort of few round trip times and round trip times in on the order of 10 microseconds, and, basically, that means you cannot aggregate. Yeah. And comment on HPCC past possible. What I really like is that this kind of congestion control, and Roan. congestion information source, more or less allows to distinguish between congestion at the endpoint and congestions in the middle. because another thing which probably the best place to talk about that. Normally, when we're talking about transition control algorithm, what we are trying to do, we are trying to influence spacing or rate something like that, and that's it. But modernized data centers are interested in environments and in particular, you usually have a lot of alternative paths. On the other few 100, So"
  },
  {
    "startTime": "01:30:01",
    "text": "reasonable action could be try to adapt rate. or try to influence entropy headers and just move on to one of those 100 as it passed. and it's probably good discussion towards the future. Just to provide conversational control guide and with some hook to for alternative reactions. And in this case, it's really useful to be able to distinguish between congestion and endpoint. in in which case you probably want to read the user independently. and congestion at the midpoint where both reducing rate and trying to move to another pass on the network reasonable actions. Thanks. Thanks. Thank you, dear. k. We're almost out of time. We have Jeff. 20 seconds. Just in 0. So that's pretty large deployment to this technology already. It's not just a IETF work, And we've considered the speed to answer the great questions. Once per RTT seems like a good speed. You could either do it through separate prop or just impose IT header on top of this in data plan traffic. the relationship between real data plane and probes prop wouldn't inherit same 5,000,000 as data plane would use So from behavior onto network, it falls exactly the same path exactly same characteristics, real data plane. So when you receive an ARC, you could just react the same way you would do on data plan signal. Thanks, Jeff. Let's very quickly take our last question. And I'm also curious, who has read out of the drafts? if people would please raise their hand if they've read any of the drafts. Okay? Alright. I'll tonight. Hi. I'm from Cisco Meraki. I'm wondering, did you way the benefits of out of band telemetry before deciding on this approach because for the concerns that were mentioned earlier. like, visibility, privacy, the NQ size, etcetera."
  },
  {
    "startTime": "01:32:03",
    "text": "I think we were quite focused on this current approach, but, again, we'll be more than glad to discuss other respective approaches as well. Okay. Great. I think we're out of time now. Thank you so much for presentation, and everybody for the discussion. See you all on track. Yes."
  }
]
