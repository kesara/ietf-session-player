[
  {
    "startTime": "00:00:23",
    "text": "all right hello everyone welcome to the PRG privacy has been suspense research group my name is Chris Wood here with chiffon sahih Sarah is with us remotely and meet echo just quickly moving forward this is the note well you\u0027ve probably seen this lots of times so far but if you haven\u0027t please take a moment to familiarize yourself with it or you can read it online Siobhan is passing around the blue sheets please make sure you sign them we have a jabber scribe correct yes so we have a JavaScript is someone willing to be a JavaScript do you want to do just jabber which one would you prefer okay let\u0027s do jabber and we also need a minute taker then any volunteers make eye contact with people I know until someone does their head Ben thank you so this is the agenda we have today we have eight minutes of sort of research he presentations to start and then followed up by some proposed individual drafts for the research group to close things out does anyone have any last minute comments revisions they would like to make before we get started doesn\u0027t look like it okay so on that note we will kick things off with Pete who will tell us about privacy standards and anti standards you might need to talk cause you didn\u0027t like okay take two so my name is Pete Snyder I come from brave software where I do privacy research I mussel co-chair of ping which is privacy group and w3c and so this is some lessons taken away from the intersection of those activities so just as a brief overview of what I\u0027ll be talking about first we\u0027ll talk about how standards impact the the privacy work that brave does is a privacy or anything vendor very briefly then I\u0027ll talk give a couple examples of anti Stan error "
  },
  {
    "startTime": "00:03:24",
    "text": "anti patterns that we see reoccurring in standards and how that makes life difficult for people who want to do privacy protections on the web and then finally I\u0027ll give just some kind of listen of additional topics that are related that I would I think warrant further discussion but I there\u0027s not time for this talk so first how do standards employees or impact our work as a privacy oriented implementer so in order to protect privacy on the web for our users great makes a bunch of modifications to the browsing environment we block state storage in different places we integrate tor and a lot of people users use tor in the box we block a whole bunch of resources from being loaded the first place from a variety different places like easy list and in-house generated lists and things like that the most relevant to this conversation is we modified the browser environment and so we end up violating standards in a large number of places because implementing those standards correctly would harm our users privacy just to give kind of a very specific example here\u0027s a recent paper that came out that gives a an overview of a bunch of fingerprinting methods that are on the web the relevant irrelevant to this conversation is not only do they do a nice work nice job of surveying existing work on how people are actually doing think of fingerprinted online but they also give the impact of each of those fingerprinting methods and and kind of a benefit how much identifying power they give in so the one I want to talk about here is on the right arm on the right hand side audio context which is a way of identifying the user based on particular it\u0027s about how the browser will do audio synthesis it turns out if I won\u0027t go to the details but it turns out that different vendors and different pieces of hardware and things like this will do this very subtly differently or sometimes not so subtly and you can use this to build up a semi identifying piece of information about the user which can be combined with others to uniquely identify them and so brave modifies this and what we do is we we know a lotta bunch of things that the browser allows you to do by default these are things like asking querying the hardware for its capabilities or different low-level things and so what brave does is we just say no when it\u0027s a third party context we just say you can\u0027t do that unless you user Ops and explicitly says violates a standard but to do so otherwise with the harm art would allow our users privacy to be hard we do this in a whole bunch of other places as well this is it not even nearly a complete list but I mention these just as a partial enumeration of how extensive the problem is so how did we wind up in this situation or why is this problem so endemic across the browser platform I only have three different examples of these kind of anti patterns we see in standards that lead to this outcome this is not anywhere near an extensive list or a comprehensive list but I need it as motivation then demonstration so also I don\u0027t mean this to be simple this is not like a just do these three things and all of a sudden privacy comes easy but I "
  },
  {
    "startTime": "00:06:24",
    "text": "mean them as a necessary but not sufficient list of problems so first is we see reoccurring over and over again that standards are extremely specific about the functionality that needs to be implemented but are extremely unspecific about how to how to mitigate the privacy harm that that\u0027s that functional the ads so for example many w3c documents have a privacy consideration section this these are things saying like implementers should be concerned about these privacy concerns that when they implement this but they are not normative they are not mandatory they are just a list of concerns but the rest of the document which is almost without fail much more much longer and much more specific so specifically the functionally that must be implemented and the result is that everybody agrees on everybody implements the harmful part and people are extremely unsure what to do about the non harm or about the mitigating part and as a result nobody can people vendors hands are tied when they want to try to do the mitigations because websites assume the standard the well-defined behavior so just as a motivating example this one you may be familiar with referred policies a is a standard that says under certain conditions notify the website that you\u0027re visiting now of where you just came from this has extremely specific very obvious privacy harm our privacy implications there\u0027s a very short piece of text saying we know that there\u0027s privacy problems here but in the mitigation section it just says vendors can do whatever they want there\u0027s no specific specificity it just says vendors may violate this at any time as a result many websites rely on this is very unusual to website operators and that\u0027s result many many websites just assume the refer policy will be standardized as it\u0027s described in the rest of the document and it now if you in if you\u0027re in brace position or somebody like Gray\u0027s position where you want to protect the users privacy and you remove the refer header or you do different things to try to make the referer header less privacy harming you break a whole bunch of websites this text seems explicitly permit user aliens to suppress the refer adder entirely what would you want it to say I would like it to smoke I have a long list of things we could probably talk about offline but to the first high order bit would be don\u0027t do this but if you had to do it I would say don\u0027t expect in third party context prefer a policy should up should only be sent on a referral to be sent on the user gesture things along these lines I think you okay well the last point is different but I think even if from the optimistic view of the true creator which are web authors we eat specifications um you just know what chrome does I think those are probably I\u0027m sure you\u0027re right that web bodies do not read specifications I\u0027m sure that web authors do read things that are distilled from specifications frequently perhaps my experience is they do what chrome does so so I think I guess my point is that your thesis seems "
  },
  {
    "startTime": "00:09:24",
    "text": "to be that on it\u0027s flexible in the specification that creates the problem I\u0027m not sure that\u0027s I\u0027m not sure making very participation of that in the particular example I think that may distill down to a distinction without a difference in that by saying vendors can do anything everybody is following the specification and so chrome is following the specification and also doing what follows its lead the standard is no longer the guiding principle I mean we can take sounds good okay example two functionality that\u0027s useful only in a very very very small set of situations but is made universally available to all websites so problem here is you have something that\u0027s very useful to a very small number of people maybe that\u0027s doing things like audio synthesis but it\u0027s made without there\u0027s no there\u0027s no permission in place there\u0027s no gating the functionality you know anything like this it\u0027s all of a sudden it\u0027s not being used just for this very narrow use case it\u0027s being used for finger printing or it\u0027s at least available for fingerprinting it\u0027s available for these kind of privacy harm uses and so as a result becomes extremely difficult to pull this off the web without breaking all bunch of websites that expect it to be in place an example here canvas element allows you to not just write things to the campus but also pull things out the use case for this is not zero but it\u0027s obviously not the common case of when you\u0027re writing things to do a canvas and so what do we see we see not we see the common use case of this is on libraries like fingerprint js2 where if you dig into the code in the lower right hand corner you can see this being used to generate unique identifiers for users now if you pull out if you think you\u0027re somebody like brave or you\u0027re somebody who\u0027s doing privacy protections you think great we\u0027ll just pull out this functionality on all of us and you broken whole bunch of useful use cases as well where the the goal in the first place should be just to say figure out a way of getting permission to the sorry gaining access or restricting access to these kinds of positive use cases in the first place again how would you do that so again I have a long range of suggestions but the the first approximation is to say user gesture say permission to say some other way of the user signaling I\u0027d like to do a thing I\u0027d like to have you back canvas likely if so reading back cameras in and of itself is often not useful you need to do something with that value for it to be useful save it to disk stay with the storage that and so those are things that other browsers already gate behind these sorts of things storage access API brave does etc and so these things get paired together I\u0027ll it may be natural to pair them in that way explicitly yeah i\u0027m i think probably seen you in a place where we\u0027re gonna put costly popular user for like all sorts of things they can\u0027t possibly give permission promises is one of these things user gesture is not a permission prompt to third-party frame it\u0027s not the permission prompt thinks about sure but not everything since we got you just lots of things "
  },
  {
    "startTime": "00:12:24",
    "text": "like pretty good for him his fingerprinting there plenty of ways to forced user pre huge gesture right that is it is not possible to trick the easy to do many things but but require me to do something would take it out of the common path for sure so these are these are additional lists of things that it\u0027s extremely hard to think of why that every single website needs son gated access to them these are all things that are used in fingerprinting techniques and i would suggest we should do something somewhere here so what\u0027s the what\u0027s the lesson to take away from the second pattern assume they people will use your functionality if you allow them to use it across the board if you think there\u0027s a the only a small subset of use cases or places where your functional is actually you needed figure out ways of getting or restricting accordingly third and final is what we see over and over and over again in standards conversations which is websites can already do bad things what is the marginal harm of allowing them to do an additional bad thing I think this is unhelpful so for example if something that the standard is proposing a new way of allowing web service to do community or applications to be communications to remote servers the common refrain is you can already go with an image tag what\u0027s the harm of allowing them to do it in a different way this is a way of expanding your horizontal privacy that indefinitely and making it impossible to stop to fix the problem after the fact an example here is client hints which is both both has aspects in the w3c and IETF the relevant part here is I won\u0027t walk through this because I assume people are familiar with since it lives here but there\u0027s a large number of there\u0027s a large amount of papers being written and research that exists showing that the exact kinds of values that can be requested through client hints can nearly identify a non-trivial number of users as a result the response is often well you can already identify users anyway what\u0027s the harm of already doing this cookies exist don\u0027t they etc etc I think this is the way of just doubling down on the problem to put put it differently when you\u0027re digging and stop digging or when you\u0027re in a hole stop digging figure out freeze the problem start thinking our ways to mitigate the problem as is and don\u0027t and don\u0027t entrench the problem indefinitely again I can I\u0027m happy to say more specifics if this is a topic of Congress of interest ok one last thing I wanted is just to say it\u0027s just kind of bullet point of a number of different things that I think would weren\u0027t for the conversation but are too much to go through in detail here one thing that we see over and over again is is a way of kind of pitching the problem forward to say we know this part of this this new standard introduces a privacy harm but we\u0027re working on the standard that\u0027s coming down at the pipeline in the future that will fix that problem I think this is extremely harmful not only does it type the future authors hands and what now they have to do to address the current problem being introduced but it gives you know it makes it extremely difficult to evaluate the privacy harm by the standard you\u0027re considering right now "
  },
  {
    "startTime": "00:15:24",
    "text": "which is to say something may change in the future that is not a basis to judge something that\u0027s gonna be introduced today second point I think is worth keeping in mind when evaluating standards is that the idea of formalizing bad practice has at least some some appeal in that you can say well if we can get all the bad uses to use this new API instead of the old API then we can reason about that new API use in some semantically valuable way I think this is not useful because what ends up happening is that actors use both api\u0027s instead of just a new one and the last is this kind of what I think is this kind of like judo move where people say well site authors use this people like sites and so users indirectly wanna you want this to exist I think this is totally not helpful it is important to consider the site site authors needs the first and foremost is the person using operating the software and to recognize those interest at verge frequently and to consider the harm to the user not to nebulously users in general so some last takeaway some last things I hope to keep in your mind is that the amount of standards getting pushed through is just extremely difficult to be able to reason about privacy wise and so to a first approximation the best thing we can do for privacy is just just slow the roll a little bit and to give things a little bit time to percolate into to percolate through the review processes second think about complexity in term of itself as a privacy harm it\u0027s not adding anything new to the platform brings some risk and also brings some reward but to not think of it as there\u0027s no privacy harm that I\u0027ve identified it so it\u0027s fine to add it is not the right criteria to consider when you\u0027re thinking about improving privacy on the platform and then third is I think a totally under considered risk is that standards work largely now or a non-trivial amount of standards work is standardizing things that are already shipped and at that point it\u0027s nearly impossible to pull them back in and so figuring out some way to reason about things before they get off the door or at least earlier in that process it\u0027s probably a place worth digging into okay so I\u0027m happy to discuss some things more about it but I just want to say thank you very much for your time I\u0027m Pete Snyder I\u0027m the privacy researcher at brave I\u0027m here to try to help do better and privacy has standards Thanks are there any questions so I have one you mentioned many examples in which standards could potentially be improved or specification secured least be potentially improved to help you know perhaps benefit the privacy of those implementing them or those users that are using the things that are implemented is there any like what sort of concrete or tangible steps can either the IRT app or the IETF take to move in that direction if you can offer some advice or I guess sure next steps so I think there\u0027s a in w3c we "
  },
  {
    "startTime": "00:18:25",
    "text": "have the idea of hearth horizontal review boards or horizontal review groups that at different iterations in the standards lifetime from conception to recommendation groups like paying the privacy group at other groups like accessibility are expected or at least have the option of giving input at that point I think for maizing that process stronger would be an extremely useful way of allowing interested in concerned actors to get involved earlier in the process Stephen most of the examples that you mentioned we have taken from the world of w3c which I\u0027m not familiar with beside the client ins do you have ideas of photo codes IETF protocols with the same sort of problem because in theory in the IETF people should avoid security considerations including privacy we have FCC 9:23 so in theory problem should not happen in the IETF but of course they do so do you have specific examples in mind from your experience with so I don\u0027t have any examples from the ITF world specifically that I would feel very confident talking about beyond just kind of yeah from the outside but I would say we also WEC standards also have these privacy and security considerations sections and there\u0027s a concerned vendor I think that is a step one of what needs to be a 10 step Road to have lemon so thanks for presenting this is really helpful one of the things that I\u0027ve noticed people do to try to mitigate risks of this sort is to have essentially a list of things that a particular site is allowed to do and that seems like I mean this is that this really wouldn\u0027t apply to IETF standards but it sort of seems like it applies to the stuff that you\u0027re actually talking about here so basically a set of entitlements I mean does that make sense to you as a way because the problem with what you\u0027re with with slowing down the the advancement of progress so to speak is it\u0027s very difficult to do because there\u0027s always somebody who wants this new feature and what do you say to them I mean the new feature is probably totally privacy violating and there really isn\u0027t a mechanism for actually preventing it from being exposed to the user when the user doesn\u0027t want it to be exposed to them there\u0027s no way to actually enforce that restriction so so it seems to me that the actual research problem here is how do we create a framework for enforcing restrictions of that type so I completely agree with you that that figuring out what sites should be allowed to do what it what any given point is extremely difficult i I slightly disagree with you and that there\u0027s not ways that vendors can enforce those choices those vendors a couple of examples came up before other things maybe policies determined by the browser offline or shipped with the browser given some knowns that of safe sites permissions are knocked around but "
  },
  {
    "startTime": "00:21:28",
    "text": "but not useless way of going about this user gestures where the frame is where the code came from which is not a variable in any standard currently but not what frame is it executing in but what who delivered it etc etc etc so I think I think I think is more ivan either for hope than well it sounds like you actually agreed with me there because what you\u0027re really saying is that we need a way to make that happen oh yes thanks garrymon diem qualcomm we call back to six years ago when I was chairing the geolocation working group from the w3c we had a discussion is he you know related to ping advice on the topic and we just then we discussed the concept of whether a webpage could or what a web web service provider couldn\u0027t declare to the user what their intentions were as far as any information they would provide and we couldn\u0027t figure out a way to actually do that without getting it without it being abused by rogue parties I\u0027m wondering now though you know when you look at the only innovations such as certificate transparency and we\u0027re getting better and better authentication into the browser all the time respective websites whether whether browser based policies with respect to individual websites could actually take the place of having to specifically advise the user from the service service itself so I was wondering what your thoughts are on this well I mean I think geolocation in the browser is actually a positive example I mean that is a of the many ways that user privacy is harmed that is not often one of them because it very explicitly says the users understand what that means and they have they opt in and I think of the hundred things that makes me concerned that that\u0027s not one of them I think for that reason I think the idea I\u0027m not sure there\u0027s any solution to the concern of the website saying I\u0027m only gonna do this with that information I think that\u0027s probably even even given another advancements I think that\u0027s helping out a way that it\u0027s going to be useful going forward we\u0027re on to the next presentation is this cute okay hi everyone I\u0027m Sandra a PhD student at EPFL and today I\u0027m going to present some work done by myself and my co-authors on traffic analysis of encrypted DNS so I\u0027m going to start by jumping straight to a conclusion which is that we did a number of experiments "
  },
  {
    "startTime": "00:24:29",
    "text": "where we did a traffic analysis of DNS over HTTP traffic and we found that monitoring and censorship it\u0027s still feasible even in the presence of encryption and that currently proposed a DNS based countermeasures against traffic analysis are not sufficient to prevent such attacks so in this talk I\u0027m going to describe some of these experiments in detail so when a client connects to a destination host generally this is preceded by a DNS lookup as we all know there are measures in place to encrypt the connections between the client and the destination hosts but DNS lookups have so far been sent in but here which makes them susceptible to eavesdropping and tampering for example if you have an adversary on the path between the client and the recursive dissolvent the adversity get get some idea of the browsing history of the user for example which is a privacy concern there is also a censorship that\u0027s based on DNS so there have been measures in place before that have been proposed to improve DNS security you have measures such as DNS SEC which look at authentication but do not provide confidentiality and you have measures such as DNS Script which allow for encryption but did not see much widespread adoption over the last couple of years you have these protocols DNS over TLS and DNS over HTTPS which have been gaining some traction the idea behind them is to set up a TLS session already or an HTTPS connection between the client and the recursive resolver and exchange DNS queries and responses over this connection and since this is encrypted ideally this should provide some privacy for the user so the scenario that we are looking at is an observer who is monitoring the connection between a client and a recursive resolver so the user visits the page and the observer tries to get some features from this DNS over HTTP or door traffic and tries to guess which web page is being visited by the user note that since the connection is encrypted the observer no longer has access to the content of the DNS queries and responses but looks at information such as sizes of packets the timings between the packets the directionality of the traffic or headers such as TLS headers and the idea that we\u0027re basing our analysis on is the fact that when a user visits a web page there are a lot of additional resources such as ads static files or images that have to be loaded as well which could be hosted on different domains so a visit to a web page actually consists of multiple DNS queries and responses and the set of "
  },
  {
    "startTime": "00:27:29",
    "text": "query seven responses could act as a fingerprint for identification of that web page we are considering two adversary goals here monitoring and censorship and I\u0027m going to speak about each of them so as I mentioned before the goal of a monitoring adversary is to look at the do edge traffic and get some features so we and I try to identify the webpage visited by the user so for this we build a classifier based on size and directionality features of the door traffic I don\u0027t go into the details there of the classifier they\u0027re available in our papers but we basically can conducted two experiments here in the first experiment we considered a case where an adversary knows the entire set of is that a user has visited and the goal of the adversary is to identify which particular webpage was visited by the user so in this experiment we considered a set of 1,500 web pages so if you consider a random classifier that tries to guess which web pages it is the classifier would try to guess this with one on 1,500 basically but what we see is that our classifier gets a 90% precision and recall where precision is a measure of correctness of all the results that word is turned by the classifier and recall says how many relevant results were returned so when the classifier has a high precision and recall score this means that not only did the classifier identify a large number of web pages it did so correctly in the second experiment we consider a bit more realistic scenario where an adversary does not know all the set of web pages that are visited by the user rather the adversary has has is interested in a subset of the webpages called the monitored set and the goal of the adversity is to determine whether the user visited a page in this monitored setting so for this experiment we looked at a set of 5,000 web pages where 1% of the web pages were in the monitored set this is generally a harder classification problem in the area of website fingerprinting and we see that we get a lower precision recall score of about 70% but this is still much higher than a random case the second goal that we considered was censorship we did a preliminary analysis of a censoring adversity the goal of the censoring adversary is slightly different the idea is to identify a web page as fast as possible and I really try to block the connection so this means that an entire door trace will not be available to the adversity and the adversity has to look at partial door traces so what we did here was we took a set of 1,500 web "
  },
  {
    "startTime": "00:30:30",
    "text": "pages and we the uniqueness of the door traffic when only the first LTL s records have been observed and the idea behind this is that when two traces are unique then we can find out which trace to block so what we found out in our analysis is that generally the fourth TLS record usually curls ons to the first doe query in our traces and the size of the TLS records also has connection to the length of the domain name so this means that one strategy that an adversary could follow would be to block on the first query this means that the user will not be able to load the page the disadvantage of this method is that it could result in high collateral damage because other pages with the same domain length could also be blocked as a result of the strategy another thing that we found out was that by the fifteenth record or so which corresponds to approximately 15% of a trace length most of the traces in our set were distinguishable so the adversary could follow a strategy where they try to block after having a high confidence that this is the trace that they want to block so this reduces the collateral damage but the disadvantage is that this would result in the user being able to load most of the page they might miss out on some of the resources the page we also did a number of experiments where we looked at the robustness of the attack and by this what I mean is when different aspects of the experimental scenario change how does be an adversary try to keep good classifier performance for example DNS traces can vary over time so how often does the adversary have to retrain their classifier then we wanted to see the effect of client location on our classifier as well as changes in the infrastructure so by infrastructure I mean we change the the resolver we experimented with CloudFlare and Google\u0027s resolver we looked at cloud flat standalone doe client as well as Firefox\u0027s in Bill client and finally we did some analysis of desktop versus Raspberry Pi environments and the main takeaway is that for best performance ideally you would train a classifier that is tailor to that particular scenario if you use a classifier trained on one set of parameters the classification performance does drop when you test on another set but it does not stop the attack so what we saw from my initial set of experiments are that monitoring and censorship are feasible even when DNS traffic is encrypted so we looked at countermeasures to prevent traffic analysis attacks so the first "
  },
  {
    "startTime": "00:33:30",
    "text": "thing we looked at was edn airspace countermeasures where a DNS is extension mechanisms for DNS so one of the options in a DNS is a padding option which allows you to add some padding to DNS queries and responses and the idea behind this is that you remove the size information that is available for the classifier to distinguish web pages so the first thing that we did was we implemented padding of DNS queries so we used cloud flash stand-alone doe client for this and we implemented a recommended padding strategy so the RFC there has padding strategies and the recommended one is to pad queries to multiples of 128 bytes which is what we implemented on cloud flask line we had also contacted CloudFlare with the initial set of results and they implemented padding of responses on their resolver how were they they padded their responses to multiples of 128 bytes whereas the recommended strategy is to add them to multiples of 4 68 bytes so we decided to compare both the strategies as well so the experiments that we did were the two e dns-based meshes which I just described we also looked at a case that we called constant padding so this is a simulated scenario we wanted to see if we had perfect padding that is if all the TLS records work to the same size how the classifier would perform so we basically set all the sizes to the maximum possible value that we saw in our trace and apply the classifier to this case and finally CloudFlare has a dns over tall service where DNS queries and responses are sent over the Tor network so we decided to experiment with this service as well to see how anonymous communication acts as a defense so this table outlines the performance of the classifier just note that the values are as decimals not as percentages here for for comparison without any counter measure the classify attains about 90% precision and recall we see that with a DNS with the current CloudFlare strategy precision and recall drops to only about 70% with the recommended padding strategy it drops to about 45% both these values are much higher than a random case which shows that a DNS based measures and do not eliminate traffic analysis if you look at constant padding it\u0027s about 7% so there is a major drop in the performance and DNS over tor achieves the best results with about 3.5% precision recall we also looked at the overhead in terms of amount of additional traffic that is added by these counter measures so we did a very "
  },
  {
    "startTime": "00:36:30",
    "text": "short experiment where we took 50 web pages and about 6 samples per web page and applied each of the counter measures and looked at the total volume that is sent and received bytes of the TLS records just note that the y-axis is in log scale here we see that the e DNS face measures as expected do not add much overhead our constant padding adds a lot of overhead because we are padding everything to the maximum size and DNS over tor is somewhere in between so we see that tor is a effective defense for the traffic analysis attacks and the reason so that interview the data sent in fixed cell sizes which reduces the variability of sizes and of exercise related features of the classifier another thing is that there\u0027s repackage ization and by that we mean the data can be merged or bundled together in tall and this affects the directionality features we look at which records have been sent and received and you when thought does this this affects the directionality features used by the classifier one thing that we are not been able to explain the clusters that we saw in the confusion graph so this graph shows web pages that have been mislabeled as one another and what we saw is that web pages generally tend to be clustered where web pages within the same cluster and to be misclassified as one another this means that the anonymity set for a particular web page is not the entire set of web pages in the test but rather only the web pages within a particular cluster so we did some initial traffic analysis but we have not being sorry feature analysis but we have not been able to determine why this is the case so I can take this opportunity to ask a clarifying question as long as you\u0027re waiting yeah sure is your tour protection is that DNS of Earth over doe over tor is DNS over Tosh so so regular to us yes okay thank you oh wait oh yeah inside to one of the ways getting help is okay sure yeah I\u0027m "
  },
  {
    "startTime": "00:39:35",
    "text": "on my last legs anyway so currently we are working on a few different things the first thing is that our experiments looked at the case where a user\u0027s visiting one page after the other and this is not exactly a real realistic user scenario so we are considering the case where you can have multiple tabs open by a user which results in some interleaving of the door traffic our initial results show that a classifier gets about 40% precision recall with about two tabs another thing is that we consider the case right now where there is no caching of the DNS records so we want to study the impact of caching as well on the classifier we also started doing a comparison with DNS so a TLS traffic and we looked at the padded DNS so a TLS traffic and we see that it is much more resistant to the classify it\u0027s about 28% as compared to dough so we have started doing or feature analysis to see why this is the case and finally we want to see if we can have counter measures which include both padding and rhe packetization but without tors overheads and latency and volume caused by headers so this is basically the summary that currently proposed ddns measures might not be enough to prevent the traffic analysis and these are some links to our paper Thanks I think really good work Thanks if you go back to the tape the table yes so so I saw when you say constant Pony I had a quick look at the paper in that I mean you\u0027ve had it to the biggest size pocket of all yes so this is so we don\u0027t actually apply the padding here but what we do is we take all the traces and we change the sizes to the same value which is the largest value that we saw on this set because the idea is that you want to see a perfect scenario where all the sizes are the same so when you said constantly there it doesn\u0027t mean just having every tear less I know no I know nonetheless why is that so different from a DNS for 6/8 diffic I don\u0027t understand why doing that makes such a difference for Indiana\u0027s for six it what we saw is that there is still a variability in the sizes that are there in the classic and the variability in the sizes is a big feature for the classifier whereas with constant padding because all the sizes are the same the size is no longer a feature this is both "
  },
  {
    "startTime": "00:42:37",
    "text": "query and response sizes so you had a lot of queries that were 128 is not enough padding yeah but I just want to clarify that the response sizes have a greater variability than the query sizes so the responses do have a higher impact were you able to determine what is it in the responses that\u0027s causing them to go more than four six eight is DNS acre I know we haven\u0027t yet done that analysis of the content of the responses interesting difference and finding out more about that and turning that into some guidance for people like you know presumably somebody\u0027s using a really dumb query name so don\u0027t do that if you don\u0027t want to be and somebody else is giving like lots of answers hi Daniel con Gilmore thank you very much for doing this work I am the guilty party for proposing Edie Anna Sarah 468 and I proposed specifically because I wanted to encourage this kind of work so I\u0027m really happy that you\u0027ve done that and great to see the results the way we arrived at the recommended padding policy was basically to look at individual DNS queries and responses and I like what you\u0027ve done here which is to look at them in combination so I have a couple of questions that maybe you can give me what your intuition is I think what you\u0027re saying with the constant padding arrangement is that the reason that it\u0027s not zero there is that different websites do different numbers of DNS lookups yes so there are a couple of things it also depends on the kind of features that we used so one of the things that we looked at was we looked so if you have a trace we looked at whether each TLS record was going from client to resolve over from resolver to client so then you have basically a sequence of you know which direction it is going in so even if you removed the sizes you still have this directionality as a feature directionality and also parallelism right if I issue three requests and responses back yes that\u0027s different from saying request response exactly yes sir so number of queries and and cadence yes yes and then I wanted to ask you to hypothesize I know you haven\u0027t done the research to dig into it and produce a printable result but why you think the DNS over TLS was markedly better than the DNS over HTTP um so I mean if you looked if we look to just of the sizes we saw that there was much less variability in sizes I\u0027m wondering whether it is due to some of the configuration messages that are being exchanged in do an additional addition to the queries like we don\u0027t distinguish those messages since we are looking at just the the TLS record size options frame yeah maybe something like that which could be different for dough and TLS hi Fross a "
  },
  {
    "startTime": "00:45:44",
    "text": "canal blabs very cool work I really appreciate that I had so one question is already mystique eg acid but the other one I had was so nurses hypotheses that the DOE Safari said that if you mix do H traffic with normal web traffic that that would obviate the signature of the DNS traffic a lot more is that something that you consider studying as well yes this is something that we thought about right now we do consider toe traffic as separate from website traffic and we feel that when you mix them this might affect the results we we haven\u0027t done some work on that yet but this is something we are considering okay thank ya Westford karai and he took my question so I\u0027m gonna ask the little tiny one at this point is your data set available yes we\u0027re planning to make a it\u0027s not available at the moment we are just sort of cleaning things up and we\u0027re planning to make it available Christian we tomorrow I am curious at how exactly you measure the lengths of the queries and responses responses so we can pick up files and we look at TLS records which are of type application data if I remember and then we take the sizes of those TLS records so us humor that there is a direct mapping between the size of the DNS messages and the size of the TRS because we also did verify this by decrypting the records and in our paper you can see we have a plot where we show the sizes of the queries as well the sizes of the records and there is a they follow the same shape essentially so what if what if an implementation was not was deliberately either groupings have worker is in the same record or splitting queries and multiple records what would I to queries in the same record all I think that um you mean changes the size of the five queries for example yes there is nothing in the protocol that says that those five carries up to go in five years because they could be packed in one big message yes but is sent as a single theorist recorder yes so if site did that with a change your results um I think it might change especially if the classifier has been trained in in a particular way as and if it\u0027s trained based on traces where "
  },
  {
    "startTime": "00:48:45",
    "text": "individual queries are in individual records this of course changes the pattern of the traffic so how if you train it in that particular case if you have a mix of traffic I think it would affect the performance of the classifier yes thank you Jeffrey askin do you have any feeling for how the position in recall will scale as you increase the universe of websites from a couple thousand to the size of the Internet yeah so this is one of the major things because our dataset is not very huge we rather have multiple experiments with smaller datasets I would assume that the precision and recall would go down but I don\u0027t know by how much yeah so this is Sarah Dickinson on Java she asks have you considered including oblivious DNS in a future analysis and yes actually Carmela I think your colleague ready to play okay you watch something she said that was not on the plan but it could be can you give us she asked for an implementation but yeah if you have anything to add for oblivious tienes maybe correct me if I\u0027m wrong but oblivious DNS has a slightly different adversary model right where the idea is that the recursive resolver does not there is no mapping between the client and the query and we are not looking at on path traffic okay but yeah we don\u0027t we haven\u0027t considered it yet so far very nice working again did you consider like to emulate the behavior of a National Security Agency or something like that the intelligent agency that you have a set of websites that you know they don\u0027t want you to go to and then so we try to fingerprint them will do whatever your method is and then you get a like domain names on various lists or entire DNS zones and try to see if an extreme like that you would be able to detect them like the techie users going to those allegedly forbidden web sites sorry just to clarify your question is whether we are going to analyze such a scenario or if your planner would be an interesting idea or have any thoughts if you would be able to detect that yes so as I mentioned in like our censorship related work like we have done a preliminary analysis at the moment but we are thinking of continuing in this direction and seeing how whether we can have a set of websites and if the user can visit those or not yeah thanks hi "
  },
  {
    "startTime": "00:51:45",
    "text": "riad Wahby nice nice work have you considered are there extra features that maybe like maybe you\u0027re leaving precision and recall on the table so for example what about inter packet arrival times yes so we did an initial feature analysis and we did consider into a packet arrival times at the moment at that time we didn\u0027t see that it it increased I mean it did increase the precision recall but not by much and usually we found that using timing as a feature can also be complicated because it also depends on which position the adversary is whether you know you\u0027re locating the adversary on router or know if you we\u0027re doing measurements right on the client or not so that\u0027s why we decided not to do the timing based features but our initial sort of analysis did show that including the timing can raise the precision and recall by a little bit all right so this is going to be I\u0027m gonna truncate this presentation because a lot of it overlaps with what was just presented thank you very much for giving everyone in the background necessary this is joint work we did with Nikita Boris offensive empathy Allah it was presented that a nrw workshop earlier this week and so just to get right into it as you may know and as was just described there so the recent shift there\u0027s a recent shift in focus on privacy in the IETF and ITF in general and particularly we\u0027re trying to protect what resources or what applications or what services you know particular clients are using and there\u0027s also that gets the commerce or the opposite side of that is what we\u0027re trying to protect who is accessing these particular resources and using these services and connections what not clearly the former is easier than the latter or rather the the former is harder than the latter because we have very distinguishing identifiers currently like IP addresses and other things that are in client software so that\u0027s what I\u0027m gonna focus on and the idea has been trying to do a lot of work to push in that direction in particular rolling out dough and dot and gos encrypted S\u0026I all these things are you know plugging the various holes that have come up over time or that have existed for a very long time and hope so you know eventually masking what is the client actually up to on the internet so in this particular model more concerned about the privacy of a particular connection that is used to request a particular resource we\u0027re generally assuming an adversary that\u0027s local and passive it can observe all packets between the client and the server and the goal is pretty simple it just wants to learn some information about that particular connection be it you know what resource was actually requested perhaps some metadata about "
  },
  {
    "startTime": "00:54:47",
    "text": "that resource referring back to in our duty paper just describing you know what HTTP method was used or sent over this particular connection and optionally they may also want to link this back to a particular client should also know that in the real world this is generally assumed to be studied in what we call the open world model which is where you know you you train potentially your classifier or whatever it is you\u0027re doing your initial preliminary assessment on on a fixed set of connections but what the actual clients do in the wild is sort of unpredictable and so you\u0027re not constrained to that what you train your you know initial experiments on so the closed world model which is generally considered to be much easier in this particular problem space and what we actually you know for this particular work studied of course gives better results for classification and identification but ultimately goal was to focus on the open world and as was presented earlier there are lots of different features available when you want to do this sort of you know identification particularly you can look at Network addresses you can look at packet timing in sizes and all these things or even the clear tax information that was previously available or is currently available depending on what client your software you\u0027re running in this work though the you know we\u0027re assuming that the things that should obviously be encrypted like DNS and sni are encrypted we\u0027re assuming that we\u0027re not you know the adversary is not looking at packet timing in sizes strictly looking at network addresses to try and figure out you know what particular website or what service a particular connection is trying to access and yeah so if you look at the sort of the spectrum of information available to you know an adversary wants to do this sort of fingerprinting back in the day before we had anything from before we had HTS and encrypted dns everything was kind of sent around in clear text it was very easy in her Kaduri for the adversary to just simply you know look at what the clients were doing and as things have sort of become more and more encrypted the features that are available sort of become a bit more difficult to act on so right now if you assume a world where we have encrypted s and I encrypted DNS you look at information available at the network layer so IP addresses you look at patterns in the traffic and the goal is to strictly identify our fingerprints a connection based just on that information and the idea this is to kind of show claim that it becomes increasingly difficult as you obviously take away the clear text information on the wire so the current state assuming you\u0027re running perhaps some older software and resolving or opening up a particular connection in HTS connection I have a client the middle who sends a clear text query to a DNS resolver gets back an answer in clear text obviously knows where they\u0027re going because they see both the query and the address everything\u0027s fine at least from the adversaries perspective and then he opens up TCP connection TOS connection "
  },
  {
    "startTime": "00:57:47",
    "text": "everything again is exchanged to clear and ultimately the resource that they\u0027re after is encrypted but depending on the adversary model again they\u0027ve already learned exactly what you know application the client is after so it could potentially be game over that particular point especially if the goal is to censor based on that you know that signal but if you add Dolph Dolph dot and dough into the mix and yes and I lots more things are encrypted but what remains currently or you know as postulate based on the current designs for these things are the ALP values that are still setting the clear in a TLS the IP address of the server to which you\u0027re connecting as well as all the other network features that were mentioned earlier that I\u0027m omitting so we\u0027re focused rickly on the network address again because that is the thesis is that the sufficiently unique identifying information for the connection so the experiments that were discussed in the paper basically worked as follows go grab a massive set of domains and write a crawler that goes and connects them identifies all the you know the top level the IP addresses that are resolved from trying or that are returned from trying to resolve the top level domains as well as the IP addresses from each live resource on a particular page do the resolution for all of those names to be at the top-level domain and all the subtrees URLs using CDNs collect all this data and then try to see you know how we unique these IP addresses are fairly straightforward for this particular experiment and if you look at the anonymity set that results from that particular experiment that the data kind of suggests that basically there\u0027s many what we call a unique IP addresses that those that fall into you know and not on a set of size like one or two and this is not you may think it\u0027s perhaps uncommon as the you know we move towards a a you know a world in which a of es since you see and everything hosts a lot of applications but still there are a lot of legacy or legacy there are older servers that you know run from behind a couch or have a unique IP address that simply looking at it can reveal exactly where you\u0027re going so that\u0027s not great but again I take this we\u0027re going to solve in this particular data is collected from closed world environments so it would become harder to identify this or do this set in the open world as we\u0027re saying earlier if you look at if you want to identify for example what is the actual page you know a client is attempting to access perhaps the logical thing would be to look at not just you know a single connection that\u0027s an initiated when loading that page but rather the set of connections that are initiated when voting that page and all of its sub resources so as the on path "
  },
  {
    "startTime": "01:00:50",
    "text": "adversary you see things like the DNS query patterns you see to us in TCP connection pattern and the set of these things are often should in theory be sufficient or more unique than each one on its own so if you were to you know as an example if you loaded New York Times comm and Safari you would see many many TLS connections kicked off many many DNS requests sent over dough or dot and that the union of that set is what we\u0027re using as the fingerprint for a particular page load so the privacy of a page load then considers the same exact adversary just assumes that the adversary has is able to bucket eyes or group you know these connections from a client into a single event and then use that to make a determination as to how unique a particular connection is and perhaps use that uniqueness to associate it with known top-level domains so the same features are available or just you know expanding the scope a little bit here and that pretty much describes or shows what I was describing so you go from a single connection to multiple connections and you look at sums instead of individuals very straightforward and same thing same thing so the simply experiments that were discussed in the paper they didn\u0027t focus on the very large IP address set or domain set that was used for the single connection IP address uh nominee experiment but rather just the top 1 million loaded them using their crawler and then compute some basic statistics for example how many unique URLs are you know reference upon loading each individual page you know how many different domains is that kick-off underneath the hood to see how many connections you\u0027re making and the results of you know doing that looking at the number of unique IP addresses or the number of unique page little fingerprints that came out of it it\u0027s basically here so you can see if you look at the x axis the anonymous head size significant significantly shifts to the left basically suggesting that by looking at the some of these connections and grouping them two individual payloads the anomaly set or the uniqueness goes up which matches our intuition again closed world open world so still could be improved so the the conclusions we kind of draw from this very very you know preliminary research is still ongoing as that clearly we need some sort of encryption of obviously clear text holes to get some sort of notion of connection privacy and we need some sort of notion of connection privacy to get some notion of page book privacy and I think perhaps that\u0027s like the ultimate goal of a lot of these things I mean encrypted sni and doand dot are great in that they\u0027re "
  },
  {
    "startTime": "01:03:50",
    "text": "focusing specifically on the connection privacy but you know perhaps there\u0027s more that could be done to get towards the the larger bigger picture that we\u0027re trying to protect there are a lot of related issues here and things that potentially not considered in this experiment and particularly equity of have a client that\u0027s doing happy eyeballs to race connections across address families or even across interfaces that might be worse in some ways because you\u0027re simply giving the network more information about where you\u0027re trying to go so it is great for performance as clearly demonstrated by all the clients that are implementing it and the benefits that it brings however from this particular spective it might make things a little bit easier for the adversary which is not necessarily great over on the plus side the things that we\u0027re doing in the HP biz to coalesce connections with secondary certs it\u0027s great because that potentially shoves more requests along a single connection and makes it\u0027s effectively removing information from pages of fingerprints that would have otherwise you know spun up new connections and perhaps add to the amount of uniqueness that exists for a particular patient fingerprint consolidation within a single CDN as well also helps because you have single connections that you know basically clients are tethering to the CDN and then sending all their connections and all their requests over it you can also do things like deploy proxies that hide the IP address and you know that that solves a lot of issues in perhaps let\u0027s see even the most obvious way forward and then there\u0027s also things that have sort of heard us especially with respect to yes and I but in this case may help and then it\u0027s deenis load balancers that are you know constantly changing the IP address that you land on so making it not easy to you know just identify a particular service by a specific IP address I mean you could do fancy things by potentially trying to identify the ASN that to which that IP address belongs and then associating you know you know the IP addresses of particular connections with asons and then looking at the Union or the set of asons that result from a particular connection and using that to identify a PLF but that has not been done yet so perhaps that\u0027s an interesting or you know useful way forward so I guess in general the the the purpose of this work was the shorter show that the the intuitive I guess it\u0027s a rather intuitive observation that yes taking away clear text information from the network is a good thing because it removes obviously the very clear privacy holes on the network but there are still things that need to be fixed but this particular problem website fingerprinting based solely on address information seems to be coming harder based on the work that the ITF is actively doing which is a good sign I think "
  },
  {
    "startTime": "01:06:50",
    "text": "but then writing back to the previous presentation it\u0027s unclear what that means for the the other side or the other information that the adversary has available to them for example so they shift their focus towards traffic analysis in which they are using information such as timing inter inter arrival time the sizes of packets that are sent to sort of infer what that what the client is doing on the network and this is something that tor and the academic research communities have been struggling with for a very long time and continue struggling with website fingerprinting just has a number of different papers some of which we\u0027ve tried to collate along with our in collaboration with Ian Goldberg sort of a repository of you know all the known attacks in this particular area looking at how bad traffic analysis is for you known on Thor connections of course much of the research is done in tour but it applies equally as well if not more to you know non Thor connections which don\u0027t do things like fix cell padding to mitigate obvious or obvious or to make traffic analysis just a little bit harder so I think next steps for this are to really kind of encourage people to take this problem a bit more seriously if they\u0027re not already and that means like asking people to do more research in this area such I mean Nikita and others are obviously doing it we should for them to keep going I think documenting to the gnome research that\u0027s been done in the area is also quite useful if you know the reason to having for having it you know a single reference that we can use to you know either assess or assess countermeasures or you know sanity check to make sure you know I just don\u0027t learn whether or not you know something that hasn\u0027t been done actually indeed has been done and then you know perhaps IV III RTF or the ITF can like work with these people who are actively working on these problems to develop mitigations to see you know what\u0027s effective and from a you know cost performance perspective so for example the previous presentation like here we got good results from sending dns over tor but it\u0027s you know there\u0027s a performance hit there so what is finding that right trade-off is difficult and so perhaps something we should implore IETF and I urge you have to be working on so that\u0027s pretty much it happy to take any questions questions hi Chris Dave Wonka Akamai thanks for presenting this and presenting something different about it then what was adding in our W I was afraid this game using things I\u0027m talking the couple of things I wanted to mention and you can figure out if they\u0027re questions or just comments I\u0027ve been working on anonymity sets on the client side instead of the server side so that we can report on activity in the web based on clients and what it struck me from what you\u0027ve shown here and what the page load combinations show is "
  },
  {
    "startTime": "01:09:50",
    "text": "clearly the solution for creating anonymity sets based on aggregation on clients that are very different than the server-side so it\u0027s completely complimentary to what we\u0027re doing in map Archy and then this last hackathon i was writing code to see and i notice oh they\u0027re so and we\u0027re going to talk about that on friday mornings so you know whoever wants to work on this let\u0027s get to talking about that and then well I\u0027ll stop because I will be there from frg but I\u0027ll sync up with you and we can talk about it perhaps afterwards yeah hey Chris we had um just a quick thing can you go back to the second graph that you showed us with the buckets so from this it looks like anonymity set of two is two orders of magnitude smaller than an amenity set of one so if we sum up everything that means that like 95% of web sites can be identified yes it was a very high number two quick questions you mentioned load balancing in Aires were you saying that load balancers our anniversary just no no just that dns-based boat balancers might give you different IP addresses if you try to resolve the same name over and over again so a simple you know and naive approach to trying to map name to IP address and looking at the first result that comes back might not always work because the answer will change on Nick Sullivan so Chris what about the children no no no but seriously this is dealing with passive adversary\u0027s have you thought of adversaries that are more actively trying to subvert such types of anonymity protections no not yet I\u0027m hopeful that Nikita and the group that\u0027s working on it can you know start taking that a little bit more seriously yeah sorry sorry we can talk offline if you wish and and the meaningless also thank you thanks all right my name is Rowan fresh-picked I work for an omlette labs this is joint work with high scanners who was my master student mad-eye\u0027s one loaf and Luca Elodie from historian and Technical University of Eindhoven respectively right so it feels like we\u0027re repeating ourselves here because I think somebody had a similar slide and the person before me had a similar slide and the IETF is focusing on protecting people\u0027s privacy which is great we have things like deprive we have DNS over TLS we have a lot of buzz around toh but the focus of all of this is mostly on privacy of traffic in flight right so we\u0027re trying to protect traffic from adversaries that are on the network but if you look at the domain name system there is a sort of an obvious elephant in the room which is the operator of the resolver that even if you protect all the traffic in flight still has access to your traffic and then they might "
  },
  {
    "startTime": "01:12:51",
    "text": "actually have legitimate reasons for doing so right if you are in enterprise network you may want to use DNS to detect indicators of compromised people that are infected with malware on your network or you want to be able to monitor our threats in large user bases for example court 9 does that they have a very strict privacy policy but they still inspect traffic in order to detect malicious behavior in in larger populations that user resolver so it is probably too easy to say they shouldn\u0027t do this but what we wondered is can we find a better way of doing that that sort of provides some privacy guarantees for users when we\u0027re inspecting traffic on the result for itself so what we did is my master student developed a potential solution for this which uses something called that is called bloom filters which some of you may be familiar with but I\u0027ll explain it in the next couple of slides more importantly we have a working prototype which is open source Oh URL is on one of the last slides if you want to have a play with it it\u0027s very much a prototype but it gives you some idea what it does and we tested this in production at surf that\u0027s the national research and education network in the Netherlands on resolvers that have a client base of roughly 200 to 250 thousand users right so first up bloom filters so bloom portals really were developed in the 1970s just basically there are a method to speed up database lookups and they are a highly efficient mechanism so insertion and look up is is roughly order one and basically you can think of a bloom filter as a probabilistic set so if you have an element in that has a may or may not have been inserted into the filter then you and and if you query a filter then if the the filter says it\u0027s not in there and it\u0027s guaranteed not to be a member of the of the set if it says yes this is a member of the of the filter then there is a small probability that this is a false positive but it\u0027s highly likely that that particular element is in the set so how does this work you take for example a domain name you run that through a set of hash functions then you get some output this is actually the sha-256 hash of that particular domain name you use each of the you use parts of the hash or the different hash functions from multiple hash functions as indices in a bit array and then you flip the bits to one if at a particular index so on the left hand side you see insertions so we insert example a common example or here as an example this is a toy example Donnie eight bits in the weight set and then if you do a lookup you go through the same process and if there is at least one bit set to zero then the element that you\u0027re "
  },
  {
    "startTime": "01:15:51",
    "text": "looking for is guaranteed not to be in the filter and if they of all of them are set to one then it is either in the filter or it was a false positive right so with a bloom filter you have basically have a trade-off between the false positive rate and a reasonable filter size so you don\u0027t want to have huge filters huge bits as you need to keep in memory but you also want to keep your false positive rate relatively low and the parameters that you use for that is the number of hash functions that you use or rather the number of indices that you extract from the hash to sort of set flip bits in the filter the size of the bit array and the expected number of elements that you expect to insert into the set and then the formula on the slide gives you a rough estimation of the false positive probability now the idea that we had was that you what you could do is to take all of the queries from your clients and insert T\u0027s into a bloom filter and this is actually interesting this is a methodology that\u0027s already used to find things like new you observe domains for example I think power DNS has an implementation that does that but what we wanted to do was use this information to check if a name was queried for but we don\u0027t were not interested in by whom that name was queried for and exactly when so what we want to do with this is to perform network level threat monitoring so we won\u0027t say that we have a domain name that we know is malicious we want to be able to say within a certain frame time frame did anybody ever query that name in our network and the bloom filters give you some nice properties with that because there are non enumerable as soon as I\u0027ve inserted something I have no clue what I insert anymore because it\u0027s turned into some a few random bits that I flip in a filter if I makes queries for lots of users into a single filter it becomes really hard to sort of distinguish queries from individuals and what is also interesting is that due to the mathematical properties of bloom filters we can actually take multiple filters and combine them into a single filter which has a higher false positive probability but contains more data and thus anonymizes even more so what are the prototype that my student develop does is that it has sort of an auto tuning mode that you can run it in against your resolver for say a few days to see where your curry pattern looks like and then it will suggest bloom filter parameters that you can set in order to have a certain false positive probability so we for example want to do hourly filters in our experiment and we want to be able to aggregate these two single day so we want to combine 24-hour lis filters into a single filter for a day when we do aggregation and we would like to have a false positive probability probability of 100,000 for the daily filters and what the graph shows you is that it\u0027s important to do the tuning because if "
  },
  {
    "startTime": "01:18:51",
    "text": "you look at the number of distinct query names that you see in a day on a resolver then on an hourly basis is that it\u0027s the red line so that\u0027s anywhere between so it\u0027s roughly 4 million query distinct names within an hour that we see but if you look at this on a daily scale the number becomes much higher and you see roughly 30 million distinct names in a single day now the future extension that we want to do is that it keeps automatically tuning so what it will do is it will ensure that all of the filters for a single Bay in Bay can be aggregated but it will use the predictions from the previous day to set up the filters for the next day so that you can you don\u0027t have to come to constantly manage the system so what do we put in there what we wanted to be able to do so surfing it is a large research network it has many universities connected and has schools connected to it to this and what we wanted to do was to be able to distinguish queries from different institutions but not from different users so basically the the things that we insert in the in the filter on this slide so if we have say evil domain Lacombe then we will insert both the second-level all of the labels but also organization a at evil domain a comb for the specific network that that query came from such that and this means that if we want to say we we get an indicator of compromise we want to work out if that indicator of compromise was ever seen on the network we can put it in a bloom filter in hotels if it was seen and then we can sort of enumerate over all of the institutions that are in there and figure out which institutions sends us that query now we test this prototype for about three weeks on a busy DNS resolver this shows that the query pattern in queries per second for that particular resolver and we studied three use cases I\u0027m not going to go over all of them because that would take too much time I will refer you to the paper there\u0027s a link on the on the last slide to the paper but what we looked at was detection of so-called booters earlier ddos as a service website we looked at hits on email blacklist and the one that i\u0027m going to talk about is the the national detection network first a little bit about the predicted versus the actual false positive rate for the filters so we ran Auto tuning for a week before we did our experiment and we chose as filter parameters that we would set 10 indices for every query that we get and the bid filter size was for in a 91 megabytes as it\u0027s roughly fifty nine megabytes in memory so it\u0027s actually quite reasonable if you compare the resolver cache on that machine which was about two gigs so our goal was to keep the daily false positive rate below one in a thousand and of course we had to estimate the number of elements that we inserted a little bit of hand-waving but after you\u0027ve used a filter you can calculate the actual false positive probability which is that the formula is given there it\u0027s explained in in the "
  },
  {
    "startTime": "01:21:52",
    "text": "paper and fertilize me I can\u0027t remember what s is but the graphs are show you the result the black line is ten ten to the minus three so that\u0027s one in a thousand so anything above so that the graphs are a bit confusing anything above the red line means we had a lower false positive probability than we actually said so the takeaway from this is that we actually had a very good false positive rate on the bloom filters and if you look at the hourly because we use the same parameters for the hourly and the daily filters otherwise we can\u0027t aggregate them then of course at an early level the probability for false positive is very very low now one of the things that we tested this with is the National detection Network and this is something that is managed by our government a Dutch national cybersecurity Center and what they have is they have a system that runs a Miss so it\u0027s a malware information service platform I think is the acronym and what they put in there are high value indicators of compromise for example indicators of compromised from the intelligence services now in the condition for participating in this national detection network is that you don\u0027t just take data from it but you also put data back into it so say you get an indicator that there is some some malware active then what they want to know is how does this affect your community because their goal is to figure out how that society is impacted by by these threats right and of course surf net wanted to participate in that but they didn\u0027t want to monitor all of their individual users they want to didn\u0027t want to throw away the privacy of all users in order to participate in something like this so this particular solution that we implemented was very interesting for them because it allowed them to take the indicators of compromised hold them against the bloom filters and figure out if there are hits on that and they could report it back to the national detection Network but they also got some indicators of sort of what threats there were in the network the graph shows you the number of threats that occurred on a daily basis so it\u0027s not a huge number of threats in the order 40 to 50 in a unique threats per day but the interesting thing about this is that surf nets privacy policy prevented them from monitoring individual curries so they couldn\u0027t do this before and now that they had the bloom filter solution they could do these lookups and actually work out whether these threats were occurring in the network and we found an actual compromise which was a one of wanna cry infected machine so what we did is as soon as we found out that one of the threads that was detected was one a cry what we did was go to surface privacy offer and say hey we found his thread that maybe needs mitigation can we now specifically monitor for this threat so rather than doing blanket surveillance we can look for a specific threat and then see who is infected and chase up that machine so this is much less invasive for users than just sort of monitoring all of "
  },
  {
    "startTime": "01:24:52",
    "text": "their traffic and just telling them no we\u0027re doing this for your good now in this case we can make a balanced decision whether or not to monitor for something or not so some other benefits no personal data is stored of course because we don\u0027t retain any IP addresses this is aggregated at the individual now level but this means that you can retain this data much longer which allows you to do historical lookups which is very interesting we think think back to for example the one a cry case where you could recognize that this strength was present or your network because he would do certain DNS queries if we had had this running at the time sort of before that threat existed as soon as the malware researchers discovered that particular query we could have gone back in time and worked out when is the first time that we actually saw infections which is something we can\u0027t do right now and that would be very valuable for threat intelligence another thing is that and this is something I as a researcher find very interesting is that you could share this data with third parties without disclosing PII too to them and then they could for example say you take filters that are collected in in different networks then you can do co-occurrence of queries if say I found this query that that I think is malicious which other networks did I see that in and was that roughly at the same time and the final thing is that as a nice side effect of how bloom filters work they you can do cardinality estimates or the number of distinct queries that are in there you can estimate that because it\u0027s sort of related to how hyper log lock works if you\u0027re familiar with that right so the prototype code has been released as open source the URLs on the slide so surf net where we trial this is planning to take this into production because they want to use this for their C cert team and adenylate labs we\u0027re creating the tools to integrate this into our open source products in particular our resolver product unbound and the goal of that is to again release this is open source software and make this really easy to deploy so that if you want to do this kind of network monitoring then at least you now have a more privacy friendly solution available than just blanket recording every query that all of your users make and I hope that it\u0027s a little bit of proof that security and privacy can go hand-in-hand and with that this is the paper the link is on the slide and the slides are up in the data tracker if you want to read the paper are there any questions hi Roland Dave Wonka super cool work I can\u0027t wait to read the paper I\u0027m the question I have is about the sharing are you comfortable giving out your full bloom filter to the NCSC or I can\u0027t believe they would give you their bloom filter back how does this sharing really okay so for the for an initial detection "
  },
  {
    "startTime": "01:27:52",
    "text": "network we\u0027re certain that they were serpent is certainly not sharing the whole bloom filter with NCC what they are sharing with ncsc is if there are the detections that they do themselves so say I have a threat I detect that and we report back we saw this threat in universities or in I don\u0027t know schools for vocational education but of course it\u0027s not the bloom filter they\u0027re sharing that what they\u0027re sharing is whether or not a threat was detected but don\u0027t you have to take the consonance the bloom filters to find out if the 1:1 burglar had the the vulnerabilities are the the text that were representing the other I\u0027m not understanding sorry okay I can\u0027t hear you can you step closer to mine oh I was just saying don\u0027t you it\u0027s the coincidence the bloom filters that shows that you that there\u0027s evidence of those attacks or vulnerabilities in the in one set yeah but so who\u0027s the one that\u0027s finding the intersections of bloom filters because that\u0027s on your trusting the so I think they\u0027re so you think you\u0027re you\u0027re sort of conflating two things the idea is that the network operator in this case surf net who participates in the national detection network thus detections on their own on their own bloom filters that they record on their resolvers and they just report infections back to the cybersecurity ncsc they don\u0027t sort of send the bloom filters they\u0027re what you could do a distributed model is that you could have a query API where somebody could send in say I has this name being queried is this name presence in your bloom filter something you do to look up and you report yes or no I agree with you that if you have the remote query that\u0027s a way that you could possibly hear but if you give the bloom filter then they can query yeah they can query I thought yeah Crick but so I wouldn\u0027t advocate giving it to some something like NCSE but for example say you have a University researcher or is interested in this and I can I can definitely see conditions under which you could share that data with them hi STP from the NCSC in the UK and thank you for your presentation I found it really interesting and I think it\u0027s a good piece of research that needs to happen in this space and and like you said it\u0027s nice to see that security and privacy can\u0027t go together and I just had a question on your previous slide I think maybe the wonderful about oh maybe it was a couple of very sorry yeah about sharing the bloom filter with third parties so in this case and I would just be concerned about the potential leg-up that you would give a threat actor in knowing that their demeanor whatever was noted you could just change their techniques so and threat intelligences shared under the TLP protocol at moon um and yeah just to say when you share it with researchers I worry about giving it to any academic that you know fancy don\u0027t I so okay so - so of course when I say sharing a third parties what I would say it should have a little asterisk that says under certain conditions right I am an academic researcher so I don\u0027t worry so much about sharing this with academic researchers I mean trust me but "
  },
  {
    "startTime": "01:30:55",
    "text": "you do have a you do have a point right you want to have certain safeguards in place before you share this kind of information I guess it also depends on the network that you collect this information in surfing is a research network so there there is a one of the goals is also to do research on the network itself so if you share that with researchers that are within say their constituency then that could be there could be good conditions for doing that actually surf and has a data sharing policy that sort of lists conditions under which this kind of data can be shared with researchers and then there are data sets that can be shared with researchers within their constituency or random academic researchers around the world and I can\u0027t speak for them but I would say that this is data that they would definitely feel comfortable sharing if the conditions are right because I talk to their privacy officer and she actually she understood this really well quite well even though she didn\u0027t have a technical background this was sort of easy to understand and her intuition was that for example this would not be subject to the GDP art because of the data that gets put in and that meant that under certain conditions we could share the information even though there are some privacy risk if you can just try and send whatever question you have to the filter and figure out if something\u0027s in there does that sort of answer your question sort of okay any other questions okay thank you return oh thanks to all our presenters can be a round of applause do we have the blue sheets has anyone not sign the blue sheets should be moving on to the internet drafts it is Roland sorry it\u0027s Fernando Marlene idea it\u0027s really low is that better now yes okay so I\u0027m for another one and I will be presenting a set of three documents to our target at beard these documents are about the security and privacy implications of numeric identifiers employed in network particles these are a set of documents that we have been working on for a few years now with the ban are safe "
  },
  {
    "startTime": "01:33:55",
    "text": "next slide please so as an introduction essentially for the last 30 years or so we have got the topic of numeric identifiers wrong in in many protocol specifications and implementations after memory break now you may recall for example you may remember issues that we have faced with PCP initial seconds numbers predictable transport protocol numbers predictable ipv4 and ipv6 fragment identifiers predictable ipv6 interface identifiers remember your audience really scrambled you can\u0027t make how anything ok yeah I think he does a little video I did yeah I don\u0027t know if I can disable good try again socially lessons that we learned we\u0027ve you know we get into fires of some protocols essentially we\u0027re not applied to all the particles so it\u0027s essentially the same problem that repeated and over again in different lamentations and in different protocol specifications next slide so essentially we work on a set of three documents these three documents used to be a single large document and based on on the advice on a number of groups we split this document into three the first document is well numeric IDs a history in which what we try to do is cover the timeline of some some old numeric identifiers this is this document targets peer second is about proposing algorithms to you know to generate these numeric identifiers you know without or avoiding the security and privacy implications and the third document is a document that we hope can be published as an ad sponsored RFC which essentially tries to force protocol specification writers to have to do a proper analysis about numeric identifiers next slide so the first document numeric ID is history essentially what we do we cover some sample numeric identifiers ipv6 interface identifiers ipv4 and ipv6 fragment identifiers and so on and what we tried to do with this work is "
  },
  {
    "startTime": "01:36:56",
    "text": "essentially to illustrate you know how essentially we faced the same problem over and over again sometimes called the same identifiers in different protocols like prime identifiers in ipv4 and ipv6 or sometimes it\u0027s the same problem at the end of the day but for different protocols this document again is targets fear next slide please the second document it\u0027s a little bit more complex essentially what we try to do is to categorize numeric identifiers based on their interoperability requirements and the failure mode interpretability requirements ell requirements that you know must become quite - based on the protocol there are different sort of requirements like uniqueness an identifier that must be monotonically increasing and so on and the failure mode is essentially well what happens if you don\u0027t comply with those requirements so based on those categories what we did is you know try to go through most of the numeric identifiers that we are aware about and try to see if we can make each of them follow into one of the categories that we define and for each of those categories what we do is prove proposed some sample algorithms that can comply with them to repairability requirements while avoiding the negative security and privacy implications or some at the previous document please document target fear peace group next mani your audio is giving way again next slide I think given the lack of time we\u0027re just going to move on to the next school with the slight No and I think this draft the third draft which is I think it\u0027s under consideration for ad sponsorship by Ben kaduk and yeah and we spoke to the we spoke to the security ad Benedict and also the author of the draft the to draft and seem like the first to draft history and generation who were under scope for and good benefit from coming to perigee and we were thinking of adopting them so do people have opinions on this and we\u0027re looking to alum at the end to to get a sense of whether the room is okay with us is there there\u0027s consensus to adopt these two these two documents so if you have comments please come and speak my "
  },
  {
    "startTime": "01:39:57",
    "text": "people read these documents show of hands if you okay does anyone have opposition to the research group adopting these documents what\u0027s the research content again what\u0027s the research content I believe you three integration of the two drafts sorry so one of them is just the survey basically evolved the the past sure problems I\u0027ve read the same I don\u0027t know what the research yeah actually yeah of the generation hopefully or another clean answer so I let him into the queue Fernando yeah sorry the connection was lost was the question that was answered neither can you answer in chapter yeah I guess in the interest of time and do the federal abilities we\u0027ll just move on to the next presentation and come back to this quality and time permitting of course we can always take is the list afterwards so on that note David other I am David offer with the Guardian project and talk about an internet draft we\u0027ve submitted recently on enabling network traffic obfuscation via pluggable transports so first of all what is what are pluggable transports mechanism for enabling the rapid development and deployment of network obfuscation techniques used to circumvent surveillance and censorship the deep tape details about this work which has been going on for some now are available at this URL the generalized architecture here is that there is a server that\u0027s exposing a public proxy that accepts connections from pluggable transport clients the client transforms traffic before it hits the public Internet the PT server reverses the transformation and then passes the traffic onto the server app there\u0027s also an optional lightweight protocol to facilitate communicating connection metadata so when you\u0027re migrating between one connection type and another for example so since we don\u0027t want to limit the behaviors of "
  },
  {
    "startTime": "01:42:59",
    "text": "what pluggable transports can do this this idea focuses on the interface between the technologies and the applications themselves not on what goes on inside of of an obfuscation technique the draft that we\u0027ve put together is based on the pluggable transports 2.1 specification which is a work of a fairly large and diverse community of people and it has two subsets one is what we call the transport api interface and the other the dispatcher interface so one that is focused around in-progress language-specific api that\u0027s integrated directly into the client app on the client side and into the server app on the other side and communication within the app or the way the app on both sides sees the pluggable transport is like a socket with the dispatcher API this is to be used between processes so there\u0027s another process on each side that handles the process of obscuring and unobscured and passes that so that the actual application doesn\u0027t have to deal with that aspect of the problem the dispatcher here can be configured with different types of proxies so you can have live at any moment different kinds of of transports available that can that are connected in different kinds of proxies and can respond to different kinds of traffic so this is sort of that architecture described in graphic form here\u0027s the Transport API where what\u0027s going on in the track in the transports actually bundled into the application the itself and here\u0027s the case with the dispatcher where the actual applications don\u0027t take part in or don\u0027t got compiled in the kind of application technique being used and here\u0027s a case where there\u0027s a mixed architecture where maybe the client uses a dispatcher type environment where the server has a transport style environment the spec also talks or the internet draft also talks about where we can by looking at the older specification which was just the dispatcher API and the issues related to linking across language and then also this idea of adapting back and forth between types of adapt types of technique and I think that\u0027s it questions Ben Schwartz what would you like to see happen with this document well yeah I guess I can give maybe my own personal response to this because I\u0027m not sure "
  },
  {
    "startTime": "01:45:59",
    "text": "overall myself but there\u0027s a been a lot of work going on in this area and it is in my mind sort of in occurs in user space of a necessity because we want these techniques to be adapted in a very rapid way so it\u0027s unclear to me what or it needs to be debated what sort of final position this has in some standardization work should some of this stuff take place a lower in the network stack or not and and if so why and if not why etcetera but we are we do hope this work took place for a limited period of time only within sort of one small set of the community and now we have a larger community that finds this interesting so there seems to be need to be some bridge between some long term standard and some just lack of unity on the topic Thanks how many people have read this decade only two thanks for your question then Ben I\u0027m one of the authors of course so yeah I think this certainly need I guess more reviewing reading from people here if you find it interesting you know please same comment so a list we can kind of decide what to do at that point it\u0027s certainly a very interesting work but it might be a bit too really at this time so let\u0027s let\u0027s get more feedback not at first thank you Joe staying here hey so I think I have three slides but the first time I represent of this was in November of 2014 in Hawaii you may or may not remember it it\u0027s been edited a lot it\u0027s been refractor refactored reflected network structure it\u0027s a created author as we had Nick Fenster and some students at Princeton and helped with it adds more modern things like blocking of ES and I in South Korea and stuff like that the reason it hasn\u0027t gone very far is because I\u0027m the one working on it and I diverted my efforts which is I don\u0027t have a lot of time for nhf work to the is 2.0 restructuring work which is recently done yay so I\u0027m doing a lot more stuff that doesn\u0027t involve administrative stuff just a real quick overview ten seconds it orders the discussion about censorship around three things prescription that is what do you want to block identification how do you technically identify those things that you\u0027d like to block and then interference the actual performance of the blocking there\u0027s a ton of small and medium issues that a bunch of people have identified in the reviews that are in our issue tracker and we\u0027re gonna be going over those in the next couple of months and incorporating all that good "
  },
  {
    "startTime": "01:49:00",
    "text": "feedback and since this is a possibly a research group draft we\u0027re gonna actually try and do some outreach to areas that haven\u0027t gotten much review of this so routing and DNS in IETF just to excerpt the chunk of it that\u0027s there and see if we get additional feedback there although some people like Stefan a long time ago reviewed an earlier draft of this the bigger issues that have been talked about on the list there\u0027s this thing about mitigations I was initially pretty just to state the problem some people feel that it would be a better draft of it actually in in addition to describing censorship techniques and included discussions of mitigations that may or may not be relevant for each technique I was initial initially pretty reluctant to do that just because it felt like it would blow up the draft but I I\u0027m I should say I can go either way on this in the sense that I think pithy statements under each technique that describe some types of mitigations may work that I think the trick is is that censorship techniques change quite a bit mitigations changed in my totally just ad-hoc or an intuitive nature they change it even more quickly some things like domain fronting you know come and go right I forgotten his first name I should know in Vitoria Victorio Burt Ola said specifically that censorship may be too negative of a framing I come from a digital rights background where we talk about when someone\u0027s blocking something you want to get access to that censored no matter what it is happy to say blocking techniques do you want to talk now I\u0027m almost done there\u0027s a chunk about non-technical forms of prescription and interference so ways of finding stuff and blocking them like self-censorship or legal mechanisms and stuff like that I\u0027m not wedded to that stuff it just felt like it would make the document sort of complete and it\u0027s sort of the rubber hose thing that might come into play in certain kinds of things and it\u0027s not like some rough description of what that rubber hose might look like there\u0027s the bigger issue of the stuff moves really quick it may need to be a regularly updated draft I\u0027m not I know that that\u0027s a different discussion in other places some people don\u0027t care about that some people feel very strongly about that it may be something we want to wait for the living standard discussion what\u0027s the research content it\u0027s a sort of you can think of it as a review article a systemization of knowledge there are some gaps in certain areas that we\u0027re working to fix and then one other thing I wanted to say that\u0027s escaping my mind oh no I\u0027ll stop her for scrolling so first of all okay you should call it s okay one rather than art see something cool more seriously and the question we should be able to being you know updated draft I guess that\u0027s a question for you is how often do things change if things change you know on the scale every two years then like just you know publish this one another one a couple years they change every two months then probably that\u0027s awesome so I think it\u0027s all a question for you better rather than me "
  },
  {
    "startTime": "01:52:00",
    "text": "it\u0027s not a constant rate of change yeah yeah sure on the Implanon this non-technical thing the sensors like the framing thing you know like the reasons like a bad name is because like people don\u0027t like censorship and if you call it there BBT people will soon grow like WPT Muslim give advert only new deputies so like no I think you should citizenship yeah and what I thought was if you change the title and everything throughout to a worldwide survey worldwide blocking techniques how does it make me feel too bad I can live with that but I mean it just seems like these be great Malory noodle article 18 since we\u0027re solving all the bigger issues on the slide right now I think for the first one maybe this is a question for you Siobhan is I mean it seems like in the Charter you are open to talking about threats not just mitigations and I do think that censorship and privacy are two sides of the same coin like so I don\u0027t know maybe we can talk about that more because I feel like it would fit within the Charter I was just rereading it before I came up to the bank agree that was more for sedition Minister sorry that\u0027s right email sucks right when you Seltzer thing thanks for this work I think the non-technical forms is a valuable addition because we\u0027re always thinking about where is something going to get routed if it the technical means aren\u0027t available and thinking about that sort of broad piece of the threat model is helpful to thinking how effective are anti censorship mechanisms thank you Stefan Stefan BOTS Maya regarding the first point yeah the fact that the draft is still not published as a rare sea so that it\u0027s complicated to keep this sort of thing up-to-date and I vote aye against including mitigations because a big problem is not only that they evolve very fast but also that can have consequences a bad advice can be really harmful for people that can for instance to show that we try to walk around censorship thing like that so it\u0027s much more touchy so I suggest to stay with rights only on of course to call censorship sense or because otherwise what people who censor don\u0027t want to be consensus but that\u0027s what they also just use Ted in waltz I hadn\u0027t thought about that if I if we put a mitigation here that someone uses then they end up in jail man I\u0027m gonna feel like is that a question oh if you\u0027re still looking for inputs on absolutely for the first one I I\u0027m not sure how much beyond RFC six nine seven three will be able to contribute even if "
  },
  {
    "startTime": "01:55:01",
    "text": "we go down the path of including mitigation so that\u0027s I mean personally I\u0027m fine with the content says they are on the second one I I don\u0027t think there\u0027s a need to change it okay yeah but maybe the title should reflect the scope which is that not this is not on online sensation it\u0027s just Internet traffic and website sensation so maybe that just fix it that\u0027s a good point our people I am the JavaScript so and I read producer whatever I forgot thanks are people opposed to adopting the draft as is does even have any comments on that okay should shoot yeah and have people read it see if your hands we\u0027re going to do a hum on adoption of the draft as is so if you\u0027re in favor of adopting the draft please hum now if you\u0027re opposed to drop in the draft please um down yeah so it sounds like you\u0027re positive support for so will confirm on the list and go from there thank you Joe just to be good the question of like how frequently the document will be updated as you said does you know kind of bleed into the living document issue so we\u0027ll discuss with column what is you know a good strategy for dealing with that should that option happen but for now the normal thing will happen I guess right so the this brings us I guess basically to the end we had to cut Fernando off earlier because of technical issues were you gonna say something from him okay yeah please this was a response to echo this question of you which is on what were the research questions ended up and Fernando said identifying the reasons for Florida IDs categorizing them based on interoperability requirements and failure modes and producing algorithms to address such requirements yes we can take that question also that question would option for that also to the list then yes that\u0027s good people would please read the drafts if you\u0027re interested and and yeah well ask whether or not people interested in adopting on the list and go from there make at this point perhaps given the few number of people that I\u0027ve actually read at doing a hum here is not the best thing so we\u0027ll just take that one to the list and with that message anything else we can end a few minutes "
  },
  {
    "startTime": "01:58:01",
    "text": "early thanks everyone Thanks we\u0027re at the blue sheet [Music] "
  }
]