[
  {
    "startTime": "00:00:04",
    "text": "even if it's not virtual i i missed the big red button the chairs had in person and it is uh now 15 30 i see so i think we can start so this is the rmcat session and i'm anna bristow and i'm here with colin and we are perkins and we are the co-chairs and let's see if i managed um so this is the note well i think we are on friday so you're probably all familiar with it uh by now if not uh uh do read it of course because it does apply to the meeting and to start with some administrative so the meeting is being recorded i think you are aware of this by now as well we do need a note taker for the meeting do we have someone that can help out taking notes now i don't see you guys it's harder to look at someone suitable so gory you are in the queue gory was that you volunteering to take notes yeah okay i can't say i cannot do it so"
  },
  {
    "startTime": "00:02:01",
    "text": "i will enjoy doing it i'll just bring that up and take notes um in the hedgehog thank you and did you want to add something else or no this was why you you left the queue now so um great so i think we don't need a separate jabber scribe because we have the jabber now integrated in mythical so we will uh keep an eye on that so let's see so uh as uh colleen hinted that we have not uh met in a while so this is the agenda we have for the day we're gonna have an update on our algorithm so we met last time for a meeting at itaf 106 and at this point we decided we had delivered our algorithms and most of our documents and we decided that we would go idle for a while to try and gain experiences with the algorithms to then come back and see how we would uh proceed after that and at that point we were thinking to go idle for about a year but then of course the pandemic arrived and we were hoping to also be able to meet in person but now of course it's high time to come back and see what has happened to the algorithms and what do we have left as work in the the working group so after we finish this introduction part we will move to looking at the update on the status of the rmcap algorithms so first an overview of it from the shares and then we will have ingmar presenting uh some updates with the scream algorithm from rfc 8298 and we have david presenting some updates on the shared"
  },
  {
    "startTime": "00:04:01",
    "text": "bottleneck detection from rfc 8382 and then we have one active working group document that has been left behind on rtcp feedback for congestion control and colin will give us the the update on that and then we will see where we are on the different activities uh how to wrap up or what we have left to to work on so to start with a recap of the working group activities we have had four algorithms that we have submitted as experimental rfcs and they have been now all published they have been in in various queues uh some of them waiting for each other but we have the the scream algorithm and the nada algorithms the two congestion controls and then we have the the coupled congestion control and the shared bottleneck detection and the last of those the nada algorithm was was published since we last after our last physical meeting and the couple congestion control had the dependency there so it was also published after we last met but they were all submitted at this point in time but now they're also all rfcs then we have had a set of requirements and evaluation documents and the requirements was actually the first document that was sent for publication from the group but that was in the in the queue for a long time because of the the webrtc bundle and then most of the evaluation documents had the dependence on the requirements so uh three of these four of these documents were all now published in january of this"
  },
  {
    "startTime": "00:06:01",
    "text": "year uh when this whole sequence of documents were completed and all published and we already had the video traffic models uh published uh before and then we have one active working group document on the feedback side and this is the draft that colin will talk about and when we last met this was waiting for the uh rtp control protocol feedback for congestion control the feedback message that was handled at that time by the avt core so that is now published in rfc 8888 and so we should now finish up this this uh document that was depending on it so we have that on the agenda so this was a was a recap of where we were with the working group we thought this would be good since we did not need in a while any questions or comments on the general update otherwise we will move on to the algorithms to have an update on those so we have four uh algorithms and igmar you're in the queue because you will be presenting soon i assume so is this why or you wanted to ask something i'm trying to figure out how to cancel slice request yes yes possible before it's impossible no no it's there i i see it i will just give the overview first and then i'll let you share oh so i i'm on the overview slide so no worries so we have the four algorithms so we have scream and ingmar"
  },
  {
    "startTime": "00:08:02",
    "text": "will give us the the update on that then we have the the nada congestion control and uh xiaojing updated the status on that uh last year that they had the full implementation uh completed of nada in the mozilla browser and this was kind of the completion for them on the work so there has as far as the authors and shares no there has not been any ongoing activities after they completed this this implementation and xiaojing and her co-workers have moved on to other projects or other employees after that so i think there is no ongoing work or nada as far as we know but there is the open source implementation then there is the coupled congestion control and there there is some ongoing research on the topic by the waters so they have some students working on coupling the the video and data flows in chromium but there are no no results or or ready things to report but still some ongoing research and then we had the shared bottleneck detection and david will give us the update on that so with that i will now uh stop my sharing and i will now i think in my view should be able to share and i will give the word to you to give us an update on scream okay and uh little i hope you hear me him well and this is an update what has been done in terms of experiments the future and uh about around screen which was developed"
  },
  {
    "startTime": "00:10:02",
    "text": "by me and so i had them published or was it the late 2017 it's been a qui a couple of years since on and uh you can skip to the next okay i need to click on number two here and there are some to start with where scream is or isn't used i know originally the screen was intended for webrtc and that was the whole intent to it back when it was uh the work started on it was implemented for open webrtc but it never got traction and it never really never really ever evolved into webrtc and so we don't want a natural financial reason we don't have any any much experimentation around that so uh but perhaps something that was a bit below my radar is that there was a can probably look at the avt core presentation where martin singelbert and yoriot presented will have integrated screen with a quick with our go quick implementation so they have some results there and and boundary issues when implementing a screen there what has been more tried more is a cloud rendered gaming and uh so now we're not talking about uh one two megabits per second it's more like uh 30 megabit per second video with high quality and then we have done quite a lot of experimentation and then we also have for remote driving uh where we have a hero head or a remote control call that we have been driving around in various projects and that also includes a high bitrate video with multiple cameras and and also a scream has some kind of uh it's a wrapper around the congestion control that makes it possible to do some benchmarking for instance over 5g networks that has been done quite a lot"
  },
  {
    "startTime": "00:12:06",
    "text": "and about to clone rendering gaming and here we have been much focused around the development of l4s and the screen was picked because it's not the perfect algorithm don't claim that but it's a known algorithm and it was quite easy to uh dig up statistics around how it worked and what kind of latency got on and that later on evolved into using implementing a gear streamer plug-in to do as a real uh experiments with a cloud rendering gaming and uh where we there we were able to try out the experiment we'd done without l4s and uh it was a uh we found a couple of issues with the green algorithms in the process and mostly related to how video encoders behave especially when you frequently update your target bit rate and i will come back to that later and uh yeah and uh then we have remote driving where we have done quite a lot to experimentation and we have other students who have integrated uh the screen algorithm with the remote control core and uh we've tried out in various environments and uh over 4g and party networks and it has been a pretty good test platform to uh benchmark how it really behaves in pole coverage for instance and and uh unfortunately we don't have much data to show because uh it's a bit sloppy behavior we run the test and when then we drop all the data you know it's a bit has been a bit of a ad hoc testing pretty much and for demo purposes and i hope hopefully we do that better later"
  },
  {
    "startTime": "00:14:01",
    "text": "on and then we have this benchmarking table that is available for for public use and and it what it does is that it it emulates a video encoder where you can have other you you can pick either a fixed bit rate or you make it rate adaptive and you can do the rate adaptation over quite a large uh bit rate range from a cup uh tens of kilobits per second up to uh more than 500 megabits per second the upper limit there is depending on what kind of laptop you are using or the virtual machine that you have in in the other end for instance however much of the because it becomes a bit cpu consuming especially given the packet pacing and that is uh involved so it's not ideal for really high bitrate applications but up to something like 500 megabit per second you can try out i believe though the highest i got was like almost 900 megabit per second over one gig gigabit line and you can also model iframes and variable frame sizes you get something that is similar to how a video encoder in a constant bitrate mode behaves but it's not i it's not it's not identical of course and you can measure rounded time with the estimated q delay which is a by uh output from the screen algorithm the throughput that you get both the transmitted bit rate and what is hacked in the other random packet loss and see marks and it's a it there is capable about trying classic easy and l4s um and the finding is uh so far is that what we'll find is that the window based"
  },
  {
    "startTime": "00:16:00",
    "text": "congestion control is which is a scream it is more or less actually tcp land but but with a little more bells and whistles that is probably good at least for cellular access because you can have a radar equal resource configuration that happens and you can have andover that can pose transmissions pulses can cause causal transmission policies on that if that is not handled you end up quite easily with a lot of data in the transmit buffers in the in the network that is just there and it's a it's it's uh it's a causes head of line blocking for subsequent video for instance so it's a bet that you have it is a window-based congestion control and then you have a lots of zero in cases like this you have lots of a lot of packets in the center that you can discard if they're all deemed too old anyway so and then you can locally on the center side of forcing you idr so you're going to get quite the fast recovery because of that and that is uh has been shown in the experimentation the remote control call for instance where you can in a really bad coverage situations you can end up with a in those situations and the feedback rate we have had a lot of discussion on calling for instance i've done some uh work around the what is the applicable uh feedback rate that is needed as it is today so something like one feedback per 16 rtp packets and that is probably overkill and uh but uh to be honest i haven't done so much uh experimentation what is the pain point pain limit here it has been more focused on a stable streaming than optimizing that core head so you in as it is now you end up with something like the the feedback bit rate is something like the 150th of the media bit rate"
  },
  {
    "startTime": "00:18:01",
    "text": "that's where we end up roughly and the video recorder that is something like 80 of the development work doing the network congestion control that has been quite easy but the video coders yeah sometimes they are really strange animals and you have a video coder that are sluggish when you sort of set the target bitrate it can take a while for it to adapt to the new target bitrate on and sometimes you have that the video encoders can become confused by frequent update dates and for instance the nvm encoder can be given what's called a systematic offset so if you set it to 20 megabits per second you will get like 22 megabits per second on the average and that has been uh can it's not it's not a disaster for screen but it's to give some kind of a sub-optimal behavior and that is not always visible if you don't uh if you use just endpoint based adaptation but it's more if you use head for rest and you can you start to see those anomalies and also that iframes they are really problematic in congestive situations because they can be really larger so there are companies that transmit iframes only when needed and also gradual decoder refresh also called periodic injury refresh can be useful uh if that is implemented in decoders and in the encoders and another way that is used in a roman caller remote control car at lower bitrates you to compress the iframes even even harder you set the qpmax and qpmain mean values to keep control of that so there are some uh issues with the video encoder that even though your congestion control"
  },
  {
    "startTime": "00:20:00",
    "text": "algorithms works like the sean you have a video encoders it's a essentially a black box that can cause a lot of issues for you and then you have a what is to be done in the future and uh currently for some kind of an lfrs implementation is in running code that is not in the in the standard so i have some plans to do an update and possibly based on improved alphas implementation later on but they say that is not planned right now but hopefully later on i believe that was general presentation and four minutes left um do we have questions or thank you ingmar so let's open up the queue and see if we have some questions so let me ask one question on because you mentioned that the code has the easy and support so the the results that you showed they were using easy and feedback or they were based on law space what results we have is that uh so far they are in sort of in official or they also what the to the degree that they are officialized in this demo here and here is he again if you probably won't see you know but you can see that the amount of marked packets you see in the sorry yeah there you can see the amount of more packets and how it behave how it controlled the bitrate of the video yeah you know that's so far it's only in that that experiment"
  },
  {
    "startTime": "00:22:01",
    "text": "that you're using in this case l4s we haven't done much experimentation with eastern thank you we have mo in the queue go ahead mo yes uh the from what i understand the original um design in in scream was motivated by uh wireless clients um do you think that that uh that original design is still uh uh part of the model or do you think it would be equally uh optimized for things like data center to data center traffic um so i'm really wondering whether or not it makes sense to start considering the full topology when we look at the congestion controls and wonder if a one-size-fits-all model is is maybe not appropriate and we need you know some tailored for specific environments i can tell i believe the most of the focus was on uh the last mile uh 4g and 5g access that was being was quoted driving but i haven't thought about the data center the data center for instance if that is applicable there and i somehow suspect that the other algorithms like if we look at the frog or bb or version 2 or something like that could be more applicable there i'm not sure but it's a pretty tailored to the streaming of the rtp media in this case and not sure how useful it is in a wider context follow-up do you think that the direction matters were you mostly optimizing for send from uh from a client or were you mostly optimizing for receive from the clients uh i'm not sure if honest to the question are you trying to optimize uh was screen mostly trying to optimize the transmit video of a mobile endpoint or the"
  },
  {
    "startTime": "00:24:01",
    "text": "receive video of a mobile endpoint or do you think that you tried to actually optimize both directions equally yeah i believe the the actual scream algorithm that is it doesn't really optimize uh if you look at it and think about the heater buffers for instance that is up to the for instance the key streamer pipeline to handle and it doesn't do re-transmissions and that stuff and uh it just handles the congestion control without uh like it's like tcp without re-transmissions more or less but it's another retardation algorithm that is more sensitive to their delay increasing well moe left the queue at least so i i guess it did do we have any last questions no if not thank you very much ingmar for the update on screen scream and then david you're up next okay we see your slides okay thank you well this is an update on the shared bottleneck detection work and thanks to michael and simona for their help putting this together oops i have to make sure i'm on the right window i think before the slides move okay some work that's happened probably since the last time we were here which was quite a while ago"
  },
  {
    "startTime": "00:26:00",
    "text": "there's been some work putting um basically the rsc version of sbd in multipath tcp and in multi-path quick and what we'll be talking about uh what i'll be talking about today is is a thorough comparative study that we've done of rfc 8382 and we recently had it published in ieee acm transactions and networking now what we do is we compare the vanilla rfc8382 with that with the stat statistics that we collect that but a different type of way of grouping a dynamic online clustering method and then we compare that with what was one of the most promising ones in the literature a different type of method a cross-correlation method that originally was an offline method and also our online adaption of that and we in the paper we look at teacup test bed experiments simulations some experiments over the internet as well and we can what we will look at today is the time it takes to detect a bottleneck and to detect the bottleneck's gone away um the effect of different path delays on how well you can detect bottlenecks and the effect of having lots and lots of parallel bottlenecks and of course there's more in the paper if you want to read it now the the rmcat version is is based on summary statistics it was based on summary statistics because"
  },
  {
    "startTime": "00:28:01",
    "text": "originally one of the requirements was um very low feedback overhead so with this method we send the summary statistics not very often and it allows it also allows for us to detect bottlenecks on senders or receivers and work with both it's a divide and conquer method the one that's written up in the rfc where we and it's very simple very light where we take take our flows in our measurements and first divide them into what's what flows they're experiencing congestion and what not then we take that group and subdivide it according to an estimate of the oscillation then we'll look at the very sort of variance how they vary as a summary statistic and then an estimate of the skew and then if if the packet loss is high enough to be statistically relevant then look at packet loss as well dividing into smaller and smaller groups till we end up with the final the most common question we were asked is why don't we use a clustering algorithm and that tends to be difficult because we don't know the number of groups we the number of groups keeps changing and the flows in each group can keep changing but we have got a novel iterative one that we present and it it sort of does the closest neighbor by an inverse square law now the other main way of doing this sort of thing is to correlate the different flows but because one-way delay is so noisy if you don't use summary statistics to help with noise"
  },
  {
    "startTime": "00:30:01",
    "text": "then you need to filter it some way and this uses wavelet filtering techniques which are easily implementable in an online way as well but the original one takes the whole delay trace of every flow puts it into matlab and gets matlab to work out what the optimal filter characteristics are for the whole of everything so you need a crystal ball for this one and then filters the trace and then does pairwise correlation coefficients okay our version a a slight modification of that works out when's a good time to estimate good filter coefficients and then estimates them on the fly okay firstly looking at detection delays so how long does it take after a bottleneck starts if we know ground truth than we did in this experiments um before we can say there's a shared bottleneck and then after it goes away how long before each of these algorithms can say it's stopped and it's not that surprising that the offline crystal ball method in green works out the start of bottleneck very very quickly it's not so good at detecting the end because it was never designed to do anything but detect bottlenecks not not stop them um our online version of that in yellow also is similar not quite as good because it's not optimal i suppose then we have the rfc based ones the dc sbd is the clustering dynamic clustering one and our mcat sbd is a straight rfc and you can see that the summary statistic ones introduce some lag and it takes"
  },
  {
    "startTime": "00:32:01",
    "text": "you know up to about four seconds sometimes more to detect or the start or a stop of a bottleneck now what we did here is different paths have different delays so if all of the the paths that the flows are going on have exactly the same delay then the source lag difference we say is zero but if the difference between the flow with the shortest um shortest transit time propagation delay and the longest propagation delay is 1000 seconds then and here the source lag difference would be one thousand and what you see here is as the difference between the propagation delays of the different flows increase the the summary statistic methods handle that very well but the correlation ones the the more lag we get the less well they handle it and you can see that by them the efficiency we use the f1 score for accuracy in this case um which is sort of a harmonic is uh harmonic mean of precision and recall but they drop off quite sharply after you get above 100 milliseconds in this case now the other thing we looked at is what happens if we have lots and lots and lots of parallel bottlenecks at once how does that affect the efficiency and we find out that the rfc version after we get above four parallel bottom x um starts to sort of drop off in its accuracy the dynamic clustering method that we have um improves on that with the summary statistics and"
  },
  {
    "startTime": "00:34:01",
    "text": "cross correlation wavelet filtered methods are pretty pretty straight regardless of how many parallel bolt necks we have so in conclusion rfc8382 performs pretty well as simple it's light it's really fast compared to all of the others it handles path delay as well summary statistics introduce a lag so that slows it down in detecting and undetecting and if you have high numbers of parallel bottles next then a different grouping mechanism such as the one that we proposed here um improves on that now a limitation of all of these methods and pretty all shared bottleneck detection methods is they assume that bottlenecks have measurable similarities and all flows for all flows sharing them a sort of a bottleneck signature and a counter example is that if one your sending pattern dominates a bottleneck then what you end up correlating is sending patterns rather than a bottleneck signature and that potentially in some circumstances can mean that you group flows together because their sending patterns matched not that they were sharing a common bottleneck so the path forward we think that for a proposed standard to be useful then that limitation should be addressed first and addressing it may be algorithm-dependent so it may depend on whether you're using scream or whatever and you may have to have that knowledge a generic algorithm may be possible using the ideas of sort of a"
  },
  {
    "startTime": "00:36:02",
    "text": "speakerphone cancelling out feedback so you may be able to do it that way as well but that requires a bit more research or the other thing you could do is just note that it's happening and then when it's happening treat your sbd results accordingly and we're calling on any interested parties to continue this week thank you okay thank you david for the update on spd do we have any quick questions for for david otherwise it's nice to see that the standard version still looks to be performing quite well in your evaluations very simple and light comparatively the once you go to clustering it's a lot more cpu to do the work sorry so david did you look at um more complex layer 2 bottlenecks where you've got some sort of um algorithm going underneath to allocate bandwidth and the bottleneck kind of is changeable maybe not characterizable did you actually manage to touch any of that aspect when you were doing your exploration and thinking can you just explain a little more because i'm just trying to picture what you're sure so i'm thinking about a radial link that's actually doing some sort of um adaptive bottleneck adaptive bitrate method maybe to counter rain fade or um movement of the of the parties and then your bottleneck doesn't fit any mold and presumably you have to kind of reject this one as being difficult to characterize but did you think about these kind of really odd ones that also appear um i can't say that we've thought that much about that one and that"
  },
  {
    "startTime": "00:38:01",
    "text": "has the same sort of limitation instead except in this time instead of the descending one you're you have this other pattern being introduced so you have to have some way of filtering that out so you see so you're able to compare it and at the moment all detection methods have this same problem yeah i i really enjoyed your your feedback so that was a super presentation and look forward to hearing more when other people advance this thing okay excellent so thank you very much david for that update i don't see anyone else in the queue and i think we also need to move on to the next presentation so colin you are up next all right i can make the shot slides okay is this working yes yes yes okay so hi i'm colin perkins i'm going to talk quickly uh about some of the uh overheads of congestion control feedback in rtp uh which is the the final working group draft we we have in this this group um so the uh goal of this draft is to outline some of the overheads of using the the various congestion control feedback packets that we we've defined in this group um so so it's looking at the rfc 888 method applying this along with the other types of feedback you get you know the the various compounded non-compound packets the different feedback profiles uh the different reporting extensions and so on um to try and sort of work through the calculations and show what sort of overhead you get from sending congestion"
  },
  {
    "startTime": "00:40:01",
    "text": "control feedback in in some typical cases uh so people can so get get an idea of um what's the cost of doing congestion control and what are the factors that influence that cost so it uh looks at two uh relatively straightforward cases um the first being that the voice over ap case and the second one being a simple point-to-point video case so in in the voiceover ap case we're looking at two-part party voice call sending um frames um you know uh so so so so many uh voice packets per second i'm wanting to send congestion feedback uh um either every frame every second frame or so on uh we're trying to set the rtcp reporting interval so we're sending our tcp reports every few audio frames maybe every second frame every fourth every sixteenth frame or or whatever uh and obviously we there's different ways of configuring rtp uh rtp and rtcp and we we can send the the reports that the feedback reports either as um regular compound rtcp packets or as non-compound packets uh i mean we can mix and match between these so so we can either send compound packets every time or we can send compound packets with uh non-compound packets between them uh so the draft works through uh the format of a bunch of these packets so it starts by looking at the format of a compound rtcp feedback packet and it sort of looks looks at the size of the various headers so it says if we're using udp uh an ipv ipv4 you've got a certain size of header you've got a certain size of header and and overhead for the sender reports you know a certain size of source description packet uh a certain amount of congestion control feedback packets so the srt"
  },
  {
    "startTime": "00:42:00",
    "text": "srtcp authentication tag and so on and i'm not going to talk through all the numbers here but the draft works through sort of counting you know the number of bytes in each of these to figure out the the packet sizes and it does the same for the non-compound packets which just send the congestion control feedback report and don't have the the other uh rtcp packets stacked in there [Music] obviously you get a much smaller packet because you don't have the source description packets and the sender reports and so um a real system would then send a mix of these so it has to send occasional non occasional compound packets in order to keep rtp working because you need the occasional sender reports and source description packets and then in between that it will send non-compound packets that are just sending congestion control feedback to reduce the overhead and obviously you can change the balance between these and what we're seeing on the slide is something that sends one uh compound packet followed by a couple of non-compound packets uh in a repeating pattern um and you know in some cases you could just send the compound packets or you could have you know one or two or three or whatever non-compound packets in between the draft then sort of looks at how you do the calculation to figure out the overheads uh and it take it goes through the rfc 3550 reporting interval calculation uh and shows that the the reporting interval depends on the number of participants in the session the average size of the packets uh the rtcp bandwidth fraction has been allocated um and that lets you you work out the the rtcp reporting interval um if we want to send a congestion control report every um a certain number of frames you set the reporting interval to be you know that that that number times the um the framing interval and you you work through the maths and that gives you a fraction for the the bandwidth overhead"
  },
  {
    "startTime": "00:44:01",
    "text": "that you'll get if if you want to send rtcp reports at that particular rate in order to get congestion control feedback uh and the core of this draft is is a bunch of charts showing the overhead in various cases uh and so for for example in in the case where you're sending um only compound packets and you're not using any of the non-compound packets to reduce the overhead um you know that this table shows that if you have your audio uh being framed into 20 millisecond packets and you're sending rtcp feedback every second frame of audio using only compound packets your rtcp bandwidth is 57 kilobits for the congestion control feedback and as you um increase the number of frames if you report every eighth frame this drops down to 15 kilobits and if you reduce it if you increase the audio framing interval then the overhead drops down more again this is only sending compound pack compound packets but it's showing that you can you can adapt the the overhead of the rtcp congestion control feedback by changing the how often you send feedback and the size of the audio packets if you do the same thing by sending compound packets in between and this is sent this is alternating compound and non-compound packets you see the the overheads reduce uh somewhat and if you send more non-compound packets in between you can reduce the overheads further and it's showing the sort of uh example behavior you get as you play with the different parameters to illustrate what what the overhead of these mechanisms is now it then does the the same sort of calculation for a point-to-point video conference uh assuming we've got two people both sending audio and video streams uh all bundled onto a single five-two pulsar you've got four active ssrc's uh one"
  },
  {
    "startTime": "00:46:00",
    "text": "audio one video for each of the two participants um and we're trying to send um congestion control feedback every so many video frames um and um the the sort of feedback you get in this case you've got your um your your udpip headers uh and you've got because there's two ssrcs you've got the feedback being aggregated into the packet and you've got one compound rtcp packet from the reporting source one from the non-reporting source assuming you're using the um the reporting groups and the aggregating feedback mechanisms and then you've got the the authentication tag for situ srtp and again that i'm not going to talk about it in in much detail but the draft works through this uh the non-reporting packet uh has assuming wearing extending compound packets has an empty sender report a source description c name and and the reporting group group source packets in it the reporting source has a complete sender report it has the the the sc name and the reporting group packet and it's got the congestion control feedback uh because you're using reporting groups this is the congestion control feedback for both ssrcs since that that they're co-located and they're seeing the same thing and the draft works through uh counting up the size of these various packets and figuring out what the overhead would be and that turns out to be uh quite large in this case you've got a lot of uh overheads because of the ssrcs and the various reporting packets you've got a couple of hundred bytes overhead plus the the feedback um and it works through and it does the the same sort of calculation plugging in the numbers into the the rtcp reporting interval calculation uh from 3550 um and"
  },
  {
    "startTime": "00:48:01",
    "text": "from that it comes out with a table showing the the bandwidth overheads uh and and what we're showing here is if you've picked a if you look on the left here the media rate column it it's making a bunch of assumptions about how the video is packetized which are not entirely realistic but are probably close enough to give a flavor of how this would work and make them abstractable you know if for example you're picking you're sending media at 200 kilobits per second um your and 16 frames per second video uh then that gives you um and you're trying to send a report for every frame that gives you one on video packet per reports uh probably free audio packets per reports depending how you do the audio packetization uh leading to an rtcp bandwidth of sort of 67 kilobits per second which is about a third of the the video rate and as as you see as you go down the table as the media rate increases uh the the overheads of the reporting go down uh and you the table shows how this varies with the frame rate you're choosing and so uh similarly um you can use reduce size packets uh by playing with the the way you you can figure out tcp uh and uh i'm not going to walk through the details but the draft does that and uh it shows that the um the the required bandwidth drops down as you might expect and reduces the overheads so that's the this is a very quick version of what's in this draft um the the latest version of the draft uh what what what i've done in this version is brought the calculations up to date with the published version of rfc 8888 corrected a couple of minor mistakes in the way the calculations were being done and the previous versions had placeholders"
  },
  {
    "startTime": "00:50:00",
    "text": "for multi-party and screen sharing uh scenarios um i've taken those out because uh when i tried to work through them i just realized there were way too many variables to to easily characterize it uh and um if i wanted it if we want something in there which would uh actually be representative of the different ways of configuring it this draft is going to balloon out to 50 pages long and it it didn't seem worth the it didn't seem worth the effort because the scenarios in there i think illustrate the the fundamentals of the behavior now so that's where we're at in this version um as i say it illustrates the various factors that that influence the the overhead of using this congestion control feedback uh and um i think hopefully gives some useful hints to implements as how you might configure and use the format um from my point of view i i think this is is done uh it's got everything i i want to put in it um i you know and i think it illustrates the point um so my my recommendation would be that if the group thinks this is useful enough to publish we should uh take it to last call um if there are a few if there are nits or details to sort out we can do that um but if to me if the group thinks that there is a um your significant if the group thinks that it needs significantly more work to to get this into a useful state i'm not convinced it's worth to me the effort of doing that so that that that's where i think this is that with that uh i think i'm done okay thank you colin for the update do we have any uh questions for colin"
  },
  {
    "startTime": "00:52:03",
    "text": "yes jonathan throw the question back at you do you think it's useful in its current state for implementers no this is something so sorry you're breaking up and i didn't hear that do you think this is useful that's where it purple matters uh i i i i i think it's useful yes i mean uh yeah it's obviously only covering some some of the basic cases but i think it shows shows the point of how to configure things but does it and does it give enough guidance that when you if you have a specific you know more complicated case but it could follow the same procedure as you did and get an answer um i i think it illustrates the general principles uh and in the more complex cases there are so many different parameters to configure that that it's hard to give specific guidance yeah but i mean but somebody who knew what their parameters were could you know take your follow your procedure and say okay for my case where it has this this and this and this is what i need to do probably um i i i think probably i mean obviously it would depend on their experience with rtp and congestion control exactly how familiar they were with the various things okay and this is of course a informational document right yeah it's targeting information do we have any more comments or questions if not i think we are also in a sense this is also moving us into our last item on the agenda that we have a few"
  },
  {
    "startTime": "00:54:01",
    "text": "minutes to to see how to wrap up and what our outstanding work points are and of course this draft that colin just presented is our last open working group document that we had not concluded and from from the author's perspective this this strat is complete so the the suggestion here i think is to send it out for for uh uh working group last call and feedback from the from the working group and there of course it's it's uh very good to get feedback on the the current status of the document and if you see that that this is uh a good good piece of advice to publish from the working group uh it now was a while since we talked about this document and uh i'm not sure if uh if the participants have had time to to read it so i think that we have to take take it to the list to to get more feedback on that point and the other thing that we have done today of course is to to try and have an update and summarize the the status of the different congestion control algorithms because we had on our shorter to move some of the one or several of the algorithms to propose standard after experiences um but i think we we have it's nice to see that there is some work on some of the algorithms but i think from the shares perspective the the conclusion is that we have not had enough activity and work that it makes sense to to bring any of the algorithms on that path"
  },
  {
    "startTime": "00:56:00",
    "text": "at the moment i think what we maybe need to discuss is is the scream side because i think this is the algorithm that has seen most use and as you had on your sliding mark to to have an update that brings in the handling of easy and maybe something that that is useful still as an experimental algorithm i would would expect in that sense so we are now also open to have have comments on any of the open points from the the rest of the participants and and what your view is yes ahead i was just going to repeat some of the things i said but let's hear from you but hey sorry um so i basically think that that um that last remaining document should be 80 sponsored and the group should probably close um last remaining one being the mine or being the screen that the one that's adopted okay am i am i wrong that there's there's only one adopted right so so the one i presented which has been adopted and then there was the potential update to screen that and i was talking about oh yeah um i would i would just close the group i mean i i if people want to update screen that"
  },
  {
    "startTime": "00:58:00",
    "text": "that can also be ad sponsored it can be in transport tswg right it's i don't see a reason to keep the group around but that's me personally with no dots or anything so i think for the open document um hope was that this was very close to being done [Music] i i guess the document can can still move but uh [Music] i don't expect to do any more work other than fix some typos people point out if it's more than that i i'm going to abandon it i mean the documents basically been around for like eight years or so right and it's occasionally getting updates which is great but um you know i would either sort of start working group last call now or um make it a sponsor martin okay just to add to what lars said um for scream this uh 8382 bis uh it is experimental document so we could actually even do it in iccrg if that's a um a better venue for it corey i wondered if there were still enough people around rncap to review colin's document other people were"
  },
  {
    "startTime": "01:00:01",
    "text": "willing to give the final version a read and therefore whether you could do a working group last call do you have any sense of how many people are ready to read it so that is a a bit hard to judge but i see that jurg at least is willing to read it in the comments in the chat we have about 30 people attending this session so and i i think it would be very good also of course uh if some of the algorithm authors can also read it so that that would be my hope but it's always hard to to assess of course ingmar also says he's willing to read it so i think that would be very good [Music] i think we could also check if i'm not sure if charging is is in the call i know she's still in the mailing list even she she has moved from cisco if she is also willing to read it from the from the the nada side so i think if we keep it in the group the idea of course is to to move it to to working group last call and to try to to finish finish it up yes sid i hope you can hear me yes we hear you oh i mean with my eddie hat on i think i have not seen any huge activities or or any activities in the mailing list and and i have not seen like i mean nada"
  },
  {
    "startTime": "01:02:00",
    "text": "uh as you reported 2020 uh it has implemented a firefox mozilla i have no information on whether that is in use uh scream uh ingomar is doing some trials and that's good to have and we can actually fix uh some updates and all these things so um that's that's fine other than that i don't see like anything happening on somebody i mean previously in one of the item meeting there was one proposal coming from um like somebody asked like can we can we make another condition control algorithm and all this thing i don't think like they're coming back so and today from this meeting i get the view like well there might not be too much energy to carry the working group on so i think uh for the this document the lab in that working role that when we have we can go directly to the working blast cop and push it to iit plastic call very soon um other than that i don't i i am i cannot convince myself like why we should have this working group and if we need we can actually keep the mailing list alive right so if there is anything some people want to come in in future or some updates want to be discussed this mail even could still be there so that's actually what i think um and yeah i think this has been working with a really good document um i have been part of this working group from very beginning um it was a quite good journey but i would love to see this work has been deployed and adopted other than that i don't see any output from this working group now so as i said i mean we had we had decided to to have some pause to see if there would be more deployment and experience from the from the algorithms but i i as i said i don't see we don't as shares"
  },
  {
    "startTime": "01:04:00",
    "text": "also don't see that any of the algorithms would be you know candidates to be pushed forward at this point so that that part of the shorter i think is yeah it's not relevant at this point in time so i i think we all all have the same view on that um yeah i see one more plus one for for doing a working group called last call on the on the document [Music] so we can of course also so coordinate on how to how to wrap up that that last document if you have other uh comments on on that one side but otherwise i think the plan is we try to finish off the the last document and then we are ready to to close the group this is my my understanding of the this the situation and the sense of the group as well uh and we are a little bit over time but gauri you are in the queue you wanted to add something yeah i just wanted to say i put them i have put the names of the people who volunteered to contribute last call in the notes so that they're now in the notes okay excellent yeah then we have an agreement there and way forward so let's do that okay great then thank you everyone for today and it was still very nice to see you all and to also get an update on what this was ongoing with at least some of the algorithms so uh anything you want to add colleen mostly i'm going to add that i still can't work the mute button uh i was just going to say thanks everyone for for participating as you say it was it was good to to get an update and come to closure on the group and yeah we we should try and get that"
  },
  {
    "startTime": "01:06:00",
    "text": "working group last call done very quickly i think okay okay thank you okay bye"
  }
]
