[
  {
    "startTime": "00:00:20",
    "text": "okay uh welcome to uh privacy pass at uh iatf 114 uh thanks for uh joining us um just real quick uh here's the note well um i think you're probably well acquainted with this but if you need a refresher you can take a quick look at this reminder a couple reminders for this meeting is it's really helpful to join the uh meeting with the on-site tool so you can join the queue and so you can um so we record your attendance as well uh so please uh join the on-site uh meeting thank you also please uh wear your mask except when speaking um i think that's the main things uh main thing i want to do right now in terms of administrative trivia is to get a note taker can somebody please volunteer to take some notes for us be really helpful let us get us through our agenda we do need a notetaker going once thank you what's your name marcus thank you um so we have a a pretty full agenda today um so with initial discussion about deployment experience uh status of the"
  },
  {
    "startTime": "00:02:00",
    "text": "core documents talk about rate limited token and then key consistency and discovery are there any uh additions modifications to this agenda that anybody would like to make okay then i think we can jump in dude uh i want to note that we have a full agenda for our very short session uh so i will be running a timer to include both presentation and questions on each topic can i oh yeah you can if you yeah why don't you do that yeah yeah yeah let me do it all right hi everyone hi everyone um it's loud i'm tommy paulie from apple um one of the co-authors on some of the documents we have here and i'll be presenting some of our current status on implementation and deployment testing here for privacy pass uh so first i wanted to point to some implementations this is of the base specs so this is the token issuance um and authentication scheme to request and redeem tokens"
  },
  {
    "startTime": "00:04:01",
    "text": "there are a couple different open source implementations if you want to try these out there's one that is uh written in go that cloudflare has that implements client origin a tester issuer either as one thing or separate things and you can use it for all the basic variants and you can also test out the rate limited variant so this is a very useful tool for doing interop there's also a new one coming up from raphael that does client origin an issuer for the basic variants so if you're interested in building stuff these are good resources to interrupt with but beyond that we are also doing some non-open source implementations um and actually getting some deployment experience at apple's developer conference in june we announced that we want to replace captchas with uh private access tokens which is really just privacy pass but that's what people wanted to call it for branding reasons um so it is just privacy pass doing the basic uh type 2 a publicly verifiable tokens and so we introduced a broader developer community to this concept and pointed back to privacy pass if they wanted to get more engaged so hopefully that will kick up some interest and i've already seen people looking at this a little bit more uh so yeah we're trying to increase awareness we are allowing developers of apps to be able to do token uh issuance and redemption as well as anything that's running within uh webkit environments on ios and mac os will automatically be able to support this now and so we have attestation and token issuance uh that we're working with to try to kind of get some tests going on in this ecosystem but the goal is to have it"
  },
  {
    "startTime": "00:06:00",
    "text": "grow and to have you know other attestation work and i look forward to seeing some of the previous privacy pass browser extensions etc be updated to be compatible with the current specs for the test that you can do with what we have on ios and mac os currently cloudflare and fastly have both deployed token issuers that are specifically for this initial demo experimentation phase these are the two issuer names you can find more information about them uh for our devices we are doing some attestation with an icloud server that is doing device and account assession but other platforms and other browsers and applications can do other types of attestation that could work with these issuers just as well ios 16 and mac os ventura are currently in beta and they support all the client functionality automatically if you want to explicitly test this out if you have one of these devices that's on these builds there are some demo origins like this cloudflare one which will just always say i'm going to give you a captcha unless you can show me a token but also in the wild there are deployments that are actually starting to use this and try it out in non-forced cases uh before i get into that just what is supported here um on our client this is only doing type two and i believe that's also the case for the uh issuers that they are doing the publicly verifiable basic token types that are using rsa blind signatures for our client we are doing a split origin a tester and issuer model and our client supports either specific origin or cross-origin tokens and there are some limitations around"
  },
  {
    "startTime": "00:08:01",
    "text": "what we'll do in the web context we only accept token challenges and redeem tokens to things that are first-party domains within a web context so that random ads and other stuff can't start trying to poke and get tokens out of you so cloudflare managed captchas is one of the things that's already doing requests for tokens and so currently right now with these beta builds we're seeing 35 000 tokens being issued per day um and 16 000 tokens that are being redeemed so that you're seeing kind of like the client fetching batches um and not necessarily spending them all but then we're actually spending them later to say okay get around this capture because i've already done the attestation and i don't have detailed latency information but it's all very very minimal it's not really impacting any user facing things so far so i believe that's it for these slides i guess if there are any questions about the specific implications let me know there are links to the open source repositories if you want to play with those too so please try it out do interoperability testing if you want all right seems like there are no questions that we can probably move on do you have anything oh you are oh nick oh sorry i didn't see it hey nick uh nick dodie cdc um thanks for uh both translating and giving us some implementation experience uh i'm curious whether well i have lots of questions but i'll just ask one i'm curious whether we've considered um"
  },
  {
    "startTime": "00:10:03",
    "text": "that some developer might just say hey i hear apple uh can attest to all the devices uh therefore i'm just gonna block access permanently to anyone who who doesn't get one of these tokens and whether we've considered sort of either protocol or implementation implementation changes that would withhold a certain small percentage of the time to encourage the developer to to realize oh i can't get this the 100 of the time that's a very very good point um so we absolutely don't want reliance like that um in our communication publicly we're like do not do i mean you should only do this if in lieu of a captcha and you always need to fall back but of course advice does not give you everything one of the nice things is that this it will not happen all the time and i didn't go into those details but this may be something we want to uh add advice to in the documents probably i think in the architecture document let's talk about the whole ecosystem so uh first there's a there's actually a human level toggle where some users can disable this but beyond that our attestation system will rate limit you but even on the device we have limits of when we are willing to get a token or not so um we will not randomly but if we see more than a couple token requests in a minute or based on what process you're in we will just ignore some requests so you certainly would not be able to do a site that did 100 and have it work but i agree that this is something that we should talk about more and i think having something randomly uh not due at some percentage of the time may be one mitigation for this all right okay thank you tommy"
  },
  {
    "startTime": "00:12:00",
    "text": "cool and i think next is the bass drafts yep and i think i'm going to present that and then chris will talk remotely about the rate limited unfortunately a lot of the authors have conflicts or in mid travel right now so all right i am still tommy polly but now i am speaking on behalf of more co-authors um chris wood and john iangar steven valdez amongst others so we have three base drafts here we have uh the architecture which essentially describes the different models for using privacy paths and deploying privacy paths we have the authentication scheme which is how an origin can request tokens and clients can redeem tokens and we have the protocol which defines two flavors of token issuance one that is privately verifiable and one that is publicly verifiable um so this is just a slide to go into some of the recent changes uh overall the authors think that these are pretty uh pretty mature and um from like a technical interrupt standpoint we think they're uh done and we'd like to get this moved along but i think there are things uh around wording that we still need a bit of work on but we hope to consider a last call soonish some of the recent changes include for architecture uh clarifying the trust model removing some old text that no longer really applied around parameterization"
  },
  {
    "startTime": "00:14:01",
    "text": "and adding a bit more text to talk about centralization although i know there are other documents proposed that will go into more depth there for the auth scheme based on the discussions at previous meetings clarifications around how to support multiple origin names being related to a given token were added we added guidance around how to handle and process token challenges clarified what you need to do in order to prevent double spending depending on what type of token context was being added and we also added test vectors so you can confirm your implementations and uh for the basic issuance uh very little changed uh there were a couple things really to clarify fully how the token verification worked some of it was just more implicitly saying oh you know how blind rsa works but we tried to make it a little bit more clear and we also talked about how you get the token validation keys from a consistent location uh so just the the couple open issues we have that i want to bring up today uh one is actually just based on an early ayanna review um ayana has been doing a great thing of actually before we go through the full process they are reviewing all of the working group documents that have iana considerations in them that are being presented so thank you ayanna for doing that um so the auth scheme document defines a registry of token types and the basic protocol issuance defines two types in there privately verifiable and a publicly verifiable token and the auth scheme also reserves some types to be able to do greasing of uh making sure that you can accept unknown types within your off challenges and redemption um ayanna did have a couple questions"
  },
  {
    "startTime": "00:16:01",
    "text": "uh we were not clear where this new registry should live they assumed it would probably be on a new privacy pass specific page i think that makes sense so the document now says that i would like to hear if anyone disagrees with that or what we should call it um that's very much a bike shed and then the other thing is what do we want the review policy to be for new entries into this uh what we put in for now in the pr is specific specification required and so that would involve designating experts and then requiring that you have a stable spec in order to allocate this i don't have particularly strong opinions this seemed like the right thing for entirely new token types because you have to have a pretty full specification be able to implement against it but i'd love to hear opinions of the working group if they can find their enqueuing buttons chris would hello uh yeah this seems fine um i don't know if we need to say anything more about the type of um like analysis that goes into a token type before it actually winds its way in the registry like hpk for comparison establishes a registry for like different algorithms and it just says if you want to be on this list you need to like have to meet certain properties um certain security properties we could say the same exact thing here um and then require that the documents like demonstrate in some way i don't know what that looks like that they actually meet the properties but provided that that's i think that this is a reasonable way forward and provided that who can assess it or able to reason about the correctness of those properties right i don't have the text in front of me right now that i propose but i i think there is a sentence there that it's recommending what the experts review and i think it says something along the lines of like it needs to meet the properties but i think that's the interesting sentence"
  },
  {
    "startTime": "00:18:00",
    "text": "to make sure is correct that it that's the right advice to the experts yeah cool thank you hey there's kenazi yeah quick warrant enthusiast uh how is this encoded uh these are not varants this is i think a 16-bit field because having a dependency on violence here isn't really necessary and that's a lot of token types all right just the the really nice thing about those is um even though you have 2 to the 62 only ever use 10. and but what it means is that you can like do whatever you want for experimentation and it makes life a lot easier so you don't have to do specific communication required and let people do i i try to push back on specification required for the entire space because the current it encourages people to just skip by and out together and squat on values if they don't want to have a publicly accessible spec so you're talking about essentially we you know the approach of reserving some experimental values that can never be allocated no so i mean mt would make this argument better than i can but that wasn't a great success in tls right um so rather having somewhere where you can just have like uh what's the term um not yeah temporary allocations i think that's fine and that i think is compatible with specification required as far as i understand like you can ask for a provisional and then you can always then say nope we is try to make this a bit more like the quick one you don't have to go full warrant even though i love that but saying some of the space is specification required and some of the space you can do provisional but provision all the atf ioni reserves the right to yank it from you at any time okay that way you avoid all these problems and that's been working really well for us that's a"
  },
  {
    "startTime": "00:20:01",
    "text": "good clarification just to make in the ion notes okay alrighty if we have no other thoughts on that then i'll move on oh um the other issue uh number 141 is it was more of a kind of architectural question about what what does the ecosystem do if the access attestation service which could be the issuance service as well uh stops doing the right thing so like it just says you know it starts giving out tokens willy-nilly to people who aren't trusted or who didn't actually solve their captcha or didn't meet the bar um and uh i think chris made this nice little hilarious spider-man's pointing at each other of essentially the failure modes here are origins are saying i'm getting some you know bad traffic coming in that you know clearly didn't actually shouldn't actually have passed the attestation checks it says issuer you know why are you giving me these bad tokens he says hey you know the tester you're not uh doing the right thing and the tester can be like oh my my checks are fine how do you know that that's actually fraudulent so we you know we definitely need a way out of this um and this was chris's suggestion for how to address this in general this should be a rather exceptional event um and essentially the ecosystem needs to come to some recognition of hmm you know either give away for origins to report back to the issuer that things are uh not working as they"
  },
  {
    "startTime": "00:22:02",
    "text": "would expect um so i think we do need some extra text on this i don't know chris if you wanted to comment on what you were imagining here um i didn't have anything specific in mind right now um because as you say as i sort of jotted down here i do think that this will likely vary based on your particular deployment whether or not it's like joint to test your issue or split a tester issue or um and what the configuration is of the different parties um we might just note that you know if if things go awry um signals start firing for reasons that they shouldn't start firing or whatever um but this is an exceptional event and you know applications should deal with it some way i don't have a very eloquent way of like stating that some way right now but hasn't described anything specific yeah it eventually needs to be at the point where if some party like an ancestor issuer starts doing bad things they kind of need to be kicked out of who's trusted by the other parties that you know the origins yeah i no longer trust this issuer or the issuer says i no longer trust this a tester because i need to make sure that my origins keep trusting me um yeah so essentially the roles of running a token issue and service um involves maintaining your credibility and you need to make sure you have some way of getting feedback to make sure you're not doing you're letting bad clients in and it's it's not like i guess if attestation fails once that that uh that particular tester should indefinitely be untrusted i can imagine scenarios where like i don't know there's like a zero day and like somehow for some reason like at that station is like subverted or whatever compromise but once that's patched everything's back to normal so"
  },
  {
    "startTime": "00:24:02",
    "text": "um i mean we can work on the text offline i think but i don't think the the implication needs to be like permanent adjustments to the trust model just like temporary or grounds or mitigations or something right someone compromises your tester if that's detected they take themselves out of the pool until they can patch their zero day and then they come back yeah yeah cool all right and i think that's it um so those are essentially the main two open issues we already have the ayana pull request out so we just need to refine that based on what we have here and then we want to add this architectural text and beyond that we think these basic ones are pretty good to go so if there are other pieces of feedback on those i'd like to hear it or if the chairs have any comment around what they want to see i think the last call is a good way to also initiate reviews sophia yes sofia sally from brave um i was having two questions the first one is it seems like it's going to be alaska or at least that's what it's been asked of is this waiting also on the standardization or the last call at least of the documents that currently are sitting in the cflg like the bo prf or the blinded rsa that com they're currently kind of dependencies on the privacy past main trust and the second thing um the new current architecture of the privacy pass protocol was kind of introduced on the december of last year and now it's there's an introduction of different issues and the testis and origins and how they can interact with each other and there's different ways that they can either collude with each other or as you just mentioned in one of the issues the different entities can be compromised has there any any interest of actually"
  },
  {
    "startTime": "00:26:01",
    "text": "of the working group to provide a more formal documents of any security slash privacy analysis of the different kind of attacks and threat model that all of those entities um colluding or interacting with each other composed cool uh for the first one maybe chris will have opinions on the relationship with cfrg documents i think that's kind of up to the chairs of privacy pass and cfrg to how they want to coordinate that i think those other specs are stable enough that it would be fine to run them in either order through that but maybe ship them off to the isg or i irs g review around the same time that's fine um and then regarding the other one i think the document does you know try to talk a bit about how you do the analysis um if there are specific things you think the architecture should add for extra analysis then we could add it but i i think it would also be perfectly fine to have further documents analyzing different aspects of this i think in general these split models that we have for privacy pass we have for oblivious http we have for using mask proxies they all share some interesting properties and i know there's even discussion in perigee about what are the terminologies we use for doing privacy analysis there so this certainly should not be the last word on how we talk about this architecture and think about analyzing these deployments yeah um a quick question about credibility of the entities that you mentioned can you say more about those like is that going to be part of the solution which entities are we talking about that"
  },
  {
    "startTime": "00:28:00",
    "text": "the parties that involved in this like you talk about credibility of those parties if they misbehave or something like this right right so in the architecture you have the entities that are redeeming tokens right and they don't they don't necessarily get to see all the information about the clients themselves and so they have a trust dependency on the entity that issues tokens um and that entity's credibility comes from the fact that either they are doing their own attestation or checks like maybe they had issued captchas or they had done some verification or they are working with one or more other services that can do that attestation of the clients um so essentially it is a transitive trust and credibility chain there and it's essentially up if an issue is going to work with multiple forms of attestation it needs to make sure that it is only using kind of good enough ones to meet the requirements of the origins that it's serving tokens to okay that makes sense thanks and chris um yeah just to respond to the crg thing because you've kind of called me out i think both of the blind rsa and the vpf documents are basically ready for research last call in that group anyways now the process does take longer unfortunately um and the iot than it does in the ietf so i guess it's up to the chairs as you suggested to figure out how they want to stack things and what the pipeline looks like um i don't think we need to block on uh you know having gone through all of the process in zfrg um to move things forward here um on the the comment that was just raised at the mic regarding like parties behaving correctly um"
  },
  {
    "startTime": "00:30:00",
    "text": "i mean i think pictures sort of par for the course like we expect article participants to uh implement the protocol honestly just like we expect tls servers to implement that protocol correctly and not post keys to twitter or whatever um and things break down when they don't behave correctly and this is a little bit different in that we're trying to design against potentially malicious parties in the protocol um but for the most part like the the behavior has specified we we just sort of assume that they're um they're following that classification cool all right i think we're probably out of time okay uh thank you tommy i guess uh you i believe chris wood is presenting the next session section [Music] okay um all right uh so uh this is an update on the rate limited privacy pass issuance protocol that we presented last time it's actually undergoing uh adoption an adoption call right now i kind of wanted to um take a step back from the internal technical details uh of the protocol because it is admittedly somewhat complex um we tried to spend a lot of time as the editors of the document clarifying what the different functions of the protocol are what the different steps are and what not um so here i just want to kind of talk about the high level uh motivation for the protocol um what the different properties are in terms of how much state it has a different uh different pieces or different entities in the vertical and what the what the desired or end resulting privacy properties are as well as touch on some open issues um so having said that uh as a recap because it wasn't covered in the um the base protocol document um"
  },
  {
    "startTime": "00:32:03",
    "text": "privacy passes now as a architecture is composed of two sub-protocols one of which is the redemption protocol this is the the protocol it's run between client and origin for the purposes of redeeming tokens and it's based on well-established http authentication mechanisms origin will challenge the client to present a token or may challenge the client to present a token with certain parameters in that challenge and the client if it can satisfy the challenge uh presents a token in response with its retried request issuance as the uh complementary protocol which effectively takes one of those token challenges from the origin and runs a protocol between the client to test or an issuer for the purposes of producing a token that's bound to that particular token challenge and we've sort of arranged things such that all of the complexity in privacy past is sort of encapsulated in the issuance protocol because there's expected to be sort of like very few implementations of these like one implementation of uh one client inflation that serves many clients and so on whereas um uh we on the flip side and the redemption side we wanted adoption to be incredibly easy so for example in the the basic type two tokens that tommy was referring to earlier like consuming and verifying a token is as simple as verifying an rsa signature um it's really straightforward okay um i'm gonna walk through sort of how these two things work in concert so on the left we have the redemption protocol which uh as as day before the origin produces a public keynote challenge the client and the output of this uh or in response to the challenge the client produces a token um and the desired property here is that the origin learns nothing about the client beyond whether or not it was able to present a token that it satisfies the"
  },
  {
    "startTime": "00:34:02",
    "text": "challenge um that just one bit on the right hand side um the the issuance protocol is again takes its input this public key and challenge turns it into a token with this advanced between client to tester and issuer which for the basic issuance protocol is just a blind signature protocol or a vrf protocol or whatever and the desired property here is that the issuer learns nothing about the client um uh in particular that it cannot link successive requests from the clients together they each appear as independent unlinkable requests unfortunately the basic insurance protocol does not provide an obvious way to do per client rate limiting so say you had an origin that wanted to to limit the number of times a particular client you know had access to a specific resource um it cannot do so because by definition the only thing it learns from the token is whether or not the token was able to present a valid token and the issuer the only thing that the issuer learns is whether or not um uh like token issue and succeeded it learns nothing about or there's no concept of state anywhere in in the issuance flow um this is problematic because there's a lot of interesting applications um where rate limiting is quite useful uh and we don't want to fall back to things like rate limiting based on shared context across clients like ip addresses you have meter pay walls where you might want to limit the number of times a particular client is able to access some content you might want to dampen the damage or activities that spots from a particular bot form are doing and so on rate limiting is used in many many places so the question we ask ourselves for this draft is how do we add great limiting support to privacy pass without compromising on the security and privacy properties that were discussed for the basic instruments protocol and before talking about sort of the the mechanism just uh i want to give a quick"
  },
  {
    "startTime": "00:36:01",
    "text": "reminder for how like rate limiting is commonly implemented in practice um uh typically it's typically done with some algorithm called a token bucket or a leaky bucket depending on which textbook you read where the token bucket or leaky bucket is driven by two independent processes um there's one process which is like the person trying to access the particular resource that consumes tokens from this bucket and they're only able to access the resource if there are tokens available to consume the the other process the token representative process will put tokens back in the bucket at a sort of fixed uh recurring rate and the rate at which you replenish tokens in the bucket determines the overall rate limit and so the way these two things interact the dynamics of the the interaction between requesting and consuming tokens determines um effectively how clients are rate limited internally um uh you can think of this being implemented uh using like a simple hash table so um when a you know a token replenish event comes in the token bucket will just identify the context associated with the rate limit and then increment some counter associated with that particular context so in this example there the context is an entry in a hash table there's a previous count and associated with it and just bumps up the count by t et is the replenish account requesting a resource does exactly the same thing uh to identify the writ limiting contacts and then go into the hash table but now it just decrements you know the the counter associated with it and if tokens are available process the request if not drops the request on the floor okay um so uh we wanted to implement basically this functionality uh under certain constraints um uh as i was saying before we want to"
  },
  {
    "startTime": "00:38:01",
    "text": "maintain the security and privacy properties that we originally had in the protocol in particular we don't want the origin to learn anything beyond a bit and this bit in this case is not only was a client able to produce a token but the the client was able to produce a token without exceeding the rate limit that is that is effectively the bit um any other sort of state at the origin side could be potentially used to track clients and so we didn't we didn't even want to explore that avenue um we also wanted adoption to being as simple as it was for privacy pass um in fact the the tokens that are produced for the rate limiting version are indistinguishable from uh the tokens for the basic issuance protocol both verified with the rsa signature um we wanted the issuance protocol the thing that actually produces tokens and necessarily deals with the state aspect of rate limiting to be as close to stateless as possible of course it can't be stateless because then you can't be really rate limiting someone has to have stake around like how many things uh how many tokens were uh issued for a particular client origin pair um and the question is like where is that state captain how is it enforced and whatnot but we want to minimize the state because we want to you know make it easy or to actually operate an issue or a tester um and importantly for um certain deployment models of privacy pass we want to make sure that neither a tester nor issue are able to link client origin pairs together so right now in the basic issuance protocol there's no there's no information exposed to the issuer during issuance um but if we now consider a rate that's applied on a per-origin basis someone intuitively someone has to see uh or enforce some state based on a specific client and a specific origin but we did not want that state to reveal anything about a specific client going to a specific origin like you know my my laptop going to example.com or whatever"
  },
  {
    "startTime": "00:40:00",
    "text": "um so uh we're trying we're aiming for something that's close to like oh db and odo and sort of style where one party sees like half of the half of the equation okay so functionally what the issuance protocol does in this document is it extends the basic issuance protocol with a couple of new properties and features um the first of which is that the issuer that's actually producing a torque and necessarily learns the origin associated with the token challenge because this is necessary because the rate limits that are sort of enforced in the system uh potentially differ on a per origin basis so the issuer needs to learn which origin is this token request for so i can pick the right limit and and respond accordingly um a testers in this uh in this protocol have a uh much more responsibility than they do in the basic issuance protocol um beyond just relaying requests and responses back and forth between client and issuer they now learn what we refer to as a stable mapping or as sort of uh described earlier the rate limiting context on a uh that is specific to a per client secret and a per origin secret and that word secret is important as a tester doesn't learn like client a is going to example.com it learns client a is going to random thing and then client b is going to different random thing um and this state is necessary uh because this in this particular design of the solution we have the attester is the entity responsible for enforcing the rate limits on uh per client and per origin on a per client of origin basis which we think is a reasonable trade-off given all things considered um uh and also differently from the basic issuance protocol token requests can now fail because if you try to issue a token or request a token you've exceeded your rate limit the tester will"
  },
  {
    "startTime": "00:42:02",
    "text": "as actually happens in the protocol it returns an error to the client okay so uh functionally just sort of walk you through what the issuance protocol does at a very high level without describing any of the technical details internally those are best kept to the draft uh this is just the issuance protocol the redemption protocol remains the same uh you'll notice on the left as input to the client there's some public keys there's a challenge there's some other things um uh specifically a secret key skc that the client maintains for uh sort of its uh for all of the rate limited requests that it has to um or that it wants to access through a particular tester because that's that's the perkline secret um but anyways uh there's a request response flow between client to tester and issuer and what the tester does uh using this request response flow is basically compute the stable mapping or compute the rate of any context um decrement uh account associated with that rate limiting context and then respond to the client with an error or with the token request accordingly and there's a lot of like complexity in it or there's a lot of stuff that happens behind the scenes to actually do this um this actual stable mapping computation um and do it in such a way that the like the a tester can't like try to figure out what the client was after um uh with without the client's active participation mary if it's okay um can i take questions at the end she's in the room i can't hear okay thank you miriam um uh as a result of this particular design um the the way state is split across the the various participants is as follows the so as before the basic issuance protocol the origin keeps uh sort of a constant amount of state in particular it just keeps the token associated with uh the token or the public key necessary to validate tokens"
  },
  {
    "startTime": "00:44:01",
    "text": "uh clients maintain uh state that's on the order of uh the number of servers that they want to request a testers now maintain the most amount of state on the order of the number of clients multiplied by the number of servers that all those clients are accessing and issuers only maintain state that's on the order of the number of servers that are actively configured for this particular issuer um there's i guess looking at the privacy properties um uh as before it's not possible for origen or tester to link any two requests together to the same client so we'd sort of maintain parity with basic privacy pass here but as i said the tester does learn this sort of state this rate limiting context um based on a per client and prohibition secret um a rationale for why this was reasonable was that the tester's already in sort of a privileged position with the client's ip address and the split tester issuer model and as a result it's sort of already trusted to handle this sensitive information and has less incentive to misbehave because if it was gonna if it wanted to just do bad things to the client you could just forward the ip address onto the issuer um okay so there's i think there's two uh there's one sort of fundamental open issue that's worth some analysis and in consideration the first of which is um uh this this sort of state that the attester learns is effectively equivalent to access patterns that clients might have in terms of like you know using browser in the web it's unclear how much leakage is actually um possible or revealed through this access through this access pattern um and in particular if an attester could combine this information with certain auxiliary information for the purposes of you know potentially learning what origin a specific client was after there's another open issue that we think we have a technical solution for it"
  },
  {
    "startTime": "00:46:00",
    "text": "would be we would have better technical solution if we had a partially blind signature um but we do not yet have one um so we have a different solution to it um details are issue 218. uh as a status update um to assist with the adoption call we have two interoperabilitations one of which is the open source implementation that tommy referred to earlier um you can test it out it supports all the basic insurance protocols as well as a very limiting one and we do have security analysis uh that was done it was submitted for peer review uh unfortunately it did not capture this um this issue just deleted on the previous page um but we're working right now to update the the analysis to make sure it is taken into account and that our mitigation does actively present it um and we'll hopefully update the work group with that analysis when complete okay that's it um thank you for letting me run over i'm sorry miria um i could take your question now jonathan um does this uh consider civil attacks or are they considered here what would be the civil attack in this case so i'm a client and i want to overwhelm an origin so i just say to the a tester oh i'm a new client now uh give me a new bucket i have now 15 buckets and i can do much more traffic yeah so uh one of the assumptions is that they access the attestation that's done between client and the tester prevents that from happening so you can't just like endlessly spin up new new identities as a malicious client and do that sort of civil attack of course if like your test attestation does not ensure that that happens then there's an obvious problem with this so um the the protocol assumes that the client a tester relationship enforces uh or prevents that from happening so for example go ahead buy ip address so it could be by a p address it could be"
  },
  {
    "startTime": "00:48:00",
    "text": "based on like device attestations you have to like you would have to have distinct devices in order to do this it depends on what type of attestation is done between the client the tester thank you mia columbia thanks for the overview um i have a question about so in the in the architecture you propose the um the rate limit is provided from the issuer to the attester and the other option would be to actually provide the count from the attester to the issue and then the issuer could drop the packet did you consider that yeah actually that was how the draft was previously written we had basically the tester send the count to the issuer but we determined that that leaks too much information in particular based on the counts that the issuer sees they could figure out whether or not two requests for the same client or not so as a simple example imagine that the issuer just saw like n requests in a row in sequence and every single time the count kept going up it would just conclude that these are obviously from the same client yeah i mean yeah so you're also leaking a little bit of information because you provide the origin count and it when it's always the same i mean it's probably always the same for many religions or whatever but like there is a lot like yeah it's a little trailer okay yeah yeah that was that was the the crux of the trade-off um nick30cdt i apologize if i'm repeating this question but um have we established evidence from um people doing this sort of um account creation or something like that that this uh metering will be effective i i can see how this is effective for the um new york times paywall and and if you want to come up with a complicated scheme to to do that maybe that's fine but um i sort of get the impression that if you're in the"
  },
  {
    "startTime": "00:50:00",
    "text": "abuse case there's not going to be a single threshold where where sort of yes no is useful but you might want to know the the number uh because you could you could you know um escalate or something that that's not always going to be like good or bad in those abuse cases but that they would want to just use that as a signal so so do we have a sense from those use cases that um a single threshold is going to be helpful i think this is still something that's under discussion in the anti-frauds community group um uh there's a lot of uncertainty with respect to you know what are the useful signals that can feed into some anti-abuse system um and what role like different types of attestation and rate limiting plays in those scenarios um uh i so i hope the conversation and an answer to your question evolves in in that group which is where sort of the practitioners are iterating on you know what is the value of this as a application specific thing um but uh i don't have an immediate answer for you because that group uh you may be you i mean you you're actively participating in it and has many conflicting opinions uh from different people so we'll see how things feel okay i'll make sure we can see that there thanks yeah to try a minute just like another author we have yeah obviously we can't know what how everything will play out but when we have discussed uh with various anti-fraud systems that would be interested in using privacy pass in general essentially like there's a spectrum from like the basic privacy pass that has no rate limit guarantees at all other than client implementation and like what they would have today with actually you know like here is your ip and here's your cookie and i can like track you and like this falls somewhere in the middle so like there are clients or origins for whom this will improve their ability to use this as a meaningful fraud signal it will not allow them to do all of the"
  },
  {
    "startTime": "00:52:00",
    "text": "things that they could do if they got an exact count for a client and re-identified someone but it it increases the number of sites that can use the use privacy pass meaningfully today and we'll see if we need to go further than that but this is a useful jump yeah i sort of view it as a another tool uh for them to use a tool that discourages use of like existing uh tools that might have like a bad privacy posture with respect to the client like tracking based on the ip address for example thanks for thanks for that discussion uh yeah just reminding the group this document is subject to an open adoption call that will end in about two weeks so please do comment on the mailing list and share with the group your opinion of whether this document is suitable for adoption and now we're on to our final section okay thank you um uh so this is this is just a reminder of a draft that we a couple of us have written on the side um after realizing that there's this common problem that's emerging in many many different application areas inside the idf oi privacy pass um in particular on um uh in an attempt to distill all the different ways that you know people deal with key consistency and discovery and practice um and uh that's the intent that's the motivation so this is not like a you know tremendously new update to that document but uh just a reminder and uh hopefully at the end we'll talk about like what we can do with this if we want to do anything with it okay um so as i said there's a number of protocols in the itf right now that uh actually depend on you know clients having uh some consistent view of their keys so they don't when actually using these systems they don't reveal information that they shouldn't reveal"
  },
  {
    "startTime": "00:54:00",
    "text": "um to a malicious service provider privacy passes one particular case where you want the issuer verification key to be consistent across all clients that are interacting with that issuer uh http is another one where you want the gateway's public encryption key to be consistent and it's like configuration in the url and all these things that provided that information but it's there um tor is another example where you want relay public keys to have be consistent across all clients and fundamentally there's like two common requirements for these different type of systems which i'll call unlinkability and authenticity uh unlikability is informally that you know servers can't link usage of a specific key to an individual user and that authenticity is that when the clients are using a key it's a key that was intended for that specific server it's not like key the key that someone else owns that is not the intended server so why is this important um imagine you had like a scenario where there's a bunch of clients interacting with some server the server makes its key k available to those clients and the server uses that key um sorry the client uses that key a client uses that key produces a function based on the key and sends the the output of the function to the server the server um assuming all clients have a consistent view of this key the only thing it learns is that this was sent from one of these end clients in the in this particular set if however the server managed to distribute the key in this particular way where one specific client has a key k1 and all other clients have a different key usage of a key might reveal what the specific client was that interacted with the service which is not great from a privacy perspective and something we'd like to avoid so consistency here means that all clients again have the same shared view of the key for the particular server there's also authenticity the other informal principle um so imagine you had now an adversary that sits between the server and the clients where the server"
  },
  {
    "startTime": "00:56:00",
    "text": "makes its key available it's authentic key available but then the adversary slips in its own malicious key ek prime and gives that to the clients clients if they don't know any better any function computed based on this particular key k prime reveals something that was originally intended for us so in the case of like osttp that might be an encapsulated request encrypted to someone that is not the actual gateway but someone else which would not be great um so we also want to avoid this um and sort of simplifying things a lot uh basically what the draft advocates for is that um these two properties on linkability authenticity mean that every single client has a consistent view of the server's intended key and then that view is correct and and systems that actually enforce these two things we call uh key consistency and correctness systems and uh the design space for this sort of thing is huge i mean there was a discussion based on key consistent ohio earlier in the week um based on one of ben's drafts um and that like touched on one solution in this design space there are other solutions that have different like complexity in terms of operation trust model and so on um so the the actual system that's used to enforce consistency and correctness can vary based on all these external factors um are based on all these factors some of them external i guess roughly speaking the design space kind of boils down to you know these four things um uh first of which is like fetching through a fetching a key through a trusted proxy so clients that you know belong to the same annuity set would ask a trusted proxy hey give me the key the consistent and correct key for this particular service that i'm using in the server and the proxy is trusted to present the correct key you can fetch and verify through a trusted proxy which is what the double check thing uh double check spec uh roughly does um"
  },
  {
    "startTime": "00:58:00",
    "text": "you can fetch through multiple untrusted proxies or less trusted proxies and verify that you get back the same answer or you could sort of uh stick all these keys on a bulletin board and audit that bulletin board and make sure that you know uh you're getting the latest contents of the audited bulletin board um or a bulletin board here it's just a you know an authentic append only log or something like that um and these these approaches uh i'm going to walk through the pictures just to really drive the point home um these are pictures are these these different solutions again they they have different applicability based on uh your your application configuration your deployment module your threat model and trust model and whatnot um and so there's no one-size-fits-all or correct solution here um it's a pilot trade-offs so in the trusted proxy case as i was saying uh there's a proxy in the middle that gets it gets a key and then bends it of that same copy to all clients pretty straightforward in the multi-proxy case where clients might go through different untrusted proxies to ask the server to present its view of the key um clients can check to see whether or not a server is trying to lie and change keys you know in a malicious attempt to tag clients um i have here listed that constancy double check is sort of an example of this slightly different but i'm kind of the close enough in spirit that i just kind of threw it down here and then there's the bulletin board external data database which stores the keys and then clients just pull it down and there's some assumption that there's an ecosystem built around this bulletin board for verifying that everything is correct and consistent um like connexing key transparency or basically an instantiation of this concept um okay so um consistency has come up time and time again um in particular this week uh and i think we're really going to have to address it with ben's draft the question is you know whether or not"
  },
  {
    "startTime": "01:00:00",
    "text": "there's value in uh this complementary document that sort of describes the the rest of the design space um or some other pieces of the design space may not be exhaustive um uh there is no specific section uh in the privacy fast charter that to sort of address the key consistency or key tagging issue um so after discussion with the shares we basically are proposing to bring this document as informational as a working group uh like supporting document or something to be published i don't think it matters that much um to sort of uh help inform the discussion around consistency double check or other solutions as they emerge and um that's our proposal cures to hear what folks think yeah tommy yeah thanks for doing this this is clearly very important um i have no idea what working group this belongs in i don't think it matters too much we don't have one to do it in but we should do this uh the the one comment i had is if i i wonder if the approach of double check not the specific protocol instantiation but the approach of it could be mentioned more explicitly here again it kind of seems like implicit when you're talking about oh you have proxies you can go through and you can go direct and like maybe laying that out and bringing some of the double check in because it feels like different protocols like ojai you know if there is a way to do a specific ohio key config lookup that can adopt double check but other things even if they don't fit into one particular config bag like different key configuration locations could use double check and use these different techniques so we could talk about techniques here and then just let other protocols say oh yeah and then you use that technique or that technique or that technique that sounds like a reasonable approach um uh and certainly we didn't mean to like"
  },
  {
    "startTime": "01:02:00",
    "text": "intentionally omit double check variant it is as you say a different it is a slightly different technique than the the multi-proxy discovery um i just kind of i like vastly oversimplified things on this particular slide so we could add a section there alternatively depending on like ben's preference here we could like just take all this content like put it in an appendix in his draft as like here are alternative solutions that if you didn't want to use double check or whatever i don't think it matters that much um uh although to tommy's point it does seem like it might be useful to have a place to somewhere to point and say you know this is the consistency mechanism i'm using in a specific deployment or in my specific protocols so uh the authors of this draft have requested an adoption call um for uh for this draft in the privacy pass working group uh i don't believe such an adoption call has been made uh yet but the the chairs will consider that um and and if an adoption call is started you'll you'll see the announcement on the list so uh please do review the draft so that you're prepared in case such an announcement appears we are over time and we've completed our agenda thank you everyone for participating and making this a great privacy pass session is"
  }
]
