[
  {
    "startTime": "00:00:22",
    "text": "hey pops 8:30 everybody welcome to NBC sorry well testing okay welcome to net BC so first off note well should everybody should be familiar with this but for this particular working group please make absolutely sure you\u0027re familiar with it because the goal this working group is to produce royalty free video codec so we definitely need everyone to pay very close attention to the IPR rules in the note well start blue sheets around I think we\u0027ll need one so I\u0027m not gonna bother with what always takes the majority of the ten minutes of chair time note taker we need a note-taker volunteer very tourist notes it doesn\u0027t have to be a every detail just major decision points volunteers anyone uh on the media Co want to volunteer for taking notes in jabber room volunteer for taking notes we\u0027re have to impose on someone now Oh Nathan did thank you very much Nathan took excellent notes last time or the time before appreciate that we have a jabber scribe volunteer that\u0027s much easier just relay the room do you want to do it since we\u0027re up here anyway and now we have our guest co-chair thank you very much man all right so quick agenda Bash just gonna spend a few minutes probably not ten on on the status of the current documents and then Thomas is going to give us an update on the test document and Steiner will give us an update on Thor and 81 progress and comparisons followed by Tim giving an update on dalla 81 transforms and then Luke is "
  },
  {
    "startTime": "00:03:26",
    "text": "going to give us the latest chromo prediction from luma as it\u0027s being used in 81 any changes people want to make to the agenda any other items to bring up if not let\u0027s go on so quick recap of where we are on our milestones we have a milestone for the requirements document that that was a workgroup last called but um because it\u0027s being used by some other standards body or some other industry consortium there may be some substantive changes to it that we foresee coming up pretty soon so we\u0027re holding off on the Shepherd\u0027s right up for now and we\u0027re waiting to see if there\u0027s gonna be some substantive changes before we progress it on to the is G hopefully that\u0027ll conclude within with in November so we\u0027re gonna update the milestone 2 to November I hope to get that done before the end of this month and the testing document we\u0027re gonna update that milestone as well because it\u0027s a not going to be concluded anytime soon we decided to keep it alive as a living document while while the codec candidates are progressing because we expect the test methodology is going to keep evolving so we don\u0027t want to freeze the document pretty much early so once we have a candidate we\u0027re comfortable with and a testing methodology we\u0027re comfortable with as being pretty stable then we\u0027ll we\u0027ll move it on the chain for the actual codec candidates we want a single merged codec and we lack that right now so we do not have a candidate yet for the milestone for either the codec spec or the reference implementation so we lack a merged candidate or merged codebase so you see Tim and stun are presenting different tidbits but not one consolidated codebase or standard so that\u0027s that\u0027s the glaring issue we need to deal with his workgroup and try to bring out the closure and Austin\u0027s going to get pushed out to July so hopefully by then we\u0027ll have more clarity on on how to come up with a converged candidate and then finally there\u0027s the milestone for carrying this codec inside of containers storage formats so we\u0027ll push that out to end of 2018 because that work hasn\u0027t even started yet any comments on the milestones or the proposed changes to them alright so let\u0027s go on to first item on our agenda Thomas so you\u0027re gonna stop us on the mu√±eco yes he is can you come on over to the queue "
  },
  {
    "startTime": "00:06:35",
    "text": "all right can you all hear me no here NC this is I\u0027m Thomas steady from Missoula this is the idea that VC testing craft version six was is there something explicitly on this there\u0027s a just two toys yeah so the testing draft has had very few changes between this version and the last version there\u0027s really only two things one of the small failing changes and changed it\u0027s almost your little faint K is there any microphone pick up that you can get ramped up you one second is that better much better thanks okay um there\u0027s a small command-line changes and intestine arrows so next slide so the first one is CLI parameters we\u0027ve made some small changes to the codecs that we test one is that we removed a constraint for our leg and frames from olymic died we previously had divorced this 225 but it turned out that was less than ideal value it basically buffered way more frames and we technically needed to on the encoder so we decided to drop that out of the command line as you can see that command line is pasted on the bottom there we just leave it up to the encoder to pick the maximum reasonable value which i think is nineteen in the current encoder but the general idea is with the command lines we want as many me you know as few possible constraint we will not impose just a minimum constraints possible at encoder big the rest so that\u0027s in the right direction so is that supposed to be crossed through the lagging frames equals 25 yes that Gesser do they do that should be removed from the command line yep I pasted okay I\u0027m in there is to show that it was there but yes delete that that\u0027s all ideally I\u0027d like to actually drop the auto ultra studying as well and that should be automatic but didn\u0027t change the code basis to do that yet so that\u0027s all second slide slide number for a dollar problem is that person we basically only allowed two very slowest encoder notes to be used for a dollar this was the default or and for I think two where we had a text file configuration and for a one that killed a CPU used equals zero and this became increasingly problematic as you know and I think in particular got much much slower over time was their current objective one vast suite of videos that\u0027s in this testing document that "
  },
  {
    "startTime": "00:09:36",
    "text": "take that took who was taking upwards of eight days to run the clips that became impractical for testing so obviously added an exception that allowed us to run faster test settings under limited so those circumstances basically if you only compare an element to a limit and you were just comparing a code change that to change as a single feature you\u0027re allowed to turn off a couple of searches and figure the searches for exportation and exportation types that speeds up a lot and as long as you specify I did this then it\u0027s a reasonable usage this is not certainly ideal I\u0027m working on a better solution for this I think a better solution will actually involve custom you know speed parameters to the encoder that are better tuned to match a real usage the one bad thing is that we by searching less partition types and sizes we kind of bias towards smaller partitions and not the rectangular partitions which is could could affect some tools in the negative way so we\u0027re trying to find something better but this is a stopgap for now and that is basically the only two changes I\u0027ve made to the document I\u0027ve not had to change the test set or to any of the videos at all those have stayed pretty much the same so this is Moe virtually from the floor mic and be lazy since there\u0027s no queue just stay up here the the testing methodology itself though not the infrastructure with the testing methodology itself still specifies that the testing should be on the maximum compression not not any other slower mode yeah so the there\u0027s basically a section at the end at a testing draft that basically says you know different things you can test write it so you could do like subjective tests and you can do across codec comparisons and there\u0027s like objective tests for tools and that one is the one that we allow into not used a very slow mode basically by turning off the exportation search oh but if you\u0027re doing like across codec comparison you obviously don\u0027t want to do that I mean I think I understand why it needs to be done for practical reasons in the infrastructure but I I wouldn\u0027t think we want the spec to say this is how we test ideally we didn\u0027t have an infrastructure problem we would test for max compression right yeah and that\u0027s certainly a issue with that doing this is a very highly codec specific and implementation specific setting so it\u0027s possible that which she does does not belong in the testing document and that could be a valuable "
  },
  {
    "startTime": "00:12:38",
    "text": "feedback okay I mean all codecs will have knobs to adjust the compression speed trade-off I think the it\u0027s important for the testing document to state that the objective comparisons will be done at max compression otherwise it\u0027s a pretty hard it\u0027s another dimension another layer of curve another dimension of curves that we have to look at to evaluate performance I know we\u0027ve presented some of those and they\u0027re useful Steiners a lot of the speed complexity trade-offs before but I think it\u0027s difficult to put those in the testing document as something that the candidates will be evaluated on right at the same time so the the one for thing is I don\u0027t want to evaluate if I specify this I don\u0027t want to evaluate the candidates in terms of you know meeting the requirements criteria on this this is purely for like individual tools and drafts particularly I do want to know like people I think people later will probably present with CPU used settings with arts ro and I would like a way to normalize this so that people people use the same CPU use settings and stuff if they need it for speed reasons but yeah this what we could do is we could just say you know they are not specified this in the draft maybe say that you know you may you may use faster speed settings as long as you\u0027re justified it like external to the draft basically try to be consistent that\u0027s also a reasonable way to take it if that\u0027s what people would prefer I can change it Jonathan Linux I mean it seems like with any codec you know you know you can go your code can go to absurdly slow right I mean this is doing I\u0027ve heard the phrase frames per day as the speed I mean anything could do exhaustive search every possible bit stream to see which one best matches you know in the extreme keep your video in the extreme case and that\u0027s just you can\u0027t say will use whatever the invitation chooses to make the slowest possible encoding and I think there has to be some threshold of this is just absurd we\u0027re not going to test this because it has no practical use okay what other people think um I\u0027ll just know that that that absurd max compression is what\u0027s traditionally used in other other codec groups but uh I\u0027m happy to entertain more practical things in our testing draft [Music] and the other strong opinions one away to other I mean based on a current feedback that sounds like I\u0027d like to specify this but made only for the a women testing and maybe not too explicit the exact parameters and so if anyone "
  },
  {
    "startTime": "00:15:38",
    "text": "but well I\u0027ll change it basically so it allows us to do this but it doesn\u0027t specify two parameters and seems like the middle ground I hear three ownage x2 that let me know otherwise I\u0027ll do that like you have no objections go ahead okay that\u0027s what I\u0027ll do for the next draft version that\u0027s all I have to present so any other comments or we can be done with this and I think we\u0027re good thank you all right next up ah [Music] okay hello everybody I have to have it closed I think I have some please what I have are some updates on on the Thor College and also have some updated charts on the performance and compression trade-offs so this time the for github has actually been updated it hasn\u0027t been updated for a while so I guess that\u0027s about time the main change is that I\u0027ve added the support for the CDF filter through which was which has been adopted in everyone what was I hope did in everyone was the seen in past version which I presented the last time so this is what have implemented for Thor another change is that I\u0027m working on faster and simpler audio proceedeth in in authorities much faster than 81 so when I first tried to use the audio from from 81 the speed of Thor was roughly haft just for the future but I think the audio can be improved and it\u0027s much more easy to experiment with the audio in Thor because Thor is faster and I get the feedback much faster and whatever I do in Thor could be back ported into every one as well and last time I mentioned that I would like to get support for the dollar entropy colder in horror but there has been no progress on that so just see that but that\u0027s the first step towards emerged codec next like this since the last meeting there "
  },
  {
    "startTime": "00:18:39",
    "text": "have been a few minor changes to see the this is due to a face-to-face meeting we had in the IOM group since the last time there\u0027s a new skip block test what we had before it was that we didn\u0027t signal Anna strength for a future block if all the coding blocks within that filter block were skipped but the trouble with that is that you have to de call the entire filter block in order to know whether all the blocks are escaped and the Harvard people don\u0027t like that they want to start a filtering as soon as I can turn our coding book as soon as possible not wait and it until the end because they don\u0027t like the Bur for anything so it was agreed to change that so now the there\u0027s no signaling for the filter block if the coding block size is 64 by 64 meaning that there\u0027s no petitioning of that block and the calling block is skipped that makes it possible to signal the filter strength just after the Skip flag for that block that adds a slight coding overhead because it means that there will be more signaling and also possibly a slight complexity increased because it also leads to more blocks being fusion but objectively the loss is less than 0.1 percent and subjectively I don\u0027t think there is a great change it could even perhaps even gain an improvement because we get to doing more filtering next like this and also recently everyone has adopted support for 128 by 128 super blocks so that also influences how we do the Skip test CDF still needs the signal at 64 + 64 resolution during the development of sealed F which was merged with the dollar-driven to form seeded I tried different block sizes and 64 by 64 was by far the best size so we don\u0027t want to change that so for a large super block 128 by once 22 single up to four presets in order to keep the the same field the block size the details haven\u0027t been what I decided yet how to do this we need to still need to investigate possible compression impacts but the most simple thing would be just to expand the instead of testing "
  },
  {
    "startTime": "00:21:39",
    "text": "for 64 over 64 blocks without partitioning and whether it\u0027s Kip or not just to check for 128 by 128 without partitioning next slide so running see deaf instead of C of F in for does give object against just as it did in every one I see gains around 1% or a half a percent 2.2 percent I\u0027ll show the results on a separate slide shortly we see quite large gains for chroma of the 4% I\u0027m not quite sure why we didn\u0027t see again that high in in everyone so I think I\u0027ll investigate that see that does add more complexity or we will see a path though it\u0027s it\u0027s more processing but that\u0027s not unexpected I also tried running CF on top of C death that gives not much gain which is not surprising since CDF is basically a superset of C of F but I noted that if I greatly simplified the CDF I do see a def just give gains and actually it turns out that see a deaf Regan\u0027s almost all of the loss if I simplify the audio but having to an extra an extra pass or filtering I think that adds a risk of over filtering and also it adds buffer requirements so it\u0027s sealed s still attractive for a fast real-time encoder currently it\u0027s a good way to speed up for what I I think that it\u0027s much better I\u0027d rather to do work on the sea deaf audio I think it might be hard to make it as fast as to see a deaf are they all but it should be possible to come close without you to big losses next like this another change since the last meeting is in a VA one we have three filters applied in cascades D blocking see death and then loop restoration and that adds a lot of buffer requirements and again hardware people don\u0027t like that so there was a new proposal from arm to an Android contribution until Google and Mozilla to reduce the buffer requirements without stat there\u0027s an either a minimum of 30 "
  },
  {
    "startTime": "00:24:42",
    "text": "lines of banned buffers but with this new proposal it\u0027s possible to reduce that choose 16 lines next slide please so the basic idea is to some normative changes and non normative changes and non normatively possible to do some shifting of the CDL or filtering but the main normative change is that when loop restoration looks outside the super block that was produced by C death it will roll it will really deep blocked output instead the CDF outfit and that breaks the dependency between CDF and Luke restoration so the changes that has that have been proposed requires no normative changes to see that it\u0027s mostly in loot restoration and the changes have no impact on the other compressed yets results and it makes the buff line buffer requirements for everyone the same as for VP 916 lines even though we have more filled filters in every one and this is moving towards adoption along with new restoration it hasn\u0027t formally be adopted yet but it will next slide so a bit more on the encoder complexity on see death as I mentioned I was working on simplifying the audio and I think that can be improved even more just as a test how far I could get I try to restrict the 50 to do no block level singling and when I do that I still get gains objective gain similar to see lbf but I think in that case the subjective gains are still much better than sealed F because we will get the the directional part of C death so in that case the encoder will just have to select the optimal strength for the entire frame but that\u0027s a quite small search space so and some other simplifications that I have tried which work well is to select the damping used in the filter core based on the frame QP and I\u0027ve also tried to decide the number of bits to use per block based on the frame QP and sorry based on the bitrate and friend pipe and I think that it still many ways to improve the CDF audio the the reason why I think it\u0027s important to have a good are the oval see that is that in a practical and "
  },
  {
    "startTime": "00:27:45",
    "text": "coders Edith can probably replace some of the more complex tools in particular in a the warm maybe not replace but just help yeah well not replacements Tannen but if you want to do every time encoder you you probably wants you can\u0027t use all the tools it\u0027s simply too complex next slide please so these are the results that I got for adding see death in for heron comparing just doing D blocking and with the deep locking plus C death and in the low complexity case is now a 6.2 percent and the chrome app is not even better if I look at the see ie de number it\u0027s actually ten point three percent which is I think it\u0027s quite impressive even in the harder the compression is it\u0027s a six point three and in the high efficiency it\u0027s still five point two percent and three point one percent in the low that I and highly lay configurations so that\u0027s that\u0027s not bad I think next slide please and if we compare this with Z of the F so the these are the gains that we get from replacing C of F with C death this ee i ee number is 2.2 percent in the low complexity low today configuration manager ops 21.1% in the high efficiency a high delay configuration so it\u0027s not a huge difference but the main reason to add C that is to improve the actual visual quality and and in every one we did some subjective tests comparing see a path with Edith and even though the change was less than 1% in in every one people could still tell the difference so that probably points towards a real difference of at least five percent next slide this mo from for Mike again virtual for Mike a question on the the the the gains for chroma look like about three to four times the gains over all right clue I as I\u0027m serious what I would like to investigate here is whether there could be an impose in the encoder it could be probably not normative things that would have to be fixed but it could be some kind of bug "
  },
  {
    "startTime": "00:30:48",
    "text": "that see that is able to partially correct but I don\u0027t really know it\u0027s an interesting results what we could always hope it is that bug and and it will get further gains Morgan Morgan for for mark again so before you finish on CDF I want to raise one issue related to the requirements document that we expect some substantive changes in I think one of them may be related to support of four to two chroma format video and I believe CDF is one of the barriers to that because the direction search does not support rectangular blocks is there any plan to address that in any way so how it currently works is that in the case of 42 we\u0027d still do the filtering as normal for the luma plane but the filtering is disabled for chroma label all filtering or disable Direction search only it\u0027s completely disabled I think so there are ways to address that like skipping the directional part and basically doing cell death at him as a comment yeah Terry vary from forearm the the directional search has only ever done on luma so what normally happens is the chroma uses the direction that luma found when it to orient its filters and since there isn\u0027t a direct correspondence between the directions we have a luma and the directions we have in chroma when you squeeze the chroma blocks into a rectangle then we disable the filter sorry Tim your little fan at the end so what do you do for karma blocks then just disable the filter disable the directions are should totally simple the filter completely the whole filter completely because you can\u0027t disable the direction searches that\u0027s only ever done on Loula what you could do is just assume a fixed direction which is what Steiner was saying and and just always filter with that direction and that would be essentially like Co PF yeah also lucious but currently I don\u0027t think in those solutions any of those solutions are going to into a v1 and it might be might be because people don\u0027t care I\u0027m sure how important it is to have good performance for 42 well I mean like I said I think there there may be some there may be some changes to the requirements to make 42 more prominently supported okay now well it is order but the compression will surface lightly "
  },
  {
    "startTime": "00:33:49",
    "text": "okay last time I had some graphs showing the compression and complexity trade-offs so if you want and I have updated those I also had some graphs showing comparison between the different codecs and I haven\u0027t opted those since the difference in Thor and isn\u0027t that big it\u0027s just 1% so what I\u0027m going to show are the compression speed relationships using our compressed yet with the objective 1 post test set and since ITF 99 there\u0027s an improvement in every one of about 5% and the encoder runs at roughly half the speed I\u0027ve been using the loaded a configuration and the BDR anchor is a the one as it was in July last year which is roughly equivalent to vp9 and next slide please so starting in July last year at zero of the compression the be dynamic goes down which is good so at the last meeting we had the VBR gain of about 20 percent and that is now about 25 cents and the graph has been steadily dropping with the additions of new tools in every one and there are still some tools left not yet enabled so I expect this to drop slightly more so we\u0027ll see next slide please and this is the complexity history note here that the y axis is logarithmic and the y axis is the frames per minute not they not seconds but minutes it\u0027s it started at around 15 last year in July and is now round one frame a minute so there\u0027s a change of the factor of 15 and it seems to be flattening somewhat but again this is a logarithmic scale on the y-axis I think this shows that the compression gains that we haven\u0027t seen in anyone don\u0027t come for free it has a big cost so if we compare vp9 with everyone I think currently everyone is basically a continuation of vp9 if you plot it with difference "
  },
  {
    "startTime": "00:36:49",
    "text": "complexity settings so you have a big toolbox and as you add more tools to the codec you get compression gains but you also get that speed penalty and the question remains whether that 2 vols box is a better tool box not just a larger so we could replace tools in Li benign whether simply better tools and get better performance with the same complexity I\u0027m not sure but if if everyone gets set opted and used as a [Music] standard people will work on this for years and and it will probably eat at night but we can\u0027t prove that yet ok this is what I had just give a quick thumb in the air of what the complexity is absolutely right now relative history but absolutely the vp9 roughly ah certainly more than 100 times I guess yeah so I guess it\u0027s a crystalline so it will probably it hasn\u0027t been a great focus to speed up everyone so that will probably get more focused as the actual tools are finalized but yeah right the reference and gallery isn\u0027t that practical yeah we can\u0027t simply I think the the specification says that we\u0027re supposed to run 4k sequences and bit but we can\u0027t practically do that now so nobody has been presenting the test results according to the specs actually because it\u0027s simply too slow and so just to know that the those numbers were for the encoder and were roughly what the decoders yeah the decoder speed is roughly 1/4 since July 2016 it\u0027s about 4 X\u0027s complex is vp9 four times the complexity I think it\u0027s closer to 16 times but I think the main reason for that is there\u0027s still some simsim de optimizations lacking in vp9 so I think four times is a more accurate number thank you thank you [Music] "
  },
  {
    "startTime": "00:39:54",
    "text": "all right stand in the pink box I\u0027m Tim Terry berry I\u0027d be presenting work on the dollar transform design this is joint work with Nathan eggy and Monty so although I got this stuff started a few years ago those two have really been doing the bulk of the work lately so I think most of the credit of the recent developments goes to them next slide um I\u0027m going to talk a little bit about what our goals were in designing transforms for dello one should be pretty non-controversial as we wanted an exact integer implementation it\u0027s just the way that video codecs have worked ever since 264 there\u0027s lots of iterative prediction with unstable filters so you want an exact specified implementation so that all all decoders agree and there\u0027s no drift um we also wanted to be able to support many different variations of the transforms so low bit depth hide the depth both square and rectangular discrete cosine transforms discrete sine transforms etc we also wanted high accuracy so this is this is 2017 as you just heard we can have lots of complexity so in times past people were afraid of having multiplies and while we\u0027d like as few multiplies as possible we can have some multiplies in there if it gives us more accurate transforms that said we want to keep software complexity as low as possible in particular paying attention to how things would be implemented in Cindy and at the same time we want to have reasonable hardware complexity which means we need low latency for small transform sizes and for all these variations we want to keep transform reuse and embedded designs in mind so that stuff will come I\u0027ll come along as you go through some of the slides here next slide um so just just to start us off this is the the four point discrete cosine transform for each sixty-four it is very low complexity so you can implement this with 8 ads and two shifts it has a few drawbacks one of them is that that it is a non-uniform scale transform so the coefficients that you get out even though the discrete cosine is this unit very transform where all the basis functions have the same magnitude of 1.0 this gives you out coefficients that have different skills that you then have to multiply by and that usually gets absorbed into the the quantization step so you know say oh we\u0027re saving one multiply but in reality in in the way encoders are designed today we do rate distortion optimization "
  },
  {
    "startTime": "00:42:54",
    "text": "with several different possible quantization levels for all the different coefficients so you actually need to do several multiplies in there in order to get a consistent estimate of distortion that backs out this scaling factor and that those extra multiplies get multiplied by the number of different options that you search in the encoder which is we just saw you know this can be quite a lot one other thing is is that when you start having larger and larger transforms so this is only a four point transform you know you start going up to say 64 point transforms you actually need a very large table of constants for all these these scale factors so we added the new goal that we wanted to have uniform scaling for all of our transforms and you know that that will cost you four multiplies in this design but as we go to larger and larger sizes it turns out you can achieve this with much less than one multiply for coefficient all right next slide so this is the vp9 four-point discrete cosine transform and I may pick on vp9 a little bit today just it\u0027s not because I think the vp9 design is bad but it\u0027s actually a fairly standard textbook design for transforms but I think we can do a little bit better and so I want to talk about some of the improvements we\u0027ve made relative to vp9 just because vp9 transforms are the ones that I know the best so this is the 4-point DCT it actually has six multiplies they are full 32-bit products so if you look at the bottom there we were actually taking two of these products and adding them together so we need the full 32-bit result in order to do that and then it additionally has eight adds two of those happen at 32 bits and then four shifts all right next slide so there are a few avenues for improvement one is is simplifying the multiplies so if you looked at the 264 design like we could just scale the outputs of those that transform then it would only cost four multiplies instead of six but the 264 design is not a real DCT it\u0027s only an approximation to a DCT so it would be a little bit less accurate but we\u0027re going to see in a bit we can actually do just as well with an accurate transform so the other approach for improving things is has to do with scaling so the vp9 DCT adds this factor of a square root of 2 relative to a unitary transform um and in fact it turns out as you make the transform larger and larger each time you double the size of the transform it adds an additional factor of of the square root of 2 so this is this is sort of okay if "
  },
  {
    "startTime": "00:45:54",
    "text": "you take the log of the width on the low that the height and that comes out to be even then you can just correct the thing with a shift but now we want to use rectangular transforms like an 8 by 4 transform or something along that and now this scale factor becomes odd and so we can\u0027t correct it with a shift we actually have to correct it by doing one multiply for coefficient in order to get something that matches the same same scale as all of our quantizers all right slide so where does this scaling actually come from structurally next slide um this is sort of the the textbook factorization of a type two discrete cosine transform so it starts out with this stage here on the left we\u0027re basically computing sums and differences of pairs of pixels sometimes called plus 1 minus 1 butterflies or something to that effect and then after that you can split the thing into a smaller discrete cosine transform and a smaller discrete sine transform alright slide so these butterflies here at the beginning are our nonunitary like if you you compute equivalent basis functions for that and say what\u0027s the magnitude of the basis function it\u0027s 1 squared plus 1 squared is is square root of that is the square root of 2 right so that\u0027s where that factor comes from next slide and because this is recursive there\u0027s another one inside there and as you as you expand the transform by a factor of 2 each time you get an additional one of these these factors of a square root of 2 and you also wind up having to do something in the discrete sine transform that would is also expansion area like this um if you want the scales to be uniform all right next slide so we\u0027d like to get rid of this extra scaling so that we don\u0027t have all these extra multiplies in our rectangular transforms so one way we can do that is we can use multiplies and in fact if you go back and look at vp9 s for point DCT they actually already do this so I don\u0027t if you flip back to the slide 4 so this step up here um is actually would be the same thing as a plus 1 minus 1 butterfly but then it has scaled the outputs out after that so that they match the discrete sine transform at the bottom there so that\u0027s that\u0027s one way to correct this going but that only got rid of it out of one stage and we\u0027re getting a set of these on at every stage so that winds up being kind of expensive so another approach is we can restrict ourselves to only using shifts and ads and use asymmetric scaling I\u0027ll describe what I mean by that in the next few slides so we have "
  },
  {
    "startTime": "00:48:58",
    "text": "there are basically two different options the the construct at the top there computes a sum and difference where the the output of the second component is have compared to what you would normally get and then the next one computes a sum and difference where the output of the first the first output is half - compared to normally get and as you see you can do this with with just by adding one shift in between the the two the two additions or subtractions so what happens is instead of instead of doing an addition and subtraction and having both of the scales increased by a factor of square root of two what we\u0027re actually doing is increasing one by a square root of two and decreasing the other by a factor of square root of two so they become asymmetric but overall you know the scaling is unity so like the determinant of this this transform as a whole is still 1 and then we can cancel out this asymmetry in subsequent steps so next slide and you do that we use constructs like this so the first one there computes a sum and difference where it halves the second input and then the second one does the same thing except it has the first input so these these kinds of constructs can can cancel out the asymmetry from the previous steps so next slide we\u0027d also like as I said to simplify the multiplies so all of these multiplies come from plane rotations between two variables so basically in all of our transform factorizations we\u0027ve decomposed it into a series of these plane rotations where we\u0027re taking two values and we are rotating them by some amount so we can actually you know instead of doing that as as a matrix multiply here we have four multiplies in two additions we can get rid of one multiply and instead add an addition by using a construct like that at the bottom here all right next slide so we can actually also arbitrarily scale the inputs and outputs of these rotations so just multiplying through um you can instead derive a series of steps which which looks like this and the important thing to note is that that all of the all of the complex stuff there is basically just reduces down to a constant and so it\u0027s it\u0027s x0 minus a constant times x1 x1 minus a constant times p0 and then p0 minus a constant times y1 so this becomes very simple and and can let us absorb scalings in the multiplies as well all right next slide so the advantages of doing this we get 25% fewer multiplies in general multiplies are much more expensive than additions in in software as well as hardware they all have this structure of "
  },
  {
    "startTime": "00:52:00",
    "text": "X plus a constant times y so when we do that in fixed point it\u0027s going to be X plus a constant times y plus a rounding offset and then then we\u0027re going to shift off that you know shifted to the rights and only take the high part of that multiplier output so the advantage of doing this is that all of our 16 bits in D stays in 16 bits so we don\u0027t actually need to compute full 32-bit products we only need the top half so if we actually had to go compute a full 32-bit product we could only do that in with half the throughput in a fixed size Sindhi register so ssse3 and neon actually both have instructions for doing exactly this kind of multiply so it\u0027s a single instruction that will do the multiply add the rounding offset and shift the product over to the right and so none of that has to has to expand out to a full 32 bits so that whole thing fits in in 16 bits next slide so putting all those things together we get a transform like this so we start off instead of having plus 1 minus 1 butterflies we have these these asymmetric scaling steps we use both types so that we get scaled outputs of in different directions that we then cancel in the next step and then we have us our rotation down there at the bottom gets replaced by one of these scaled rotations using only 3 multiplies so we have one more addition and three more multiplies than then the the h.264 transform but we have uniform scale all right next slide the other thing if you counted very carefully there are 3-1 halves on the board our unless on the slide but I only say two shifts at the top and that\u0027s because two of them are actually the same value so as you as you get larger and larger transforms you\u0027ll also be able to share more and more of these shifts between the stages like this and that\u0027s just because of the way that we arrange them all right next slide um so expanding that out we can do an 8-point DCT next slide or 16-point DCT and that keeps going up to 64 points I yeah it probably would have taken a few hours to make the drawings for that so I didn\u0027t do that but took a few hours to make that one for these but the other other point to make is these things do have embedded structure so both the endpoint DCT and the endpoint discrete "
  },
  {
    "startTime": "00:55:01",
    "text": "Coast are discrete sine transform are embedded inside a discrete cosine transform that is 4 times larger so that embedding actually skips a because of the asymmetries so if you only go up one level then then we\u0027re actually taking asymmetric inputs and so it\u0027s not exactly the transform you need basically need two sets of right yes yeah you need everything from there yeah you even set in an odd set essentially all right next slide so a few notes on accuracy so all of these these right shifts and multiplies introduce rounding errors we want to keep those as small as possible so we can the the way we go about this is that we shift up the input by some number of bits before we do any of the transform and then we do the full four transform quantize code D quantize inverse transform and then on the other end when we finally get down to pixels we ship down the output again so how much do you shift while we found diminishing returns at about four bits and that was enough to make all of the discrete cosine transforms match a double precision floating point implementation after rounding to the nearest pixel value so with just a four bit up shift we get the error down below one half of a pixel step for 8-bit input all right is that open for Micah is that compared to a the same approximation or compared to a full full DCT implemented in double floating-point that\u0027s that\u0027s a full you know bog standard DCT you know not any much straight from the formula giant matrix multiply implementation that\u0027s good what about what um so the error winds up being the same for higher improvements and go to the next slide I\u0027ll talk about that and something uncouple sides yeah that\u0027s it basically the the accuracy is less important for higher bit depths because what you actually care about is accuracy relative to your quantizer and so higher bit depths use higher quantizers to get similar bit rates so we shift up less for higher bit depths on basically 10 bits is a two bit shift in twelve bits we have no that shifts so it injects a little bit more noise but it doesn\u0027t matter but as a result we can use the same transforms for all bit depths all right you know from the floor again so I think that also means you can use the same Sindhi one right for all the input that\u0027s that\u0027s that\u0027s correct you can use the exact same implementation that\u0027s nice alright go back yeah next slide so "
  },
  {
    "startTime": "00:58:08",
    "text": "how does this compare with vp9 so vp9 also shifts up the inputs but by not as many as four bits and then it shifts down the outputs by more than four bits and actually has to do it sometimes in between row and column transforms too and that\u0027s because they have this extra factor of a square root of two that that they grow by every every transform size so what\u0027s actually happening is is the scale these vp9 coefficients grows as the transform progresses so any rounding errors that you introduced early in the process get magnified as that scaling increases whereas in dala all the stages have the same scale so all of the rounding errors are injected at the same level and they do accumulate but we don\u0027t magnify them all right next slide that\u0027s the one we just did so another important point to talk about is the difference between scaling and dynamic range so everything here has has orthonormal or unitary scaling right so the magnitude of the basis functions is 1.0 but the dynamic range of the output still increases so the dynamic range here I mean the minimum or maximum output values you can actually have so all of your unitary transforms are essentially n dimensional rotations and you can think of the input as a big n-dimensional box and the length of the diagonal of that box is going to be longer than the length of any of the edges so as you rotate it you can get larger values than you started with um in fact it\u0027s by a factor of square root of 2 every time and doubles which you know is in addition to the scaling that vp9 does and in it they\u0027re not the same scaling so we still have this factor of square root of 2 and the size of our coefficients but that\u0027s okay because the l2 norm is still preserved by the transform so the question you might ask is how big can the outputs actually be next slide so with a 4-bit upshift all the transforms with 64 pixels or less fit in 16 bits so that\u0027s a 9 bit residual 4 bit up shift and then 3 bits of dynamic range expansion which is half a bit for each of the powers of 2 and 64 so that includes your four by four or four eight eight by four eight by eight four by sixteen and sixteen by for all of the column transforms all the way up to 64 point also fit in sixteen bits so that means that that 16 bits is the maximum size that you need for a hardware transpose buffer so in between row and column stages the hardware has to buffer the coefficients so it can transpose them which is a fairly significant gate cost so being able to keep that small as nice um it also means that when you\u0027re writing Cindy you can write a Cindy for the row transforms and it used to be simply for the column "
  },
  {
    "startTime": "01:01:08",
    "text": "transforms and it all fits in 16 bits and for all sizes and then you can have a separate version once things start going large at the 16 bits so comparing to vp9 they have larger intermediaries in the transforms but they always shift their final coefficients down to fit in 16bits so we think this is a Miss optimization it\u0027s it\u0027s actually just as easy to do this shift down and pack while you\u0027re doing quantization so we we have not tried to do this extra shift at the end it also helps avoid double rounding and and simplifies rate distortion optimizations is you don\u0027t have to have any special cases for different scale factors depending on your block size all right slide a quick question move from floor mic again so you don\u0027t you don\u0027t have a fixed shift between the row and column stages you you accumulate a certain number of pixels total pixels first before you do your shift um so we don\u0027t have a shift between the row and column stages at all who would if you have bigger than than a 64 pixel so if you have bigger than 64 pixel then we start going to wider Cindy once once you once the values start exceeding 60 minutes so you just go bigger than 16-bit intermediates yeah well I would never shift down we never shift down okay I mean you can shift down after you quantize right because when you quantize you\u0027re gonna have the same values you would have had regardless of what your shift was but yeah I mean that the point is you\u0027re gonna have to go you\u0027re gonna have to go up to 32 bits in the transforms at some stage um because we\u0027ve eliminated this extra scaling we do that at a later stage than vp9 does and also because we don\u0027t do extra of shifting for high bit depth we do it at a later stage in the vp9 dose um so we can keep you in 16 bits longer um but yeah I mean at some point you do have to go to go up to 32 bits and that\u0027s true both in vp9 in and in us you know for the larger transforms alright so a few notes on reversibility so when you have steps of this general form where you take a variable and you add to that variable sum function on all the variables except the one you\u0027re adding to um that\u0027s called a lifting step there can be an whirring like that that function could be arbitrary it doesn\u0027t have to be linear like as has no special properties at all but because the function is not a very is not a function of the variable you\u0027re modifying it\u0027s exactly reversible right so on on the the decoder side you can just subtract off that function and get exactly the value that you started with what that means is we can make inverse transform by just reversing all the steps of our forward transform and so it turns out that all of the steps that I have described so far that we use to build our transforms happen to be "
  },
  {
    "startTime": "01:04:09",
    "text": "lifting steps all right so so why is this good why would you want to do this so we really wanted reversibility in dala because we used lapping instead of a deblocking filter so do blocking filters have this nice property that they\u0027re low-pass on so they tend to blur out details over consecutive frames um whereas on the other hand forward and inverse lapping are matched so any any details that you have um do not get blurred out by by applying the the lapping filter they instead just get shifted around and when you apply the opposite of the lapping filter then they get restored so if those two are not exactly matched then you\u0027ll build up these rounding errors over multiple frames and this is the same problem of you know we essentially have an unstable filter so because we have an exact integer specification of our transforms on you know there you would never get encode or decode or mismatch but it would cost bits to correct these rounding errors in the encoder so that was bad all right next slide um do we actually need perfect reversibility um so it seems to help compared to transforms that don\u0027t have it we\u0027ve seen some small coding gain improvements but it\u0027s probably not required but we get it basically for free from from the structure of our design we don\u0027t actually have it in dal anymore so when you do this 4-bit up shift and then do the transform and into the 4-bit down shift that down shift is not reversible so that breaks it you can restore it by using twelve that references even if you have eight put input data basically just avoiding the down shift down to two by four bits at the end and there\u0027s a nice blog post there by Monty that that goes through and shows you what the this error buildup looks like and what happens when you switch to twelve the references and it essentially goes away but it turned out also that just using CL PF from Thor or the da lady ringing filter solves the problem by adding essentially one of these low-pass filters back that that we didn\u0027t have an art deblocking filter um so that prevents these errors from building up right next slide Moses from the for Mike again so this is the implementation in dala that is not reversible because they downshift what about a v1 um so so a v1 also has the this four bit up shift in four bit down shifts so it\u0027s also not exactly reversible so all if you use twelve internal bit-depth will you get the invertibility right so and also to be clear I\u0027m talking about invertibility between going from coefficients to pixels back to coefficients right which is the step we needed for the laughing filter on so you still have invertibility in the sense of going from pixels to coefficients back to pixels right yeah so where I was going with it is if you had a skipping skipping the quantizers then you could do lossless "
  },
  {
    "startTime": "01:07:11",
    "text": "coding with the real transform instead of a instead of the current four point right transform so that first gains that\u0027s actually something we tried back in vp9 with an early version of these transforms um and I think just replacing the the four point Walsh Hadamard transform that they use with a four point DCT was about 25% worse in terms of the lossless bitrate so I don\u0027t know if if you allowed using larger transform sizes if you you know instead of just fixing everything down an adaptive are do over write that the transform size you may be able to do slightly better than that but but just doing a straight swap of four point oct.4 we saw we saw the same thing with h.264 high profile somebody finally implemented high profile and switched everything to 8x8 you see loss but if they intelligently switch between four by four and eight by eight there\u0027s usually considerable gain so maybe some adding some heuristics to optimize that may end up with a pretty good lossless codec yeah alright Exide so the other other nice feature of reversibility is the effect it has on dynamic range right so as we said the transform coefficient values are larger than your pixel values because your forward transform expands the dynamic range your inverse transform is also an n-dimensional rotation so how do we know that it doesn\u0027t expand dynamic range right like if I have two coefficients X 0 and X 1 and they both just barely fit in 16 bits how do I know that X 0 plus X 1 won\u0027t overflow and the answer is is because the transform is reversible so all the values that I compute in my inverse are going to be the same as in my forward transform you know plus or minus any quantization error I\u0027ve introduced so this means that I\u0027m only guaranteed to avoid overflows if the coefficients come as the result of transforming pixels so if I decode random garbage I might get random overflows um but we can just define that that you know those cases aren\u0027t our undefined behavior right we don\u0027t I don\u0027t think anybody actually cares about the quality of decoding random garbage that\u0027s that\u0027s the same approach 264 took so one note about discrete sine transforms there are two types that we care about type 4 and type 7 so for inter predictions residuals the the prediction error you get is asymmetric so the error close to the edges you\u0027re predicting from is much smaller than the error far away from those edges which means you want an asymmetric transform to code them then if you say ok what\u0027s the optimal transform to use it winds up being this type 7 DST and get that by taking the a linearly increasing "
  },
  {
    "startTime": "01:10:12",
    "text": "correlation metrics and and taking the limit as the correlation approaches one and solving the eigen system and say what do you get the type seven DST pops out so type seven DST factorizations are much nastier than the type fours which are the ones that we have embedded inside of our DCT so the type 4 is there at the top and the type 7 is this thing down here and the real problem is this n plus 1/2 thing inside your trig functions which means what this actually is is is a trig transform embedded inside of a 2n plus 1 sighs fast Fourier transform and so pulling that out of there and still retaining a fast algorithm is a bit Messier since it\u0027s not a power of two so next slide type four transforms turned out to be almost as good in there are already embedded inside of all of our DC T\u0027s but our current approach is that we use type sevens for the very small ones currently only four point eight eight fight and then use the embedded type force for all of the larger D STS so the small ones are the factorizations don\u0027t get that bad and it turns out to be where you can get at least some gains by using the correct DSD side so comparing overall complexity the first three columns there are dollar TX on the next three columns our TX mg which is sort of the the AV 1 extension of the vp9 transforms to handle all these things like rectangular transforms in and unify hide the depth and load that depth etc so we generally have a few more ads but not that many more ads you can see like for the 32 point DCT it\u0027s 6.2 versus 6.0 but we have far fewer multiplies per coefficient right again for that DCT 2.7 versus 4.1 so we actually can wind up with with 39 percent fewer applause I think for the 32-point DST we actually implemented the the Cindy for the eight-point DCT and directly compared that to the existing Cindy for for the Avon transforms and it was benchmarked at 26.2 percent faster and that\u0027s mostly result of using using fewer multiplies and using cheaper multiplies right so none of our multiplies have to go up to a full 32-bit product and we don\u0027t have to do any 32-bit editions so we\u0027re able to get higher Cindy throughput you know say just a small note for the discrete sine transform we\u0027re using the type 7 while txm G\u0027s is there\u0027s only using a type 4 @ "
  },
  {
    "startTime": "01:13:12",
    "text": "for the 8 point discrete sine transform which is why ours is a bit more complex on particularly on the additions and that\u0027s just a result of the type 7 factorization is is not as good alright so a few hardware considerations inter prediction requires reconstructed pixels from your neighboring blocks so you think about it this serializes the reconstruction of those blocks including the inverse transform part of that reconstruction which is a particular problem for encoders and the decoders you can sort of start the transforms early and it only serializes adding the residuals but on the encoder side you need to know what pixels to transform so that that part becomes completely serial unfortunately when we do our 3 multiply rotations we those multiplies are all changed consecutively like each one depends on the output of the previous one which winds up being a bottleneck for small transform sizes for hardware alright next slide so just for the 4-point DCT and DST we\u0027ve replaced them with transforms that are not perfectly reversible and not lifting based but we basically replace the three block three multiply block with a four multiplier it\u0027s just like the matrix multiply so all the multiplies proceed in parallel but we still only use the top half of that multiply output so we still get full Sindhi throughput and then for the DST we use a custom factorization that uses two five parallel multiplies so I said these are not exactly reversible but they do solve the hardware latency problem Exide so additional consideration most hardware is already multi standard and includes vp9 in all the vp9 transforms and so they dedicate a lot of gates to having lots of parallel multiplies so we can replace a bunch of the serial multiplies in our rotations with these parallel multiplies without introducing any additional multiplies and so anything that anything that\u0027s of the form you know x0 plus a times X 1 u 0 plus B times u 0 and then y1 plus a times x1 ruies have this ABA structure for the constants we can replace with this little more gnarly looking thing on the right but if you reduce it down it\u0027s 1 addition 3 multiplies that all happen in parallel and then 2 more additions so it\u0027s the same number of operations but the multiplies can happen in parallel so this is again no longer exactly reversible so we\u0027re still experimenting to see what impact that has on accuracy and making sure it doesn\u0027t introduce any new potential overflows that would prevent us from from keeping our 17 16 bits so that\u0027s the everything on the the "
  },
  {
    "startTime": "01:16:15",
    "text": "design of our transforms I guess anybody have any questions they can through most of Mo\u0027s during the presentation so Moses and Florida had one more final question kind of a broad one so these look like they compare these transforms look quite they compare very favorably to vp9 and av1 have you looked at Thor which is basically a chibi see if you look the comparisons to the Thor transforms so the HTPC transforms um so so we haven\u0027t done direct comparisons at least in terms of for example coding performance um in terms of complexity like I they if I understand correctly the Thor Tramp storms are basically giant matrix multiplies um and and so you know that you can get away with that for very small transforms but as they get much larger I think that this will wind up being significantly faster right okay about 15 minutes behind Luke you can use the rest of the time if you want because I think we technically have the room until 11:30 but if you want to speed up 11 that was 11:30 the next session starts alright so Luke I got a five minutes if you can I suspect you can actually go to about 11:15 because the new chairs won\u0027t need a whole half-hour to set up so okay thank you so so look just try to go quick on whatever you can there can you hear me again alright so you have the slides up for my presentation can you see them no I can\u0027t see them almost slide one okay I can see them though so yeah I\u0027m gonna present an update to the CFL Draft for VC so if we go to the first slide chroma from luma is essentially an intra prediction tool so it has no dependencies on other frames it is only available to chroma planes and it basically works by predicting chroma pixels using coincident reconstructed luma pixels so let\u0027s go to the next slide to see the difference from what we proposed before so prior proposal was on a dowel implementation so now we\u0027ve changed that to reflect what was proposed for a v1 most significant changes that we no longer rely on pvq so prediction is now done in the spatial domain we consider the only the AC contribution of reconstructed pixels I\u0027ll talk about that a bit later but that is similar to what was happening before in the pvq version of CFL we use the existing DC "
  },
  {
    "startTime": "01:19:15",
    "text": "pred so DC prediction for the chroma DC contribution this is already available in a v1 there\u0027s already fast implementations it requires no signaling and it is more precise than what is always used before so that\u0027s also very interesting so going on to the next slide so the differences we can talk about maybe Dola and Thor which are you know codecs that people know here I already said before we went away from frequency and we\u0027re now going for the spatial domain for prediction the Thor implementation is implied in the signaling the doubt implementation use the pvq gain and the sign bit to send the information we send the information explicitly using joint signs and an index value the activation mechanism was a threshold for Thor it was also signaled in doubt we have a special UV only mode in a v1 so anyone has separate prediction modes for intra and intra luma and intra chroma so we take advantage of that to have this UV only mode called CFL pred we do encoder instead of doing encoder model fitting we will do a rate constraint search and we do know a decoder model fitting since the information in signal in the bits tree moving on to the flow of the operations we see that if subsampling is used as though if chroma subsampling is used well the luma surface will not be the same as the chroma surface so we must do a luma subsampling that is equivalent to the chroma subsampling that\u0027s being done we subtract away the average this gives us the AC contribution in the spatial domain and this is usually on a chroma transform size block then we\u0027ll decode the signals scaling factors from the bit stream and we\u0027ll multiply that these are in q3 precision but then once we multiply that goes down to q0 and we add in the DC pred the chroma DC pred to that value and that gives us our final prediction so if we look at the codebook that we end up with on the next slide oh okay nevermind oh that\u0027s good okay so alright so basically why do we go with the chroma DC pred is that when we use the AC contribution this each contribution is zero mean which means that it sums to zero and doing so simplifies the linear regression equation for beta which makes it the average of the Karma reference pixels and the DC pred is a very good predictor of that as it tries to predict it using the neighboring pixels that are adjacent to the above and left borders of the block and it requires no signaling so we don\u0027t have to signal the beta value so alpha will be signaled but beta won\u0027t moving on to the next slide "
  },
  {
    "startTime": "01:22:15",
    "text": "we have the scaling code book so basically this shows you when we do the search what happens so we start in the middle of this grid and we can change the scaling factor for chroma correction with a chroma CR and chroma C B and we move from negative to positive and you can see all the different tones that you can get this of course is only a subset of the codebook we have it is goes from minus 2 to 2 in q3 so that it goes up in steps of 1 8 0 0 is not allowed as it is DC pred we pick our value using a rate constraint search as I said before since we are signalling the Alpha value the when we do we can\u0027t use a liner regression because that value won\u0027t be our D optimal so what we do instead is we do the same thing as any other parameter in the encoder that requires rate is that we take the weighted rate and add that to the distortion value and pick the the parameter that minimizes that and that gets signal to do decoder the next slide will explain how we go about signaling lists so we will join both times so there\u0027s gonna be 2 scaling parameters one for CRN one for CB so we joined them together a sign can either be 0 negative or positive and since 0 0 isn\u0027t allowed because that\u0027s DC pred we have eight values which we sent to our multi symbol encoder as an eight value symbol now for each non zero scaling factor we will send a value excluding zero but all the way to 2 inclusively and this again with a step of 1/8 this gives us 16 values for our multi symbol and this actually maxes out what Multi symbol entropy coding can give us which is a 16 value CDF going on to the next slide we can see results from our analyzer there\u0027s a link you can click there sadly it got moved behind the image you can see the distribution of how many times modes get used so these are UV modes that in a v1 we can see that there\u0027s about 44% of the time DC pr√™t will get picked but a CFL comes in at about 17% we observe it between 15 and 20% in different sequences for a v1 as you can see the other contender modes or best motors still slightly below so we see that actually performs other chroma modes that are available in the encoder and you can actually see this live in the analyzer in real time moving on to the results for subset one we can see that there is a minus 4.65 CIE de 2010 tidge it is the bt right so it gives us a rate decrease with the same level of quality we use the CIE P D value because it is the only one that "
  },
  {
    "startTime": "01:25:15",
    "text": "considers both luma and chroma and does so in a perceptually uniform white if you click on the links below you can see the full breakdown with all the values so subset one are still images and objective one fast our video sequences as you can see in that point we are giving about an on average two point forty one percent reduction this is for a single tool CFL overall of 81 so that\u0027s pretty interesting there is also psnr games these are illumise and our gains the reason for that is since we have better predictions we actually reduce the amount of bits so and that gives us this metric actually gives better gains because it has same level quality but it\u0027ll have fewer bits to do so and since this is the area between a rate difference and quality that I\u0027ll give you a negative value so that\u0027s very good if we move on to the next slide we see that it is actually very good for screen content coding so here we we have on average about 5% reduction for the screen for the gaming twitch data set which is on slide 11 yes so notable mentions here are the minecraft sequence so CFL alone gives a minus 20% reduction on both minecraft sequences that are in that test sets and we see also good results for GTA and star graph at about 5% each first tiede 2000 and you know still some significant gains for psnr luma only so you know we\u0027re really impressed with the results from this tool and that\u0027s why we\u0027re proposing it as an update for the CFL proposal for a net VC I did I do that in good timing perfect thank you very much thanks for coming when sitting for a Tim to time usurper any other questions for for Luke on the chroma formula tool any other final items off the agenda all right so make sure that I get the blue sheets signed if you came in late anyone still here from net VC is make sure to get them where is the blue sheet by the way anybody needs it right please raise your hand we\u0027ll get it to you otherwise thank you very much I\u0027ll see you 101 thanks Matt for standing in for our co-chair [Music] "
  },
  {
    "startTime": "01:28:26",
    "text": "summarizes you "
  }
]