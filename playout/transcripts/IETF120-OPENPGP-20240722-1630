[
  {
    "startTime": "00:05:18",
    "text": "which order they go in, if that works for you? yeah that's good then i suggest first the existing adopted one the um open pgp draft open pqc open pgpg and then the next brain pull That sounds good to me. Thank you for that you Thank you. No other agenda bushing Okay So a brief report back on the crypto refresh It is in Off 48, which is the final stage before the RFC editor turns it into an RFC The final, final stage And as in Off 48, the authors only"
  },
  {
    "startTime": "00:06:00",
    "text": "need to approve the set of changes and cleanup that was done by the RFC editor. This is one of the largest sets of changes and one of the largest documents that we have dealt with. It's probably not the largest, but it was quite an intensive off 48 process But as of Saturday, all the authors have approved, and it will be RFC 9580 It is not done yet. I was hoping to be able to say, we have an RFC today, but it was done on the weekend. We'll see what we can get done, maybe by the end of this meeting, by the end of the week we'll have it. So that's, that is good news We are in good shape here. Yeah, yeah, excellent We'll see what we can get done. Maybe by the end of this meeting, by the end of the week, we'll have it. So that is good news. We are in good shape here. Yeah, yeah, next one's good. So just a note here about interoperability. We have three adopted drafts And so as these drafts come up and as we're discussing them, we're going to want to hear if you're implementing or if you're reviewing it We're going to want to hear if you have ideas for test vectors or the interoperability test suite which is out there so just something to keep in mind, you know, we are an implementer's community We're trying to make things that work for people and we want to, we want to if you have ideas for test vectors or the interoperability test suite, which is out there. So just something to keep in mind, you know, we are an implementer's community. We're trying to make things that work for people, and we want to, we want any reports you have that you think other people in the group might not know about what you're doing towards any of these adopted drafts or towards what will be soon, RFC 9580 you know we'd love to hear about that so as those come up you know don't be shy about raising those concerns That's the final one. We're going to love to hear about that so as those come up you know don't be shy about raising those concerns that's the final one we're gonna turn it over now to I believe one are you gonna to give the OpenPPGPPQC? No, the report I have something. Oh, sorry, sorry now to, I believe, Kwon. Are you going to give the OpenPPGPQC? No, the reporter from this. Oh, sorry, Patrick first. Yeah, thank you. So Patrick, I'll share the slide and I'll give you control of those One second Okay so Patrick you should be able to go backwards and forwards and Yes, okay and maybe briefly introduce yourself since I think Yeah, sure. Yeah, hello everyone I'm Patrick Brunswick"
  },
  {
    "startTime": "00:08:00",
    "text": "um i've been working on any mail since 20 okay. And maybe briefly introduce yourself. Yeah, sure. Yeah, hello everyone. I'm Patrick Brunswick. I've been working on Enigmail since 2003. Enigmail used to be a plug-in for Thunderbird I'm still maintaining it for a couple of forks of Thunderbird like Postbox, the monkey still X-Monkey still and Epirus In my professional life, I'm ValueStream product owner at the Swiss bank Yeah, I've started or we've started with these OpenPGP email summits a few years ago to bring together the various communities or sub-communities like those who implement email clients or plugins like Protons Thunderbird, R2Mail2, to Delta Chat, Planck or PEP We also have people from contact and mavelop, usually they couldn't come this time We have people from OPGPGP implementational libraries like Sequoia Open PGP, JAS, RPPGP, RPGP, or RMP like Sequoia, Open PGP, JAS, R-PGPGP, GOPGP or RMP. In the past also, GnuPG joined the same sessions. We have supported from key servers like HockeyPuck and Hagrid in the past also Christian Fiskastrand from SKS key servers joined frequently. And many of us also work on standardization like OpenPGP version 6 PQC, LAMPS, AutoC, and many more So the idea is really to bring together the various parties to have an open discussion and find common agreements and also just to get to know each other It's been the eighth summit since 2015. It's usually yearly but we had a break during COVID"
  },
  {
    "startTime": "00:10:00",
    "text": "The way we work is an unconference style, that is the agenda is fully driven by the participants and you can see on the picture topics that people brought up. It's a bit more than we manage to discuss usually. The way is really that we bring up the topics and then we vote which topics we want to present or discuss We have two or three breakout rooms where we can talk in smaller groups and we usually also have plenary sessions with topics of more general interest and what is important to me we also have a joint dinner or two joint dinners, so a social part where we can just get to know each other and really turn into a community What did we decide? discuss? We usually start with what has happened since the last summit So everyone who had a to do since the last summit gives a quick report back. We usually have a ratio of 60 to 80% of achievement so that's quite good, I'd say So this time we started, well, quite often obviously, with the schism between Open PGP and Labour PGP Many people also neutral people, tried to mark thomas werner without success and my fear was that it would be a hot topic during the whole summit But it turned out, and this was the good thing that we had just schism at first that after the session it was a general consensus that the schism is dead and we can't fix it for the time being And we could put it aside and talk about the real topics that we had We also found that GnuPG is not that important anymore in the world of open PGP email because many"
  },
  {
    "startTime": "00:12:00",
    "text": "email clients moved away from OpenPG GnuPG or those email clients that you still are not pg are not those that are used the most frequently Then we had a first session about PQC in OpenPP version four The idea was that this could be a quick time to market than versions six. However, it's not specified and would add a lot more complexity Furthermore, we found out the PQC in version 4 would only be available for encryption but not for signing And version four, PQC sub keys would be more fragile than shipping a full version 6 PQC key. And then end we didn't come to a real consensus about that Then we had a session about header protection. So protect the subject and other headers inside the encrypted mail body DKG presented the current status of the draft It got more and more complex Basically, each time DKG presents it it turns more complex, mostly because it's difficult with legacy email clients that don't support the feature and therefore it's getting really difficult to implement that Then we had a session about session about forward compatibility so how to deal with things that aren't specify yet so how should we for behave when when it comes? to a V6 key, but also? how should v6 behave if pq specified yet. So how should V4 behave when it comes to a V6 key, but also how should V6 behave if PQC subkey would be detected? The consensus was that there is no real general consensus. It highly depends on the U"
  },
  {
    "startTime": "00:14:00",
    "text": "use case of the mail client or the user, basically. It seems easy is no real general consensus. It highly depends on the use case of the male client or the user, basically. It seems feasible to have some forward compatibility for for certificates like ignore certificates that you don't understand But when it comes to private keys, it's very difficult to have anything Then we had a session about HKP and WG WKDE, and the session was more about designing a new version of the WKKK protocol or API We would probably not call it WKD anymore just to be not in conflict with Werner And we came to the conclusion that we would try to keep the change as minimum as possible, but still that we should allow some redirection of WKD to HKP The next session was about key distribution methods We collected the various methods for key distribution and we counted a total of nine methods like auto-crime key servers, WK and so on. All of them have their advanced and disadvantages and actually the conclusion was to overcome all these advantages and disadvantages we should maybe invent a new key distribution method but this is like adding a 10th so not really an option either Then migration to open page version 6 was certainly the largest or most important of all topics. And I'll come to that in the next session on the next slide. Then also this is a topic that Andrew will present in more depth Yeah, then a session about automatic encryption to publish keys mainly Proton, tried to turn on the lookup of keys on"
  },
  {
    "startTime": "00:16:00",
    "text": "keys on keys that openpGP.org to automatically encrypt to people who have published keys It generated an unfortunately, a lot of complaints and bad press because people all of a sudden received unencrypt mails that they couldn't decrypt So it seems that there is an expectation mismatch between what uploading a key to a key server actually means There was no conclusion It seems that this topic is again, depending on the use case and the implementation The last session was, about how to initiate work on autocrypt 2.0. First is discussed the topics, so what could be the first things that we would implement in Autocrypt 2.0. And we try to find out who would be interested in working on it There was no real conclusion apart from, yeah, let's start and do something Finally, I'll present a little deep dive into migration and to certificate migration from version 4 to version 6 The idea of the session was that we would try to overcome the period when both version four and version 6 are alive, but not all implementations already support version six and what to do with that Adoption can be quite slow because people have long-living version 4 keys. I mean, some people create keys with the lifespan of 10 years So that's quite something that we have to foresee. And people also are sometimes reluctant on upgrading there mail clients and stick to old libraries We found out that much of the adopted data"
  },
  {
    "startTime": "00:18:00",
    "text": "stick to old libraries. We found out that much of the adoption is actually needs to be handled in the mail use agent and not in the open pgpgp library of course the open PGP library needs to support whatever we specify but the actual work needs to be done in the mail client it's obvious but version for version 6 can only be used if all participants in a conversation have a version 6 key. Otherwise, you either need to have separate encryption or you need to fall back to version 4. So we discussed the idea of linking keys and that shouldn't only be specific for version four to version six, but which would also allow to work to migrate to PQC at the later stage The idea is that the linking of keys should be done in a way that it allows for automatic processing by the male user agent We found out that it should be equivalent to assigning subkey binding from a cryptographic and user experience point of view We discussed whether EIDs should be linked or whether just the keys should be linked but we couldn't find an agreement It seems to depend on the use case, for instance Autocrypt declares the U user ID and decorative, so you could use anything And version six, for version six, also user IDs are optional. Therefore, there was no real conclusion on that we had a lengthy discussion on whether we should allow chains or trees and possibly loops for this linking. And we can to the conclusion that all of that would be far too complicated. So the idea is that we disallow chains and trees for simplicity simply and that is if you create a second link,"
  },
  {
    "startTime": "00:20:00",
    "text": "to another key, you by definition, you dropped the link that you created before before The direction of linking needs to be a preference that the user sets. So you could also link you can link a version 4 key to a version 6 key and say, I want to migrate to version 6 but you could in theory also do the opposite, you should like like Then we discussed how the sender mail user agent could migrate to version six So the idea is that the mail user agent links version 4 and version 6 key and publish both of them with whatever mechanism the mail client uses to publish keys like HKP, WKD, auto-com whatever it is The idea is that when you send a mail, you send it with both keys and ideally with the fallback key or the version four key first and the version 6 key later, second, so that a library that doesn't understand the version six key can verify the version four signal signature We discussed about aborting the migration The idea is you simply revoked a new key and you're basically done Once they're completely at version six, the cleanup, so the full migration would be that you revoke the old key and also the link And how should the sender? migrate to version 6? The idea is that if you cannot verify, the linkage, so if you can't fetch the new key, for instance, you simply ignore the linkage. If you can verify the link, you encrypt to the preferred key first and if that's not possible for you, you encrypt to default"
  },
  {
    "startTime": "00:22:00",
    "text": "fallback key. This way we found a mechanism that allows to link keys and still be able to use either of them individually As I said, Andrew will present a draft for exact this later and that about it. So this is basically a brief summary of the summit Any questions? Thank you, Patrick. While we're waiting for questions let me just ask you to mention if we have is there another summit coming up and who would be welcome? at the summit? Sure, I'm planning an next summit, April next year Basically, anybody is invited, especially though who work on implementing either a mail user agent and open PGP library standardization, key server and so on, please join us I usually publish every standardization, key server, and so on, please join us. I usually publish everything. There is a mailing list where I publish everything and I'll also send an AVT invitation to the IETF working group Thank you. I don't see anyone join the queue right now, so I think we'll move to the next session, but thank you for presenting that Yeah, thanks, Matt. I mean, he's also worth pointing out that the I mean, I kind of like the interaction between the kind of developer meeting in the summit and that feeding back into changes to the proposed drafts that we've been adopting that seems like a good modus operandi to me anyway So thanks and thanks for hosting it. Yeah very productive meeting thanks thank you Patrick OK And so we're on PQC"
  },
  {
    "startTime": "00:24:00",
    "text": "Does this clicker work for you? No who's presenting this? Hello. Aaron, you're up, are you? Yeah, yeah here. Can you hear me? Control of the slides Yep, it seems like it's working. Okay, so this is going to be a brief update of what has happened since the interim meeting two months ago and so we've done some changes to the draft we've fixed the MLDSA signature size that was wrong in the IPD. We did some minor fixes around with parameters orders that were in the wrong place or were missing And the major change that probably with done is to change the parallel encryption guidance. That was a very old issue This means when you have two recipients that you don't want to encrypt the same recipient both in PQC and traditional way how to avoid this, and it's now basically up to the overlaying implementation to choose whether they can do it at all or whether they can't And then we added some MLDSA test factors. I announced this because as of few days ago, also OpenPGPJS implemented ML MLDSA. And therefore, we were being test factors. I announced this because as a few days ago, also OpenPGPJS implemented MLDS. And therefore, we were able to generate them with Gov-Crypto and verified them in OpenPGPJS This is the current implementation status so Go-Crypto is probably the one that is the closest to completion That has only SLHDSA that is still around three submission because we don't have a library that does SLHDSA as IPD OpenPGG has also implements ML Chem and MLDSA, but just in the 67 variant and 700 768. RMP implements only"
  },
  {
    "startTime": "00:26:00",
    "text": "MLCAM IPD and also implements, it doesn't implement completely different 67 variant and the 768. R&P implements only an LCAM IPD and also implements it doesn't implement completely the V6 standard Please also FICO keep me honest if I'm saying anything jump in the queue and correct it. LibG Crypt instead seems to be implementing all the algorithms but is not upstreamed and since new PG we know is not really friendly with B6, it's having issues there therefore the only test factors we could reliably test and generate were the one with MLKAM and MLDSA MLDSA We have a few changes that we would like to present and try to get in before IDF 121 And these are a bit more substantial changes as well as life the issue that was being presented before about whether to bind to V6 or not. But let's start with the key derivation and combination We, as announced, the interim meeting, we're still using a provisional version of the cam combiner. This means is explicitly noted as it might change. It's not a huge change code-wise, but it will probably be a breaking change and as soon as we manage to settle the NIST compliance and the CFRG maybe has come out with, the working group has come out with with an idea maybe we'll we'll fix it we'll fix that Mike, since next slide is about is about binding encryption maybe it's best if you can to the mic now So"
  },
  {
    "startTime": "00:28:00",
    "text": "yeah, this is going to come up in labs also This is a shared issue. My question for the open PGP community is, do you want to wait for? CFRG? CFRG may take a year or more to decide what it's going to do about chems, hybrid chems So open PGP need to move faster than that or are you happy to wait? I would be happy to move faster than that. Like if the outcome is to take years, I would like to move fast I don't know whether the whole working group would like to move faster, but at least from my perspective is I think we can afford to wait till we have other issues that are open that we don't have consensus on and stuff like this personally I think it's nice to have a CAM combined that is that is across implementations and so on, but literally we're talking about at least in Go something like 20 lines of code. So it's not that even having a different one will great impact our implementation size or end anything like that. I tend to agree with that. I mean, I think we should, I think we need this faster than it's going to be delivered from CFRG That said, I think we should make this decision intentionally rather than by accident. The timing issue the interrupt between protocols issue, you know, one crypto library that will work across protocols and the security analysis are all, I think, important considerations here. We have a role will work across protocols, and the security analysis are all, I think, important considerations here. Yeah, we have our own security analysis in the draft, and I think that that, like, we've managed to get something so far without the FRG i will also say this at the cfg I think that like we've managed to get something so far without the FRG I will also say this at the CFRG session that is basically we can't afford to wait forever to have a result I was also in the working group of the CFRG so I've seen what was going on there"
  },
  {
    "startTime": "00:30:00",
    "text": "So yeah Quinn I'm Quinn Dengen Nis. I personally like to push things forward faster However, there is a good possibility that after the SEPAji has some endorsement on some basis cam combiner one or more, I don't know for sure right now but at that time, that could be possible that one of the game combiners that their SEP Archie endorses or specifies could on also be compliant with NIST. And then that would be perfect for everybody so that their CepaG endorses or specifies could also be compliant with NIST and then it would be perfect for everybody so there's some pro for for be patient a little bit more but at the same time I want things to move fast But there is potential good benefit for being patient I see so I think we have I wasn't to give a brief recap of what happened in CFRG I think there seems to be an idea to standardize some can combiners with a drafted that is partly generic so offers a generic way to combine any two camps maintaining the security properties as well as some specific instances. And I don't think the specific instances would already cover X448, for instance So we already go out of the specific instances and we would need to implement the generic version most likely And said this there is clearly the, we all agreed in the working group that there is the let's say dump version of the can combine that is pretty much put everything in it and that will be safe. The problem is the"
  },
  {
    "startTime": "00:32:00",
    "text": "fact that that is very, very little efficient Open PGP, please in my opinion is not really inefficient protocol in the first place It doesn't need to be like TLS. So we might also live with very explicit cam combiner that gives us the safety properties we want. But that's like, as I said, I don't want to put I don't want to put ourselves ahead of the process I would first like to see what happens in CFRG before they happen to take this decision Regarding the NIST compliance. Sorry, Aaron, just on that, I mean, I think maybe not everybody's familiar with CFRG. So one thing as CFRD does not produce standards. I think he said it might be standardizing something. It's a research group that doesn't do that Yeah. And secondly, so CFRG is the CryptoFORM research group. A bunch of people, including Aaron, myself and Mike were involved in a design team to see what should CFRG do about requirements for KEMS That will be discussed later this week and CFRG then if it follows the outcome of that design team might then produce specifications for some CAMs that could be used by OpenPGP and i think to answer mike's original question, or to comment on Mike's original question, I guess we should be aware of at what point in time do we want to fix the chem that's used by Open PGP. And at may, that point in time, might be before the chem combiner, so yeah at that point in time maybe before uh CFRG has finished or maybe CFRG's work will be far enough along that we can just kind of adopt it I don't think we'll know for a while, but the time to make that decision is not right now it's a little bit down the road it is down the road, but let me just also comment that we have not designed this to be a generic chem combiner, right? we have one code point that says use this chem combiner"
  },
  {
    "startTime": "00:34:00",
    "text": "with these two chems right and just because we choose one if we if we decide to standardize one before CFRG gives its blessing, and we find out that they get a blessing to one that is not the one that we chose, it is possible for us as the Open VGB Working Group to define another code point that says use this other chem combiner so we're not picking a generic combiner that's going to necessarily sit with us for everyone future pair of kems. Just remind you there. Mike? Go ahead, Mike. Yeah, I like this. I think this to me seems like the right direction. I strongly second both Steven and DKG like this. I think this is, this to me seems like the right direction. I strongly second both Stephen and, uh, DKG. Um, the, because yeah, it's where registering code points for pairs of things. And if it comes out, later that the research says, oh, we also need to bind some other ASCII art to protect this other attack you know, you can spin this again, right? There's nothing preventing that So that seems right. It's move fast with what we think is correct now. And if it turns out that it's not right or we want to synchronize it with other groups later that later There is also a last point about the NIST compliance is the fact that we do not, in this draft right now, we do not have a NIST-approved curve So the NIST compliance of the KAM combiner also is it can be considered shaky because whether we want it at all this I think is a this point applies definitely to the following the next presentation when let's look about missed and this goes all. This I think is a this point applies definitely to the following the next presentation when we'll look about missed and this curves. Curve to 551.9 is list approved? but in the ed version not in the x version as far as I know I see this question in the chat"
  },
  {
    "startTime": "00:36:00",
    "text": "It's fascinating how a cam combine can always bring everyone to a discussion. Can I move to the next topic? Please. Okay Okay, so bring everyone to a discussion. Can I move to the next topic? Please. So binding encryption to keys to v6. This means that an encryption, yes, means an encryption key can only a pqcce encryption key can only be V6. No, means it's can be before or B6. We don't talk about V3, V5 or anything of that sort We had a trap on the on the maining list and the main list and I tried to recap a little in one slide, the opinions I've seen there, and basically, were like not all implementations are ready yet for being on the mailing list and I tried to recap a little in one slide the opinions I've seen there and basically we're like not all implementations are ready yet for v6 and clearly against because it's, we put current a quicker deploy so it means that we many implementations are ready to just implement PQC straight off if the before battle test and implement implementation But some people presented the fact that it is more difficult to ensure backwards compatibility because this means that some implementations may choke on finding unknown algorithms or just the size of keys. It might hinder the adoption of B6 because some people might just say, well, I have PQC encryption before, why do I need V6? V6? And then it limits the combinations. If we bind it to V6, we have less implementation less combination that we need to cater to less test factors less complexity We had this discussion at the last interim as well and there was a new no point that is Libra BGP allows V5 PQ sub keys, but they can be attached to it sub-primary keys. So they have a binding"
  },
  {
    "startTime": "00:38:00",
    "text": "but they do not have the binding towards primary and subkey so So that also opens a new hole kind of worms that we don't want to touch Again, there was some people arguing for reducing the complexity and holding back to the 6 migration This discussion happened all over again at the Open PGP in the summit where Patrick before told us about it, and honestly, we haven't reached a consent there either Arguments are very similar, less work and implementations are maybe made be ready to have this six but we pull it down to be mostly R&P being not V6 compatible and this implementation seems to be considering this implementing this anyway and has a partial implementation of V6 made from made from Johannes now there is a new argument for the yes that is that is only one migration will be necessary So you can bundle up the two migration to be six into PQ into one single bundle and just to leave that this is true technically also without a binding, but it makes it easier to sell This is what we came up with in the end among the authors that is the current draft has test vectors for encryption with V4C so it's definitely allowed it's i would say the main thing we've been testing so far, but as of the latest draft, we also offer V6 vectors with EQC. And so this may change There is a clear consensus on allowing PQC using SEAPDV1. This means no matter whether it's before or V6, you will be able to communicate with mixed recipients using PQC and non-PQC. This is something that is that what there's"
  },
  {
    "startTime": "00:40:00",
    "text": "already everyone seems to be agreeing, so at least this is not the problem. There are slightly more people preferring a binding to v6 counting the number of voices but this is not really a consensus and the people opposing the binding said they would accept the binding if there were a clear V6 support and a clear migraine strategy So the decision as authors of our proposal is to consider an increasing support for V6 in the next draft coming approximately halfway between now and the next IETF. We would like to plan the binding to V6 In case you disagree with this statement, please scream at me now, scream at me in the mailing list or stream at us on the mailing list, and we'll won't do that pretty much So That's it. We have a nice graph, nice little thingy at the end where you can clear on our issues in case you're interested in too looking at more boring issues that we have on the table Datatracker, on the issue tracker tracker So I think Mike raised a question in the chat, that's probably worse taking at this point. I don't know if you want to join the queue and say it out loud, Mike. Or? Hmm I'll read it out loud Is the OpenPGP working group ready? to assign a milestone for publishing the PQRFC? Generally, are we thinking within one month of the final tips draft?"
  },
  {
    "startTime": "00:42:00",
    "text": "or by 2027 be fine? fine? Personally, I think that having such a strict deadline, such as a within one month of FIPS standards, is even not possible. We also have other issues open, like such as this one. If I could since next IETF is close to Christmas if I could make a wish, is that we would be ready by then, but this is clearly wish for thinking the thing is also I think that by 2027 is a little bit late some fun little bit late. Some funding is going to end at the end of the year or early next year for MTG, the other authors of the draft and it will be nice if by the end of this year we have something that looks so past of stable changing a KDF is not a bit breaking, it's not a big change, but it's still a breaking change. It would require updating all the implementation all the test factors and everything Thank you So what's the summary of that? What's the answer to this? Aaron, what's the answer to this? The answer in two read is it would be nice if by the end of this? year we had something stable, but this is not up to us. It's up to the working group to the site Yeah, but I mean, it's useful to know what people what goals people would like they're their milestones they're mostly fictional so but it's it's kind of useful to have a target. So the author group were kind of for the moment kind of suggesting that aiming for this work to be stable by the end of this year is a goal yeah I think that's optimistic but okay"
  },
  {
    "startTime": "00:44:00",
    "text": "um you've got to be if not you don't go ahead of suggesting that aiming for this work to be stable by the end of this year is a goal. I think that's optimistic, but okay. You've got to be, if not, we don't go anywhere. I like optimism. Sure Yeah Okay. Any other questions about the PQC drafts? If not, I think we moved to Quint Yep. Okay, let me just catch your slides up. Join up here on the pink X. I don't think you have to stay there anymore, but don't know. I don't know. I think the mic The camera is going to turn. We'll see what happens. You can test it by walking around the room if you want to try and test it. But I don't know we there's two cameras in the room and one of them is focused on the chairs. Maybe Meade Echo can point the camera towards the speaker uh no i think that happens automatically even as he speaks Hello. So I don't know That's okay. I'm Quindang at Ness I'm Since the last meeting the working group decided to have a new draft which specifies the M Chem, MLDS, with the NIST curves and with the brain pool curves based on the interest and the decision from the working group that only specifying high the brain pool curves based on the interest and the decision from the working group that only specifying hybrid chems and hybrid signatures would post quantum algorithms for now So that's why in this draft we have only hybrid modes for camps and for signatures How can I move to next slide? please. Yeah, we have a sticker just more. Yeah, here is the draft location and we have the GitHub location as well for the discussions and comments on all the material related to the draft"
  },
  {
    "startTime": "00:46:00",
    "text": "draft draft Just be brief and go to the main points that we want to discuss in this meeting for the hybrid camp We are for the NIST curves. We are have three options The first one is we go with the MIS Chem Level 1 with NIST P-26 and the ML cam level 3 we goes with the P-384 and also for level 5 we use P-384 as well well And for the signatures, the high use p384 as well there and for the signatures the hybrid signatures for the level one of level two MLDS we use this P-256 and level 3, we uses P-356 and level five we goes with a P-384 as well The reason for using the P-384 for level three level five is that P384 is kind of more widely available instead of the PRA P521 It's not really been used much So to make the adoption a lot easier and to push the post-quets security out there, faster and better, we are comfortable with B380 to be used in the order options beside the level one security option. And P-3-4 is pretty good and B521 to me is overkill so that the reason"
  },
  {
    "startTime": "00:48:00",
    "text": "for for the three options for each from a NIST curve From the brain pool curve, my co-authors, they do have two options for each, and the first one is they go with ML Chem 760 with the brand pool curve 256 and the level 5 the 1024 cam they use it ML cam 768 with the brand pool curve 256 and the level 5 the 1024 cam they use the brainpool curve 384 And similarly, for the signatures, they go with the level 3MLDSA with a brain pool curve 256 and the level 5 they goes with the brain proof curve 3 84. And yep that's what we currently have in this draft next slide please please um we basically follow the main draft to discuss things and spec things in the draft and currently the cam combiner is basically equal to the cam combiner in the original draft so that we think there would be some changes in the future but we don't know for sure what needs to be changed yet and that needs to be discussed and sing with the main draft all now So that's why we decided not making any huge changes right now with the camp compiler in the draft, because we don't know exactly what what is the best thing to do. Next slide please We have some questions for the working group and three main points"
  },
  {
    "startTime": "00:50:00",
    "text": "are here Eventually, we would like approval and get code points for these options in the draft so that people could use post-quantum for OpenBitchie OpenBGP And about the chem combine, a similar question like we had in the previous talk how should we go from here? for the cam combiner, for the KDF? Yeah, we certainly on one point I would like to make it clear here that from the previous draft actually even even the case, we don't have X-251-9 difference or the X-448 if you have they are not currently misapproved but with ML Chem is approved you can be you you can be certain certified for post-quant security for 440 are not currently niss-approved, but with ML Chem is approved, you can be, you can get certified for post-quant security for your solutions if you would like to have miscertified or FIPT approved for your system so you don't need both of them to be NIST approved to be able to have post quantum security solutions in a NIST-approved mode so yeah so that's why the can combiner is in relevant and important in both cases Next slide I'm sorry, I talked. I had of my slides slides Yeah, and I will have to have to group with the direction, with the guys guidance from the group and to help with the key combine or KDF moving along And I will point out, and I will give my assessment"
  },
  {
    "startTime": "00:52:00",
    "text": "to the structure of the KDF that we have in the future whether or not it is need approved or not and what needs to be changed if needed So I will be going along with this and may have been a weak group making sure to achieve the goal if the goal is to get a NIST compliance, KDF and hybrid cam or hybrid signatures And also, one more thing. We are using the prehash version of the MLDS there in this draft and also potentially will using the prehash version of the MLDC in this draft and also potentially we will do something with the hash station in this draft and also potentially we will do something with the the hash page signature SPIN Plus it's called SLHD DHS we do we plan to use a proposed to the working group using the preview of variants variance and also we afford a little editorial term, we would like to make sure that the has value goes into the signing function we're not called that a message anymore if we consider the prehast at the step all together so we just call the hash is the hash the message the message We're not calling it the hash of the message is the message to be less confusion I guess So part of that has to do with the, the history of Open PGP, right? You're referring to the fact that there's the open PGP signature process, which involves hashing the message right? And you're saying that with this track you're not doing an open PGP sign. And the open PGP signature process generally is you have the message and then it's hashed, and then the hash is fed into the signature signet into the signature scheme"
  },
  {
    "startTime": "00:54:00",
    "text": "and are you saying that with this proposal you're changing that behavior in open PGP? No, I think it just called using a different term to be a scheme. And are you saying that with this proposal, you're changing that behavior in open feature fee? No, I think it just called using a different term to be to be more clear. But I think that's the card of prehast and that we apply. Okay and so are you binding this particular signature scheme to a digest scheme a digest algorithm Like each, each signature code point has a specific digest that it uses? Hey, uh, Falco, are you here? Yep it's coming we have to check that that was I think, specified in the main draft, but we might not have taken that over Maybe there's still a gap. I'm not sure We will check that. Okay Yeah. And one thing uh this important to this as well um there's still a gap. I'm not sure. We will check that. OK. Yeah, and one thing, this is important to this as well. In the MLDCs that we're having, for the pH function signing, we actually include the prehash function step as a formal step inside the signing function So basically, having two formally two different signing functions One signing function is for the normal pure signature and a the BHA signing which includes inside the pre-hast step Okay. So that means, yeah you have two different signatures yeah maybe if I may add, so this is just on this, we put it on this slide because in the crypto refresh, it is already actually misinterpreted for the CFIG curves. So the pure at DSA is used where the hash at DSA should have been used"
  },
  {
    "startTime": "00:56:00",
    "text": "This doesn't matter because in open page we're always hashing the signature algorithm ID So there cannot be any mix-up even if later the other variant, the correct variant would be added. There would be no problem, no security problem But the idea is if you pre-hash like it is done in OpenPPGP, then you use the hand variant and what you input into your signature function is H of M so the hash of M and the in the crypto refresh there's a reinterpretation so this is just here we mentioned it to clarify that this would be a bit different, so that people are not surprised when they read it and compare it to the crypto refresh but it's a really a minor point actually. It's not really important It's absolutely compliant with or conforming with the way open picture functions Thank you king see you in the queue yeah um at the risk of the derailing this whole conversation about this indeed minor point, but I believe that the pure EDDSA of a message, sorry the pure EDD essay of a hash of a message is not the same as the hash EDDSA of a the message, right, because there are some, like, very a message, sorry, the pure EDDSA of a hash of a message is not the same as the hash EDDSA of the message, right? Because there are some like variant specific context in the in the RFC that separates them as far as I know So I think for the crypto refresh, it is important which one we specify in the sense that I mean the thing that everybody implement is using the normal EDDSA function not the prehash one and then and then hash of the message into that as the EDDA message, right? I don't think we can reinterpret that in the crypto"
  },
  {
    "startTime": "00:58:00",
    "text": "refresh as using the hash EDDA on the message message I'm not sure what to the mailing this because it requires people to go back and look at the CFRG's mistake in defining two API for the same thing from years ago So maybe we'll take that to the mailing list. If there's discussion about how to use, whether to use pure or hashed APIs in post-quantum signatures, that's probably a, that's probably to the main list. If there's discussion about how to use, whether to use pure or hashed APIs in post-quantum signatures, that's probably a good topic as a thread on the mailing list I think that's the last slide. That's the last slide. Yep So I think the question at what point do you want to? suggest bringing this draft for adoption? Or is that now or is that in the next revision? or what's your ideas? I have to discuss this with my co-authors Focal and others would you like to ask for formal adoption now or would you love to take a look at a draft and then ask for adoption? later? I'm not sure what good I think we're ready to call for adoption now So in my view, we can ask can ask for adoption because it's only at most mine things that need to be corrected all in all it's just a fork of the main draft We split off the combinations with the nist and brainpool curves we added one code point for nix and otherwise we're not intended to make any changes. Okay so yeah, I mean, I'm guessing people haven't read this so we can get in time to read it. Just to be clear then, if the working group decided to adopt this in a while the questions that are shown on screen here now"
  },
  {
    "startTime": "01:00:00",
    "text": "would be decided by the working group, correct? Yes. Yep Just for context, when we just for context when we discuss adopting this at that point the working group would be discussing most versus May and so on, as you are context. When we discuss adopting this, at that point, the working group will be discussing most versus May and so on. I'm serving the working group desire Yeah, exactly. I just want to make that clear. Yeah, I understand okay so i think the action here is probably giving people a little bit of time to read it, and then the chairs will have a look and see when it seems like a reasonable time to issue a call for adoption. That sounds perfect, yeah I don't want to vote to vote something they don't will have a look and see when is when it seems like a reasonable time to issue a call for adoption yeah that sounds perfect yeah i i i don't want to vote in something they they don't know for sure yet yeah so we'll give a bit of time and in a while we'll consider a call for adoption Awesome, thank you All right. Thank you. Thank you. Sorry before you walk away I do have will have â€“ yes, I will ask the mean question here, which is like, do we read need it, or like, what's the, you know, concrete? motivation? Because in my mind, the the non non-quantum algorithms are purely there as a fallback right and we have as Quinn mentioned, the combination of ML Chem and X259 with NIST-approved can be considered approved as far as they understood So is there really a need to also define? ML cam combined with the NIST curves and or the brainpool curves as well? well? Oh, that's a good question"
  },
  {
    "startTime": "01:02:00",
    "text": "The My guess is that there are a lot of places people have a NIST curve, and my My guess is that there are a lot of places, people have a NIST curve, and they might not have the X-2519 or or x-448 and if the people, they personally desire to have a hedge with the new pole quantum by using the hybrid mode then this draft would give them those options so I think yeah maybe on the web more extra one-night different than the P-256, but there are a ton of P-56 traffic, and for a lot of people who would like or or need to be nist compliance or paper support ton of P-56 traffic and for for a lot of people who would like or or need to be miscompliance or phipps approved then they they have P-56 be 56 and they they they need to be able to use that for the hedge benefits, if that what they want to do So I think both drafts have their strong purposes to serve and we find with either poor post-quantum or the hedge the hybrid modes, we are happy with both and people we don't we don't have any reason to not happen people who want the hybrid mode and bow draft would be fine so serve their own purposes thanks yeah i mean just from my personal view x25519 is already quite widely deployed in OpenPP implementations. So I I'm, I think for most implementations,"
  },
  {
    "startTime": "01:04:00",
    "text": "that are implementing peak you see that shouldn't be a concern. But I mean, obviously, if some disagrees, feel free to pick up Yeah, I'm not a, of my purpose is that I promote you know, good secure crypto deployments and solutions So the more good solutions we have available for people that's the better. That's my goal Falco you're in the youth though in terms of like the number of algorithms and combinations that we need to test and so on right so that's kind of the reason why I'm asking I'll go. Yeah. So I can speak for the BSI, and unfortunately no one from the BSI is here today, but it is just a fact that the brainpool curves are the only curve that are approved for governmental applications and yeah this has a wider impact, of course in in in germany and maybe also another European countries that are using the same standards so open GP is actually used in such related products for government applications. And does it just a requirement? or not having this would have an impact it's not just nice to have for those that that are dry yeah driving these services to want to update these services for PQ. Sure yeah i mean i think yeah i mean it's a this is a generic issue that crops up every time we talk about what to do about curves. Um, and it'll be the same here. I think so at some point the work group will discuss adoption Also, if the work group decides not to adopt it, I think a lot of the registries will be specification"
  },
  {
    "startTime": "01:06:00",
    "text": "required so the code points could still be arranged, even if the working group doesn't want to adopt this work it could still get registered code points and meet people's requirements Understood, yeah. Whatever served the best the working group the best i do it But right now in the draft, we only have the main requirements for the for the options the for the options so implementers are not required to element all of them they they don't need to element them if they don't need them or they say it's not beneficial to them. But they are many implementers, I believe they would be beneficial to them to at least support one of those options. For intraoperability once there are signatures and recipients, in the wild, then we may oblige implementations to adopt them all just so that they can actually interoperate. So we can say may but if there's a large pool of folks who are signing with these keys implementations that don't allow. I think it will feel that the pressure. But yeah, that's very correct. But that also means that their demand if that, what happens, their demand for that kind of option and there demands you want to meet that demand and office up to you yep Okay, so yeah, so we'll get to have a discussion about adoption in the not too distant future. Wonderful, yeah. Thank you Thank you. OK, thanks Who's next? Is it Andrew or Daniel? I think on the agenda we have Daniel fett That is the agenda we have daniel fett. That is... Where's Daniel slide? Persisting symmetrical. Yeah fucking minutes signature salt as well. He snuck in the signature"
  },
  {
    "startTime": "01:08:00",
    "text": "He snuck in the signature soft as well. I'll give you control of the slides, Daniel. Yes, thank you Yes he snuck in the signal i'll give you control of the slides then yes thank you um yes so this is a short update um from the the interim meeting the the main thing that happened is that this draft got adopted So that's previous. There were a few numbers or a small number of changes that I proposed in the interim meeting and that I made in the I get version of the draft If you open the slides from the meeting material, there's a link to the the diff the interim meeting and that I made in the idea version of the draft. If you open the slides from the meeting materials, there's a link to the div with draft Huygens so you can see what was changed since the call for adoption which is I mean, sort of minor things, but not entirely inconsequential either but you know the large ideas obviously the same And I won't go through all the inches again, but you can see them there or in the slides from the interim and if you have any feedback or questions, you can, you know, send them on the list or wherever And then so we have experimental implementations in Openpgb.js and Googlepbpgpb, but those have not been up updated yet to those to those changes So that should still happen So here is just a quick reminder of the current status of the draft Basically, it proposes to new algorithm IDs in the what is now the public key algorithm registry and would be proposed renamed to the persistent"
  },
  {
    "startTime": "01:10:00",
    "text": "key algorithms registry One is AED for symmetrically encrypting messages or files for long-term storage and archival And one is H-Mack for storing symmetric attestations and perhaps also storing, for example, signature verification status, like I verified the signal and it was correct things like that and then we require the key material for these keys to be stored AAD encrypted If they're stored encrypted, you have to use S2K usage octet to file what the current draft says, and the purpose of that is that AED bites the secret key material to the public key material, which means that they will be bound to the specific algorithm and also to the fingerprint, so you can be sure that you know, give a message you know, was encrypted with the same algorithm as that you're attempting to decrypted with for example All right So then the rest of this presentation is basically asking, like, what else should we should we add if anything in this draft. There's a few things that we could talk about that the current draft doesn't One of them is usage guidance so currently the the draft doesn't say that much about how these algorithms should actually be used other than, you know, you can"
  },
  {
    "startTime": "01:12:00",
    "text": "use them in a pk draft doesn't say that much about how these algorithms should actually be used other than, you know, you can use them in a PKK and in a signature packet But we could say something like well if you if you send an email or if you create a app might want to encrypt them symmetrically, although of course that's very, you know, application specific but that's one thing we could talk about to make it easier to use, perhaps and the other main thing in this category is should we talk about how to indeed test to signature? verification results as I as a mentioned in the beginning, might want to store the fact that you know, I've verified a signature And even if, let's say, in 10 years, that signature algorithm is no longer considered to be secure, can still check the symmetric attestation, let's say and still trust that when you receive the email it look to be okay, let's say And we could you know, create a third-party confirmation signature for that purpose or you know invent a different mechanism but we might want to think about how to do that and then write that down although it's not necessarily obvious to me whether that should happen in draft or somewhere else But, you know, that's something we could think about And then the other main thing in my mind is we might want to spec out the the persistence metric key algorithm space a bit more So one idea that I had and have mentioned on the list, but not in the drive is that we might want to define the entire second half of the algorithm ID space as being"
  },
  {
    "startTime": "01:14:00",
    "text": "for symmetric algorithms, which means that can just let the first bit and see whether it's a symmetric algorithm or not And the benefit of that would be that for the for the let's say, semantic differences that there are, you can, you know, no advance. Like, for example, with a symmetric algorithm, you can't use the public key to encrypt and you can't use the public key to verify, obviously So that might be an argument for, you know, partitioning them clean And then we might even want to carve it a space for experimental algorithms symmetric experimental algorithms For example, 200 to 20 or 228, you know, 1 space for experimental algorithms, symmetric experimental algorithms. For example, 200 to 210 or 228, you know, 128 plus 100 where, you know, we can define those. And then we might even want to put these two for the time being wild still draft and not an RFC yet. Although practically speaking for existing algorithms for existing implementations is probably makes no difference whether we use 128 or 200, but we might still want to make changes to these algorithms and then for testing purposes we might want to use the different algorithm ID So yeah that's about it for persistent symmetricies. So I'll pause there. So basically, yeah, the questions are, should we need add those things? Should we add anything else and or do you have any other? feedback? Let me ask another question to the list of questions that Daniel's raised here"
  },
  {
    "startTime": "01:16:00",
    "text": "which is is anybody else trying to implement this? looking at it as it currently stands? We have adopted the draft I see you in the queue Yeah, thank you for the presentation. I think it's very interesting to have this feature because actually symmetric encryption is we should not forget PQ secure and by far more reliable than any other public any conceivable public key encryption so one thing that I remember that when I read the draft is that I think it was not accounted for that this is really used as an encryption from A to B, from Alice to Bob as a replacement for public key encryption I think there was one, I'm not sure, you can correct me if I'm wrong but I think there was one thing that that was not fitting to that scenario So, first of all, am I? right in this? Do you simply mean it for the symmetric? re-encryption or also for communication? Ah, so in my mind, it's not intended for communication except as as my mind it's not intended for communication except as part of, well, currently it's quite common to if you send an email to someone to also encrypt a copy to yourself also asymmetrically right? And also to encrypt draft asymmetrically and all of that, of course unnecessary So that part, it's intended to replace, but it's not I didn't have a use case in mind where you symmetrically encrypt something and then send it to someone else necessarily, if that's what I mean Yeah, that's what I meant so but wouldn't it be maybe use? if we edit in the first place"
  },
  {
    "startTime": "01:18:00",
    "text": "just to enable this also because as I said it's kind of a PQC algorithm it's and I know that this is sometimes required so in some scenarios, the classification level is so high that you don't rely on public keys you cannot rely on any public key scheme then you will need a different mechanism than open PGP or you need to go to passwords password based encryption but i think uh persistent key is much better, because also then it can recite on a token, which is secured by a short pincode, a relatively short pincode So all these advantages. It's just replaced for public key. I mean, if we introduce it in the first place, why restrict? its usage? Would you consider this to me? yeah i think you need to change much Sorry. Yeah, sorry I think I have a small delay but but yeah I think that's reasonable. I mean, currently the draft doesn't say that much about intended use cases, right? But certainly we could say something about it and then yeah, that sounds reasonable to me Then to daniel gillmor question if it is if it is a communication algorithm, which is PQC qualifies for PQC we might be able to implement it as part of our project i i don't i cannot promise it but we would I would try to that we can implement this too Okay, Andrew, let's take the adventure of your audio Hi, how's my audio? Perfect. Oh, okay So, yeah, yeah of your audio. Hi, how's my audio? Perfect. Oh, okay. So, um, oh yeah, so I'm just following on from Falco's question, if, if we, don't envisage using this for"
  },
  {
    "startTime": "01:20:00",
    "text": "Alice to Bob communication, if we're only using this for encrypt to sell for an or re-encryption or archival purposes or so on, is there any point supporting? key distribution for this? So I'm thinking particularly about key service but it could equally apply to WKD double KD. If we don't need to support any distribution of these public keys, because they're effectively stub keys, then would it make sense if we could just filter them out completely? and just not distribute them? Yes. In fact, the current draft says that you're not a allowed to create a public key with these algorithms because it doesn't really make sense you can't do anything with them So, I mean, obviously, that's also up for the debate, but to me, I agree, I don't think they're relevant to key servers Thank you. Hi implementer, I'm wondering, whether, so we currently have this interoperability test suite and the goal of having the specified is so you can have interoperability between two of your own clients, right? So there will need to be some form of interoperability between two clients, even if they're just controlled by the same person. And so I'm wondering, actually, how would you form of interoperability between two clients, even if they're just controlled by the same person. And so I'm wondering actually how you see fitting this into the test suite. Are there changes that need to be made? to SOP, for instance, in order to actually be able to test this across implementations? Right. So we have a brand of SOP with a few tests which basically just tests encryption If we want to test the full round trip the thing that would need to change is that you would need to be able to encrypt"
  },
  {
    "startTime": "01:22:00",
    "text": "with a private key right? And perhaps also well, I think you can already pass a password when encrypting, although it's intended for decrypting the signing key, right? But you might want to encrypt using a locked or an encry private key. And similarly, also verify things using potentially locked private key, right? So that's the that's the main difference both for SAP and for open HP APIs in general, I would say If you have ideas about how to structure that for SOP I would welcome a merge request on that document. Thanks Okay, don't see other questions So I guess on this, then the, you know will evolve, bring a this then the you know the draft will evolve bring uh do bring questions to the mailing list as you think they need to be answered and then process it as usual I got to suggest we switched over to Andrew now and we'll pop back to you down Daniel for the salt discussion later if that's okay thank you daniel huang you thank you The place with the keys I can give you control of the slides, Andrew Thank you. Even if somebody invented them for you Yes, just for the purposes of everybody else's comfort, this is a small experiment in PowerPoint karaoke, so I was unavailable until, you know, just yesterday so I didn't have time to update my slides"
  },
  {
    "startTime": "01:24:00",
    "text": "and DKG kindly made up some slides. So I'm not going to present DKG slides, but drop in my own point wherever I see appropriate so I'm going to give it a go go Excellent. Okay, that's working So just to recap what Patrick said earlier this was a significant topic of discussion at the recent Open PGP email summit and the basic idea is, can we replace human written and human readable key transition documents with something that is machine readable and automation? The idea being that we shouldn't expect people to keep a single key forever. People will actually get rid of key and produce new ones primary keys. This is over, you know, except periods of time. So how do we manage this process of transition between one primary key and another primary key? As we get new algorithms, new versions, or, you know, just new preferences, or perhaps a particular way of doing things as deprecated for whatever reason And we need to be able to rule forwards and sometimes perhaps even roll backwards I'm not sure what the safe complexity, you know, bullet point here is so I'll skip it. Sorry, Daniel So so four, four, automated use, we need something equivalent to a binding signature or a certification whereby a signature by one key"
  },
  {
    "startTime": "01:26:00",
    "text": "over another or something cryptographically equivalent to it allows an implementation to be able to take one key and use that to find a different key and use that key instead So in this process we talk about preferred and fallback keys The preferred key will in a typical use case the preferred key would be, say, a V6 post-quantum key. And the fallback key will be a V4 elliptic curve key, or some such The theory behind this is that Bob wants to find how Alice's key, Bob will look up Alice's key on a key server, or he will use WKD, or perhaps pick through the autocrypt headers, or whatever the key distribution method shouldn't be that relevant The point being that Bob finds that Alice has a new signature on her key and based on that new self-signature, he should be able to discover another key that Alice would rather that Bob uses And this should be possible to do completely transparent to the user Now, once you start talking about doing things completely transparently, then you have problems like, how do you prevent web bugs, or like a deal where somebody basically creates an infinite sequence of these lookups and locks up your machine? There's all sorts of attack services that you could potentially introduce So one of the guiding principles behind this has been to try and keep it as simple as possible So, yeah a quick list of the design goals So the fallback certificate, which is the original"
  },
  {
    "startTime": "01:28:00",
    "text": "key we may want to simply solve revoke this, say, don't use this anymore use this new key, that's all that I want you to use Or what I would say is probably going to be the more common use case, particularly when we go to post-quant is we don't expire the fall key, the original key, but we create the new key, and we have some means of saying you may use this new key if you can support it, and I would prefer that you do but you can all, you can continue to use the old key if that is the only thing that you support. So there is a directional relationship between these two. There's the fallback key and the preferred key and we have in kind of like a graph theory view of things, which have directed arrows pointing between these keys we want to be able to do this in both directions as well because, say, for example, Bob looks up Alice's key on a key server gets the new key the preferred key, but say it has no usable encryption sub keys If there is a fallback, key that exists out there somewhere, then Bob should also be able to find that even if he doesn't already have it So we want to be able to put a pointer in both directions. So we have designed this signature sub packet so that it can work in either a forwards or an in inverse mode. And the canonical scenario will be that you would have a forwards subpocket on the fallback key pointing to the replacement key and you would have an inverse subpocket pointing from the preferred replacement key back to the fallback key so that it can be traversed"
  },
  {
    "startTime": "01:30:00",
    "text": "in both directions One of the other goals as well was that we wanted to minimize the size of the wire format and this ruled out of couple of design possibilities straight away because if we are talking about post-quantum keys, we don't want to be, for example, embedding an entire key that may be a post-quantam key inside a signature subpocket that may have to be generated many times as people change the preferences on their keys and have to preserve this over time The size of the key could potentially get in fact many times as people change the preferences on their keys and have to preserve this over time, the size of the key could potentially get interactively large So I have touched on this pretty much already, so the relationship is between the primary key and the primary key There is no relationship between sub keys Those subkeys are handled using subkey binding signatures as well is only for changing one primary key for another primary key so um because of the two ways we want to use this we want to use it whether the fallback key, the original key, is either revoked or not. So it can either live in the soft revocation or if it is not revoked, it has to live in the direct key signature over the prime key and because there is a theoretical possibility that we could use this, to steal or to claim ownership of a signature made by somebody else's key this is a similar concern that we have with subject binding signatures over signing capable subkeys Those require back signature in this we want the by direction"
  },
  {
    "startTime": "01:32:00",
    "text": "signatures. Because that because we have a signature going in both directions, that means we need the act of consent of both key holders And if those key holders happen to be different, then, you know, one of them cannot, you know, unilaterally override the preferences of the other. So, if terms of, I talked about graph theory, so in terms of the graph type, of them cannot unilaterally override the preferences of the other. So in terms of, I talked about graph theory, so in terms of the graph topology, the simple model is certificate X is the fallback key, certificate Y is the preferred key. There is a forwards relationship that targets certificate Y that is attached to certificate X, and there's a similar inverse relationship that targets certificate X but lives restored on certificate Y And these are done using signatures sub packets Now there are potential problems with this. The most obvious one is, we can create loops. So Certificate X can prefer certificate Y, which can prefer a certificate Thank you on at infinite item. And this is one form of potential logic bomb that we want to avoid Other logic bombs would be say, infinitely reprimed tree structures or indefinitely recursing three structures And in order to keep this as simple as possible, the general view amongst the implementers was that we severely restrict the graph topology that we allow So what we're going to allow, or would we be propose that we're going to allow, is that any particular certificate, may have one sub packet targeting one preferred key or it may have one subpocket targeting a list of fullback"
  },
  {
    "startTime": "01:34:00",
    "text": "keys So in the forwards mode, there can only be one target in the inverse mode there can be multiple targets and those are ordered. The idea being that if Bob finds certificate X, he can then look up certificate one Say certificate Y does not have a usable encryption subkey, he can then depending on the order of the inverse sub packet he can then try certificate X or he could potentially try certificate z. The idea being that if Alice has multiple keys, all but the preferred key will identify the preferred key and then the preferred key will identify a list of all of the non-preferred form fallback keys. And this makes the parsing as simple as reasonably possible. The downside of this is that if you ever do change your preferred key, you have to go and update all of your phone fallback keys So that is potentially a problem but that is a problem for the key owner whereas all of the other problems with logic bombs and so on are problems for the relying party so on balance we prefer to put this problem onto the key owner rather than onto the relying parties Excuse me a second Sorry, didn't want to deafen you there So, how is this actually done? so the the sub packets contains a short header which contains a version on the list of flags and a byte The list of flags that we only currently have two that are defined. One is that there is no replacement key, so it's an explicit"
  },
  {
    "startTime": "01:36:00",
    "text": "null reference. And then the other one is whether this is a forwards or an inverse relationship that it represents and then for each of the targets the targets are represented by three fields The first field is the version of the primary key of the target, so say version 4 or version 6. That will then tell you the length of the next field which is the fingerprint of the target So these two fields are they're exactly the same as we use in the intended recipient fingerprint subpocket and the issuer fingerprint subpocket version plus fingerprint But we also add a third field which we call the imprint and this is inspired slightly by the proposed for a first party attest third party certifications which use not the fingerprint hashing algorithm of the target key, but the hashing algorithm that is used in the signature that this subpatch is contained within The reason for that is to make sure that the reference to the target is always at least as strong as the entire signature that it is in because one of the potential issues with a some with a model like this would be you could have a chat who that it is in. Because one of the potential issues with a model like this would be you could have a shah 2 or shah 3 signature containing a sub packet that refers to a shah one version four fingerprint which would reduce the cryptographic strength of the binding So the idea is that whatever hash you use for the enclosing signature you must also use in the identification So I"
  },
  {
    "startTime": "01:38:00",
    "text": "identification So there's, we use both the fingerprint and the imprint because we still need the fingerprint to be able to do a network lookup So if you want to look this key, this target key up on a key server, you still need to do it by fingerprint The reason for that is that the in imprint will depend on the hash algorithm that the imprint uses is context dependent so it is not a stable identifier so you cannot use it for a network-based lookup So one gives you the stable identifier and the other one gives you the crypticry strength. So we use both of them This increases the size of the packet a bit and for example, if the fingerprint hash algorithm is the cms as the signature hash algorithm that's used for the imprint then you're essentially putting the same field in to twice. But it's a in the big scheme of things, that's quite a small price to pay we believe there are a couple of potential wrinkles with this, but I don't think they're insurmountable One of them is that if you are using partial weights in the web of trust, you have to be careful about the weight that you assign to replace keys If you have a bunch of keys that are all equivalent to each other, you shouldn't really count them as multiple keys in terms of the web of trust for calculating partial trust weights. Because then somebody can game the system the other thing is that these equivalent groups because people are making these signatures and over overriding the self-signatures, these equivalent groups will change over time. So the question probably for a higher level is how do you maintain these? Do you maintain a state history of these when you're trying to say verify"
  },
  {
    "startTime": "01:40:00",
    "text": "historical signatures? And yeah, the issue about the trans- of these when you're trying to say verify historical signatures and yeah the issue about the transitivity you still the relying party always needs to understand the preferred primary key And if that is, for example, a post-quantum signature algorithm then the fallback mechanism doesn't work So these are the some slight shortcomings that we do need to be aware of Andrew will post draft IETF soon I actually did it this morning. It's waiting for chair approval The there is a there is a reference to the previous version of the draft in the meeting agenda there's to the previous version of the draft in the meeting agenda, there is very little change between the previous draft and the upcoming one The only differences are some editories changes that Falcone mentioned to me on the list a couple of weeks ago So feedback please, questions please and some indication of whether people are going to be implementing it soon or whether people would be willing to implement it would be very, very helpful. Thank you very much Thank you, Andrew. Do folks have questions? There were a couple things that came through in the chat I don't know, Mike, or use this if you want to raise those Or anyone else? Yes, it seems to me, not your idea"
  },
  {
    "startTime": "01:42:00",
    "text": "adding a lot of complexity for aligning the EVE photographic strength with the One. And yes, one needs to go and before needs to go, but but the second pre-image resistance of Sha'an is not in question, is it, right? And that's what's important It is not in question at the moment no The question here is whether we can define a generic mechanism. So we this is not specifically targeting Shawa One, although Xia Wan is potentially a minor concern, but there may be issues in the future with other hashing algorithms and we want to be able to handle any issues that might arise in the future So yes, this is a very concern design. It's slightly over-engineering but we preferred slightly over-engineering if that gave us the ability to have completely generic mechanism but we are only talking about the strength of the hash algorithm we used to produce fingerprints and if that's tall we're saying shatoo will fall on one day before we moved on then we've got other problems, don't we? Yeah potentially Again, this is very much belt and braces"
  },
  {
    "startTime": "01:44:00",
    "text": "It is absolutely belt and braces, and I agree with you a D is, there is redundancy in it um braces. It is absolutely belt and basis, and I agree with you, it is, there is redundancy in it. But this is erring on the side of caution Well, but that's not free Anyway, let's take it off. No, no, it's not. No, it's not, no Everything collects caution and redundancy does come at a price, absolutely So this is a working group draft, so the working group can say we have a consensus to make changes to it too, right? It's in its current form it's now draft IETF. Thank you Stephen, for putting the okay button But this is something that we can make a decision on as a working group about whether we need this conservative or not speaking of conservatism i also notice that there is a version of number in the sub packet. Can you explain why we need that, Andrew? Andrew? I mean, I might see what the working group says So the version number in the sub packet so I think is historically, there have been a few instances of sub packets being defined with version numbers in them I'm agnostic about whether it is required So I personally will go with the working group If the working group is happy to leave this version number out and use a different code point for any updated packet format in the future I personally wouldn't have a problem with that that that So, I mean, I guess I have a chair like question, given that this was mentioned as a kind of migration to"
  },
  {
    "startTime": "01:46:00",
    "text": "in Patrick's early discussion, what to be think is the best way for kind of progressing with this to have to in Patrick's early discussion. What do we think is the best way for kind of progressing with this? Obviously, we have made a discussion, but have an interesting we think is the best way for kind of progressing with this to obviously we have made a discussion but have an interim meeting at some point on this topic I don't know what people think is the best way to progress it I it might be that you're having an interim meeting where people discuss this in the next few years is sensible so I just want to see if people think that that would be the case or if that's a stupid idea Andrew, what do you think? I'll let Aaron go first Okay I am heavily biased clearly on this topic, but I really like the idea of having a discussion specific on to how to allow a smooth business migration. This was also one of the points I had in the slides before. Many people are not willing to have the binding if there is not such a thing to find beforehand So having an intermittent in between the two IDFs where we tackle this point is for me. Thumbs up I would be happy to do that, you know we need to fill out at the moment it's the draft is quite technical in terms of the wire format, but it's would do, it would benefit from a lot more higher level context And I think a meeting would would be useful for that I will try to schedule a meeting probably for some time in September So you will see a note about that on the list soon And please, when you see the scheduler, let us know your availability"
  },
  {
    "startTime": "01:48:00",
    "text": "Thank you. Thanks, Andrew. Thanks for being available and being with slides slides Okay, so I think we have one more topic, which was Yeah, Daniel, you want to bring back the salt discussion? And just so people know, is there there, we... As DKG said at the beginning, we have stickers, I put some of them down at the back there by the water thing. So you can grab those on your way out if you like stickers Meanwhile, so the Salt's non-working group draft with something that somebody might want to propose So let me give you control. Yes Okay, so we got like 12 minutes, Daniel, so I don't think we have any other business after this, so go ahead Great, thanks. Yeah so before I say anything else I have to uh 12 minutes, Daniel, so I don't think we have any other business after this, so go ahead. Great, thanks. Yeah, so before I say anything else, I have to say that this is not a based on my idea. This is, you know, credit goes to Sequoia for implementing and coming up with the idea of having a signature soldness notation. But that being said, yeah So what is the idea here? So we added a salt to V6 signature for a couple different reasons One of them is to hedge again hash algorithms being compromised especially, you know, chosen prefix attacks and another is to protect against fault attacks in the terministic signing algorithm such as EDDS And basically the idea here is that you know, we might want to do something for V6, sorry, for V4 signal"
  },
  {
    "startTime": "01:50:00",
    "text": "as well, for the during the time that we see do something for V6, sorry, for V4 signatures as well, for the, during the time that we still have to create them in some scenarios where V6 is not fully rolled out or so supported yet. So the basic idea is to create a signature note where V6 is not fully rolled out or supported yet. So the basic idea is to create a signature notation with a random salt in it which is, you know, perfectly backwards compatible as long as the notation is not marked as critical all implementations should and as far as I know do just ignore it so you know that's a safe place to put some random data And the length in the draft is dependent on hash algorithm same as for V6 signature. So this is unlike Sequoia's implementation which is hard-coded 32 octets, but okay, this is my minor, minor difference So, yeah, as I said, the main security advantage of this is to protect against fault attacks Basically, if you sign the same thing twice with EDDS and you accidentally or if an attacker is able to introduce a fault in one of the in the signing process during one of them you might leak something about the private key meter or the attacker might be able to learn something about the private key material so the idea is here is to basically never sign the same thing using EDDSA and"
  },
  {
    "startTime": "01:52:00",
    "text": "by always having assault somewhere in the in the in the in the in the in the becomes the input to EDD right So on the downside, downside unlike v6 v6 signature consulting it doesn't necessarily protect against common prefix attacks because the notations go after the message data in the input to the hash so you know I should also make it clear that none of this is intended to replace V6 or somehow slow down or make it unnecessary to deploy V6 That's not the intention. We should still deploy V6 obviously, but for the time being while we still create v4 signatures as well this is sort of a stopgap in the sense of, you know, better than nothing, so to speak um so yeah, the current status is obviously, Sequoia implemented with the notation name salt at notations. Sequoia, pgpgp.org open pgpgp.js, v6 and now gopbpgbv3, copy it with our own notation name basically because it didn't occur to us that we could reuse theirs, but the KG pointed out on the list that we could have done that, which is of course, true But so that's what we, that's what we have for now. And there it depends on the hash algorithm as well the length And what this draft basically says is, you know, could just register solved by itself in the IANA register so that we don't have to use these long notation names with the domain"
  },
  {
    "startTime": "01:54:00",
    "text": "in them So yeah, that's about it So I guess are there, is there any feedback and or other thoughts and is this something that the working group feels that we should adopt? So I had a question about the by binding it to the hash algorithm length That makes sense if I'm generating it, but I don't understand what to do if I receive it and it doesn't match Are we saying that this invalidates a signature, if you know about this? or is it, like, what's the, what's the what is that binding between the hash algorithm and the salt length meet? Yeah, that's a good question and the current draft does really say my is that binding between the hash algorithm and the salt length yeah that's a good question and the current draft does really say much about it mostly i just copied that from the V6 signature salt thing. So I would kind of propose that we do the same thing as there but I'm open to thoughts So if we do the same thing, then implementations that do same thing as there, but I'm open to thoughts. So if we do the same thing, then implementations that don't know about this will accept a signature that implementations that do know about it will reject. That seems a little bit strange Right. Yeah, perhaps we should accept any anything. I'm not sure Okay, other, so we don't have other anybody else in the queue here So I guess it has me has been some discussion on the list about this. I suspect we should let that discussion continue and we'll see where it lands, if it lands on an adoption call or if it lands on some other place. So I think we'll just continue discussion on the list I think is the is the outcome here Okay, that's good. Thanks, thank you"
  },
  {
    "startTime": "01:56:00",
    "text": "think I think that's more of this else Yep we're just on any other business. We're on any other business? Does anybody have other business? If does then thank you and we, like we said, we probably have an interesting you and we, like we said, we probably have an interim meeting about the replacement stuff in the coming months and the post-quantum and other work is going to progress and will be discussed, I guess in november when you're all coming to dublin i hope so i'll see you there thanks everyone thank you to our presenters if you want stickers they're up in the front or grab some at the back there take as many as you like thank you Thank you right first session complete. Yes What's that? Oh, nudgeton ton Part of me wants to adopt that draft so we can just say that we process to draft But then yeah, yeah I mean, there's the amount of just say that we process to draft. But then yeah, yeah. I mean, there's the amount of... Well, I mean, protecting against fault attacks is not trivial, right? So, people can inject fault I mean, I'd love to see a"
  }
]
