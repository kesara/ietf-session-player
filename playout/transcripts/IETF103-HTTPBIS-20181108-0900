[
  {
    "startTime": "00:01:23",
    "text": "so it\u0027s really nice to know we\u0027re gonna get started in about one minute you know dress I mean generally we don\u0027t have that here except for a Roy yeah I think we try to go to culture them okay let\u0027s get started and that means we should probably turn our music off all right now sad well coming to the HTTP working group if you\u0027re not here for the HTV working group you\u0027re welcome to stay this is the note well these are the terms under which we participate in the ITF regarding things like the code of conduct your conduct and intellectual property if you\u0027re not familiar with it you can find out more on the ITF website the mark can you explain the photo on the previous slide I\u0027m very confused well we can talk about it after the session it\u0027s a very special place usually we treat this slide seriously but we kind of just move on but but given recent events we wanted to dig in just a tiny bit and with that I will hand off to my well to one of my co-chairs hey there\u0027s been some talk about this last few days and I I just wanted to notice note there with respect to the code of conduct our working group generally is very very professional and we appreciate that I have been in four different sessions over the last two days where expletives have been used at "
  },
  {
    "startTime": "00:04:23",
    "text": "the microphone reference to other people\u0027s contributions so I just want to re-emphasize what we expect from professional standards from this group as well as all of the groups but I use your the fine folks that get to listen to me so thank you for the time and keeping it in mind and not to belabor the point but just add to that I think you know we\u0027ve worked really hard to build a good culture in this working group I don\u0027t want to see that continue so if you see something concerns you please bring it to our attention we will be listening so the blue sheets are circulating check we have a jabber relay thank you we need a scribe for the session we have a scribe fantastic so smooth and and last let\u0027s welcome Tommy we now have a third chair for the working group so welcome Tommy if you don\u0027t know Tommy it\u0027s Tommy Pali from Apple hello Tommy thanks for helping out so today\u0027s agenda we talked about core last time this time we\u0027re gonna go through the active extension drafts we have a number to go through but some of them I think are gonna be quite brief we\u0027re gonna have a discussion about quick and HTTP and then we\u0027re gonna end up with proposed and related work does anybody have any agenda bashing to do mr. Iyengar you coming to the mic right now okay good all right let\u0027s go ahead and get started then so first active extension drafts we have the CD and loop specification Patrick is active chair on this one you wanna summarize where we\u0027re at oh yeah well the discussion here is we opened working group last call for the seeking further comments there have been no further comments that I can think of since I opened last call although there were a number of comments during the development of the draft so I feel good about that I don\u0027t feel it\u0027s been ignored you know in general like expressions of interest to implements or statements that you never implement this for you know whatever specific reason or nonspecific reason you might have are certainly interesting at this phase of the process and I think that last call is supposed to finish up you know this week I obviously won\u0027t get around to thinking about doing shepherding or declaring consensus or not until you know till next week so that\u0027s kind of where we are open the mic for comments one tolson one of the standards we have started to hold for some of these things is deployment or at least some experience with implementation do we have any of that I\u0027ll get to this Mike yeah okay thanks "
  },
  {
    "startTime": "00:07:24",
    "text": "mark Nottingham a draft author and implement or or at least employee of implement or fastly has implemented and deployed this extension Chris lemons from Comcast we have interest in implementing and deploying this extension this is simple straightforward we should do it Nick Sullivan CloudFlare co-author CloudFlare has plans to implement this very soon and the only sort of objections that I\u0027ve heard to this draft have sort of been worked about amongst the active participants there so you know obviously the last call is still open for a couple more days but it does seem like we have consensus to you know move forward and send this one send this one off to the eyes chief so thank you to the authors and a nice sort of a cross industry collaboration there nice to see it\u0027s gonna register variance is the next thing on the agenda so the variant spec is waiting I think for implementation experience to follow nicely from Martin\u0027s comment there I\u0027ve been talking to lathe Hedstrom on traffic server and he\u0027s gearing up to implement that he\u0027s been distracted by some other stuff but he\u0027s relatively confident that he has an implementation strategy for that that will be successful if we look at the issues list no no not sad hometown damn it okay let\u0027s see you know thank you that\u0027ll be helpful Terk mode yeah so we\u0027ve got three open issues now it doesn\u0027t mean that that\u0027s only might happen until I think we get more implementation experience this is where we\u0027re at the first one we had a request from Jeffery askin who wanted an eye on a registry for any content negotiation mechanism that uses variant which I I go back and forth on I understand that having you know lists of things that do certain things is useful for finding those things but it\u0027s not serving the traditional function of a registry which is disambiguating and and provision preventing collisions so I guess I\u0027m a little wary of creating yet another hurdle for people to do things you know if you have to register something to you know have it conform what what do people think about this would this be a useful thing you know we already have registries for the actual headers that they would be creating so "
  },
  {
    "startTime": "00:10:25",
    "text": "the question is do we need another registry for for this or or I guess in core we\u0027re talking about pulling and I think we\u0027re going to pull the HTTP header registry out of the message header registry one of the things we could do there is create a column in that registry that says this is a client in mechanism as a boolean any any thoughts does anyone care is anyone awake okay let\u0027s start does anyone care I guess I care slightly Chris lemons the registry with extra beta is better than multiple registries so I like your boolean suggestion okay so if we do this at all we should put it as a bump on the existing registry rather than a separate one okay so tell us what\u0027s the argument for doing it at all rather than relying on the definition of the header fields well I think Jeffrey\u0027s point and and I\u0027ve seen other cases where this is I think we have another issue somewhere about this as well it\u0027s just kicking my mind for the moment but you know it\u0027s it\u0027s nice to be able to say oh I\u0027m looking for things that do X in this case it\u0027s variants I think the other one was client hints and and if you\u0027re coming from the outside and you don\u0027t know how to find out what the complete set of currently defined client ins is then you have to search through everything and get that knowledge and so having a registry is one way to collect that knowledge in one place I I\u0027m finding it hard to articulate why but I feel like it\u0027s not a good thing to do that yeah I guess maybe I\u0027ve just been conditioned to create as few process hurdles for doing things as possible yeah I think that\u0027s part of the part of my my concern there that in the fact that this is this is just one aspect of many that relate to the definition of header fields and I\u0027m not sure this rises to the level of importance that would require that that sort of management right but what about creating an extra column in the header registry would that be more palatable same same concern I think whether it be in a different registry or as an annotation on an existing registry it\u0027s still overhead right well we have you know requests for this is it a variant header then we have a request for is it a client hint we would obviously have to have think have things like is it cashable right is it request as a response I can think of probably 15 or 20 columns right now does it relate to the content rather than the request itself is a connection-oriented all of those things sort of rise to that level and once we start down that path we have to really think hard about where weather ends right it strikes me we have a github repos one of the things we could do is just create a wiki page for hey these are the current client hints and we "
  },
  {
    "startTime": "00:13:27",
    "text": "could do it for all of these things and it would be less and not normative reference them but no I just don\u0027t know for information purposes his is the following so and we can probably link to them from the working group website so that people can find them easily yeah and populating columns and populating the matrix with question marks which would be photo just even a header matrix yeah right how do people feel about that Mike Bishop I can see doing it on github I\u0027m kind of sympathetic to your comment there that it\u0027s more like developer documentation and so I\u0027m almost wondering if this belongs somewhere more like Indian would they accept a contribution femen\u0027s sure that\u0027d be one thing I mean I think we can do it on on the wiki just as an informal non normative product of people who are interested and if people want to copy that into other places that\u0027s fine too okay it sounds like we have a direction on that that I\u0027ll actually like that I think that makes sense next issue adopt structured headers I think this is relatively straightforward we\u0027ll talk about structuring headers in a minute I think that it\u0027s getting mature enough to be able to do that the final one is I think once we get some implementation feedback one of the more interesting and obvious things to do with variants is to specify it for cookies so that people can create define how their responses vary on cookies and so that\u0027s right now TBD but that\u0027s I intend to do that at this point before we ship this back sorry just to be clear you\u0027re not talking about the the sort of level that we have right now with vari you\u0027re talking about something a little more granular fun great yeah that\u0027s the point of variance right and so this would be a sort of bespoke addition to two variants specifically for cookies or it would it be something that was sort of extend into other header fields as well it would well everything that is variance is bespoke it\u0027s always per header defined per header so I\u0027m a little bit befuddled still about the level of granularity you have in mind I mean clearly some people use cookies as in in purely opaque fashion and so it\u0027s really a presence or absence thing some adventurous Souls and invoke a great deal of semantics into structures within them but don\u0027t really generally disclose them and I\u0027m wondering whether your intention here is to offer in them opportunity to disclose that semantics and if not what exactly you have in mind so no not to do that that would it be more like the approach we took in key "
  },
  {
    "startTime": "00:16:28",
    "text": "which we decided was not viable I think that what I have in mind and it\u0027s somewhat hand wavy is that if we can describe a function that is useful for an interest instead of use cases people can adapt their cookies to match that if they so choose if if it doesn\u0027t work for them they don\u0027t have to use it they can continue to fall back to very as they would know and I think the challenge is defining something that\u0027s useful enough in enough use cases to make it compelling and if we can\u0027t do that then we won\u0027t do it every again just as a suggestion for a possible way of confirming the usefulness of the work product it might be great to get first gauge from the community of whether there are people that with such things would go and retool their their cookie production because if you have people who are willing to to adopt it then going and figuring out what use cases sounds like a great exercise but going through the use cases and hoping that they\u0027re shiny enough sounds like there\u0027s a failure case there that might be pretty I\u0027ve had enough discussions with people creating websites in my day job and when this comes up there is shining a nice I think it\u0027s worth trying but I think the next step for this is for me to sketch a design in the issue and then if people want to discuss it there we\u0027ll see how far we get so next up is BC P 56 bits the only open issue we have on bc p 56 bits is an editorial one to stop referring to the RFC 7230 series of documents and refer to HTTP core and my previously only co-chair and i have talked about this in the past for quite some time i think he would like to see these documents move on into the ether yeah i\u0027m hoping we have a working group discussion here whether or not we think this is worth holding open for HTTP core or whether this is more of a i have been known to call this an attractive nuisance having this document hanging around it\u0027s a very interesting document with very wide applicability you know it will forever you know want to be updated to current circumstances but i think there\u0027s also a lot of value in getting it published as you know acp is used as a substrate like every day and that\u0027s what it\u0027s a bill to so you know park and i guess have just a little bit of disagreement amongst when we shouldn\u0027t victory and move on but um other than that i don\u0027t i think we\u0027ve come to a help a compromise what i\u0027d like to see happen i think is i\u0027m comfortable enough with the state of core that i think we can go ahead and change the references go through last call put it in the RFC editor queue and let it wait until the "
  },
  {
    "startTime": "00:19:29",
    "text": "core catches up so i like to think of this as sort of RFC purgatory yes Thompson I think 72 thirty is fine and I would rather not see it live in purgatory it\u0027s currently on our work slate and I\u0027d like it off the works late because we have quite a lot of things on that list already and I don\u0027t see there being a a significant benefit to having it reference to the core stuff given the amount of work that we have on core remaining I\u0027m not as confident as the the editor eleven of that document that will they will finish that work in the sort of timeframe that I\u0027m thinking often and yet your wellspring of confidence is that or else where is this that\u0027s that\u0027s a charade I I think that have my intentions for this document to be very long-lived I do not want to go and revise it again any time soon and I think the the advantage of course is that the semantics document brings a much better introduction for people this this document is effectively an entry point into HTTP for new people new to the community and so I want to give them the best possible launching point for that and and so I think cores is a good way to do that and I\u0027m not too worried about it not being on our slate because once it\u0027s gone to last call it\u0027s not an our slate good so apologies it looks like we missed a comment or a question from the the jabber room so I\u0027d like to go back a topic if that\u0027s okay that was from Thomas Peterson will issue number five for nine addressed scenarios where applications use cookies to determine the user preference that overrides accept language and do you need to drop a relay because I can take that on if you haven\u0027t got one I thought we did Oh Chris upgraded from jabberchess oh there was an opportunity I mean it was the sound upgrade okay so I will I will take over as jabber relay from the Thank you Thank You Ted very kind I think that\u0027s T I think that\u0027s an important use case for variance and I\u0027ve talked to that with at least one or two people in the week here and so that\u0027s I actually think that probably worth a new issue just to make sure that we do address that through some mechanism whether it be the cookie mechanism or something else because it\u0027s an important use case back to the issue at hand you have IT open issues and encore now realize that that\u0027s probably not the same magnitude of work as the idea that we have open on on quick but we discussed a 15 this week we have a number of them with proposals and a "
  },
  {
    "startTime": "00:22:29",
    "text": "number of them were editorial so I I share a little bit of not pessimism but conservatism towards you know when when core will meet its deadline however I think we might be able to scope the issue to you know just making a blocking reference and setting off to the editor versus calling it done where the reference only does 70 to 30 right I I think 70 to 30 is sufficient I think we can put some you know parenthetical language suggesting that you know there might be you know others language is coming that\u0027s Mike Bishop I think particularly for people who are coming and I I agree with you on the as an introduction having that semantic split is important and I mean we can try and address it in this document and say the thing we\u0027re referencing didn\u0027t handle this as well but we will get to it or we can just point to the thing that gets it right and move on so I\u0027m in favor of go ahead and finish the stock off I have a blocking reference if nothing else I\u0027ve convinced somebody that HTTP core will be the document they live with for the rest of their career so yeah that\u0027s good it\u0027s an incentive thank you it that way I think if I have to pick between right and fast like right in the long term and I point out that one important feature is if we we get working group and then ITF consensus in the document just because it doesn\u0027t have an RFC number yet we still have consensus and we can point to it a laxity of a thought here not really I can sort of see both sides of argument depends how quickly want to get the document published I mean you can put a reference to existing RFC\u0027s and add a comment saying how RC editorially other documents get published within certain period of time please have where there are Frances I noticed she said published and not done I said that yes but I\u0027m not entirely awake to understand why it matters I think you\u0027re probably reading more into it that I intended he wants to get published I want to get done I think so one Thomson we\u0027re acting like these things are immutable again and it sucks that we have this process that ends up with these things that are nominally immutable but to pretend that either one "
  },
  {
    "startTime": "00:25:32",
    "text": "of these documents is gonna stay the same for the rest of someone\u0027s career is a joke but it\u0027s a funny joke it relieves us yeah so let\u0027s not worry about it too much and move on and we\u0027ll let the editors and chairs work this one out I\u0027d say you have your input yeah I\u0027m also thinking that you know once should you choose to publish the document quickly I hope that a revision of it is not going to take you know 15 or 20 years since the document you are trying to update so I think it\u0027s going to be much cooler well I have so much time to work on these things okay I think I think that\u0027s correct we do have our input and I think now that we have three chairs it\u0027s easy to make decisions because we have a tiebreaker although I think I\u0027m now having flesh I think we might see some Minority Report memes come out also we\u0027ll see okay let\u0027s move on secondary certificates god that\u0027s so tiny and I\u0027m getting all - my eyes are bad see that\u0027s true but then I\u0027d have to do something we\u0027ll find out all right so Claude Claude recapping from last time round there are two ways in which secondary search make sure it\u0027s easier to attack and we would kind of like to fix that first attack vector is that if you\u0027re able to get a Miss issued search you now don\u0027t have to get a Miss you should search with the victim domain and the attacker domain in the same search you can have two separate search and there\u0027s no obvious link between them and CT logs so that\u0027s a pain and then the second one is assuming that you\u0027ve been able to compromise a victim cert it is easier to use for whatever value of easier induced navigation is easier than hijack and TCP connection we know that both are possible but it\u0027s also much easier to do social engineering than BGP hijack I see so when we return we will have the new version of slots or "
  },
  {
    "startTime": "00:28:43",
    "text": "all right so slides so stepping through the designs that are immediately obvious there\u0027s the one that we had last time where the certificate can explicitly state what primary domain it is should be used with that is a little bit sad because then you have to either get it right in deciding which domain the client is likely to have connected to first or you have to enumerate the entire set of possible demands and every search and you need to reissue all the certs when you add something to that set neither one of those is particularly nice and the other issue here is that it doesn\u0027t let CD ends coalesce across multiple different unrelated properties that might all be hosted on the same CDN which is something we\u0027d kind of like to be able to do the next slide what we talked about last time around was to try and define a pool that wasn\u0027t necessarily based on just pointing to that first search and so we had ideas of let\u0027s put an extension with a tag that proves you owned something and we started trying to work through that but in a sense the last IETF and figure out what that might look like probably the extension contains a domain because that\u0027s what CAS know how to validate easily and we\u0027re trying to handle both the case of an origin with lots and lots of domains which a CDN that handles all their certain answer - once looks like one of those but you can also imagine large entities like Google Microsoft Facebook would fall under that as well and then there\u0027s also the case where CD ends that let customers bring their own certificates and that proves to be a little bit more challenging next slide the reason that that\u0027s more challenging is that with that authorized domain thing you you don\u0027t want customers to put other customers on the certificates and they shouldn\u0027t have to but we want to be able to coalesce across those certificates and that implies that there needs to be some amount of not just point to the primary search but pointing to a previous cert on the connection and some way to build the cheat which may be a little bit unfortunate but I think that\u0027s where we come to with requirements next slide so this is kind of what it would look like what we sketched out during the meeting in Montreal where all of them are tagged with some affiliation and so long as the affiliation is the same with everything you\u0027re fine but that\u0027s a challenging path for a bring you insert CDN next slide now we also had a proposal along the way of well let the CDN sign off on "
  },
  {
    "startTime": "00:31:46",
    "text": "customers tagging their search to be affiliated with the CDN and then everything\u0027s related with the CDN can be used together that actually turned out not to work because assuming the attacker has already compromised the victim domain the attacker then signs up as a customer of the CDN gets a legitimate search for the attackers domain that\u0027s also affiliated with the CDN and they can masquerade as the CDN with regard to those two customers and that\u0027s exactly the attack we\u0027re trying to prevent so the outcome from this is anything that\u0027s marked with the affiliation has to only be held by that entity so the CDN the search that are marked as affiliated to the CDN need to be held by the CDN not by the customer so next slide so if we\u0027re trying to get around all this every problem can be solved by another layer of interaction right and in this case it\u0027s not quite indirection it\u0027s putting the two of them together so if we have a requirement and say that requirement has to be something that\u0027s already been proven if you have the large origins with lots and lots of certs it\u0027s very easy you just work every cert as affiliated to some domain you own and requires the same domain that you\u0027ve owned and any of this search can be used together because any of this sort satisfy the requirement for any of the other certs and if you have a CDN that allows customers to bring their own certs next slide then what you end up doing is ask every customer to put requires the CDN and then the certificate the certificate from the CDN will say this is the proof that I am the CDN in question and there can be a requirement and the one that\u0027s used in TLS that\u0027s okay doesn\u0027t matter that extension doesn\u0027t mean anything when it\u0027s used in the primary handshake but the requirement they are would say you can only use it as a secondary cert if you\u0027ve previously proved that you hold a certain that\u0027s authoritative for this domain so you\u0027re building a chain there the effect of that is that you do one additional exported Authenticator before you can use an unrelated certificate and I think that\u0027s a reasonable price to pay for each CD end on the connections where it wants to provide secondary certificates has to first prove that I am the CDN no thanks Lud hardhat warning this is something that we\u0027ve sketched out an email there are lots of details and the text in the PR is still kind of raw there are lots of comments from Martin that I haven\u0027t addressed and I\u0027m not sure exactly how best to address them so there\u0027s stuff to work out here but I think for the requirements that we\u0027ve designed the requirements that we\u0027ve articulated this is a design and possibly the best design "
  },
  {
    "startTime": "00:34:49",
    "text": "that I\u0027ve been able to come we\u0027ll come up with so far yeah I\u0027m sorry I got really lost somewhere about two slides ago I\u0027m seeing other people maybe are lost as well so um okay can you go back one more okay so I\u0027m gonna connect the midgets if I understand this correctly I\u0027m gonna connect to my hypothetical example we have example.com an example that org hosted on cbn.com right oh I guess yes secondary one secondary to but Isis anything right so and those two are unaffiliated and then you have but they both are cost okay so yeah I guess you think the thing I wanted the thing I want to do is I want to host like Mozilla and fastly on CloudFlare CDN you know or Mozilla and Mozilla unlike fastest applause flare or fastly and this to have neural shape each other right that\u0027s like that\u0027s the thing we\u0027re trying to achieve um and that but there was nevertheless use the same connection that\u0027s that\u0027s the thing we\u0027re trying to achieve correct correct okay we\u0027ve got like a dozen people on me Tyco so we\u0027ve got to use the mics okay so this is so this is this is the slide that okay so what so as I understand what you\u0027re saying is in order for me to do this I in order for in order for in order for you to mainly because in the given CDN they have to put in they have to put another extension in the certificate that says I am posted on this CDN correct I mean in this this seems like - introducing like an entirely new concept into the into the into the like validation and trial speken ISM that we\u0027ve never had before and to be clear I think that\u0027s what you have to have to be able to do secondary search correctly yeah exactly you\u0027re changing the way in which the certs are used so it\u0027s not totally surprising that you need the search to have a slightly different semantics after all the the attack that arises with secondary certs is when these certs are used outside of their intended context and so the effectively this requires declaration is a declaration of the intended context for the search to be used I think I think I\u0027m back as a Tamron or didn\u0027t happen like I know about reasoning about "
  },
  {
    "startTime": "00:37:50",
    "text": "this not that worries me I don\u0027t know maybe I\u0027ll start reasoning about this but I surely do so when we discussed it at the hackathon Nick actually had a fairly simple algorithm for Trend for tracking with resurges valid and it was basically just you take a set and as certificates are accepted as are considered valid on the connection you take the subject the sand and the affiliated tags and throw them in that set and then when you get a new certificate you look at its requires and if there\u0027s any overlap between requires and the set already on the connection you accept the sir overlap between the affiliated know between the requires whose requires the new Emilio you have this set of names right yeah and you\u0027re building the set of names and you start with the set of names just being the name that you initially looked up right and other names on this work where you start with the name that you originally looked up and you validate the cert on that basis correct now that cert gives you a pool of other names correct you add those to the to the set and that sort also has some requires that does not matter because that\u0027s the cert you got until us this is all TLS right well kind of but where do I go to talk about how this works in TLS so maybe I\u0027m just like confused or what attack this is intended to stop huh start with that like the this our dumbest form of this attack is the one where um I um I operate some big attractive site and I obtained through illegitimate mechanisms a certificate for um you know for some other victim site and I host that was a secondary that I cater why does this stop that we\u0027re looking at is the legitimate search so you acquired the private key to their actual circuit why is that I\u0027m interested in because that\u0027s the attack the certificate transparency can\u0027t catch if you get a Miss issued certificate for that other domain and so you can stick whatever you want into that cert then this forces you to put a requires to your domain in that cert that goes in CT log and then there\u0027s there\u0027s basically a trail of breadcrumbs from the attacker back to who did it sure and so I I\u0027ve got a week of like vulnerability wallet while all those ESP expert as far your windows happen no does not because the "
  },
  {
    "startTime": "00:40:52",
    "text": "the concern the concern the situation right is how is that is the DNS hijacking is there not any duty Asshai to assert and they didn\u0027t care so long as they could do it within that one week they would issue a cert that covered both domains and then coalescing we pick it up I mean yes this was this was the objection to coalescing from Ryan\u0027s levy I mean no but this is the objection to removing the D at the DNS check from coalescing I mean so I mean if we think that is not like an issue okay but like that then I\u0027m not sure why I\u0027m fixing this either I think that if you have the DNS check in there then this still works but with the DNS check you would still have to subvert DNS for that to work if the clients doing the DNS check if you want to say you can\u0027t use origin to get rid of the DNS check that\u0027s fine I actually think that\u0027s probably even a good idea but that\u0027s independent of secondary service my point is not to make the situation worse and I agree with you if we want to fix the situation we need to change origin okay okay I guess like I guess I mean this is sort of an odd threat model and that like the presumptive the purlins situation is that like going back to others I thought your understanding was it is that like how have I compromised user certificate why didn\u0027t your assumption I compromised our certificate in like what like the you like if I come as a CDN like things are really really bad right and that\u0027s the most obvious way to comprehend so there\u0027s some other your some other mechanism aligned there was not a compromised the CDN the converse is just difficut honestly no because the scenario that that was raised as an objection was assume it\u0027s assumed that he has been compromised somehow okay I think it\u0027s a fairly remote possible I mean I guess he has some theory we like there\u0027s like a timing Oracle on the key without like remote compromise can we have discussion of the mic please so the the main reason that I like this is that it prevents an attacker who\u0027s compromised the certificate that doesn\u0027t have the CD ends name in it to hijack a connection and that\u0027s that\u0027s the main thing that this brings so in order to hijack a connection or in order to use a secondary search that secondary cert has to have the name of something that\u0027s in the primary search and so if you just issue a certificate for your site and you\u0027re using it independently on your site and someone compromises that they can\u0027t use it on a secondary connection and that that\u0027s what this provides it\u0027s my understanding right they can\u0027t have any secondary "
  },
  {
    "startTime": "00:43:56",
    "text": "certain needs to that that you issue has to have a require that includes San that\u0027s on the original connection and so if you\u0027re just having if you have a site that\u0027s hosted independently and you don\u0027t have any requires then it can\u0027t you can\u0027t be used to for secondary certs that\u0027s right but I want try to restate it so let me see if I understand this properly which is I you know I operate you know I operate example comm I like have like hosted like my own Amazon ec2 instance like there\u0027s nothing to do anybody else right and and then someone who operates you know example it some attacker is attacker comm they have like an account on a CD yet and what they do is they hack the hack my machine on Amazon the exfiltrate the key and they call it the CDN and they say I want to be like the customer for example calm and here\u0027s like my priority my certificate and pleasing Stephanie\u0027s that was suppose to happen here they would have to actually be able to get a new certificate issued as you in order to do that but if they can\u0027t they type refined prevent detect and prevent as I understood it is that because I\u0027m not hosting the CDN I just like an ordinary difficulty no requires and so they need to like import that certificate and that key into the CDN right not just import it well the attack is yes they could put it on their own machine and present it as a secondary search so anyone who would connect to them would believe that connection is acceptable for your domain okay but I mean so um Mike do you have more slides to get to know okay I think we can spend another five May ten on this but not more than that I mean but they attract the traffic somehow right so sure okay so so so they take it and they put it on their own attractive site phishing emails or whatever that gets people to connect to that site this is DK key so I just want to just clarify what defense we think this offers additionally and we think that the defense is that these certificates are going to the breadcrumbs that these certificates leave in the CT log is the name of the CDN where the attack took place no so so there are two different attacks one of them depends on the CT log so the first thing that this is intended to stop is the case where the attacker could just go request a search "
  },
  {
    "startTime": "00:46:58",
    "text": "that has no obvious link to them from a CA so Mis issued sir and then try to reuse it so this forces one on the missus shoot cert to have a reference to the site they actually control okay does it it looks to me like it hasn\u0027t a reference so customer to calm there that\u0027s the victim right yes sorry it doesn\u0027t it\u0027s in and I\u0027m I\u0027m the attacker I operate customer one I happen to be on cbn.com so it doesn\u0027t give a reference to the site that I control it gives a reference to the CDN that I\u0027m a customer of correct so the bread comes we\u0027re talking about is at least as you said in the quotation earlier one layer of indirection so if there are say four CDNs on which such an attack is plausible you\u0027ve reduced your breadcrumbs have have reduced the search space for the attacker by by 75% so you\u0027re considering the case where they would take the compromised cert and go to the CD and to the correct CDN and upload the cert again under their account yeah I would have to check but I suspect if a second customer claimed control of the same domain that one customer is already having a serve we would not permit that interesting it\u0027s about anger yeah I think maybe rephrased attack that I think that the thing that this changes about the security model where you\u0027re trying to prevent the attack is that this reduces the DNS compromise to the compromise of the primary domain if I understand that correctly this is like you must compromise the DNS for the primary domain to be able to attack this thing and if there\u0027s any property that violates it for instance that the CDN that you\u0027re uploading to does not check for example that the customers associate the domain and he uploads a search for another domain then that would violate that constraint or that the primary domain is actually compromisable that would also violate that constraint as well if the primary domain is compromisable then you are no different than you exactly would with regular TLS which would be I\u0027m gonna cut the queue after dkg and and start thinking and talking about how we can get to some agreement here whether we need design team or something else I think we need some some more time to discuss this in detail most a smaller group so that we can work this out but um what this does is change the the requirements for an attack of a key compromise on this issuance - from the "
  },
  {
    "startTime": "00:50:00",
    "text": "existing state of play which is you get the missus wants or key compromise and then hijack bgp dns port what have you - that or get the key compromise and whatnot and then somehow compromise the the CDN effectively if you can join the set that the the closed group that\u0027s being referred to here by this new label you gain access to the same capabilities so we are talking about systemic Lee making the overall system that much weaker now of course we the primary defense is the fact that someone is the certificate itself and the authentication associated with the certificate itself but these secondary mechanisms are now being broadened to include a new set of things I\u0027m wondering if we need something simpler in that case keep in mind that this is this is a sort of a secondary defense that we have that is somewhat underpinned by CT and always almost sorts of things but maybe the simpler thing is simply a flag saying I\u0027m okay being a secondary and and then we avoid some of the the the rationale require of stars risk and require star is that but then you this has proposed going much much further than that and the complexity of thinking through the whole affiliated bit and the the require star thing gets sort of gets me to the point that I think well mr. doner maybe maybe that\u0027s not so bad it so I am personally I am hesitant to do that simply because if we do if we take the case of all the customers put the slag on that then there\u0027s effectively no difference between having this flag on most certs and not doing anything versus what the draft currently says the same attacks are possible on the sender requirements TKG again so sorry in the earlier discussion I think one of the problems is that there are two attacks and they\u0027re quite distinct the business issuance and then there\u0027s two key compromise of an existing cert and I feel like in the discussion sometimes in the back and forth that we\u0027ve just had here we start talking in one scenario and then the answer the solution the like oh this is okay because it pops up in the other scenario and I don\u0027t think that\u0027s actually a legitimate way to do this kind of argument so in the discussion that I was raising earlier where I was like look what at what are these breadcrumbs we were talking about mrs. yards and the end you said well that\u0027s okay because then the CDN is gonna check these two "
  },
  {
    "startTime": "00:53:01",
    "text": "things the note that there are two customers on the same domain with the same domain in their network but that\u0027s a key compromise attack which is distinct so in the missus attack I want to scope my Commons with that in the Miss assurance attack where I\u0027m capable of getting a certificate missus you to me with whatever fields I won\u0027t put in it okay then the only breadcrumbs of this leaves is that I deployed this Miss issued cert on CDN X that is true so if you go to a different CDN than the one that\u0027s being attacked maybe there\u0027s no CDN this being attacked right now I\u0027m attacking ecers Amazon instance then in that case all CD ends are a different CDN one and then I get a Miss issuance if you go there with the missus you\u0027d search then that CDN could serve it a secondary or as primary so yes so that\u0027s the so the defense that we\u0027re talking about in the Miss issuance case is don\u0027t worry we\u0027ll at least know which CDN this miss issued cert was deployed on and you can go hunt it down from there I just want to make sure that word that that that\u0027s the that\u0027s the level of defense we\u0027re talking about yes I think so on that case so the attack that I was initially looking at was where the the primary domain was the attacker zones the attacker self hosting but you I think you\u0027re correct that there is still if you go to a CDN with a Miss issued cert well at that point you don\u0027t need secondary service but you can go to a CDN with two different shirts and then have the CDN serve your attack traffic why don\u0027t you need to seem very sir why don\u0027t you need a secondary like cuz if you get a Miss issued cert and go great and count on a CDN yeah then just I can\u0027t point the DNS at the CDN fair enough okay it sounds like there\u0027s a lot more to talk about so I think first of all take you to the list and if the folks who are engaged in this discussion want to get together while we\u0027re all still in one place I think that might be a fine thing to do so Mike if you want to try and coordinate that if folks are interested in that please talk to Mike and I\u0027m looking at the people who were going to the Mike to questions who\u0027s got the blue sheets and who needs blue sheets can you push them towards many of these hands push them towards the hands all right so next up we have the structured headers yes so our structured headers spec is is coming on "
  },
  {
    "startTime": "00:56:02",
    "text": "nicely although in in spurts we have I was talking to Paul Henning about this other while ago and we\u0027re kind of getting put where we think it\u0027s it\u0027s pretty baked we have a few issues open as you can see which we\u0027ll go through in a minute we have a couple of third party implementations now I\u0027m intending still to put together an implementation one of those third party implementations is in Chrome for their work on sign exchanges and and we\u0027ve had some feedback from those folks which is great I last time around I mentioned we\u0027d started collecting a common test corpus for for structured headers and we\u0027ve had some third-party contributions to that as well which is great so I think the besides these issues here the the before we actually ship this I want to make sure that we have multiple full implementations and reasonably complete test week for it which I hope we can do kind of by the end of the year at the latest is my current feeling so diving to these issues I\u0027m going to skip this first one because that\u0027s just a check that\u0027s a bug in this back the second one identifier range recently we reintroduced the possibility of having identifiers so in other words constrained strings that are not quoted as values previously we had only been using them as names for things like dictionaries and that gives people a lot of flexibility some people want it to be prettier on the wire for h1 and so that gives them the ability also to back-porch some existing headers on destruction headers which is a nice thing and some of the feedback we had was that the current syntax of identifiers although relatively generous excludes a few things which makes them sad because they can\u0027t put certain things into identifiers for example host names and so I have a checklist here of different things we could put into identifiers to expand its use cases just a little bit one is just the period and if we include the period identifier that gives us host names so for example we could backport the origin and potentially even the host header yeah in in destructure headers which would be nice excuse me the colon would give us things like alt used and access control allow arjen as well as ipv6 addresses where they appear percent gives us percent-encoding obviously like in the AL pn header and uppercase alpha gives us HTTP methods so for example in so the access control the the course headers I think I don\u0027t know if I actually say it down here you had a discussion with out of them Karen who was kind of the source of a lot of this because he wants to back port some of this on to the core specification whether he that\u0027s adequate and he\u0027s got an open question about what acceptable characters are in hosts which is a whole nother ball of wax that I want to get into right now I think from our standpoint the downside of allowing these things is that especially if we "
  },
  {
    "startTime": "00:59:02",
    "text": "allow things like periods in identifiers then mapping into certain programming languages becomes not dangerous I don\u0027t think it\u0027s a security issue it just removes the possibility of certain mappings I don\u0027t I don\u0027t think that\u0027s necessarily a horrible thing it\u0027s just worth noting any thoughts what until some this is interesting I would suggest maybe creating a different grammar as in a different type I know we\u0027ve talked about that in the past but but the ability to have these these simple identifiers in a lot of cases is is highly valuable and things like payable PN and that give me the heebies because LPNs not exactly like the old one so it can I restate what you\u0027re saying just to make sure I understand right now we use identifiers for in the current spec we use identifiers both for example the name in a dictionary values of key equals value for the key as well as for values potentially and you\u0027re talking about bifurcating that so that the key what we use for names is still quite constrained whereas if you use these things and values they\u0027re much more open is that the case yeah I think that\u0027s about right because you think about the cases where we want to do some of the more extended things those are those cases where you\u0027re talking about the parameter values rather than the parameter names I think it makes the spec a little more complex but it doesn\u0027t make you implementation more complex so that should be okay refer to it as something other than an identifier yeah we can little bear word or we can change the other one to name or we\u0027ll figure that out right yeah I\u0027m not against I think that\u0027s fine I mean with all the things in structure header is one of the things we have to do is figure out how we identify it in a value when it\u0027s you know you know if multiple those can be present you need to distinguish from other ones but we already have that problem with this I don\u0027t think it\u0027s an issue you know I don\u0027t know if you\u0027ve managed to maintain the sort of lexical distinguish ability of type in all of this so you can look at the value on the wire and say well there\u0027s a or B right so far we have so how you have so that suggests that maybe we don\u0027t yeah we have to think about that a little bit more and I think it\u0027s workable yeah you can identify it electrically except at the top level the top level you have to send a hit to your parser so you have to say I\u0027m about to par as a dictionary and then the rest happens right okay so if we\u0027re already there then I think then then I think this is probably workable okay I like that thank you any other comments okay next up I had a thought of including a date type in the spec as you can tell a lot of the work that you know I\u0027m kind of looking at now is just making sure that if in the "
  },
  {
    "startTime": "01:02:03",
    "text": "future conceivably we want to back port existing headers on to structured headers we have that option although it needs to be done carefully a lot of thought and so one of those is date because there a lot of date headers and so this would be you know a new type for values I personally since come to the position that it\u0027s not terribly useful to have a structured header whose HTTP 1 textual representation is a date as we know it right now that\u0027s a horrible horrible thing and that if we want to represent date types and structured headers it\u0027s much more effective to just do it as an integer number of seconds since the epoch and that means we need another way to back port those date headers but I think that\u0027s probably fine if people don\u0027t agree with that please say so otherwise what\u0027s close this with no action so just to restate what you\u0027re saying in cases where I want to define a new header field that contains a date or a date somewhere in there I would use a number and define it as a number and that\u0027s very simple and straightforward and for the existing header fields date expires and so forth the we just suck it up well so it said engineer it could be a float if he needs that kind of precision but um people always ask for it yeah I think in my my current hand-waving for that is is if we want a back port last modified for example we\u0027d create a new header field that says you know last modified structured and then have rules about when you transform from one of the other words I think I think those that that\u0027s a reasonable outcome here if we do head down that path I suggest that the first instance that uses a date should be very carefully looked at to see whether or not it\u0027s integer or float or what-have-you because as soon as we pick one thing it would be very nice to use the same thing everywhere that appears and then we have to have the wonderful debate about milliseconds and all sorts of other things like that but I would I would strongly recommend that we\u0027d be watching for that moment because that moment will be a sort of pivotal so um not really to this draft but I think I\u0027m probably gonna start working on a draft to sketch that space out pretty soon all right so it watch carefully yeah Pensacola if we want to use numbers since that book why sending them as ASCII instead of binary if we are going all the way this year well if there were numbers they wouldn\u0027t be sent as ASCII if we ever come up with an encoding for these things that isn\u0027t asking right now we\u0027re constrained by HTTP 1 but the a lot of the thinking behind structured headers is is that we want to create a cowpath where it\u0027s very easy to create an alternate encoding of the header values that can map to HDPE one successfully but also have a more efficient expression on the wire so we\u0027ll get there Alex II may be ignorant question for daytime\u0027s to ever need timezone to "
  },
  {
    "startTime": "01:05:04",
    "text": "be preserved it\u0027s locked to UTC in HTTP ok well that\u0027s what obviously influenced where the float is sufficient not sure well I mean you know any we could always create a more a more structured header that has you know a number field for the time and then a string field for the order identifier for the time zone and so forth and so on ok so I think we\u0027ll close this one with no action the potential future action on other stuff and finally um this last issue represents if nothing else just a bit of angst on my part you know we have intentionally defined structured headers to be strict that if you have a parsing failure it reliable he blows up in your face and that is designed and and with a lot of care to make sure that we have a clear crisp line for interoperability and that thing\u0027s feel quickly and reliably so that people fix the bugs rather than perpetuating them and getting us in this nasty cycle we\u0027ve had with every existing HTTP header and I\u0027ve had long chats without event castering to get buy-in from him that this is the right approach and I\u0027m pretty comfortable with that the only place that that kind of gives me pause is when you have a header that is owned by many parties or upended by many parties so for example we use structured headers for the CD and loop pattern and the way that structured headers is composed right now if there is one part of a header field value and that includes you know ones that are comma separated CF you know host a appends this host being pens that if any part is syntactically invalid you throw the whole thing away and so you get into situations where if you have one bad actor on a chain then the thing becomes useless and that causes me some discomfort however you know in discussion on the issue we\u0027ve kind of come to the place that we don\u0027t have any really better answer that we can put into structure headers I think that a that uses structured headers in this kind of fashion where multiple parties are involved potentially you have to maybe have some text around well if Parzen fails then you can try and parse the value but we don\u0027t define that or error handler something like that I don\u0027t really have a better answer for this but I just wanted to see if anyone shared my discomfort or had a better insight into how to address this yeah Thompson this is one of the really awkward ones isn\u0027t it because so looking at the example there it\u0027s it\u0027s a good example if you pass a chain to what individual one then you get one answer and if you if you clone them together with commas you get a different answer in this case they\u0027re all both would produce a parsing error but but you can imagine constructions that would appear to be valid when when strung together but not when they\u0027re taken individually and I don\u0027t think that we should be requiring parsers to pass both forms and it\u0027s unfortunate but what "
  },
  {
    "startTime": "01:08:06",
    "text": "you really want to do is is have the power pass every single instance of the header field individually and if you if you could require that that would be great but that\u0027s not how the API is currently work is it the problems deeper it\u0027s that an intermediary can choose to combine them in any fashion that it once and once the intermediaries combined them you lose the ability to distinguish them if there are bugs like this right yeah so I suspect the only answer we\u0027re gonna have here is some general advice about how you handle those kinds of bugs and maybe hey intermediaries please don\u0027t combine our headers because Chris lemons I agree the I think if you have people mangling headers that can mangle them in amazing numbers of ways and I don\u0027t think there\u0027s really any hope at that point everyone does their best and you figure out what\u0027s what\u0027s up okay I think I think this the I think this probably we should close for the spec for CDN loop or in last call I might try and craft some texts warning people about this particular feature and suggesting some strategies to handle it but beyond that I don\u0027t know what we can do and before I close this I\u0027ll figure if we can add generic text warning about this to our spec yeah I was about to agree with that I think there\u0027s probably some text here that we can add no concrete action but at the same time just creating the little red flag for people so they don\u0027t I mean my city in loop is probably you know one of the few examples of the header like that most headers are just you know client-server we\u0027ve inherited a feud and we have is that it yeah that people don\u0027t use but that nonetheless we still exhibit this problem the other header that\u0027s have done right now that\u0027s being worked on like this is the distributed tracing work and they\u0027re on again Afghan looking at structured headers pit Sakura a yesterday really had suggestion that we could add properties to Hatteras on on the discussion on about binary Harris and one of the properties around with you know never index that we have in h2 was don\u0027t fault so that could be useful here as well sorry I didn\u0027t quite get a thorough suggestion from Willie that we could add other properties on top of you know value name and we could have properties that describe stuff like never index or do not fault that could be helpful here that you wouldn\u0027t concatenate in couldn\u0027t concatenate specific adults now I\u0027m not sure how "
  },
  {
    "startTime": "01:11:12",
    "text": "that would impact this back well then you couldn\u0027t if the intermediary couldn\u0027t concatenate those headers together handle values right you couldn\u0027t discard it could discourage only one of those and not all so sorry are you are you asking that we introduce a new requirement HTTP that intermediaries don\u0027t concatenate some headers when notified yes well that was the session from Willie on the mailing list regarding the different topic what that would be helpful in this case as well so that\u0027s kind of the suggestion I don\u0027t know if we can back port that on the existing web yeah we can\u0027t look like h3 that\u0027s forgetting the optics from now okay almost there I think that\u0027s all the input I needed for structure headers so expect this spec to kind of firm up pretty soon if folks have not looked at it recently please take a look we especially want to make sure that we get all the bugs out of and that\u0027s what hopefully the implementation and testing will get us to we think we\u0027re pretty far because a chrome my sink has done a pretty good go at implementing and I have at very little in the way of bugs but enough to know that they\u0027re actually paying attention and giving feedback so I\u0027m pretty happy about that um yeah and if you want new features in the spec of the windows closing very very very fast so next up cache digest I don\u0027t think we have a specific update for this spec the authors are still working on and I know and it is getting attention can we expect it to be going to work in group last call any time soon no if you if you don\u0027t mind yes that\u0027ll be great and we should jabber scribe jabber scribe your attention is needed sorry I was in fact pretending to be a medical doctor instead of a regular doctor general hand wavy question what percentage of headers are amenable supporting their definition to structured headers that\u0027s a great question and I actually have a little spreadsheet that lays that out a lot and it depends on exactly what you mean by amenable if you mean directly supporting the current syntax in structured headers without any changes a good chunk that\u0027s with a huge asterisk because it depends on what is the acceptable error handling around those headers if you\u0027re happy to "
  },
  {
    "startTime": "01:14:12",
    "text": "take structured headers error handling model then a lot of them if you don\u0027t want to take structure headers air handling model because the actual headers in use on the Internet have a wider range than what it you know the syntax might suggest then maybe not so many and if you\u0027re willing to do the trick like I talked about with date where you create a parallel header and negotiate its use then well pretty much all of them in some fashion okay so just just so I can narrow down the hand wavey slightly when you say a big chunk is the chunk greater or lesser than 50% I would say right now of the standardized headers the headers in the registry greater than 50% thank you cuz it also regarding cache - yes - I mean is that our previous conclusion was to wait for browser implementation experiments experience before doing it last call and we are still waiting for that I\u0027m not sure if it\u0027s going to happen so no no that\u0027s the situation okay thank you all right so the next one is client hints I believe that is that update no I have a less than an email update I can read oh yeah oh yes that\u0027s right so a short update on the current status of client hints this is courtesy of EOF except CH lifetime and the caching mechanism have pending PRS in fetch to integrate their processing with a fetch an HTML specifications client hints are now limited the same origin and secure connections there are plans to use feature policy as an explicit delegation mechanism for pages to send specific coin it\u0027s to certain third parties if you\u0027re not familiar feature policy is a emerging specification in the w3c please stop selecting me while I\u0027m reading off my phone there are exciting plans Yaffe\u0027s words to use client hints to minimize the fingerprinting surface that browsers currently exposed and I think what he\u0027s referring there to there is creating new client hints that could in aggregate replace the you a header that\u0027s right now a hand wavy proposal it\u0027s not anything official so that I\u0027m sure will be discussed over time since the list of headers keeps getting longer we\u0027re contemplating using a sec CH prefix for them or similar in order to reduce the probability that some server will misinterpret them as well as reduce the administrative complexity stop slacking me as well as reduce the administrative complexity adding those headers to the course safe list and I think there\u0027s there\u0027s a discussion there in the in the past as a working group we\u0027ve been stop it we\u0027ve been very reluctant to start using your names "
  },
  {
    "startTime": "01:17:12",
    "text": "we\u0027ve been at very reluctant to define prefixes for headers I know that third parties and in specific the browsers use the sec prefix to denote that something shouldn\u0027t be exposed to script and that\u0027s I guess fine for them but once you start building more complex prefixes then you have precedence issues and everything else so we can have that discussion if necessary I think it\u0027s fine to say personally I think it\u0027s fine to say yes put a sec prefix on it if that evinces the appropriate brow of behaving the browser\u0027s but we don\u0027t need to mandate a sexy H prefix we could say that\u0027s convention just like we have convention for except - and content - but it doesn\u0027t need to be specified John a please stop doing that and Kazuko similarly to the way that savedata hen was removed from the IETF draft were thing to further removing the DPR viewport width and with hints to their own spec which is better integrated with the fetch in HTML in order to create better separation between the client hints mechanism and the features that use it so that\u0027s to me personally a little surprising in that we are gutting this specification apparently there\u0027s a framework yeah Thompson that and until this point I was I was perfectly content to let this one ride we have no real plans to implement it or anything but it was Wow so there\u0027s a whole bunch of things to address here there were claims that privacy was being improved here that I think were completely unsubstantiated I mean basically this is that this is an annette degradation in privacy we\u0027ve got a lot of safeguards around that but it\u0027s still in that degradation in privacy and we shouldn\u0027t be selling at us otherwise the feature policy stuff continues to give me the creeps in various ways and extending it and the ways I was in it but there\u0027s probably a comment for the w3c but um what else was there I don\u0027t know though there\u0027s a huge list of things in that and that last one would was a complete surprise to me as well no this I thought was so close to finished it it seems like this is a deliberate attempt to like scum little and start us all over again so I really surprised to hear that I think we as chairs need to have a discussion with the authors or the sorry the editors but I would strongly suggest that folks who have these concerns coordinate their participation here and in other venues so that we\u0027re having the same conversations and so the feedback is noticed elsewhere as well right yeah I mean that last one was like I\u0027m winning the race I\u0027m winning the race limit committing a suicide magnetars from the line it\u0027s just crazy yeah yeah okay I should I should be a little more polite breathe yes I\u0027m a little "
  },
  {
    "startTime": "01:20:12",
    "text": "surprised by by this because I thought it was done yeah yeah Eric Kinnear Apple I can echo some of those sentiments but I think the the core thing I want to say is the ideas behind a lot of this I think are really good and valuable and are something that are very interesting and so I\u0027d like to see it not disappear or be otherwise lost so those comments came in this morning before the meeting so the chairs were you know a little surprised by their content as well and now that they\u0027re you know in the record will forward them to the list so that they can be part of theirs the discussion that wasn\u0027t meant to cut you off David I was gonna ask you to send them to the list so yeah we definitely need to have a discussion with the editors and voilá will report the results of that the working group as well but please feel free once that is for the list to make your feelings donor and that takes us to the next item which is cookies and this specification has been going on for some time we\u0027ve recently added a new John Wallander he\u0027s getting up to speed he\u0027s not just familiarize himself with toolchain so we\u0027re hoping I think that this is gonna see some motion pretty soon I think I speak for my co-chairs when I say we\u0027re a little concerned about the progress on this and the amount of time it\u0027s taking and so I think we\u0027re gonna get John up to speed and the next step is I feel like we as chairs probably need to go off and assess where it\u0027s at and how much work is to be done and then figure out what the appropriate next steps are so watch this space anything to add okay okay so that takes us to quicken HTTP Mike is this the same presentation Justin he did you update this one as well I do have a new version from you tell me - okay let\u0027s do the switch doing switch it contains some review files so I understand there was a flash of bees from in the plenary but one of the topics coming out of HTTP HTTP quick and the quick working group is perhaps we should consider a different name for this deliverable so my lead-in was a story from dr. Seuss that my cousin introduced me to where mrs. McKee of has 23 kids all named Dave and she\u0027s regretting the situation because she can\u0027t disambiguate she comes up with "
  },
  {
    "startTime": "01:23:12",
    "text": "lots of very interesting names that she wishes she had use instead but she didn\u0027t because it was too late for an HTP were not in the same state next slide so we do have the chance to reconsider what we\u0027re going to call this thing and HTTP quick doesn\u0027t clearly relate to HTTP 1 1 it should be too as kazoo who pointed out in quick it doesn\u0027t work well with logs that expect to be able to stick a number there in the log it\u0027s not really HTTP to over quick we\u0027ve diverged pretty noticeably from how HTTP to worked the HTTP mapping itself is not quick quickest the transport and our last meeting when we discussed perhaps backporting some of the things that we\u0027ve changed in HTTP over quick to HTV to as extensions we made the collective decision not to do that and there was a comment I believe from a chair that that\u0027s because as quick as the future and we plan to keep moving forward now I don\u0027t think that that implies we won\u0027t be maintaining it gp2 just like we\u0027re continuing to maintain HTTP 1.1 but I think if we believe that our work as part of quic is intended to move forward and continue this progression there is kind of an implicit name to consider and that is perhaps htv3 yes apparently the three secures so the Charter in the working group was to produce an HTTP mapping quick we did queue back on the way but particularly if we\u0027re going to call it HTTP 3 but whether we do or not really it does need HTTP working groups input because this collection of people is the we are the experts on HCV it\u0027s also in the Charter for quick yeah that took so I think it would make sense to do joint working group last call across both groups when we get to that point but also we\u0027re getting the point where we\u0027re starting to see interoperable implementations so this is the time please start reviewing the doc if you haven\u0027t been participating quick and then going forward it probably makes more sense for ownership and maintenance to be in this working group instead of quick because quick will be focused on the transport and potentially different application mappings so this is a proposal it can\u0027t happen unless this working group also agrees to that but let\u0027s see what we can agree on clarification question by saying by saying that http-based parking oracle sign offs documents does that include Cuba document yes I "
  },
  {
    "startTime": "01:26:14",
    "text": "think that would make sense Thank You balance not being true so maybe just this is your last slide yeah okay just to interject I will magically transform into the quick working group chair in front of your eyes Kenny so we had a discussion earlier this week and the quick working group agreed that it would defer to the HTV working group on the naming of that deliverable and so this decision is in our hands yeah so I\u0027m gonna merge but before we get one going um I would say there are two important aspects to this discussion you know one is the name so we won\u0027t we won\u0027t enjoy that and the other one though that I think it\u0027s more important is that you know future maintenance of this comes over to this working group and this has been the point that I have had some insistence upon I think it\u0027s a strong part of this proposal yeah but that being said let\u0027s open the line with those and it does look like doing that would require Turner changes and updates yeah so Thompson like I think that the fact that this working group looks after HTTP is obvious and we should take you back along with that just to make that very clear my plan is currently to merged the pion 1973 can you just can you tell us come on please summarize Martin oh my plan is to add at least one more commit to that before it merges Bret Jordan I think it should be done here as well I would support that I think there\u0027s a market perception issue the we need to be very cognizant of and if we do something like some of the other names have been proposed or I should be quick I think there\u0027s going to be problems in the market as far as understanding that this you know where it\u0027s at and so I\u0027d be much more in favor of an HTTP 3 simply because it would help confuse the market less that it\u0027s something that kind of is replacing HTTP 2 I already get that question when I bring it up internally inside the company and people are like oh it\u0027s just HTTP tooth no it\u0027s not HTTP 2 and there\u0027s I know this group is really focused on engineering but there is a PR marketing message that we need to be very cognizant of so I certainly think it\u0027s fine it\u0027s good in proper flesh work tap for this work to serve in migrating HTTP at some point I think for precisely the same reasons that fetches elucidate I don\u0027t like hate UDP three basic crazy exactly confusing about the status of this document is back to http namely it doesn\u0027t carry the fact that like it doesn\u0027t carry the fact - for two reasons one it doesn\u0027t carry the fact that it\u0027s "
  },
  {
    "startTime": "01:29:15",
    "text": "over quick so like people aren\u0027t you can be like well what is this that aspect of TCP and the second is it basically implies as I said reviews meaning that that basically we\u0027re tired of h-2b - and that this is the thing which is now the thing whereas I thinking huh so as I said um I think almost everybody here was the room when I said this previously I\u0027m not gonna lie down the road over this but I think it\u0027s the wrong answer I do however think PA 73 is excellent gen-i younger I think that what you said about HTTP TV in the future is is I think I agree with that the problem that you\u0027re going to face is this arguing that HTTP 2 is not the past and that\u0027s a signaling problem in general I think going forward if we believe that we\u0027re going to need standards developing for both quick and TCP I think it\u0027s worthwhile thinking this is the way we want to keep doing things like is the next step is the next evolution of HTTP to the thing that runs on top of TCP for instance going to be HTTP 4 if so is the signaling that we have like the odd-even numbers basically TCP and quick or something like that I don\u0027t know I\u0027m not trying to create structure where that doesn\u0027t need to be but signaling here being clear might be useful going forward and I\u0027ll say that it\u0027s it\u0027s it\u0027s completely reasonable HTTP 2 was specifically says it runs only on top of TCP it\u0027s completely fine to say that there are different versions but it does create a very clear fork that\u0027s I don\u0027t know but that there are positives and negatives to that as you all appreciate and understand I think that\u0027s where really I see tensions 15 second last call vodka get in if you have to get in your comment now Erik Kinnear so I actually think HB 3 is fine and in the long term HB 2 is explicitly over TC over TCP and it seems like HTTP 3 could be over quick but could also be over TCP and that could become the future that doesn\u0027t mean that HB 2 isn\u0027t good or isn\u0027t what we currently fall back - if quick isn\u0027t working so on one hand like yes there\u0027s messaging around to the current state of things but at the same time let\u0027s not contort ourselves into too much of a twist trying to describe the current state of things without allowing room for the future state of things looks like it\u0027s - falls back to h1 yeah month old some maybe maybe this is a recognition of the fact that we we are forking the protocol at every major release and that concerns "
  },
  {
    "startTime": "01:32:18",
    "text": "me a little bit but I guess this is where we are right if if we if we talk about revising in a major way HTTP to the thing that we do over TCP and calling it HD before I suspect we\u0027re gonna end up with a lot of confusion probably worse than the confusion you\u0027re trying to address right now but maybe that\u0027s just a nature of the beast so Martin could I dig into what you just said there you caught it a fork and it I didn\u0027t say Fork but I will if you want you need it to work okay Jenna our line Sir John our lines are cut Roy our lines are cut so yes that was more than 15 seconds you know the the the semantics of HTTP versioning are set down in 21:45 and each new major version is a new mapping of the core HP semantics which do not change right wire and so we have a new mapping to the wire we create a new version number and I know that there are certainly marketing and perception issues to all of this but that\u0027s the heart of HP versioning so I people keep on saying fork I don\u0027t understand how that is the case here I think the the fork is the fact that we don\u0027t obsolete the previous version wait wait yeah that\u0027s the nature of network protocols I mean we can\u0027t tell everyone to stop using HTTP one we haven\u0027t we we took them stuck using HP 0.9 really we can\u0027t even do that because browsers are still supported because they can\u0027t turn it off so you know implicit in all of this is that statement though and this number is bigger than that number it means that we we this is our best offering right and so if we if we want to define define a new thing that runs on TCP maybe that won\u0027t be our best offering at that point in time and we have to grapple with that problem then well I would think that that you know we have the negotiation mechanism in place but we don\u0027t need to version that you know you don\u0027t need for Tommy pointed out you know with HB 2 you use TLS negotiate its use without changing the major version number so I\u0027m not sure that I agree with that I\u0027m gonna sit down having interest of time for now and personally my inclination if we needed to revise the TCP version and I know this is not chair approved would be to revisit our decision that there are no minor version numbers and say it should be 2 runs over TCP we data its 2.1 ted hardy reflecting from the Trebor room from Roy fielding I don\u0027t see any value in presuming that this is suddenly h3 when it was barely even tested yet what is wrong with calling in HQ until is proved useful which could be in a few months speaking from for myself I think one of the critical questions isn\u0027t so much this version but what your pattern is going to be when quick starts revving faster than you do in particular we\u0027ve already had proposals for new quick versions we could have ended up coming "
  },
  {
    "startTime": "01:35:18",
    "text": "out with two quick versions had the discussion about the spin bit gone slightly differently and having a different version of HTTP for every version of quick it\u0027s going to quickly become untenable so I suggest you think when you\u0027re making this decision primarily about what characteristics of the underlying transport effect what mark was putting forward about semantics and I think those are about recovery and restart and and pretty much nothing else I could be wrong about that though so I\u0027m suggesting that the working group think about whether there are other things about the underlying transport where the changes in those transports employ semantics changes to http and I think that\u0027s not a conversation that quick and it\u0027s HDV mapping group has has had at the level of detail that this working group probably needs to happen so the spec currently says that the only requirement on the quick version is that it must use TLS version 1.3 or greater as its crypto handshake there are some certainly some version dependent properties like the existence of streams that are technically variable by version we don\u0027t have any version that doesn\u0027t use them but technically their version dependent properties that the mapping depends on so we might want to be more explicit about a version that supports the following things can be used for each whatever Chris lemons I think the observations been made a number of places and is correct that this is effectively a marketing decision do we use the letter Q or do we use the number three that\u0027s basically what it boils down to and there are a lot of nuances we\u0027ve discussed about what do we do if Q or three isn\u0027t sufficient and I think those are great questions but I don\u0027t think that answering whether Q is better than three answers any of those questions and I think that unbalanced the three serves as a better marketing position because if somebody comes to me and says should I use should I use whatever we call it three or quick the answer I\u0027m going to give them is basically yes you should use this if you have to choose between H 2 and H 3 well then I\u0027m gonna recommend H 3 unless you can\u0027t in which case you use H 2 in the same way that they say if they say if I should I use H 2 or should I use H 1 I said well you should use H 2 unless you can\u0027t in which case you use H 1 which is exactly the same thing you\u0027d say if you say should I use H 1 or should I use H 0.9 well no unless you "
  },
  {
    "startTime": "01:38:19",
    "text": "can\u0027t so it\u0027s the exact if the exact same messaging and I think it sends a consistent messaging from the team to people as as what our best offering is because I do think this is is the best offering and I will address the other question which I think everyone seems to agree on maybe because no one talked about it yeah this absolutely makes sense in this working group HTTP work belongs in the HTTP group thank you Mike okay so we want to do some homes to get some consensus on this from the room so the first question is whether about whether this group wants to take on the maintenance for these deliverables going forward and if we commit to that we\u0027re after the initial versions are published within the quick working group so if you support HTT this kind of owning the maintenance for these kupack and whatever we call HTTP when it runs over quick if you support that please hum now and if you do not support that please hum now all right that seems pretty clear and then the next question is going to be on the name so do you want to just say do you support h3 okay I think right great so if you support calling HTTP running over quick HTTP 3 or h3 for short please hum now and if you do not support calling this protocol HTTP 3 please hum now so not unanimous but it definitely seems like there is more consensus for just calling it going to be called HTTP 3 okay thank you all moving on we\u0027ve got 20 minutes left and we have two presentation I\u0027m under adoption of the nice green logo go away loris this isn\u0027t your place here\u0027s how if you never want to see that logo again "
  },
  {
    "startTime": "01:41:23",
    "text": "all right so I\u0027m Eric Kinnear and we\u0027ve had a draft up for a little while about using hb2 as a transport for arbitrary byte streams do we have a clarifying question folks we\u0027ve got twenty minutes for two person is it Sakura I just wanted to mention that I have a competing proposal for did that just uses byte stream asset value for sale though the protocol zodavia header and then it just works assuming the endpoint 0 8 4 4 1 okay let\u0027s get to this presentation and you can get yeah we should probably talk so very briefly a short touch on the use cases in kind of why this is a document as opposed to something we assume you can just do we\u0027re looking to provide kind of generic transport for a secure arbitrary byte stream that is bi-directional between two endpoints at the same time with h2 we\u0027ve seen a lot of benefits that come from having multiplex streams over a single underlying transport connection so you have things like a low set-up cost for new streams a single congestion in recovery context you can make peer-to-peer connections that\u0027s one of the things that requires some change here is we need to make sure the clients and servers as we think of them in h2 can do kind of the same things this obviously would provide you the ability to tunnel traffic as necessary and we\u0027ve actually deployed something like this although with choosing one of the different options that we\u0027re about to go through for a kind of remote IPC protocol where we wanted to have multiple smaller streams for different messages that go across this underlying transport but we want to multiplex them all down on to a single transport next slide please so the reason to come to h2 and use that is because it provides this great framing layer that already has many of the desired transport features so you already have exchange of configuration settings you have multiple extremes you have flow control you can do stream relationships and all of the other things that h2 provides are much more transport oriented in addition to the mapping on top of that and that\u0027s kind of the goal here is to separate what those transport features are from these should be semantics and mapping that are on top and provide this as a generic transport and this should be ringing some of the quick bells in your head because quick has separated the transport and its features from the HTTP mapping on top so one potential path that this could take is as people using "
  },
  {
    "startTime": "01:44:25",
    "text": "HTTP over quick or now HTTP 3 can fall back to HTTP to folks who are using quick transport directly could use something like this if not necessarily this to fall back to running it over TCP and TLS next slide please so to very briefly go through three potential strategies and the more we talk about these the less of a difficult decision it seems but I wanted to go through some of the thought process behind how we got putting our so far the first potential strategy is you make an email PN token and effectively at that point it\u0027s a new protocol in the sense that you send whatever the heck you want and because you\u0027ve negotiated that at the beginning with Deyo PN the server expects it the client expects it everybody talks happily the second potential strategy is you allow empty headers frames because it turns out that pretty much gives everything we need and that\u0027s what we\u0027ve actually done in the initial round of this but then that gets kind of ugly because now you\u0027ve got to decide am I doing this or am I using normal HTTP to with if Foley should be semantics on top and it changes some of the restrictions around what\u0027s acquired in a headers frame or not so that felt kind of ugly so that\u0027s how we arrived at the third potential strategy which is to introduce a new frame as an extension that you negotiate so you know who can talk it who can\u0027t you can do that alongside hcp streams using headers frames but you have a separate frame that is conceptually that clear Sam I have a clarification question sorry from - look how does I don\u0027t understand strategy - can you say just a bit more about that sure do you want to go through one first and then go through the slide about - all right yeah can we go to the next slide yeah yeah this is the only slide about this one so we\u0027ll just breeze through it so theoretically you could have the server off or something like h2t you can uh negotiate alongside h2 you could always fall back to then choosing h2 if that\u0027s what you needed to do conceptually this is a new protocol it\u0027s kind of a key it also means that you\u0027re telling the whole world this is what I\u0027m doing which in some cases may not be desirable so moving on to the next slide edit frames so - John this question it turns out that in order to provide what we want and we\u0027ll look at the stream state diagram a little bit later but all we effectively need to do is open a stream and be able to send data frames on it however the way to open a stream is with a headers frame and there are required headers that belong in that headers frame and I think that\u0027s generally a good thing so technically you could send a header string with no actual headers present which is nice it\u0027s easy for implementations to do because you remove like four lines of code of sanity checking that people are doing the right thing and now people can do the wrong thing but again that\u0027s kind of unclean "
  },
  {
    "startTime": "01:47:28",
    "text": "because now you\u0027re kind of doing the wrong thing does that start to get here - question so just to actually jump in a little bit to hopefully clarify this this is likely the like if you did another version negotiation this is likely the type of change that you\u0027d have to do to that h2t to actually make it work yeah what we had been doing before was because we owned both sides essentially was doing the hack of declaring by coup that we didn\u0027t care about headers frames that\u0027s not compatible with normal HIV to endpoints and I think that\u0027s the fundamental problem with the strategy definitely um is there\u0027s only clarification questions or only clarifications now let\u0027s get to the last one next one please so the kind of natural follow-on to that is negotiate the use of a new frame as an extension on you effectively bilaterally declare support for it and if everybody supports it you can use it but conceptually a stream frame is the same as an empty headers frame so that\u0027s what\u0027s currently described in the 0-0 version of the draft if you go to the next slide I believe we have details on what it does so it behaves the same as a headers frame in the sense that it modifies the stream state in the same manner so sending or receiving a stream frame fully opens the stream in both directions it\u0027s allowed at the same time it has the same stream limits from settings so you have the same ability to limit the resource usage that other people are causing you to do and the streams that you have open allow you to send data in both directions the same way they have previously next one so that\u0027s effectively saying when you send a receive a headers frame in the state diagram you also can send a receive a stream frame and then you end up with a fully open stream the reserved states don\u0027t really apply here but other than that if you ever send a data frame with an stream on it you become half closed this is a fairly familiar transport concept and if you then reset the stream or and stream in both directions you end up with a closed string and that gets you a nice happy bi-directional body stream in both directions one more slide please so the things we\u0027re kind of looking at here are is there anything else that people use this for I know I\u0027ve heard rumblings from a variety of people of oh this is the kind of thing that we would use for X are there options that we didn\u0027t look at in those three and doesn\u0027t have any preference towards something other than third one because as we talk through it again and again it starts to become the most favorable option in my mind so just to do a time check we have 11 minutes left I think where are you how much I need at least six minutes okay so let\u0027s just be pretty brief here I think we can take more comments and other questions onto the list so it needs to keep your questions Mike Bishop first off I\u0027m glad you\u0027re doing this because back in each two days the intent was for the framing layer the "
  },
  {
    "startTime": "01:50:30",
    "text": "original intent was for it to be separable but we\u0027re what we arrived at frail PN was that we didn\u0027t want to have an ALP n token for framing Lior plus what you\u0027re doing over it the token implies that whole ensemble and that\u0027s also true and quick where the ALP n token implies a protocol that runs over quick but you don\u0027t say quick so I really feel like that pattern has worked well for us and maybe it\u0027s not an LPN token for this you still need to communicate what you\u0027re running over these arbitrary byte streams that protocol needs an LPN token that includes all of this your modified framing layer yeah other than that I mean I think this is a perfectly reasonable way to lift it out I like that I think that\u0027s a very clean matches what we\u0027ve done elsewhere and let\u0027s you negotiate whatever you want to over top of this without having to do that some other way so with anger I think the use case when you\u0027re done align these arbitrary byte streams through proxies would be desirable to have a headers frame to decide for to use that for routing information before you turn that stream into an arbitrary byte stream that could be optional but I think it would be very useful to add a header to allow a header stream before the arbitrate stream lines are cut by the way yeah we should talk a little bit more about the proxy case because we\u0027ve been thinking about that but if you\u0027ve got any experience there or something we could do that\u0027d be awesome to talk mr. Cora I agree with support that we need headers for proxy case and this kind of bring us to eight four four one where we have so the protocol that just works and is used for web sockets which are effectively the bicep as well right so we are kind of we would end up with two solutions for kind of same problem one of the distinctions here is the intent is to allow both sides to open one of these byte streams okay but yes it B it would be good to talk and see if we can resolve some of that or come to something common David\u0027s Knaus e Google so first off thanks for doing this I really like it my the use case I have in mind not that I\u0027m deploying anything just stuff is any like crazy cool stuff you could do with quick transport getting the fall back to HTTP or over TCP that said there\u0027s a lot of mention in your slides and the of the of settings HTTP but conceptually when you look at quick those are in the HTTP mapping document not any quick transport document so how do we reconcile that yes and how do you reconcile that with the transport parameters yeah so maybe we need a little something clean there we need to poke out a bit more I agree with "
  },
  {
    "startTime": "01:53:31",
    "text": "that Facebook so one questions about a lower pressure especially when we wanted to shut it down is by Direction stream so in HTTP case most likely the stream may be end of edit with the lifetime is very short and now we do lot annealing so we have no idea about a pelear application behavior so they can be hours or days and as middle box between kind of server I that\u0027s any consideration how to secondary both side to shut it down is Chris do you want the middle box to be shutting it down or do you want the middle box needed focus on one unit Arista either to maintain you have to sir today I don\u0027t know how to shut it down is because of before we have a go away we can exploit it tell remoter scientists say hey this is a last Austrian ID I want to process since this is one long last of all can be forever and there\u0027s no way to like a synchrony on the side to stop using this student yeah right so you yeah you can still use go away but you\u0027re in the same cases if you had like a hanging get kind of a case or something with a very long responsive yeah so we\u0027re pretty much out of time trying to give there\u0027s a quick or a cheapy um seriously I just wanna say that I think one the one difference between transpose parameters and settings is of course that settings can be sent at any time during the connections he wanna be so you don\u0027t have to kind of figure out exactly how to do this but that said I think I agree exactly with what David said this also suggests that there\u0027s a better potential name for this you call it PCP plus plus or maybe even DCP two can I quickly ask question what was it just invited presentation or the chairs we\u0027re gonna talk amongst ourselves into the transport area folks and figure out the right disposition for this if that was the right answer here for you man okay I\u0027m gonna talk real quick about a proposal that I\u0027m bring to the working group we talked a bit about the list called it\u0027s the cache header next slide so there\u0027s this thing that is very widely deployed on the web called X cache it\u0027s a header that originated next slide in sorry squid and for me the motivation for this work was partially explained by this blog post by many the pitch point CEO you know every CD and after uses different debug headers and some don\u0027t send anything back debugging your web traffic as a result is incredibly frustrating and you can read about all these frustrations at that link at the bottom next slide so as I mentioned squid does this it sends a "
  },
  {
    "startTime": "01:56:31",
    "text": "couple of different response headers I did a bit of research here and there\u0027s a bunch of resources about how to figure out what these things mean in squid noting very specifically there\u0027s any not any documentation in squid about this which is lots of fun nextslide Akamai gives you a number of different request and response headers to play with of which one is X cache and they have lots of things yep next CloudFlare has something very similar called CF cache status and a bunch of other stuff they use a bunch of status codes to indicate different error conditions I\u0027m not going to address that in this presentation but that\u0027s maybe something we can talk about the future next fastly does X cache and a bunch of other things that are X cache ish next slide cloud front just uses X cache next slide traffic server gives you X cache and lots of other fun stuff next slide so my point in all of that is X cache is the low-hanging fruits if we want to improve debugging of CD ends and reverse proxies it\u0027s really nice if we could just agree on that it\u0027s not currently in our operable every different provider does something slightly different the semantics are slightly different everyone\u0027s a special snowflake so let\u0027s create a better header and let\u0027s take the X dash off because you know you all know how much I hate X - next slide so this is the proposal in an example of the proposal in in the specification you use a a cache action to denote what the cache did and then you have a number of optional parameters to say which node did it and what the cache key it was using was and you know latency and age and freshness and lots of different fun stuff this is a strawman it\u0027s based upon me looking at what people are currently doing I\u0027m more than happy to evolve this based home working group discussion next slide a couple of notable things my intention for this is only to record stating caches I think that\u0027s a nice little chunk of something we can make progress on it\u0027s not - do you know every possible intermediary action that\u0027s something we can address separately and so it\u0027s really just about cache state those all those parameters are optional so an implementation can choose how complex it wants to be and how much information it wants to convey it can also extend those with with signatures or with you know implementation specific information if it wants to negotiation for how to trigger these things being sent is out of scope some folks feel like they should always be sent other folks feel like you need to need to authenticate in some fashion or trigger them in some fashion I don\u0027t want to really tie hit people\u0027s hands there but of course we can talk about that and right now it used the structured headers yeah a couple of sticking points to note the exact cement because the cache actions are probably gonna be take a lot of work to nail down we want to make sure we actually cover the space well and that we get good interoperability there and handling vendor extensive extension as well is something we\u0027ll probably need to trade carefully around and so this is my proposal I would love for the worker group to adopt it it seems like there was good discussion on the list so what "
  },
  {
    "startTime": "01:59:31",
    "text": "do you think I have a comment from Roy fielding in the jabber room indicating that CDN would be a more specific name than cache because this is not applicable to all caches and that shorter would be better I\u0027m gonna disagree with that I think we chose CDN loop we had CDN for the very specific reason it was incredibly CDN specific this is applicable to reverse proxy caches I\u0027d I see no reason why a server-side cache couldn\u0027t do it I see no reason why browser cache couldn\u0027t do it if you really wanted to I don\u0027t know a response with him yet so ok latency yes I am adoption yes I\u0027m putting out there Chris lemons from Comcast the I\u0027ll speak up in support of this it is super fun to debug things that go across multiple caches simultaneously and you didn\u0027t even post the the most fun format that ATS uses which is this super terse like whole bunch of letters yeah Dawid and they put it in the via header instead right so yeah this is this is lightyears ahead of of my experience and it would be fantastic to be able to stack those CD and on top of CDN on top of origin caches and yeah this is this is not just CD ends there\u0027s a variety of different kind of caches that are involved any time content gets from an from an origin all the way down to a user and visibility into what\u0027s going on is is critical yeah when it was aimed to me traffic server that via stuff was all black magic and it wasn\u0027t documented anywhere he had to know a guy to know what it meant it was it was good fun so Tommy and I huddled over the fate of one of marks drafts and I gotta say that was a much less lonely experience than usual so that was great this is I think kind of too fresh to the working group to make a real decision here because mark is seeking adoption we will open that question on the list and look for get a more interest before making its determination Adam as ad last word vote but we\u0027re out of time oh not as ad I have a comment whatever room are we open for that then you are cut off thank you everyone for coming thanks "
  },
  {
    "startTime": "02:03:23",
    "text": "when the beat drops out "
  }
]