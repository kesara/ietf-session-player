[
  {
    "startTime": "00:00:04",
    "text": "things about this it will be it\u0027ll be massaged before it becomes the final minutes and and also it\u0027s not a bad idea to kind of like screen scrape capture it once in a while because every every once in a while the ether pad has a hiccup yeah yeah and that\u0027s never good thank you I always take I always screen scrape the agenda and use that to build my own notes which is what I did in what I sent to you yesterday incidentally Warren could you monitor Jabbar well all right that would be great if you could it should be right off the tools page yeah so what we\u0027re gonna get going now folks good morning this is the benchmarking methodology group and I\u0027m al Morton thank you for working so hard to find this location it\u0027s one of the more difficult rooms to find in in the London Metropole I hope you all brought your coats because occasionally it gets really cold in these dungeon rooms so you may need some help with that has anyone not signed the blue sheets yes right all right so so as I mentioned this is the etherpad where we\u0027re going to collectively create our meeting minutes I\u0027ve got two folks Pierre and Carsten who are going to be primarily in charge of that but anyone who anyone who praises a question at the microphone and would like to be sure that their name is spelled correctly and would like to be sure that the phrasing of their question is exactly what they want feel free to type it on the ether pad thank you also it helps it helps actually what I\u0027ll do right right up here at the top "
  },
  {
    "startTime": "00:03:05",
    "text": "it helps to say that al is this purple color so if I type anything in that that way folks know what what I have pasted it other folks can can do that as well so there\u0027s Pierre going in see this works just like magic great and here\u0027s Carsten fantastic all right so here we go we and we also have us currently seven participants on meet echo and they can see the slides but they will only be able to hear you if you speak directly into a microphone so what when we\u0027re conversing in general in the room to make to make it more possible for these folks to participate please join us at the microphone and of course the little sign up there says to say your name when you\u0027re about to make a contribution to the idea all right all right so this is our meeting at ITF 101 as i said i\u0027m al morton sarah banks is intending to join us remotely today she was unable to to travel here but we will see her i suppose shortly and and you\u0027ll certainly hear her participating as we go i have the suggestion to move close to the front because we\u0027re usually a small crowd but feel free to sit wherever you want if you\u0027re not subscribed to the BMW g mailing list and you would like to be there\u0027s a nice link there in the slides that you can download from the agenda or from any any other place of of interest okay next slide so here\u0027s our note wall which is why i asked you to sit up front this is this are our IPR considerations basically every every contribution that\u0027s a speaking at the microphone document hallway conversations certain hallway conversations are also covered but they\u0027re they\u0027re covered by our IPR policy what we ask is that you disclose any IPR can in connection with those comments in a timely fashion so uh and actually the the truth is now everyone who\u0027s registered has read this because you have to click that you acknowledge it when you register for the ITF so this is mostly a reminder and if you\u0027re unfamiliar with any of these best current practices documents then that "
  },
  {
    "startTime": "00:06:06",
    "text": "would be a good thing to take a look at as well so here\u0027s our agenda for today as I mentioned we have two no takers but everyone in the in the room can participate on that PRN Karsten thank you we have a jabber scribe I think is your driver scribe of machine not working it I\u0027ll be working soon all right one one Kumari our ad advisor area director advisor sitting up front here and he\u0027s so he\u0027s playing both the important role of AD advisor and and jabber scribe thank you so I\u0027ve just mentioned the intellectual property rights statement and the blue sheets where we all sign in on attendance that\u0027s all moving around oh and that\u0027s got to stop that\u0027s not gonna be good all right I think it\u0027s I think it\u0027s just on the other side it\u0027s it\u0027s on the other side of the wall they\u0027ve basically got to turn that down somehow yeah all right okay so so here\u0027s our room here\u0027s our planned agenda we\u0027re going to do the working group status and the Charter and the milestones briefly hopefully assuming sarah joins us will have a quick status of the SDN controller benchmark drafts if not I can provide that and then a sue Dean Jacob is with us and he\u0027s going to be presenting the evpn and PBB evpn draft updates to that then we have a whole list of continuing proposals here some of for which some of the authors may not join us so we may be able to move through this through some of this fairly quickly I\u0027m thinking perhaps item 5 maybe one of those quick ones where I\u0027ll give a brief status so it\u0027s network service layer ad abstract model back to back frame benchmark updates to the firewall benchmarking for modern firewalls return of the vnf benchmarking methodology and considerations for benchmarking network platform of virtualization platforms the last two items are working group discussion we have the Etsy nfe liaison on nfe benchmarking a normative specification we\u0027re going to take a look at that and our last item is to sort of close on the topic of reach are during "
  },
  {
    "startTime": "00:09:06",
    "text": "the Working Group we\u0027ve circulated charter text we circulated charter milestones let\u0027s finish that up and move it on to the next stage of approval any questions about the agenda any bashing needed does anybody need a drink of water all right so let\u0027s say that we\u0027ve closed on the agenda and let\u0027s move right along to the quick working group status so we\u0027ve completed IETF last call on the revised Sdn controller graphs and there\u0027s two terminology and methodology that\u0027s how we\u0027ve traditionally split our work here in the benchmarking methodology workgroup we got quite a few comments during last call and the authors have revised those drafts they are now on the agenda for the April April mumble first week of April I believe iesg meeting so that means we\u0027ll probably get some more comments from ADEs shortly and hopefully that will all go smoothly and we\u0027ll get those approved in short order so we we had an interim meeting that was back in February last week of February I believe or first actually first day of March I think was March first and that\u0027s where we really saw in some detail the new proposal for benchmarking modern firewalls had a real chance to go through that specification there and had some good comments on it there\u0027s been updates so we\u0027ll be talking about that again today and then the our status is really that the proposals keep coming as you as you saw on the agenda all of these are now active proposals so we will consider those and milestones for them as we consider retiring any questions on the your working group status at least this part of it this is sort of part one because we have no new "
  },
  {
    "startTime": "00:12:07",
    "text": "RFC\u0027s this time but that\u0027s not bad because at the last meeting we announced about six so we\u0027ve we\u0027ve been making really good progress we\u0027ll update the Charter as soon as we can and we do have this nice supplementary you\u0027re working group page which has been moved now to to one of Sarah\u0027s administrated websites so we we have that available if you want if you want to quick up quick heads up or getting started guide to joining the working group this is the place to find it I wrote this up some time ago and it\u0027s still still fairly valid all the links are good well so let me ask the question who is new to benchmarking methodology working group who\u0027s attending our session for the first time everyone\u0027s a repeater that\u0027s great very good all right well well well welcome back then everyone so so that I think that concludes the the working group status except for the milestones and here\u0027s our status on milestones basically they\u0027re all done which is why we\u0027re reach are during we don\u0027t have to talk about any of them being late which is good news so so we\u0027ll have a new set of milestones and hopefully that will be part of our discussion at the end of today all right so we always have this quick discussion about a standard paragraph which I have developed some time ago and like to see incorporated in our drafts the reason for that is is this that we have a scope which is laboratory testing only isolated test environment and for years folks in the security Directorate or other directorates would look at what we were doing and just absolutely flip out and you can\u0027t send traffic like this over the network and they had never read our Charter the scope of our charter for lab only testing was you know not not generally included in the drafts because we all knew what we were talking about and and and so you know we sort of jointly developed this paragraph for the introduction and or security considerations sections and you\u0027re welcome to modify this or to use it a wholesale if it applies to your work and we\u0027ve we\u0027ve effectively reduced the amount of difficulties that the Directorate reviewers who\u0027ve never been in the room with us have with this kind of thing so just a word to the wise that that is available all right let\u0027s see here so this is our work proposal summary matrix and what I think what I\u0027m going to do is bring this up again when we get to the new work but let\u0027s let\u0027s go back "
  },
  {
    "startTime": "00:15:08",
    "text": "to the agenda and begin dopes beginning to pick it up from there so I think let\u0027s see here now we don\u0027t have Sarah and we do have Bala now that\u0027s good okay so like I can sort of see who\u0027s joining us and then so can you in fact so great so so we don\u0027t have Sarah we don\u0027t have move on who generally speak about the Sdn controller performance but I\u0027ve already given a brief status on that if if either one of them joins us we\u0027ll reinsert that in the agenda or where we need to ok so then moving on to the to the next topic here is that topic would be the benchmarking methodology for evpn and PBB evpn and our presenter is going to be sitting Jacob so let\u0027s let\u0027s get to the point where Sue Dean is in the is in the queue and I will bring up sitting slides which are here presentation mode on those okay and and and soothing can you join us and speak can you hear me uh yes yes we can hear you now thank you yeah hi hi Ann hi good morning everyone so this is the draft which I\u0027m going to present as a benchmarking of evpn and PBB VPN services so which is both become the RFC is in pest workgroup and can we move to the next slide it\u0027s um I\u0027ve moved but it\u0027s a I see it\u0027s still coming on on me check out there so so hang on okay thank you thank you so actually we can we can see it here in the room so Dean okay right then I\u0027ll read from my slide so the comments which was received from the previous idea of 99 and the offline comments which were you know in the mail exchanges so they require the clarity the topology then terminology sections and certain area sections has to be moved out of the graph and one important comment is was received this with ESI withdrawal that is the flushing of Max in the local Ethernet segment that has to be "
  },
  {
    "startTime": "00:18:10",
    "text": "especially earmark and the remove removal of that type fire out so all this sections are taken care and we have modified based on all the comments which these are the highlights of the comments and all the sub comments we have incorporated the draft so then moving to the next slide so yep we\u0027re there yeah perfect thank you so these are all the high level you know the benchmarking parameters we have defined for this services that is the macula the matte flesh the Mac aging high-availability the ARP and nd scaling scale convergence and the soak so based on eight parameters these are the eight parameters we defined through benchmark this services evie plane and Peabody VPN services okay and yeah they\u0027re going I\u0027ve moved to slide four sitting oh thank you all so special thanks to Sarah for guiding us and she was very helpful in giving the comments and the total outlook of the draft and thank you all for the valuable feedback Google given in by DT of ninety nine and offline comments and really value that and really appreciate that and thanks al for the support you\u0027re very welcome and tonight and I I mentioned at this point Sedin that I\u0027ve been trying to recruit some some folks who are able to provide some additional support in the you know in the form of perhaps some suggestions for additional tests and and things of that nature does that sound good yeah sounds good and I mean because all in here we are working for the community and always welcome very good okay well are there are there any questions for Reza Dean on the benchmarking of Ethernet Virtual Private Networks and provider provider backbone Ethernet provider Virtual Private Networks it\u0027s a mouth seems not ok so very good well then thanks for thanks for updating the drafts again this time sue Dean and you know I think though your your last slide basically says next steps request for adoption so I think we\u0027re like three-quarters but basically 3/4 toward adopting this draft I mean on the on the summary of proposals I\u0027ve already got this one marked green and I don\u0027t think there\u0027s going to be much argument about adding a milestone with respect to this "
  },
  {
    "startTime": "00:21:10",
    "text": "draft so I think that I think you\u0027re you\u0027re sort of well on your way to adoption and and let\u0027s just look for further suggestions on development and bring them in as soon as we can thank you thanks for the support very good I\u0027ve got your thank you slide up right now too so that\u0027s a perfect thank you would be all right thank you City yeah all right so then back to the agenda so now so now we\u0027re going to look at the continuing proposals and unfortunately slide item number this item number five this one that we we don\u0027t have a presentation for this time but I\u0027m going to give a I\u0027m going to give a quick status and that is that so this is the MS Li M network service layer abstract model so we\u0027re where we had a presentation on this remotely in the Singapore meeting the IETF 100 the the interesting part of this proposal was that it it it attempted to model the service sort of the service layer in a way that it could be tested and the the current model was more for control of a bgp router so what we\u0027ve challenged the author to do is to is to make this modeling effort more specific to benchmarking and and so that is something he\u0027s working on and he correspond with me by email basically to say that the the works in progress and he\u0027s expecting to complete the draft this month with this benchmarking example built in and so we\u0027re looking forward to that but at the moment we\u0027re kind of on hold waiting for update so so just so I\u0027m just mentioning in my quick notes here example benchmarking example will come in okay so then so so then now we\u0027re up to item 6 which is a proposal from your chairman al Morton to update the RFC 25:44 back to back frame benchmark so let me bring that deck of slides up here that\u0027s up at the top "
  },
  {
    "startTime": "00:24:13",
    "text": "okay all right welcome gentlemen please please sight find in sign the blue sheet there others someone\u0027s got it for you right behind you very good so so we have this draft on sort of updating the RFC 25:44 back to back frame benchmark and let\u0027s take a look at that so it 24:44 specifies this benchmark it\u0027s defined in RFC 1242 as the longest burst of frames that a device under test can process without loss so tests of this parameter are intended to determine the extent of data buffering in the device now once we learned once we learned that intent and also the test that was involved we\u0027ve got with somewhat further investigation we found ways in which we could actually improve sort of getting at this quoted sentence and I\u0027ll describe that briefly but one other thing that we noticed is that in RFC 25:44 there\u0027s an extremely concise wording of the objective very concise procedure and very concise reporting and bye-bye concise I mean brief and perhaps at at some point it would be worthwhile to add some additional detail so that\u0027s another thing that this draft does so what\u0027s gonna end here all right so in at the Singapore meeting let me actually I\u0027m gonna skip ahead here just a little bit we we ran some tests recently using the back to back frame benchmark and and what we noted was that for some fixed frame sizes the testing was extremely consistent and what what that means is that the length of the bursts that could be accommodated without loss was fairly stable in terms of repeated tests also we noticed that with some frame sizes it was highly variable and there was a there was a reason for that which I\u0027ll get to if we if we have one more time but essentially anytime that the any time that the the device under test was almost able to "
  },
  {
    "startTime": "00:27:15",
    "text": "accommodate the frame rate at a given frame size chosen then you would see basically no buffering in the device or no no no ability for a burst of packets to exceed the buffer size because packet processing packet header processing was was fast enough that devices that the packets were moving through there was there may have been some buffer developed there but it simply was not possible to overflow with the buffers on a regular basis every once in a while you might see something that was generally this the cause of inconsistency so some of the test equipment reported frame lengths which were extremely long unexpectedly long but that turned out to be just a limit of the test equipment it would send a 30 second frame of packets and they would all pass through because they were at a fairly high frame size and therefore a low packet header rate and the header handling rate was of the device under test was sufficient to handle those packets so you basically couldn\u0027t create a a loss and the device test the test device would send a maximum burst of 30 seconds and then it would report the maximum burst size without lost is 30 seconds well we all know none of our devices under tests have 30 seconds of buffering so this this needs to be noted and added to the Edit to the procedure so it was also noted that calculating the extent of the buffer time in the DUT helped to explain the results for all these frame sizes and and that\u0027s the that\u0027s the corrective calculation that we\u0027re basically proposing here and and it was observed that the actual buffer time could be estimated once you had calculated a throughput prerequisite test you could tell whether frames in the device under whether frames would be just passed through the device under test or whether they would be buffered and if they were buffered then you could actually measure the buffer the total buffer size if they weren\u0027t if they were just passing through the device under test then there was no way to do that and in this in this testing regime and and I\u0027ve basically got a slide which which attempts to show that although it\u0027s it\u0027s just it\u0027s just too small for most of you to see but I can explain it briefly we\u0027ve got different packet sizes here 64 bytes 128 256 up to 15 18 and and the most important bar here is the green bar which is the maximum theoretical frame rate for the device under test with its interfaces and and so forth and we had a test with a couple of different V switches the O vs v "
  },
  {
    "startTime": "00:30:17",
    "text": "switches in blue and the VPP V switch vector packet processing is in red and we see throughput in frames per second so that\u0027s the y-axis here and and with 64 byte frames we could not attain the maximum throughput that\u0027s not all that surprising it\u0027s very common with the smallest frame sizes and so at this frame size we were able to very accurately estimate the buffer size because now that here the packet header processing was less than the theoretical maximum throughput of back-to-back frames and and so we always saw some burst length which which had some loss in it now here we\u0027re sort of right on the 128 byte frames we\u0027re kind of right on the cusp of the maximum theoretical so here we saw some variation but that\u0027s completely explainable sometimes it would handle it handle the worst sometimes it wouldn\u0027t there would be some variation in the results by the time we get to 256 byte frames they\u0027re their throughput is equal to the maximum theoretical so now the packet frame processing rate of the device under test is preventing the buffers from growing so this is this is the effect that that we felt was really important to to be able to handle and to and actually to cover as part of the test procedure so what we\u0027re recommending then is this prerequisite test of the 2544 throughput any questions so far as any who in here is run the 2544 back to back frame throughput test one Doug thank you so that\u0027s good so well I\u0027m not gonna I\u0027m not going to go through this procedure then in in detail but you\u0027ve you\u0027ve basically you can basically see what\u0027s happening here we\u0027re gonna we always start out with a list of fixed frame sizes the tests are composed of repeated trials where we\u0027re searching to find the largest burst length that the device under test can handle without loss for each frame size it the trial requires sending a burst length and Counting the forwarded frames to be sure that none of them have been lost and we\u0027re seeking the longest burst length with zero loss we\u0027ve got a test the test outcome is the burst length for each after the searching you\u0027ve repeat you you have found the longest burst that you can send without loss and then we repeat the test n times with the searching and and the burst lengths with zero loss are then subsequently averaged and the average link is the benchmark over in Java and end-to-end tests so "
  },
  {
    "startTime": "00:33:23",
    "text": "well let me talk about the updates now now we\u0027ve got a little bit of background here so we\u0027ve basically clarified text describing what is measured when we report this this length and the corrected the corrected burst length I\u0027ll talk more about that in a minute but let\u0027s let\u0027s just stare at this for a moment knowledge of the approximate buffer storage size in timer bytes may be useful to estimate whether frame losses will occur if device under test forwarding is temporarily suspended in a production environment due to unexpected interruption of frame processing and then this parenthetical an interruption of duration greater than the estimated buffer would certainly cost cause lost frames now you you can\u0027t really estimate what the sort of what the minimum in in practice what the what the guaranteed number of frames that that could be a cover that could be accommodated during a short interruption of forwarding you can\u0027t really you can\u0027t really guarantee that because you really don\u0027t know the state of the buffers when forwarding is interrupted and the truth is that all the buffers could be almost full when forwarding is interrupted briefly and that would cause the sort of the additional bursts or the the short interruption time to be zero so actually always zero is is the but so now we\u0027ve at least kind of got this range that we can characterize with this test so this is the potential benefit of essentially using this correction factor and there\u0027s there\u0027s lots of detail in in terms of background in in these two references so what we\u0027ve basically got here is a presentation from last summer at the open platform for any of the summit and that\u0027s where some of these tests were analyzed in detail and we\u0027ve also got a wiki that supported that testing which explains the calculations and even more detail than the slides in this presentation so I I think for the sake of for the sake of time I\u0027ll let you read the draft go over the calculations yourselves but effectively with this when you have the the implied buffer time it\u0027s it\u0027s the average number of back-to-back frames "
  },
  {
    "startTime": "00:36:25",
    "text": "that you can send in a burst divided by the maximum theoretical frame rate for the interfaces that you\u0027re using so that\u0027s the so that\u0027s the implied buffer time the length of the burst divided by the rate at which the the burst is sent but when we want to correct that what we need to recognize is that with packet header processing in progress some of the but some many of the packets that have been sent into the device under test have been processed and passed out of the device by the time the burst eventually causes a loss so what we try to do is estimate that by by using the theoretical well actually the ratio of the of the measured RFC 2544 throughput the maximum offered load that the device can handle we use that to correct this implied buffer time and and so what we\u0027re effectively doing is estimating the number of frames which have passed through the device in the burst and that way what\u0027s left is the frames that have been buffered before we see the tail drop laws so that\u0027s I mean that\u0027s basically the calculation we\u0027re going to report that as by frame size we\u0027re gonna report the frame length will have the min and the max and the standard deviation of the repeated trials and and then this calculation of the corrected buffer time in in seconds so that all of that will be part of the reporting for this metric and and obviously these parameters static and configuration number of test repetitions the minimum step size in deciding on the bursts and and so forth so any questions about that material okay well then so this is the most interesting part I wanted to get to the point where we have a little discussion here when we\u0027re searching for a burst size should we should we include a particular search algorithm here should we my guess is that we should always start from a burst length which were fairly sure that the device under test can accommodate and that way that bursts will show zero loss and then we can begin to step up in a linear fashion to reach the burst size which possesses a loss so that\u0027s my thought on the on one search algorithm but there could be others so I\u0027m looking for input from the group any any ideas there and well then let\u0027s consider this other "
  },
  {
    "startTime": "00:39:25",
    "text": "one because it\u0027s also related to we\u0027re searching so should the search include trial repetition whenever a frame loss is observed to to potentially avoid the effects of background loss with with virtualized devices under test virtualized switches and so forth every once in a while you have sort of this this unexpected mode of operation where some some extremely important background process has to be handled like file system flush or something something that\u0027s completely non-maskable and and you have to have it but it could it could end up unfortunately influencing your test results so I think that it would be worthwhile to immediately verify when we see if rain loss whether that loss occurs in a reliable way at a at a given burst length so we have a comment so I Carsten a constant wasn\u0027t so I think that\u0027s I would like to pose the question the other way around how to get to a realistic measurement result if only testing at a given time because you know these kind of things that happen from time to time in virtualized systems we see them as well but sometimes they happen only once an hour and yeah I\u0027m not sure what is what is the intention of this methodology should it actually yield the optimum result or a realistic result or the minimum results well if the if the phenomenon if they\u0027re not phenomenon that causes the occasional once an hour of loss is not the phenomenon we\u0027re trying to measure the the capacity of the buffers in the device then it would be really good if we could somehow separate those two effects and and one way to do that is is to perform long-term testing let\u0027s say at the throughput level and to look at the results every five minutes during the hour and then to find out how often some background process causes loss and influences the results right but then you have to do a lot of repetitions in the end yes like a real large number efficient yes or alternatively state just simply like do everything possible to make sure that there are no background processes happening you know flush the fire system before make sure all of the interrupts have been cleared "
  },
  {
    "startTime": "00:42:26",
    "text": "okay I don\u0027t know what could be done now that\u0027s I mean if you can legitimately if you can legitimately kill demons that would would not necessarily be would not necessarily be running or something you know I mean I\u0027m truly unnecessary background processes I think that would be a legitimate thing to do but then some of these other things just like file system flush they have to be lived with so that then your point is let\u0027s get that let\u0027s let\u0027s encourage those to happen before the test just like we\u0027ve done in the past with you know making sure all the all the ARP learning all the Mac running is done before a test takes place so that basically so that maintenance and administrative kinds of things have have occurred outside of the I interval well if you give me a to socket system I will challenge that I can get very different results if the virtual machine is running on the first circle on the second circuit because some interrupts cannot be moved away from the first circuit so yeah I\u0027m not sure I know if the results should be really well reproducible then it\u0027s it takes a lot of effort to define the environment and to make sure people are not circumventing it yeah yeah that\u0027s true that\u0027s true so I think that i think that the draft currently encourages people to in general to operate this test in under circumstances that appear to be you know the kind of test environment we\u0027ve been able to count on in the physical world where if you basically if you see unexpected loss you you troubleshoot your environment and try to scare that stuff out I agree it\u0027s going to be much more difficult to do that here now but I think we should still encourage that and so anyway I think that\u0027s I mean I think we\u0027ve had a good discussion about the sort of the trade-offs here of adding this repeated trial aspect and let\u0027s let\u0027s think about that some more and and then see what we can see what we can agree on when it comes to some text I\u0027ll probably propose something coming out of this meeting "
  },
  {
    "startTime": "00:45:32",
    "text": "rather than attempt to do it here okay well let\u0027s see so as far as next steps go I\u0027d like folks to take a look at this it\u0027s not that long a draft we could create a milestone for this work that\u0027s actually part of the proposal for the retargeting we\u0027re hoping at least I\u0027m hoping for a working group adoption for this draft we can\u0027t ask anything about that today because who\u0027s read the draft by the way anybody know okay so we can\u0027t we can\u0027t ask that question again today but I really encourage folks to read it and well we could still propose a milestone if folks are interested in this topic and this is one of the ones that I think where we can we could probably really learn a lot about the environment that we\u0027re focusing on this virtualized testing environment through benchmarking like this so I guess that\u0027s it any any further questions or comments on this topic No okay well good thanks for your attention and we\u0027ll move on to the next thing all right so let\u0027s see here I\u0027ll kill this one so back to the agenda so now we\u0027ve got item number seven which is updates to the firewall benchmarking for modern firewalls draft Karstens going to be our presenter and let me get those slides up for you Karsten so there we go I\u0027ll bring this version of it up all right and you\u0027re you\u0027re all ready to stand in the box that\u0027s great let me give you the let me give you this and make sure it\u0027s working yep it\u0027s working okay very good okay so actually my colleague Bala Bala Raja has done most of the work but he can\u0027t be here today because of another project so I\u0027m the co-author so we\u0027ve worked together with a group of people called the the net sec open group to create this next-gen firewall performance benchmarking methodology document as anybody but attended the interim meeting two weeks ago okay cool Thanks so I don\u0027t want to spend too much time but there\u0027s some background about this group has been formed and it\u0027s outside "
  },
  {
    "startTime": "00:48:35",
    "text": "ITF what we decided to that we would really like to submit contribute our test plans to the ITF a vm WT so the idea is to have new methodology a benchmarking methodology and terminology for next-gen security devices firewalls but not only firewalls basically all of the network security solutions out there today like IDs intuition detection unified threat management web application firewalls and all sorts of other beasts and of course there is pre-existent our efore firewall security the firewall benchmarking sorry testing but that\u0027s about ten years old now and we really want to strongly improve the applicability for for today\u0027s solutions and we also want to improve the reproducibility and transparency of this kind of tests because the old IRC was like like they used to do they provided some guidelines the question is how how do people interpret these guidelines in the same way and like tulips would come to the same conclusion so one of the ideas and requirements for our work is also that multiple labs come to the same conclusion and as a footnote in the end we want to create a certification program about this so just a quick walk through of the draft currently we are in version zero to version zero three is ready but I didn\u0027t want to upload it last minute so it will be uploaded after this meeting and so basically introduction scope and so on like normal there is a pretty extensive section on the test setup testbed configuration how to make sure the test beds are all configured in the same way some guidelines for the UT configuration participant configuration as well and then a section called test bed considerations how to make sure that things become reproducible that everybody has the same kind of test bed setup section six is reporting guidelines basically definitions of the key performance indicators and we found that even basic things like I don\u0027t know throughput or something like that it\u0027s always an interesting question like how do you define basic KPI so we wanted to define those and then in Section seven we currently have ten ideas for detailed test scenarios so each of these test cases is like multiple pages of description and starts with a basics that you could find also in the old RC 3511 right right yes and but moves much beyond these the details much beyond with the details provided and we put a lot of effort specifically on SSL "
  },
  {
    "startTime": "00:51:36",
    "text": "HTTPS testing the test cases seven through ten and I think that\u0027s what we where we see the whole internet going there is probably in a few years from now no unencrypted h-e-b traffic anymore so if I was need to be tested specifically with SSL TLS HTTPS in mind so the test setup is kind of as expected there is a solution on the test or device on the tests in the middle there can be some aggregations which is a communication routers if the test equipment needs them and the test equipment is expected to emulate clients and servers on both sides and routers if needed from the configuration perspective of the do team so far nothing No no surprises the surprise has come rather when we talk about like what do we actually need to test and what\u0027s actually in scoping because of the wide variety of different solutions that exists out there there is no single set of test methodology that can be applied to everybody anymore and also the amount of tests required to test more to validate more advanced solutions are growing explosively so we we looked at what kind of test areas there could be like on the left hand side basic things like web filtering antivirus SSL inspection it\u0027s actually not the basic things is like starting with more advanced things and we looked at you know what can we actually provide and of course that would be very weak would become a very long document in the end so we said we\u0027re going to focus on a certain subset of test methodology for now that we can actually manage and of course any contributions are welcome both in this leftmost column area which is like the initial scope from from the point of view of the current authors and also in the futuroscope so as my we can get done of course we\u0027d be happy but we wanted to confine our scope to things that we can do ourselves from the existing set of contributors so next-gen firewall future scope would be web filtering what was this data loss prevention DDoS denial of service certificate validation and these are all functions that are implemented today but they are not implemented by everybody and so we thought we focus on that in the future and then there are all of these other security functions which we haven\u0027t started discussing next-gen intrusion prevention service web application firewall breach prevention systems as brokers and I forgot again what 80 was so so there are lots of different different functions and the matrix is still open and ready to be filled as soon as we can get some "
  },
  {
    "startTime": "00:54:38",
    "text": "contributors for that so the KPI definitions that we\u0027ve that we have so far is on some basic things like on the TCP on the HTTP layer and things like time to last by 10 to first byte and actually it turns out people have a good understanding of all time to first byte but we already started to having some discussions about type 2 last byte and that\u0027s just one example so time to last byte just just to give you some insight means like how long does it take to deliver the whole HTTP response or HTTP response all of the content back and in most cases people have only looked at how long does it take for the first bite to arrive and then the assumption was well the rest will just trickle through the high in some power but I think that\u0027s that\u0027s not what we see all the time and so time to last byte is also interesting but then discussions come in you know this depends on the on the size of the request and response and some people the vendors always like to create responses that are only one byte large because that improves the performance of the device and of course the operators want to have realistic sizes and the test left sometimes want to have very large response sizes so these are the kind of discussions we\u0027ve been having in the last few months we basically started discussing this around a year ago and we started the detailed test test plan discussions Brian would know better what is listening I think six seven eight months ago so I\u0027d like to just walk you through one of the test cases as an example there\u0027s 7.1 throughput performance with the traffic mix and so the first question is what do we want to do here we just want to determine the average throughput performance of the next-gen firewall and that test case has already been in 3511 so what\u0027s the difference here first we define a specific application traffic mix so it\u0027s not a uniform stream of frames or packets that\u0027s coming in with arbitrary content or no content in fact these are all HTTP HTTP requests which follow a certain distribution of URLs of certificates and so on and that has been well-defined then there are a couple of variable test parameters that the user can choose like what how many clients they want to have how many servers they want to have what\u0027s the traffic distribution between v4 and v6 I think defining something statically it would be unwise because v6 traffic is growing over the years and we\u0027d have to adopt to the reality and also the initial end target throughput so let\u0027s say we start with 10 percent of line rate and then with the target throughput would be hundred percent layer it and we need to see what is the actual performance of the solution and so that requires some binary search in that case "
  },
  {
    "startTime": "00:57:38",
    "text": "so we start in the test procedure to run with 10 percent for example then go to 100 percent and then go through a binary search so that\u0027s much like many other BMW G I\u0027ve sees have worked this out before and so to to understand like what is the actual throughput of the system we have to define a maximum failed application transaction rate currently it\u0027s set to 0.01% so every 10 request may fail all the other 9099 are expected to succeed and we also defined what is the maximum number of sessions that might be dropped unexpectedly by the Dutch this is an expected TCP reset percentages and what\u0027s the maximum deviation of the time to last byte so let\u0027s say for example a firewall would actually run through all of this request successfully but would have widely differing latencies because maybe it gets out of it gets lost within which sequence to process the requests and then of course the time to last byte would be so much differing that it doesn\u0027t actually make sense to use the device anymore for this kind of load so that\u0027s why we want to limit the TTL DTV ation so they say the difference between the maximum and the minimum time to last byte shall not exceed a certain a certain threshold and the same in the same way also the maximum TCP connect time is controlled in this test case let\u0027s say the device gets loaded and then it takes more and more and more time to actually start responding in any way so it takes time to connect new TCP sessions then sometimes a way for our device to manage very high load but of course at some point it becomes unreasonable and again we need to we felt we need to limit the maximum TCP connect on please Albertson asking a question as a participant but when we say binary search did we all know what we mean when we say that do we all know that we\u0027re starting high or starting low did we all know that there\u0027s a step size involved no matter how you search I\u0027m I\u0027m thinking that there\u0027s I see were nodding I see I see a possibility for us in one of our works to to define this with a little more specificity do you have any thoughts on that yeah I think we\u0027re we\u0027re defining this in the draft if not to a level of detail that you think is reasonable then we can "
  },
  {
    "startTime": "01:00:39",
    "text": "certainly expand the definition it\u0027s not a big deal I think there are no there is no magic behind binary search that\u0027s just a question of definition to make sure everybody uses it in the same way right and and and and then I\u0027m sort of thinking that this this repetition of checking certain results that that would be important to have that if we if we agree that that\u0027s a valuable thing at some point that that we it would be easy to see how we would build that into binary searches and linear searches and so forth if we have the if we have the good degree of specificity what do you use repetition well like I mentioned before if you know if you get a result with in the data playing testing if you get a result with loss run that test again and and and now see if it\u0027s still roughly the same level of loss or or yeah or now we\u0027re now suddenly clear the problem is that binary search is a very time consuming method so let\u0027s say we need 10 iterations to get to the precision of 0.01 percent failed transaction rate of water now actually no that\u0027s not the point I don\u0027t actually know what what precision we find here but I remember for one percent precision an average of like seven iterations are required unless of course the device is just passing the initial maximum rate with no loss at all in which case it\u0027s only one little rate or two iterations initial end targets but if that\u0027s not the case then around seven iterations are required let\u0027s assume each of those runs two minutes then we already have 15 15 minutes but normally you want to run the test case a little longer so if you you have half an hour and then if you want to repeat these the whole series of tests multiple times it easily becomes days of testing just waiting for results yeah sometimes a little difficult in the lab sure so I\u0027m actually open if anybody has better ideas than binary search which we once try to linear search in some situations where the results were very difficult to predict but of course with linear search you immediately have many many more iterations from the start so I guess by my thought about that is the the the tests tests of the past should be able to help us fine tune the parameters for the searches of the future and and that that there\u0027s basically always some correlation between results across across tests that tests the same configuration and and there might be there might be a way that we can improve test time I mean if that test time is always one of our big constraints if if we if we build that "
  },
  {
    "startTime": "01:03:40",
    "text": "fact into the the repetition and by by kind of tightening the search range based on previous results that might help I don\u0027t know if you you may have already tried things like that well I\u0027m certainly open for suggestions sure and we haven\u0027t always tried out everything because we are on to a test equipment can do right it\u0027s right if we have to do everything manually it doesn\u0027t make sense typically you program some iteration method and that\u0027s depending on what the commercial test equipment are there supports yeah so the good news is that we we have some folks from the commercial test equipment world usually right right joining us or watching our lists very closely and and when we you know when we decide that we need something with a lot of motivation and they agree on the value then we\u0027ll will influence what\u0027s available to us yep I\u0027m pretty sure an experiment are both contributing to this effort yes very good so another question I had or comment Carsten is this phrase test results acceptance criteria we have to be with me really careful with the wording on this because all we\u0027ve always said that our benchmarking and and really any performance measurement in the ietf that we don\u0027t declare kind of like a pass/fail criteria we we seldom set performance objectives here in a numerical way we may we may recognize that they exist but but the ITF itself hasn\u0027t standardized them we allow others to say here\u0027s the target rate and and and then you use the ietf procedure to find out whether you\u0027ve met that or not so I mean we at the same time we\u0027ve always had a kind of acceptance criteria RFC 2544 throughput is run with zero loss and and and and that\u0027s a numerical sort of acceptance threshold or target objective for the test that you\u0027re running and this is this is additional dimensions which have the same I mean they have the same feel but I I think we\u0027ve got to be careful about how we word these and it may be that we may be that we asked the testers to provide the the point o 1% themselves or maybe suggest this said this set of this set of criteria has been used in in an appendix something like that I always thought from the beginning of my career that the zero loss in RC 25:44 was a strong and very helpful statement and over the last few years we\u0027ve jointly suffered from you know when is "
  },
  {
    "startTime": "01:06:42",
    "text": "trying to deviate from the zero loss in the virtual world and the subsequent attempts to standardize any non zero loss right so I think there is great interest in these numbers and I don\u0027t want to shy away from them if we completely remove these targets from the draft I think it would lose a lot of its value because we really want to set like expectations are you say minimum expectations for for next gen network security devices now I understand also of course that is it\u0027s one of the more difficult decisions to come to a specific number number yes maybe we can move them into an NX or something but I definitely I would personally prefer very much to keep the numbers in the document yes I think without that it\u0027s just another thing that can be tweaked for each user\u0027s preferences yeah yeah and and and if there\u0027s complete freedom here that\u0027s that that\u0027s not achieving the goal of let\u0027s avoid spec moonship and and have comparable results from every test from every lab so so we\u0027ve got we\u0027ve got in some way we have to somehow balance the desire for reproducibility with with our version here to setting acceptance criteria and and one of the ways we can do that is to get rid of the words acceptance Queen so but but but that\u0027s let\u0027s let\u0027s work let\u0027s work this aspect I I appreciate your bringing this topic for discussion Thanks yeah and admittedly there are there is still a wording to be improved and aligned with normal ATF team WG wording so we\u0027re open for those come examples now this is a great start thank you one way to make sure that the numbers if any here are representative and realistic is also to bring in some users and we have this set of calls weekly calls that are aside from in WG calls they\u0027re basically just work working calls right and currently we\u0027re bringing in some users from financial sector mobile mobile service provider mobile operator and automotive sector so like just some users enterprise and so it\u0027s vital to make sure that we actually fulfill their requirements that\u0027s fantastic and somehow we need to you know in in in some way include these participants directly in BMW G work if we can maybe there maybe there\u0027s the opportunity for an interim meeting in the future where you know sort of jointly BMW G and net sec open get together with some of their industry participants because I mean just yesterday in the Operations Directorate group we we talked about how do we get "
  },
  {
    "startTime": "01:09:44",
    "text": "more user input and then make our specifications more relevant and and this is right on point if you guys are able to if net sec open is able to help us do that great let\u0027s cut scope okay sure we\u0027re happy to so it goes on with the other test cases and I don\u0027t want to bother you with more test case discussions here so in 7.2 concurrent TCP connection capacity with HTTP traffic it\u0027s a similar you know question you know how to get to a good description and we also defined some things here like the HTP object size and the rate at which this capacity test has been has to be conducted to make sure it\u0027s actually realistic one of the main motivations of this homework has been that the data sheets of firewall vendors have shown less and less connection to the reality of the actual throughput of their devices in real-life scenarios and one of the reasons was that this test case for example is typically carried out with zero traffic so basically the vendors just opened the session and maybe send a request to get one byte response in the beginning but then they leave the session open and they change the time also that the sessions are in keeping kept open and then sessions sit there says this sit there and it\u0027s basically just only a memory game you know how much memory can I put in my firewall and how long does it take to fill that memory but if and then usually we came in as an independent test lab and we would try to reproduce this it doesn\u0027t work because of course we keep sending requests over all of these connections or at least a subset and often these can these connections are dysfunctional so the the device is completely overloaded and it doesn\u0027t know what to do anymore it has no capacity to process packet anymore it\u0027s only busy opening more and more and more connections and that\u0027s what we wanted to get away from and make sure that this the result from this test case yields a realistic number of maximum connections so I spoke about the traffic weeks before and one of the test equipment vendors involved in this work has contributed a traffic mix that they prepared since october i don\u0027t know for around a year and so they basically defined a modern enterprise parameter traffic mix we already had this discussed in the interim meeting and i remember Farrah and you and some others saying yeah this is wonderful please not not specifically this distribution but in general the idea please let add more traffic mixes for other scenarios because an enterprise parameter is important but it\u0027s not the only traffic mix and for example a mobile operator one would look very different so this is a blend currently of 70% encrypted 30% unencrypted traffic there are a lot of different URLs and the exact URLs probably don\u0027t matter because "
  },
  {
    "startTime": "01:12:46",
    "text": "they change all the time anyway if we would look at the exact office 365 URLs probably Microsoft has changed and three times since this was generated but what is important is that the certificates don\u0027t met don\u0027t change at the same frequency and that the type of traffic distribution like how many connections does the client open to actually get to office 3 to use office 365 that also doesn\u0027t change so frequently and the number of URLs used in this parameter mix also doesn\u0027t change so frequently so there\u0027s always a few heavy applications and a long tail and these are quite important to use to create a more realistic cetera again so there are 10,000 unique URLs 1002 main names 400 certificates and this makes sure that the final work cannot be optimized the configuration cannot be optimized by a vendor just to survive ok let\u0027s dump all this memory let\u0027s reduce our certification store to 1 and just send all the same certification again and again buffer it and catch it and then optimize what throughput so we really want to have a realistic results and that\u0027s one of the means to getting there so those traffic mix is already implemented in at least one emulator and I think another vendors getting up to speed and we\u0027re also see trying to see how we can actually implement this mix in a open source emulator the t-rex the t-rex emulator which would then just be a capture and replay I\u0027m not sure how this is possible certificates but we\u0027ll see so that\u0027s basically all I wanted to say about the content proposed schedule I promise to upload oh I think I\u0027m actually made a mistake so this should be draft or true and zero three four five six seven so I need to fix this so I have an offset of one in this graph numbers so draft zero two has been uploaded I mean before the deadline and zero three will be uploaded tomorrow with the traffic mix and X has been edit in two more test cases and subsequently I think we want to proceed quickly and an important milestone would be in June to add security effectiveness section so so far we only focus on benchmarking but of course you just those more advanced security devices especially intrusion prevention intrusion detection of course there is no point of testing the through word the important point is to test different types of attack vectors and there is a nice database which we\u0027re looking into probably having around 1000 attacks that we want to test we won\u0027t define them all in the document but we\u0027ll probably point to the NIST database and select some that\u0027s a that\u0027s that\u0027s a great that\u0027s gonna be a great benefit definitely referenced that and you know we should all we should all look it over "
  },
  {
    "startTime": "01:15:46",
    "text": "and help help to choose the ones that ninja target right and then separately we\u0027re running some concept testing with a couple of solutions we will be running let\u0027s put it that way especially for the performance benchmarking test cases those pocs are scheduled to be completed end of June and then hopefully in July and before the deadline for Montreal for one or two will have a stable draft to be submitted our question back to you because I\u0027m not too familiar with the whole process that\u0027s like how do we get to an RFC and what other steps I need to observe well so it would be really good if we adopted our Charter within the next two weeks I mean I think we can make steps toward doing that and get some milestones agreed and then assuming Warren can successfully negotiate the new charter with the iesg which hasn\u0027t been a problem since 1989 then you know we will will be in full go ahead mode now the with this schedule trying to get a stable draft by July we\u0027re basically we\u0027re basically you\u0027re looking at trying to get interim meetings together to advance this work as quickly as possible and I\u0027m gonna and if so if that\u0027s what you\u0027re asking and and that\u0027s what the working group is is ready to to do then we could probably get pretty close to July and I think that\u0027s I think I think that\u0027s that\u0027s a little bit on the demanding side but I can also I can also imagine that if you want to get it done before all the summer vacations start I can understand it because then you\u0027re looking at two months of not much happening now I want to get it done because we\u0027re going to get it done because we think there\u0027s some urgency and specifically because they want to use the results for certification within the next like open group so we\u0027re progressing the work anyway and if an interim meeting is as easy as like a one-hour conference call like we staged two weeks ago I think we can we can even use the existing calls and just you know give them if one of the calls or you know from time to time call under your guidance in Sarah\u0027s guidance and actually make them ITF intern meetings well I think I suspect that that\u0027s possible as long as we obey the you know sort of our meeting announcement sort of pre announcement rules things of that nature and and and as long as it\u0027s accessible to everyone in the ITF and we published the minutes and so forth although all the usual interim meeting stuff then we\u0027re we\u0027re quite good to go "
  },
  {
    "startTime": "01:18:49",
    "text": "on that I think I think it would be it\u0027s probably reasonable to to to maybe have let\u0027s say let\u0027s say working group consensus by July and we have multiple interim meetings and so forth I mean then there\u0027s our area director review which has which has gone quickly and smoothly for good drafts and then and then there\u0027s the IES them there\u0027s the IETF last call which takes about two or three weeks and and then another couple weeks to get it on iesg agenda so I think I think by the time by the time you\u0027re looking at ie ie sge our governing body approval we\u0027re probably we\u0027re probably looking at sometime in August for those steps to take place but but folks in net SEC open can say this achieved consensus in the workgroup and it\u0027s moving through the approval process if we achieve Orton group consensus by July okay that\u0027s all good and I mean of course we\u0027re a bit selfish in terms of the to the two reasons we were we are actually putting this work in the ITF is one of course we think it will be much better recognized in the industry and we think if we spend a whole lot of work creating a good good test plan then make sense to give it to the best you know organization in the industry that\u0027s seen that\u0027s recognized not only by the service by the world but also by enterprises and the second is of course are we want to benefit from the whole process and experience and you know making sure this is actually technically a good tenant yeah so so in fact when it comes to July we have a another face-to-face meeting scheduled in July that\u0027s in Montreal yeah it\u0027s probably going to be accessible to easily accessible to a lot of the net sec open folks could travel there for the day or whatever and so so that might be a good sort of a good time to put the finishing touches on the working group consensus aspect for whatever whatever group of tests you\u0027re able to put together by then and and and then take it from there right okay so request correction please review and please feel free to contribute on each of the sections we have assigned contributors for these the test case lists that I explained before and in addition specifically new traffic mixes contributions are requested and security effectiveness test methods okay and if anybody is interested and we\u0027re more than happy to invite you guys to the weekend course that we have we actually have two weekly calls 101 on performance benchmarking one hour security effectiveness that\u0027s it very cool any questions folks or comments looks like a pretty organized effort to me I "
  },
  {
    "startTime": "01:21:51",
    "text": "think I thank you for bringing it here I mean not only is it going to help you it\u0027s gonna help our visibility in this industry as well I I think I think we should we should plan on two things we should plan on getting a security area advisor someone who you know is sort of willing to look over this from the security Directorate I get like for an early review or things of that nature just so that we have the benefit of IT F\u0027s security expertise right from the start because that\u0027s basically what we\u0027re we\u0027re talking about the the intersection of benchmarking and security that\u0027s right so the opposite group has asked me to present this draft great which I will do I think tomorrow or Thursday tomorrow I think that\u0027s good that\u0027s good I\u0027m really glad to hear that and so and by the way I shouldn\u0027t forget thinking for the other people who are attending so Erica and Tim from UNH I will are participating in the effort and if you if you have anything good to snice to say about organization that\u0027s actually Brian rank means what is the director of the whole that\u0027s like opening very good very good good I\u0027m glad good glad some new folks from net sec open have joined us for this that\u0027s great mister is there a problem ok all right well when we get to the Charter will see that you know I\u0027ve obviously recognized this as a very active draft and so we have a a milestone proposed for this we may need to revise that milestone though so based on your your most recent version of this work fast in a meeting scheduled so uh well try to remember to to do that when we get there all right let me drop this let\u0027s see in this in that slideshow all right all right very good so now let\u0027s see here we\u0027ve got oh I\u0027ve already got this one up that\u0027s good so I think our let me make sure I think our next topic is this one with Raphael is that correct laughs yeah yes absolutely all right so let me let me see what I can do with this I think that\u0027s about as big as I can make it so Raphael welcome "
  },
  {
    "startTime": "01:24:54",
    "text": "back so you go I\u0027m sorry I didn\u0027t understand the quad can you go away so I I thought you\u0027d like next Ivan well I don\u0027t think this will work cuz it\u0027s a PDF oh does alright alright yeah sort of well don\u0027t worry I\u0027ll true it up after you press it so basically this is a work done my some research background I\u0027m going to tell the story line about this it\u0027s about vnf benchmarking methodology and mostly I\u0027m here to talk what we know so far that we came to this district and answered is for questions here along the presentation so basically what we know so far about benchmarking TNS you know in our concepts from the research I\u0027m doing research in university benchmarking vnf we know that there are different concepts for vnf since the definition for run time like before program until different ways of compiling TNS optimizing and what you might find the state and then the data plane of the vnf there are different compositions of the vnf itself like project Clearwater and some of the NFS are truly programmable via Sdn concepts and openness which we know that also there are different motivations for benchmarking genna\u0027s from different actors like PMF developers service provider infrastructure providers compare to compare via naps with physical network functions to know the the footprint of vnfs and to have some analytical development of Afyon s as well we know that there are different factors that TNF depends on the performance of the TNF basically it\u0027s all the blocks here with the green config box we have done tests to evaluate that and this is basically the storyline I started this work in 2015 like proposing a paper and some ideas how benchmarking and maps as a service then we went to in Feig then we had to bend up G and then we we had a like a stop over there to that we are quite naive at that time to make progress if the draft we see that still the considerations for benchmarking that became the RFC 81 72 hours to being performant here so we took our time we develop at some code run experiments we came with publications and now we are back here to see how how the group is open to receive this work again and we have the open "
  },
  {
    "startTime": "01:27:54",
    "text": "source to be released this year too so our contributions for further for the group here are basically on basically on the on the running code that we have done our experience and the there they are see that we published so mostly the draft was updated because it was not focused on methodology itself and in the beginning so there was a comprehensive or rewriting Don based on experiences in the Ronnie code that we we have as a target for this their draft is actually to be it kind of like a solid foundation for for via net which marking methodology itself as a generic framework and we think that specific vnf method benchmark methodologies could be derived from this this document we aim to approach like publications that we are performing and and we seen the literature there are other groups also interested in this that we have been exchanging emails with and mostly activity is also inside Etsy and Fe the scope of the the the document basically consider VN access black box and the finds methodology further but this is something that we need to discuss as well because as I showed in the beginning there are many vnf that open source and are open for instrumentation internal instrumentation so we think now by the and this is open for discussion the group I think we consider quite box approaches as a particular case with some proper considerations of internal Genesis fermentation this is something that we need to discuss the terminology base comes from Etsy innately framework it\u0027s also inside there are the drive itself most of the heifers there we can\u0027t we come with also referred to RFC 1242 and there\u0027s IC 81 72 we don\u0027t have any other reference of an FD I think inside IDs so we need to to grab most of these definitions of vnf from from at C this is something also that it\u0027s open for discussion and so which were the music the major technical changes front from this ref to the past is that we realize it that we have different ways of testing which Vienna\u0027s or he come from dimensioning verification and which marking has at Sanofi protesting working group did and we know that at the end some of them cause both of them close to two benchmarking we have a generic range vnf benchmarking setup specified in the in the in the draft that with generic components I\u0027m going to show it we have the official for what is the deployments "
  },
  {
    "startTime": "01:30:56",
    "text": "and are you in influencing aspects of the Vienna performance itself to be considered so what what we consider as a generic benchmarking setup is that we have all these components basically it\u0027s a coordinator interfacing agents that are sending active probes to the system under test constitute a composite of execution environment on a vnf and where possible monitor components can be deployed inside the execution environment and also the vnf not of all of these components are mandatory in and we consider it also this these components can be aggregated in on only a single white or black box and we consider also that in describing it indirect that are the possibilities of these components influence the performance of the DNF itself and the test for the general description of the the draft Lacombe we consider the definition of two terms in the two to make it generic enough for other vnf benchmarking methodologies be a vnf benchmarking layout it\u0027s it\u0027s the structure and the function definition of parameters that compose the test setup that is going to define the the method of benchmarking the the DNF and the vnf performance profile is basically the extracted metrics that associate the target metrics that are extracted from the vnf after the test correlated with the benchmarking layout ideally we see that with the benchmarking layout specification and the configuration definitions that are also specified in the graph it\u0027s possible for a user or any other person that has the skeleton of hardware and software components deployed is discussed and it\u0027s possible to reproduce all the deployment scenario and repeat the experiments define it by them the procedures itself or there define it in the in the draft it\u0027s just an initial proposal we have a lot of work to do in that we are seen from the the work done so far and here we end up do you have how you can accept specify more and more details about the procedures so we didn\u0027t come with an orthodox proposal for procedures were more open-minded to see how the community thinks about it but basically those for the deployment and see the expression of the of the features step by step but and the testing procedures I think this terminology here is align it with the the DEA\u0027s own proposal and see we have a trial that\u0027s basically a one iteration to extract a single singular measurement from the net which markings matrix we have a test that "
  },
  {
    "startTime": "01:33:56",
    "text": "defines the particular components to define this this trials so one task and then fight multiple trials and we have a method that is basically a set of parameters that can compose a range of parameters for for for multiple configurations and define various tests and we consider that we must define particular cases for as defined in the RFC 81 72 and methodologies for those cases as well currently they\u0027re just specified some considerations that we must take in for these cases for the noise behavior what each component in the generic setup would take actions for how it would react how the matrix would be influenced by that and we think that I need an additional item here would be a white box here in that particular case where possible considerations of internal vnf instrumentation might be defined consider the VNS benchmarking report I think it\u0027s important here to specify we don\u0027t have a particular I think people in the in the research between the academia are very interested in seeing a report format of metrics how would we define it but from our perspective we just understand that benchmarking report for enf would consider the the function a structure in the function parameters that were defined in the benchmarking layout we also define that it must aim the statistical significance of the trials and iteration of over many tests as was defined by all in the right back draft in virtualization scenarios we don\u0027t have strict boundaries of the DNF we don\u0027t have strict education environment that different behaviors like one one change in the line of code can change the whole iteration of how the vnf works so we need to repeat experiments many times so we we aim to see statistics in different ways to find it in these reports we\u0027re actually looking how we could post some metrics like definition of possible metrics and even the specification of outliers in today\u0027s this this report this is a research that is being done and we we think that also the performance profile must be associated with the three by three matrix coverage by the VM Doug G so we have a open source reference implementation for that there are two publications the first one is the definition of the framework itself there the the PDF contained this presentation contains links further the papers and the first "
  },
  {
    "startTime": "01:36:58",
    "text": "one defines how this frame ecology was define it coded and the design principles define it there we aim to be to have a way of specific specifying a framework that is lightweight that we can have comparable metrics repeatable experiments and can be fully configurable to execute experiments so we think this this framework can be a reference implementation that realized that this vnf make mark methodologies in the document currently I\u0027m reviewing the the code we got because this was developing a partnership with a company and we got the open-source approval to release the code open source so now I\u0027m documenting and refactoring the code to release it but we think it will be released by the second half of this year so the idea is that the draft and the and and the open source code walk side by side this is I think one of the interest in the in the working group as well in the ITF and we have a lot of work to do we know that we came here as a open minded view of what is a DNA benchmarking methodology we have open source running you have reference implementation for there that we consider it we think the draft can be a common grout for for DNF niche market methodologies what we the list that things that we need to do are much bigger than what you have done and basically I think we need to refine the scope to see if we consider white box ENS as well I think this is something that we need to discuss we need to assert the terminology considering if the draft is considering a proper terminology are taking most of it from the Etsy and Nephi we consider also is a exemplifying the benchmarking procedures and parameters or maybe referencing the open source code or maybe itself in the in the drafting as generic a generic format we are going to explain that for sure explain each particular case as a subsection defining the objectives the procedures and the reporting the possible reporting format for that and also the the definitions of the report well we\u0027re done we are doing research to see what\u0027s the best approach to have a report format or not how it would be beaut we need to adjust the draft which are in conformance FRC 2119 to define what are the most the cans the shoots of "
  },
  {
    "startTime": "01:40:00",
    "text": "it and possibly we think that in the future we can have a liaison statement to attend a fee for this approach so I think this is all and we would like to see your comments suggestions critics we are more than welcome Shannon thank you all Thank You Raphael if before we start with the questions let me ask where are the blue sheet where is the blue sheet has anyone not signed the blue sheet there\u0027s a gentleman in the back there right good okay so that\u0027s good have to keep track of that administrative thing all right so let me open the floor to questions for Rafael anybody had had a chance to read the draft or any of the publications well I took a look at one of your publications Rafael and I\u0027ll quickly try to get back to that here let\u0027s see probably easiest go by it already yes so I I looked at taking the open V switch to the gym and thank you and I I saw we didn\u0027t get to this level of detail today but I saw a few places where I think I think some of the some of the models if you will for the descriptions of parameters and so forth there were there were frame sizes associated with you know one of the one of the models for the V switch for example I and I thought to myself that the frame sizes are really a traffic generator parameter and and in fact the V switch the visa the V switch should be surprised by the frame sizes that are in use it shouldn\u0027t have any any configuration that relates to that so just as a as a general comment for folks who haven\u0027t you know read read your draft or the publications I I think that there may be some opportunities to to kind of better coordinate what parameters are used with with each entity in your in your test setup and and also I thought it was I mean I thought it was kind of interesting that this is about vnf benchmark but strictly speaking the virtual switch is part of the infrastructure and the vnfs sit of above that in a in a typical etsy diagram right so so so in that way it\u0027s a I mean it\u0027s a good place to start but but don\u0027t don\u0027t expect to keep being able to benchmark these switches with yes I totally agree and the the thing "
  },
  {
    "startTime": "01:43:00",
    "text": "about putting open the switch there is like this the second publication was more of testing the automation of the the framework itself of the gym that the source code and how we the perspectives of automating the the benchmarking tests I know open when the switch is as you said is use it mostly as an infrastructure element we know that there are certain possible ways of using openness which is also as a part of a vnf itself as one of the early definitions of what is a vnf one of those those projects and yeah and we are considering the parameter definitions III I agree this is this it was just an initial proposal yet we are actually it\u0027s a one of the ideas of the framework needs to make it fully configurable so the configuration of the frame for example the frame size is where we\u0027re mostly for the agent agent part of the the framework how it would Express the infrastructure and later on have a reference to get that and as a result and correlate that in our in a database hmm I can show you the how it works later again but let\u0027s plan let\u0027s plan to get together one on one and some of those details so we have a proposal of a demo late this year probably they\u0027re trying to put it at sitcom but let I can show you the the code and how it\u0027s run actually I\u0027m finishing up HDTV so um short on time to make this code happen like by the end of the high peerage I haven\u0027t read it yet I haven\u0027t caught up this has been around for a little while and I haven\u0027t I\u0027d seen it a year or two ago I think but I\u0027ll catch up I promise but just to things you\u0027re so involved in open doing open source with this have you considered working with Opie nav they\u0027re pretty good testing outfit for for stuff like this and I know they\u0027re looking into vnf testing and they\u0027d be interested in this as a project so firstly second thing any any liaison AIT an early test working group will get attention I guarantee you okay for the the first one about Opie NFV when we are developing the when I was developing the source code I check it order the project inside like yardstick and we switch birth and well what I saw that is that there was even another framework called thought for for automation of test and what I saw that I could make it happen "
  },
  {
    "startTime": "01:46:01",
    "text": "triopia nappy but I saw it mostly attached it somehow if OpenStack and and the orchestration and configuration would were kind of like a quite attach it with each other so in this framework III tried to to make the configuration of the the benchmarking setup itself totally independent from the restoration of the elements and the setup of the environment that\u0027s what one of the reasons I put it outside up up in FDIC totally the the initial proposal of the of the the code was to be a benchmarking as a service so it provides interface the API that an orchestration or any open FP testing function could make use of it this is the the main target of the source code as well and for the liaison yeah we\u0027re open minded for for any ideas all right well I think I think what we would what we\u0027d want to do is is sort of get a good understanding of the work in your draft that means getting people in the community to review it and then we\u0027ll consider sort of writing a liaison to Etsy on this specific topic we\u0027ve got later in the agenda a response to Etsy that we would need to consider here and that\u0027s a different is it I mean it\u0027s a slightly different thing but we might be able to put the two together on or not we\u0027ll see just for efficiency so one more question what kind of open source code have you published and are you planning to publish the the open source code is is not published yet it\u0027s going to be published by the the second half of this year and it\u0027s a full framework develop it in Python the first publication here I can show you later it says how the framework is define it the specification the design issues and how we benchmark it V a V RMS of project work Clearwater so yeah it\u0027s it\u0027s stable it\u0027s there are many components inside and the idea is that we have generic ways of interfacing progress progress in the way that tools that stress and send the stimulus for for the vnf the traffic stimulus in a most generic way possible and we extract those metrics and we feel it that correlated with the structural definition of the benchmarking setup and we are using now elasticsearch and then "
  },
  {
    "startTime": "01:49:02",
    "text": "Cabana for for visualization of the tools of the matrix and then yeah and all the messages the is totally component is a micro service the idea is that we have all the messages as rest rest we have a REST API and full flexible messages among the components programmable messages yeah I\u0027m happy to share the code as soon as possible but and also to explain it and share the publication\u0027s so you mentioned OPN efi before does the code depend on any OPN athena all the code is fully independent ok good Thanks and so I think I think I understood you to say rafael that when you saw heavy reliance on OpenStack in projects like yardstick and OPN Fe that was kind of a well that was kind of a negative for you guys you wanted you wanted to do the management yourselves and can control and configuration yourselves so that that kind of puts it in the same class of a project as V switch per project that at OPM ëthey and but I think that\u0027s that\u0027s interesting that that\u0027s like that sort of aligns it with testers or one as well where the we\u0027re just some mechanism was imagined you know over on the management side that would put the VNS together and and and set them up ready for for a characterization so that that\u0027s interesting you\u0027re basically taking taking that management role away from OpenStack but that allows you to get very explicitly what you want from from the system yeah we consider benchmarking Vienna\u0027s for example deployed using kubernetes OpenStack you know any network utilization platform that might be available very good very good we have another question please sir are you Tara yes someone call me something so what is a Brynner yet there is your patients there is ur people don\u0027t it\u0027s kind of library things and but is it you think about the Priya net also yeah you know parable all sodium some function with with this API is also the Vienna and in a separately is only thing is some I sort of we wrap your things that I that that\u0027s good the the P for runtime api is like just an example I have colleagues working with developing "
  },
  {
    "startTime": "01:52:02",
    "text": "basically a compiler before compiler and they\u0027re working also if implementation use case for that and I have a colleague for example working to develop to make a broadband network gateway where you have multiple DNS inside the vnf in Southwick let\u0027s say vnf components and I think this is truly possible the idea was just to put but an example what what what event F can be how V\u0026F can be abstracted and we are seeing this closer now to this programmable packet program allowing dependent protocol pipelines like before and and the idea is that we are also having benchmarking scenarios with before I say vnf like before define about before programs and now we see that this can be changed at runtime I mean the pipeline\u0027s themselves so this exemplifies as a DNF can be abstracted and how we can consider it inside the draft itself all right any more questions I guess not well thanks very much for a feel for this excellent presentation thank you much appreciated let\u0027s see let\u0027s get rid of that and now we\u0027re back to this and so I don\u0027t see Samuel on the on the list but a list of remote participants but he\u0027s got a draft on benchmarking network virtualization platforms it kind of follows on from the the work that Jacob rap who\u0027s his co-author did on data center benchmarking that was with the Lucian Nemerov and and some others so um I encourage people to take a look at that we\u0027re not going to hear about it today and I\u0027m gonna quickly bash the agenda here my my my my inkling is that the most important of these next two topics is reach our Turing BMW G will get to the liaison after we go through that but my my thought is that we probably ought to consider the retiring discussion like right now so that\u0027s uh so let\u0027s do that let\u0027s see so where is the Charter text hmm I think it may be oh boy this is gonna be a challenge isn\u0027t it wait a minute wait a minute let\u0027s see if it\u0027s this one the "
  },
  {
    "startTime": "01:55:14",
    "text": "milestones yes this has the milestones all right I think I\u0027ve got it here so this is let me make that I\u0027ll make this bigger as big as I can make it I didn\u0027t help let me make this bigger as big as I can make it where\u0027s the where\u0027s the big text button here here we go [Music] come on I know there\u0027s a way to do this oh maybe it\u0027s zoom so yeah zoom in and zoom out we\u0027ll do this okay let\u0027s yeah that that\u0027s getting better let\u0027s try that one more all right here we go so we\u0027ve retorted this this group a number of times our Charter has always been tweaked a little bit as we perform this process and the main ways in which we\u0027ve done that this time are mostly included right in this paragraph here so just for highlighting it the the scope of BMW G has been extended to develop the methods for virtual network functions and their unique supporting infrastructure such as Sdn controllers and V switches and this is a this is a part of the the Charter which we basically took up as an as a work item last time saying that we first had to fulfill getting agreement on a considerations draft which we did and then we went on to talk about the considerations for V switch benchmarking and got that approved and now we\u0027re on the verge actually of fulfilling one of the other infrastructure method drafts which is on Sdn controllers so we\u0027re we\u0027re we\u0027re clearly doing this and now I I want to be sure that we\u0027re looking at benchmarks for platform capacity and performance characteristics of virtual routers firewalls and other security functions because that\u0027s being proposed signaling control gateways and other forms of gateways are included benchmarks will foster comparison between physical and virtual network functions and also cover unique features of network function virtualization systems also with the emergence of virtualized test systems specification "
  },
  {
    "startTime": "01:58:15",
    "text": "for a test system calibration are also in scope now i had a little discussion with one of our new folks here Doug during the week which lines up very closely with this Doug I think and I see Doug nod nodding so we may get some new proposals right along these lines and and that would be good so then the the rest of the Charter is pretty much as it\u0027s always been you know each recommendation will describe a class of network function and and a set of metrics that aid in the description of those performance characteristics that we decide on our benchmarks the set of relevant benchmarks will be developed with input from the community of users and we\u0027re definitely going to do that we we could we could stand to do a little more outreach in that area at at the various operator groups but will will ask that folks do that in the fullness of time and we\u0027re distinguished from other initiatives in the ITF because we\u0027re characterization of these technologies in the lab environment and that clearly allows us to do more than than what can folks can do on the production network and we\u0027re striving for vendor independence and universal applicability to a different a given technology class demands of a particular technology may vary from deployment to deployment so a specific non goal of the working group is to define acceptance criteria or performance requirements so it\u0027s there and it\u0027s been there for actually a long time I have to think of some different words there but I think that\u0027s I mean we\u0027ve we\u0027ve always had zero as a righty RIA for RFC 2544 throughput so it\u0027s a it\u0027s a matter of expanding that correctly and then provide a forum for development of advanced measurement techniques with insight from the operator communities okay any comments on the text of the Charter we\u0027ve reviewed this on the on the mailing list several times we have discussed it at the interim meeting and at the meeting in Singapore I\u0027m now asking for any final comments okay we have one where do you find this new welcome Doug as a new person you always say your name at the microphone when you don\u0027t come I\u0027m just curious where it is that I like to comment on it I want to read it okay okay well that\u0027s that\u0027s fine I have sent it to the list in a message "
  },
  {
    "startTime": "02:01:15",
    "text": "that I\u0027m sure I can find a link for but if we have a Kenny come if we make any comments today we will I will be resending this text to the list and and and it\u0027s possible that with our area director at the microphone we may be making some changes right now Lauren Kumari so I fully agree with the last sentence I D BMW G should be blah blah blah right but do we think that that\u0027s very likely to happen and if not should we just drop it it\u0027s true but I I think I think we should continue to encourage it and I almost had a chance to go to to an an odd meeting I think it was last September but it ended up the reason I wanted to go was to get input that I could kind deliver to the quick interim meeting that was taking place and it turned out they were the same days so I do have some intention at times to kind of try to follow through on what we talked about in in getting this user feedback and and and I think it it also it also seats us very well in the in the Operations Directorate given that we\u0027re we\u0027re looking for input from these organizations so I don\u0027t want to lose that connection either that that really the benchmarking ought to be as much as we can driven by the community of users who are going to use the benchmarks alright let\u0027s talk about the so here\u0027s one place where we\u0027re gonna make some modifications perhaps in the proposed milestones and and I look to I looked at Karsten here I think given the given the working group meeting in midst in mid-july and area director review that could potentially follow that for a couple of weeks by the time we get this to last call and iesg review I\u0027m thinking August 2018 is probably the the more realistic deadline for this methodology for next generation firewall benchmark see I see Karsten nodding so that\u0027s good so so let\u0027s note the other milestones that we\u0027re proposing here the update to twenty five forty four back-to-back benchmarking we saw a presentation on that today methodology for evpn benchmarking saw a presentation on that today we\u0027ve got considerations for benchmarking network virtualization platforms this is um when we didn\u0027t see today but what for which there is an active draft the network service layer "
  },
  {
    "startTime": "02:04:17",
    "text": "models and I said automated vmf benchmarking rafael is is that a reasonable sort of description for your Jim open source tool and I mean or maybe you\u0027d like to edit the wording here a little bit and please step to the mic if you wait no sir I think it fits the the current air draft but I think also we couldn\u0027t make it a little bit more generic because I think we can add automation an automation as a consideration into the current draft and as a consideration and a recommendation and possible so as a reference for the open source code yeah I would like the VMF benchmarking only if it\u0027s possible to make it more generic and the part of the automation would come together with it okay so just so just in general so general vnf benchmarking okay so we have made some changes here and that\u0027s the last milestone that I\u0027m proposing with this initial reach are during mr. Lynch oh yes let\u0027s go that\u0027s my that\u0027s my dad Pierre just a question the test oh nine work that\u0027s being done since it\u0027s it\u0027s kind of positioned as an update to 2544 for virtual platforms is there any intention to bring it back here well my thought on that is is this our scope in and we\u0027ll talk about this this specification in a minute for those who are wondering the background is that it\u0027s it\u0027s benchmarking for the NF VI and I think a 2544 update ought to include not just the network virtual the NF e aí but the world of physical and virtual and so that\u0027s a that\u0027s a that\u0027s a bigger update work which we\u0027re currently proceeding on if you look back at our last five or six years of history we\u0027ve been updating RFC 25:44 sort of a section at a time and I think we ought to have an update in the future near future that pulls all of that together it will pull in consideration from test zero zero nine once we obtain agreement on that and share it with the I mean actually it\u0027s shared all the drafts are shared with the world right now so but I think I\u0027d like to take that up under the same scope which is benchmarking for you "
  },
  {
    "startTime": "02:07:18",
    "text": "know infrastructure devices in the network in general fair enough I agree but you don\u0027t want to stick it in here stigma is one of those we don\u0027t have an active draft on it so that is a requirement for one of these miles I\u0027m making that a requirement at this point we can we can I mean what we need is a set of solid milestones that we can point to a draft and get this charter approved we can discuss we can have the rest of this discussion we just had once we get there this this has started come on man this has started to happen in the latter half of the meeting here and I wonder if it if it\u0027s somehow oh look at that there\u0027s a nice new island this is a this is a new Windows 10 machine and I\u0027m I\u0027m actually can you send a screen check for the minute you yeah sure I I\u0027m absolutely astounded by the photography that that they that they bring up just beautiful it\u0027s almost almost distracting from work and so so that\u0027s what I mean that\u0027s a good point that at some point in the future the 25:44 update has to come of course I look forward to those extensive discussions anyway I agree I mean it needs to happen so one comment on this general DNF benchmarking so if we substitute vnf by P and you know would this have any chance to have a draft on the general benchmarking for all physical functions network functions out there you know routers switches fixed network devices bng normal quorum IMS you know anything that we can imagine in our world would you agree to have a draft on one standard that says it covers all it doesn\u0027t make sense from my point of view and only you know the actually the selection of like this election of like these which Class B IMS to I mean it I have all respect for your work but I think it does not belong in one in one draft it\u0027s like to have an ice cream shop that also sales philosopher he doesn\u0027t make sense you know and and Kirsten that\u0027s it that\u0027s a it\u0027s a it\u0027s an excellent point which I sort of mulling over myself and I think now that I think back to it it was why I sort of inserted the word automated there as a as a as a possible distinguishing attribute of the work that Raphael and his team are approaching here so i i i i really resonate with the idea that that you you "
  },
  {
    "startTime": "02:10:19",
    "text": "cannot write a single draft that includes all the relevant metrics for all the different pmf sand vnfs in the world and get all those metrics right and and and useful i mean that we have typically done that on a per technology basis so how do we distinguish this milestone and make it make it make it worthwhile make it something that you consider in the scope of your work but not throw the but but not but not make it a boil the ocean kind of milestone i think thanks for the comment these were the kind of comments that i was waiting to see and the idea i mean the initial idea of the drive and it\u0027s open reviews of course it\u0027s that we could make a ground foundation for different types of ENS and with a generic framework to address benchmarks in generic reenacts I know that of course different types of your nerves have different metrics and different procedures targets than them and reports I don\u0027t know how to put this but automation is the the fundamental part of the work of course and I\u0027m also the running code but the idea is to be to express generic procedure is generic considerations generics attack and other benchmarking methodology specifications like VMs or open this which could come and address based on these procedures generic procedures of the generic vinovich mic methodology and the definition of the generic set up there is a specific case for that where certain conditions were addressed certain configurations certain procedures and certain target metrics or extract that that was my my initial idea but I\u0027m open for for discussions about that I understand where the it sounds vague the general nature argument it\u0027s up for the group so what does this help at all to add automation not as the first adjective but but kind of a follow on that does that sort of fit your goal I think snow globe for for the purpose of following the the the chart of the group yeah Karsten you\u0027re thinking about it is there any way we can improve this that would help you okay all right oh right well we can refine this but let\u0027s let\u0027s let me ask the room any further comments on the text of the "
  },
  {
    "startTime": "02:13:20",
    "text": "Charter or the milestones as currently proposed I I feel I feel like I I guess I could ask for a hum that that\u0027s a that\u0027s a reasonable thing to do for for the folks in the room come on man who is this let\u0027s see oh this is sue Dean all right sir Dean I think we can hear you now go ahead uh could you please explain that I didn\u0027t hear the world can you hear me now yes but the word that\u0027s something like Network element I didn\u0027t yeah um you mentioned that we chattering to incorporate the network infrastructure network infrastructure yes that they\u0027re nesting that that\u0027s up here the the scope of BMW G has been extended to develop methods for virtual network functions and their unique supporting infrastructure okay Bess it involves the service provider infrastructure I I missed the first part of what he said sorry III what I was mentioning is service provider infrastructure whether it a it will include that of course of course okay thank you thanks a lot yes thank you sorry I\u0027m just missing a word here there for some reason today no problem yeah connection is problem thank you yeah thank you thanks a lot Doug I\u0027m wondering in in the second paragraph where you mentioned the new scope should services be mentioned in there or is it implied from the first paragraph ah where would you insert it and I\u0027m just assuming in these Sdn networks you want to test the services across them so one of the problems is that this this terminology of network service has been a real mess in in Etsy nfe they used it to mean something which no service provider would ever have agreed to and yet somehow it got approved there so I think it\u0027s just the terminology is just too ambiguous to throw it down now now we\u0027re gonna have to tackle the problem a little bit in this in this network service layer abstract modeling mm-hmm but I I don\u0027t "
  },
  {
    "startTime": "02:16:23",
    "text": "think I want to message this in these are mentioned in the first paragraph is there a definition for it up there for Network Devices systems and services yeah but I mean they\u0027re there it could be anything like like like TCP providing a reliable byte stream service to the layer above you know it\u0027s a it\u0027s a more generic thing my baby Warren has a clarification for us yeah Orin Kumari so in general people haven\u0027t really been paying that strict attention to the BMW G Charter and the document comes out and it kind of looks BMW GS and everything okay looks like a BMW G document right so I don\u0027t really think that having it or not having it was gonna change things I don\u0027t think that you know if they were a services document whatever that means um and it wasn\u0027t explicitly listed in where would say no you can\u0027t work on that so I don\u0027t think it\u0027s worth the worth the bath good thanks yeah okay so I think we\u0027ve had a decent discussion of this especially the new text so let me ask the group I\u0027m gonna have three opportunities to hum it should be mutually exclusive you only have hum in one of the three categories there\u0027ll be a hum for support for the new charter humm if you object to the new charter and a hum if you have no opinion whatsoever all right we\u0027ll start with the first one please hum now if you support and agree with the new charter I hear some noise [Laughter] [Music] please hum if you object please hum if you have no opinion all right so what I heard there was support for the support for the Charter no objections and and no abstentions thank you very much all right so we will of course ratify this on the list with the changes we made I\u0027ll probably give about a week for comments but what will take care of that in in the fullness of time thank you very much for your time on that very important step so really quickly in the last 12 minutes we have a oh this\u0027ll do yes here here we go so we have we have a liaison from Etsy nfe and it\u0027s on the specification with this tremendously long title specification for of networking benchmarks and measurement methods for the NF VI and that\u0027s a network function virtualization infrastructure so they\u0027re currently up to version zero six of this work and it\u0027s coming from the Etsy n Fe testing "
  },
  {
    "startTime": "02:19:23",
    "text": "and open source working group which our colleague with us today Pierre Lynch chairs thank you for joining us Pierre so the background is that obviously 2544 is quite old can vote and drive in any state and the demands for benchmarking have really grown a lot and really the last four years it\u0027s become exponentially important again it\u0027s time to look at the specific problems for benchmarking again Fei and for a normative specification we\u0027ve started to write normative specifications in etsy nfe and this is one the idea is that consistency and repeatability are critical so here\u0027s the the table of contents with the topics that are most important flagged on the left here we\u0027ve got a general framework for benchmark definitions it\u0027s kind of like the template we use in our definition documents we we go into great detail on the test setup some configuration test device and function capabilities so now here\u0027s where we\u0027re asking the vendors here\u0027s some new there\u0027s some new capabilities we want this is this is easy place for them to find that traffic generator receiver requirements and the general functional requirements so we actually have two throughput we have two benchmarks in there on throughput and latency leaving the last meeting we agreed to write benchmarks on packet delay variation and that\u0027ll take two forms and that\u0027ll be a new benchmark and there\u0027s also going to be a benchmark on packet loss just the the classic measurement of loss ratio so there\u0027s going to be at least two more benchmarks in this and the methods of measurement section we\u0027ve got a great deal of detail to organize the hierarchy of testing where a trial is like a single measurement under the test conditions when you\u0027re seeking a goal like the binary search we talked about then you\u0027re going to run a series of trials within a test if you if you repeat tests then we have sets of tests so that\u0027s the set hierarchy which wasn\u0027t in your list Raphael and then we have a method above that the method changes significant parameters for the the lower levels of the hierarchy to examine like frame size and protocols encapsulations the things of that nature so that\u0027s the hierarchy that we\u0027re that we\u0027re working with in the methods of measurement and and what approach should have said first but it\u0027s worthwhile saying now we started out this work in Etsy with a survey of the current benchmarking published results the test campaigns where people had learned a lot of things and we put down the key learnings and we talked about how we would address them in the document and we\u0027ve been working "
  },
  {
    "startTime": "02:22:23",
    "text": "through all of these sections looking for opportunities to address these issues as they\u0027ve come up so that\u0027s a that that\u0027s sort of the the way this has worked so the known issues specification gaps are our search algorithms and so you know here again I\u0027ve this is on my mind so I was talking about it before but we have to do some better a little better specification of this after we\u0027ve talked about it some more and we also need some sort of automated way to monitor the infrastructure this is currently mentioned during benchmarking and this is for example in in Oh P and V right now we\u0027ve got one of the daemons collecti that runs and it can collect CPU utilization and some of the other platform metrics while the benchmarking is taking place of course that you have to test to be sure that your platform collection isn\u0027t influencing the results so you have to rerun it without that that\u0027s a form of calibration and here\u0027s one of the tool gaps we\u0027d love to have automated collection software and of the for the software and hardware configuration in all the meaningful dimensions if we could pull that out of a platform and then make intelligent comparisons with that same information run from another platform then we can easily see the differences and what might be significant to explain different results when we see them and and and so that\u0027s a that\u0027s kind of a Holy Grail that we need to fulfill but we\u0027ve identified that in this draft and this this picture that\u0027s impossible to see is a flow diagram that basically talks about the this is the trial procedure over here which feeds into the test and feeds into the sets and the methods and so forth so that\u0027s the it\u0027s basically what\u0027s going on with this draft so my that\u0027s a really quick look but what I\u0027m encouraging people to do in our community is to is to read this through let\u0027s and and also let\u0027s not take up any overlapping work with this effort at the moment let us let us contribute to it let us take benefit from it and then it\u0027s sometime in the future when we\u0027re looking at updating RFC 2544 let\u0027s take what we\u0027ve learned that\u0027s applicable to the the entire universe of benchmarking and and run that so any questions or comments on on the liaison none all right well I think I\u0027m gonna set a date for comments and let me simply make it one month so one month "
  },
  {
    "startTime": "02:25:25",
    "text": "and that\u0027s an action of a message to the list to set that because we\u0027ve gone through this a couple of times if if we\u0027re if we\u0027re not going to say anything let\u0027s let\u0027s decide that very good okay so the the last item on the on the agenda here is let me get rid of this one or it\u0027s not agenda where\u0027s our agenda Oh way back here okay last I\u0027m not on the agenda is is any other business any other business anybody thinking about doing some work in this world that they would like to tell us about for a few moments everybody signed the blue sheet good thank you all right well I I thank all our remote participants while they can still hear us I thank you all for participating today and I\u0027m declaring this session closed which means now we can talk amongst ourselves about things that we were afraid to come to the microphone to talk about thanks everybody we\u0027ll see you in Montreal and on the list very good oh well I let me put the times in real quick which is real easy to do thank you I always like to thanks I always like to let\u0027s say says this was 932 and it\u0027s now 11 okay 56 there we go and it\u0027s 17 the red Thank You Warren the the red button the red button does not cause anything to explode I used it when when we when we had late when we head the phone we had the pac-man show up in the queue over here was Sue Dean asking to make a comment I have to press the button to allow him into the into the microphone so he can speak so it basically basically promotes that they\u0027re like oh yeah they did it they did a great job finding the equipment for that [Laughter] absolutely so let\u0027s note-takers let\u0027s get a let\u0027s get a screenshot of this before we lose it that\u0027s right that\u0027s right now [Music] "
  },
  {
    "startTime": "02:29:37",
    "text": "that discusses that RFC absolutely absolutely this is this is this is the home of RFC 25 two or three years ago and the team of guys he lived about okay than those but they came up with 1242 and actually originally RFC 1940 it was updated to twenty five point or like Scott 25:44 its frames is there an equivalent well it has always been used for both frameless entities the only the only RFC\u0027s that that concentrate on layer 2 and frames are 2888 equipments are 2889 the full mesh land switching RC and the terminology which goes with it which is 21:27 I gotta deal with a lot of customers in there they\u0027re big oh yeah and I got Anderson bringing in 1564 is better than any yeah yeah it takes it takes off where people were crying "
  }
]