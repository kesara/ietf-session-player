[
  {
    "startTime": "00:00:33",
    "text": "South, This is Ivy of Northwell under which, we have conduct our business. It is a standardization activities. I would like to remind you that, by participating You actively accept that all the contributions, that is everything you say, that, alright, become, part of our government records. If you are not familiar with the note, please check out your own Asia. Next slide, please. also, the right app contact guidelines, And, asking us, and so just think, be having, professional to our colleagues, please follow that and, be accordingly. Next slide. I guess 7 minute is familiar with the participation I have in these tools. Next slide. Yeah. I just want to save time. Yes. Please join the joint note taking Lou provided the link to the chat window as well. And, next slide, please Yeah. Before that, This is the 2nd section of, the dot net working group at this IPF And we had our first session on Monday, on wireless topics and also started a little bit on couple of contributions on our data plan."
  },
  {
    "startTime": "00:02:00",
    "text": "For this, session, the focus is on the enhanced, data plan after, updates on requirements, Texolome is a key. Aspect, of our discussions And our last interim, this year, was about the taxonomy. Thanks to the contributors. We had a really good Discussion, on a taxonomy. This slide and the following slide, there is actually some upsides from, David, but And, with the driving, this discussion, what, we expect, ultimately, we expect from the taxonomay to characterize the number of queuing proposals on the table. And if you go to the next slide there, please. So, we would like, to progress, somehow on selecting, which, yeah, the queuing mechanism of which proposal would be and that's not, queuing. And for that, we need the background understanding. That's what us only helps. And as we, concluded at the the interim I mentioned the the taxonomy draft on the table, is is a really good start is a beginning of the discussion and then test to the contributor's authors. We are getting an update on that one. Because I would, not spend more time, but press over to the next one, which is the updates on the scaling requirements. I dropped I think it would be talking to its office. Okay. Lines, which this button Okay. So this button goes forward with the works."
  },
  {
    "startTime": "00:04:00",
    "text": "Thank good morning, everyone. Calling from China Mobile, and this the requirements for scaling and then that's And the status, this document was updated from, version 04 to version, 5. According to the discussion around last meeting, which I mean, that, we're saved us several, comments before before, the meeting, but after the cutoff, so some of, say, differences are updated updates updates. Was, presented in last meeting. And, the changes are more about add some text to enhance the explanation of requirements. And, I think it it is, stable and close to the last call according to milestone, of the topology. So, the main updates are here. In section, 3.7 as a tax related to consider and and jeter bounce achievable, end to end latency bounds and, complexity in evaluation and the selection of succatable, then not queuing mechanisms. Does this section where this us a lot a lot of time. And, the final text was put put put by debit, by debit, and it was also presented in us meetings. And, in section, 3.8, we add some text about providing queuing solution with multiple levels of then that, capabilities, as the queuing mechanism the realm it was showed in the bigger, at at at@rowbelow. So it's not, separate requirements, but just some text to be added in the, text. And, for the section change the reference from, normative to informative"
  },
  {
    "startTime": "00:06:02",
    "text": "and, cracked some minor spelling IRIS so here a overview of technical comments, it's been about section 3 and the I think they are stable now. We may have some, had some discussions, more of discussion happened in requirement 4, comment, 6 through comment, 7, I think. So other requirements, having to call it so much, of the comments. So, Since most of the changes are presented in last meeting, we don't really talk, so much details in this meeting. And the next step is that just a how to coordinate with a draft taxonomy and to see if there's any husband of the ground draft. And we also hope more review before, Last call. Thank you. Any any comments? You Thank you. I am seeing no problems. Let's move on to the tax because I'm from. Good morning, everyone. I'm it. Data plane and excellent taxonomy. It is version 1. Actually, this draft was requested to be initiated by the chairs. To better understand the current solutions of the networking groups. So, I'm at the presentation. The interim meeting, on, on January this year, And during that interim meeting, we had some comments"
  },
  {
    "startTime": "00:08:00",
    "text": "And based on the comments, this version 1, has been produced. Okay? So let's go over the let's go let's go to see the overview of the draft. The purpose of the draft is to facilitate the understanding of the data plan solutions which are, currently suggested or can be suggested in the future. So this code is to provide the criteria for coal age data playing solutions, and to provide examples of each category, And the strengths and the limitations of the categorical are presented as well. And, according to the comments in the last meeting, in this version the suitability of the solutions, of the then that, Services, to the solutions that suitability aspect has been edit, However, the the candidate solutions currently being just the is not described in detail intentionally. So that's the auto this auto scope. Here, we in this draft, we use the term solution. But, isn't that a single functional entity? But a combination of, multiple data playing functional entities. Such as regulators, queues and schedulers. And those solutions can, distribute it over a network. For example, a regulator at the network edge, can be a part of the solution. Yeah. These the, these are the ones that you have to understand for the of And Before going into the main taxonomy. Like to introduce some of the solutions currently used in the"
  },
  {
    "startTime": "00:10:01",
    "text": "and the working group. And some other exam pools mentioned in the draft. The current solution of that net Ken can be of 4 types based on its origin. For example, many of the current solution is originated from CQF. They are As CQF, TCQF, and ECQF currently being, processed in the IEEE can be a part of the solution. Being proposing the networking group The second type is Fail queuing variant. Fair Caring is a very specific solution that has been So just from, 30 years ago, we have one variant of your king. It is called CSCORE. Another third type. The third type is time of a shape of where we have one such solution called TQF. And, finally, we have a EDF based solution. Interestingly in the networking group, that solution is called EDF. But it is different from the original ETF. So you have to be aware of that effect. So these are the 5 types are currently pre currently proposed in that solutions. And in the draft, some other examples are included as well. They are ATS, at deficit Round Rogen. So these are the These are those all the solutions mentioned in the trefs. And according to the comments made in the interim meeting in January, Yeah. Yeah. In the In the draft, we have also mentioned the DNS service category as well. This categorization is based on the"
  },
  {
    "startTime": "00:12:00",
    "text": "currently working group document, this one, requirements for reliable iOS industry or services. In bad documents, a service is cut is characterized Buy is latency requirements First side, and it's a period of the city. And the network scale. For example, The display information service defined in section 4.4 in Betreft, we can categorize it. You can characterize this service as loose lose latency bound. Larger burst, nonperiodic, and small scale service. So, based on this characterization, we can categorize services like that. Now this is the main part of the draft. How can you, categorize solutions? In this in this draft, there are 7 categorization or 7 taxonomies. You can see them. The taxonomy name and left of the table, I would I won't go into detail all the taxonomy and the categorization. Because it easily takes some time. You can take a look at the draft. However, I will just go, briefly over it. The 1st category, 1st taxonomy is about the performance characteristic. The name of the text item is the per hub dominant factor for end to end latency bound. Every solution can be categorized into 1 of 3. Let's call it category 1. Category 1 is make pack length over service rate. Category 2 is some. Some of my spec lengths over link capacity"
  },
  {
    "startTime": "00:14:01",
    "text": "The 3rd 3rd category is somewhat makes you a bird size of link capacity. So it may Be very confusing. But in terms of the value of that, category, The first and second categories are of Similar level. Are similar. Because the service rate is something that allocated to a single flow And the link capacity is over all the flows. So some of the maximum pack lengths overall in capacity is similar. The categorical loan value. However, the, category 3 value and be much larger. Second text along is periodicity. One can be categorized as periodic or nonperiodated. The 3rd category is network synchronization. It can be a solution. It can be pages in Kronos. Frequencies in Kronos or asynchronous. How can you define asynchronous? How can you define synchronous? A dead is kind of tricky. And in the draft. We suggest the criteria called NTIE. What is NTI? You can take a look at the draft because I don't have time to explained The 4th category is traffic granularity, how fine how, course A scheduler handles. The traffic. Is is the traffic granularity It can be flow level, flow aggregate level, or class level. So as you can see, the more finer the flow level solutions are very I can say a flow protecting or flow isolating kind of property property. However, the collectible solutions are much simpler. To implement"
  },
  {
    "startTime": "00:16:00",
    "text": "the sec the next taxonomy is about the work conserving manner. Solution can, A solution can Trust with packet, as soon as possible. Whenever it is possible, then it is worth considering. The local engineering solution does not send a packet even if it is available. So we can divide this too. And according to the target transmission time, whether, there is target transmission time or not. We can we can define solution to be on time or in time. On time solution have certain target transmission time or target transmission intro if a solution does not have such a target, It is in time. Finally, solutions can be defined by its service order, of the packets from different flows. How can you decide the service order? A package from different law. If it is rate based, then it is, categorized as rate based as well. And so on, there are, 3 other categories. So these are the tax enemies of draft is proposed this is the main part. From now on, I will try to give you some more information which is not described in the draft. So, from now on, I will Go faster. The 7 taxonomies you just sew. Seems Pedro and independent. But actually what happened is they have some rough intercorrelation. Profit dependence. For example, attractive granularity whether it's flow level or class level. And the pop up dominant factor for an independent later bound is somewhat related How how are they related?"
  },
  {
    "startTime": "00:18:01",
    "text": "For even for flow level solutions have a category 1 or category 2 and due to a latency bound factor. So l here is the maximum picky lengths of the flow. Small r is the service rate of the flow. And the signal, I mean, some some of the l over r. Large you are installing capacity. So there are 3 categories according to the dominant factor, these two cat these two texts are gonna be put into together like that. And as you can see, there are, many solutions data part of one of the categories. For example, c score, is flow level solution with this, latency bound characteristic. At seek variations. A class level and this Performance characteristic. However, in the interim meeting last time we had, we have a comment. From Nomura, Nomura Fin, the CQF and its variance can be category of 2. When? The burst is suppressed to a packet level at the networkedge. This is what happens in the SQL. That is what I've heard. And if that is the case, bird size becomes exactly the same as the measure on package size and these 2 cannot be distinguished. So the CKF invariance can be categorized into this This category, but anyhow, the Maine, principle of operation, It's not changed. The just the serve the performance level becomes the same. So this categorization still hoarse. And Another grouping of those tax enemies"
  },
  {
    "startTime": "00:20:00",
    "text": "second grouping of text enemies are more complex. They are, involved with it is 4 textonomy types. Actually, this can be quite rough, and I'm not I'm not so sure. I cannot assure you that this is the right figure because there are so many other aspect that has to be considered as well. But according to the current solutions being proposed in the net, and the additional examples, I suggested this is fairly, categorization. For example, work conserving based on the work conserving or not. This can be divided into 2. And if, one is worth conserving, then it has to be in time. It does you cannot have any. Particular target transmission time. On the other hand, no no contributing solution can be either in time or on time. And on time solutions, which will tag a transmission into a ball can the further, categorize it as a non periodic and period. And so on. The purity solution can be divided into 3 different, category. But all the currently suggested solutions intended. Is only frequency synchronize solutions, and they are all periodty. And they are all on time and no no conceiving. Arrow. Etcetera. So based on this taxonomic grouping, there are 4 different types of solutions currently being suggested. Finally, service order is quite independent from each and the current solutions are either rate based or time based. So this is the this is not the whole whole picture of the taxonomy, rather it is text enemy For the current solutions only,"
  },
  {
    "startTime": "00:22:00",
    "text": "Okay. So, yeah, I think this figure Very well categorized the current solutions. And then they walk in the road So, how these, categorization fits the current services being properly in the networking group This this taxonomy, this categorization for the the net service. Right? And this, this other part is about it and the solutions. They can be met roughly like that. The same color pair Match is well with each other. For example, if a service wants a type latency bound, it should it can be better with one of these 2 category. Or full level solution. If the service is strictly periodic, And the solution can be period. And so on. A lot if the service has larger side, Then flow level solution may fit well. And if the service and the service is for the largest scale network. Then a rate based or Quest level solution, may fit well. This is just a rough, rough match Not not the 100th person to crack. This is just a rough message. So I tried to summarize the strengths of each category with the with this, box with Pointing bus. Is it pointing bugs? Yeah. Anyway, from left to right, I will just simply explain what are these aspect. At the rate base solutions, fit well to large scale network. Why? Because the automation control is simple."
  },
  {
    "startTime": "00:24:01",
    "text": "Only the sum of the service rate of all the flows depends on the acceptance of the flow. They have decided the oxytons of the flow. So you can handle larger number of dynamic flow where the flows join and leave very pickly, Yeah. So rate base solution have dash rings. Wild time based solution. Has Simple and return latency calculation. Is very straightforward. And it is very intuitive. That's the strengths of the time based solution. The flow level solution The flow level solution can minimize the effect of other flows burst. Or its existence. So you can be called as flow isolation property. The flow level solution, protect or isolate a flow. From each other. Very well. So if a bird size is very large for for the services. Services. Then flow level solution fits well. The class level solution has a very obvious strengths, strengths, It's simplistic. It is very simple. Doesn't add it flows state. Main tenants. It fits you well to large scale. It is very well known fact, I guess. You may not very well, Understand. The work conserving scheme. So I'll take some time in in presenting the work and serving schedulers, strings. Its strength is the statistical multiplexing game. Which is the t. Strange of the internet, how it has survived. How how it has succeeded? Well, it has Much lower average rate."
  },
  {
    "startTime": "00:26:04",
    "text": "And it also has the maximum observed latency. So, Every solution can have its theoreticalmaximum pound. Maximum blade touch your pound. Regardless, let's hope that theoretical maximum bound. This work conducting solution can have much lower observable maximum latency. So it's, part of the strength. Another one is that because of its efficiency, The low priority traffic or the best effort traffic. Has much more room. To be served. This is the work engineering scheduler's, strengths. On the other hand, working on doing flex scheduler has, drew back or burst stock inhalation. The burst accumulation. The burst accumulation means the the maximum bird size of the same flow single flow increases over hops. That is called the 1st accumulation. Because of the burst of accumulation, what considering scheduler may 8 More buffer space. So the normal contributing schedulers have bad strengths, opposite strengths. It has less for required. And it has less teacher. And so on. Yep, and Periodic solutions naturally fits the periodic services as well. Okay. So I think this concludes the Call. The final one I'd let I'd like to show you is that then how this taxonomy can divide or distinguishes the solution is currently being proposed. Based on the flow level,"
  },
  {
    "startTime": "00:28:03",
    "text": "or while conserving or periodic synchronization, current current current solution being proposed can be categorized, into this one, None were conserving flow level solutions. And class level work on driving solutions. And PAS variants and CK variance They are slightly different. But anyhow, they can be considered this, period. Class level. Solutions. These are also, rough visualization. Okay. Thank you. Please take a look at the draft. And these are the references. I have we have used as a tech's enemy, the then the solution in SMS, solution. Are quite a few. Well, you can take a look. And also normal things e easy trip references also included Thank you. Any Question. Yes. You've got, shafu. Hi. Hi. This is the cell phone plan for the Janu, thank you for the presentation. Can you, turn to page, Yeah. Can you specify? Yeah. Yeah. Yes. Uh-uh. Oh, thank you. For the database, curriculum, may have different opinions that the, the advising control of Radapace resolution, I think that it invays a TL Relay and the customer source. It's a"
  },
  {
    "startTime": "00:30:04",
    "text": "They may need to manage the buffalo sauce I need to know that for war flows. I single can give an example at the Middle East Sankly. Thank you for the question. If I understood correctly, your question is that even rate based solutions Need to consider the buffer space. Yes. And, make sure we're size. Yes. It is it is partly true. If the If the viewing node, routers, or switches, have a limited amount of buffer then definitely, there has to be considered there as well. But in reality, The buffer space is quite Appundant. Most of cases, So we don't care much of the corporate spaces. When we Consider the automation decision But, you are right. Yeah. Buffer size also has to be considered. Yeah, thank you. Okay. Sounds good. I think that we we we have good quizzes can and this Thank you. Thank you. Hi, Torres. Thomas Eckard. Yeah. So just quickly on the, on the buffer space size, right? So that, that is actually an interesting thing, that let me we may wanna revisit in general. So if if we look at, you know, the, the lower end of, of forwarding plane chips in the high speed. There are 2 classes Right? So the ones with a lot of buffer, which, which are meant to be used for routers, and the one with a lot less buffer meant for switches. But, if you look at for example, metropolitan networks, right? Many of them, try to, you know, get away with just the low buffer, right, whether that is still too low for us for what we are trying to chief or not, I think that's actually an interesting question between these two different"
  },
  {
    "startTime": "00:32:02",
    "text": "type of devices, with a different size of buffer. But I I think that's kind of mental note for, to, to be investigated. Yeah. If the if if if our device is has very little over sizes, and I think That's the one possible scenario. Right? Well, in that case, we have to consider serial save at the popular size. But you have to remember that. In all the solutions currently being propositioned in their services, Has very low and to handle the attention power. Yeah. Exactly. Yeah. And in order to meet those and attend the late bound, The buffer side itself should not be very large. Yeah. Yeah. So these are, I think, trade off. No. think I for for somebody coming from the industry that doesn't want try to drag through all of our details here. It would just be good to to to know that whatever we need as buffer size would fit, for example, into these lower buffering, equipment and, you know, these type of applicability statements that we can get out maybe in every solution, we can specify the The request buffer side Yep. A kind of parameter that solution. That's a very grant. Thank you. What I wanted to get to is I think the, the thing that that might all be be useful to add is the older algorithms that so to some extent, they're here. ATS, I would say, is an older algorithm, not not well scalable, But for example, would it be fair to say that kind of cease score is what RFC 2212 inserve is doing just, you know, scalable. And so that that we try to have this, you know, even, even if we haven't considered them because we've think they're not scalable, that, that we have them in for comparison because then people can already know. This is roughly the same service I'm getting. Right. No. That's exactly it. Yeah. Thanks. That's a good idea too. Actually, the field king and C score. In line of integrity services,"
  },
  {
    "startTime": "00:34:01",
    "text": "started from very long time ago. So that's a very good idea. Thank you. Period. This transform from cities. Thank you for your hard work on the tailed category. My concern is that like, the page shown that there, it can be the treat treatment solutions can be calculated based on their performance. And the, for example, the latency a lost tolerance I I think, just like the this SLA requirements, but as your, you proposed the draft, their various exist in just their latency band, and there it are, it has category based on the per hopper dominated factor. So Right. It it's not just not under tight latency or loose latency? Is there any relationship with her the title or less, latency. That's that's a very good question. Actually, I think, personally, I think, many of the solutions we are being we are currently proposing Doesn't have explicit expression. For end to end latency bound. Near, near, However, we can roughly roughly figure out. What are the per hope. Behavior characteristic And based on the characteristic, The Probably latency bound should be and Yeah. These are the the basis of categorizing the solution industry part So Yes. There is no strict mathematical proof. That each solution fit into this category But, I think it is agreeable. One exception to that rule, Is the c score or for your king. Has explicit and to attend the latest you found express."
  },
  {
    "startTime": "00:36:01",
    "text": "So it is correct. It is very accurate. But other than that, it is a rough categorization. Okay. I suggest to, for to do more, Calvert calarification or the category based on the performance. To more That's more more, category based on the latency word jitter word the north. Mhmm. Like, you show in the in the nice, you I add some clarification on suggest you to that, that, Okay. Thank you. I remember that you suggested to include the Jitter performance as well. Yes. So you can do more. Yeah. I'll try my best. Okay. To include that aspect as well. Yeah. Thank you. Thank you. Hi, Dan Bogdanovic. So the Going back to the buffer. There are When it gets down to silicon, You have some of the other silicon where the buffer and the table sizes are pretty much fixed. And then they have some other incidences where you can play with that and saying I want more for the table's size or one more for the buffer size, and you have to do a trade off because you have a finite amount of resources on the dye that you can use. That's correct. There's always a trade off that you have to be willing to do, and then you have to decide will I do a trade off on the flow side and have some less flows. And I will be reprogramming the flows more. Where I will be using less buffers. doing the reprogramings of the flow lines. Right. Right. So this is the key trade of when you are And a system that you have to do and you have to be aware of how many resources are you using within the system. Thank you. For the comment. That's very accurate. Actually, the the solutions in in other, area, may have, flow state table and so on, but indebted."
  },
  {
    "startTime": "00:38:01",
    "text": "Dennis Solutions, usually cannot maintain the flow state. So I suspect that the full table or any other table type of information. That information amount, it should not be a very large. Yeah. But still, we have to consider the balance between 2 aspect and it has to be specified as well. Thank you. While Balaji is gonna ask these questions, we're gonna start pull asking, leading towards asking about adoption. But I wanna do that in parallel with the, the discussion. So blush? Okay. Balaji. I have a high level comment. So on on Monday, we had some passion about wireless friendly. Functionalities in order to provide, the latency bound. Of intent to have. So my high level comment would be that, it would be great to involve some aspect of of that wireless, special special cases in order to to ever read, these, queuing methods also from that perspective. Okay. Thank you very much. Yes. Right. Now our working group, collaborate the wireless effort and the wire effort altogether. But I admit their current solutions, basically concentrated on the wired part of Yeah. Wireless aspect has to be considered. In the future and from now on. Thank you. Thank you. Okay. We We had a little participation in the poll, not heavy. And, something like, 20%. A red,"
  },
  {
    "startTime": "00:40:02",
    "text": "so we have had some people that have read I'm interested if, we're interested in knowing who would think who thinks that this is a good foundation for our not that it's perfect. I actually have some technical comments myself. I'm wanting to change and clarify some of the terms but that doesn't change the question of, is this a good foundation is this a good starting point? Not end point, but starting point. I should have started the poll. Sorry. We found what I was stating. In terms of, my own comment, I'll we can work offline. I think the, service ordering because we also it could we use ordering in terms of pre auth. It'd be better to find a different term just to avoid ordering I think also some of the other terms are not as tightly defined. Example, tightened loose latency is not, I don't think we have a good tight definition of that. So I think we need to do that. But personally, I think We could do that after adoption. Okay. Thank you. I'll take the number. Yeah. I'll I'll take the I'll make a comment on the list too. So But as chair who's running the poll, I wanna just say look like we have some, pretty good support of the participants saying, yes, they think it's a good start. As always, we judge consensus on the list. So expect us to take this to the list. We're a little over, so I I'm gonna ask that the person who says no I'd really like to hear why they think no as part of the adoption pole. You can expressed us. Why you think it's not a good a starting point, that would be really appreciated. Thank you, and, and, Actually, thank you so much for the presentation. Thank you very much."
  },
  {
    "startTime": "00:42:00",
    "text": "Yeah. Hi. Good morning. This is the silver plan for number 30 I will present the the the the 9 booster for the magnesium. Next slide, please. So for normal course in 7 tonight, the updates as below, clarify the policies of the latency conversation, And the bus, the accommodation contacts according to the discussion in the managed And the developmental figure for each operation, especially options with, latency, you're determined to see, they have and, describe between implementation was served for to absorb a latency the e t integration or e t t covering. And, supplemental common topology example and at last stage, the some more me considerations. No slides. So this is the motivation. Why do we introduce EDF for not scanning requirements. But I will no longer provide, detailed description again. Next slide. Next slide, please. Thank you. So this, the first update this page here illustrates how latency conversations work for this conversation, with the help of Nathan We can always get the ID, web pattern. That is used by EDF scheduler to run the packets. No matter what is the shape of the real world pattern, 1st, a specific flow. The idea lawyer Patton, You're close to re re re"
  },
  {
    "startTime": "00:44:01",
    "text": "a lower pattern plus you need to business situation. And the debt. A deallower pattern offer nor the 8th. Equals to the ideal part of in interest load plusagemultiplee. Next slide. This is the second update. Generally, the 4 options that the EDF may take I I will not work so good to tell for each option In summary, The one And I love to say that is the traditional idea for based on traffic regulation. Why the one on the right side is the We'll commend you the EDF based on the latest conversation which is a termed as a CDF Make a salad. So this is the setup update for antimemode, it may anime, Eliminate the bused accommodation Make an but for design, more simple. Anthermal motor can be further implement the first is etintegration. Mhmm. The party is scheduled by the scheduler configured with anti mobile based bpluse. The end of winter to end the latency, it's intervention. The multiple hubs to the multiple hubs, plus the The 7 and most of the use, the EDD, the company, part of this is scheduled by together configured with anti mode based I e, And, 10, scheduled by the postal scheduler configured with the motor. Please The end to end, the latency is in the range of flow, but the multiple, hope is a minus 1, to the multiple Hobbs And Medicine 1, plus the"
  },
  {
    "startTime": "00:46:03",
    "text": "both 2 most of them may provide the cheater of the level buddy DI. Next slide. Yeah. So here we are Homan topology example, use the tool describe the services scanning deeply off to the network needs suppose the link speed is 100 GPS. The cheeseburg of Easyflow is that, target size is, 1000 biz. Quality panel wire. This is, 10. I'm a PPS. But the I'll speak of each flow is a different which is natural in the actual networks In this apology, we let the each flow face of full competitive flows at each hub, Next please. So this page shows the automated flaws for each delay level. It may support progressively 6,000, and 500 a day of dozen other floats in this example. There's no our provision. The admission check It's a know the best, but at the side, the per easy deliver were duration. And the provide the customer customize the probabilities the pohofer witness overflow is based on this RSP notebook, not it is a key spoke, to be the service department of Never select. According to criteria we define in tax enrollment document 39 mechanism is, non pruritic asynchronous collasalo What's the concern? And the nonlocal concern configurable, intangibles, intangibles, delay is the scheduling scheme."
  },
  {
    "startTime": "00:48:01",
    "text": "And that the probability tends to dominate the factor is then lower you the self For non periodic there's no definitive political qualification. You need to offer scheduling power. For a single illness, worked on enough routes, and, obviously, Okay. For class the philosophy is a global, the particular lowest, And the it can be configured in in time mode mode on animal mode, to to women's consumer or a novel algorithm for the delay based, the another flow is scheduled at the case I It's a it's what I need to be determined upon the virus. Next slide. So that is the word of this, question. Any questions are the comments sounded. Any questions? It's 12. No. Okay. Will continue to describe the tga from mechanism Yep. Next, please. When we're working for tools, 6, we have the following updates Canal application of some of the monthly relationship, Dup. BTM, BOM and FDM. And the run time of police and delayed due to elimination. And, common popularity, example, and, public and normal considerations. Negative slide. Yeah. For module arrangements, Moe,"
  },
  {
    "startTime": "00:50:01",
    "text": "most of the scanning mechanisms, have, conductives in between latency performance and the services in t as in TS, introduce a flowing Canadian investor based, GCLO. Lotation circle. But you need the time synchronization and the has a scattered pinity issues, and GSL calculation, application, and the installation. To me that, large scale requirements the years, to introduce a Thomas with the resources to an ASV. The related autonomous world scheduling, and doesn't let the play. We call it Tigra from the Next slide. And the credits, I need to to introduce time period, which is a termed us or choose a tuition period. Keep going for me, could you track the the time slot, the type of resources for each unit. Team. And, with the car put the original paging period of the good in repair. The later matches the architecture hardware capacity of the device. Requiring only a few logon will be used or a single pipe part calculation is based on time and obtain a flexible margin relationship between the incoming times would and outgoing times would, easily it shouldn't be noted that the the or choose a bridging period is not the only used management and the control plane we're also under the plane. Next, please. So this is the the BTM or BOOM and the FDM. For a given topology installed a listening in a boutique of and, located in this time slot, independently, and that they fixed the DTM or the BOM for a Unity log"
  },
  {
    "startTime": "00:52:01",
    "text": "between any AT and S that loads. PTM is at the log out going time slot map back into the remote ongoing time slot And the DUM is the richest region PL with the face difference between the local. Opportunity to improve at the total amount or change the tuition period. PTM or may be detected based on the pathology where we needed the fix the PTM or POM. A tenant flow may allocate the time slot resource I each node and obtain flexible FTMO. Doesn't need it is a latency requirement, The flag support of TMO. Achieve of low Internet. Yeah, there's an argument that that, following the network IP. I think it is harder to avoid the congestion and attach it to the notes. Next, Hello. So this is the Run time of policy and delay cheetah alibination. After the police and, the network entries load, There may be a one time activation between the ideal arrival type under the real arrival time at the schedule The run time at WACG is treated as a part of a policy because even the package may have different the policy debate. So that, Jitter occurs. The policy delay may be unlimited by the calculated the the point, the dumping delay and the egress node, end up point, the planting delay, You're close to the present a budget minus, hurt and the police intelligence. Next please. Yeah. Here we also describe the 7th scanning deeply of trigger mechanism. Based on the same methodology, for example, Next week."
  },
  {
    "startTime": "00:54:02",
    "text": "So this page here shows the animated flows for each time slot, we support the the world's arrival and the different reserve with the outgoing time slot. May, may, may, produce a different language of residents know the delay. It may support, 10,000, dozen other floats in this example. Yes, that there was a law over provision that is admission check is a note to not are passed off of law consum wall timeslot. And it provide Customize the buffalo the hope letters. The prohylatons of law is the best I need is rspoke. And to achieve with the by Thomas with the reservation, note that by it. It's, a textbook. Next, please. This is the taxonomic considerations. I okay. Yes. So according to the criteria defined in tax we document. Tiga from regulation. It's a peer redick fluocancy singleness, class level. Can consider, and then we'll go and consider configurable. In time, untime, mode, configurable. The time would be the screening Skie. Yeah, the it has to dominate the factory the time slot of the pilot. For PLADIC, firstly, there's a time appeal with a p containing multiple tennis loads. And then secondly, of low is assigned repeatedly to a particle set of time slots the for fluocuses and colonized Chris to frequency aligned to ensure that the all nodes have the same time on labs made, made, made, made, made, for class level. Don't let the flows."
  },
  {
    "startTime": "00:56:00",
    "text": "It's group by Thomasville, the ID. For work and conserve and non work or conserve and configurable the security, configured with yintang about car, as he will work on the gym, while you configure this randomly with the, it's a network non worker conservator. For Yintangmu computable. Scheduler may be configured with intangibles to all other animal thermostat technical role is a security based, I All the going comes through the what an either salute her with the bond of others. That that can't feed the world, the kids also, low bandwidth is consumption, but, urgent foods. Next please. Okay. That is the wall of situation. Somebody will, welcome to any questions that are coming Any questions? 13 now. Thank you for that. I think next up is Chanel. Thank you. The c score draft, version 2. In this revision, Not much have been revised. The category of the c score according to the taxonomy draft is addressed. And the strengths of the c score has been described. That's it. However, in this presentation, I will add some Explanation about the c score the better To help you better understand, With an example that starts from ECKF."
  },
  {
    "startTime": "00:58:02",
    "text": "So I don't have much time. So this is the one you just saw in the taxonomy draft. Presentation. Cisco office into rate based, flow level, l over r, category. As well as the work conserving, SKIN. So this one, you have also seen it. Nah. Just I'll give you a brief amount of time to look over it again. And from now on, I will describe the strengths of the c score. First, it has very simple audio control in. The automation and the reservation process depends only on the service rate to the flows. While others require to consider, the birth sizes of all the flows. So you can accommodate the lagging number flows. That joins and live dynamically. The second strength is fire a flow isolation capability. It can minimize the effect of other flows 1st, And one of their strengths, I have not mentioned in the draft, is that it has the explicit expression for the late late inch bound. That you are seeing now. In the upper part of the slide. You can take a look at the symbols on the right below side, you can have some time to get the expression of what the meaning of the expression. The important Some term here is is l over r. At the very end of the expression. This is the"
  },
  {
    "startTime": "01:00:01",
    "text": "Per Hope dominant factor. The end of 10 late stage bound. This l over r. So the whole category belongs here. So it provides a near perfect flow isolation why it called near? Because the elmex elmex is what? Make sure you unpack your links off maximum packet length at nodeh over all the flows. So it depends on out of flow. But other than that, every parameter here belongs to the flow itself. So the end of 10 to latency bound itself is very, flow flows own parameter, dependent. And according to that fact, the admission process is very simple. And so on. So if I do tell the latency bound is guaranteed to will upon this other mission, It alters minimally. As other flows join a leaf. That is the strengths of the c score. The 3rd strength is service rings is from its steptiscal Multiplacing. Gay, as I have just described in the previous, slide. If a small average latency, if a small observed, the maximum latency, Does more room to best ever traffic? Altogether, it can be called a statistical multiplexing game. About these, box plot. There are 4 solutions, presenting. The left 2 are called Jitter virtual clots, and a core jitter virtual clock. These 2 are, proposed by EONSTOKE a long time ago is very well known scheme but it is non work consuming stateless working. They these 4, including the virtual cloud,"
  },
  {
    "startTime": "01:02:01",
    "text": "And c score, They all have the same Maximum and 210 latency bound. In this parameter, it is 0.32 millisecond. You can see that or forced schemes meet. The theoretical measurement bound. Yeah. But she much lower than the like the they're gonna come up late touch about. More than that, the normal contributing workings schemes The averages are very high. These are the distribution of and turn the latency back. So the simulation has been repeated quiet some time quite many times and the distribution of end to end latency is, plotted here. The average latency the normal conserving schemes are very high. While the war conserving fir king's schemes has very low average and due to latency, And the maximum Maximum observed latency as well. So it certainly shows the strength of the maximum bird plexing cane. And this is why Internet has succeeded. Yeah. This is the major revision in the draft. From now on, I will try to make you understand better. Why this query is very, useful. Let's start with the example of, enhanced is enhanced the CQF. Which will which is being processed in the truth Lee. And I was told that this, scheme is quite stable now. This is called enhanced CQM. Similar to CQF, the time is divided into slots. But different slab lengths are all located that that 2 different classes, as you can see here. But the"
  },
  {
    "startTime": "01:04:00",
    "text": "the basic secret, end return related bound still applies. There is, Similar to or close to the hope count times the solar time. This is the end to end latency bound of basic CQF. So you can see here the higher the class The silver blanks is small. So end to end latency bound is differentiated. There's a whole idea of the eSic However, the how the solar lens is determined? This is the main this is the main problem. How you define the slow lanes? Roughly speaking, it depends on the maximum birth site the sum of maximum bird sizes of the flows in that class. And if the maximum bird site is suppressed equal to the package site, then the maximum package, some of the maximum package site is determines the slope lengths. However, that determined that determination has to be done in the most congested link in the Neto. So it has to be very large. Moreover, if, If a single class s log length is determined that way, It also affects to other classes, low length. So the slow length is very a rep unflexible, I'm out of saying. It is very rigid so when more flows are automated later, then all the slow lengths has to be increased. Or initial slow lens has to be large enough to accommodate the future flow of emissions. So I call it, it is flow interdependency. Flows are interdependent to each other. How can you solve it? For example, we can come up the flow level, ECF. That is not based on the class,"
  },
  {
    "startTime": "01:06:02",
    "text": "but based on the flow itself, the slow links are up assigned. The solar lens can be assigned for example, depending on their maximum packet length, and the service rate of the flow. If that happens, a single box area occurs to the maximum pack length. And how do you decide the service order. You can decide the earliest slot and the time first. So what whenever the slot ends, the old view, the slope ends, That flow will be served. You can serve it like that, then you can have full level e c by doing that, the end of latency bounced to the same. And even when more flows are automated, there's a no change in the slot time Thus, on alternate latency bound, you can achieve low isolation by doing the full However, there's a stool problem. That Bandwidth can be wasted because of the variable package size. Because the slow length has been assigned based on the mission So we can improve it even further By having variable sites lost scheduler, flow level, easy calf. So now the slow length is not fixed. But variable according to the keep it has to accommodate No AC Bandwidth. Similar and the 10 and the latency bound. So this is the The best you can do with a c cap, the e c cap type of the scheduler's Yeah. We're gonna have to, close soon. Could you Okay. Speed up a little. Yeah. This is the last slide. But we already have the solution. The solution's name is called"
  },
  {
    "startTime": "01:08:01",
    "text": "C score. It it is flow level, very precise slot scheduler, but it is even work conserving. That means even before this load and the time, the packet can be served. And it's, Therical maximum edit latency is similar. Slightly different, but because of the non preemption, can characteristic of the Cisco Thank you. That River question. Any questions? Shuffle. Hello. This is several plan for the city here. Yeah, Sanjay, for the presentation. I think maybe we can, submit the the common topology, basically, for example, to shoot the the the the the benefit of Cisco, I have 2 concerns the first thing is that, the hope of Nathan's the level of the population. So for example, it may be 10 megabytes or about 100 megabytes and, the second of consensus is that, can see, can see school provide the multiple Hubble date. Similar to multiple priority So, Risha, could you repeat the case again. Can you make it a little shorter? Yeah. Okay. I'm sorry. Terrible in English. The first, concern is that that, can see school, provide the, common apology example. To shield the benefit of, yeah, to show you the benefits. And the second consent is,"
  },
  {
    "startTime": "01:10:00",
    "text": "I I want to know if a Cisco can provide the Mighty Pool class lower yeah. Yeah. Oh, yeah. Yeah. Yeah. Yeah. Of course. Of course. The syringe. Okay. I can. We might have to take it to the list. Because we're Okay. Sorry. In in the short term, We should switch. in a short answer, I can say that the maximum and return the latency bound overflow can be adjusted Yeah. With any value. I'll just stick to any value by or just think the service to the floor So I have the answers to your question. And if you have a more question, please. Let let's take the discussion to the list. We're running into, controls, time. Thank you. Hello, everyone. My name is from each is honor to present it on time voting with pipe for QMESSA. There are 5 contents that are presented today. Then that is provide, bounded and attend the latency. If a maximum latency, maximum latency variance. Guaranteed it it It is called on time for the on time, faulting is critical feature in the industry. So we proposed a new method to that allowed the more accurate on time voting and more flexible flow setting. Our terminal motor separated and the tendency into the 2 component variable delay and fix the delay."
  },
  {
    "startTime": "01:12:00",
    "text": "Did you have a delay? Has it, offer and low bound? For you to input queuing delay, processing delay, and output delay. The for the in order to provide on time forwarding. Variable delay or must be must be smaller than maximum end to end the requirement minus It's fix the delay. And variable delay low bound, must be bigger than minimum latency requirement minus fix the delay. So We assume the variable delay upper and low bound is divided and control or assign the node to delay upper and low bound. And then when a packet arrive at the time t, you know, to calculate the minimum departure time and maximum departure time. Minimum departure time is calculated calculated to buy whatever time tplus know, to delay low bound and minus that mean, which is the minimum minimum time it take to takes from the time of packet leave the queue until they leave the note and then maximum departure time is calculated by arrival time plus no due delay or power bound minus their max. That makes which is the maximum output delay, expected the maximum output delay. And then package is a place 54 q, pipe or q, in the ascending order of the nominal departure time. It is the midpoint between the minimum and maximum departure time. And then rental minimum departure time of the head of a queue has reached it then then the packet is"
  },
  {
    "startTime": "01:14:00",
    "text": "decued, and then finally leave the, finally when a picker leave the note, substracting the residence time from the from the list received the remaining understand the latency upper and lower bound. This is the exam for the mechan's, data plane operation. Assume the there is 3flow. 1st packet p 1, alright, by the north, p one place at the head of a queue, but, cannot leave the queue calculate, cannot leave the queue before the calculated minimum departure time, 1.2 millisecond. And packet second, the picky pick 2. Arrive at 0 point for me the second and calculated the nominal departure time is 1.2750 7 milliseconds since the nominal departure time of the peak p 2 is smaller than p 1. So p 2 is a playset ahead of a queue. Finally, p 3 or a lot, arrival at 0.6 milliseconds and Since the calculated we nominal departure time is smaller than p2. So p 3 is placed at the head of a cure again. And then at the 0.9millisecondpissory Leave the QS. It's minimum departure time is has reached it. Following the p 3, the p 2 immediately leave the queue as he minimum departure time had has passed it."
  },
  {
    "startTime": "01:16:00",
    "text": "Finally, p 3 is decue at the 1.2 milliseconds. Is minimum departure. A p 1 minimum departure time. Is it not working? Tread night. Try again. Next. Next priest. Oops. Controloperations, firstly, collect the resource and delay in related information. And if the new flow, requested, select pass And that's that passes, can says satisfy the requirement and following to condition and calculate the no delay upper and low bound To all note for on pass. And then to those delay output and low bound have have met the following two condition. We Our method, the last node is the node that can take vinyl action to ensure the variable delay low and upper bound for one time. So, no delay upwind low bound of the last node, it must be determined 1st last note delay upper bound is recommended to set the as rods as possible to, possible Buffaloosa. So the last to know that allow the flow. And Last to know, the delay low bound is minimum time to transmit at, any any any any any preceding packet of the airflow"
  },
  {
    "startTime": "01:18:00",
    "text": "of the last note. And previous note, No delay. Upper bound is remaining value at the substating the know to delay low bound over the last note from the variable delay, operant. And previous node to delay low boundaries Assign as the remaining value after subtracting the node to delay upper bound, over last, you know, the from the you able to delay low bound. Propose the solus solution has the following characterist for the retirement the command of this creepy described in the DNS scaling requirement. The solution does not require this time synchronization. But need to frequency sick. Synchronization. Solution largest single hole propagation undulated supportive and high link speed is supported. And does not require to maintain the flow station, state. flow And Loebast against note and interface and topology changes. And flow flow fluctuation inherently does not Oh, cure. And There are no scale issue. Required, regarding to the number of the hops."
  },
  {
    "startTime": "01:20:04",
    "text": "Thank you. Any question and comment? We've got I'm Torres. Yeah. Thank you very much. Couple of of points. So I guess you're assuming with a PIFO that you can, explicitly say what the intended departure time is departure time. Yeah. So in terms of the PIFO has a, basically controlled parameter So you you didn't explain further how you're planning to use the Python. So I I assume you're calculating the intended departure time the nominal departure time, sorry. Yeah. And you're giving that as the control parameter to to pay for. Yeah. 1st, when a packet arrived and We calculate, minimum and departure time according to the no due delay. Open and low bar. And then Sure. According to the nominal departure time, deque in queuing in PfeifferQ and and this is arrange the ordering this This time is using, ordering time, time And then Package is departure at the minimum departure time. Right. So, Well, 1st of all, these, high speed PIFO implementations they're they're just in the process of coming. There is a hopefully good paper from CMU this year. Forget the conference that is at that that's showing a scalable algorithm. There was another one last year. The Implementations from researchers that were done before weren't really well scalable. There's still the point missing that, the pay for control"
  },
  {
    "startTime": "01:22:01",
    "text": "parameter can be an actual time stamp, right? So that is that is still for, for them, you know, I've, I've been trying to push them to also include that, but they want to get the current work first published. So currently, the, the, the control parameter is a priority, right? So you can't continuously increment it. So, so there is one more level of FPGA complexity to, to, to, to get working before pifols. Can really, start to move into into the industry. As far as this mechanism is concerned, this is very similar to, what, we did a few years back as a research paper, LBF. Which is referenced in our GLBF document. And so the reason why we gave up on that is because the flexibility of this mechanism is so large that we couldn't figure out, and I don't think you have done that either. To describe an actual calculus, meaning the math that I can take to figure coming, flows coming that want to get admitted. I do need a calculation, and I need to know what latency can I guarantee to them and, what total bandwidth can I guarantee in obviously, the characteristics of the flows that you have, like, birth size, and and rate how you characterize the flows, will have a big impact on, you know, what you need calculate as the worst per hop latency? And, implicitly these delays that you give them for hop is, some, some form of shaping, that you know, has an impact on the bird size hop by hop. So there's a humongous amount of flexibility and If you can make progress in actually getting towards the calculus, that would be great, because I I pretty much gave up, which is why we kind of converted the idea to the, okay, what can we do simple with the Python? And that was then the simple damper with, using, the ATS UBS calculus, and that is the gb GLDF draft that we already"
  },
  {
    "startTime": "01:24:00",
    "text": "presented, right? So, and the big benefit of that is that so it's a very simple calculus, and, with the damper, we can even implement it, in FIFO. So it can even be, earlier implemented. So this whole flexible stuff, yeah, the the calculus is treat the maybe the the pifos will come earlier before Someone has a working calculus for this. And 2. Hangville? I'm calling from. Just for the, maybe, at atat mobility, none of these Yes. For point Nice. 3. I just want to remind that Hi. Gulling is paid. We are, influence the time control and the buffer. Sets. I didn't see the explanation in this section, gave a ready reasonable or analysis. So, I hope I hope that, both in your draft, you may should they enhance this section, to really get capture the points of the behind the requirements. Thank you. Shafu. Hi. This is Ruben from the city. I have 2 conditions. The first one is, I saw the text in the document that that the no delay with the low bound, and the no delay with up on the should it be, installed, I each know that for the flow does it currently in the pocket I'm afraid. For I I will repeat them by quality again. For the no delay, of a low bounded and another delay of, up bondido. Does the parameters for the flow"
  },
  {
    "startTime": "01:26:00",
    "text": "is carried in the pocket no. That will be carried in packet We assume assume the, configurate that the node if the Previous note, the or with the know the delay open and low bound, same. Then 2 parameters can be carried by Turkey. Okay. Thank you. Okay. The second question is that, Yes. The document, described the the, noted today of up, bounded, you the related with the parked that, have already existing in the queue I wanted to know if there are any mathematic, equation to to compute the, the purchase amount Did you say we need them to add the mathematical Yes. Yes. To to to calculate that the park is about the the the the know the delay of I put on it. Okay. We, we try to update the the mathematical Okay. Thank you. Thank you. Okay. So we, had made a miscalculation in the schedule, and we thought we had less time than we do so since we have more time, we'd like to invite Jeanine to come back and for Shao Fu to come back into queue."
  },
  {
    "startTime": "01:28:02",
    "text": "And to finish asking his question, and maybe we could discuss it a little bit. We have extra time for discussion. So if you know it's still in the room, hopefully, Do you want the presentation back up? It's sure. Was just the previous presentation. And, Shafu, you wanna repeat your question, Thank you. Oh, okay. Thank you, for giving me, more time we describe, okay, Tido. Maybe I can repeat them by by 2 quotations and the the first one is, I think it is a benefit for us to provide, comment apology based the example to show the services skurning provided by Yeezy solution based on, yeah, common topology. That that is my first policy. Yeah. A common topology, that was discussed in inter meeting, different Yeah. And I think the conclusion there was that because some of the vendor services based on the small skill network. And some are based on the largest scale networks. They all fit into different solutions. So if the solutions are meant to be used in different topology. How can we come up with the comment apology to talk to evaluate all the solutions. That was our conclusion, I think. My opinion is that the maybe way on the emphasize the comment apology. I think maybe you need to use for for, for each solution, to provide the, apology that"
  },
  {
    "startTime": "01:30:01",
    "text": "it is singles the is suitable for is fruition. To provide the sample, qualification of the, the The the I would put over the solution. Yeah. Any solution can come up with its own topology to provide the strings or showed, suitability But in that case, solution have each different topology. Then we cannot make a common sense of common topology. Right? I agree with you that, I I I have said that maybe not common topology, but, maybe here in the white deal topology, for each year solution. I think it is maybe to give you more access to the unique solution to to show the qualification of the output of user solution. Let's discuss this. Let's discuss that in somewhere else. I think we cannot make an agreement that he Yeah. So so perhaps we can perhaps we can take, introduce that concept of a reference topology into the taxonomy document to illustrate what's meant by scale. Because if this is tied to scale, we have these terms, which are a little amorphous in the taxonomy, maybe by giving reference references, it'll help qualify those terms and maybe evaluation of different solutions. Yeah. I agree. Okay. Okay. I will think more about it. A reference topology Okay. Yep. Thank you. We have question. another Yes. This is can can I continue? Yeah. Shafu, continue. You had a"
  },
  {
    "startTime": "01:32:03",
    "text": "two part questions. You only asked the first part. Yes. The second condition, yes, it is, also, about the the mud honorable. Yes, I understand that Cisco can provide multiple level. For example, the global latency is just the public side Divided by the service provider. Yes. So it seems that the the probability is the just the the target interval. That means, for flow, the power of the wind is in the and after the time of duration to set up the party to send the. So The propositions for this flow, it's just that the tuck it into Yes, sir. For different flows how different the public intervals So I think that this is the, multiple levels provided a bicycle I'm a little hue if I understand the I'm not sure if I understood your, okay, incorrectly, but the equation here, I mean, the expression shown here is end to end latency bound. Yeah. But but but at the end of the latest boundary is, computed by the, the horrible data is supported by hope it comes. No. Not at all. No. No. That's not the case. Some of the solutions may come up with a per her voltage bound, the egg, and then multiply by a whole count. Right? That's the case of CQF. But in the work conserving rate based solutions, has pay burst only once property. The papers to only one's property can be seen in this expression as well. The large b"
  },
  {
    "startTime": "01:34:00",
    "text": "is the maximum burst of That flow. And it is not multiplied by the hook count. Right? So that's why it is called papers only once. So the per hope latency bound if you insist is the it includes the b It includes the b propeplatange bound includes b, but over all the whole, over the hopes the burst paid versus paid only once And the end of 10 to 8 hash bound is also a function of B Once so he doesn't add the cell mode. The uh-uh voltage bound. I think I have explained you so many times. 44. Yes, sir. a solid, still misunderstand the unit. I'm I think that we can, discuss any more in the mail is because it this is a well, I can recommend you a very good paper. Okay. Thank you. you. Thank Abdusilum, Yes. I sort of agree with so it's for, should be a reference, Maybe not I will not say it's apology, but, As he said, there should be some kind of reference And to answer to the chair, I was the one who may be said no. My reason is that the the the this Tech Soon is very important. But, I still don't see the the the the as a taxonomy, would like to see as the chef will say to"
  },
  {
    "startTime": "01:36:01",
    "text": "maybe the use case, most mostly at the let's say the the application for the dead net or let's say the, what we say, they'll say use cases. Which is current. As you said, there was one slide. You mentioned number 9. It was, it it it maybe I think it's the best taxonomy. I can feel it. It's close to the use cases. Slide number 9. That's, maybe if we can focus on the slide number 9, uh-uh, or you make it more in details, reference, I think that will the No. No. Not this, of this slide. Yeah. Okay. I understand. It was having the service service rate. And the floor and there was something a 33 3, categories. And, you you you had one of the light has some categories but they are in the same level. Sometimes categories you can, and you went very deep per in in making groups of these, it's it's fine. But at least in the end, I would like to feel that as as Sheffield said, there's come some kind of referencing to the current implementation, which is the deep net and the power of application, of the use case, you can say, So so that's the that way we can we can use this document as a a background. Thank you. Okay. Thank you very much for the comment. briefly answer your question. I will The reference topology, We'll be needing. And we will come up with that topologies, but I cannot say that can be only one. Maybe 1 or 2, maybe including wireless environment as well. So"
  },
  {
    "startTime": "01:38:02",
    "text": "I totally, and so as long we'll discuss about that. Come up with 1 or 1 or 2 or multiple, reference topologies. And the use case Actually, the focus of the taxonomy draft is not the use case it's, you know, a very complex job to at least all the use cases. Then we have a use case draft in our walking group as well, or 2. So 101 We will reference those use case. Not inventing a new new use cases. Thank you Thomas. Thomas, Edgar. Yep. On on the reference topologies, So I think you mentioned it, many detmets. And before, that that the rings where something that was shown also to be a lovely the most difficult part for the, for the deterministic latency algorithms. Which I think is also why, for example, if I re I think I remember correctly the validation that was done by, the professor that was doing the UBS algorithm they also used a ring that And for me, you know, it may not be, you know, common everywhere, but rings are a very cheap way to get through a city. So they are they're certainly, you know, rings. For metro areas. So a very simple thing is to come up for for kind of algorithm for scaling, a large ring, right, obviously, for just quick validation or so it's a single simple, smaller ring, like, in these research papers, and then each of the notes can also be a hub for more routers. Right? So that that that seems like a very good, thing that can be scaled up and down and has the introduced the big problem that in the ring, delay can increase if I remember correctly. And it actually also real well. So I think that that would be good. Sorry. Yeah. Ring is Ring looks very simple, but it is very tricky as well as that. It introduced the cycle in the topology"
  },
  {
    "startTime": "01:40:02",
    "text": "and the burst of accumulation can feed forward and you can explode. So it is a very good, reference architecture. Yep. Yep. Ring should be, definitely will be considered Thank you. Still Lou Yeah. So I think this is a chair comment. I agree with the speaker that, this document is not about, use cases. I'm sorry. The taxonomy document is not about use cases. That's the use case. Document. So I agree that it doesn't belong there. In terms of having some reference topologies and perhaps multiple that sounds like a great idea. From a process standpoint, you listed the authors that's the ones, inserting it as, coming up with those topologies. An individual document, you're correct that belongs with the authors. But as soon as we adopt, it belongs to the working group and anyone can suggest that So the question is, Do you want to do get your topologies in And then go for adoption pull. Or do you want to adopt and then have the working group work with you on those apologies. And in the end, the working group's gonna work on them because it's gonna be a working group document. Yeah. I I'm not very familiar the working group process, but I definitely give the objective as, working group document first and then proceed as a working group Thanks, man. Okay. That that's great. I think we would like that as well. I haven't talked to y'all about it, but expect he'll be on, agreement on that. But that does mean that the working group works on the the apologies that anyone can provide input, not just the offers. Thank you very much. Thank you."
  },
  {
    "startTime": "01:42:01",
    "text": "More cases? Yeah. This one. We can't hear you. I I think he's no longer in the meeting. So maybe he moved to that one. Okay. I'll, Antoine. I was expressing I wanted to express my agreement with, document being adopted before the topology work being included in the in the together is the project document. The page document is already a large amount of work, and Once again, thank you for putting this work together. I think that topologies and, use cases, which should come from operators and people deploying networks. From natural vendors and academics sometimes figuring a top an effect approach from a use case is very funny, the size, but doesn't come close to real life deployments. And if we can have experience from, network operators that can show, example, to purchase, whereas you want to deploy national debt net I think it would benefit the evaluation of the different solutions that we study in last year. That's to fit with real life deployment. Yes. Thank you. So that's why we need many inputs from all of our working group colleagues please provide inputs. Thank you. Yeah. And and and, Tuan, if if if you have any actual, topologies to offer, that would be great. Right? So I've I've repeatedly struggled, you know, especially when helping researchers"
  },
  {
    "startTime": "01:44:00",
    "text": "to know what topologies can publicly be be given, right, because the one I know, are, are very often, not public, right? So which is why What we, as as vendors typically try to do is then to to do, you know, simplifications down to reference, like, what I said, ring with each ringnote being a hub or so that's, sufficiently anonymous that it doesn't, point to any individual deployment. I know which, you know, So, yeah, but, anything that that you do know that is, realistic, that would help greatly. Yeah. Just to clarify, I have the same struggle and, this is curious regularly every time you do network measurement, evaluation of to purchase Having real life deployment is a real struggle and, we would revalue I would really value input from operators on that. So, then, Antonio, yes, that will be ideal. If operators contribute topology, but we are contribution driven. So We have to live with what what is contributed Yes. Please make Yeah. Just, just, my comment. Was one item alignment by the the use cases. It's it says a reference application of the use cases, which is say documented already can be a reference to to help the text sooner than me. So, or even the topologies can be as it's usually the topologies for, use case. So if If you want to reference a apology or even a UK reference, the use case it's it's similar. It's not different. I I don't see much difference between them. But anyway, you can at least we have some kind of reference for the taxonomy. Thanks."
  },
  {
    "startTime": "01:46:02",
    "text": "Yes. Yeah. Yeah. Of course, the topology and use cases are related. That's why we have already referenced some of the use cases draft Currently being proposed processed in the working group. Thank you. I lost the slide decks. Bella's Sorry. Janice, can you give me control again? Yeah. Sorry. Just somehow, something went wrong with my Webex, so I can't assume with Mymit, just wanted one one quick comment. On that stuff. So I think we may have, you know, the same type of topology also, issues also for other functionalities in in dead net, right? So for example, what what I've seen much in the past is that the path steering that that we wanna have to have the path on which we guarantee the bandwidth. That, that, of course, has a lot of different options that we haven't really talked about that much except for potentially for every flow to, to stick down the path. And, but then there are other options like these, IGP topology extensions, so on, and how good or how bad they work does work a lot on topology. And, but but but but the complexities of of for for for that path selection. That's actually a different issue than the complexity for our admission control for for the bounded latency, right? So or we'll have to be somewhat aware that whatever we're doing for the bounded latency work here, may not necessarily expose all the complexities that we get into when we're talking about, you know, fixing down the path, for, for the guarantee because complexity, we should leave out if if possible because that Otherwise, we're taking on too much. Right. I think I believe that if the reference topology is given in the taxonomy draft, they do, clearly influence"
  },
  {
    "startTime": "01:48:01",
    "text": "Other documents in the 10 networking group as well. For example, the requirements Now, of course, the older solutions document, So we have to, I think, be very careful. And we we would like to get as much input as possible. Thank you. K. Next presentation coming up. Thank you. Hello, everyone. This is from ZT. My topic is flow aggregation for enhanced net That's weird. Okay. Thank you. So this is a the agenda for flow aggregation in enhanced deadlet. We may 1st discuss the gaps and the comments for the flow activation and provide a consideration for affluent aggregation in, scaling network first, the gaps and the requirement for the flow aggregation, that is we need to provide, aggregating, to aggregated their debt led flows to, resolve the scalability issues as the transit allows. For 1st, as per RC, night Life 552, there it, that may have have their concern from their scalability, issues. And even their, flow aggregation, their at their aggregator level, steer has their, scalability, problem. For example, it will have, their, to maintain their"
  },
  {
    "startTime": "01:50:01",
    "text": "aggregate level, states by their, trusted laws and, either we're be challenging for their operation, securing a control, and and as we, discussed just now for the category, class level, maybe, could be provided to simplify and to, dynamically to, for the traffic fluctuation, and it can provided the, the, aggregation by their same level of the service requirements. So 1st, requirement for the flow aggregation, it it has said that, we considered it can, pro, maybe considered for, pro pro of overflow aggregation on their class level. Sorry. We, the first, problem to, resolve the capability, issues when we grat we grabbed, aggregated the flows by the class level can, further to resolve their scalability and as their transit lows it can, provider that transmission, to, Esther for the limited, the aggregated Lambos, to, reduce to their, states of their, and flow the peripheral states maintaining And the second, gap and the requirement is that we Sorry."
  },
  {
    "startTime": "01:52:01",
    "text": "As their requirement, in, draft, describe, describe the different levels of applications may differ in their SLA requirements for example, just to mention that the, lost lose the, latency, title, latency, or the stricter jitter, stricter latency if and so on. This, may be, be categorized by, based on the perform performance. And they're, we just discussed that the treatment is is solutions in data play also can be categorized based on the performance. And they're so, for their requirements, there, based on the flow aggregation, we lead to, provide their, provoke behavior for example, the different, treatment solutions And so, we, may be considered to provide the pro a flow aggregation, by the by aggregating the that the individual flows into, their a class level to, see lecture, treatment based on their performance, to provide the their, fine grained their, provisioning And there the third is that we may to, correct, correct aggregate ethylate flows across, different to the the cell domains for the requirements, their different lateral, implementation may be intended for different domains. For example, there domains has no, cooperation between the the domains and their so their,"
  },
  {
    "startTime": "01:54:03",
    "text": "into working should be, provided for their flow aggregation. For example, the mapping between the there are, different domains for the queues requirements. So, for after discussing the gaps and the requirements for flow aggregation in enhanced debt led, we may provide considerations about for example, aggregating that flat flows on aerobic level, class level. So, for example, we may consider the flow a classification and the, flow and dedication. For their class, class, for their flow classification, we may provider the aggregation for the treatment based on the, dataflow specific choose characteristics, under aggregated level, class level. And, of, we may consider their, they're a predefined classes, for the different levels over the the applications. And therefore, and, flow adding authentications we, once discussed further, in their requirements, draft that we may provide their explicator and that if you identify for the, for example, the ipv6 encapsulation. So, if we do the aggregating, the aggregate class level, or maybe we, we consider their, or, explicit class, and then find identification and, maybe encapsulated for using their existing, existing or field or a new field, This is for a later consideration"
  },
  {
    "startTime": "01:56:03",
    "text": "So, this is the first, initial version for a flow aggregation, which has to propose the the gaps and the requirements for in in hospital. So, maybe later, we will discuss detailed for the, requirements. For example, in scenarios, the 5 gs and then, to discuss detailed for other the later, extensions for for definite. So, comments and suggestions are working. Thank you. Time for a couple questions. Hi, Jun. This is Jozong from Huawei Technologies. Several comments for this contribution. The first one is that, could you maybe 10 Yeah. A previous slides before. Maybe the first requirement. Yes. I noticed that the requirement, especially, especially the scalability requirement is is reached from RC on 9522, which is the traffic engineering scalability. Kind of this kind of issues. Yeah. But from the beginning of the net, in the net detector framework. Or the RFC that have already been published, the flow aggregation has already been considered, I think, from the beginning. I'm not sure whether to, select or the to get the requirements from the traffic engineer re aspect can be used in data directly. Yeah. That is the first question or comment. The second one is that you have mentioned that a lot of different queuing mechanisms may request a different degree of low aggregation, a great about this point. So I think maybe the colonial"
  },
  {
    "startTime": "01:58:00",
    "text": "because I knew, the different kinds of human mechanisms can define their flow aggregation, independently. So I'm wondering whether it is, necessary for the working group to have a document, independently to this us the, for aggregation. Okay. I I understand your concern. But first, first, the comment that, there are FC 95 22 has been released just under in the e the existing version is that, for the flat architecture, it has defined the flow aggregation yet but it just defined the flow aggregation on aggregate lever lodged on their class lever. So, aggregate level is not it a can it cannot, resolve this capability properly. It's just improve their scalability problems. So there it is, this it is specified clearly in their Fc 9522, it, it it is does that consum, still still have the scalability to consume, and it is still be challenging for the, for example, the state control or the state of maintaining. So I think we, in, in enhanced the letter, willing to enhance their their flow aggregation to resolve the scalability issues, So this is yeah. I understand this point because the requirement document of Johan Sapan and have already raised this point. Right? And that just, what I'm trying to say is that the intended framework there is for aggregation already. And, from the whole process of than that the flow aggregation is, considered. For example, in, yaw model, there will be a layer of flow aggregation. To allow different"
  },
  {
    "startTime": "02:00:01",
    "text": "type of segregation. Actually, also class based is allowed. So and also, yeah, the and I think the the traffic engineering scalability doesn't fit in, dentists narrow directly. That is the point. Yep. I agree, but I think that it the existing flow aggregation is not very clearly clarified in in that letter. Working groups. So what we Yeah. So so if you raise the requirement just, point out what is not clearly defined. That's not working group. Raised another traffic engineer in document to to, you know, raise their apartment. I think that is the point. Yeah. Okay. I've I think we, I think for their extensions, may be related to the traffic engineer, but, first, that we may propose there, further, clarification for the flow aggregation. So there may be some relationship betrayened the definite and the the traffic engineer may Did that may be their further proposal. Okay. Here's the maybe we have some we we can have some conclusion yet. But here's some suggestions first just to analyze the existing data documents, especially about the free, flow aggregation part. Maybe you can list them, from the dynamic architecture to then a young model and to other than that, especially the enhanced debt net requirements document. Because the the requirements have already at least still there about the scalability. And, Yeah. Maybe we we haven't got to the second point yet. Okay. Do want to give some response to that. I I think it would be good to take the points to the list because we're actually over and there's one more person in the queue but I think the point also that you made Shasong about looking at the existing documents"
  },
  {
    "startTime": "02:02:00",
    "text": "and seeing where the gap is in aggregation, take a look in particular at the Yang document and the aggregation section there. And see and look at and see if there's anything missing. So I think what you're asking for is already covered. But perhaps I'm wrong So please take a look there. And, if you can take your second point to the list, that'd be great. Alper, you came in, before the end of the session. So if you can make it quick, that you're fine. If not, please, take it to the list. Alright. Well, he dropped off. I really, we want to thank everyone this is for both myself and. Thank everyone for participating in a in a great session, a great 2 sessions, And a really wonderful IETF. We thank all the the contributors that who for the documents and the discussion. And, notably, also, our in room delegates who have really helped us out here. So thank you all, and we will see you in person in the next Thank you. Thank you. So this is I see this has 2 Luke, can you suggest, in the next meeting which makes me think that I can attend? Excuse me. Our next meeting is going to be in Vancouver. As the next Iutf. No. No. Not, next meeting IITF, the next session. I don't have this schedule in front of me. I'm sorry. But I would say, you know, open open"
  }
]
