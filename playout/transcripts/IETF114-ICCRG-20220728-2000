[
  {
    "startTime": "00:01:30",
    "text": "so what"
  },
  {
    "startTime": "00:02:17",
    "text": "next one hello folks hello trying to figure out how to get my slides up here so hopefully your audio works more reliably this time yeah i'm trying to figure out how to share i'm not seeing my slides up here and share yet so a bit dismayed okay well i don't know if i can do it this way let's not try and share my slides uh"
  },
  {
    "startTime": "00:04:03",
    "text": "you can see my screen yes we can see you but please use presentations uh yes i'm going to do that well if you can't hear me we are in a bit of trouble but not for too long because i won't be talking for too long by saying welcome this is the iccrg meeting at idea 14. um if you meant to be somewhere else this is more fun i promise you so just stay here anyway um i'm going to just very quickly go through and show you the note well if you are not familiar with this you should become familiar with it and if you don't uh um uh if you if you're not familiar with this again hit me up i'm happy to share this and talk to you about it uh know that the irtf note well is a little bit different the ietf note 12 however high level it still applies um i'm not going to go over the goals of the irtf they are in the slides you can look at this the only thing i'll point out is that this is a room where we are not doing standards development so remember that as we have conversation and discussion today finally the agenda we've got three topics today uh got product congestion control bob uh briscoe and vivi gold will be talking about that um i have after that um the agenda"
  },
  {
    "startTime": "00:06:02",
    "text": "slightly changed we're gonna have uh present packet reordering in multipath transport scenarios after that and colin will be go talking about condition control work especially with respect to the new proposal and transport area about condition control work in the irtf and in iccrg uh so we'll we'll spend the last 10 minutes or so talking about that today um that's all i've got uh i want to quickly get somebody to do uh scribing uh i might as well do javascript again [Music] uh yeah who's taking notes we need somebody to take minutes it's gonna be a lot you don't have to take minutes through the presentations just the q a for the most part hello hello you want me to assign a person i can't see people in the room here and call out people ah it seems like new cultural is volunteering oh excellent thank you neil all right i will send you your gifts later um and i'll charge the ietf irtf for it uh we're gonna go on to the first talk bob you're already up there uh take it away we got uh 30 minutes for this thanks bob"
  },
  {
    "startTime": "00:08:00",
    "text": "i assume you're driving the presentation are you jenna uh so you should be able to you want me i can do that if you want me to well i'm going to stop my i'm going to mic without any way of controlling anything got it i can do it let me get it going what's that light out there right okay so um i had a look back and realized that when we've presented things about prior congestion control twice in iccrg before we always just said the latest thing we've done and we never actually gave the sort of basic results so this is what this is all about there's the people involved in the authorship of the draft and the experiments that i'm going to go through next slide please jenna um yeah so just just uh some caveats um what you're going to see is comparative results between different congestion controls and different um aqms and the primary reason is to share insights that we got from those and you'll obviously get the numbers in the results as well but um the the test traffic isn't designed to be um realistic it's designed to be um explain these effects i want to talk about so next right so you've you've probably seen the right hand picture first the the left hand of this slide is um more what the congestion control what the difference is between the dc-tcp and the congestion control are on the left the right-hand side is the principle by which dc-tcp and prague"
  },
  {
    "startTime": "00:10:01",
    "text": "work i'll start with the left and i don't i'm not going to go through all these i'm just going to say really the list isn't that long and that's why it's there and this is what is in the draft to explain um assuming you start from dc tcp what the differences are so on the right um i've i always explain the point of dc-tcp and scalable congestion controls generally as having small sore teeth so you don't have a large variation in the buffer to get full utilization and if you push down delay um by setting the set point of the an aqm low you don't start losing utilization so you get the best of both worlds so next slide is going to be empirical results that prove that just with a single flow [Music] on the left is is a cubic flow acn cubic in fact to show that it's not ecm that's doing it but it's the cubic cubic key part of it um happens to be using fq coddle but of any classic aqm would do and the left hand column of results uses a five millisecond target and the middle column uses a one millisecond target and then the right hand column is prago dual pi squared also using one millisecond target the horizontal axis is a range of five link rates and five round trip times so you've got 25 different cases for each column and you've got q delay and utilization as shown in the previous plot now shown here and you can see as you push down the queue delay set point for fq codal from five to one millisecond"
  },
  {
    "startTime": "00:12:00",
    "text": "you do get lower mean delay which is the blocks and you do get lower 99th percentile delay which is the little dots on top but you lose out on utilization it the mean which is the the um little bar in the middle of the horizontal bar in the middle of the vertical bar goes down to about 87 88 and um you can see the um first percentile and 99th percentile also um well the first percentile goes down below 85 uh in the highest bdp case tested whereas on the right you can see um if you use a one millisecond threshold with tcp prague because of the small saw teeth you get near 100 utilization um as well and that's just showing you proof of that point next slide um so this now um the next point i want to make runs over about four slides first of all when you look at a plot of delay um the the little blue dots in the middle column that the columns are the same by the way the same five link rates same five round trip times giving you 25 of each um case but both plots this time a q delay and the trouble is as with all um delay metrics smaller is better so you can't actually see the good ones um whereas if you show a log plot which is what the bottom one is you can see it's significantly below the other ones and the other interesting pattern there is it's getting lower as the link rate goes up now when you work out how many packets there"
  },
  {
    "startTime": "00:14:00",
    "text": "are in the queue um you find that that's because as the link rate gets higher by the way as the round trip time changes it doesn't change so each step is flat but the link rate is obviously making a difference and you find that there's um about a mean of one packet in the queue and um the 99th percentile is about two packets as the link rate goes up and um the important um point here is this is one prague flow and one cubic flow competing with each other in a dual queue and the reason that delay continues to go down um you'll see on the next slide um in the dual q case it's it's due to both the coupling and pacing now let me explain so the the top this time series shows you um the number of packets in the queue over a time series but just in the lq but bear in mind there is also a cq here that isn't shown there's a second queue that is serving the um classic flow because it's one on one remember so what's happening is when this queue is zero it allows the cq to drain its packets and so you can see it's about half of the time the queue is zero and that what what happens is the coupling um from this eq causes the marking in in this lq to push back the prague flow until it leaves enough gaps to be about one to one um rate between the two flows um and that's why as the rate goes up then the"
  },
  {
    "startTime": "00:16:00",
    "text": "q delay gets lower and lower because we're we're just the size of the queue is essentially just the size of the burst from pacing which in this case are two packets so you don't have a standing queue at all and it's not touching that one millisecond threshold it's just being affected by the marking from the cq coupled across not by its own aqm so it's not causing a standing queue up to the one millisecond threshold and so the size of this gear is purely the size of the bursts that come from pacing so in this case because the pacing is um about two packets you get small bumps with lots of small gaps but if the pacing was larger or if the bursts were larger you'd get larger and fewer gaps but um the next slide you'll see that if we this is a zoom in back on the very first plot i showed of the delay versus utilization but just zooming in on the delay the one i said didn't um stayed flat and the utilization stayed good but here you notice the delay is not reducing even though this isn't a log plot you can see that and that's because this is a single flow in the lq without a flow in the cq and so then it is butting up against the um the threshold of one millisecond in the lq and that one millisecond is configured so again the same um link rates and round trip times um and the main and p99 shown there um the the red it doesn't exist because this is just pulled out zoomed out of the um other plot it's an inset from the other plot so it's not it's not a problem that there's a standing queue up to one millisecond because one millisecond is"
  },
  {
    "startTime": "00:18:01",
    "text": "is um small enough it's just interesting that um when you have the traffic in the other queue the lq is actually um just causing one or two packets of queuing not um a standing queue up to the threshold and that insight shows you that if if you mark a cue from any other queue that's related to it you can get rid of a standing queue which could be useful um for instance if you had a virtual queue which some of you may know what a virtual queue is i haven't got time to explain it now but it it will um it's a q it's what the q would be if your link rate was slightly slower than the real q and if you mark with that you will lose the standing q and you so you just get the bursts not the standing q so that can be useful okay um next slide please jenna right um jumping to a completely different point we've still got one flow for each uh one steady state flow for each congestion control here and this is just showing what happens when you have more and more or different numbers of flows of different types um so now along the bottom instead of link rate and round trip time we're at one link rate which is showing bottom left 40 megabits per second and one round trip time 10 milliseconds but we've got different numbers of flows a and b if you look at the little gray box at the bottom example a2 b8 means two a a-type flows and a b eight b type flows a um is depends on the column um on the left column a is e c and cubic and b is um cubic without acn on the middle column um sorry am i right"
  },
  {
    "startTime": "00:20:03",
    "text": "oh sorry i completely forgot billy was meant to be jumping in here because she's got to get a flight yes janet can you jump to video slides sorry i'll come back to that sorry for this interruption sorry it's just i have to take a flight so we're gonna sandwich my results on apple uh quick in between bob's slides um so we have started working on prague conduction control as some of you know from the wdc developer session that we had some a few months ago and these are some early results from our lab testing um and next slide please these results are over ethernet and for upstream traffic next slide um as you can see these are we tried to do the testing on similar bandwidths and different rtt combinations as they have been done for tcp prague just to you know compare the two implementations and we see sort of similar results and um in in general if you look at the different results prague has almost 90 percent reduction in queuing delay as compared to cubic for for the low bandwidth case you know the four megabits per second i think this you see these results because the queuing delay is a little higher because you know for four megabits per second to have a one millisecond cueing threshold you need to send less than one packet it's just 0.33 amount of the packet so that's something we don't do we don't do fractional"
  },
  {
    "startTime": "00:22:01",
    "text": "congestion window so that's something to look at for low bandwidths next slide please uh this is the application good put plotted as utilization so there's a little bit difference as compared to tcp product because first of all we're not plotting the link throughput but the application could put and which includes the link utilization includes header sizes while this one doesn't and the second difference that we have uh is we didn't start the measurement at the steady state so we'll improve this plot to match the tcp plot and um this should get to higher link utilizations for both cubic and proc the important thing to see here is the comparison between the two and we are able to get a similar throughput for both of them next slide please this is a test done where we have two competing flows one of them is cubic the other one is prague and again we're measuring the queuing delay for the 25 different combinations the results are very similar as to what we saw for the single flow as well and you can take a look at these results on the slides next slide please um this test was done to run four prog flows at staggered times with a gap of 10 seconds and you can see that uh the flows the first flow start and it has uh almost the you use the the full link and then the second and the third and the fourth starts and they start to"
  },
  {
    "startTime": "00:24:00",
    "text": "converge um at at around between 20 to 40 megabits per second there is one uh flow that seems to pop out and the other three flows tend to stay together and we're investigating the reason for for this behavior next slide comparing this with comparing prague with cubic prag is on the left side that i already showed cubic is on the right side and as you can see cubic is has a lot more variation for throughput for the four different flows next slide this is uh the corresponding smooth rtt plot for the same test where you can see all these four flows the measured smooth rtt at the quick layer at the end host is staying between 20 and 22 or sometimes 20 20 and 23 so this is very this is very good result we have much much less deviation from the base rtt the base rtt here is 20 millisecond as you can see and the deviation is very little and it also reduces the jitter and next slide please and comparing this with cubic you can see the amount of jitter and deviation from the base rtt for cubic next slide please um so those were the results from apple quick we are continuously working on the improving the conduction controller and now i'll go through a few points that are important while people are getting started on this and they might have"
  },
  {
    "startTime": "00:26:00",
    "text": "questions on these so i thought i'll just include that next slide please um some of the folks asked me whether we need to use reno for implementing prague requirements and that's not true you don't need to use that you can use your default condition controller behavior like cubic and use that for reductions and increase during the loss it's only during the ce you have to change the behavior for prague and there are other caveats that you should look at the draft for for example rtt independence and pacing another thing um that's interesting for that's interesting for prague is you know when you do a reduction due to loss sorry when you do a reduction due to ce and then you see a packet loss right after you did a reduction due to ce within the same rtt would you want to do another reduction independent of what you reduced due to ce or would you like to combine the two so that the total reduction is either 30 percent or for cubic or 50 for reno this is something to investigate um and it would be great if folks can try this out and show their results with this um with this approach next slide please uh another important thing for l4s and prog is pacing is mandatory because otherwise you will create burstiness and you will see a lot of marks um and for pacing uh there are obviously things to consider like if your congestion control is in user space you know for example quick protocol has conduction control in user space"
  },
  {
    "startTime": "00:28:00",
    "text": "what kind of pacing would you want to use because you need more fine grain pacing than what you might have been using right now so if you use user space spacing you could have skew and timers that could basically cause some amount of bursting if the pacing is not accurate on linux operating system there are ways to offload this to kernel there are apis like as socket max spacing rate and sotx time and you need to use a fair q queueing discipline for both of them and some of some of the quick implementers have already tried sotx time so they have some experience and you can try either of the approach and you know decide what works best for your implementation for the condition controllers in kernel some tcp still exists in the kernel stack so for that it's pretty simple on linux you can use the sk pacing rate on the sk and then you you have a callback for tso segments which allows you to set the burst size um so that is all from me today um as i have to leave now so if you have any questions you can ask bob and if bob doesn't have those answers please feel free to send questions to the mailing list thank you thank you safe journey yeah sorry janna can you um does anyone want to ask billy questions no she's probably wanting to get off isn't she um okay want one back yep um so here this is a plot of normalized uh this is this is now going back to tcp"
  },
  {
    "startTime": "00:30:01",
    "text": "prague so vinnie was um implemented prague in quick this is going back to the linux implementation of tcp prague um before we had to let video so she could get her flight right this shows along the bottom different numbers of flows for instance a to b eight meaning two a flows and eight b flows on the left they're all ecn cubic on the right they're all prague in coddle and in the middle they're prague versus cubic in um a dual queue so the the cubic ones go into the classic q and the prague ones into the other queue and the aim is obviously to get roughly hit the ratio of one but it's not a ratio sorry it's a normalized rate per flow where one means your flow is going at one end of the capacity where there are end flows so along the bottom you see you've got a1 b1 a2b2 so for a2b2 n would be four and you're getting um if you're getting a quarter then your the normalized rate would be one um and you'll see about halfway along it it also starts testing a naught b b10 a1 b9 a2 b8 so um we have actually tested the full matrix but um this gives you representative results and you can see the fq coddle is pretty um spot on one all the time with very little variance if none apart from the odd um bump and fart which they are in fact uh hash collisions given we did a lot of tests on this"
  },
  {
    "startTime": "00:32:01",
    "text": "and on the left-hand column um you can see that pi is similarly the mean is sitting at one but the variance is greater i should add that the top plot is cubic and the bottom plot is cubic versus reno um as the as the other flow so top is cubic cubic and the bottom is cubic reno for the pine um for the pie case and the middle is pie cubic at the top and pyrino at the bottom and you can see in the middle the dual pie squared it's not quite on one but it's not far off it sort of wanders around a bit and um the blue one is sometimes above one sometimes below um but not significantly far off but the the classic flow has a similar variance to the pi one and the prague flow has a um flows have a similar variance but well not quite as good as the fq gotta one by any stretch of the imagination but pretty good um i'm i'm going to jump over the point in with the grave um call out in the interest of time so um please that you should be able to read about that or we can talk about it on the list next slide and then i think we're pretty close to done um so this one is now with mixed round trip time flows which has been a point of contention with the dual queue algorithm so we solve this using an rtg rtt independence approach within prague within tcp prague and here just to explain the plots again you've got a number b number and that this time the numbers represent"
  },
  {
    "startTime": "00:34:01",
    "text": "again if you look at the key at the bottom they represent different around trip times so something like a5 b100 means five milliseconds versus 100 milliseconds and um it tests a is always prag in the middle uh column and b is um either cubic or reno top or bottom uh sorry um not top or bottom uh reno's the red triangle and the black star is cubic and the top one is rate ratio and the bottom one is window ratio so you'll see um fq coddle gets the rate ratio pretty much one to one because it's a it's a fq schedule so you would expect that um the single queue there it's on the left um pi you'll see the rate varies with round trip time um across that sweep of round trip times but the window stays pretty much close to one because they're window fare um whereas on the right you can see the window is is pushed um into being different because the rate is equal now dual pi squared um effectively emulates a single qaqm by this rtt reduction algorithm that's in it it's just slightly worse um so you can see that the worst case is 6.3 a ratio rather than 5.5 so that's 1.14 harm metric if you like worse um i won't go through all the writing which i've put on there for anyone who wants to study this in their own time okay next thanks jenna oh yes there's this is the last last um set of plots"
  },
  {
    "startTime": "00:36:00",
    "text": "this is now the first one that isn't steady state stuff we um to give a feel for what happens when you've got um web light load this is heavy web light load plus a long running flow from both flows we've got um cubic ecn versus cubic non-ecn in on the left-hand column and the right hand column left hand being pi right being effical again middle is prague versus cubic so all the time we're trying to compare ecn versus non-ecn so we cut out the any any effect of vcn itself um the plots of the along the bottom two plots are q delay and utilization again as as at the start but this time remember with a lot of short flows as well and there's a very heavy load of short flows here um and you can see the q delay is pretty much like it was before um in the dual pi square case in the fq coddle case it's quite a lot worse particularly at the low round trip times because if you look at the scale there it's auto scaled up to nearly 250 milliseconds of the 99th percentile on fq coddle um and utilization is probably best on the pi one but it's it's um the profile of the utilization is pretty much the same for the dual um pi squared and fq coddle cases and and really the the point here is that the way prague works um the long-running flow um has an ewma ewma in it which um recognizes there's there's a load here of unresponsive short flows and it um makes some headroom for them to keep the the delay down and that was"
  },
  {
    "startTime": "00:38:00",
    "text": "all part of the design of dc tcp and that's what we wanted for the internet the the top plots are cumulative distribution functions and they're complementary and they're also log scale i have shown one of these before so along the bottom scale is the q delay and the and the vertical is um percentiles on a log scale and each one of those is just one case of the other um you know as i mentioned before there are 25 cases and it's just picking one of them 120 meg 10 millisecond case in each case and the gray background plots of the the ones off the other plot so it's picking out the two that are in the either experiment one experiment two or experiment three but you can still compare it with the others and as you can see the blue um prague one there is right down much lower um even at five nines percentile it's uh about six milliseconds and a 99th percentile it's about two milliseconds um and uh so that um i've shown you that uh plot top middle if you've been in any presentations about um l4s before um and it also shows where fk coddle is and pi on the same thing but just to point out that that is um sort of seeing fk coddle in a good light because it's picking one of the better ones for fq codel okay thank you janna i think there's one more slide plus a sort of final yeah just a summing up slide i think um yeah so the messages are if you don't want a standing queue in your buffer control marking from another queue um possibly a um a coupled queue or a"
  },
  {
    "startTime": "00:40:00",
    "text": "virtual queue um secondly the proper place to address round-trip time independence around triton dependence is in a congestion control like prague which is being newly deployed for low latency which is where the problem is and third long proud flows leave headroom for the short ones so and that and that's a feature that's intended for the utilization to be reduced when you've got a lot of short flows that are unresponsive effectively and just finally to say these results we use these um sort of plots um to check for regressions um and these have been stable since about that says july 19 i think this is probably jana the the first version of the plots i sent you i think a new one says we can go back to 2016 and and they're stable like this okay any questions for either me or biddy and i'll try and channel it to vidi i can't see you by the way i'm gonna uh i'm gonna step in here very quickly we don't have a lot of time so let's take one or two quick questions i need to switch do you have people there at the mic i don't see anybody in the mic but there are a couple of discussion questions on the uh chat there bob that might be something that if you can get back to your laptop you can respond there you know i'm jenna i'm having trouble understanding you can you oh that is a problem in my life in general i said uh we'll take a quick question right"
  },
  {
    "startTime": "00:42:00",
    "text": "yep john you're on you're on the thing go for a jonathan okay now as you probably know by now i'm going to be quite critical but i'll try to keep it sensible for this particular group so first question is um you mentioned that it would need a very smooth pacing in order to reduce burstiness um what happens if burstness is introduced by the link right right yes so um that that's actually semi-answered by the point about um marking with another queue and um i i mentioned if you mark with a virtual cue um you can um ensure that the any any bursts coming into your l4s link whether you know if if caused by like you say another link like a wi-fi upstream or something they will still come into the um the l4s aqm and the flow will still hit the um threshold but if you put that threshold in the virtual queue it at least will ensure that the bursts don't get into the real queue even though um so effectively it will reduce the utilization of the um bursty flow um up to the well it depends depends where you put the threshold in the um virtual queue you can you can absorb some of the burst in the virtual queue and then above that the burst will go into the real queue but the reason for"
  },
  {
    "startTime": "00:44:00",
    "text": "doing that is that what you don't want to do is to set the threshold in the um real queue i mean say for instance you're getting 10 millisecond burst you don't want to set the threshold at 10 milliseconds just in case you get bursts um and then when you don't get burst you get a standing queue up to 10 milliseconds whereas if you put it in the virtual queue you will get um when you have bursts you you will still get bursts in in the real queue or at least the top of them but when you haven't got burst so you've got an ethernet link coming in um as well and you've got traffic coming over that then at least you will have the smoothness of that link so the aim here is not necessarily to get rid of the bursts from the wi-fi all that although that would be another a possible approach where you put a shaper in or something but only if you really knew that you had the burst in the first place but it's more to make sure that you don't configure your thresholds just in case you've got burstiness if you haven't always got it so you you get the benefit of smooth smooth links when you're using them and you can still absorb the bursts from the burst you want is that a um oh and i i should add that that um virtual cue is in in the design of docsis from the start we would have to add it to the linux implementation to test it and we haven't tested anything i've just said that was that's just how i believe we'll be able to do it but i have started doing this with my colleague joachim mitsun right i'm going to say we need to move on uh to the next one and i encourage you to continue this conversation on either the chat channel"
  },
  {
    "startTime": "00:46:02",
    "text": "or on email uh bob we yeah we have 15 minutes left we've got two more things to do so thank you bob vidi for the presentation i do appreciate it there's some like i said there's some feedback on the chat please go take a look i'm going to switch over to natalie i'm also going to try to come here and eat your head from here as you leave let me see if i can do that i can't do that i was going to eat you up bob so low so hello everybody my name is natalie romo moreno from dutch telecom and welcome to this presentation about packet reordering in multi-path transport scenarios next slide please should i control it or do you have the control again okay perfect so i'm going to start with a brief introduction explaining the problem that we try to address here and then i will show some experimental results and conclusions so uh first of all when we compare multiple transport with single path transport characteristics we know that in the case of the multipath scenario we experience an extraordinary higher jitter and also a significantly higher out of order delivery this comes precisely from the heterogeneous nature of the different a parts which are bundled within the multipath connection now these characteristics this high jitter and high out of order are exposed by the multiple protocols either to the higher layer application that makes use of them or to the end-to-end traffic that is carried by them in the case an intermediary"
  },
  {
    "startTime": "00:48:01",
    "text": "multi-path transport is used as it is the case of the atss framework specified in 3gbp which defines a 5g and wi-fi mno boundary so from the mp protocols those who have a strict reliability and strict inaudible delivery like mptcp and mp quick in stream mode they only experience a high jitter dominated by the path latency difference but for those protocols which do not have this strict in order delivery like mptcp mpqi with the datagram extension and ssttp with the cmt extension we experience jitter at all but we also experience a high out of order delivery also dominated by the path latency difference now the problem is that the service running over the internet are designed to cope with the characteristics of the single path transport so if we add this high jitter and this high out of order delivery introduced for example by an atss splitting use case this might cause this might result in quality of experience and quality of service degradation so coming from this statement what we did was to conduct some work to investigate the demand of imperial multipath reordering and latency difference compensation precisely in the scenario where an intermediary multipath transport is used as it is the case of the atss splitting scenario in the first stage we demonstrate the impact of no reordering with different traffic types carried over a multiple transport and afterwards we evaluate different solutions to correct this out-of-order delivery and digital the explanation of these algorithms that are going to be shown here is also in this draft that i linked in the presentation so the tests that we executed were executed using the mpdccp framework but the results are also applicable to other"
  },
  {
    "startTime": "00:50:00",
    "text": "multipath solutions like mpquik we generate the traffic tcp udp using iperf and we also generated quick traffic using the quick go implementation next slide please so as i said in an initial stage we demonstrate the what happens when there is no reordering at all so for this test we used plain utp traffic and quick traffic over a multi-part dccp connection the mp3 connection has two subfloors or two parts each one of them with a 10 megabit per second capacity and the latency difference between them is at 15 milliseconds we use a priority by statistic mode that means there is a primary link that is going to be used by default once the primary link is congested the secondary link will start being used and the congestion control used in the quick traffic is nearing so in the left we see the results for the udp traffic we see that both links are fully utilized and that we achieve full aggregated bandwidth and to the right we have the quick results and then we can see that only one path is fully utilized even if the secondary path is attempt to be used sometimes so what happened in the quick case is that as soon as the secondary path starts being utilized the bad latency difference causes a lot of scrambling this scrambling causes duplicate acknowledgements which are an indication for the quick protocol of packet loss so the reliability mechanisms and the congestion control react immediately there is a packet retransmission and the congestion window is cut and therefore the throughput is also reduced so at the end we end up utilizing only one path in conclusion for a protocol like udp"
  },
  {
    "startTime": "00:52:02",
    "text": "where there is no any congestion control or any reliability there is also no need of any real no demand of any reorders so the scrambling introduced by the multipath a transport doesn't have any impact on the overall performance but unlike udp which has a congestion control and therefore there is a demand for another delivery and therefore it fails to use the aggregated bandwidth due to the impact of the packet of scrambling next slide please so now that we demonstrate that there is an impact when no reordering is in place and that this impact is also correlated with the characteristics of the current traffic we proceed to evaluate some solutions to correct this a out of order delete the first one is a reordering algorithm based on connection sequence number with an static tag so what we do here is basically read sequence number to verify the inorder arrival if a gap is detected there is a buffer whose which starts the receive packet until the missing one arrives or until our fixed timer expires in this case we use the same setup as we had before we use quick traffic because we know there is an impact over this type of traffic but with the only difference that we introduce a reordering algorithm with an aesthetic timer of 50 milliseconds which is higher than the pad latency difference and therefore enough to correct all the scrum the results are shown in the figure in this scenario we managed to fully utilize both parts and to have the full aggregated bank width close to the 20 megabits per second so basically the reordering algorithm in this case corrects the scrambling eliminates the duplicate acknowledgements and therefore prevents the congestion control in the quick stream to react and as a result the application manages to fully utilize a both parts bandwidth next slide please"
  },
  {
    "startTime": "00:54:04",
    "text": "so this reordering with the static tag works well as long as we know what the path latency difference is and if this latency difference doesn't change but there might be the case where this latency difference goes either above or below the timer that we configure so this is what we try to test here we have a the same setup as before but we have a pad latency difference of 20 milliseconds and a timer of only 15 milliseconds the results are in the graph of the in the left side so we see that the secondary path is not fully utilized and there is an impact on the overall aggregated bandwidth to correct this problem the solution that we test is the same a reordering bases on sequence numbers but with a dynamic type this dynamic number is going to be updated to be equal to the latency difference of the paths and this latency difference is going to be estimated using the rdt measurements provided by the condition controlling place so we see the result to the right and we see that it works we have again both parts fully utilized and a fully aggregated bandwidth because the timer manages to adapt to the network conditions and the scrambling is fully correct now at this point we prove that this sequence-based reordering a solves the problem of the scrambling so there is no more scrambling and a congestion control like new arena doesn't react anymore but this sequence-based reordering doesn't do anything with the latency difference of the butts so we still have the problem of the high jitter even though the new arena doesn't react to that next slide please no there is one before there's gotta be one before yeah the other one uh so now the question is what happens when the congestion control is not the"
  },
  {
    "startTime": "00:56:00",
    "text": "last base of congestion control but a latency sensitive congestion control like vbr so to test that we use tcpr carried over our multipaticcp framework we have the same same setup of the static reordering with a timer higher than the bad latency difference so we know that the scrambler that the scrambling is corrected but we still see that only one path is utilized and this happens because for a congestion controller like bbr which is latency sensitive the jumps in the latency generated by the by the scheduling through the through both butts generate a throttling of the throughput under the assumption that this hey fixing the latency come from buffer blood so to correct this problem we use or we test a solution which is the delay equalization or the path latency difference compensation what we do with the solution is that at the receiving side we delay the incoming packets in the fastest path to equal the latency of this lowest path again this delay has to be equal to the pad latency difference that is estimated from the rtt measurements the results are in the right so the correction of the latency difference or the equalization of the latency in both paths solves the problem of the reaction of the vbr congestion control and we have again full utilization of both paths and full bandwidth aggregation and next slide please so to summarize all these things we demonstrate that there is an impact when no reordering is used and we also demonstrate that this impact on the performance barriers depending on the characteristics of the current traffic or servicing the multiple transport and therefore different solutions are suitable for different traffic types now within the atss splitting scenario the characteristics of the traffic carried over the multiple network might be unknown so in this case we need certain in-network reordering mechanism to"
  },
  {
    "startTime": "00:58:01",
    "text": "guarantee an optimal performance additionally all the results that i presented here an additional one with more analysis were presented in the 3gpp sa2 working group and there it was also concluded that in-network in network re-ordering support is required in this document that is linked in the presentation it is also recommended that to achieve an ultimate performance it is better to use a combination of all these solutions together so a sequence number base a a sequence number based reordering with a dynamic expiration timer a delay equalization and also some algorithms to detect fast packet loss detection next slide please now the question here is how this fits into itf scopes so far reordering and latency difference compensations for multiple protocols is not a standardizer at all similar to what happens to schedule it within the multipath context board are so far out of scope of serious standardizations there are some individual informational drafts about it but therefore are implementation specific without a specification a consistent behavior between client and server in down enabling direction is not insured and also these scheduling and reordering algorithms are mainly agnostic to a idf protocols so they will not be part of the standard of mptcp mpdccp or mp quick so our question here is what is the right place or where to say hey were in itf to address this multiple operating topics like reordering and scheduling that's it thank you very much if you have any questions uh i think this is a magnus is in the queue magnus if you want to respond to this quickly and that'd be great i want to switch to giving colin in a couple of minutes"
  },
  {
    "startTime": "01:00:00",
    "text": "before we leave the room go ahead language westland uh your new reno did it include rack did you did it include any rack functionality and no i just tested with the default a configuration of the quick go so that was basically now okay so i guess it would more likely would behave slightly better but anyway and when it comes to this i think what you're actually asking around the itf scope here it's a question i mean you're tunneling over an mp if you have an application protocol on top directly on mp this is not the question it's only when you're doing tunneling and have another flow so it's a question of tunneling over mp protocol when this arises i'll say in authority that it's a conversation in terms of where does it fit in the itf i encourage you to reach out over email and we can we can i can pull the right people in but uh please reach out over even and i'm happy to take it from there um thank you for your presentation again um before we leave the room i know we are past time but i just want to give colin a couple of minutes to talk about uh uh corn are you there to talk about condition control work go for it colin all right thanks jonah um so i just wanted to give people a little bit of a heads up for some work which is uh potentially happening in the ietf side of things um so those of you who are in the transport area working group earlier in the week are on the transmit area list will have seen that the transport ads are considering chartering"
  },
  {
    "startTime": "01:02:00",
    "text": "a new congestion control working group in the ietf um a lot of the goal of that looks to be to update rfc 5033 which is the the guidelines for how the itf standardizes congestion control algorithms um but there's also suggestions that the the group will consider developing standards track congestion control algorithms after that uh this obviously raises a number of questions about the relation of this group and uh what's happening in iccrg um the expectation i have is certainly that iccrg is going to continue in the irtf um as a home for research and experimentation on congestion control and it will continue to be separate to standards track work in ietf the expectation i think from the itf side is is that proposals for standards track congestion control algorithms will have been pretty thoroughly vetted by the research community before they get to the ietf and i expect iccrg will will certainly play a role in doing that um and um in you know facilitating discussion uh providing review of the documents um and uh i think icc rg is um a uh a really good venue for people to talk about this this this sort of work it's it's got really strong links to the research community and i think it really sort of helps to add value when um just by allowing the the researchers the uh the industry the standards community to exchange ideas um and you know the the the work in icci has been really good at getting practical experience getting"
  },
  {
    "startTime": "01:04:01",
    "text": "implementers talking to each other each other and i think that's really important and i hope it continues iccig is also going to continue to be able to publish our experimental rfcs um they're a really great way to document congestion control algorithms um i think that they're especially important perhaps as a way of documenting things that are perhaps hoping to move into the itf standards track just as a way of clearly describing the baseline um that said i don't expect iccrg to turn into an experimental rfc factory um it's a venue for research experimentation and discussion first of all so uh yeah um i think jana's been doing a great job as chair i'm expecting and hoping that he will continue as as chair of the group uh but we we are certainly looking to appoint new co-chair candidates to uh help move things along in iccrg and uh uh janna and i are talking to some potential people in that space so uh that's what i think is happening um if there are any questions about it i'm happy to try and answer them from the irtf side and i guess talk to martin duke in the ietf side if there are questions about that i don't know if any of the transport ads are on this in this meeting thanks yeah i soon have walked in at exactly the right moment uh martin duke from google and transport a.d um uh yeah so i just to be clear i completely endorse everything colin has said i think he and i are in complete alignment about the vision here and um i i i in fact insist that i see ccrit continue to to exist"
  },
  {
    "startTime": "01:06:01",
    "text": "um and do what it has been doing if not more um a very possible outcome is that we do this congested working group and it completes its assigned tasks and um there's little if any standardization effort and the group closes and like ccrg will be there as a kadesh control forum as it has been for for years so um yeah i just wanted to just be absolutely clear about our complete alignment on this thanks thank you both colin and martin i think by the next meeting we should have a lot more clarity on what's going on so i would encourage people to stay tuned and definitely show up to the next iccrg meeting and the next idea of course both of those will have some interesting bits for those interested in construction control and how that work is done in the istar space so with that um thank you everybody um please enjoy the amazing heat in philadelphia i for one will not have to deal with that but i hope you have wonderful rest of your week in ietf thank you so you"
  }
]
