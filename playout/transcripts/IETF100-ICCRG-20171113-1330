[
  {
    "startTime": "00:03:37",
    "text": "gender so what yeah yeah all right people welcome to Isis ERG I\u0027m going to show you the not well until I have a volunteer to take in it to take minutes if it takes forever we\u0027ll just have to read every word here I need a volunteer I didn\u0027t get one previously I really need one come on yeah all you have to read it for a half an hour I mean I mean that I\u0027m gonna read every word to you I saw mangoes laughs laughing please please please I\u0027m sure you got it thank you so much and you\u0027re doing Java right come on dev escribe is easy somebody chat about jabber jabber yo Chabot ah thank you ah we got a scribe in a Java Script thank you very much people I\u0027ll leave you from the iltf IP our policy document there is a scribe and jabber question a very very quick update because I called these things I see draft as we well this is me speaking us also ant yeah just to give you an update here because I\u0027m also of both of these we asked for feedback on Lisa and uh as an author I want to thank the reviewers as a chair also want to thank two reviewers it was really useful we\u0027re not giving up on it but we\u0027re gonna merge it with the thing at at the bottom which is coupling document where we\u0027re gonna change the algorithm quite a bit to make it I think a lot more interesting it\u0027s a major update with the planning but this isn\u0027t a long-term future so you\u0027re gonna hear back from us that this is not that the reviews were not in vain thank you very much it\u0027s gonna be a part of this because it\u0027s the same problem space anyway okay today\u0027s agenda is pretty packed I\u0027d like to remind people to keep within the time frames and exhaust Nicholas to begin no point me reading the agenda "
  },
  {
    "startTime": "00:06:46",
    "text": "then if you just come on let\u0027s not waste time we can press the button for you just say next that\u0027s okay yeah hello everyone my name is Nicola queuing I\u0027m from sickness for those know what he doesn\u0027t know what it is at the French National Space Agency and I\u0027m here to present some results on MPT CP and VB are just I just want to quickly go through some myths that you may have on Sat comm systems I\u0027m short of time so I will do fast but basically when we look at if the high-level architecture of networks even though you may think that satellite it was like quite specific and strange it\u0027s not the case it\u0027s very similar to what you may have in Syria or Texas it\u0027s just that we don\u0027t have an actual interoperable standard that is quite a big problem we have but that other thing another myth on the next slide if the latency so basically indeed we have some 500 milliseconds latency on the links but we all know thanks to all the buffer bloat issues we\u0027ve seen in the past and also what has been done through the right project that latency is not just the signal propagation delay and also there are many cases such as boats plane and others we world is for example where you don\u0027t have any choice anyway so you will have to deal with this latency and honestly it\u0027s not that bad so next slide please this is some browsing experience we have using a real satellite access on the left you have a light page on the right it\u0027s a more heavy page and even though the long page the large page takes some time to have the data to actually come and once we have the first bits of the connection everything is quite loaded and fluid so and they and also on the light page on the list you can see that even though it is some answers and some things that is that requires some interactivity it\u0027s it\u0027s not that bad next piece the last but not least miss is that we use middle boxes and that is not a myth we have nasty middle boxes we split TCP connections so we actually use some transpired competition that is not that bad but for the for the TCP connections we make our own TCP connection so we break the end-to-end connectivity and they are the main problem we have is that it\u0027s difficult to have the support and to consider the whole recent improvements you may we may have other servers or as a client for example also TF all things and other thing next slide please we use me the boxes for many reasons and basically we have a quite specific "
  },
  {
    "startTime": "00:09:47",
    "text": "network I will not go into the details because we have we don\u0027t have much time but the main problem also we have is that it\u0027s a small community so it\u0027s difficult for us to push some specific modifications to TCP for our use case because for terrestrial and fixed networks it\u0027s a corner case but and also we have a scarce resource to deal with so that\u0027s why we it difficult for us to actually push things in favors and client next please and also if we look for HTTP 1 service we have three different pages here that are loaded we use open zone which is an open source satellite emulation platform and we have different case we have an i w 10 w 60 and we have a very basic HTTP proxy I do in the satellite network and what we can see is that what what the peg for the performance enhancement proxy well that we deploy can do even better than the iw 60 even for the long page so that\u0027s why when it\u0027s difficult for us to get rid of this strange boxes because for writes such good results at the moment for the traffic that is going through Bob on satellite networks next piece the thing I want that why we put in results on BB r and MP DCP is that we are following what\u0027s happening on the transport and we are constantly figuring out whether we actually need these boxes or not and and we have they aware there were some modifications in the NFC 2760 and such as higher congestion window packet pacing and these are the things we see in the transport happening at the moment so that\u0027s why we put in from results of PBR over that cone and also MP TCP because that\u0027s a good opportunity for earth to have an e breed access with SEO and satellite access next please so this is a platform we have used for DVR we have different file ftp ftps ins as I serve at the client downloading files which are all state by or cubic or VB our favor next please on the Left we can feed the results of cubic and on the right of BB r and the top curve is a good put and the bottom curve the state of the queue we can see that cubic is acting as expected basically it\u0027s fulfilled all the queue which on the bottom of the blue we can see that lots of kudos is happen and we have an overshoot of capacity on the right we see CPP a PBR despite the strange satellite configurations we have the latency and the isometric links we can feel that the we have a protocol that at the whole can use available "
  },
  {
    "startTime": "00:12:49",
    "text": "capacity and with a low Q occupancy next please however when we look at fairness then we may not agree on what the flow rates furnace is and if it is actually useful or not but what we can see here is that at 200 seconds all the cubed fluid flows are strangling between each other to actually grab the capacity whereas with VBR we have the flows that were first on the pipe but and keep the the capacity for a long time longer for we have from a late comer fairness issue with PBR with our version of PBR at least though for the moment we still are running further tests on the wizard we still needs to have some specific accelerations bb/sbb are looking at more application level metrics but we think that in looking at the queue occupancy and other shoots it\u0027s quite interesting however all traffic flows in the Internet today are not ABR to still so the thing is we still for the moment we\u0027ll have to make some TCP speeds and so on anyway next piece and the other use case I will show you is about the MPT CPUs gave v on the top the trend for the 5 GCM networks architecture and at the bottom the same thing that we can have on satellite access at the moment we have an MP TCP concentrator here that basically it here because we may not have participe deployed if we were on Terrace so that is a use case we consider but we are more focusing on metrics at the relevance of this use case anyway at the moment next slide please so here we have 2 MP TCP proxies one the the aggregator I just show on the slide before end of the one at the satellite terminal level for example and what we can see here is that we still have I need to provide here that we still split the connection TCP connection here and what we can see is that this page that you just made tree we have between the links the MP TCP somehow if of interest for both accesses again that is an unfair comparison because we compare MP TCP that has twice the capacity as every single ones but the you just image we could have results in the lower capacity but next slide please and that\u0027s another important result we have had in these studies but basically when we have an MP TCP aggregator let\u0027s say at the terminal side of the from one "
  },
  {
    "startTime": "00:15:49",
    "text": "who has a satellite internet access as soon as we deploy an MP TCP sing at the client we cannot accelerate the traffic any more on the satellite link so today if you are using satellite access you will not be able to use MP TCP on this you have use case so there have been some discussions on that in the in the NPT to be mailing list but another thing is that despite the fact that we cannot accelerate the traffic still MP TCP is doing better than each of the single accesses and have another slide basically we have made some tests on VBR we have seen an increased training testing trade-off between the queue occupancy and the actual food that you can gain how we see from late comer fairness but what is flow rate furnace anyway and but the thing is we still don\u0027t know at the moment whether we will deploy from we will still need in the futures on TCP acceleration on the satellite things when we have only TCP PBR light traffic but at the moment the share of the we still have lots of cubic flows and lots of other flows that are not PBR so we we will not stop deploying peps today and so we also have worked on MPT I don\u0027t know how much time I have left but we also have worked on MPT CP that seems to manage the large asymmetry we have when we use that right thing however we may do better than that so that is something we are working on working on adapting this template is be scheduling for specific use cases although one problem we have that when we use MPT VP at the terminal side we cannot accelerate the traffic anymore on the satellite thing so that is a problem that we have and which we are working on and also one point I wanted to make that we believe that for the moment even though we have some good improvements on TCP and lots of work done on TCP improving TCP for either cellular enter STR networks the satellite chases are so strange that they cannot be won TCP that favorite that work for every any kind of traffic so they always would we would always need some specific or more and modifications and next slide please so these are the people who worked to do preparing the results that have been used for this presentation and also if you have questions of the platform is used and don\u0027t hesitate most of them are "
  },
  {
    "startTime": "00:18:51",
    "text": "open source or open for researchers anyway and also we have an open platform if you want to do tests on real satellite link we can help you at setting it up and making from research projects so that\u0027s all for me so if you have any questions would be free and yes questions quick ones maybe my name is Anita from G this is a very very interesting experiment for MP TCP so as the being clean stood over there if I access all drink over there can i duplicate or experiment you did what do we have we need to get more information from you okay so I\u0027ve directed a pre-k to Yorick\u0027s experiment and then the rings over there is the ordering information I need to get to replicate your experiment yes I provided on the slides all you need to replicate the experiments yeah maybe if that works Logistics are interesting with us "
  },
  {
    "startTime": "00:21:59",
    "text": "all right so sorry my name is Ron bliss and I\u0027m presenting a paper that was published at the last month at the ICMP it\u0027s about the experimental a variation of ER congestion control misses joint work with Maria Martina so on you all know Google Google\u0027s congestion control we are because I already presented it I think three times here so the overall objective was to replace lost based congestion control because our belief is that we can do better in the community I think and the goal was also to achieve high throughput but with a small queue and it\u0027s basically a model-based approach and you see the model here on the right hand side so we did some experimental a variation based on the original publication in the ACM queue paper and the Linux 4.0 4.9 implementation and so our key findings are basically that the model does not work very well for multiple flows at the bottleneck that it causes massive neck loss in small buffers and it also has different kinds of unfairness especially with respect to suppression of floss based congestion controls so what\u0027s wrong with sorry what\u0027s wrong with the model so the network model is fine for for the bottleneck but bbr uses this at the sender and that basically lags the dynamics of multiple senders so that\u0027s one of the problems so the behavior in the single flow case is quite straightforward on if you have only a single flow for example at 100 megabit bottleneck you see that as soon as PBR probes for more bandwidth it will see also the delivery rate here it detects is a dashed line one round-trip time later is basically the same because there\u0027s not much more bandwidth to provide so in that case it tries to reduce the queue that has been built by the down phase and the gain cycling so the flow probes and cannot get higher delivery rate since the bottleneck is already fully utilized so and the excess data is then removed afterwards so so far so fine so that\u0027s also what we saw in the experiments so our test setup we used one gigabit per second bottleneck and you can see here the throughput the dips here are basically the probe RTT phases and if you look at the RTT you can also see you nicely the probe "
  },
  {
    "startTime": "00:24:59",
    "text": "bandwidth cycles here and the probe oddity phase so everything works expected since the model fits but what about the multiple flow case so if we have multiple flows let\u0027s assume we have simplified case two flows now sharing the 100 megabit per second so that we have 50 megabits each and we\u0027re starting with one flow that starts to probe and so the gain cycling phase up will then try to get more bandwidth and it actually gets more bandwidth because the other flow then basically loses bandwidth so the first floor that is probing actually measures a higher delivery rate and that leads to the in the next phase to using that higher bandwidth it just saw due to the maximum filter that is also applied so the windowed maximum filter keeps the sent right now at the new high level and the other flow also are keeps it\u0027s sending rate at the old level due to the maximum filter so that basically means that for multiple flows are both flows together so this chart here shows basically the the individual sent rates and the combined sent ray which is the red line here and as you can see both flows together actually are way above the bottleneck capacity so since this is a rate based approach the amount of in-flight data actually steadily increases and the bottleneck becomes overloaded so in our paper we have actually proof that this is a consistent behavior of PBR so what does that mean in a large buffer which means the buffer is at least the VDP large bbr basically operates at its in-flight cap which means you have 1 to 1.5 bdp cute so VB is operating point is not at the optimal point let the paper point it out but it\u0027s more or less depending on the buffer size so it\u0027s here for example at to VP mate so in a smaller buffer actually what happens is that now since the the right side here has shifted to the left you will find the the operating point of B B are even beyond the point B here so cubics operating point is here YB BRS operating points is even above the buffer size capacity so our experimental a variation setup looks like this so we used basically several "
  },
  {
    "startTime": "00:27:59",
    "text": "pcs and the hardware switch so on the sender here has three ten gigabit per second interfaces the receiver on the other side also and we have a software based e PDK switch which allows us to look into let\u0027s say Q States and also perform delay emulation if required and we have the bottling here 10 gigabit per second link which is then attached to another switch though we have a typical dumbbell topology for the 10 gigabit per second experiment and we have a little bit simplified topology due to restrictions of the transceiver speeds we needed a dedicated one gigabit per second link because these interfaces were not able to switch down the link speed edge so we use the 4.9 version implementation of PBR and used bottlenecks at one gigabit and 10 gigabit per second so the RTT was around 20 milliseconds in most scenarios and we had two types of bottleneck buffers the large version is 160 milliseconds while the small one is 60 minutes so these correspond with base ret to a PDP you or 0.8 times the bdp the sender is not application limited to use i3 format and we repeated every experiment at least five times and I\u0027m showing always the results of single representative runs so let\u0027s start with two flows in a large buffer scenario so two flows with the same minimum RTT the the prediction of the model says that basically the PBRs operating at it\u0027s in flight caps OBB our uses accuse one be deep in the buffer and that can be seen here so we have an RTT base our TT of twenty milliseconds and this is basically doubled nearly all the time the two flows are active so a single flow operates at the base or DT but as soon as the second flow starts we have one bpq so if we increase the minimum RTG to 40 milliseconds this should double and this is what we also can see here so it\u0027s not then the effective oddity is at 80 milliseconds and so if we use 80 milliseconds or 80 min we see in that effective ret doubles it was at 160 milliseconds so multiple flows in a small buffer where small is not really small it\u0027s it\u0027s 80% BDP so if we use six flows which means two per interface here "
  },
  {
    "startTime": "00:30:59",
    "text": "PBR causes massive packet loss because the operating point is as just explained even beyond the buffer capacity so um if you look closely at the senders transmission rate here you can see that you\u0027re ascending above bottleneck capacity actually so the even one beyond one gigabit per second so we have roughly here 10% overload sorry what about cubic cubic in the same scenario doesn\u0027t cause such a massive overload or the bottleneck it smoothly more or less operates at one gigabit per second capacity and if you compare it I had the number of retransmissions for example you can see basically that in small buffer scenarios you have three orders of magnitude differences actually with respect to the numbers of retransmissions so that\u0027s quite a lot what about interrupt interpret all fairness so if you are a let VBR run versus cubic for example in this case we have one gigabit per second bottleneck one baby our flow against one cubic flow and we have small buffers so baby are basically suppresses the loss based congestion control here because yeah VBR is so aggressive that it causes basically packet loss and cubic then backs off while PBR tries to get as much bandwidth as possible but this are still more or less within the model of PBR because we only have a single PBR flow so our expectation was that it get it gets worse as soon as you started an additional be awful and that is really the case so to bebe arbors is to cubic floors for example the model doesn\u0027t hold anymore and in that case multiple PBR flows simply behave more aggressively you can see the start of the SEC PBR flow here around this point in time are the basically the the bandwidth of the cubic flow drops down even more and even a second cubic floor isn\u0027t able to get any any reasonable bandwidth here so our loss based congestion control and small buffers gets severely suppressed if you have multiple PBR flows because as explained earlier the operating point of view RS here and at least in the version that we tested it it doesn\u0027t react to packet loss so congestion packet loss as congestion signal is basically ignored so intra protocol fairness is also interesting so are again six floors here twenty "
  },
  {
    "startTime": "00:34:00",
    "text": "milliseconds or eighty we were not able to find any real consistent fairness behavior so you can see our different scenarios like more or less fair share the bandwidth or sustained faces of few flows getting only a small portion of bandwidth by others get most of it and something in between and also here even in a small buffer sometimes you see are quite good fairness so what about RTD fairness um since the the prediction is that bbro operates at it\u0027s in flight cap and in-flight cap then basically is calculated on base of the scene R DT and times the available bandwidth so the bandwidth delay product we just use twenty milliseconds forty milliseconds in 80 milliseconds a space RT t for three different flows flowing at the same time and so the prediction is that each PBR flow operates at it\u0027s in flat cap of to V DP in a large buffer so we have a large profit scenario here and as you can see that the flow with the largest RTT basically is which is the red one here gets the most most of the bandwidth sherry so the 20 millisecond flow is ya know nearly here down at the bottom while the 40 milliseconds flow still gets some shit so this is due to the fact that the larger rgt means more data in flight and this directly leads to unfairness so float with the larger RTG have basically an advantage here so on to summarize net PBRs basically model based congestion control and works well with no congestion is present so for a single flow at the at the bottleneck it works perfectly right but for multiple flows on PBR steadily increases the amount of in-flight data and some large buffers PBR operates its in-flight cap and also has a consequence of showing oddity unfairness and in small buffers we saw a high amount of packet losses because our PPR also ignores packet loss as congestion signal so we found no consistent fairness behavior and especially we saw unfairness to flows with loss based congestion control cubic or Reno for example so as you all know BB ours already news but probably right now its many application limited where it\u0027s used so we maybe don\u0027t see these kinds of effects and these were all more or less generated and then in a lab so "
  },
  {
    "startTime": "00:37:01",
    "text": "that\u0027s kind of maybe not real-world scenario but still think know what you\u0027re worth to to think about so BB are still on a development and I think new we\u0027ll give an update on further arm yeah further development of PBR right questions yes chance questions alright well this is good we have a big room and no questions but we\u0027re gonna has another one about BB ah so I suppose probably the questions are gonna come at the end of the PVR it\u0027s not oh okay there is one Stuart Cherisher from Apple thanks for this work I think it\u0027s very interesting one thing I was wondering about is BB r has a cap of two times the bandwidth delay products but to work that out it has to know what the bandwidth and the delay are once it\u0027s got too much data in flight I was wondering if there\u0027s then a risk that it\u0027s estimate if the round-trip delay will be inflated because of the over full buffers which means it\u0027s bandwidth delay product estimate and the cap of that grow and the things spirals out of control because it\u0027s actually not getting the accurate data and it sounds like you didn\u0027t see that I wondered if you had any intuition of why no actually we see that but basically for example in the in the beginning of so the flow starts you basically see or that the flows will measure actually a higher effective oddity in this case we have more less than some kind of late comer advantage right and that\u0027s what we saw but after some time actually bbr is able to more less synchronize the flows going into probe Rd T and then they have more or less the ground truth of seeing the right minimum RT and then that corrects itself but but as you\u0027re right so in the beginning where you have a strong overshoot so we saw flows actually having more overshoot when when when they\u0027re on the starting face thank you well this is Jeff using from a panic we\u0027ve done experiments like this over the internet we were using data centers in Frankfurt London and Australia and the one between London and Frankfurt was actually interesting it\u0027s a short RTT but it\u0027s across the production internet and bibi are running on standard linux with three parallel flows same endpoints was doing a sustained 50% packet loss rate but maintaining 300 megabits per "
  },
  {
    "startTime": "00:40:03",
    "text": "second stable II so it\u0027s this sort of it brings up basically to be DP like that\u0027s the theory and practice are exactly the same but this sort of bizarre behavior that absolutely sustained massive loss rate when I tried to bring in cubicle Reno no packet sweat you know they just got crowded out on loss as you\u0027d expect so it stabilizes at a phenomenal loss rate when under Germany to Australia it hits about 140 million megabits per second which is great on the production Internet interestingly though the loss rate is way down it\u0027s earning on short delay it finds this sort of stable point just beyond a pull queue and sits there yeah Polly right which you know if I want everyone else to die I\u0027m a happy little person so Jemaine got an address Wow number of threads here but just very quickly there\u0027s the the RTD inflation doesn\u0027t really happen because PVR uses for its beauty estimate it uses the minimum on TT measured so it\u0027s not the continuous oddity or the SRT or the instantaneous oddity so the minimum oddity is measured and as Roland pointed out there is a probe RTT phase which happens which allows which is designed to allow flows to measure a minimum RTT so that during periods when the network is not fully utilized so that\u0027s that\u0027s one thing I\u0027m definitely curious to see more if you have data on this that you clearly you do I\u0027d love to see this have you published this would be wonderful to have this data I just say that Nina the presentation coming up which might address some of these points yeah I think we should try and get to that so thank you very much let\u0027s let\u0027s see if we managed our first remote presentation I don\u0027t know how to do this can you guys hear me yeah we hear you oh yeah okay can you see me you see what I\u0027ll be showing you yeah I think I should probably yeah yeah Neal are you in a spaceship I wish I\u0027m actually in New York and it\u0027s it\u0027s past 1:00 a.m. here so if I doze off will somebody yell at me nothing happens yeah gotta go to a presentation molds "
  },
  {
    "startTime": "00:43:04",
    "text": "slideshow does it do anything I hope no it doesn\u0027t all right just a second just a second almost there Mira yeah Shawn meringue options yet what they showed an arrangement color comic arrangement mirror this place that should be it yeah hey good enough I hope looks good alright alright go ahead sorry thank you yeah so my name is Neil Cardwell and I\u0027m gonna give a quick update I guess I just have ten minutes on recent work we\u0027ve been doing at Google on some updates to the VBR algorithm this is joint work with the folks you see there yeah so we\u0027re going to start out with a quick review of super quick review of PPR version one we\u0027ll make a quick sense Roland did a nice job of summarizing version one then we\u0027re going to dive into some recent work that we doing on BB our version two and then we\u0027re gonna focus our discussion on the behavior and shallow buffers since obviously you know this has come up in in Roland\u0027s talk and in the Q\u0026A session afterwards and then we will have a quick conclusion next slide please so the motivating problem here for bbr as has been mentioned is that you know there are these issues of philosophies congestion control if the packet loss comes before the congestion which can happen in shallow buffers and with bursty traffic or random loss then your loss based congestion control can get low throughput but on the other hand if losses come after congestion which happens in deep buffers you get the problems that people have called buffer blow with high delays next slide please next you wanna let him take questions is it okay to take questions now this is fine now Boston Bob Frisco yeah sharply loss alone is not a good proxy detect to detect congestion whereas the bah-bah-bah big where was the evidence but don\u0027t actually week it is just made of meat meat for us so so bbr is basically a new kind of approach that builds an explicit model of the network path and version one had just the "
  },
  {
    "startTime": "00:46:05",
    "text": "maximum bandwidth that\u0027s been recently seen in the minimum round-trip time and it uses that model to control its sending and to try to vary its pacing rate and see went to try to explore the network and build feed samples to that model next page please so you know just a quick summary of where where we\u0027ve been so far so with VP our version one we deployed it to Google for TCP and quick traffic on google.com and YouTube and when backbone connections the code is available in Linux TCP and quick and there\u0027s also active work on bvr going on at Netflix and we\u0027re in close communication with that team there\u0027s some internet drafts discovering describing the algorithm and we\u0027ve presented it ITF and there\u0027s a sort of an overview paper in the communications of the ACM next slide please so currently we\u0027re working on improving the VBR algorithm and there the effort is sort of continuing on multiple fronts one of our big of embassies is reducing the packet loss rate in shallow buffers which has been discussed nicely today this means that we were trying to tune the algorithm so that I can handle both deterministic and stochastic loss in a reasonable way and alongside this steady state improvement we\u0027re also working on a faster exit of the start upload in the presence of packet loss we\u0027re also working on reducing queuing delay in general but you know especially that helps in deeper buffers with an approach that essentially paces at a lower rate below with a game below one pacing game below one to try to pull the in-flight down closer to the estimated VDP on a more frequent basis say once per gain cycle of eight round trips and Randall Stewart came up with a nice name for that nice buzzword rating to target we\u0027re also working partly through the previously mentioned mechanisms of reducing losses and reducing queuing delay but also through some other mechanisms we\u0027re working on improving fairness both between PR flows and with lost based congestion control that\u0027s sort of a lengthy topic so we\u0027ll have to get to the details on that another day another IETF another area where we\u0027re working is on improving the the VP our throughput in Wi-Fi and cellular and cable networks where we see all sorts of very common act aggregation and decimation features and the work there is sort of in in two areas one is improving the bandwidth estimator for this case and then the other is making sure we\u0027ve provision enough data in flight by modeling aggregation and we\u0027re seeing pretty decent results with the latest Wi-Fi LAN testbed increasing "
  },
  {
    "startTime": "00:49:07",
    "text": "bandwidth from 40 megabits with version one to 270 with version two you and we\u0027re working on improving behavior in the data center with lots of flows and basically the goal is to use PBR for all Google TCP and quick traffic whether it\u0027s data center where public Internet next slide please so we\u0027ve deployed a couple smaller scale changes in the Google\u0027s network so far and those are running in production to see how those behave so far they seem to be doing fine next slide so just a quick comment on the sort of the major issue we\u0027ve been focusing and that has come up today a lot so bbr has a known issue with paper and cello buffers which roland discussed and some depth and you know the real cause has to do with interactions between man with probing and then with estimator and you know core part of this is basically that the bandwidth probing is based on a simple static proportion of the model parameters so we go at one point two five times estimate bandwidth once every eight round-trips and you know this was based on a set of competing trade-offs among real-world issues that we saw in you know real deployments you know cell systems often have dynamic bandwidth allocation so they need a big backlog whereas shallow buffered switches with lots of bursty traffic they need some compensation for that stochastic loss so there are a couple of different trade-offs going on here but basically for BPR version one we we M for simplicity and robustness and the cases that we were deploying it in so it we had a sort of one-size-fits-all static probing system the bbr version two is is going to use a dynamically adaptive approach that all discussed briefly here so next slide please so there are a couple different changes here that one I would say they\u0027re basically three main buckets into which these changes fall the first one is a generalization and a simplification of the the long-term bandwidth estimator which was previously only targeted scenarios with polices but we sort of generalized to apply to fast recovery in general and here the bandwidth estimate is a windowed bandwidth average over the last two to three round-trip times the second big changes is some new algorithm parameters to adapt to shallow buffers not surprisingly we have a an estimate of the maximum amount of data that we think we can safely inject in-flight into the network before we think we\u0027re causing loss and second we have the current volume of data that we\u0027re going to use the next time we probe the network to see if there\u0027s more capacity and you know it starts gently in one packet and then doubles upon success to rapidly explore if more bandwidth suddenly becomes available and then the "
  },
  {
    "startTime": "00:52:08",
    "text": "third big piece is a sort of new full by pipe and buffer estimator that uses a loss rate signal current iteration is just using something simple looking at the loss rate over the scale of a round trip being 5% but we\u0027re looking at other alternatives as well and once we once that estimator estimates that the pipe and buffer are currently full it takes some steps that will be surprising to people in this room we sat at the estimate of the maximum safe volume of data that we can have in flight to the current flight size then we do a multiplicative decrease of the in-flight cap to 0.85 times that estimate and then we wait that sort of backed off level for an amount of time that is currently a sort of scaleable function in the 1 to 4 second range as a function of the estimated bandwidth to give us a sort of RTT fare mechanism that also can reap robe then within a sort of reasonable amount of time and you know this also gives us a sort of design curve that we can adjust to trade off you know how gentle we want to be on Reno and cubic versus how reasonable we think it is to wait say the way cubic waits 40 seconds for repo being bandwidth if you if you\u0027re running at 10 gigabits say or even 18 seconds if you\u0027re running it at a gigabit and your rtt is on our milliseconds anyway so that\u0027s a discussion for another day but this is continuing work next slide please so this just is a scenario to give you a sense of how this behaves within the v2 approach so this is um you know 100 megabit path 100 millisecond RTT very shallow buffer it\u0027s just 5% of the BGP and this is just a 60 second Volta transfers with six flows this is staggered entry times so this is sort of comparing cubic PBR version one and then the current iteration of VB our version 2 with throughput on the left you know you can see that basically cubic has some nice fairness properties the barrel version one does not of the level of fairness that would like but VBR version to as a pretty decent fairness and sort of approaches cubic and then there you transmit rate you know cubics is nice and low less than one percent BB are version one as has been discussed is pretty high here it was 15% and then PVR version to the current iteration as a loss rate that\u0027s about an order of magnitude lower than PPR version one that\u0027s in the neighborhood of 1.3 1.4 percent next slide please and this is just a picture to sort of give you a sense of the the convergence properties and the fairness properties of the various flavors here so we\u0027ve got "
  },
  {
    "startTime": "00:55:08",
    "text": "cubic on the top PPR version 1 and middle and then the current iteration of the API version 2 bottom you know that you can see basically PV original ones fairness issues there and then you can see that cubic and VBR version to do a nice job of leaving a little bit of headroom there in terms of idle space so the new flows can come in and discover the available bandwidth and you can see that you know it\u0027s cubic and VBR version 2 are not constantly riding right up against the 100 megabit mark so that they can keep the loss rate nice and low alright next slide please so that was just a quick discussion of PBR version 1 and then sort of some quick taste of whom PBR version 2 and continued work continues on both the Linux TCP implementation and the quick implementation at Google and as I said there\u0027s work some nice work on the way for PBR in FreeBSD TCP Netflix and of course we are always happy to hear more test results and more research results as we continue to work on PBR on our end as well so yeah so I guess next slide and then any questions yeah questions Bob okay do you wanna say it again all right again this pub again yeah the question on where did the evidence come from that loss is not a good estimator for congestion I don\u0027t think we\u0027ve seen any evidence for that and I think that\u0027s sort of one of the underlying problems here in that there\u0027s this sort of prejudice against using it where\u0027s it\u0027s all information that you need to have I know I know you\u0027ve you\u0027ve started to use it here but you haven\u0027t really used it you\u0027ve just sort of tried to avoid it so I wouldn\u0027t say they were trying to of just trying to avoid it I mean what I think we are trying to use the law signal here but anyway back to the question of about what we think about loss and in its relationship to congestion so the the way I think about it is that you know as I said early on in the slides it\u0027s very common for example for buffers to be very deep you know the well and often discussed buffer blow problem so that\u0027s one end of the spectrum at which you know because you have this deep buffer it gets once it gets it\u0027s congested sense really "
  },
  {
    "startTime": "00:58:10",
    "text": "once the the pipe is full and you have a little bit of queue but there\u0027s the packet loss doesn\u0027t happen until the queue gets way out of control and quite a bit too long and so you know that\u0027s one case in which packet loss is not a good proxy for congestion because congestion happened very early and then the packet loss happened when the queue got to be seconds bigger um so that\u0027s that\u0027s one and in the spectrum that we definitely obviously see it\u0027s it\u0027s well described in many contexts the other situation where we see that there\u0027s this real disconnect between packet loss and the point of congestion is in high speed shallow buffered winds so if you have a network that you build out of commodity switches and let\u0027s say they\u0027re operating at ten gigabits or faster and let\u0027s say that your switches have you know some number of megabytes of buffer memory because they\u0027re just commodities which is you know they\u0027re what you can afford for your look large hide speed network so in this case if you have a couple of megabytes of switch buffer and you\u0027re going at ten gigabits or faster than you know often in these scenarios then you have just one or two milliseconds of buffer space essentially and if your round-trip time is say 100 milliseconds or 200 milliseconds which can obviously happen then you can huh you you can and we do see in Google\u0027s high-speed who had backgrounds that you get these scenarios where a flow is operating with 100 millisecond round-trip time and during that hundred milliseconds there can be all sorts of little bursty congestion episodes where that two milliseconds of buffer fills up and then it drains and then it fills up and then it drains and you\u0027re gonna have any number of little bursty congestion episodes during your RTT and if you use lost traditional rien or cubic style las based congestion control then basically operating in that buyer environment every round-trip time you would get a signal that you interpret to mean slow down and no matter what you do you you get a signal to slow down but meanwhile there could be that link could be 90% idle because maybe there\u0027s 90 milliseconds of total silence and only 10 milliseconds split up into little chunks here and there or that 2 millisecond buffer was full so this is a kind of the other kind of environment that\u0027s important at least to us and it\u0027s an environment where packet loss is not closely related to congestion sorry for the long answer but yeah I just said I cut the line off the steward so I appreciate this discussion but listeners keep it limited so in that case where you\u0027ve got these little episodes of loss during a round-trip time I think what baby I was trying to do by effectively trying to ignore that it\u0027s it\u0027s holding you know if they are "
  },
  {
    "startTime": "01:01:11",
    "text": "other flows coming and going that the short flows if you like then it\u0027s holding that the buffer high and then they are taking it higher rather than trying to respond slightly to them so there\u0027s a bit of buffer for those small is coming in in other words all these all these the assumption that that loss is not really loss or it\u0027s not really congestion means that baby are is holding or going at a certain delay level in the in the buffer or higher rather than trying to go slightly under to allow all these other little things to happen well it\u0027s an it I think that\u0027s a fair description of the VR version one I think as you probably saw in the presentation we\u0027re trying to incorporate the kinds of considerations that you are mentioning right now so I think I think we are on largely the same page as far as that particular issue is concerned we don\u0027t want to you want to respond to the bandwidth that\u0027s actually available to the flow and we want to use loss as a signal to do that in addition to the delivery rate and the round-trip time of the path you know yeah and remember I just wanted to add a comment to one you\u0027ll see a bit um actually I you know with two one is that um it\u0027s not like inside Google we don\u0027t care about packet loss because it costs us the students a substantial amount of RAM and the cost of that is quite significant and the second thing is that those aren\u0027t the only environments where you get non congested loss I mean you know the the other classic case is Wi-Fi and there are things that happen in lots of networks in shall we say poorer in Rome or remote parts of the world where loss is where the overall loss rate is some kind of indication of congestion but it\u0027s not a reliable indication of congestion in the way that it was designed to be in Reno so I think BBI 2 is going and by the way this is the first time I\u0027ve heard exactly what Neal\u0027s planning will give you attitude the I think the about Libya - is heading in the right direction of softening all the responses to some a reasonable control algorithm rather than being super super hard on everything and that was BBA ones mistake being super hard on those variants there wasn\u0027t the question what does the command so we can get to the next question right okay me I could even say I saw that you have this kind of threshold and they\u0027re like five percent of loss you do something so that\u0027s already great so you don\u0027t even know lost completely I think this is really important for the safety of the internet however five percent teams still hide so "
  },
  {
    "startTime": "01:04:12",
    "text": "I guess that doesn\u0027t solve the fairness problem you have with loss based congestion control right well it doesn\u0027t in itself solve the problem that\u0027s that\u0027s true but I think there are number of things you think about in there so the the the loss right there in that threshold is is well first of all this is just the current iteration of the VR and we\u0027re also also evaluating other alternatives but it\u0027s like in the last rate there is actually just sampled over 1 down trip time and and so it\u0027s a different matter than what you might be thinking of when you think about 5% it\u0027s not 5% globally across the lifespan of the flow it\u0027s just in that round-trip time and that the PDP is that most reno and kinetic flows run out you had 5% pretty quickly due to the quantization but anyway so the let\u0027s see the the other consideration i can think about is that what matters you know in cubic in terms of what they can utilize and in terms of bandwidth is not really the loss as it would be measured with primers really the distance in time between the lost epochs and I think right now or personally I\u0027m thinking about it in terms of making sure the lost epochs that BB are might create are far enough apart that we know and cubic can get decent then with as they ramp up in between those loss epochs that\u0027s you to work I guess well no that\u0027s that\u0027s in the algorithm but I guess tuning the exact relationship between current bandwidth and that interval is is something we\u0027re actively working on and I think you heard some of my thoughts about we want to be reasonably fair to these algorithms but hey they\u0027re not really fair to myself B they\u0027re not really RTT fair to themselves for sure and then see they have some unreasonably long timescales for adjusting in some cases so I think it\u0027s gonna be tricky that curve but I think now we at least have a dial in the algorithm to choose this explicitly and unconsciously and then I have a quick comment so for data senders if you wanted a Singlish loss from congestion there\u0027s a much easier solution which is colleseum yeah fair enough absolutely okay last one Stuart Cheshire Apple I know we\u0027re short on time so I\u0027ll make this brief my comment is kind of along the same lines as Bob\u0027s and sort of long the same lines as Geoff\u0027s about massive packet loss what concerns me is we have this belief that I he repeated that loss is not always due to congestion sometimes it\u0027s just random stuff that happens and and I hear this a lot from my management at Apple people who studied computer science but not networking and the one thing they remember from their networking class is "
  },
  {
    "startTime": "01:07:13",
    "text": "that the internet loses packets all the time and therefore TCP is wrong to treat that as a sign of congestion because it\u0027s not it\u0027s just the way the internet works it\u0027s throwing packets away for no reason all the time and that may or may not be true it may be that there are Wi-Fi algorithms that are over-aggressive with the transmit rate they pick and their rate had apt ation is not finding a reliable operating point but if that\u0027s true the question is what do we do about it and why answer is we put bigger buffers in these switches we improve the Wi-Fi algorithms so that random loss is not happening are quite the same level that\u0027s one approach the other approach is to say oh forget it let\u0027s just ignore loss loss means nothing anymore if you lose packets go faster power through the loss and and what I\u0027m concerned about is is sure that worked in small-scale tests but but but it but if Apple and Android were to upgrade and Microsoft were to upgrade you know basically 90% of the hosts on the Internet to be using this strategy if you\u0027ve got 50% packet loss on every link then after 10 links you\u0027ve only got one in a thousand packets getting out the other side and I worry we\u0027ll be in a situation where we\u0027re all dumping a firehose of data into the network and basically nothing is coming out the other side and that takes bandwidth on shared links it\u0027s using Wi-Fi spectrum it\u0027s using battery power it\u0027s consuming a bunch of resources for data that doesn\u0027t survive to the other side right so I I think heart basically during the talk that the issues that you were just mentioning aren\u0027t the things that definitely happen with the ER version 1 and we are absolutely taking these issues head-on and as you heard NDB our version 2 there\u0027s going to be the loss signal explicitly factored into the response of the algorithm in terms of how it estimates whether the pipe is full and how it backs off and then how long it decides to stay backed off so anything you\u0027ve ever seen - I think you\u0027ll see a something hopefully more to your liking well we\u0027ll see you want to get to the next IETF all right thanks was a good discussion and we all the time but good all right yeah this is Roland again so this is John worked with Mario Felix and Martina it\u0027s about TCP Lola which is an approach toward low latency and high throughput congestion control so the motivation is to achieve high throughput and low delay so this is typically considered as a trade-off or even as conflicting goals so we think this is not necessarily so and so we try to mitigate actually this trade-off so there are several approaches here we all know like aqm more it\u0027s weeks to existing congestion control and coming "
  },
  {
    "startTime": "01:10:13",
    "text": "with aqm workers l- back off with ECNs or that you can keep the utilization high even if you have a qm and traditional law space tcp operating on that and yeah there\u0027s also the other approach of trying out new congestion controls so we wanted to investigate how far we can actually get with the congestion control achieving low queuing delay high utilization and throughput we want to be scalable so that means that we can also use even ten gigabit per second and me on so in several orders of magnitude of bandwidth that you need to scale with we want to achieve in our TT fairness and the whole approach should work with at least and beginning with regular Ted or cues our focus was basically on wide area networks and not data center networks specifically so this is not excluded but wasn\u0027t wasn\u0027t in our initial focus so the general goal is basically if you know our point of view to determine a suitable amount of in-flight data and order to achieve high bottleneck link utilization so only on the other side we want to avoid creating any standing queues in order to keep the community lay low so what Lola does is it provides a configurable fixed targeted I value so we can say we want to have a queuing delay you know more as five milliseconds for example and on our definition of congestion we think that congestion is when we see a persistent queuing delay above the fixed target so the challenge then is basically also to achieve good convergence to fairness so that the the problem is you may end up with if you only want to achieve the first two goals without considering fairness you may end up with a total amount of in-flight data which is quite okay but you may see unequal ratios so um the idea is then that basically you need to in cry and increase the in-flight data one sender while reducing it for others and the this particular challenge is you have a small Q the flow is more or less interact via the Q and so you have less room then for interaction if you want to keep the queuing delay small so this is basically more difficult and we want to do that without sacrificing the low delay also not allowing overshooting in the queue for example and in order to appropriately inform more bandwidth and so on so the basic approach is that we use queuing delay "
  },
  {
    "startTime": "01:13:14",
    "text": "thresholds so this is showing the bottleneck buffer queue here and we define more or less the areas so one the first area is below a threshold targets are called Q low where the link utilization is basically unknown so of course that is more or less maybe measurement error or noise so we cannot be sure that below that threshold we have reasonable link utilization so once we cross that threshold we are in an area which is kind of okay because our queuing delay is still under control so we try to stay below Q target which is as I just briefly introduced may be set to 5 milliseconds for example and so we want to achieve in this green area your high throughput and low delay but once we we got beyond Q target we are half a state of congestion and we try to actually back off then so um you need to estimate the queuing delay right now we\u0027re using minimum filter over fixed time period in order to measure the standing Q and we also have a heuristic in order to adapt to any network path changes so for example if you have path change and the minimum R DT actually increases then we you need to adapt to that once a while so overall Lola is a congestion window based approach and we also use packet pacing which is beneficial for its operation in order to get better measurements but it\u0027s not necessary as for example in PPR so the flow States or the works roughly like this um we have a slow start phase which is nearly as always you\u0027re doubling sending rate Mullis so we have some exit condition where we try to exit early in order to avoid overshooting too much in the queue in through the queue then in this yellow area here where the link utilization is unknown we\u0027re starting just are like cubic to increase the congestion window until we basically cross this queue low threshold here once we cross that so the estimated queuing delay is larger than Q lower here we enter a mode which is called fare flow balancing so this is a special stage where we try to achieve fairness which is explained on the next slide and then once we actually across the the queue targets delay so when we enter a more or less congestion state each flow holds its congestion we know for some time in order to let others detect actually that they crossed all together on this state here or the the target here and then "
  },
  {
    "startTime": "01:16:17",
    "text": "this is also important on some action is trigger namely in the so called Taylor decrease so this state or this action tries to actually drain the queue completely but still keep the unity utilization high so each flow tries to reduce the congestion window by a suitable amount actually of a data and then you\u0027re going back to cubic increase and so on so we have Melissa suck so how does that flow balancing work so it\u0027s it\u0027s a novel convergence to fairness mechanism so it tries to actually eat the amount of data that each flow each flow makes you at the bottleneck and the challenge is here to also dynamically scale on the allowed amount of data with respect to be given delay charted so depends basically also on the the speed that you\u0027re operating and so the throughput and so on and the problem is that the knowledge about the current shares is actually not available so we we need to basically try to come up with some heuristic in order to achieve fair share so a flow with more currently having a larger congestion window than the fair share just keeps its congestion window so for example the green one here starts alone then second one joins here and as you can see in this gray areas here which is the face of fair flow band editing you can see that the the green one always keeps its congestion window until they more or less come together and the other flow here the flow with a smaller then the fair share actually increases its congestion we know as you can see here and this curve here is cubic in order to be scaled or in order to be scalable so we have a simple heuristic to use or calculate and lower the amount of data in the queue more or less affect you share if you will which is time dependent so over time actually the allowed amount of data in the queue increases and now the problem is then that you need to model a synchronize um at some point in time but that\u0027s a point of synchronization is basically that event here so there\u0027s the good chance that every flow will reset its its timer so the the testbed setup is nearly the same as in VBR case so this don\u0027t tell you much about that but the cue low target was the cue low threshold story was set to 1 millisecond cue target was set 5 milliseconds here Croatian time is set to 250 milliseconds and measure window actually set 40 milliseconds these values are not really "
  },
  {
    "startTime": "01:19:18",
    "text": "optimized so there\u0027s much room for improvement maybe and so let\u0027s look at the results so our two flows sharing a 10 gigabit per second so the first flow starts and is able actually the green one here is able to fully utilize the bandwidth second flow starts and then you can see that they\u0027re starting the fare flow balancing phase here and basically they achieve quite fair share here in comparison to other delay based approaches like TCP Vegas Vegas is basically not able even with what the single flow to to utilize the the available bandwidth because it basically misinterprets jitter as congestion signal and therefore it takes off towards word and so fresh air is also not achieved so trying to keep the queuing delay lower is also the second goal high throughput low queuing delay and as you can see if you\u0027ve brought here the RTT basically our single flow just say smoothly and then our second flow starts both flows do there have flow badness in face and you can see that you have always just little spikes here which correspond to the 5 milliseconds queuing delay target in comparison to that has already known cubic TCP always fills the buffer completely and this also leads to higher queuing delay in seniority so the queuing delay basically is also our TT independent we buried for example the RTT in this case to 5 or 5 milliseconds 61 milliseconds 101 milliseconds and as you can see always our Lola gist adds 5 milliseconds to the base or DT and so it keeps the target of 5 milliseconds queuing delay action so the queuing delay is independent of the base Rd T and also the rate which is not shown here and also with respect to the number of senators which can be seen in the next slide so we have several flow starting here in succession in this case there were 18 flows just for clarity I show only two flows but it\u0027s it\u0027s nearly the same for the others so in in this case even while we have several flow starting your Lola still able to control the overall queuing delay and basically produce no packet loss so in contrast to that comparison to PPR for example PBRs flow starts in succession you can see that bbr first fill the buffer completely even overshoot some times and then yeah that\u0027s so this is measured basically at the sender and it also produces several or retransmissions here it\u0027s just you "
  },
  {
    "startTime": "01:22:18",
    "text": "can see so what about RTD fairness fairness can be even achieved by a Lola in the sense that if you have two different flows with different oddities they are able to get a fair share nevertheless and this is showing a small buffer scenario so you still converge to fair shares even though they have different our duties 20 mile 21 milliseconds and 100 1 millisecond basically so in comparison to that cubic isn\u0027t able to achieve fairness here because it just converges through similar congestion windows but the the flow with a larger RTG needs to have more in-flight data and so the the flow with the larger RTD is enable basically to completely get a larger share here so this is just showing first test of the overall concepts so this was published on the LCN conference last month so the the parameters are not thoroughly optimized yet it\u0027s not a full-fledged PCP brine so you just are looking into several aspects and so there\u0027s room for further investigations like use a one-way delay instead of our TT measurements right now we are using the rtt but it\u0027s also easy to use one-way delay and said in order to get rid of any disturbances on the reverse path and we didn\u0027t look into the issues of delayed and compressed x furthermore we don\u0027t know how it behaves in wireless environments also multiple bottlenecks enarans may be a problem and we also did some work on coexistence with loss based variants but we don\u0027t want to build in any compatibility mode but instead use separate queues for example or maybe also a queue so what a plan is not to rely on indirect measurements of queuing delay but instead use explicit feedback from the network and so we are doing more research in this direction thank you welcome all right thanks for being faster than planned and any questions I didn\u0027t want to cut the baby at least and you get right in the meantime me a cool even I could also talk to Ronan data but anyway so I like the fair queuing approach that\u0027s really nice and I just want to come and say he gives the minimum the overall menu amount of time I\u0027ve seen but I think you actually you try to make really hard sure that you empty queue from time to time right so you\u0027re really actually try to you know when you think because empty then taking you minimum round-trip time example that could probably improve your isn\u0027t over time and various scenarios to make it "
  },
  {
    "startTime": "01:25:19",
    "text": "more stable it\u0027s my experience I mean it does actually so is it empty secures and always and when it crosses the the target so that there\u0027s this Taylor decrease for you just I mean just make sure because you might have route changes or whatever that you actually not take the overall minimum but because you know that he I\u0027m dzq Jets take that many more at that point of time yeah it\u0027s small comment yeah all right well Thanks a good afternoon I\u0027m David Hayes with Simula and into the pink box please sorry I\u0027ll be talking about work I\u0027ve done with a number of other people and the last two especially large Eric and Hugo have been implementing this work into Linux what we\u0027re looking at is the problem where you have large amounts of data that have to be transferred over the internet they don\u0027t have to be transferred as fast as possible so perhaps we can send them in a lesson best effort way but they also have a timeliness constraint next slide and because of that they often have to be treated completed by some sort of soft deadline we\u0027d like to have a sort of a deadline aware less than best effort now what that is next slide is it\u0027ll have these sort of qualities it\u0027ll be something that keeps disruption of current best effort flows where things and other types of traffic to a minimum so do good and react earlier to congestion than a normal best effort flow be pragmatic in that you still want to meet a deadline and if things are bad then be more aggressive but still do no harm and never be more aggressive than a best-effort type flow next nice now our approach was to model this first to get an idea of the dynamics and then we\u0027ve developed our framework a way of doing this that can adapt in principle any way any underlying congestion control algorithm to to do this we have a publication there that you\u0027re welcome to look at next thing now our quick overview probably the quickest you\u0027ve ever seen of a network utility maximization you have centers and receivers through the internet and one of the ways to model this is next slide by saying that the sender\u0027s try to maximize their utility they get more utility for particular send right send rate but the restriction is that they can\u0027t collectively exceed the capacity on any link through it along their path "
  },
  {
    "startTime": "01:28:20",
    "text": "ok next slide things now it very nicely happens that if you work with the Lagrangian dual default solve this optimization then it can be solved distributive lis and it\u0027s sort of looks a bit like TCP where you have a price but the price is not dollars and since in this case it can be lost ECM delay or other congestion type ways and you do a calculation based on your price to get the rate you send sort of a bit like what TCP does on average ok next things now if we want to make this less than best effort there are really two ways you could do this one way is that you could change the utility function to make it less invest effort and that\u0027s the equivalent of changing the congestion control algorithm now the other way you can do it is you could inflate the price okay so if you inflate the price for one congestion control algorithms and it will think there\u0027s more congestion than there really is and send slower so that\u0027s the other way and that\u0027s the way we\u0027re looking at in more detail here next Thanks ok just in price inflation so you have the sum of the total price that you\u0027ve got for your whole link which could be your RTT or could be a loss or your ecn signals and we weight that by a number between not quite zero and one to get an inflated price if the weight is one then it\u0027s best effort if it\u0027s not one and less than one then we have a degree of lbs lbs lbs next thanks now to control that price with respect to the deadlines then at a slower at short time scales the congestion control will just react as normal just work as it normally does to the inflated price but at longer time intervals we want to keep the idea of the deadline involved so we will adjust what that weight is relative to the deadline and we\u0027ve experimented with a couple of ways of doing that a PID controller which is not too bad once you sort of configure it because you\u0027re always controlling something between zero and one and one I\u0027ll be showing here is a model based control because a lot of TCP algorithms have good models describing their average behavior so that\u0027s one of the ways we\u0027ve looked at next links okay the first thing we did was apply it to cubic why this cubics "
  },
  {
    "startTime": "01:31:22",
    "text": "not the best for this yes s we know but um if you have a network where most things are behaving like cubic then it\u0027s like with like two ways you could do it inflate the response or inflate the price that\u0027s the indirect way we tried this just because just changing the beta seems easy it turns out that\u0027s more complicated to do in an actual Linux kernel just for one particular flow sorry what we did do is look at inflating the price you could say we drop extra packets but that\u0027s not very nice it\u0027s like shooting yourself in the foot instead we introduce concept called phantom ecn signals that allow the lost space protocol to react in a ecn like way but without actually losing anything next thanks now as a simple test scenario we\u0027ve set up six cubic TCP flows starting and stopping of two note in a short interval between 1000 and 1000 and 10 seconds there\u0027s nothing except the background traffic which is 10% and then the deadlines aware less than best effort flow if it could send at 10% for the whole time to the deadline that\u0027s all it would need but of course we want it to adapt to congestion next so this is what we get when we try to vary beta and the reason why that doesn\u0027t work so well and doesn\u0027t give you a good amount of El Venus as such is because Cuba\u0027s quite aggressive and loss is not a very regular signal okay so rare a signal so cubic is rapidly ramping up again now with phantom ecn next we get a better degree of lb eNOS you could say however one problem with this is how do you know when congestion goes away well you know because there\u0027s no loss but loss is quite rare and it takes you a while to work out that congestion has gone away so as a result at the time when nothing else is happening this sort of thing can\u0027t actually take advantage of the sphere capacity that\u0027s there now if we weren\u0027t using loss to detect that congestion was gone away about something like delay we could probably do a little bit better so we chose next slide - to do this with Vegas eventually um because it\u0027s a common delay based protocol that everybody\u0027s quite familiar with but the problem with having something like Vegas working a network where all the other flows may be something like cubic is especially when the deadlines approaching how does Vegas ramped up enough to compete with cubic how do you "
  },
  {
    "startTime": "01:34:23",
    "text": "make this congestion controls where one\u0027s working with Lawson and others working with delay actually worked together and Tang and Co actually looked at this when they\u0027re trying to make it work with fast but it sort of half worked and they had to put a little fudge factor in well quite a big fudge factor to make it work but we\u0027ve sort of extended that idea to do it a little bit better next things and what we do is we build a composite congestion sort of parameter based on lost delay and ECM and weight it and it\u0027s a weighted probability we calculate weighted by the particular congestion controls reaction to congestion so cubic it might have a beta of 0.7 or point eight something like Vegas its plus one minus one but what\u0027s it well if your window size is two minus one is 50% but if your window size is 100 minus one is 1% so relative to that but Vegas also reacts to loss so you\u0027ve got to handle the both of them as well Thanks you\u0027re getting really good okay so next slide so what we do is we inflate like we did before we\u0027ll inflate the delay part but we have this extra parameter Phi which gives us sort of a balance between the aggressiveness of cubic and Vegas at that particular time so we\u0027re measuring the relative reactions to congestion and we inflate that now the lost part because Vegas is reacting to both of them when it\u0027s really congested and we\u0027re getting near a deadline and we want to compete a bit more like cubic then we probabilistically ignore some losses there okay but we are never in the end more aggressive than cubic because we keep track of what a loss base flow would be reducing relative to what we\u0027re doing now this is example of the Vegas face one now it\u0027s less than best effort um it is reacting on short timescales to the congestion that\u0027s happening when there\u0027s nothing at all it can take all of the capacity which is what we want to be able to do take advantage of empty capacity and well in this case it\u0027s slightly misses this deadline now we could adjust our control to make sure that didn\u0027t happen but in our case it\u0027s a soft deadline and we\u0027d rather be nicer than then more strict to the deadline in this example okay next thanks and next again this now "
  },
  {
    "startTime": "01:37:27",
    "text": "a little more ambitious experiment where we set up in this case trying to look at more realistic traffic next thinks so we used timox and based on some real traffic traces so ended up with hundreds or thousands of concurrent short and long and all mixture type TCP flow starting and coming all of the time and then had our competing deadlines aware less and best-effort flow with that next thinks and we did a bit of work making sure that it was all stationary so we could collect proper statistics for it next now this is one of our summary statistics I\u0027m only going to show one where we just looked at what your completion time is relative to the offered load that\u0027s not the actual load because the actual load will depend on retransmissions and other things because it\u0027s TCP happening there and what you find is that the cubic base one because of the phantom essence and not being able to take advantage of all the capacity um yes it always does quite good as far as the deadline goes but when there\u0027s not much happening it can\u0027t actually send our Bob transfer as fast as it should be able to and do it without disturbing the other traffic the vegas-based one actually can and some of our other results which you\u0027ll have to wait to the next publication actually show that the impact on other types of traffic the other traffic happening at the same time and so on is very minimal with the Vegas and more with that and much better than if you have say a normal best effort flow and we also look at fairness and some other measures ok next things now where we are at the moment is we have our mostly working stand-alone version of this in linux part of it involves some kernel modifications which /eric has been doing and then we have what\u0027s called a Lib da lb module which the application talks to at the beginning to open a connection to tell it how much data it wants to send and what the deadline is and then that opens the connection but the application talks directly to the kernel as it normally would when it sends data in the background this meta control is collecting stats from what\u0027s happening in the kernel what the loss what how many packets lost our what the delays are and so on to calculate those parameters so it can adjust through the control the dynamics of whatever underlying congestion control we happen to be using the examples we had was Vegas and cubic but it can be applied to "
  },
  {
    "startTime": "01:40:27",
    "text": "any of them Thanks next next step is we\u0027re in the neat project which you\u0027ll find more about in the taps working group but we\u0027re going to put this as part of the neat framework so neat sitting over the top will when an application opens wants to open a connection neat we\u0027ll have a look at what congestion control algorithms are available which one might be the best one for deadlines aware less than best effort to adapt if there\u0027s a delay based one that\u0027s usually better if not we\u0027ll just use cubic or something and well sorry back one step and choose it underneath you can read more about it there and um in the taps group more about NEET next things so this is how it works in neat so we have the neat library applications to talk to neat but there\u0027s all these fancy policy manager stuff and we keep statistics on what\u0027s happened for previous flows so we can choose better and so on and we still have the same bit in the kernel Thank You next okay this is the last one will show you no need to go after that no need to go to the next slide so our deadlines are we\u0027re less than best effort mechanism it is for Bob type transfers where you want a complete within some sort of loose deadline but needn\u0027t be nice to all the other traffic that\u0027s flowing hey it\u0027s sort of based on the network utility maximization idea we inflate prices we tested it with cubic in Vegas but in principle it could be applied to any type of congestion control algorithm working as a meta control and our ongoing work integrating to neat we have some large-scale tests which we\u0027re working with one of our partners Dell EMC EMC across the internet with some live tests as well okay thank you something question anybody I\u0027m very sorry for this actually you know we should have asked for a longer time stop but then the presentations that came in were too good and so I sent emails to people asking them to be short which isn\u0027t easy but all right it reminds me to go to all days of you know too many presentations coming here which is good thing I\u0027m going to talk about LED back plus plus is a low priority TCP congestion control which we have actually shipped in Windows for several scenarios and extract these the background so an anecdote "
  },
  {
    "startTime": "01:43:27",
    "text": "so basically windows crash dump upload basically took out in-flight Wi-Fi so that\u0027s how we started looking into this problem so our goals were that we want to do less than best efforts our service but we don\u0027t want to interrupt foreground traffic like interactive web pages right cause and things like that we basically did some literature survey and then LED back seemed like the best solution because it ramps up and there is no competing traffic and yields to foreground flows but we did find problems with LED back which led us to make some improvements quick recap so let byte works by measuring the base delay and then it adds its own target delay on top and it basically if the delay is less than target then there\u0027s an additive increase the delay is higher than target then there\u0027s an idea to decrease no particular strict requirements from slow start and it reacts to packet loss just like standard TCP next right please so these are the problems that we found so one way delay measurements which was required by the RFC was is hard to do with TCP so there is no standard clock frequency or synchronization and there there\u0027s there are cross skew problems there is a late comer advantage problem because it\u0027s a delay based condition control so flow that arrives later measures a higher base delay and it basically gets a higher share of the network there is also internet but fairness problems if you have too low priority connections competing then essentially there is a stable queue but they don\u0027t each get this fair fair sure even if their base delays is measured as the same the recommendations around slow start were vague so that was another thing we thought was a drawback with the RFC there\u0027s also a latency drift problem that we noticed which is if you relate the lead back connection to run for a long time like longer than 10 minutes we noticed that the base delay would actually keep increasing because of the queue that was built up by the led bad connection itself so we would notice this latency ratcheting problem which would keep increasing and then there\u0027s the low latency competition problem which is if if the if the queue never builds up if it\u0027s a fashion of network connection then led by basically behaves like standard TCP and doesn\u0027t be able to standardise can we go to the next slide please so basically we did these essentially five changes instead of one by dealer measurements we use round trip latency measurements so slower than reno conditions will do increase so the gain factor we actually made it adaptive versus using a fixed value instead of additive decrease we do a multiplicative decrease so and then modified slow start and then initial and periodic slowdowns so that we can more accurately measure to the Bayes delay this has been shipping since the Windows 10 anniversary update it is in use by the error reporting service as well as peer-to-peer Windows Update we are "
  },
  {
    "startTime": "01:46:27",
    "text": "working on making this API public and the configuration as well next slide please so what are the so the advantage of round latency is that it\u0027s already available in TCP there\u0027s no need for clock synchronization this disadvantage obviously is the receiver delays and the deal attacks so we did some mitigations here we enable the tcp times to have option implicitly we filter the RTD samples just as recommended in the RFC to the minimum of the four most recent samples and we use a target delay of 60 milliseconds which is different than the one recommended in the RFC which was 100 milliseconds or less the we basically picked a value that was the typical server-side delayed AK and we found that 60 milliseconds works much better for Skype versus hundred milliseconds which covers 2/3 of the budget for a good quality wide connection next slide please slower than Reno so essentially instead of growing the congestion window in the current avoidant phase like standard TCP we basically introduce a factor which is of makes the condition window growth a fraction of standard TCP essentially F we experimented with fixed values for F but it never worked out very well for connections that are a wide range of base delays we find that picking an adaptive scheme actually works much better so essentially we we find that 16 is a good trade-off for essentially low latency connections to not eat of eat a large share of the bandwidth when the when the latency is low so this essentially solved the low latency competition problem so I\u0027ll be showing a bunch of graphs later which explained this in a little bit more detail next I please multiplicative fricative decrease was proposed in a paper but we found that just implementing a multiplicative decrease factor was not good enough to solve the fairness problem essentially if to let bad flows don\u0027t have the same base delay then just introducing matically gate of decrease was not good enough essentially we had to cap the multiplicative decrease coefficient to be at least point 5 for it to be effective we also had to ensure that the congestion window never drops below two packets next slide please so a slow start this is interesting so we did not want to skip slow start because skipping it we found that it then when there was no competing traffic let backhoe trample like that plus plus would ramp up very slowly so we still do a slow start but we essentially also apply the adaptive rate reduction factor to the congestion window increase so that the slow start for our let bat plus plus flow is actually slower the ramp is slower than a standard TCP we also limit the initial congestion window to two packets we also exert the initial slow start if the queuing delay becomes 3/4 of the clarification question you say you you "
  },
  {
    "startTime": "01:49:29",
    "text": "add the reduction factor to the increase yeah so what\u0027s the increase at that point what\u0027s the increase that you do so essentially it would be similar to the the multiplicative decrease light that I showed before so it would be a fraction of F and then yeah we the exit slow start early if we find that the queuing delay becomes 3/4 of the target which is extremely second value that I described earlier but we only apply this exit for the initial slow start because for subsequent slow starts we do have an SS Thresh value next slide please so we had this latency drift problem and the way we solved that was by introducing initial and periodic slowdowns so what we want to do is we want to actually yield to the network to figure out what the accurate based delay is this is especially important when you\u0027re LED pad plus this connection is going on for a long long time so to force these gaps essentially the RFC said that there will be automatically because of the person as there would be periods in the network where it will be measured but it practically when we did these experiments we found that that didn\u0027t work very effectively so we introduced this notion of a slowdown so it\u0027s basically an interval where the led by plus plus connection voluntarily reduces its condition window down to two packets for 2 RT T\u0027s and then after to our titties we again do slow start until we reach the previous ssthresh value the initial slowdown starts twice the RTT after the first slow start ends and then for periodic slowdown what we do is we measure the duration for the connection to have ramped up from the after the tour duty slow down phase back to the original ssthresh value so we measure the duration and then we multiply that value by 9 so that effectively it becomes only a 10% drop in throughput for let\u0027s say there was no competing connections then we would essentially see only a 10% drop and through this is actually a configurable value but we found that 10% to be an acceptable drop we found that this solved the legislative problem next slide please I\u0027m going to go over a bunch of measurements so this is basically just showing that standard TCP when there are other short flows it basically doesn\u0027t healed and this the graph below shows that so the purple graph is basically your short flows and then the red is like that plus plus so as soon as short flows ramp up led by plus plus very quickly yields and it\u0027s able to ramp back up when there are other no competing flows next right please so standard TCP again this is the problem so slow start bends up a bunch of queue sometimes up to one to two seconds these graphs are not in the Train science same time scale so for like that plus plus we see that it achieves it\u0027s essentially stabilizes around 120 milliseconds or "
  },
  {
    "startTime": "01:52:30",
    "text": "fluency next light please so this is the ratcheting effect that I was talking about standard like bad over time essentially the period of 10 minutes would keep increasing the base delay so the bay essentially the idle a would keep on increasing so over time we find that it goes from about 120 to like 180 and then over 200 milliseconds just applying the multiplicative decrease as I described below didn\u0027t solve this problem we had to actually also cap the coefficient as well as introduce the periodic slowdowns and then we see that they\u0027re able to keep the we don\u0027t see this problem of phase delay continuously increasing for long-running connections next slide please standard like that yes there is there is a late comer advantage problem so the yellow flow essentially came in later and it basically took over pretty much all of the bottleneck bandwidth because of its higher phase delay measurement led by plus plus as you can see the green flow essentially starts later after the purple flow and essentially end up fair sharing the last graph here is both of them essentially accurately measure their base really and it\u0027s the same value next slide please this is the low-latency competition problem so we if when we run standard TCP with LED back fill as plus but we keep a fixed reduction factor we see that standardise be basically only gets 2/3 of the bandwidth this is the case where for example the latency is like 1 millisecond or 10 millisecond very low value on the page delay but once we introduce the adaptive value in this case because let\u0027s assume the latency is less than 10 milliseconds we would end up with f-16 and then we find that led by plus the seals and more majority of the bottleneck bandwidth goes to standard TCP next slide please so yeah in summary we found a bunch of problems with led bat as described in the RFC so let bat plus plus is an attempt to solve those problems and our experiments show that it achieves what what we set out to do it\u0027s already deployed on millions of systems for the scenarios that I pointed out earlier we\u0027re working on making the API and the knob public we are working on a draft submission I had a question here to the room whether it should be to ICC RG or TCP M as chairs our agreement our our view on it is that this should be our G because I mean and we\u0027ll fight for it ok questions Michael speaking as one of the "
  },
  {
    "startTime": "01:55:30",
    "text": "other chairs fighting for it I mean it\u0027s a fair question to ask but in this specific case I believe the submission could also be TCP M what would happen in that case is that you would forward it to ICC Archie to discuss and so at the end of the day it would probably be discussed here again conceptually for that specific document the mission could target TCP m at the end of the day in my opinion angourie Faris\u0027s TS v WG chair which is a guess the other possibility again if you sent in the first we would send it back here same answer so and I don\u0027t think it matters where you do the adoption if it\u0027s useful to discuss it discuss it rattling set draft a bit my encouragement and then talk about details later I don\u0027t know how many of you remember my tears V area presentation explaining with a happy trash can off the IETF but it\u0027s our role it\u0027s always been our role so you know bring us the stuff first and we\u0027ll deal with it until it\u0027s ready for you guys see that yes I will actually fight for this and I specifically I say that because it\u0027s a condition control it\u0027s a permission control and I see crg has condition control and it\u0027s darn named but but let\u0027s not litigate that here I do have a one particular bit of feedback it seems like there are there are two separable pieces in the work that you\u0027ve done and this is excellent by the way thank you so much for doing the work and for bringing it here this is very very valuable and useful some of the points you mentioned we knew early on when we were doing that badan it was like like the stuff about you know the we were expecting there to be empty quiet periods in the network we didn\u0027t know but it\u0027s definitely good to know that drift is a real thing and but bbr also it\u0027s similar that is definitely yeah we did notice that the RFC actually talks about these problems even entered like by fairness and alaikum all of those things are right talked about but we didn\u0027t have didn\u0027t find a good solution so we have to go build one so I think the separable pieces are one of them is actually a direct update to the Redbird algorithm itself and the second one is making that but making that but useful with TCP and I know that folks at Apple have implemented LED bad with TCP as well so there may be I think there may be value in separating this into algorithmic part and how do you make it work with TCP and those two pieces could be in different places that\u0027s just a bit of input but but I yeah I I would like to discuss the question of where it belongs maybe not right here thank you just for the record I\u0027m cutting the line after my trick shot off just to be sure a lot of cigarettes so the original that Pat was experimental but done through the ITF track and since it\u0027s been seeing quite a bit of deployment right and and there are fixes "
  },
  {
    "startTime": "01:58:33",
    "text": "that are looking very interesting and they made a based on actual running this thing I would hope that we could make the follow on standards track but the original that seems to have problems that maybe we don\u0027t want to do it with that but this one might be right and if we\u0027re doing stylus track it means meaning means it needs to be done to the ITF remind me to not have beer at lunch again on Monday so that still means that if so I like the splitter China proposed as a possibility right that means that your algorithm apart doesn\u0027t necessarily need to be standards track but it might also be I mean going on a little bit of a rant here right we have done that there\u0027s an HTTP as informational we\u0027ve done cubic as I think informational experimental we\u0027re now talking so cubic isn\u0027t even publish it we\u0027re already talking about adding into the down registry so that our standard track Alice\u0027s can can point to it so we I think we are too cautious with doing stuff on standards track so I\u0027ll quickly respond to that with the chair head on I don\u0027t think that condition controllers need to be standardized I said this in the past but that\u0027s my short response yeah Spencer Dawkins transport Area Director the I\u0027m happy to what you all work this out and try to be as helpful as I can be the thing that\u0027s probably worth me saying here is for asking you all to think about who else would see the work in the IETF and talking people in other areas if that\u0027s a concern you know if there\u0027s a consideration and it\u0027s worth considering if it\u0027s not a consideration figured I said that I am curious how much more work work work you think needs to be done here I mean like do you think do you feel like you\u0027re through or so one of the problems that we haven\u0027t soiled yet is working well with aqm now we do have some solutions here which I have not talked about in the slides but we that\u0027s a work in progress so that\u0027s the only part that remains to be figured out as far from our site but of course there could be other feedback from the community and we\u0027d be happy to address any kind of you know gaps here cool if I\u0027m if you\u0027re thinking that there is still research to be done and I\u0027m actually also on the IR s G so I get to have that conversation if you if you think that there\u0027s still research to be done I think that doing it in a research you kind of place would make could make a lot of sense but I guess a I think those III don\u0027t have opinions about where it should end up because I don\u0027t understand all the things that need to be balanced but those are the things that I was thinking okay okay thank you "
  },
  {
    "startTime": "02:01:33",
    "text": "we have one more person but I\u0027ll ask for an elevator pitch just overview of one slide just to give an idea of what it is and then we have to cut it I\u0027m really sorry for that but come on tell me which is which slide to show pick one or two slides one minute above the end so there is no more time really that\u0027s why I sent you an email saying it has to be an elevator pitch you just have to tell you can just not go through no you don\u0027t talk about one slide about what it what it is so good okay Congress possessed actually this is a draft of in the research to provide the transport service for the out higher bandwidth and also low latency application and basically the the motivation is to try to do something which the current transport cannot support such as air we are and the tactile Internet and based on the analysis of thought is all you need to talk into the microphone we can hear best on the analysis of the current solution we take the approach that lets a network device will be involved more in this regard and so we also want a simplest solution protocol to support ease we try to get rid of the same fate of the ICP so our design target is that the end user or application can directly use a new service and also the new service can coexist with the traditional TCP and is both can share the resource of network also the quality of service can be adaptive to the application requirement which means the application can directly change it so the service provider can also manage the service and the performance and the scalability this is the most concerned part which we think that the target should be achievable by the vendor and hardware also last one is that the new service is transport acoustic means that both TCP and UDP can use it so I only have time to consider this slice but if you are interested please read the draft which was submitted to the 6mm first but because we have a lot of other area to do not like a congestion control and the security and the UDP so the this draft "
  },
  {
    "startTime": "02:04:35",
    "text": "is a forced to introduction of the ideas and we may have other what to do so if I interested give us feedback okay David black is tztg chair public service announcement this draft is on the TSU w/g agenda for Friday where Lin will have a lot more time to present and talk about it please show up for that yes great yeah thank you very much for putting it on that agenda all right thank you apologies for this and thanks all for coming if blue sheets are here if you didn\u0027t get to sign the blue sheets and come out here and do it and thank you this answers is your G for today "
  }
]