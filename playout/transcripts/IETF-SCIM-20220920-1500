[
  {
    "startTime": "00:00:09",
    "text": "good morning good morning I was hoping someone would respond to make sure my audio was working thank you totally understand we also need some note takers and Aaron if you can get us started would be great sure thing good morning everybody um it'd be great if someone could volunteer to take notes there is a little um note-taking tool inside of this meeting software you should find it with a little pencil icon um can we get at least one volunteer all you have to do is just um yes thanks Nancy for the link uh roughly take notes on what people are presenting um and especially more importantly than the presentation content is any of the replies that people are in the discussion that happens here"
  },
  {
    "startTime": "00:02:14",
    "text": "we really can't begin the session unless we have at least one volunteer foreign thank you very much go ahead and that was General yeah thanks you should be able to just um click the little edit Link in that at the link Nancy sent and sign in with your data tracker foreign so thank you Joel for that a quick intro to the agenda um you should be able to find the agenda Link in this in the software as well where it's in the notes document actually and uh we have a couple of drafts to cover today so hopefully we'll hear from um from everybody who has updates on their drafts I'm starting with Pamela we have Pamela first on the agenda but I don't see her in the participant list here so we may have to bump her to later hopefully she shows up shortly um so I only got one set of slides emailed to me ahead of time um but again sorry for the short notice of sending out the request for them if you have slides that you want to present you can share your screen through this that's fine or if you want you could also email them to me right now and um I can upload them"
  },
  {
    "startTime": "00:04:00",
    "text": "Danny says Pam is joining now great in that case let's let Pam start once she gets here is there a chat feature in in the software here that there is how do I get there is yeah um on the left column there's the little uh list of all the people and there's a separate tab kind of next to that with little chat Bubbles and that'll switch the participant list into the chat View yeah so Matt you might see two little bubbles above the participant list that's the chat panel I do not see the bubbles of the participants first yeah this is interesting how did you join that because I I cannot chat with you either so it looks like I just joined using the link and moved in with the data tracker credentials went through the front door I think but um I'll just look at the notes and if there's anything it's important that happens in chat um I'm just hoping maybe to see some questions that people have them and answer them if they don't want to say them out loud or or can't because they don't have Mike or something like that yeah I'll be sure to repeat anything uh in the that comes in the chat so that they can all all hear it um there is one other way to get to the chat which is if you go to the um it's called Zoo lip so"
  },
  {
    "startTime": "00:06:01",
    "text": "it's a zulip.ietf.org and they've now integrated that with this meeting tool so there is a skim room there and um it's the same as the one for this meeting it's a little bit hard to find it but hopefully you can get in that way too uh okay Pamela is here fantastic can you guys hear me okay yes oh yeah oh good I was about to panic okay so use cases are we um and I think I have 25 minutes right but I'm happy to keep it shorter to make sure we're still on agenda here sure thing I will just make sure to cut you off after 25 minutes if you're not already done that's right just came me off all right let's share my screen so uh the quick wow uh is it well I guess we're gonna find out window yeah all right that works so the the quick update here on the the use cases is um I have been going through the specs and looking for the terms that are heavily used in this in RFC 7643 and 44. um wait sorry Pamela are you sharing your screen I'm not I just thought I'd give a quick oh okay quick update so I've been going through to try to find the terms um because I you know in some sense if we can use the terms in the spec within the use cases I think that's probably the best way to get people um who are reading the use cases sort of into the spirit of using these these terms in context so that when they read this back they can understand what's happening and so um what I ended up doing is going down a rat hole of interesting proportions so let me"
  },
  {
    "startTime": "00:08:01",
    "text": "share the picture that I'm drawing and I'm you know what I need is your guidance here on where the Rat Hole should end come on permission denied by System is what it's saying right now do we know if that's my system or your system I think that means it's yours I think it's is your browser granted permission well that was you know a bit of a detaining well um all right fine I just gave you control so you should be able to share huh perfect so it'll only let me share the deck that's in the queue the um cursive-based pagination deck right that's because your slides were not uploaded on the data tracker but there should be a way for you to share your screen yeah I'm gonna try the entire screen just to see um if that's gonna work yeah I'm still getting an error is there someone like Danny are you online maybe I can share this with you very quickly there's really it's really just one slide so we're not talking about rocket science experiment here"
  },
  {
    "startTime": "00:10:00",
    "text": "but it is fairly important from my point and to get the feedback that I need [Music] um Danny are you there can I share it with you and see if you can share it oh yeah all right hold on all right well let me give you the let me give you the conundrum I'm in right now and then you can you know I think we can probably get this going with even without the picture in fact let's just do it without the picture because I don't want to waste everyone's time um I've also lost you all um there we are so um so I will get Danny to to send the picture in a second but the the issue that I'm having right now is on external ID and treatment of external ID so what the specifications Define is a provisioning domain right and that provisioning domain is Affiliated to the client right so there's a provisioning domain and if you look through the specs there's a provision the provisioning client right either pushes or pulls um into the provisioning domain right pushes to the provisioning domain or sorry pushes out of the provisioning domain or pulls into the provisioning domain now what isn't clear in the specs as far as I can interpret them is exactly what happens in the case of different clients in different provisioning domains so the client the external ID is defined as being relative right and defined only to the scope of a given provisioning domain and so the question that I have for you that maybe you guys can weigh in on will I share this with Danny is uh in the case where um the provisioning client is pushing a new resource to the skim server right they have a chance to specify that they external ID right and as long as the server agrees that that external ID"
  },
  {
    "startTime": "00:12:01",
    "text": "is not a collision then generally speaking the skim server will accept it but they may return back a 201 right within with a differentiating um external ID so that piece is well defined in the spec what is not well defined is that if you now have a client in a separate provisioning domain that wants to access that same resource we know we have an external ID that is um scoped to The provisioning Domain but the provisioning domain has you know this is perhaps the first time the client in the the new provisioning domain has ever accessed the resource so the question becomes what is the external ID that gets returned does that make sense I can I can answer that perfect um yeah it's Phil philhunt here um so this goes back to one of the issues you have in writing standards and that the focus the primary focus always when you read us back is what's the interop issue um and giving implementers the flexibility to do things how they want so there wasn't a consensus on describing the entire procedure for this so that's why it's not all there and that's so because some people will want to do something sophisticated and some people will want to do this trivially so what the contract is really saying is that the client that sets an external ID entity is can keep using that external ID to reference that object and that that opens takes some pressure off because then the client doesn't have to remember all the skim identifiers which are permanent"
  },
  {
    "startTime": "00:14:01",
    "text": "um so the contract is between the client and the server so what a server implementer could do then is bind the client credential or client subject to that external ID so that client aim asks the question client B can use a different identifier if it so chooses now that is not exposed you can't list the multiple external identifiers each client then sees its own another implementer can say there can be one and one only it's up to the implementer or the design of their system underneath as to what the overall capability is and so the spec by intention was silent on that matter so what should be happening is if a client is trying to contact skim in another domain it should always be able to use that external identifier but that doesn't mean that another client from another yet another domain would see the same identifiers um but yeah I don't think it's the greatest solution but that's the solution the group chose okay so for those of you who have implementations right if I do I mean there's if I did share it Danny I did share this slide with you so that you can put it up just so that we have some kind of reference right for the conversation but here's the question so if I am a new client accessing I didn't create the resource so I never negotiated that external ID um interaction right or the The Binding essentially between the skim server and the client then um do I get no external ID back so so now finally thank you so much Danny I'm so sorry you all um so if you're going to create the resource then the negotiation of the external ID right with provisioning domain a here on the left hand side is very straightforward right so the"
  },
  {
    "startTime": "00:16:01",
    "text": "question is if you're going to query the resource on the right hand side um then are you um are you going to get an external ID back at all what is the expectation for the implemented and and is there a way for that implementer that second client to negotiate an external ID so that does have its own external handle or is expected to use this system server's ID it's only it's only specified that if the same client asks it back it would get back the external ID just it's up to the implementer of the service to decide whether they're binding it to the client specifically or or to the entire object it's it's silent on that matter so you can't count on it if you're doing a multi-client system that's the downside what you can count on is that the URI for a skim object never ever changes and you can calculate the URI because the identifier also never changes so you can share that identifier between multiple clients that's always guaranteed to be stable so it's unlike a graph API uh skim this skim identifiers are permanent and this is why the whole we're into the delete Resurrection discussion because identifiers are permanent we want them to survive deletion and Resurrection as well so that's the strongest part and I think it's what most people are using when they have to maintain that relationship they'll store the skim identifier locally or the whole skim URI um um the identifier is actually better in my opinion because even over time the domain name for the server might change more often than the skim identifier does so the identifier is is the permanent"
  },
  {
    "startTime": "00:18:01",
    "text": "identifier that you can always count on um still there were people who said I can't store anything in my client it's a it's a a limited capability client or whatever it just wants to use a key that it's got which might be a user ID or something so I don't know that and this is totally unscientific but my experience is that for all the reasons you described external ID is not actually used that often and that was no surprise to me in our many implementations of skim external ID was one of the most confusing things and difficult to implement because it didn't add an off a lot of utility to the clients and and the patterns that developers were used to following was that when creating a resource it was the server that would allocate the ID and then the client would just remember that idea if it needed to be remembered it needed to be referenced in the future and so we just didn't Implement external ID for this reason that we're talking about it didn't add a lot of utility for the client and it was very confusing on the server side especially trying to bind an ID to client credentials turned out to be a lot of overhead on the service side yep okay is I mean is this something then that I mean to me it's a very confusing concept and I think we should try to communicate it in the use cases and and uh Concepts piece so that people at least understand that piece I mean is there ever a case where an external ID can be negotiated for a client that did not create the resource because that seems like the only uh possibility here and I've also never heard of a skim server managing different external ideas per client but"
  },
  {
    "startTime": "00:20:00",
    "text": "that seems to be the that's the implication of the standard well I think as Matt said and in my experience external ID is really a Fail-Safe connector it's not what clients use they use the permanent URI that's that's the way it's done and and part of the simplification is to do that I I want to sort of alert you to there's another parallel that group that went through this and that's the open ID foundation and shared signals one of the problems they went is they rolled out with open ID without specifying in the contract how to reference objects over time so that you create a single sign you create a sign on assertion and that oauth that jot token flows through the system to the downstream client web server that then establishes its own session but the session of the client web server is completely independent of the the open ID provider and one of the things they forgot to say is if I talk about this object ever in the future here's the common identifier between the two so Google talked about this publicly so I'll comment I mean they said you know they have a hundred thousand at the time so this was eight years ago nine years ago they have over a hundred thousand active clients against the Google IDP not one of them there's no standards can do it so if Google said this this token is revoked or this session is revoked the clients are tracking in a completely different way so once they so all that to say there was a strong need to have agreement on identifiers um what started then was uh in the shared signal Community they started the"
  },
  {
    "startTime": "00:22:00",
    "text": "subject identifier drafts so that in addition to specifying a subject you could specify a subject in different ways it became very quickly and I don't mean to be derogatory it became spaghetti on the wall where I'll throw out every identifier I know for the subject and hopefully you understand one of them and so they created a spec so you could do that that was one of the ways one of the use cases behind the subject identifier draft um and that's useful but it's also scary it's because you didn't establish the standard at the outset um you you the whole thing blows up so everybody's trying to help I think we lost it though oops okay does anyone else have comments on this because I think that yeah I see Danny on the queue hey uh yeah so in in my experience uh so I guess there's two parts there um although I raised my head originally was just one so to the question of uh different clients negotiating uh like different external IDs I've never seen that sort of in the the major that implementations out there uh and I've actually seen external ID be leveraged uh fairly frequently for uh sort of you know larger Enterprise uh applications using skim where there's an expectation that uh like this game manager is probably coming from an organization's connected IDP uh and specifically for scenarios where uh maybe like a migration from one IDP to another is happening uh IDP being a identity provider or anywhere I don't want to cause confusion with any acronyms um I buy it uh if you're moving from"
  },
  {
    "startTime": "00:24:00",
    "text": "Identity provider A to B um it may be that your different systems have different uh you know we'll call them friendly names here things like email address a username type thing where you don't have the right information to match on that but that external ID may be something that you can leverage and it's being pre-populated um you know knowing that you're you know for your first IDP gave it such and such internal identifier and you can then Port that over and leverage that as uh sort of that I I do agree it's a Fail-Safe mechanism for identifying or matching you know an object um so yeah I've never seen like different clients return different uh extra IDs as something that anybody has implemented I've always seen it as just sort of a a straight string um but I I do see value in it and I I think I see it implemented probably more than uh some of the other folks I'll learn my hand now okay I mean I feel like we could do a revisionist uh approach to external ID to state that there's two use cases where it's huge I mean if there's ever a document to put that in right we can talk about the Enterprise IDP use case in the migration use case and say this you know external ideas where this is valuable and it does seem to be about really like you said it's a fail safe during initial creation of the objects right whether that's creation of the object for migration or creation of a very long-term very authoritative relationship between an Enterprise IDP and uh you know and a data store uh does anyone have heartburn if we at least try that in draft format you hear me yes this case is where at least not necessarily your skin content but we see cases where a user profile gets populated from multiple data sources and each data source actually identifies that particular identity to use"
  },
  {
    "startTime": "00:26:02",
    "text": "different external IDs in another pretty case the identity like external ID is not stored necessarily in the central scheme profile but it's kept from other Pro data source basis and that mapping exists to allow allocate the client a different like a clients from team data sources to it to essentially identify the users in their system is easier so I think we we didn't see you on the Queue but I see Danny Mayer on the queue so Danny I don't know if you had a comment okay fermented this on a internal organization and basically uh the organization did have a unique ID um always so it didn't matter that the number the how many skim clients we had of the external ID would be always the same then we had an IDP involved as well as well as you know HR systems so they all shared that one ID but you know you can only do this really within a single ecosystem if you have multiple ecosystems then you have a whole different ballgame and then you have to figure figure out how to integrate that that's going to be a hard uh Hard Sell okay yeah that's I mean that's the dilemma I have right now is how how we characterize this in the way that people are actually using it like it sounds to me like there's a sort of a split right there are at least you know so it sounds like OCTA is using external ID more Faithfully I would say to at least how it's explained in the spec than others"
  },
  {
    "startTime": "00:28:00",
    "text": "because the others just haven't had the need for it um okay I know I'm going to get caned off here so I will I will try and summarize this in um and mail it to the list so that others can weigh in and I may take a survey if that's okay Nancy on the list yeah that would be great and then if you can upload this slide to the meeting thanks all right thank you foreign thank you um next up we have Danny and Matt who which of you is going to present or both and already loaded um this is Matt I'll present to these slides so I'm trying to think how do I Advance the slides here I would I have to give you the control you should see the controls in a second now okay all right I see them do you see little numbers along the Bottom now all right thank you um yeah so I guess the first thing to mention is that currently the only draft that's publishes the first revision and uh there's a second revision underway that addresses an ambiguity that was brought up on the list and I'll get to that in a minute so unfortunately prior to this meeting we did not have a chance to upload the latest draft but it's very similar to the first one it just has a few wording changes and addresses the ambiguity um let me just go through really quickly and and and talk about the reason for the draft this year the draft was born out of um multiple implementations uh service provider implementations in front of many different application apis and databases and what we found is we"
  },
  {
    "startTime": "00:30:01",
    "text": "implemented skim service providers is that the pagination strategy was already defined in existing code bases that we were writing scheme implementations on implementations on top of if we were riding on top of an API that already supported index offset pagination then it was a very easy implementation for us but if our underlying database or our underlying API supported only cursor-based pagination it was really difficult to implement indexed pagination on top of that so you know it's a really easy diagram to visualize this is that you know you've got you've got your your typical applications kind of on the right hand side of this slide where there's a web API usually or a database that's sitting on top of either a directory API or a database API and then you're building your skim service provider either directly to the web API that's already existing or you're reaching underneath and talking to the database layer and pagination strategy is already established you don't really have the opportunity as a skim service provider to go back and change any code in the application to make it easier for you to implement index pagination and so we've needed to have a way of implementing cursor patch Nation and that's really what the the spec is about or the uh the draft it's just a proposal for cursor-based pagination um and when we you know kind of got together as an interest group as prior to recharging the work group Phil and I put our heads together and we we just you know we tried to think about what are the scenarios where where pagination is important and uh you know it's obviously when there's going to be large results sets and it turns out the most common use case for this is a is a client requesting all of the resources"
  },
  {
    "startTime": "00:32:01",
    "text": "for users or for groups so that they could maintain a kind of a local cache of the information that was Pro that was on the service provider and um this actually happened to be our use case most of the time when we were retrieving large data sets was to get the initial load of users or groups onto the client and then the client would periodically make queries to the service provider to keep this local cash up to date and this is a very typical scenario for identity management systems is to have a local representation of all of the identities including all of those that come from skim service providers so that identity management tasks can be accomplished and so we kind of proposed a question which is that if if all we really need is the ability to kind of keep a client and a server up to date um you know in sync with each other do we need pagination or can we just get away with having some kind of a change notification which I think Phil will talk about and we propose that to the interest group and the response back as I remember it I don't have the notes to prove this but I remember distinctly many people being having continued interest in pagination in spite of also having the ability to do change notification and so um what Danny and I will will be saying in submitting this draft is that if we need pagination and skim we need to have some interoperability for cursor pagination um as an overview there's just two there's just new query parameters account query parameter isn't actually new it's just a cursor query parameter and it's uh it's pretty simple in terms of the response as well there's a next cursor and a previous cursor and a response which ends up"
  },
  {
    "startTime": "00:34:01",
    "text": "looking a little bit like this where you've gone ahead and asked for a resource specified do you want cursor pagination you get an X cursor in your response which you can include in the subsequent request to get the next page so I don't want to go through you know all of the draft in slide form but it's a very simple proposal and like I mentioned earlier it's very similar to the original proposal what we'll have in the next three revision is just a few wording changes and some some grammar punctuations that were in the original draft pick some of that stuff and then um uh there was an ambiguity that was discovered and pointed out on the list about uh you know what do I do as a service provider if I support both cursor and index pagination if the cursor parameter is optional which is what the original draft specified and there is an ambiguity there and we resolve that by just simply making the cursor query parameter mandatory and if you want the first page you just Supply an empty cursor and that was the way that WS2 ended up implementing it worked great for them so that solved ambiguity and we just written the draft so that's really all I have on on on the slides and I'm happy to take any questions or or concerns about the draft or or maybe additions that we could make and uh put them in panties are you cute yeah Danny go ahead um I just want to mentioned that one of the other issues is even you if you do change notification you may find that you need"
  },
  {
    "startTime": "00:36:01",
    "text": "um uh uh with the paint set and do some sort of cursor or count notification I think we lost the word there can you repeat that yeah the if you um are doing the change set notification the the change set may be large so you may need to do cursor or space term uh 10 sets that's true yeah you might need pagination if the change set was large yes got it thank you yeah Philip thank you uh I'm a little confused um in the event notification events are always per object there's no set mode Phil if they're if so you're saying that if if changes were per object that you would never need pagination because you're only chain you're only getting a notification on a single object change that there would never be a kind of a batch of changes that you would receive as a client right yeah okay at all the only place where it may get complex is is a group which is why we have a minimum profile where um you might not want to send all of the change data in a group or you might not want to send an event for every change to a group so you may say the group has changed and the client has to go pull that group to"
  },
  {
    "startTime": "00:38:02",
    "text": "get its current state all right larger group gets and you're often it changes so it's the opposite you'll get you'll get uh reduced numbers of changes or the or the the asserting party May May aggregate changes to a single object but you'll never get multiple objects in a single event that's that's not even allowed by set okay so so Danny's uh you know concern isn't an issue then it's basically what you're saying no no okay yeah I I'm I think that one of the concerns although it wasn't spoken when we when we had the meeting of interested people was that what if you know even in the light of a of an upcoming change notification mechanism it would if what if a client didn't want to implement that and they want to just kind of go old school and just submit a query can I please have pagination I feel like there was still some interest in pagination in spite of having changed notifications yeah yeah and I still confused because um so one of the trigger issues is the Max results limit which is not a mandatory thing it's just that servers are allowed to assert it but if you are paginating that wouldn't solve your Max results because if you're only allowed a thousand results in a query the fact that you pulled it back in 10 pages doesn't let you break through Max results if that is that's a security breach so you're still the objective is to download all of the data if you're doing that then you're far better for both sides just to download it in one call and stream it to disk assuming that the service provider gives you permission to breach to go over that Max results because you're a privileged client you're allowed to do that and that's"
  },
  {
    "startTime": "00:40:00",
    "text": "fine that's allowed within the spec and you don't need to do this and the reason I say this is because everything I'm looking at with cursor-based paging is is that it's meant for a small subset and when you look at products like Microsoft SQL Server they do this in virtual memory so while it works if you try to lock up the entire set you're asking the server to store all that data in virtual memory and if you look at stack Overflow you'll see thousands of reports of thrashing that starts occurring on SQL serving and that that's why I say it opens the door for um for for denial of service attack because you only need one or two clients to make that call and if they're allowed to make that call you run out of memory on your server and you could take out an entire cluster the other problem is that something can you help they're backed by a directory by a database but if you have a highly sharded system or an API that is not backed by a database but by some intermediary system implementing cursors may be impossible so if people are doing this to support coordinated provisioning and synchronization you're getting it something that may only address half the possible given implementations out there so I I think you can argue cursor Bank paging is useful for other reasons and I could support that my concern is that a lot of systems are not doing monolithic architectures where we're talking about a skim directory sitting on top of a database um that's not even skims Windows skim is a provisioning API only it's not meant"
  },
  {
    "startTime": "00:42:00",
    "text": "to act like a database to do this kind of functionality um okay Phil um I would like to hear from any from other folks about the particular Point um about whether the idea is that the cursors are in fact tied to the database mechanism or if it's possible to implement it in other non-non-relational database systems or without making a copy in memory does anybody have any experience there to share Let's ignore the Q really quick because I think those those comments were uh Beholder yeah I'll just mention that um you know in in our case in implementing multiple service providers um we just simply reuse the cursor from the underlying API or database if if it wasn't you know we didn't and and there always is a mechanism by which you can page results and we did not have to implement any kind of cursor implementation within the Sim service provider itself and I don't know if our experience in in 35 uh you know skim service providers is is perfect but it obviously isn't but it it did end up being a very useful thing for us not to have to do translation from cursor based to index based because in trying to translate from cursor-based index base which by the way is the only pagination style that you can use by by the current spec then you do have to keep pull result sets in the service provider which brings up the problem that Phil was mentioning is that even implementing index pagination on top of a cursor-based pagination code base you can deplete the memory very quickly in a service provider so that was our our main reason in needing to do cursors is"
  },
  {
    "startTime": "00:44:00",
    "text": "to match the underlying code base so that we didn't have to have a lot of memory in the service provider foreign yeah just mentioned one thing in closing in in rebuttal to to fill a lot of the concerns about curse of pagination that that Phil brings up I believe are also applicable to index you know there's they're the same really when you get under the covers in terms of your liability and in implementing them and there are reams of information about how this is how this can be done and protect yourself from the the possible memory depletion attacks that we're concerned about okay thanks uh either Danny is this are you in the queue for this topic yeah I'd like to say that uh I I agree with Matt on that the uh just pulling everyone into memory just to serve it up is can be a real problem um you know you have to have a hundred thousand uh um employees um you're gonna have a problem um getting all the data that you need into memory bit that you can then serve up um I wouldn't like to try it but you know you know we shouldn't be dependent on on the translation between uh server or indexing and how things operate behind the scenes I think that's sort of off topic really um we can't answer that okay uh Danny's owner did you want to address this too yeah I had originally arranged my hand for something else but I'll have to remember what that is later"
  },
  {
    "startTime": "00:46:01",
    "text": "um uh so yeah on this topic I've uh had a chance with like our uh engineering teams here at Microsoft this was requested uh during the July like itf14 skim session uh and in a you know hypothetical world where we did build a some service provider it's currently we only have a client uh if we did build a service provider we would you know with pretty uh it would be more likely than not uh that we would prefer to use and actually would want to require uh cursor based pagination uh which is a a separate problem actually hey that's the other thing I originally raised my hand for which is uh but I guess a long term uh I think Matt and I would like to find a way for a service writer to let's say only support uh cursor-based pagination for precisely that like denial of service type reason that was originally uh brought up surrounding uh index-based imagination but um so to get back to the original question that we're still on uh if Microsoft massive you know Azure trajectory big big directory uh if we were to implement a skim server it seems today based off of the research that we've done that we would uh want to go with uh I like cursor based Foundation model uh and actually require it to make sure that we never had to serve up uh results Beyond a certain amount okay thanks uh Pamela did you have something yes and on the same topic um so I mean I think we have to keep in mind that we are you know we are creating architecting an interface not an implementation right so I think you can go either side and say that there are ways to DDOS both sides"
  },
  {
    "startTime": "00:48:00",
    "text": "there are ways to run out of memory on both sides right but at least for now well we have we have a default method and we're all we're trying to do I think is add a second option that could a lot you know with the goal of allowing people in fact to offer you know things that work better for their particular implementation right like we're just talking about interfaces that can be you know maybe lighter weight Alternatives in certain use cases I mean is that right yeah that's right like I I mean we we just needed to be able to have a way to have an interoperable implementation that uh that we could even expose we just found like we if we had an underlying application that used cursor pagination in its API or database we could not create a skim service provider for that application unless we had some way of doing cursor pagination and so rather than just leave that application out of our library of things that we offered for skim we had to come up with something because implementing a service provider that translated between cursor and index pagination just it wasn't possible but the resources that we would have available in the skim service provider so that's why we proposed the draft was so that we could have um some you know we would at least tell the world here's what we did right and we feel like it it might be useful for others and I don't know how many people have implemented the draft but it's been more than just us so there's been other interests for it tiny it would be useful for others to come forward or if you know Matt who who other well I know that WSU implemented it because they were the last ones to comment on the on the mailing list that they had implemented it and um I've had enough questions on the mailing list to make me think that others have implemented it I just don't have their their email domains on on hand right now"
  },
  {
    "startTime": "00:50:01",
    "text": "to know which companies they came from I just yeah there's been interest though I would also be particularly interested in hearing if there's if someone's implemented it the curse or pagination without using the underlying cursors in a database do you know of any of those I don't know I mean some databases I think support you know databases support both index and cursor and there's databases that support just cursor and databases just for index and uh I think that [Music] um yeah it would you would most likely just follow the underlying database strategy in your skim implementation well my implementation I didn't so well in your implementation was your data but we didn't have a thousand users either in the uh um so it um it wasn't as bad but uh but looking ahead to trying to deal with a hundred thousand users this is a big chore to try and figure out what the best strategy is a quick um time check here we are we are ahead of schedule we have two more topics left um so I do want to make sure we get to those but we um if there is any more discussion here we can spend another five minutes or so on this topic if if not um you know we will will submit the draft the new revision it's very similar to the old one and just watch for people to comment on on the mailing list um just happy to have any feedback that we"
  },
  {
    "startTime": "00:52:01",
    "text": "can get that would result in in more interoperability and you know just I guess in closing if you know and this is also to to the discussion that Phil and I have had in the past I can understand an argument possibly that would say we don't need pagination at all in skim and that we can rely on Max results and and other mechanisms by which you know pagination just isn't even necessary you just if you if you have a query that results in a large result set you get all of the results up to Max result and there's just never any pagination the concern that I had was that if we if index pagination is required which it is in the current spec um that makes it difficult for people to create skim implementations and uh you know just the adoption of skin might be affected by that so I guess it really boils down to me is if we're going to have pagination in skim I think we need to address cursor and index if we're not going to have pagination at all then we need to make that change in the spec I think Matt you need to explain why cursor pagination is better than a simple gut because Max results is an issue either way if you can get the service provider to waive your Max results so you can download the entire data set which is your use case whether you download it via cursor or whether you download it in one shot it's the same thing so right I'm not arguing with you again and just do the argument I heard against that was I don't have the memory in my client to deal with that well that's why you have a disk so"
  },
  {
    "startTime": "00:54:01",
    "text": "that's really easy for service providers to implement in a super Universal it also supports bootstrap cases and all the other cases you want to cover cursor base Pages paging means I'm going to take up to date that I need you to hold that cursor open I mean I ran into this with ldap a lot of times where the clients we had the most troubled with were the ones that wanted State maintained for three or four days because their processor was too slow um that led to a number of lost if you have a network problem as time goes by that soon as you have a network problem or other reason you lose that state and then the client never actually completes a synchronization so a number of problems open up but if your use cases I want cursor-based pagination because it's better for the server for a thousand results or yes then I would support the spec but if you're saying I want to be able to download the entire data set I don't think this is the correct tool you don't you don't download the entire data set with events you can't but there is a need now and then for people to be able to do an export and import on mass and that's more the scenario I think you're coming after cursor if as a consultant uh paging seems to be the simplest way given no other options available but if we're here at the standards Community we're talking about not an open source approach which those who want to participate can participate there is a bit of an implication even though it'll always be optional that that cursor-based paging be supported if if we we say so there'll be a lot of pressure to support it and that really changes how people are going to implement the data systems underneath skim um"
  },
  {
    "startTime": "00:56:00",
    "text": "I think it's not it's it's just difficult for everybody I do think one important concession that was helpful to me um that you made was index-based paging versus cursor-based paging those observations make sense to me because it's the index-based Beijing and holding State on all of that entire index that's the problem uh I I would agree a cursor-based paging only um will make it more doable but then I keep coming back to um why not just why not just download it all at once um right less of a threat yeah I I know we're out of time but uh I I think it's in our in our implementations it's more the fact that index-based pagination is not optional in the spec well though yeah we can talk about water as a two-point X so if I want to implement pagination at all um you know I I have to implement some form of collagenation in order to be an Opera interoperable and if I am going to implement pagination in order to be interoperable I need to be able to follow the pagination strategy of the existing code base like like Danny was mentioning like there's if you're going to put a skim interface in front of azure active directory and you talk to the engineering team and they're like we would prefer cursors that conversation is being had all over anytime anybody's thinking about do I create a skim implementation these questions come up for the engineers and they have a really strong preference and that was our case so we either have a choice you're talking about a breaking change well not that's why we tried the author the draft the way that we did is we did not want to create a breaking change that would say look you don't have to"
  },
  {
    "startTime": "00:58:00",
    "text": "implement pagination at all and that's our choice we tried to add something that would be able to allow interoperable cursor pagination without affecting index pagination and that's the way that the draft reads The Proposal that that Danny mentioned earlier which is having a server only be able to support cursor pagination we haven't actually written that into the draft because you know I'm concerned about that being a break can change but it would be really nice to be able to say as a server I only support pagination and its cursor pagination yeah that'd be that'd be a whole new protocol basically unfortunately yeah so that's why the draft's written the way that it is because it's not breaking it makes cursor pagination optional but I still think it's difficult for some service providers to implement index pagination which is required foreign we have I want to keep us moving on the agenda um do we have any action items um on this topic before we wrap it up the only action item that I think uh we propose is that we would resubmit the draft with revisions I mentioned earlier that eliminate the uh ambiguity and that will put a a draft in the in our in our library that's that's actually valid the old one is is several years old and this updates that and I think puts it on the docket for the discussion if we need it okay great"
  },
  {
    "startTime": "01:00:02",
    "text": "um I'm looking at the agenda again I made a mistake this is my fault um I skipped over one of them or Miss misread one of them so we are now a little bit behind schedule sorry um Philip can you can we give you 18 minutes instead of 20 and then we can give Daniel the rest of the Jenny the rest of the time between US Dollars you have to go a little bit quick on those that's fine I um I don't need that much time at all so I think we'll be on schedule shortly we'll be way ahead um there hasn't been much uh yeah just for the minutes make sure we are now on the skin profile for security event tokens yeah so there's been a uh an update to the draft um not much has changed I removed a lot of the discussion about Kafka and message bus systems but I think there's enough still left in there as an example I think in reality um a service provider domain and a client domain will have their own event or bus-based systems that are likely happening or you're going to have like a Master Slave replication hierarchy um the spec doesn't say how you do that but that's sort of acknowledging the reality that you may have hundreds of servers out there in in one domain and then in a client domain again you have an unknown set of infrastructures so the objective with skim advance is to say we need to across those administrative boundaries we need to share be able to share change events from skim and security events from skim across so there is a mechanism that's been defined by the"
  },
  {
    "startTime": "01:02:00",
    "text": "security event specs for for either doing that physical transfer by push or by polling long polling that allows events to be delivered in real time and the Assumption then is that essentially there's a single Gateway between domains communicating those events and then once that event crosses into the receiving domain some kind of system processes that event and then decides whether it gets sent to a provisioning manager or some other system that that reconciles what to do inside the the domain so for example if we had Salesforce as one issuing events back to Microsoft Azure um Salesforce doesn't need to know anything about what's going on in Azure at all it's up to Azure inside of its domain so this is all hypothetical to reconcile that change decide what to do about it can go back to Salesforce for more information and then can decide the parallel request there so all that to set up Danny sent out an email saying there's no recovery mechanism in skim events well that's part of by design that there shouldn't need to be one the recovery mechanism that is in set transfer methods is short-term recovery to deal with failures in the delivery mechanism so that data Integrity is maintained what is supposed to happen is that the event receiver if they need a recovery mechanism in the long-term system can maintain that themselves using whatever architecture they choose to do that so if you were using Kafka as a message bus and you drop those events and once you drop that event successfully and it's confirmed received or placed on the bus then you can acknowledge back and set"
  },
  {
    "startTime": "01:04:00",
    "text": "event transfer that I have successfully received the event so now recovery is not an issue that bus locally on the receiver side then can be used to bootstrap new servers it can you can go back anytime so the change log becomes yours so that was the design for Recovery is that the recovery is under the complete control of the client um and they can do whatever they want to do they can make this simple or they can make this based on a huge infrastructure can go either way um the other reason for doing this is that once you move from 50 000 entries up to half a million or 500 million entries having a centralized changelog style mechanism becomes harder and harder to do just the same way that um doing a cursor across that many entities becomes harder and harder to do um so the idea is is that now you have hundreds of servers each accepting and processing changes what are they going to do with those changes they'll probably drop it onto a message bus of some kind and then some router System picks up those changes and republishes them to all the clients that are registered to receive events so this is what's envisioned by the shared signals event framework put out by open ID not just for um uh security events but also we can use it for provisioning events and that's how this is all supposed to come together um when we went through this on the security event side we realized that to maintain a change log on the service provider side or an aggregated recovery mechanism for every possible client would blow up quickly because you end up with a permutation and combination problem because you're maintaining speculative"
  },
  {
    "startTime": "01:06:01",
    "text": "copies for each security entity that may maintain it so not only do you have 500 million entries in your database you have every change event for all those 500 million times every single client that you've got it won't work so the idea is is that the recovery mechanism for events is base is up to the receiver to decide to implement if the receiver needs a recovery mechanism that's long-term they can build that to whatever mechanism and architecture they choose to use so they could do their own ldapis change log on their side there's nothing saying you can't do that and you can fit your own Master replication or whatever strategy you have you can do whatever you want so the idea is that skim events fit in with the shared signals framework so that it's not layering any additional things and then skim event is just then a schema definition we're not inventing any new protocol here uh there's implementations out there so that's really all the update that I have and we've talked a lot about that um in my responses to to Matt's question and one of the things that I still Envision is you still are going to have use cases that neither paging addresses nor skim events addresses which are how do I bootstrap a new skim replica or a new copy how do I do this when the situations where I want a full copy of the entire data set is that something even the skim working group should work on or not but I would argue neither draft solves this problems nor should it so there's that the other thing is is that this may seem complex"
  },
  {
    "startTime": "01:08:00",
    "text": "but this was the decision of the skim working group to move this out to the security events group because all of these different standards groups were developing signals based on uh jot tokens and we wanted to have common processing common abilities to transfer them and to do things all the same way it gives the appearance that it's a big huge thing that you have to implement but really it's a shared system shared set of specs which means there's already implementations available for you to use so the work I'm doing right now in reality is to implement this using the open ID shared signals framework particularly the one Cisco Duo developed and so that's going on and I've had some feedback for the shared signals group because there's some unnecessary restrictions but that's really just about it so I can turn over the rest of the time to questions or or for other things go ahead venting I do have a question my understanding the scheme is really for identities uh there are different and it is translating to profiles correctly if I'm wrong uh so I just want like in terms of Security even scope that it only covers profiles for the identities or it also covers account management login sessions and other events which looks like I'm not sure if the scope of the user profiles so that's probably my first question because uh just just to understand the scope of the humans itself the second"
  },
  {
    "startTime": "01:10:00",
    "text": "question yeah well just to answer that it's not not for different resource types it is it matches skim events matches the skim protocol essentially so if you do a a create event if you do a create and skim there's an analogous create event uh in the event architecture so it is not specific to any kind of resource type at all so uh and then you can decide which types of objects you're in you're issuing events for that would be up to the the parties to negotiate foreign fields which have strong air caps or it's just a matter of skin or all of them are streams all the data type is implied that would depend on the event definition itself um if you look at the original RFC I think it's 8417 for a security event token it gives an example of a password reset event and I've mentioned that in the new skim events draft but the idea there was that first of all password reset is a high I'll call it a higher level event there's there's there are skim events that match the skim protocol create patch put delete and so forth but the security event for password reset event is simply saying uh Phil hunt reset this this object which is Phil hunt reset his password and in the example it shows that you could also have down the way or if you're uh in a local relationship you could add extended data that says he's reset his password now four times"
  },
  {
    "startTime": "01:12:01",
    "text": "um because that might be additional information the spec is it all depends on what we put in the event as to what specific data can go in but at the moment the spec Just Simply specifies Skin data what event goes there's two versions of uh right now for the protocol events like like a create event um in one version you're sharing all the raw data um which is presumably being used for security partners that are tightly related and they're essentially doing replication and then there's another one which shares no data at all it just simply says this object changed um and then it's up to the receiver to go back and pull that object so that it can reconcile the changes that occurred and it also allows access controls to flow it to work and things like that um so if you read this back uh it either is there's two different versions where you're passing all of the data or you're passing almost none of the data um and either way it can work um and if you look at the security events which are higher level events um it shows you how you can pass standardized claims as well as additional claims that you might say well in our deployment we need to know the password reset count so we're going to add this additional data and you could devise any other information you want to exchange There's No Limit there it's just a matter of how much you want it standardized it's similar in the sense that in skim there is the user object there's the group object there's the Enterprise user object and people often do their own extensions locally and that's fine it's all a question of do you want to register that with Diana and how much you want to formalize these new"
  },
  {
    "startTime": "01:14:00",
    "text": "attributes so it all works essentially the same way thank you okay if you don't mind Phil I'd like to um close this topic and move on to the last two sounds good great thank you um so Danny's owner um you've got the rest of the 15 minutes of the session um for your two drafts sorry for cutting them a little bit short no worries uh I think I can still run under time probably uh great so yeah the roles and entitlements draft uh back at ietf 14 uh I wrote a revision to the draft I emailed about it to the uh to the working group mailing list uh I I've gotten a bit of feedback um this I guess I'll start off with a reminder uh anybody who might potentially be interested in a way to retrieve roles like the illustracceptable values uh please go review it and provide feedback um I well I'm gonna you know work with the chairs to put this up for a call for adoption at some point in the next couple of weeks probably um I I guess uh so I'll I'll start um there's like I can list off the updates but I think I also did that via email uh does anybody have any thoughts on the draft any comments concerns suggestions uh one thing this is the entitlement uh draft right uh yes the roles and entitlements draft that particular I have to have a question in terms of school for the problem we try to address because usually a user's uh or assumptions the"
  },
  {
    "startTime": "01:16:01",
    "text": "usually users entitlement all the rules uh it's probably limited different Downstream systems might have different roles or entitlements and the permission hierarchies so I just warned in this particular case given that the directories or it is usually is centralized and it's in some cases centralized where they have a lot of Downstream systems so I just wondered like as a single attribute might not be the same or applicable to all the systems all the downstream systems that actually is integrated with I just learned in this particular case is a spec trade or at least who's trying to address that particular problem or it have a different assumption here um so I I didn't really think about um I guess systems were like the skim API is built on top of multiple uh like I guess you know distributed and somewhat disconnected systems um the problem that I was trying to address with the draft is that with both the roles of entitlements attributes on a user resource uh typically there are a finite set of acceptable values for those uh the values are already predetermined somewhere based on what roles or entitlements exist in the system uh so you can't just provide you know a value for a role of uh you know anything any possible character combination it would usually get rejected in a lot of the at least the the fast game implementations I've seen um and so in that case there's um I I would classify it as an interoperability problem where um if a skin client and a skim service provider are becoming acquainted for the first time and trying to work with each other um if the skim client wants to sort of increase its chances of making successful requests so that is you know formulating the body of a request to you"
  },
  {
    "startTime": "01:18:01",
    "text": "know create or update a user and to provide them with uh you know roles or entitlements in that external system uh currently there's no way inside of the standard for uh any you know any anything acting as a skin client to retrieve a list of the acceptable values for roles or entitlements and you know almost any implementer is going to document this somewhere uh they you know so whoever develops the skim client could in turn you know go and like do manual work like read a piece of documentation and then write code to you know add them to a list somewhere uh but that's obviously not a scalable solution so the the goal here was entirely uh they're like to allow without human intervention you know you can go and then connect a skim client to a hundred different skim service provider implementations that have implemented some you know varying list of roles and entitlements or you know roles and or entitlements and allow the skin client to know you know via the standard what are the possible acceptable values for these attributes that way requests are not built with uh disallowed values and this gives back has a caveat in it uh where uh the Sim service provider can sort of partially accept a uh a request from a skim client and you know any attributes uh that it chooses uh the skin service provider can just drop them so you know conceivably that's actually a thing that the uh this game service writer could do of just oh hey you gave me a wrong you know a role that I don't have so I'm going to ignore it uh in practice in the SAS world there's a lot of uh service right implementations where if you provide a value that is not part of the predefined list of values for that application uh the requests will get rejected which is you know sort of wasted computational and network energy and all that of trying to make requests that fails and so that is the goal so I"
  },
  {
    "startTime": "01:20:00",
    "text": "think so to answer your question of like the all the downstream systems that's probably on the skim server provider implementer to uh build something you know whether it's manually writing a list or you know an internal thing that queries all the different sources to uh be able to show that because if the if the skin service writer is only going to accept a finite set of values those values have to be defined somewhere already so this is really just surface those values that you're already restricting on somewhere else so uh so essentially it's on the scope of the problem and try to address it's uh I would just I would think this is a more like a cross grain level a kind of level uh roles and the entitlements so it's not going to address the permissions or entitles for the granular resources within a particular uh with a particular like uh steam service provider yeah so this is I think sort of a fundamental question and it's an area that's maybe a little ill-defined um at least in practice um inside of the standard uh which is like I guess good examples are good um like use cases behind the roles and entitlements attributes uh typically what I've seen you know anecdotally from my experience is uh that roles tend to be those high level uh you know course set of permissions rather than fine grain um entitlements um I I haven't seen implemented as much as roles uh some like some of the few times I've seen it's typically been used uh more in the in the vein of like license assignment or like no feature assignments inside of an application I I think conceivably entitlements could also be used in that same way for um a a finer grained set of permissions um there may also be you know justification behind a third set of action a third attribute covering"
  },
  {
    "startTime": "01:22:01",
    "text": "permissions versus entitlements versus roles uh but with the two today they're not um in my experience particularly well defined or well understood um so I think that's more a problem for the spec rather than for specifically my draft which is more about discoverability of the values that are allowed uh thank you um uh any other questions that question actually serves a really good uh got a whole lot of thoughts out there uh thank you one thing uh if not I'll move on to the referential values thing which shouldn't take that long uh okay uh so yeah I um man I'm not actually sure if I ever emailed this one to the uh mailing list if not I I'll go check and make sure that I do mail it uh so uh I I wrote this draft um you know back during its 114 as well the idea had been floating around for a while um so in a nutshell uh that this it's trying to solve that same set of problems that I was just talking about with Thrills entitlement things of uh how can a stem client be well informed enough to sort of increase its rate of successful uh requests so you know knowing all of the intricacies of what is and is not allowed with the skim service provider's implementation uh so this referential value location uh not to be confused with reference attributes um it has to do with um so if we use like the the manager attribute for example in the the core user schema uh it's defined inside of a paragraph somewhere in the um this game uh the schema spec that the manager the value for like manager.value should be the ID value"
  },
  {
    "startTime": "01:24:02",
    "text": "of another user in the directory programmatically there's no way to to determine that and the problem is that there's other attributes besides manager uh that people will Implement and you know they may be custom attributes you know their own steam extensions uh it may be uh I don't know things like uh job title or you know potentially role even would work here as well actually um but there may be a predefined list somewhere of you know we for this attribute we will only accept this set of values um so a cost center I think the like Enterprise user cost center is probably a really good example as well um so the problem is that if there's a finite set of acceptable values for an attribute and those are defined somewhere that uh it's really helpful for the skim client if it can go and discover this somewhere so that this draft is written essentially it has a couple of new um some attributes or attributes to the actual like schema schema um or schema definition so like the the properties that make up the you know the schema definition of an attribute um and those are to say uh yes you know yes or no true or false this attribute is uh the values are restricted based on you know a list that is defined somewhere else and then there's two sub attributes that tell you the resource type that houses those attributes or rather those values and the the attribute or like the schema your URI for the the attribute where those values will be present so you know you could using like the the manager as an example for users uh it would say uh well for the manager attribute this value is you know sort of constrained by uh a value on the user resource that exists at you know um you know and just give the URI for the core schema ID value"
  },
  {
    "startTime": "01:26:00",
    "text": "which is actually a common attribute whatever it is we got into weird educate stuff but um I I've seen ample the use for this uh and some of the skin implementations that I've had to work with and I think from a discoverability standpoint this would be really helpful uh sort of at scale um to help solve problems and be more efficient there's uh yeah so uh I'll make sure I email them excuse me uh talking to us uh I'll make sure I email the uh the mailing list if I haven't already on this one which I I'm suspecting that I never got around to it uh and please share your feedback to anybody I yield my time great thanks we only have a couple minutes left so um let's I guess wrap this up um for everybody who's got uh in progress draft would love to get those posted um and then we will continue the discussion on the mailing list um I did already submit the request for a session for ietf 115 which is in London and there will be remote options of course available to join remotely if you can't make it there um so that'll be the next time we get together to uh talk in person and yeah thanks Joel for taking fantastic notes from the session um so yeah any closing thoughts from anybody else great in that case um oh Nancy's here I was just gonna say thank you Aaron for running most of this today of course happy too hopefully next time I'll do a little better keeping track of the time"
  },
  {
    "startTime": "01:28:00",
    "text": "well and hopefully I will be healthy enough to help too all right in that case um we will give you all the 90 seconds back and thanks everybody for joining thanks everyone yeah thank you"
  }
]
