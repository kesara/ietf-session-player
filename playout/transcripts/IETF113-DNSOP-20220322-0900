[
  {
    "startTime": "00:00:14",
    "text": "we do have some technical problems for the presenter the cable is stable okay all right so we're still setting up uh the presenter screen can somebody raise his hand in remote and so we can test the mic line paul please go ahead can you speak okay we we don't hear you yet okay good let me check this okay [Music] sure i can try can you hear me okay yeah we hear you yeah okay maybe it's false audio yeah okay"
  },
  {
    "startTime": "00:02:01",
    "text": "all right yeah would be testing testing [Music] yeah paul can you try again okay okay okay we get started yeah okay another test from the room yeah that one seems to work yeah okay okay welcome this is the dns working group dynastop working group and next slide please so the chairs tim vicinski suzanne uh and myself benno alexander here is the so-called delegate it's help he's helping us uh with the practical issues and things of running at the the hybrid session uh warren is our ad sitting in front of here we have a jabber scribe which is suzanne and we have the note taker the midi taker and this paul hoffman thank you very much next slide we assume that you are all aware of the note well next slide and we also like to mention the code of conduct"
  },
  {
    "startTime": "00:04:00",
    "text": "here at the itf well the short version is be kind and be respectful to each other if you find with your interactions on mailing list or one-on-one communications the lines here the code of conduct is not followed please contact the dynastop chairs and i also want to mention there is an ombuds team so if you have questions and guidance and support okay next slide um yeah so we are now running a hybrid uh meeting in uh after two and a half years thanks yeah we i don't have the slight display in front of me um for all in person attendees here in the room uh if you want to go to the mic maybe you already learned that yesterday but i want to repeat that if you go to the mic you have to raise your hand on your telephone so everyone in the room even though you're not intend to go to the bike though please register with the following quad code because it also resists you as being as attending the the dinosaur meeting so it's the blue sheet also thank you yeah next slide so for the document updates uh tim uh or the dinosaur chairs did send an email yesterday this morning about the status updates we want to keep it brief because we have a full agenda so we rather reserve the time for the presenters week so next slide we did finish dynasec ayanna considerations it's published as rfc and also dns tcp requirements has been published as an rfc okay next slide please"
  },
  {
    "startTime": "00:06:01",
    "text": "svcp https service binding it's still in ist call there are some um well discussion in the isd a new id is asked by the ise and the process is going forward um yes next slide yeah so the interims we had scheduled two interims between the itf 112 and 113 this one it seems to work quite well for us so we uh planned one or two drafts in an uh into a meeting of an hour so we can discuss it in detail it worked well for us and we are also planning to plan to one or two ite sorry interim meetings between the current itf meeting and the next one in in july um yeah next slide um yeah this is important slide to mention and there are questions feel free to ask we didn't adopt any new work in the past one and a half year one year one half year there were two well quite a lot of active dynastop working group documents and it was difficult to make progress with all documents in being active documents in the working group so in the past year we finished a number of the working group documents we're in good shape i think that the top chairs think we can go forward adopting new work and also warrant our area director i think we're in good shape so we did a survey about which working group documents were or potential working group documents are of great interest from the first survey four four five months ago the dinosaur dinosaur bootstrapping and the dinosaur"
  },
  {
    "startTime": "00:08:02",
    "text": "automation drafts were selected as the most relevant or there were the drafts that got the most interest from the dinosaur working group so we will start at dinus op call for adoption later this week for these two drafts besides that of course there's new work presented today there's discussions on the mailing list that's good but we will [Music] have another survey for the working group to fill in to indicate which drafts the audience the the participants find interesting and find work relevant for the working group so we can make another selection for new work so we want to prioritize our work also in coordination or with feedback from the genus of working group um yes thank you maybe you want to mention something yeah next slide yeah indeed it's also a word from warren huh that screen still doesn't sorry yeah um so warren kumari um so bino in the chairs just went through like a long thing on you know we should do stuff with the working group and make sure the working group has all the consensus and blah blah blah and that's a great idea but what i would really like is if we could also do something in addition there has been some discussion on a bcp on dns sec and there was some discussion in that on maybe we just start up a really short-lived working group just to do the dns sec as a bcp document and i sort of discussed that with the isg and there was some feedback that that sounds like a lot of admin work and faff so what would be nice is if we could do"
  },
  {
    "startTime": "00:10:00",
    "text": "this whole process with you know the survey and the working group has you know provides advice on what how we prioritize stuff but if we could also maybe please do the bcp on dns sec and do that with a really high priority and bump it to the front of the queue and so i'm sort of asking the working group for a favor of if it would mind considering that there's been a fair bit of feedback that a bcp on dnsec would be good and so you know i would really like it if we could do that do it as a fast track thing and sort of move it to the front of the queue of how we prioritize stuff so hopefully we can do that thank you thank you warren okay yeah next slide yep so document status the document status has been sent as an email to the mailing list if you have further questions please contact the chairs or discuss this on the mailing list yeah okay thank you all our work is on data tracker and in github so there are references and urls also in the agenda where we organize our work next slide please so this is a little bit out of the ordinary but it's relevant to the call for adoption of dynasac automation i would like to ask ulrich for three to four minutes he promised yeah yeah so yeah it's not an agenda no problem i think i know what's on the slide okay hello my name is i work for the swedish internet registry um yes so we you probably have heard us talk about the multi sign up dns automation and one of the problems is that for multisigner bose or several signers need to support the same algorithm and [Music]"
  },
  {
    "startTime": "00:12:01",
    "text": "well if you uh if you own a domain name and let's if you're in the size of the coca-colas and stuff of this world then you probably can persuade your service providers to support the algorithms you want but for the rest of us that's not the case and so if you want to move your domains and your service providers have a distinct set of algorithms we would need to do something to be able to move these domains and uh there's yeah so right now the rfcs requires to be that there'll be signatures of all the algorithms that are in the dns key set and that can be done in the if the signers don't support all the algorithms obviously and so we would need to adjust the dns sec rc but we would have to do some additional work and right now it is not not really clear how to do that and we need to discuss that and uh yeah we started some discussions on the mailing list and i would like for more people to join in and see if we can solve this problem thank you thank you yeah um next slide we're almost done agenda well we're done agenda so these are the current working group business so it's a presentation by duane in two minutes and these are the other so next slide please these are the new business are the zero zero presentation do have a lot of new work in the working group because well the existing documents are almost finished and we are discussing the details on the mailing list but none of the presenters or authors did ask for a presentation slot so there's a lot of new business i won't go off the of the agenda next slide i think we can get started thank you so i would like to us invite dwayne to give the first"
  },
  {
    "startTime": "00:14:01",
    "text": "his first presentation dwayne do you want to run your own slides or do you prefer no i think you should do it actually so that's this one yeah so this is about uh negative caching of dns resolution failures oh sorry we we selected the wrong set thank you all right let's do that one oh okay we go for the the negative one probably this one okay uh dwayne can you give me the share right yes please i think be easy sorry it seems like we are it seems like we uploaded the same slides twice on the different titles and now we are missing yours okay um maybe you can stop that's annoying blue that's optional let's see yeah it is whoops it's fully automated but it's apparently it's developed [Music] do you see my screen uh yet uh we can you can give dwayne permission to share his screen i think already all right"
  },
  {
    "startTime": "00:16:05",
    "text": "sort of like it did start for a moment but then it came back again can you send me the slides there you go next i keep getting error object cannot be found here okay um can you send me your slides uh of the first presentation the glue not optional just by email yeah i'll send you both just in case excellent yeah thank you so let's let's do the negative caching first and meanwhile we we work on sharing your other slideshow sorry for that that's okay it might be my fault i just uploaded him yesterday okay the mail has been sent all right so this is a presentation on negative caching of dns resolution failures this is a new draft um not adopted by the working group next slide so the the the gist of the problem that we want to talk about today is that uh it appears recursive name servers are really bad at caching resolution failures such as timeouts and um server failure"
  },
  {
    "startTime": "00:18:02",
    "text": "codes of refuse codes uh validation dynastic validation failures and loops the graphic on the right is um data from verisign's com net name servers during the facebook outage of october last year and it shows um how the rates of queries increased for those three facebook related domains from you know a reasonable amount to kind of an insane amount over the six or so hours of the outage next slide so this this list is also in in the draft and these are some of the um incidents and evidence of uh you know failure to cash uh resolution failures over the years the facebook outage i just mentioned uh for that we saw 128 times increase in queries to comnet name servers um in 2021 last year myself and some verisign colleagues presented some work at the dns work meeting where we um we were doing some experiments with a uh a botnet domain and we intentionally caused one of the the domains that the body used to return surfail errors and we saw a 1200 x increase in uh responses in in inquiries to our uh sinkhole name servers i suspect a lot of us are familiar with the the tsuname work from also 2021 uh in a network they recorded a 500x increase um 2020 there was the nxns attack uh research that was published 1620x increase going back further in time the ksk rollover during the post revocation phase of the key rollover we observed an 80x increase in dns key"
  },
  {
    "startTime": "00:20:01",
    "text": "queries at the root servers um victor duchovny presented something at dnsoarc where he is doing a um a daily survey of dinasek and uh at the end of his survey he retried all of the names that had failed he retried them all again and that led to a very significant amplification effect the dine attack recorded or talked about a 10x increase in retries from resolvers and uh if you go back even to 2009 you may remember this um paper and blog post called rollover and die from uh from when the ripe ncc did a rollover of some inaudibles next slide so briefly this slide talks about the uh the existing requirements that you know we were able to find in some of the rfcs so rfc 2308 uh is all about negative caching and it says that negative caching should no longer be seen as an optional part of a dns resolver however it also does say that caching timeouts and caching server failures is optional so specifically that is something that we think needs to be updated uh rc 4697 talks about how an iterative resolver must not send a query for the nsr set of a non-responsive zone to any of the name servers for that zone's parent zone so one of the reasons that we call it this specifically is because that is exactly what was happening in the case of the facebook outage although 46.97 talks about the nsr set"
  },
  {
    "startTime": "00:22:01",
    "text": "um you know during the facebook outage we weren't exactly seeing ns queries but we were definitely seeing queries to the parent zone for facebook.com 46.97 also has some other um should level requirements that uh that we won't go into in this presentation and then 8767 which is um uh sort of stale i believe uh talks about says attempts to refresh from non-responsive or otherwise failing authoritative name servers are recommended to be done no more frequently than every 30 seconds so again that's sort of a should level requirement and it's definitely not what we observe in reality next slide so this is kind of what we propose in the draft for updated requirements um first resolvers must cache [Music] all types of resolution failures for at least five seconds and not longer than five minutes resolvers should employ an exponential backup algorithm in their uh in the amount of time that they cache negatively are actually in in back off in the sense that they retry so you know every time they retry they should retry uh they should wait a longer time between retries uh a resolver must not retry more than twice meaning three queries in total before considering a server unresponsive and then lastly we want to reiterate or strengthen the requirements about requiring parent name servers of a zone experiencing failures next slide um okay so that's it um"
  },
  {
    "startTime": "00:24:00",
    "text": "if uh we also presented this material sort of at the dns arc meeting um a month or so ago and it has a little bit more uh information on the background and some some data so if folks would find that interesting i would encourage you to go look for that otherwise at this point i would like to ask for discussion and see if this is something the group is interested in taking on thank you thank you dwayne i see one person in the queue that's paul at all sorry leemon paul please go ahead hi paul um if you go one slide back i guess um you have the exponential back off as a shoot why is that not a must because that seems like the really important thing to sort of stem the flood of messages um well to me the most important thing is the at least five seconds because you know what we see in fact is certain resolvers query like thousands of times per second in in failure mode so if we could go from thousands of times per second from once every five seconds i think that would be a huge win but i would also be willing to make exponential back off a level requirement if there was support for that fever i pretty much supported adopting that because i think it is it is important the question is will we get the people who are doing this uh with the thousand requests per second to to actually read that but uh i think we should give it a go thanks ralph so uh largely man um i will just state"
  },
  {
    "startTime": "00:26:00",
    "text": "my support for this i think this is very important work i'm not going to get into the details this of course that's going to be hashed out as we work on it but i strongly support adopting this work to the working group thanks thank you um hazel please go uh please go ahead hi um so first thing i think it sounds like a good idea um one thing i was wondering you said about five seconds and about you know you're getting thousands of queries um part of me does wonder if depending on the proportion of this traffic that's coming from shall we say large public resolver um operators um i've got to say my name i'm hazel smith google um uh is that gonna like like is that timeout supposed to be like her backhand task like you know if you've got you've got a low balance that sprays across you know i know i pick a number out of my out of my hat 10 000 um back ends each of which is a name server recursive name server each with its own local cache is if each of those ways five seconds given how many people asked for facebook you know every second of the day is that gonna be enough if each one of those waits five seconds and then all of them they all once go huh i don't know who facebook is better ask yeah thanks hazel um so it is our intention that it is per back-end server as you said so um uh i wouldn't uh maybe we need to be a little bit more specific in the in the language about that but uh that is the intention what what we see today again uh if if there's a large recursive operator that has thousands of backend servers uh what we see at verifying some of our data is we see thousands of requests per second from each of those backend servers right so that's how you get to a"
  },
  {
    "startTime": "00:28:02",
    "text": "million queries per second obviously so wow okay so yeah we we would be we would be very happy if each back-end server could limit itself to five queries per second or one car every five seconds sorry thanks can i speak jim reed i strongly support this draft i think it's much needed so kudos to doing his colleagues for actually writing this up i think that you really should adopt this i've got one or two questions though first of all is the numbers we've got there the same resolvers must cash for five seconds and not longer than five minutes i think we should have something a bit more evidence-based i don't really know care where these particular numbers come from but i do think if we're going to develop this further we should try to have an evidence-based approach to that maybe these values should be smaller maybe they should be larger i don't know and we should reconsider also what the second order effects would be once those values are chosen and put into place and the other question i have is more of a meta one dwayne is that you saw there was a big spike in queries after the facebook outage were you able to identify the name seven implementations that were responsible for the bulk of those queries and i would imagine they're going to be coming from a very small number because the bulk of the resolver traffic these days is coming from you know like google's quality and the others the other all four services yeah thanks jim so um in the draft i believe maybe not i don't know if it was published but verisign published a blog post on on the outage as well and in there we were able to identify some of the um sources for this right so uh we were seeing a lot of the uh traffic coming from large recursive operators um we didn't we didn't really attempt to identify uh you know all the open source implementations out there"
  },
  {
    "startTime": "00:30:00",
    "text": "but we did analysis or identification based on source address not on say software version if that makes sense okay thanks i can point you to that uh if you need to sorry yeah so um we bumped uh joao from the queue but if we do have time for a short question okay okay sorry uh dwayne go ahead uh i was just gonna say if if anyone needs the link to that um other blog post i'd be happy to provide it okay thank you so to your last request call for adoption we will uh run the survey so and your draft will certainly uh in the service of the working group can indicate their preference on their work and that we set priorities but we will follow up on that and we will report well we run the survey probably one or two weeks something like that so early april half april will uh set out a plan for adoption of work yeah okay great thanks thank you very much and thank you for sending us the the other presentation yeah it's correct please right yeah okay thanks benno so uh so this is about um the the glue is not optional draft um this was originally written by mark andrews and he kind of allowed uh myself and schumann huck and paul waters to join him as co-authors the the title originally of course is glue is not optional but as we'll see uh you know throughout this presentation there may be need to change that slidell title slightly so uh next last time this work was presented we were at revision two of the draft so here are the changes uh summarized since then i'm going to talk in detail about all of these in the uh forthcoming slides so just go ahead to the next slide please"
  },
  {
    "startTime": "00:32:01",
    "text": "um so one sort of important change is that we added some clarifying text that this draft only um refers to requirements on name server software implementations and doesn't really say anything at all about data placed in zones or or how registry should operate so a few people told me they were very pleased with that edition they were concerned about how it might impact their registry operation but that's not the intention of this draft next so all references to orphan or promoted glue have been removed this is something that was a little bit contentious before and it seems like the group settled on just removing it so that has been done next so based on some discussions in other working groups like deprive it it seems like we might be headed in a direction where the concept of glue could be expanding to uh things or record types other than a and quad a records so for example there's uh there's this ds glue draft by ben uh and so the question for us becomes should this talk him and talk about referral glue or just continue to work use the phrase glue to mean uh only addresses of ns records below the zone cut at this point in the in the zero four version of the draft we've gone ahead and changed glue to referral glue in in most places kind of to test the waters and see how it sounds to me it sounds a little strange but um we went ahead and did that and then also i think there's maybe a little bit of an open question about where to define glue although maybe maybe somebody else has some updates on this at some point there was talk of putting it in the dns terminology document"
  },
  {
    "startTime": "00:34:02",
    "text": "and if it doesn't go there then it seems like maybe it needs to go in this document but i would probably not like that um so we'll see next another aspect that was uh controversial with the previous version was uh the requirements around sibling glue so at this point uh we've made sibling glue optional and that's why the you know the sort of strong title may need to to change but what what the current version says is that when a name server generates a referral response it should include all available glue records in the additional section and after adding all the in domain glue records if not all sibling glue records fit due to message size constraints then the name server is not required to set tc equals one so therefore we have slightly different requirements for in-domain glue and sibling glue in domain glue is uh all of in domain glue records are required but not all sibling blue records are required next and then i think the last change that uh we want to talk about is to the document now talks about transports a little bit more generically you know it used to say if all glue works don't fit in a udp response then the client should use tcp but now it says you know if message constraints if size constraints prevent the inclusion of all glue records then the client should use another transport to retrieve the full response and next i think that's it okay so at this point we see sort of the the main outstanding issues are uh you know the phrasing of glue versus"
  },
  {
    "startTime": "00:36:01",
    "text": "referral glue and uh where glue actually becomes defined then i think it's time for discussion thank you dwayne can i invite ben ben schwartz to the mic hi uh um i agree that the the terminology around glue is kind of confusing uh it's uh i there are a few conflicting definitions in different rfcs i i don't really understand how referral glue clarifies because in my view all glue is for referral and the things that have been contemplated in deprive are about enabling secure referral which may require more information to be conveyed in the glue so i also don't see why those those things wouldn't be subject to the same rules as everything else so i i guess i don't understand the need for this distinction in my view uh glue is whatever records have been placed in the parent for the purpose of referrals and the the parent doesn't really might have some opinions about what it accepts there but once they're in they you know i think they should be treated uniformly so i i kind of see what you're saying but we already sort of have a situation now where we're treating different types of glue differently right where we have sibling glue which is different than uh in domain glue and yeah so when a name server is you know going through and filling in the decimal section it needs to have some sense of priority right uh unless we're just going to say"
  },
  {
    "startTime": "00:38:00",
    "text": "all again go back to all types of glue some no my um so the distinction that seems relevant there is about the location of it as opposed to say the rr type i think that if you want to draw a distinction in the draft i think the important distinction is the the location of the glue record the owner name of the blue record relative to the other names in play okay thanks ben thank you uh paul paul hoffman so um because the terminology document is still open um i think it's the best place to put this not that i want to do the work to put it there um but i think that since we haven't closed it out and because people writing future documents are more likely to look for it there than in some other rfc just put in the terminology document we can keep it open for longer thank you brett red car i wanted to add um same my weight to the same thing that i think the um the definition should be in the terminology document um as an operator who quite often has to hire new staff and bring them on board and train them and stuff um and the amount of terminology in dns it makes that quite challenging to have one place where all that sits is really really useful and actually i think that terminology document should um be in constant edit so we should be publishing it now and again but as soon as it's published changes happens uh um it always requires updates basically"
  },
  {
    "startTime": "00:40:06",
    "text": "okay thank you ralph ralph baby in the room yeah so on the glue with this referral group i don't have an opinion it's just words all glue s pens that is for referral i haven't seen anything else um the other thing is the registry statement in there i mean registries define the name server right and registry to kind of define the a record let's go into this zone so if they don't need to follow this then not sure if we are winning something here i'm sorry rob can you can you say that again so if you are registering a domain the the place you put the a record and quarter a and name server record in is the registry so if the registry don't need to follow this then meaning they don't need to input the stuff then what are we gaining so i found that statement that registries don't have to do something a bit weird so um i i think the you know the impetus for this work was was not about how registered but it was about when a name server needs to set the targeted or not so the sound sounds very bad from your side so it's a lot of noise or white noise on top of your voice there's a crack on top of that yeah how about now much better yeah perfect yeah okay i just muted and or unmuted um well i guess we should take that discussion to the list ralph because i"
  },
  {
    "startTime": "00:42:01",
    "text": "think the original impetus for the work was was really you know the behavior of name servers in and whether or not they set the tc bit in their responses and what i've heard from some a few folks that that work on the registry side is is um you know different registries have different modes of operation with respect to to how they treat uh glue records or address records in their registry and and they were concerned that they would have to um change their models their their registry models and they didn't want to do that obviously some have um you know i think it gets down to um host object versus um the i figure what the other type is called but um there were concerns that they would have to change their their registry models for this yeah okay thank you next one hazel hazel smith google um so firstly i would say that um i think it's a good idea to make clearer about the definitions of glue i mean it's possible i'm just hilariously under-informed here but um i've always struggled to know whether i should call the sort of ds records that are you know parent side of the zone cut whether i should call them glue or not and i'm sure i did try and look for a straight out from this and couldn't find one before so ah wherever this ends up it would be good to have a straight answer somewhere a community consensus on this point um i also knew what paul was saying that perhaps it should be in the terminology document and to me that sounds like that might be a better place for it but as long as it goes somewhere that's my feeling thank you uh alexander in the room um thank you um alexander mayor veronica deity speaking from the registry side i think it's very important to differentiate the three steps between a registry"
  },
  {
    "startTime": "00:44:01",
    "text": "accepting data which is the first step the second step of the zone file that the registry generates for a name server and then a search step where the nameserver actually hands out a certain set of records and i think that document should be constrained to the search step maybe there should be eventually a second document that talks about the second step about what is going to go into the onto the zone um and and i would leave it up to the registry community in regex or whatever to define what actually is acceptable and we are trying to be very uh liberating what we accept in terms not to confuse our deer registers because they send all kind of data that we don't really put into the zone thank you alex yeah so dwayne do you have some additional questions or enough sufficient feedback um so i think on the uh you know where where the definition goes i think we're pretty clear on that it sounds like uh we're still a little bit uh unclear about exactly what glue means and and you know whether referral glue what that means so um you know i think that goes back to the group to the list and go from there thank you thank you very much joanne next up in the agenda is i think ben yeah ben do you want to run your own slides shall we run the slides it's available on the meet echo yeah i've requested grind it excellent please go ahead okay uh hey everybody"
  },
  {
    "startTime": "00:46:00",
    "text": "uh this is about a zero zero draft that is seeking adoption in this workings um the authors are myself and uh my co-author also from google robert evans so the the point of this draft is to talk about what happens when you try to do dane and also use these new svcb or https record types and also it talks about how to do dane with quick which i think may be an inch maybe a a major bike shed point but is actually a very very tiny change so this is particularly inspired by some discussions that have been happening in dprive and also in add around the use of svcb records to convey information about the secure about the support for encrypted transports for a dns server both recursive and authoritative but it actually applies generally to things like using http 3 with dane yeah these things are in the same draft because they are actually connected one of the key purposes of service bindings is to enable upgrading too quick and skipping a tcp bootstrap step and also uh the only other way to get to quick is using the http specific alt service mechanism so if you're not doing http then uh svcb is the only way currently defined to to get to quick so this uh and also this is just a very small amount of text okay i want to have a some brief background about dane um since we spent some time reading the day in rfcs maybe not everybody has uh has looked at them"
  },
  {
    "startTime": "00:48:00",
    "text": "recently so dane is based on publishing tlsa queries which contain tls certificates or fingerprints or public key fingerprints and those queries are made for a name of this form underscore port dot underscore transport dot underscore base name or in the documents it's sometimes called the base domain this query follows cnames as usual and this is all very logical it's uh the the tls query base domain is based on the redirected transport endpoint it's based on the thing you're actually going to establish a socket to essentially it lives as close as possible to the a and quad a records and the dain clients are supposed to set an sni extension that contains this base name okay simple enough clear but there are a few wrinkles in this design wrinkle one is the handling of cnames so if there is a cname on the base domain the dane specification says that the client should essentially follow that cname chain to its end look at the the final target name and then append these prefix labels look for a tlsa record if it can't resolve the tlsa record there it should uh rewind to the beginning of the cname chain try a different base domain uh emit another tlsa query so there are actually two two different queries for different names potentially required on every attempt to connect using dane that's that's an interesting wrinkle there are also some documents describing how to use dane with mx and"
  },
  {
    "startTime": "00:50:00",
    "text": "it and srv records and those have their own wrinkles so the mx and dane specification tells clients that they actually need to maintain three different reference identifiers so they should they should construct the base domain put that in the sni so they're telling the server i want this dom your certificate for this name the the base domain but they should also accept any certificate that comes back and covers these other identifiers even if it actually doesn't provide them with the name they were asking for in the sni so that's that's pretty weird from a tls standpoint and it gets even weirder in a way with srv where srv says that the point uh the expectation here is that the target server host name should be in the certificate but clients are should not ask for it they should ask for the service name in other words uh if there's a some sort if there's srb indirection clients should ask for the thing before the srv in direction but expect the thing after the srv in direction literally don't ask for the thing that we actually expect you to get um and you know there's logic behind each of these decisions but um but these are the considerations that sort of went into our design so what does the draft actually say uh it's it's really very simple it says you do the svcb thing and then you do the dane thing so the output of svcb essentially is a list of tuples target name which is a concept within svcb transport and port number"
  },
  {
    "startTime": "00:52:01",
    "text": "and so this draft just says you take those tuples and you treat each of them totally independently as an input to plain old basic dane so that's a that is very much like the srv specification we uh we tried to stick close to that because svcp is very much inspired by srv and and is essentially an evolution of that idea and uh but we did add some simplification so one of them is well or uh there are some differences one is there's only one reference identity there's none of none of this like try to validate against one name but if it doesn't match here some other names that you should also check that seems like it will significantly simplify implementation when using common ssl type libraries but there is also a another difference which is that srb records prohibit the use of cnames after the srv in direction svcb does not and even mentions it in the draft as something that you can you can do i think so so wrinkle one still applies here but ideally that's sort of hidden within the abstraction that identifies dane that's within the dane black box the other thing that this draft says and this is literally the entire text of the section is to update rfc 6698 that's the original dane specification or well that's an early dane specification with a uh a different definition of what the transport prefix labels mean and the key here is that the current text just says tcp udp and sctp and this raises a question about what happens if you have dtls and quick"
  },
  {
    "startTime": "00:54:02",
    "text": "running on the same endpoint dtls and quick are demuxiable at the transport layer so there's no absolutely necessary reason why they would have to have the same certificate or the same key material serving both of them if they were both operational but maybe more importantly than that is that there have been some proposals to use the presence of a tlsa record to indicate which transport is in use and so if you are trying to tell the client uh whether to use or if the client is trying to ask does this server support dtls we'd like the tlsa record to be unambiguous uh you know the presence of a tlsa record for udp should be an unambiguous indicator that the server supports dtls and it has been right up until the publication of quick and then now it's sort of a little unclear so the goal is to remove this ambiguity and also sharpen the semantics here essentially arguing that these so-called transport names are really about the immediate underlying protocol below tls the protocol that contains the tls handshake here's just a quick example of the kind of thing that motivates this so if you imagine you have a dns server again may be authoritative maybe recursive doesn't actually matter very much for this and it's got some set of service bindings including some advertisements for dns over tls and dns over quick maybe on different ports then these are the these are the tlsa records that you would have to publish to go along with them so this is a very simple draft we're"
  },
  {
    "startTime": "00:56:00",
    "text": "seeking adoption in the working group i'm happy to answer any questions thank you ben yeah we do have time for questions wes please go ahead oh yes my video is upside down uh that works well because of the topic so the one the the reason that dane chases the target is who is in control of the certificate in particular right so um the one thing that you should definitely consider in your draft is think about it this way if if i'm the owner of a website and i have a cname to my cdn right i do not want to do the dane updating of a tlsa record to keep tracking the cdn certificate as it changes you know you don't want a million customers potentially getting those those tlsa records wrong so the reason for the name chasing is that but still allows that you know if the cdn actually doesn't have a tlsa record because uh they're they don't support tlsa then i can publish one anyway right i can work around it um so that's yeah of course you could always do that by copying if you're willing to copy paste their ip addresses into your zone as well well so that right that's that's always the problem right that you want to do the least amount of management possible so um following to the target is is you know quintessentially the purpose so you definitely want to do something point being regardless of why dane does it right right you need to be able to account for the fact that client that uh zone owners do not necessarily want to do a whole lot of tracking of remote certificates"
  },
  {
    "startTime": "00:58:00",
    "text": "that's a real pain in danger so i i think i think that we are um i think that we are doing the thing that that pushes furthest in that direction yes i agree with that okay thank you so wish you're looking much younger upside down i think i will follow your example next time i'll have to start walking around that way sam simone wiley please go ahead we can't hear you no sam we don't hear you sam we don't hear you i can see you nope um samuel you could sam you can also type in your question and suzanne can relate it or we can relay that to the room okay okay well um for timekeeping uh bend some final words and call for good options i'll just uh so this is a call for but this is a a request for adoption um so uh please consider whether you'd like to see this work in dns out we've had some discussion on the mailing list mostly about this last point about uh about wrinkle one and and c names with uh with some argument that this maybe should actually be deprecated across all of dane and"
  },
  {
    "startTime": "01:00:01",
    "text": "uh and that as a first step we should not permit it here so essentially the uh the client would follow c name chains to the end and would only do a single tlsa query for the final alias target and not not rolling back to the beginning of the cname change okay thank you thank you okay next presenter is willem william toro in room william unfortunately that that screen doesn't work so you can either you'll need to resort to that one okay if that's good for you yes so i also asked to yeah i granted you access so no that this is this is fine too or i'll just say move a little bit closer to the microphone that's okay i just need to i can't read the i need to read this text okay so uh this is also this is new work also maybe looking for adoption or at least get feedback from you to see if you like this id driver and dnsec a dry run or a practice run is a testing process where the effects of a possible failure are intentionally mitigated according to wikipedia so on the picture you see the italian astronaut samantha christopher doing a dry run of a fix of the international space station in a swimming pool this work is"
  },
  {
    "startTime": "01:02:01",
    "text": "i'm doing this work with your ghost kevs which is the inbound developer at enemy labs and roy argens next slide oh you should be able to do that oh should i oh how oh yes yeah well yeah sorry guys okay can you forward the slides with me yeah okay you need to stop sharing this right so that i can start for whatever reason okay i stopped there you go so we are on slides yes two now what's the table so dnsec deployment is lagging the top level domain domains are assigned for 90 more than 90 percent but second level domains are not here's an overview of the top 10 largest domains and so some of them have even zero signed domains or are not signed themselves i suppose and next slide please so um what's the rea why will people uh deploy dns sec so one of the thoughts has been that the dane would be a driver of dinosaur this is a slide from a presentation given at the last dns orc showing the amount of till then deployment for a mail exchange you can see"
  },
  {
    "startTime": "01:04:01",
    "text": "that the only domain where then deployment is really taking off is dot now and dot nel already has a large dns sex signed population of domains because there's a financial incentive basically in dothanelle if you do denyseck on your domain you get it for cheaper next slide so the the other reasons for dinosaur to lag behind a little bit is that dinosaur you appear as appear as network failures which is not nice you cannot see that it's something with dinosaur next flight and also enabling dnsec is not without risks we are unforeseen failures i'm referring i'm showing here the picture of a report of the recent failure with slack and they did they did it uh they did actually did uh dry run that in a sec deployment but still uh managed to yeah be off the grid for the ttl time that the ds had in the parent next slide so there have been efforts to mitigate this one very nice one is the extended dns errors instead of dynasec failure appearing as a network failure it will show you the actual error next slide here you see a query for dnssec failure failure.org i believe and the ede or the edns extension with the extended error saying something is not"
  },
  {
    "startTime": "01:06:00",
    "text": "good with your set record or something next slide and this is a call out to mozilla because this is a firefox running doe to cloudflare that does provide the ede coder but mozilla is not displaying it why not now this is important next slide and recently roy irons came with a new draft the extended dns error reporting which is not only conveying the error to the client querying the name but also to the reporting agent of the operators running the domain which is good because they can then fix it and next slide but unfortunately the error has then already happened so the extended dns error reporting is really good but that happens after the fact so what is driver and dna next slide is get the dns error reports but then without the end user experiencing the failure so operators can try out the insect deployments for a period of time before rolling to actual dns next slide so this is conveniently signaled with a new special ds algorithm next slide the idea is to put the ds algorithm or what the the s algorithm would have been in a actual dna sec as the first byte in the digest field and the digest just following after that next"
  },
  {
    "startTime": "01:08:01",
    "text": "flight so here you see what the the ds record for itf.org and how that would look like in driver and the intersect so it's the the digest type field is changed from one to dry run and moved to the first byte of inserted basically into the digest next slide so the id then is for resolvers to look at the ds resource record set and so if there is now the dsr set it's insecure of course but then if there is a ds set a validating resolver has to pick a ds it will choose to validate the domain domain in this case it will look is there a driver in ds if so we are going to try that first then does it validate yes great you you return the answer with the id bits head if not send the report to the reporting agent of the operator following extended dns error reporting and then do the validation again but with uh pretending that there had not been a dryron ds records in the parent so if dryron ds is your only ds record in the parent the it will fall back to insecure do you want to take questions right now because we have paul in the queue or afterwards okay okay continue uh so"
  },
  {
    "startTime": "01:10:00",
    "text": "okay so this flowchart is for because it conveys the id easiest but in actual implementation uh i've so the one of the others is yogos tesseranike which is a the developer of the inbound recursive resofa what we actually want to do is take a ds driver and the s and a non driver and the simultaneously in the validation process and then you know evaluate them simultaneously and also have two security statuses with the rr set in the cache because otherwise it will be yeah it would be like a bit complicated for implementations it would be another straw on the camera i suppose next slide so this also enables opportunistic dane you know if you have the excellent if not then you know so not not useful for signaling right which which as it happens with a male for example next slide is this backwards compatible with existing deployments yes because the spec says if a validating resolver does not recognize the dsr algorithm it should just ignore the ds altogether the next slide so another colleague of my tom carpe did some measurements during the hackathon trying to determine what the effect is in in the real world of having at the ds record with a strange uh digest type"
  },
  {
    "startTime": "01:12:01",
    "text": "and well there were some issues with vibe atlas at the moment but we could the results for a running driver next to actual dns deployment that was done and as you can see the error margin is larger than the actual effect next slide cds works as before you can we think it's perfectly fine to bootstrap actual dnsec from a driver and dnsec by having a cds key in your zone c dns key or not cds resource record i mean cdns key is another thing because you you cannot signal what kind of ditches type you want to have in the parent next slide i think there's also opportunity to do something else as well have clients uh participate in the dry run with a veteran query flag there are some bits available in edness next to the do bit but maybe nor more realistically next slide it's just the edness option uh acting as a flag i hear alexander uh good morning yeah so uh both things cannot be tested with the light atlas so we have to look into other ways to see how backwards compatible that would be next slide"
  },
  {
    "startTime": "01:14:00",
    "text": "so you could sign your zone uh and not put a ds in your parent and have your authoritative service uh report a reporting agent right so wherefore do we need the uh extra the new ds algorithm to signal driver and uh dnf um well because it's the the the extended dns error reporting edns option is not transitive a staff resolver may not receive it and so for stock validation it's better if it can see that driven is the intention by having access to the complete validation chain also without it you would not test the the link to the ds record and uh yeah you would not have the benefit of those nice other features like boots strapping actual dns sec from driver nds and yeah it's in then combining driver and with actual dnsec it's nice that the ds the driver on vs points to the keys that are intended to be driving so it's nice to be explicit about this next slide so this is the id and i'm uh curious what you think of it and if you if you believe that we should adopt it if you think it's a good idea yes we have a little bit of time for questions here yeah cool sir okay hey uh this is uh gavin brown from central nick um i think this is a useful uh tool in the in the in the toolbox but"
  },
  {
    "startTime": "01:16:01",
    "text": "i just wanted to flag a couple of issues on the provisioning side so um i can't speak for other implementations but i can say on our side um we validate ds records with which we receive over epp and one of the things we do is we check the hash length and at the moment um every algorithm that's in a ds record has a has a corresponding fixed hash length and this would make hash lengths variable because obviously if the dry run record is set in that in that algorithm field then the implementation now has to know i read the first bit off and then i'll validate the length of the hash the second thing is the way that we as registries receive ds records there are two interfaces in ebp this is called the ds data interface which for this work with completely fine there's also the dns key interface where all we receive is the key data and we generate the registry generates the ds record and there's no way at this point if a registry is using the key data interface to know whether to set the dry run flag or not so we'd need to extend that rfc the epp extension to add a dry run flag that could be provided along with the dns key data when adding a ds record to a domain in an epp server yeah that sounds like a good id and also the first issue alternatively you could have a separate driver and ds digest type for each corresponding to the actual dns digest type shane hi shane kerr so i was skeptical when i first read this but actually when you explained it it makes a lot of sense to me uh have you thought about whether it be applicable for helping measure the potential impact of like a root uh rollover i have not but i'm i'm doing it now [Laughter] yeah i guess that would be helpful indeed you could do a driver and now you"
  },
  {
    "startTime": "01:18:02",
    "text": "would need a driver and trust anchor then uh yeah yeah i think especially if we're thinking about an algorithm role at the root it might be useful to have this beforehand yeah yeah well we we have thought about that driven would be very useful for testing algorithm wall overs in just your own domain but i i guess oh mikey might just yeah so um [Music] validation already is one of the most complex code pieces in a resolver we are now adding complexity to that for kind of uh no benefit because you're giving out sort of insecure if you are if you are encountered and i think that we should be pushing ede get clients to implement ede and have that as a goal because that gives the end user a clear indication and if you implement dnsec you'll find these things so i don't think that this is something that we should do getting more and more complex stuff on in the resolver to the point where only a few people can run resolvers because it's getting so complex yeah yes sir i think the most complex element of this is the extended dns error reporting up to the operators which has already been worked on in the work group and uh and the goal my opinion on those [Laughter] and and the goal is to give operators more confidence right that to give the operators more confidence that uh they're yeah well for nl half of the zones are assigned and it works so clearly it is possible if you are you put enough energy into it yes best would be if all tlds would"
  },
  {
    "startTime": "01:20:00",
    "text": "give out the domains cheaper if you do the insect that i agree thank you benjamin please go ahead hi i i think this is a recurring pattern now of people trying to uh stuff things into fake digest types with uh in the ds record um seeing as i wrote a draft to do it and and you wrote a draft to do it and we're not the only two rather than um then keep doing this for every idea that people have for stuffing things into the ds record maybe we should have a general purpose um meta digest type where we can then avoid polluting the digest type space with lots of different weird things that aren't digests as to this proposal specifically i it's not my area really but i do wonder you know would we be better off just providing some uh some best practice guidance about how to set up a a duplicate of your zone in order to enable dns tech there see that it works and then do the same thing on the real one excuse me um thank you ben uh your mirror story we are out of time please send your request to the mailing list okay thank you thank you willem um take it indeed the continued discussion on the mailing list and we'll go forward from that thank you next presentation is remote by um state hash based signatures thanks ben good morning good morning um i apologize for two things in advance a that my slides are much duller than the"
  },
  {
    "startTime": "01:22:00",
    "text": "ones that william just showed with the beautiful pictures on them uh and uh be for my dog who has just walked into the room and might start barking if something happens outside so apologies in advance um this is about a draft that andrew frickley from verizon and i uh co-authored on the use of stateful hash based signature schemes for dnsec next slide please um maybe a little bit of context for that so um i'm sure you've all heard the discussion uh about the advent of quantum computers and maybe paul hoffman has talked your ears off about this um and that there are varying uh um opinions about when practical quantum computers might come uh to be uh around but what there is consensus on is that these would break current public key algorithms and that this causes uh problems for internet cryptography and i we've heard estimations vary anywhere between 15 to 50 years where obviously if it's indeed 50 years we shouldn't really be worried right now whereas if it's 15 years we might need to start scratching our heads as a consequence post quantum crypto algorithms pqc algorithms are seeing a lot of development maybe you've heard of the nist standardization effort which is currently in the final phase um and there is also momentum to start deployment of pqc algorithms there's already been a lot of experiments in for example um the tls space um but because of this standardization effort you can expect requirements for pqc support to appear in uh government tenders uh in the near future because as soon as this becomes an official standard governments are going to require uh manufacturers to support these algorithms next slide please um but then there is the argument that dns signatures have an effective zero"
  },
  {
    "startTime": "01:24:00",
    "text": "year shelf life right so they they don't last much beyond their expiration time so why should we now already care about pqc for the nsx well the answer that we have for that is that standardization implementation and transition of algorithms take a long time and an illustrative example of that is the odd 10 odd years that it took for example to standardize elliptic curve algorithms from when they first appeared uh for use in um something like the insect from them actually being standardized in rfc and then implemented and the fellowship there's my dog and implemented in um i'm just gonna close my door implement it in software um there are other challenges with the new and i put this in quotes pqc algorithms that are currently being considered for standardization by nist there are worries about their long-term security which is precisely why they're running the standardization effort to figure out which of the proposed algorithms are actually secure for use but they also have rather unfavorable parameters for use in dns so that's something that that is a concern for us because of the challenges with new pqc algorithms that are still around but also uh because of the long time it takes to standardize algorithms for the use in dns sick um it is our belief so andrew and i state that we think that we need a safe fallback to be standardized just in case and and keep that in mind with this draft next slide please and what we're proposing is to standardize stateful hash-based signature schemes for uh for dnsec um a very very quick primer on stateful hbss they were first proposed by uh uh ralph merkel and are constructed using merkle trees i'm not going to bore you"
  },
  {
    "startTime": "01:26:01",
    "text": "with all of the details um the the takeaway is that they they are considered to have very strong security if you use the secure cryptographic hash function basically they inherit the security properties of the hash function um and that there are actually some good security proofs for this their workings are very well understood they are you're very unlikely to encounter crypto analytical surprises that suddenly break the security of these schemes and they remain secure in the face of powerful quantum computers i note that stateless hash based signature schemes also exist for example sphinx plus is such a scheme that's being considered by nist next slide please roland yes to take a question from the room yes stephen go ahead hi roland stephen farrell so so i think there's a problem with stateful hashtag signatures is that you have a finite number of signing operations i'll get to that is that okay well but the reason i say that is because i think it means safe is not the right way of describing the whole situation okay i'll i'll take that we'll take that on board um when i'm when i okay so maybe i should clarify that when we say safe or secure we mean provided that you respect the limitations of these algorithms and indeed the fact that you can only make a finite number of signatures is a limitation i was about to get to that so maybe next slide um this is a very very very simple uh picture of what touchy scheme looks like what you see on the slide is uh uh the the merkle tree that constructs the the public key of the uh signature scheme um underneath the every leaf is associated with a um a one-time used"
  },
  {
    "startTime": "01:28:01",
    "text": "public key as steven pointed out um you can only create a finite number of signatures that's because the private key is actually constructed uh as a set of one-time use private keys that you can only use to create one signature and should never reuse them so basically a signature is composed of the one-time signature using the one-time private key on the data that you're signing and then there is an authentication path in the merkle tree which is uh the list of intermediate nodes in the tree that you need to reconstruct the root hash in order to validate the signature and in this case those are the red nodes in the merkle tree next slide please as steven pointed out you can only create a but there are limitations to these algorithms you can only create a finite number of signatures with the signing key um as the private key consists of a collection of uh ots keys it is essential that you keep stayed for your signer because if you reuse one of the uh one-time signing keys this breaks security of the scheme and this is potentially a challenge for online signers and distributed setups and i'll say something about that on the next slide another limitation is that signatures are very large they're typically larger than 2.5 kilobytes per signature but the upside is that the public keys are very small in the order of 70 bytes um so for dns this essentially requires eating a zero to transport signatures as part of a message and arguably you need tcp transport or or a different transport so we are therefore not claiming that this should be a preferred option for the insect but it could be a safe fallback of course if implemented securely"
  },
  {
    "startTime": "01:30:00",
    "text": "and given the time it takes to standardize these new algorithms we argue that we should start standardizing this now so that we have at least the same fallback in place in case we need it next slide please maybe a sidestep if you have online uh or multi-signer setups where you have multiple signers that need to create signatures independently you can use a so-called multi-tree scheme for that where you basically have a top-level tree under which you have different hierarchies that you can distribute among multiple signers and there is more information about this in the draft next slide please so the draft status the draft proposes how to use stateful schemes in dns sec we currently include three already iatf standardized algorithms in the draft so that's hss lms and two variants of xmss and the rc numbers are on the slide here um this draft we consider this draft sort of roughly complete in the sense that everything we would think needs to be in there is in there but of course it needs a review and we are interested to hear and i heard already heard multiple times from benno that that he's going to run a survey or that the chairs are going to run a survey so we'll wait for that but whether there would be an interest in adopting this and moving this draft forward um i would say that the the way we set up this draft is very comparable to how um for example the eddsa uh rc is structured um so it really only contains the essentials of how do you use this algorithm in dns section uh anonymous have done or will do um i kind of lost track of that a proof of concept implementation of this and unbound i see ben on nodding so i guess that's that's been done um next slide please"
  },
  {
    "startTime": "01:32:02",
    "text": "we're also considering some follow-up work we're considering a draft on implementation considerations which would look at interoper operability uh the trade-offs that you have if you need use these multi-tree setups uh parameter choices uh transport considerations etc although we do realize that transport considerations might open up a can of worms because that applies to much more than just the use of these signature schemes um i think that's the last slide next slide please yes uh now is your time for your questions um and there are some thanks to people who reviewed this draft on the slide thank you roland graham hey gavin brown from central nick so a couple of points could you give an indication of how much state needs to be stored on the signer if it's an online signer um and and for how long um yes i can so the um the state that you need to keep is which one-time signature private keys you have essentially used so you need to track and this could just be a sequence number where you store the the sequence number of the keys and for how much time that depends so one of the param if you choose the parameters for the for the key you basically determine how many signatures you can create with that particular key um and uh you would need to keep this state for the lifetime uh of the key answer your question it does a little bit but obviously if you've got a second a combined signing key or or something then that basically means that your source your sap for your zone has a has a finite lifetime because if you use up all of those you have to roll your key before you get to the point where you've used up all the available signature leaf signatures"
  },
  {
    "startTime": "01:34:00",
    "text": "yes and that's so that yes you're correct and that's obviously um a challenge because there's a trade-off here now with every so basically one of the parameters is the depth of the merkle tree that you use so obviously as soon as you add another um level 3 the number of signatures that you can create with that key doubles however um your signature length will also grow but in a linear fashion so every level of the tree adds um one step in the authentication part of the signature so that's linear and then the number of uh signatures that you can create doubles so you can create keys that can uh generate quite a lot of signatures but obviously this is not suitable for every type of zone if you have a very dynamic zone you're going to run out of signatures at some point yeah okay and one final point um you describe this as a sort of fullback in case um you know quantum crypto comes along and everyone breaks everyone's algorithms so the proposal in this draft is that to that we should implement this but not use it um that's a good question so the the what we are what we are thinking of is that you you might need at least as a secure option for the root or for safety of these where you you have a slightly better understanding of how that zone is operated and how many signatures you need to create etc um if i think of of the i'm and i i don't know the number on the top of my head it's it's 86 something the rfc that that sort of specifies which are recommended algorithms and and which ones you probably should not use this this would not be a um an algorithm that you must implement but it's something that you you probably should try and implement but you must be able to validate it that's that's how i look at it um but yeah that's a fair point"
  },
  {
    "startTime": "01:36:02",
    "text": "thank you okay paul hoffman so we're a little bit short of time so please keep your questions and answers brief sure so this is actually a comment roland you said that you're also considering doing an implementation uh considerations document i absolutely would not support adopting this spec in the working group until we have that implementations document in parallel given that you just listed a whole bunch of mine fields and such like that i think that in fact i think the implementations consideration should be part of this document okay thanks for the comment all right stephen so in general i think doing this would be a bad idea doing it now would be a really bad idea um so the problem with stateful signatures is that it creates a whole bunch of weird corner cases and the nist competition is likely to produce non-stateful post-quantum signature schemes in the next year or two it would be a much better option to wait for those and never think about stateful signatures for dns sec in my opinion i don't share your optimism but okay hi peter thomason from the esec um you said that um these signatures should be a safe fallback option but not um you're not pending for them to be the preferred or a preferred dns mechanism um so i wonder how you can have both at the same time let's say for example you need the fallback um in place when in 20 years from now something happens with the post quantum algorithms then you already need the root zone to be signed with these things and if you already need to have that in place then it would have to be supported on all resolver infrastructure and already i guess within a few years from now so"
  },
  {
    "startTime": "01:38:00",
    "text": "how does it go together with it not being preferred in the sense that you can avoid deployment maybe you can't and then it would be kind of required to have it everywhere wouldn't it yeah i think so so i think that's that that might be conflating two things so um when um when when we say that this should not be the uh preferred option what we mean is that operationally it should not be the preferred option right so unless there is a very good reason to deploy this operationally you shouldn't do that um the um [Music] support for it uh implementing support support for it is a different matter and because signatures effectively have a zero year shelf life you can deploy such a fallback option only when you really need it so you should be ready to go to deploy it but that doesn't mean you need to use it in operation so there's no need to actually use this in operation for signing until you really need it because of the way the nsx works okay um martin i don't know how you did it but you actually somehow skipped into the closed queue so i'm sorry we are out of time so please send your comment to the mailing list thank you okay thank you roland please we continue the discussion on the mailing list and your draft will also be be part of the survey thank you um next up is uh donald not eastlake you want to run your own slides we run your slides uh sure i can run my slides well if you give me control of your uploaded slides we we do have them ready also if you prefer well i mean i was gonna i could run yours or i can let's see yeah i can do there we go so"
  },
  {
    "startTime": "01:40:00",
    "text": "so i challenge you to finish all the slides in 15 minutes uh i'll see if i can cut be quick here so that the last presenter has a chance so this is about expressing communication service requirements in dns queries this is a zero zero draft so it's not doesn't claim to be fully uh polished at this point and so the goal is you can get answers back that depend on what for example you'd like a minimum latency connection or maximum bandwidth or like wants to work through recursive servers so things like meta rrs are not really appropriate with no changes of course to the dns on the wire protocol or messages so you know the only things that are survived through a recursive server is the q name q type and q class and really q name is the only practical place to encode additional uh information you might want to encode so there's already people well aware it's a very knowledgeable audience i'm presenting to i must admit uh ways to encode uh things about the service or the communications protocol you're gonna use and if you can get different responses back depending on whether you're gonna be talking tcp or udp it seems reasonable you might want different answers based on the kind of quality of service you want um so these underscores are used in a number of ways these days there's also the rldh restricted letters digital hyphen labels defined in 58 90 which effectively i view them as ways to enter to change the interpretation of labels so currently the only one defined is the prefix for internationalized labels that can be interpreted as having a strict set of unicode in them um and none of these are the rldh labels i think should affect anything to do with uh protocol on the wire or very special processing or security"
  },
  {
    "startTime": "01:42:03",
    "text": "so what sort of things might you want to say well you sort of a course qos might be useful where you just ask for like minimum jitter or minimum packet loss or things like that or with reasonable values for other things or you might actually want to specify a more precise metric where you say a minimum acceptable bandwidth is some number of megabits or megabytes or whatever so the draft is zero geograph has a specific proposed format for for uh labels and uh yes they might start with qs hyphen hyphen following the rldh format and qs either way happens to not be a country code or you know it seems like it sort of stands for qos to some extent and then just have a hexadecimal encoding of tlvs where you might just have one tlb or possibly several and a hexadecimal is not the densest encoding you can imagine but it avoids case uh insensitivity problems with dns and should be easily readable for debugging and other purposes like that so what would an example be what it would actually look like so this is one which according to the information currently in the draft would be a request for uh minimum latency uh for example.com so qs hyphen hyphens prefix happens to be that type 1 is is course qos this is that's one byte of value and the value 08 hex is currently the value suggested in the draft for minimum latency so qs hyphen iphone1108.example.com it means i want an answer back that is minimum latency so what what kind of data are you getting back well there's lots of different things obviously but one possibility just a one would be a semantic address it's an address that has not just the identification of a interface identifier but also encode"
  },
  {
    "startTime": "01:44:00",
    "text": "some additional information about how to connect to that related to routing or how to set the header fields or who knows what so you might just be doing a quad a retrieval with a quality of service specified and get back uh a ip address which has some additional information in the lower order bits and uh one way you could use that is you could have a thing where your application does the dns query takes the ad as it gets back and just makes a network connection the application is could be pretty ignorant of all this stuff and doesn't have to understand these things and maybe the first hop router or something like that notices the uh special the uh quad the ipv6 address and does the right thing to get you the network connection with the quality of service that you are seeking um so you know what how much you have to tweak there's really no necessary change at the resolver end what do you have to do with the server end well you know obviously you can if you just want to test out your your resolver or application or whatever to create these things you can always just ignore this if this is a prefix label in front of your domain name ignore it by using wildcard if you have a small number of values like these course qos values or a few specific metrics you could just have those as entries in your zone whatever but indeed if you want to support more general qos metrics where somehow it dynamically computes something about how to handle uh your connection based on uh more precise or complicated qs things then there's some sort of thing would have to get done at that authoritative server perhaps calling some back end or other system to get some information so this draft also creates a and a registry for these are ldh labels which i think is a good thing and has its expert review and give some suggestions to the expert and creates a registry for the uh"
  },
  {
    "startTime": "01:46:00",
    "text": "these service request types i'd like to ask people to take a look at the draft certainly welcome any kind of comments or suggestions on the draft and again here's the draft uh name and uh i'd like to ask if there are any questions or comments at this point thank you thank you donald uh ben please go ahead hi ben schwartz i uh i'm going to i think say my my standard status thing which is this sounds like a job for service bindings uh so i i don't find the idea of um encoding the quality of service in in the label very appealing um although i think i understand where it's coming from uh i you know i think that i'm uh if i were presented with this problem what i would do is to say that each of these different quality of service labels is something that can be encoded into an svcb record so you query for the name you get back a list of options and those options can be tagged with an indication of what kind of quality of service you might expect from them i that's uh certainly something i'll look at a bit but uh that requires of course that the application uh know about this stuff whereas what's been expressed here especially if you can use a simatic address means you could have a an application that is just given a domain name and application knows nothing about any of this but it magically gets its uh network connection with the desired quality of service because when it gets the quad a that has something in the bottom bits that they caught or make that be a semantic address that causes the quality of service it wants yeah i it seems like a little bit too"
  },
  {
    "startTime": "01:48:00",
    "text": "much of a retrofit to me um you know if if you think that that kind of degree of retrofit is is required i think it would be valuable to understand a little bit more about what the motivating use case is okay thank you this is always from the swedish universe registry um in my head quality of service is a property of the network pass and i don't know how the uh resolver or resolving paths might intersect with the network path but there's absolutely no reason to assume that it does and so i really don't see how this solves the qs problem uh you mean the resolving path the path by which the the queries and responses are that that dns query the the quality of service on dns query has nothing to do with the this is the idea is to get back interviews but this query i have to send out to you know make my network path do something if i send that to some resolver somewhere on the internet how would that make my network path better the idea is that the information you get back in response to the query would and it might be able to do that even if it's just a query for an address by using semantic addresses it could be querying for other things as well of course but the idea is that the information returned would be uh such as to make it either more likely or certain or whatever that you would get the quality of service you wanted okay i mean if you think there should be more information about how that would work or something then that's a perfectly reasonable criticism of the draft but i i think it's possible that that would work i don't know okay thank you thank you all and uh well please continue the discussion on the mailing list any other questions you have for the"
  },
  {
    "startTime": "01:50:01",
    "text": "working group donald i don't think so okay thank you okay our final uh draft presentation will be by thiru so sorry donald can i stop sharing your slides you can oh okay are you convinced i can do this perfect thank you do you want to run your own slides or we do that for you yeah it would be great if you can run the slides hey good morning uh hey good morning everyone uh we presented this draft uh in the last two idf sessions uh next slide please yeah this was originally uh unstructured dns opera page that was uh presented at several idf sessions and based on the feedback we got from the working group this structured uh error page draft was being created so the idea was basically to get a possible json for the user and the id troubleshooting especially when there is dns filtering and the client displays or logs the json with its own uh that's the background or recap of why this draft was created next slide please one of the major comments that we got in the last itf was that uh don't introduce a new adns option rather free you see extra field in the adhd option itself and that's the change we did in this version was to introduce recast uh extra dn text field in the ada option to have a structure json fields next slide please so as you can see in this figure it's it's going to be the extra text field with the json attributes to indicate reasons why the domain was being filtered by uh the dns filtering service next slide yeah we had also updated the security considerations based on the feedback from the working group that uh"
  },
  {
    "startTime": "01:52:01",
    "text": "the error page would be displayed only if the error resolver encrypted resolver has a sufficient reputation and in the previous version of the draft we had mandated encrypted resolver as a prerequisite for receiving and processing the extra text field and we had also added text to make sure that this error page is going to be displayed in an isolated environment just like captive portals not to get confused with the actual content provider giving the content back to the user and uh in case if the encrypted resolver is not uh trusted by the client then only the host name of the encrypted resolver could be provided that way the client does not have to visit the error page pointed out by the c and r fields so those are the major changes that we did to this draft and uh next slide please yeah i think we have addressed all the comments from the working group and we have been presenting this throughout several itu sessions and would like to hear comments and requests for working group caption thank you are there any questions from the room remote or in person ralph please go ahead not a question really more more comments so this shows a good usage of the sort of extended dns errors and what you can do with it and i just want to say that we have an experimental implementation for that and uh if somebody wants to work with us i'm happy to to work with them thank you thank you um tommy you're next all right hello tommy paulie from apple um i just wanted to speak up to mention that i'm definitely supportive of doing some work in this area and solving it on the client we are seeing a lot of cases where"
  },
  {
    "startTime": "01:54:00",
    "text": "there is filtering going on or blocking and the the signals are not explicit and they often interfere with uh actually displaying a page if they're redirecting you to something but your original connection is using tls so this would be a very useful mechanism i think you know the details may need to be iterated upon by the working group but it'd be great to see this in in the working groups hands and iterated upon there thank you thanks tommy uh ben hey my uh my biggest concern with this draft um it seems like it's still there fundamentally which is that this uh this draft is is at base meant to uh to create a situation where the where the client ultimately opens a web page that is selected by the resolver and that's just a really big change from the security posture that we currently have between clients and resolvers even resolvers that we are you know that are in some sense trusted even even like a trusted recursive resolver program like mozilla's does not allow the the resolver to in any way direct the user to open particular pages or websites that is the the real point of concern for me if we could find a way to keep this truly machine readable so that the the information is presented by the resolver to the user agent and then the the user agent can figure out how to present that information safely to the user that would be much more appealing to me thanks ben and there's several fields in the station structure like the"
  },
  {
    "startTime": "01:56:01",
    "text": "justification field which is just a plain text and not an error page in itself so the error pages are an optional text and the human friendly organization name or the justification text fields are purely machine passable and the user agent can display whatever appropriate error page it wants to display the other fields are pretty much optional so i think as a working group we can decide which one is mandatory which one is optional and then make uh progress on this sure so yeah if the draft could be clear about what's uh what's optional here and and focus on the machine readable components that would be much more appealing to me um one way that you might one way that you might pursue that is by replacing some of the the metadata information with like a contact field like a mailto link if you know if you need more information you know here's the link to here here's an email address to talk to tech support here's a phone number that you can call sure okay thank you thanks ben um no other questions and comments so dear yeah we were testing your patience sorry yeah we explained the procedure now for adopting new working group documents so we will run the survey and uh your drug will be surely part of that of the survey thank you yeah okay thank you all we come to conclusion of the working group i would like to thank all the presenters of drafts thank you very much i would like also to thank all the remote and in-person attendees here in the dinosaur working group it's it's good to see you back here in the room and i would like to thank alexander for helping us out as a delegate and that closes the working group thank you"
  },
  {
    "startTime": "01:58:00",
    "text": "for for today thank you warren the dinosaur working group is closed for today"
  }
]
