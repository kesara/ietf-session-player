[
  {
    "startTime": "00:00:48",
    "text": "[Music] [Music] right good morning everybody thanks for making it out to this chill working group room for the inaugural mops working group session I think you\u0027re familiar with II know well and you should note it well all the usual conditions for participating in the IETF apply um a couple of bits of administrivia assuming there are no bashes to the agenda we have blue sheets that are about ready to circulate are circulating Glen did are about to circulate um I understand that we are in that several people have a conflict with the TSV working group that\u0027s on at the same time so I expect there will be people coming in and going out for different agenda items in either working group and we\u0027ve noted that we "
  },
  {
    "startTime": "00:03:48",
    "text": "should make sure not to collide with that working group going forward so there\u0027s that but it also means that we have a javis tribe Thank You Aaron but we don\u0027t actually have a note-taker at this point is there anybody willing to be a note-taker for this session please don\u0027t all volunteer at once Thank You Ronnie um so okay great thank you very much we can move on so are there any batches to the agenda that\u0027s been posted for a week or so see me not moving on yeah so this is our inaugural working group meeting we have met several times before in multiple contexts and different guises so I just wanted to remind people of what our what our Charter says there\u0027s a lot of soliciting of input here and regular updates so this is the general area of focus for this working group and what you\u0027re gonna what we\u0027re going to talk about this morning has a lot to do with updates from various I\u0027m actually supposed to stand on this pink thing of them much better now I can talk to the mic microphone stand so there will be a lot of updates from other from other areas from other organizations one of the things that I want to do with this session because it is a working group and and not just a conference session is start to work out our mode of operation so I\u0027m hoping that we will actually have some interactive discussion around some of the things that are presented in terms of you know ask questions about stuff as being presented on work going on elsewhere we have one discussion of you know an awareness-raising with regards to an issue with streaming and quick we can have a discussion after that presentation about you know is there is our further follow on what do we think about it so on and so forth so generally speaking just all that say that this really should be a I\u0027m hoping this will be an interactive session notwithstanding the fact that it is Thursday morning after a very long week thankfully it\u0027s not Friday morning after a very long week okay all right so unless there\u0027s any questions so far [Music] yeah this is just a further note about my earlier point about what we\u0027re here to do our outputs are meant to be some documents the presentations that get posted in Proceedings for all see and also potentially some dispatch like activities where we might refer people to other working groups that last aspect "
  },
  {
    "startTime": "00:06:48",
    "text": "I think will be particularly useful when we have people who you know join our efforts from other organizations who may not know I may not know where to go in the IETF or may not know where to take their work within the IETF so that\u0027s sort of the general context and that\u0027s it for me so next up Jake hey Jay Colin and I was gonna give an update on the the one document we have so far thanks okay so the doc status I think it went on to the milestones list when we first started the working group or something but I don\u0027t think it\u0027s been adopted by the working group and I\u0027m not sure whether that counts as consensus or not I guess probably not anyway we probably should discuss that and make a decision the some of the offline comments last time there were some there were several supportive comments of the general direction a few refinements and a few people seem to indicate that it\u0027s it\u0027s not totally clear that that the duct is super useful but I\u0027m not sure whether that means that that we should refine it until it\u0027s more useful or that the general concept at all is is it not necessarily worth writing down so a little discussion on that would be lovely yeah I haven\u0027t actually touched it since since supposed to hit last time but I do have a few there was some feedback from ollie so I have a few - dues to put in there and I think that the suggestion from from lastly that this is not a taxonomy it\u0027s a operational considerations for media streaming for from edges indicates changing the name as we perhaps adopt it and and put it forward and then there\u0027s the I would say the kind of most there is some a lot of the feedback I got is that there is one really good insight that it would be great to get in there somehow which is the sort of mismatch between the capacity and the demand for like large-scale events and you know in the sense of presenting it with good archival value this is these are the slides that I had last time I think I\u0027ve shown them a few times it\u0027s you know just the the when you when you take a kind of large scale number actually published a new number since then I probably should have read on the math a little bit so it would move up I Oh call "
  },
  {
    "startTime": "00:09:49",
    "text": "it 2.4 million instead but the this the core insight is the same it\u0027s just that when you take a sort of large-scale provider just the the thing that they\u0027re able to do even even a good-sized CDN is it doesn\u0027t match the sort of real capacity to consume the popular things from the audience size even with the demand we have today and you know it\u0027s not clear depending which numbers you look at like how much worse it\u0027s getting and how much better it\u0027s getting in a different time so the best idea I have on how to capture this in a sort of you know an insight that applies across time is maybe the trendline with the accumulated annualized growth duration rate or something I\u0027m not sure that\u0027s good I would love suggestions on how to do it better and just other suggestions on what should or should not be in the stock would be very welcome so that is my update do we have any questions discussion adoption calls that the chairs would like to perform etc thank you Jake so maybe we can start with opinions on that last point about what is an actual useful way to explain or articulate the demand versus capacity disconnect that you were describing in this graph in a useful way that will be worth publishing for archival purposes Kyle Rose Akamai how about just if you were to unicast all of this traffic how much bandwidth would that be so I put a formula in there one for how much it is per you know per stream with with unicast and did a couple of example example multiplication is to say if you have this many millions of years it means this much bandwidth it doesn\u0027t really make that point you know in as punchy away as the as the mismatch does right it just presents the raw number the oh if you have half a million users you need you know 600 gigabits which may or may not be easy you know in two years time but or today it\u0027s just the the question of like there is a mapping but making the point that there is a mismatch and that the mismatch has been sort of growing and persistent and and problematic is the part where I feel like it\u0027s it\u0027s weak and could use a "
  },
  {
    "startTime": "00:12:49",
    "text": "proved presentation in a way that that still carries some punch right with recognition that moving forward networks changed right that\u0027s that\u0027s like the part the part that I\u0027m struggling to really communicate in the document so this is the best idea I\u0027ve got so far I think it could be improved in and I\u0027m looking for suggestions yeah thank you Mark Nottingham cold just thinking out loud maybe it would help to kind of dive into where you think the constraints are you know like you know these are big numbers and you know but but the Internet has grown over time and CDN capacities have grown over time is it you know is it last mile probably not I mean at least where I live everybody\u0027s getting hundred mega bitten more pretty soon you know is is it middle mile maybe that\u0027s more interesting is it CPU is it is it network card is is it the economics of your current relationships you know maybe just unpick those bits might help a little bit that\u0027s a good point thank you yeah the in some cases it is the access networks in a sense it is inherently the kind of global provider in some kind of you know which may be multiple yeah thank you that\u0027s good there\u0027s another dimension to this which a Dave or Ansari which be really useful to capture which is that people tend to assume there\u0027s a high correlation between the very high demand content and live content and to some extent that\u0027s true if it\u0027s a football game or a rock concert or something like that but there\u0027s other forms of very high demand content like newly released episodes on Netflix where pre placement in caches close very close to the edge tremendously ameliorates this this bandwidth unicast bandwidth need without having to resort to you know the kind of multicast tricks we use that are needed for live live content so that\u0027s the second dimensions of the problem I think it\u0027s really useful to capture somehow you thank you I was kind of bundling that under the live verses on demand comment that I got from Ali last time but I think that\u0027s a helpful refinement thank you Dave blending from over here maybe even breaking up to two separate charts one showing sort of the bandwidth requirements for file based or highly cacheable content and then a second one that\u0027s sort of more live oriented because then particularly for the live I think an additional parameter to try and start catching is latency as well because with the because as we evaluate you know other approaches with highly cache content latency isn\u0027t a big deal you for watching a movie and it takes five seconds or 10 or 20 seconds from initiation to your scene that\u0027s fine but "
  },
  {
    "startTime": "00:15:51",
    "text": "when you\u0027re in live content since it tends to be sports oriented latency is really critical and you can\u0027t do 20 seconds you can\u0027t do 30 seconds you need to get like you know 2 to 6 seconds yeah maybe trying to measure that and the other aspect doesn\u0027t suggest was there\u0027s this other component here which is that you get the adaptive bit rates coming into the equation and so I\u0027m not sure how to capture it but I think it\u0027s important to recognize that these rates are variable even during a session and there\u0027s some latter that you and so maybe it\u0027s a it\u0027s introduced a standardized ladder a lot of this content and then do the charting based upon those you know because a lot of typically has these live rates that you can choose from stared as other sounds that is good for this tact it\u0027s a bit fascinating idea thank you yeah I do have some text in there about a br that that speaks a little bit to these points I can\u0027t remember off the top of my head not but um yeah that\u0027s that\u0027s I will overview and and see if that addresses any of your comments thank you okay so I\u0027m hearing engagement at least in this particular point and it sounds like it would take a bit of document real estate to tease apart some of the possible approaches to answering it so that sounds to me like there is interesting this document going forward and to your point about whether or not it\u0027s officially a working group document I think I think I don\u0027t know if I want to try it hum this early in the morning but um perhaps I should this it is indeed the document is indeed in kind of a strange state because it was adopted as one of the milestones as we went through the chartering process but on the other hand we didn\u0027t have a formal working group discussion about whether or not we wanted to adopt it so I guess at this point I would ask I will ask for a hum are we in favor of adopting Jake\u0027s document and carrying on updates to carry out this discussion over the next you know month sorry months 18 months whatever she tried to tease apart and come up with good articulations of these things so in favor adopted of adopting the document please hum now against adopting the document as a working group document please come now all right thank you all and thank you Jake so so what I heard was some hums in favor of adopting the document and a very little disagreement with adopting it as oh yeah yeah sorry maybe a home for those who how many have read it just comment from the Java room you get three hums in favor okay there\u0027s three additional hums in favor thank you great thank you great anything else "
  },
  {
    "startTime": "00:18:55",
    "text": "seeing Aaron Hawke akamai not speaking for Jabbar but for myself so I confess I have not read the document but the discussion I thought was a caused me to think that there you know a CDN based media delivery is a pretty mature area there are industry standard key performance metrics and I think to the extent that we can avoid reinventing those in the slightly different way which will probably cause more confusion than it solves it\u0027s probably a good thing so I think it\u0027s probably a good thing to pull that information into this community since maybe not everybody here is familiar with those but I some of the discussions here seemed like pretty well trod territory in the in the CDN land and so let\u0027s maybe we can find some citations that are good good sources for that stuff but it sounds very useful if you happen to know any such annotations please send them along I would very much appreciate it they talk to our marketing department let\u0027s say let\u0027s keep engineering shall we that\u0027s the point well-taken although I will also observe that part of the part maybe that\u0027s exactly the right answer and I\u0027m certainly all for not reinventing something as well well shaking out elsewhere as long as at the end of the day we wind up with something that will communicate to the IETF community something about the scale and scope of you know problems to be addressed yeah yeah I wasn\u0027t trying to imply otherwise I just think when we\u0027re doing something that if already exists somewhere else we should probably just suck it in by boundary I have a comment from the job room that I wanted to pass on Chris lemon says he\u0027s read the document it\u0027s got room for improvement looks like it\u0027s going in a direction that will be useful with excellent Chris oh sorry Scott Davis also said I proved it I am pretty much aligned with Chris so two more thumbs up excellent thank you this is almost Jake are you looking for co-authors if I have any volunteers for co-authors I would absolutely love to offload some work yeah yes thank you that would be lovely anybody who you know wants in RFC on the resume this one looks like a good path forward yeah okay great thanks so when I send to the mailing list that for confirmation that you know we\u0027re dropping this as a working group document I will say that you would be happy to have a co-author yes thank you good so everybody here can think about whether or not they want to help and you\u0027ll have an opportunity to volunteer okay thank you very much just singing I\u0027ll run you but I\u0027m doing the notes on the eat up it so if anyone wants to okay "
  },
  {
    "startTime": "00:21:56",
    "text": "Thank You Ronnie yes so that it be I didn\u0027t put the URL to our etherpad in my slide deck but it\u0027s on the agenda page anyway yeah if if other people are able to join in on the etherpad and help Ronnie with the notes that would be awesome okay Warren suggests that we please save the etherpad every now and then in case it accidentally deletes itself okay Sanjay please all right hello good morning everybody my name is Sanjay Mishra I wanted Verizon but I\u0027m representing as a member of streaming video Alliance so just giving a quick readout on these tubing video alliances lab initiative okay so the way I want to run the slides are just to give you a quick word or two on what streaming video Alliance is and the different working groups within these streaming video Alliance and then kind of jump into the SVA labs initiative as seen from one of the working groups implementation of it and then some update on what we have done within the ITF all right so two things here one is that streaming video Alliance is really nothing but an ecosystem of content providers content distributors ISPs and the network technology vendors working on over-the-top streaming media issues and by and large the Alliance depends on ITF protocols pretty much for all aspects of streaming but also the fact that the ecosystem provides a way for companies with various degrees of experience and extra expertise can work together and work jointly on issues and coming up with best practices or guidelines or in many cases specifications as well and the next slide just want to give you a warning there\u0027s a lot of words here I guess the objective was to see how many words can fit into a slide well and so I think that got done but obviously you can\u0027t read anything there so there are basically a lot of working groups within the streaming video Alliance and just to call out just maybe one or two or three of you know what the working groups are focusing on so let\u0027s take an "
  },
  {
    "startTime": "00:24:58",
    "text": "example of live-streaming working group they focus on issues like quality latency and scalability measurement group focuses on Keowee networking and transport working group focuses on streaming at scale privacy and protection group focuses on piracy and content protection theft user privacy etc so there are various degrees of work that goes on within each of these working groups and they end up producing as I said various levels of documentation best practices is one common thing some specifications and also guidelines based on the experience in the industry and in addition now through to the IETF mantra of running code Alliance also works on specifications and api\u0027s is actually doing a lot of work now to turn those best practices and in specifications specifically into api\u0027s and they\u0027re doing that by establishing an open-source platform Alliance calls this and SVA Labs initiative and I think the following slides gives you a little bit more in-depth view of what that is so if you look at the SVA lab from these streaming working groups the open caching working groups point of view the open caching is I don\u0027t know if it\u0027s it\u0027s a fancy name but it really is is something same as the concept within this ad and I where you have an upstream CDN which can be a commercial CDN or even a content provider and you have a downstream CDN which can be an ISP so it really is using the CDN IRF sees in terms of how an upstream CDN can delegate content down to the downstream CDN in cases where it decides that that\u0027s the best path to do so open caching is really leveraging CDN IR FCS and the concept is very very similar and the you have really three main pieces within the open caching architecture what you have is the control function that are within the upstream CDN and you have the corresponding control functions within the downstream CDN and then you have the open caching nodes which are nothing but really distributed caching or CDN environment within the downstream OSI NSR what you see down below there that are distributed through the network footprint so it distributes the content easily and the API is basically connects all the dotted lines so what we\u0027re doing is that identifying all the key functions laying out the api\u0027s and then we want to open-source those and make them available for make them available for industry for collaboration in addition to that the working group is also has set up a testbed where we are testing features such as client stickiness what we have seen is that some of the media players don\u0027t always "
  },
  {
    "startTime": "00:27:58",
    "text": "stick to the last redirected source so the behavior is not very consistent so we\u0027ve had some conversations previously in IETF on this but I don\u0027t think we had enough data that we could bring back to pinpoint exactly where the problem is whether it\u0027s the standards or whether it\u0027s just the implementation of those standards where the inconsistency is so what we\u0027re hoping to do is that run through our test pad generate enough data and learn from it and see if there\u0027s any issues that point towards the standards if so then we can bring it back to the ITF okay and just to kind of give an idea on what really open caching has done so far from these using the CDN our CDN RFC\u0027s really there are three or three major areas I would say that we can publish the api\u0027s one we would call as the service provisioning API is and these would be both directions coming in from the upstream CDN to the downstream and also in cases from and then from downstream CDN functions down to the ocn footprint and capacity is usually going up from the downstream CDN to the upstream CDN providing the footprint information so that the localization can happen easily and then the content management API is these focus on coming in from the upstream CDN to the downstream CDN specifically for managing the content down to the ocean level we also have api\u0027s which we have not standardized them either because companies felt that some of this was just not quite there yet and in some cases these are very specific to to features and functions logging is one such API basically billing and performance related API is that we have and then the capacity insight which would be something that the downstream CDN can advertise to the upstream CDN so that they can figure out how to allocate content in future so these are not really the main API is functions that we have and the intent is to make these available through the open API but the hope that we will get a wider set of contributors also these would be really restful api x\u0027 and we plan to support in multiple languages so just to kind of bring all of that together the intent is that I think we have done enough work in terms of whether technology is and now that we have the information enough to publish api\u0027s our hope is that this will increase adoption also encourage collaboration from the industry and as we publish these API is one of the core benefit we also look at is that we can bring back the feedback into the IETF and we have done that based on the open "
  },
  {
    "startTime": "00:30:59",
    "text": "caching implementation we\u0027ve got a couple of RFC\u0027s there are within this CDN our working group being these are these have been accepted as the working group documents and they\u0027re just progressing through the typical process of working group evaluation I think with that said that\u0027s all I have and open for any questions great Thank You Sanjay um I guess one starting question I have with regards to you\u0027ve outlined a lot of the work that\u0027s related to the CD and I work have you provided an update to the CD and I working group or what\u0027s the status err I mean they don\u0027t seem to be meeting this week but have you provided in so we\u0027re working closely with in the so the documents that I listed out so yeah no I I realize that you\u0027re working with the CD and I work what I\u0027m where I\u0027m trying to get at is that this sort of provided an overview of how industry is using the existing standards and it strikes me that you know that presumably would be interesting directly to the CD and I working group or maybe their focus on the work that they\u0027re getting done and don\u0027t have time for these industry updates I mean I\u0027m trying to well its get a sense of whether or not some of the Kevin MA from one of the co-chairs has been in the loop on what the SVA is doing streaming video Alliance is doing and he\u0027s provided some input and guidance as to how we should be looking at you know coming up with the work so yeah he\u0027s given us some input and guidance right so I so then I guess my takeaway is from what you\u0027re saying is that this is a useful place a useful venue for IETF in general an update on how the CD ni standards are being used in industry elsewhere without getting in the way of the actual CD and I work correct okay um so with that i arin Fulk again minor comment could you take a look at the slide that\u0027s on the screen now no no just go look at the screen turn around look up can you see the link no no no they cannot yeah well this is the supposedly a new fonts that in the color scheme that the SBA came up with it I really don\u0027t like it and you just can\u0027t really read anything on it so yes you could send some feedback in the idea yes I take that feedback thank you I\u0027ve already given this feedback to them but you know certainly this is helpful yeah I think you just got 30 more feedbacks so sorry I think we have a clear message for the ITF back to the SVA your template is those are my words I do have a comment from the jabber room but Chris lemon says it\u0027s been a few meetings since the CBN I met and the CD and I work is mostly proceeding on the list right which makes sense I just wanted to make "
  },
  {
    "startTime": "00:34:01",
    "text": "sure that you know I don\u0027t want mocks to get in the way of other other work okay any other comments or questions all right Thank You Sanjay all right thank you [Music] hello a so Glenn Dean Comcast NBC Universal wearing a different hat right now so I was a on the program committee for the recent centi 2019 technical conference it was a very just experience and what I want to do is provide some feedback or some sharing of things I observed at that event that I think are relevant to the ITF and hopefully sparks some interests and work here and some lessons so for those who are unfamiliar with it simply is essentially a standards body that focuses on what they call professional content tips typically came out of the TV world but involves both production a lot of studios that don\u0027t do TV but do movies as well participate as empty so it\u0027s really professional creation of content professional distribution of content and that whole thing one of the things they\u0027ve been very engaged on for the last few years is a change from an old transport protocol called SDI which is what all the equipment in a studio historically was connected with into things our IP base which are very much you seen or using ITF based technologies and protocols so one of the things I want to bring back your observations of where that is right now so that\u0027s clearly still ongoing there\u0027s still a lot of studios out there with umbrellas they got a replace a lot of equipment that are still migrating to 2110 and and going on to the IP networks but a couple things that really came out of the the conference that people were found interesting to talk about and really engage on microphones where the challenges of building IP networks for media there are several people that gave talks on network architectures that were designed entirely to allow for extremely high bandwidth low latency connections between editing stations that you mean Bay\u0027s cameras and recording devices and storage bays and clouds it was you know a few of the things as a different guy made me cringe when I saw them because you know some people said well you don\u0027t want to build love multi-tiered hierarchical networks in your shop you want to build one big giant flat network where everybody\u0027s on the same segment so you can get high bandwidth it made me cringe roll into the ITF there may be some guidance work that we might want to draft some materials on for best practices of how "
  },
  {
    "startTime": "00:37:02",
    "text": "to build such networks and I realize that crosses a little bit in the operation thing we have expertise service so for future working this group that might be an area we want to sort of tap into and play with the second one was something which kind of surprised me even though I I do live in this world I didn\u0027t realize how much focused people really had on time still yeah there is numerous papers that talked about time and this has been brought on because the old protocols did carry time information around with them and all the devices and were able to synchronize and be you know engaged and synchronizing their actions but also when they blend content together in pieces that all worked really well under IPE they lost that and so one of the things they\u0027ve been trying to figure out 2110 dear daddy and mechanisms for sharing time but the time sources are then becoming a problem and so you know it\u0027s a complex environment they heavily have doors PTP which is a time standard of course if you if you\u0027re familiar with it there is other mechanisms though they didn\u0027t lock into only p2p there\u0027s other ones they are proposing discussing because they have a lot of scenarios where they\u0027re offline or they one of the scenarios that was brought up was if you\u0027re trying to source from GPS information and your GPS is all go down or your antennas are covered in snow you have a bit of a problem how do you deal with that it\u0027s some new work that they were trying to get other parties to take up so likewise there may be work here the ITF or you may want to take up around time again I time is time time is coming in for time it\u0027s a question in the middle before I go on the next point or on even I heard I had similar question because people that were trying to transport the it depends how you transport the media and they were trying to transport the media using RTP but use the GPS times them how to synchronize between the g3 at times then how to convey in in RTP because the precision is different of the of the information so right so I think it\u0027s important because I mean again it would be good also for us to know what how do cups how the data is how\u0027s the medians transported in order to figure out what we can do also here in other working group in order to to help with with this timing it\u0027s what was mostly because there was sending the information from different sources and they want to to combine them when in the the production area and that\u0027s where the timing and they were getting from the camera the GPS time was that yes and and a tad on that one of the concerns that many people started raising was worries over security in integrity of time because if you imagine a lot of stuff that\u0027s getting you know produced and streamed out live if you start tinkering with the time you can cause a lot of problems make screens go dark and cost people a lot of money or if you wanna be really nefarious you know makes the time go bad and then start inserting your own material that\u0027s out of sync with it cause a lot of sort of social activism "
  },
  {
    "startTime": "00:40:02",
    "text": "if you will Aaron I comment from the jabber room Scott Davis says time is everything in production there are fine grained time mechanisms and legacy productions that is critical to make the processes work I think he\u0027s agreeing with you do you have more I have a comment but I can wait to you know I this is my one slide I I have one final point on this one slide and then go ahead make it okay the one thing I got really excited about when I was sitting in this conference was the number people started benching ipv6 historically the the you know professional media industry has not been a big supporter of v6 and I people started talking about v6 was like yes so I just wanted to bring that back to the ITF they\u0027re starting to listen to us and they\u0027re getting on board which is awesome that\u0027s great so Aaron Faulk Akamai I this is the time well see there was another presentation that\u0027s actually in a few different forms this week on the itu-t t30 vision of you know IP of the future first I want to say it looks like SEM T has beaten them by looking at 2110 so that\u0027s great that you know sort of getting a little further into the future but at a more serious note time actually appears on their list and one of the things that sort of left me unsatisfied about the discussion there was it was more sort of a statement of we need better precision and here what this what I\u0027m hearing here is that we\u0027re actually running better precision in lots of different ways and and so the question that I have is what exactly are the problems and challenges that are coming out of this I think it would be actually really useful information for the IDF to understand like what are the limitations where are things not working so that we can actually talk about some potential work here I you know this is one slide with a few bullets so it\u0027s very high level but I think that this is a very valid activity for this working group is to try to bring in some of these challenges from operational areas on like where the protocols we\u0027re hitting some walls and what they can do where there\u0027s too many solutions or like the stuff that Roni was just measuring mentioning and so I think if we could get some drafts on like you know hey here\u0027s an industry situation where you know this is painful that would be really helpful for the IETF to say oh well here\u0027s an area or maybe we should apply some engineering I agree with you I fully agree that I think specifically are on time capturing some of their use cases they\u0027re thinking through them let\u0027s capture him bring them over in here and get to work on it I never ran again so on the issue of time historically the media industry in the networking industry have lived in completely separate bubbles and we\u0027re seeing sort of the consequences of those bubbles evolving in parallel but but that may be entirely irrelevant because "
  },
  {
    "startTime": "00:43:03",
    "text": "the cloud data center people have created a third bubble which is probably going to blow both of these bubbles away because they met because most the data center operators now have a sub nanosecond atomic clock synchronization across their entire data centers and they don\u0027t do it with either of the things we\u0027re talking about here so it\u0027d be really useful to spin them into this conversation because if you\u0027re gonna run in one of those data centers your problem basically evaporates because you have nanosecond scale time synchronization across the entire data center melting point that\u0027s your point Dave I had not considered that so if you\u0027re only worried about distribution you\u0027re right the these groups are also worried about video production and video productions running in the in data centers file infrastructures okay so all right what top point is look at what these guys are doing because we can put Tomic clocks almost anywhere now at reasonable cost well and you know there isn\u0027t going to be one single way people do this it\u0027s a very diverse set of environments it ranges from people doing field production in the jungle recording a movie to people connected to live networks to people in stadiums doing live streaming at low latency so there there\u0027s a lot of areas here to get really into different nets and different details all I\u0027m saying is that the way the technology is going it is reasonable to assume you can afford an atomic clock most places but can you carry them on an airplane can you can your plane yes so go through Thoreau through TSA awesome yes you may need to have a refrigerator in order to keep the temperature stable though so going back to Jake\u0027s document I again I haven\u0027t looked at it but does the taxonomy that he\u0027s looking at it\u0027s mostly on is it mostly on the distribution side results on the production side it was primarily on the edge side rather than that just in the center so I think that there\u0027s yeah so I think that the focus that I think it needs to have is is is more on the you know do we have the capacity or looking at the capacity gap okay well I think that Glenn\u0027s raising a different set of questions on the production side and so maybe a taxonomy document to help inform the IETF on like what do these environments really look at this is kind of like what TCP Sat did in the early days was to try to pull in some information on like you know what our that what\u0027s the variation in in these environments to understand what I mean from a networking point of view okay so maybe we can look forward to causing a draft to appear and then next time we can discuss whether or not it\u0027s something we need to adopt good thanks thank you good I got another comment from javi that\u0027s what happens when I talk too much Mike English says even distribution spans data centers and networks time synchronization is still concerned in heterogeneous environments yeah all "
  },
  {
    "startTime": "00:46:11",
    "text": "right switching gears Igor you\u0027re in the room here we are I\u0027m Eva little bit of Akamai and my goal today is basically to bring some information to the people who were not who are not following a working group every moment and to share some of the observations about what quick will bring to the people who operate media streaming who are responsible for quality of service what kind of changes some challenges that you\u0027ll encounter alright so I I\u0027m probably preaching to the choir when they say that monitoring and troubleshooting network is important for quality of service at least to this group but it\u0027s basically that\u0027s what operators life to do on the network is to be able to see delay and loss and hopefully see it before their customers call them and tell them about the problem that they\u0027re facing when you are looking at TCP streams people have been traditionally linear the sequence numbers and if a symmetric if a path is symmetric X numbers and you could put together estimate of the loss and the estimate of the delay are quick as people are aware it\u0027s a protocol that encrypting his headers it\u0027s doing it for some very good reasons including security user privacy especially in the world of migrating user age users who come from one network to another and for protocol ossification reasons which is basically trying to get the middle boxes to keep the hands away from parts of the protocol that they shouldn\u0027t be looking at and now so one of the goals of quic is to be able to evolve the protocol quickly now one of the alternatives is well if you cannot see anything in quit there is no signal in it let\u0027s just observe similar TCP flows that actually worked pretty well today because a quick on the internet represents single digit of traffic I totally expect that once good working group publishes a spec and there is a number of implementations every single browser has one and every city and will have an implementation there will be a step function in the amount of quick track percentage of traffic on the internet so we\u0027ll need to develop more viable more viable tools and Spencer cut "
  },
  {
    "startTime": "00:49:14",
    "text": "equation Spencer Dawkins two things I wanted to mention thank you for bringing this work here Igor I could be wrong about this but to the best of my knowledge this is the first time a chartered ops working group has talked about this issue and I think it\u0027s been really important speaking is the former area director of the spin bit yeah you love jumping ahead to my love flight but but yeah that\u0027s I completely agree with you yes excellent on this on this slide when you say just observe similar TCP flows is not a good answer how bad is it and you don\u0027t you don\u0027t have to answer but I\u0027m saying I think that\u0027s the engineering question right that\u0027s gonna really matter it\u0027s always is this as this conversation goes so I love it to have actually extra time here and I\u0027m happy to answer this this question so how bad is it I don\u0027t know whether it\u0027s really bad today but the TCP and UDP have very different treat gets very different treatment but by the network it\u0027s actually not uncommon to have a network under a large UDP flood and that typically I would assume because it\u0027s much easier to compromise the user space application and send spoofed UDP traffic and typically it networks what they do is a police rate limit on the UDP and their links so you may immediately get very different performance from UDP application and from a TCP application so absorbing TCP and UDP may actually diverge quite a bit Bernard about Microsoft just wanted to make a comment that I think this may be assuming that quic is used in some of the same manners that TCP is but what I\u0027m saying is is somewhat different streaming architectures being used with quick such as things like scalable video coding and one of the problems that introduced is introduces is that it\u0027s no longer sufficient to just know aggregate loss you really want to know the loss of each of the layers that are being transmitted and that\u0027s something you can really only get from the end systems it\u0027s not really easy to observe that in in transit even if you have even if you were to do everything in the clear you\u0027d have to have the right RTP markings and observing all that is kind of complex so and I would observe that we\u0027ve created a whole bunch of new metrics on the sender and receive side that actually get you much more detailed loss data than you could possibly get through network observation so "
  },
  {
    "startTime": "00:52:15",
    "text": "just something to think about that this stuff isn\u0027t necessarily going to be collected the way it used to be that it may be collected at the a player not even anything at the end points right I mean definitely you cannot get everything you possibly want just by observing transport I mean that\u0027s that\u0027s the same thing in TCP world and UDP world absolutely true all right so just a little bit of about well media streaming right now is by far the biggest quick use case so that\u0027s why it\u0027s but it\u0027s important for this group and the streaming is actually very sensitive to changes in round-trip time and the packet loss now so what is available once quick basically removes all the implicit signals from the transport layer the world there was work on putting back an explicit signal for delay measurement it\u0027s a spin bit it is available it\u0027s in quick version one I mean of course it\u0027s subject to user agents actually supporting turning it on but it is part of the standard and it works in a pretty simple way you just have a server that echoing back those bits as it received from the last packet from the client and clients spinning the bit basically it\u0027s flipping it to another opposite value from the last packet that they received from the server and you could see an unpaired observer can measure round-trip time in each direction so you you can you can be on each either of the directions and you could measure the round-trip time of connections it\u0027s using one of the bits that were previously resolved now it\u0027s dedicated for spin bit in quick short header so it\u0027s basically in the first byte and there is a link to some more about it lost bits so the loss loss trip signal is not going to be in quit version one that mostly due to timing but there are proposals to how to do the loss measurement and there is a quick extension draft or maybe there will be several that are actively in the works and will do very soon one of them from us mechanisms how to add loss reporting and the loss reporting is goal is to be able to first of all observe that there is loss quantify how much and to be able "
  },
  {
    "startTime": "00:55:15",
    "text": "to decide to determine whether its upstream loss or downstream loss which is very important when you try to actually troubleshoot the source of it like with a spin bit it\u0027s good it would be using a reserve of bits that are available include version 1 header and right now there are two drafts available for slightly different but somewhat similar methods of measuring loss they all just drafts on principle of operation so actually since again we have probably a couple minutes I don\u0027t have a slide here you could read the draft or look at the more extensive presentation in Montreal and TSE WG but so one of the operational principles for one of the proposals is essentially you have a square wave so if you have a marking that alternates every n packets and by absorbing that you could observe the upstream loss and then you have a another bit that\u0027s effectively driven by the protocols loss detection machinery if a loss is over packet is detected it increments the counter and if it sends out a packet it marks is counter is nonzero and decrement the counter so basically from that information you can determine and to end lost which is because it\u0027s what our protocol machinery determines now knowing end to end loss and upstream loss you can figure out everything else so that\u0027s basically our proposals mechanisms around others and just before we close it\u0027s kind of a for completeness some people said well one way to determine loss in quit is to decrypt all those headers so here the encryption key is with some trusted middle box and now you don\u0027t have a problem there are lots of problems with that from security prosperity from privacy perspective but it\u0027s actually even from implementation perspective decrypting a whole ton of connections real time is hard key distribution under lossy conditions is hard and problematic and just not a good idea so basically we have to have an explicit signal in the protocol to enable this kind of measurement and so what do we want for the operators for people who actually care about this for people who think it\u0027s an absorbing loss as important to do number one thing is "
  },
  {
    "startTime": "00:58:17",
    "text": "if you have a discussion happening around you just pick up and say it\u0027s important that probably one of the main reasons as Spencer said that we\u0027re still here talking about experiments as opposed to another proposal already in version one is that we haven\u0027t had a huge operator presence in quit working group from the beginning and so we\u0027re here now and of course watch folk with a quick extension draft that\u0027s and then there are a number of links for people who are interested in learning a lot more about everything to talk about thank you thank you questions yeah mr. Dawkins : and I have finally used the spin bit to figure out who was going to speak first so thank you oh I say thank you for bringing this to the to the working groups to the working groups attention I love being able to say that about ops it if I understood your point about about explicit signaling your talk is were you talking about involving the endpoints that in that or did you mean something else yes explicit signaling means that the sender of a packet put some signal that explicitly meant for the pass for a particular specific purpose as opposed to what we have with many other protocols TCP is a great example where there are protocol machine there\u0027s protocol machineries that observers try to lachan and infer information so I\u0027m sure this happened many times before this but the one I\u0027m remembering is the manu ib workshop in 2015 where we were basically saying how will how will network operators understand what\u0027s happening to their networks same problem here just different different operators which is which sounds like a good thing I\u0027ve never understood how we were going to be able to do this without involving the without involving the endpoints and especially the interaction between what\u0027s in the quick invariants so that basically you could have network operations network operators that could consistently look at what they need across versions of quick vs. being able to ever change anything because the point was that the very invariants aren\u0027t don\u0027t vary so I think that I think that like I say I think that you\u0027re definitely headed in "
  },
  {
    "startTime": "01:01:17",
    "text": "the right direction and I only wish we\u0027d been smart enough to do that a couple of years ago because I think you\u0027re definitely headed in the right direction thank you I think so as far as invariants that is a great question quick is a pretty new thing that people have not had much experience with anything like it especially on anything like this scale so the signals we\u0027re talking about are also quite new that many people nobody really nobody has any experience at scale so all of these things are quite experimental and so forth that\u0027s why it\u0027s their involvement one I can totally see if they take off as one of the important means for people to monitor their network then they\u0027re going to be deemed very very useful and nobody will think of removing them from the future were kind of the protocol if there are no problems with them hi Colin Perkins so gory Fairhurst and I have a draft in the transport working group which is talking about the effects of transfer header encryption on future transport Briscoe V pollution now one of the things that talks about is the implicates the implications for network operations and management it\u0027s in last call in the TSV WG it received a a fair amount it received a fair amount of feedback primarily from the security related people it would be useful I think if this community could also have looked at it and see if it makes sense and if people have concerns or comments about what we\u0027re saying draft ia TFTs vwg transport encrypt I believe yes definitely it\u0027s a if the draft that inspired some of the work that we\u0027re doing and this is actually put in bits and the wire that work with more general observation about effect of indifferent James bruising BBC I\u0027m not the most knowledgeable and quick so if I get this wrong somebody stop me so one crazy idea is is instead of having quick doing affirmative a Khan port Rangers switching it so we\u0027re doing negative AK which is what a lot of Arc based protocols use and then use one of those reserve bits for the signaling this would be in the client opts into telling when there\u0027s loss in the network but the details of how much needs to be retransmitted is still inside of the encrypted payload I\u0027m happy to talk to you offline and to get the figure out the details the general just about just "
  },
  {
    "startTime": "01:04:18",
    "text": "the overall quick is that it\u0027s trying to make sure that the only signals we put here are truly explicit signals about this particular purpose so if we try to marry a little bit of protocol machinery to be in the clear it may be different it may become difficult but I\u0027m happy to talk to you hi core sanjay mishra from verizon so i think this is really good and I was at a quick meeting where the discussion did happen on this and I know a lot of operators didn\u0027t necessarily stood up to the mic to to support it but I think it\u0027s implicitly there and you know I think this there\u0027s a lot of exhaustion also on on the operators is part because they\u0027ve stood up significantly and Mark can vouch for that just to get spin bit in the in quick so I think it\u0027s not it\u0027s it\u0027s not lack of interest in this there\u0027s a tremendous interest and certainly I think you the point you made it\u0027s well taken and you know I speak for Verizon but I think other operators are also very equally interested in in having in seeing this work progress so I think we\u0027ll we\u0027ll keep an eye on that and in provide comments and things as needed thank you thanks ahead I\u0027d like to take the comment if it up liver so I have heard like Bernie talking about like a lot of like lost information and other information metrics will be collected in the in points and also here like Spencer saying like yeah how do you do without involving end points right so I so would it be so would it be very unrealistic to think like in the future that we have more a collaborative client and network in place where basically client is helping and the network with providing some information about these kind of metrics they think like well I\u0027m suffering so maybe there is something that I cannot do on the server maybe something I share with the network so network fix itself something like that and on that note there also initiative being here it\u0027s coming initiative and unlike mask where you actually put some explicit proxy for quick traffic and stuff like that where you can have some kind of explicit channel which is basically authenticated and agreed on where you share some information and I think that that is another another thing that we might want to look at or more explicit more like a collaborative way of solving the problem you start up and doing everyone does this best and nothing at it so that\u0027s that would be one way of looking at the problems yes yeah I totally say that people will come up with very "
  },
  {
    "startTime": "01:07:19",
    "text": "interesting ways of solving problems and people are pretty creative around here and if you find clients and that willing to talk to cooperating cooperating middle box and through some control plain communications happening that\u0027s helping performance or anything else it will happen if it\u0027s useful and if clients find it useful hello there I am Emil second for more lunch and so we go we asked this slot because network operation on monitoring is based on the observation of the delay on the packet loss and we have been a safe fighting in the quick working group to get the spin bit so to observe the delay so on so with the spin bit a non pass device can observe there\u0027s a decomposition of the delay let\u0027s say on the right on the left on nowhere is the big bitter and so we are now asking to get the same thing for packet last observation on were close to good something and that is I suppose is going to be useful for media distribution right so I mean I wouldn\u0027t call it fighting but there is definitely working group with a collection of very spirited high spirited an opinion appear native people so it\u0027s fun but good things happen definitely and yes so like I said there is an extension draft that\u0027s going to be prepared we\u0027ll try to get it in time for the quick interim will definitely post it on the list and were more than welcome actually very very welcome a feedback from the community feedback and the technical staff feedback on the privacy implications or whatever people find that they really want to contribute about motorcoach video from Telecom Italia with operators we are very interested in this technology we have the developer of the other alternative methodology but we support this kind of measurement because in our opinion packet loss and delay in a species way is very important to monitor our network is not also important to motor the application by is also important to motile the network in particular to segment the network in the many segments each segment each segment is the domain of each operator so we want to distinguish the measurement to the segment the network to understand where is the problem "
  },
  {
    "startTime": "01:10:20",
    "text": "so this kind list of measurement in a species way is very important in encrypted protocol this is our opinion so we support very much this kind of measurement thank you yeah that\u0027s a common feedback that essentially what operators want is they want to be able to if there is loss for example figure out where it\u0027s coming from and to just know whom to call Spencer Dawkins I I did not get up here to say this but I can definitely can confirm that there was fighting in the quick working group about the spin but but more relevant probably to this discussion is the plus buff that we had in Berlin that was a tussle of amazing proportions to the point where you know I made the decision not to charter a working group there which was intended to provide visibility to on path observers so I mean it\u0027s like you know couldn\u0027t even shorter at the work so that the part where you\u0027re talking about on about involving the end points with explicit signaling and is you know instead of district distributing keys to theoretically on path observers until the first day we do multipath quick then that you know then they won\u0027t be anyway but forget that they say you have no idea how important what you\u0027re talking what you\u0027re proposing yes thank you yeah I mean the the slide about distributing T\u0027s to unpassed observers was mostly like just don\u0027t even think about that kind of thing right with regard to the exposure of the session key I think it\u0027s already already a bad idea especially for the inter-domain troubleshooting nobody in coming too shaky of course I mean this is really but on on a more larger scope I think with a spin bit the quick rocking group it\u0027s a enable enabling a framework where a hand hand to end point are exposing information which is integral which integrity is protected by hand to end on this is something that the TSV working group should six about encryption is not all the part you have integrity on encryption and they should take care of both not only encryption right that\u0027s actually a very valid point that one of the things that you get from using quick headers and quic protocol mechanisms is that you get integrity protection of that of the signal and as far as the endpoints are concerned so by having endpoints engage in this it\u0027s given "
  },
  {
    "startTime": "01:13:23",
    "text": "endpoints a choice to provide the signal we hope that most of them will all of the drafts for spin bit and for the lost bits involve the provisions that mostly they\u0027re about protocol and the protocol ossification but also for privacy that a small fraction of the traffic should not have the signal exposed it\u0027s not gonna compromise because very small problem a portion of the traffic ability to monitor it but it will give additional protection against protocol ossification or some possible privacy implications for a minority of clients who may choose not to expose the signal so I\u0027m conscious that we\u0027re sort of slipping into talking about material and not just raising awareness so call and I\u0027ll give you the last comment on this and then we\u0027ll close this segment and pose the session okay thank you - two quick comments the first is in the conversational media world that\u0027s a fairly extensive endpoint reporting framework in a TCP and clearly that\u0027s not directly relevant for this but there may be some inspiration that you can take from that things have been done in that space the second is that if you\u0027re reporting on endpoint you know if you have endpoint reporting on the behavior of an encrypted protocol there are some interesting questions about the integrity of that from whether the network can trust the reports to be accurate and thinking about whether there are ways of somehow validating that the reports come here from the endpoints match what\u0027s actually going on may be useful the third great solution yep I know that connects working group has done some work in that regard previously I mean there are not enough bits for that but that\u0027s a very valid point and I mean theoretically you could have a TCP malicious TCP sender who is sending TCP packets that look like gratuitous loss so that\u0027s not an entirely new problem for the space but if it\u0027s ECP if a tcp sand tries to manipulate whoever it lost you know the information but wherever it lost packets in terms of the acts it effects TC tcp behavior right if a quick receiver sending reports does the same thing it doesn\u0027t affect the quick perform that\u0027s the difference yeah okay so my takeaway is that our awareness has been raised on this topic and that the work is being done is currently being done in the quick working group so the action item for people here is to keep an eye on the work being done there and contribute voices and opinions as appropriate and "
  },
  {
    "startTime": "01:16:24",
    "text": "then I guess and if if further if furthermore code cohesive input is needed from people interested in video we may add a future in a future time decide to try to articulate video need with regards this yes Ronnie not just to be more precise it\u0027s it\u0027s currently there\u0027s the work now in both in I ppm and quick working group and I assume that today in T is the area they will try to discuss where to do future quick work so for that if people are interesting this world they should voice their opinion where it should be earned Oh mostly because of the overload of all these working groups right um we\u0027re quite sure it shouldn\u0027t be done in this working group so that\u0027s one off the list right okay great thank you very much so now we\u0027ve done one of these mops working group meetings and hopefully people have a better sense of some of the type of things that we can discuss and you can be thinking of things that we should be you know we should be bringing to the table when we next meet so I\u0027ll ask you to do that if you\u0027re not on the mops working group mailing list you can certainly go ahead and join and my last plea before we have a word from the area director is if you haven\u0027t signed the blue sheets please make sure you do before leaving here yeah responsibility for this group I\u0027m quite happy with the first meeting they are at least the timing and the quick topic that that coming between the video cross area so I\u0027m quite appear this good but don\u0027t sleep on this right we need to continue yeah yeah exactly right let\u0027s let\u0027s keep the keep the momentum going okay so blue sheets are up here for anyone who needs them apart from that go warm up thank you very much and see you in Vancouver [Applause] [Music] [Music] "
  }
]