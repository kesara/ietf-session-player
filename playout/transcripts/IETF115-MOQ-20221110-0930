[
  {
    "startTime": "00:00:05",
    "text": "all right foreign thank you guys can you hear us now okay now I'll repeat myself entirely because the poor people uh remote couldn't hear that at all uh welcome to moq now in 3D as a working group for the first time uh this is our first meeting as a working group at a plenary ITF as you know we had an interim but this is our first one uh for those of you who are not used to the ITF by this point in the week people gather a little bit slowly for the first one so we're going to give it a couple of minutes but not too many because we have a pretty packed agenda uh so please find your seats as quickly as you can uh remember uh hopefully by the by this point that you do need to be wearing a mask if you're not actively eating or drinking so if you're drinking go ahead and take your mask off but then put it right back on"
  },
  {
    "startTime": "00:02:33",
    "text": "foreign all right good morning everybody we're gonna get started uh so uh this is the note well it's Thursday I think everyone's read the note well uh if you haven't please do it contains a lot of important information and guidelines about how we do work here at the ietf next one uh again it's Thursday I think most people have figured it out but uh if you're here make sure can I turn it up can I just I can get I'll just get nice and close hello thank you uh all right if you're uh if you're in person use the little mute Echo client to get yourself uh into the queue uh and make sure you're wearing your mask uh unless you're speaking as Ted has said uh and uh remote participants uh make sure uh your AV is off unless you're presenting or talking uh and use headset Etc thanks um if you need any general information about London uh agenda Etc you can find it here our agenda is here I'm Alan this is Ted gory has volunteered to take notes uh Luke is backing him up if anybody else wants to jump in the note stock and uh"
  },
  {
    "startTime": "00:04:01",
    "text": "and help them out I'm sure they appreciate it uh is there somebody who will relay comments from uh zulip thank you it's okay I think the chairs can probably also keep an eye for that video okay so what are we going to talk about uh administrivia and agenda bashing uh we're going to give a report from our interim uh which was just last month I think some people might have missed it uh because it happened so quickly uh so we'll just talk about some of the decisions we made there um then Luke's going to give a presentation on the combined proposal uh from the authors of uh Rush warp and quick R uh and we'll spend some time discussing that Colin is going to talk about uh relays uh and how they work and we'll have a I'm sure healthy discussion about that and then we'll talk about possibly having interim an interim in January before uh Yokohama does anybody want to bash the agenda okay uh you may have seen this note on the list uh zahed has been appointed as a mock technical advisor so if you need technical advice fund sorry uh okay uh things that we talked about at the interim uh we're going to use GitHub to manage our documents uh as all the cool kids do um and we're going to use issues mostly to track and PR's for changes please try to keep discussion off the PRS and uh just into the issues and also on the list of course um some folks were concerned about uh the way retention works for slack so we're in the process of setting up some kind of bridge for slack so that we will have a wage a means to retain the messages but until that time uh please"
  },
  {
    "startTime": "00:06:01",
    "text": "don't uh conduct working group discussions over slack um stay tuned on the list for more updates there um and we talked about what we're going to put on this agenda and uh we are here and you had your chance to bash the agenda so moving on so all right with that uh we'll bring up Luke to uh talk about uh the combined proposal oops oh I need to request it looks good hello media real quick um good morning uh I've got a lot of slides this is going to be quick uh but let's let's start going uh I'm Luke I work with twitch and I've been working on the uh combined rush and warp and quick R draft um just to try and find base Common Ground between them uh so before we begin there's probably a lot of quick enthusiasts in the crowd maybe not so many media enthusiasts so I really wanted to cover some Basics um starting off just medium coding uh it's not it can get complicated but it's a really uh simple level you have iframes that have effectively just static images and then as time goes you can have uh keyframes that will reference one or more frames in the past so this is Delta encoding the idea is"
  },
  {
    "startTime": "00:08:00",
    "text": "it's just two there's too many bits if you try to encode every image separately and every so often you'll see we'll reset the chain and make a new iframe but this is a kind of common encoding actually this is um really simple but it's low latency and you see this a lot with like webrtc and conferencing uh we could have more complicated encoding uh this is one possible example we have B frames they they're annoying um because they can reference future frames as well uh this this has a lot of ramifications on how you write these frames to the wire uh but in this example we have um some B frames that will reference the eyes and the P's but themselves are not referenced um and it you see this more common from like Hardware encoders that have a very fixed structure but the benefit of stuff like B frames and more references in general is that you can enhance the uh the compression ratio you can effectively more references you use the less bits you need to encode typically and then finally you can get some just really gnarly encodings especially if you let a software encoder and give it like a you know a few hours it will produce the most jumbled mess the most complicated possible encoding uh every frame referencing every other frame more you know uh just creating a tangled web of uh dependencies and it's really hard to know how you're supposed to send this over the network um but we try one nice property worth mentioning is that when there's a new iframe uh we can you typically have what are called closed gobs where you do not reference the previous group of pictures so that's a nice property you cannot have a um a dependency go across this uh this line uh so we abuse that and a lot of protocols abuse that so we're making a live media protocol a live media transport and there's a few high-level goals that I've been um really trying to adhere to so number one make sure we respect the encoding over the network so the idea being is we want to send media as it needs to be decoded and as it is encoded we don't want to"
  },
  {
    "startTime": "00:10:01",
    "text": "try to do our own delivery mechanism we want to make sure that uh if this Frame depends on this Frame then over the network this Frame also depends on this Frame um I've been mostly focusing on number two minimizing latency during congestion I've been trying to improve um our our live video system uh to try and reduce latency but the idea being is that when networks suffer you need to have a plan to decide how to keep latency low and number three of course use Quick um it's in the working group name after all but um funnily enough uh we've discovered quick later in the process and it just kind of had all these natural properties we did not set out to use Quick initially um but yeah it's in the charter so some more little history um we have this existing you know this kind of annoying Gap structure with B frames how do you serialize this over TCP um how do you make like rtmp hls Dash how do they work and more or less they take this structure and they put it into one big chain uh the idea being is uh you make sure that everything all dependencies are first in the chain and this is what's called decode order it screws a little bit with timestamps if you see the very bottom because of B frames don't worry too much about that but this is one of the reasons why B frames add latency uh and uh effectively we just have a long continuous uh string of uh frames this is nice and easy this is why like rtmp sees a lot of use still today is because you just have this chain send it over the network it arrives on the other end and you can decode it the problem though is congestion what do we do in this congestion let's say we can hypothetically only send half the frames which is simplifying a little bit because frames have different sizes but let's say we can only send half of them what do we do well the solution really is uh the congestion controller will come in you know Reno cubic bbr whatever and pick a point in the TCP stream it will send"
  },
  {
    "startTime": "00:12:01",
    "text": "everything before this point and everything afterwards it will just queue and as new frames arrive we just append them write them to the TCP socket they get added to the end of the queue and this continues and continues where new frames are put in further deeper and deeper into a cube there's some ways you can get around this to some workarounds um like you can avoid flushing frames to the TCP socket until the last possible minute um and you can do stuff like ABR with hls Dash but more or less you have headline blocking and this is one of the you know it's very similar to http um in terms of uh the analogies here where we're we're introducing a lot of dependencies that shouldn't actually exist um a lot of these these new frames these B's and the p's don't necessarily depend on any of the previous frames that have been written to TCP but we make them dependencies over the network so rule them this is these aren't hard and fast rules but typically a goal one I would say is don't introduce dependencies over the network the idea being is we should keep the media dependencies like this Frame depends on this Frame and at a transport level we should not introduce new dependencies just because it's easier um because otherwise we just add latency during congestion it's an oversimplification but as are these slides um so how does RTP sold this uh the idea being is uh hey let's we can't use TCP let's break frames into individual packets and MTU boundaries so we take our existing frames and we we slice them up um and transmit them ourselves the idea is that we do fragmentation congest control reassembly everything in user space all by ourselves but we get away from TCP by doing that and now with congestion what do we do what do we do when we can only send half the frames well the idea being is because we do Transmissions ourselves we can choose which packets and more importantly which frames the transmit so we selectively"
  },
  {
    "startTime": "00:14:00",
    "text": "decide which packets to transmit and re-transmit such that we try to avoid something like the B frames in this example they're spurious they they're Leaf nodes effectively on our graph um and we try to make sure that there's dependencies so we really prefer we really make sure we send the newest iframe that's the most important thing and we'll spend extra time re-transmitting that or using FEC or whatever to make sure it gets there so in the best case we've got four frames that arrived on the other end unfortunately there's also a worst case where if you don't if you're not intelligent or let's say you don't have congestion control that's rapid and you just like fire hose the network with all your packets and they just drop randomly you get this case where you get hearts of frames arrive or you don't deliver frames based on dependencies like we lost a packet for our iframe and that just causes decode failures going forward so we lost the same number of packets in this example but we only decoded the first frame and everything else was lost so it's very important when you're doing RTP to make sure that you have a very clear understanding of the media encoding the dependencies and how the packets are all related to each other um but rule number two just to simplify is don't drop partial frames technically slices but you need to deliver if if a frame is split over multiple packets they should all be delivered or none of them should be delivered ideally you don't want to waste bandwidth delivering partial partial frames they're not decodable for the most part so how does rush solve this so this is what meta's been using for like four or five years now apparently um is uh um you split the media into Quick streams so every frame is a separate uh quick stream and the idea being here is like quick make sure that all the the frame is fragmented and uh and uh transmitted and uh arrives at the other side in order so frames can arrive out of order into um in relation to each other but within"
  },
  {
    "startTime": "00:16:01",
    "text": "a frame it will arrive reliably um so same thing what do we do here and I should note as well is quick handles just again all the networking stuff you don't have to do a whole lot in user space so you just have to split the media into frames it's a lot simpler um but what do we do what do we do um when uh there's congestion we can only drop half the frames well just like r2p there's a similar best case what we call reset stream we effectively say I want to drop this Frame or we don't even make a stream for it in the first place um uh very very similar to RTP use case where you can pick and choose which uh which frames you should drop although if you don't make the right decisions in this case we dropped two frames only but we've dropped the worst ones or some really bad ones and you cause decode errors again like if you drop this iframe a great example nothing after it in that Gap is decodable without causing artifacts all over the place um so one of the things that Rush does and quick R does is we put the dependencies on The Wire the idea being is that the sender already knows the dependencies but relays also know the dependencies so they can make these intelligent dropping decisions and this is foreshadowing a little bit to Cullen's presentation so I won't cover it too much so rule three don't drop dependencies in graph Theory terms drop the leaf nodes first you don't want to drop the root of the graph because it just causes everything else to be lost as well there is a caveat there a lot of these Leaf nodes aren't that big in terms of file size like B frames really don't contribute much of the the bitrate but they're still the best to drop first so why am I here why am I standing up here because I've been working on something else for like three years now um something called Warp um the idea being here is it's closer to actually TCP where we just have uh things in decode order but we we make a quick stream per Gap so the dashed line"
  },
  {
    "startTime": "00:18:01",
    "text": "is missing for some reason but every iframe effectively we make a brand new quick stream and what we do is uh we mark them with a delivery order a little bit of metadata that says that this stream should be delivered before this stream or this GOP segment should be delivered before this segment so I color coded them here but in this case the green uh on the on the right these are the newest frames and these are delivery order one I want to deliver these first because we're live we care about new frames first older frames are less important it's a heuristic but generally that's what users prefer so what do we do this congestion we can only deliver half these frames again same congestion control like as a lot of the uh the other examples we we learn that there's issues on the network we need to figure out what to drop and the way that work works is we deliver based on this order so we actually kind of reshuffle um the the order we send packets so we send the newest packets first in this case the iframe the packet five is sent first or frame five is sent first and then we'll do the p and then the B so we've delivered the newest frames first and then with the remaining congestion window um we will actually go back and try this in the original iframe uh frame zero but it's a requirement for all the frames that follow it um the idea being is that the congest controller in my case I'm just using bbr just tells you what it can send these are the packets I can send right now and we go in in descending order and and send the streams until eventually we hit the bottleneck which is uh we just can't send anymore and as new frames are generated so this is a new and a PB frame um these are appended to the newest stream which is delivery order one and uh what's really cool is these kind of jump the queue like in a TCP World these are added to the very end and this prioritized world or this delivery order these um some of the"
  },
  {
    "startTime": "00:20:00",
    "text": "frames might get sent like the P frame was is now in the congestion window it gets sent uh whereas the B frame is falling on the other end uh we will starve that and we will starve the older uh frames as well um these frames just sitting in Ram they're these are again these are quick streams so there's two quick streams in this example the yellow quick stream sitting in Ram using a tiny bit of flow control like Max streams but it's not using anything over the network it's not competing uh with other packets and eventually what you can do is you can just reset the stream it's not required um but at some point it's going to be you know after like some number of seconds you're gonna be like hey you're not you're just not going to get sent anytime soon so we should just free up this memory and stop worrying about having this you know you don't want to have an indefinite chain of frames that need to be sent from like minutes and hours ago just sitting in Ram um so this is optional I will mention and I'll explain why because one of the things that warp I really really hope it um I claim it does is it offers variable latency and relay support the idea being that the receiver can choose how much latency they want the latency versus quality trade-off and this works across relays which I think is really cool so just to kind of visualize this it's a hard concept so I've done my best to try and um visualize this but we've got we've got a playback session right now we've got two two frames in the receive buffer two video frames and a little bit of audio um I should note in this example audio is slightly prioritized over the video but that's up to you in our case audio is more important um we had the live playhead that dash line that's what the encoder is spitting out and this is the earliest we could possibly receive a frame over the network you can think of it like min rtt um and then we have these other playheads on the left the idea being is the receiver in outcasts the uh the video player chooses when they want to start playback and they choose the size of the Jitter buffer to get into networking terms"
  },
  {
    "startTime": "00:22:01",
    "text": "um so as we continue we generated a little bit more audio and we delivered some that audio over the network uh the encoder also generated frame three of video but there's a little bit of congestion it hasn't arrived yet um but either way we started with this low low latency playhead for real-time conferencing in this example it's got two frames of latency which is a little absurd but it's just an example uh the idea being is it starts playing out of the Jitter buffer almost immediately so as we continue um we've got some more congestion actually we're not seeing frame four should have been delivered at this point and we we don't even have audio for that equivalent of frame four uh so at this point the low latency playhead is already running into a situation where it doesn't have any video to display and it's just going to show the previous frame um next slide and um we uh we have a new new kid in the block interactive latency interactive versus real time it's a debatable I say interactive latency being just just higher latency it's like twitch style agency I would call it um but there's some debate over with the best name for that latency range is but uh anyway they start watching video they've started and now low latency a real-time conferencing uh latency is just running out of the audio buffer it's gonna start just having silence uh very soon we still have a massive congestion none of the new frames have been generated or delivered sorry they haven't been delivered they have been generated but they're sitting blocked based on congestion control but the important thing is because we prioritize or because we have this delivery order frame five and a little bit of audio that that uh corresponds to it was delivered just in time it's an iframe so we're allowed to deliver it whereas all the previous frames we weren't allowed to skip there were peas um but just in time for our real-time latency user to get a little bit of something uh as a medium latency uh viewer catches up and they're about to hit a gap of"
  },
  {
    "startTime": "00:24:01",
    "text": "missing video um we add one more uh viewer to the mix this is I'm just calling our non-interactive latency this is somebody with maybe a poor Network maybe it's somebody in Brazil on their mobile phone you know like they cannot sustain the latencies that the other users uh uh need or they don't want it maybe they're just on the couch or something like they're just they there's no reason to have low latency they're just trying to watch football um so they they joined the picture too and we're recovering a bit from congestion on the right side you see that we've delivered frame six we've delivered some audio we're catching up there's still a big gaping Gap in the middle which is where the medium latency is um so that user right now is they're listening to audio you see the audio is continuous they hear it but their video is um is Frozen or stuttering much like a conference call if you've ever used everybody's use a conference call at this point where um that's what the user experience uh is like and then finally we keep going forward we get some more frames popping into the picture we've now caught up to the live playhead everything's being delivered um and we can go back and we can start back filling the gaps in the buffer so we've delivered frame three and um just in time for the high latency user to get it even though the medium and low latency viewer did not get it and then we deliver frame four uh so we uh we finally managed to save the day we patched all the holes in the Gap um just in time for that viewer didn't see any issues at all and then there's a there's a fourth one added this is a archive worker it could be like 30 seconds delayed the idea is that it's sitting there it will just scoop up all the frames eventually and then upload them to storage for a Flawless of odd so the same stream you could have somebody with real-time latency that gets a lot of lost frames and you have the other Spectrum no loss at all but just High latency so just to recap this is the exact same delivery exact same network conditions"
  },
  {
    "startTime": "00:26:01",
    "text": "exact same packets being sent over the network but what the site of the user experience is how far how big the Jitter buffer was more or less um but because we we had this delivery order we can have these different user experiences so the real-time latency user had the lowest latency but they dropped video and a little bit of audio which is is usually required if you're doing real-time conferencing but it's not ideal I would say in higher latency situations like a medium latency they just dropped a little bit of video but audio was Flawless that's what we're looking for from twitch for example like users don't need real time they prefer to hear the broadcaster the streamer even though they might miss a little bit of video and then a high latency where they just didn't see any issues at all they didn't even know there was congestion um maybe ABR kicked in at some point and they would notice that but in terms of frame losses and frame drops they didn't notice anything and then finally the archive worker it's just whatever latency you want you could wait wait as long as you want uh well Ram persists pretty much um until the streams are eventually reset uh either side could reset like the sender could send a reset stream and the receiver could send to stop sending to say like please just don't send me anymore I've already skipped over this content that's mostly just to say bandwidth though so anyway with all that in play uh we've got um this combined draft I've been working on uh we're trying to combine warp rush and a little bit of quick R the the main concept is any number of frames per stream is a segment uh there's a lot of bike shedding on this name uh so don't don't get too attached to it um and the delivery order and dependencies are written on the wire and this is to support relays the idea is that these dropping decisions can be the same no matter where the congestion occurs uh if the congestion occurs on the first top versus the last hop should be the same experience um so here's our example encoding again um there's many ways you could break"
  },
  {
    "startTime": "00:28:02",
    "text": "warp which is effectively you just break into gaps very easy uh this is backwards compatible with hls and dash more or less you just take the same segments and send them over quick uh you can fragment a little bit more you can say what if we send reference frames and non-reference frames as separate segments separate quick streams we have a little marker at the front that says here's the here's the delivery order and maybe in a little market that says here the dependencies between streams um uh but yeah we could break it further because in this case we have these B frames at the top depend on each other even though ondexual encoding they don't you could break them into individual segments too you can break it down as far as you want in this case we've got a segment for an individual frame so it's not fate sharing with the other frames when it doesn't need to be and then finally you could just break it every frame into its own segment this is effectively Rush um the numbers here like the delivery order uh one two three four five all the way up those are arbitrary it's based on your application uh in the use case in in this case we want to deliver the newer frames first but there's certainly use cases where you want to deliver the older frames first in our case like for example you're serving an ad you don't want to skip any of the ad video so you you kind of flip it around you turn on the head of line blocking and say that actually the first iframe which is delivery order for make a delivery order one make that the first one that we deliver so there's no skipping allowed um so there's a lot of business logic we'll kind of talk about this at some point in the future about delivery order um there's no right answer effectively to how you prioritize frames somebody has to decide which frames are more important somebody has to decide it's audio more important somebody has to decide even like which Renditions and whatnot are important um so what's next uh I I forgot to draw a bike but this is a shed uh"
  },
  {
    "startTime": "00:30:01",
    "text": "the the names are really hard so we're not sure if we should call this warp or rush or meaver quick or some made up name um and especially the name of segments like all these segments are they layers are they fragments chunks media quick prefix things fubars like every word every name we've looked at has like existing connotations that it clashes with um and then uh I've been working on this draft mostly um there's a GitHub repo kicksolated warp draft I found a lot of issues but there's a lot of core things missing like a wire format I've defined how media should be sent but we don't even have a way of how the bytes should be written to streams uh yet uh we're also trying to figure out how CDN support works um right now the protocol is push-based it's very simple web transport you push but we want to figure out what's the middle ground we can find to make sure that existing cdns that are all HP based and pull can still work with this protocol um so it doesn't really matter if it's push versus pull so long as you send media the same way over quick streams but we want to ideally have one protocol for both contribution and distribution um and we need to figure out where the middle ground is there and just all the media stuff like how do you send tracks how do you choose tracks how do you do you know which codecs which container even uh the current draft has like uh cmap but we still haven't even decided um so everything that's actually required to make sure media playback works is currently missing just just the networking stuff is what I've been focusing on um I've also been working on a demo uh quick.video demo I will say it's very crude it's just hosted on a single host uh tiny instance in US West 2. um I'm working on it too but there's uh this code also on GitHub to show how this works and some examples and then"
  },
  {
    "startTime": "00:32:00",
    "text": "finally with adoption I don't know the process I don't think this is ready for adoption yet but I really want to get the idea from the room is this the right direction how do we feel um what do we think is missing what do we think should be laid out first um a lot of contributors so I'm Luke again representing the twitch folks um most of them are online and then uh from Cisco we've got Suez Cullen and meta Cairo Allen and Jordy have been uh helping a lot with this draft among others I've seen uh there's a few folks have uh left a lot of issues uh lately too and that's all I got speaking of contributors uh the queue is quite full uh at this point Sam you're up first between me good morning uh samhurst BBC r d uh thanks very much for the presentation that was really cool uh and Kudos on the handwritten slides I love that um so I have a question about the delivery order number that you were showing up there is that just a constantly incrementing number as it goes through or is that a uh more sort of dynamic value that changes depending on one particular instance in time yeah so um we still need to work out as this is one continue right now it's one continuous number space it's just a un 64. um how warp Works in production for right how I've coded it is it's effectively just the presentation timestamp plus three seconds if it's audio just to give audio a little bit of a head start but effectively it's an arbitrary number so long as the lower number is um delivered first or in my case I'm doing the presentation timestamp but anyway um it's arbitrary okay okay oh um great overview um Grosso for simplification like you said but I think it's the right oversimplification for uh transport"
  },
  {
    "startTime": "00:34:01",
    "text": "folks one important aspect that I I'm not sure was um was convey which I think it matters a lot to the the for the transport folks to understand is that there's differences between congestion control and rate control and um and even within congestion control there's also you know when you talk about prioritization there's different concepts of the partition about whether you're talking about your local cues or whether you're talking about marking things properly so that the network cues um handle your congestion the way that you intended so first I think we want to make sure that people understand that over long time Windows we're not using congestion control to to instantaneously decide which packets are said that the the congestion controllers overall average um bandwidth estimate gets fed up slowly to the app and that changes the media encoding rate so you're not just going to drop individual parts of your of your stream you're going to select a totally different stream encoding in order to meet the budget that your condition control is telling you over a long time that's not useful for an instantaneous decision that the congestion control says you know what I I know I'm not going to send this packet because I don't have a window open this in this packet so that's where an immediate priority can help on on a marking from the app layer to the to the quick layer it can drop something immediately or hold something back immediately but the longer time windows those are going to be controlled by rate controller feedback to change the media encoding itself so I think it's important to keep keep those things in mind and also the difference between congestion control decisions to do something for internal cues versus how they get marked for Network cues because you know Wireless you know wmm and and Mobile Wireless qcis and and we know wired dscps and things like that there should be different ways to map the application Level priorities to those Network priorities and I think it's important to get that right in in"
  },
  {
    "startTime": "00:36:01",
    "text": "these drafts as well yeah one thing I didn't mention and you rightly pointed out is that these don't have to be the same bit rate like these segments the encoder as it sees congestion can start decreasing the bit rate in response this is meant to be a response when you can't change the encoder in time especially for contribution like you have to switch it switch Renditions and iframe boundaries um but yeah and and prioritizations only as effective as how quickly you can detect congestion if you have something like buffer bloat everything's over the network unprioritized so giving more information to the network layer helps as well and I think this this order that you have um I think it came across in the presentation as this is something baked into the protocol but that's an application Level thing that it can just decide whatever um you know delivery Priority it wants uh for things because for for some people the user experience may be terrible if it's stuttering that that order that you showed before would be under congestion you your video is constantly stuttering right you you cancel something that's being played right now and you try to play the Live Edge and so you get someone you know always for half a second of the Live Edge instead of a full second of continuous video so I think that user experience is up to the app to figure out what the right you know order and priorities should be not something back into the into the protocol uh thanks for that I will notice that some of the discussion that's currently happening uh on the uh chat uh has uh has touched on ABR and other things like that so if you're watching this on YouTube please do go seek out uh that chat stream as well uh which should be linked from uh the iitf site uh because there's uh certainly some working group discussion happening in that I'm sure I'm from Huawei and I see is a combined protocol from three dropped but there is a lot of things in quick cards missing here I think the pop sub relationship and they use Quick datagram and for the pub sub I think maybe it can bring a lot"
  },
  {
    "startTime": "00:38:01",
    "text": "of State in the Renee I think that's why you drop it yeah this is meant to be a base draft as when we take everything combined and the idea is something like quick R could either be added on top of this draft or just a separate draft entirely we're not sure yet this is meant to be the common ground yeah I can see the pops up being implemented over over these drugs and maybe you're not in there but the quick datagram scene to do something new to in the bass drops so uh I think in um quick streams are more than capable of doing even real-time latency so as long as again you have some way of deciding which quick streams to send first and which fragments to send first so I don't think datagrams are necessary um which is a controversial statement but um uh yeah by having more tight control over what the quick library sends end of the day it's the same media getting fragmented the difference is if you do it in the application layer or if quick does it as long as you can tell the quick library to fragment in such a way it's the same thing yeah I think RTP is a kind of like the quick datagram instead of the stream right so the benefits by using the datagram is that you can pre-plan some metadata to each datagram so you can drop it in more more fine-grained way okay yeah possibly fierce I guess the the Assumption I appears a handle on the BBC r d is um the Assumption here was was is sort of traditional kind of representation kind of breakdown of the video as opposed to like SVC type uh kind of so then if you think if we're thinking that then it's then are you gonna is there an ABR stage or is this really the ABR"
  },
  {
    "startTime": "00:40:00",
    "text": "um because I guess that's happening in webrtc already um with the SFU approach where you just pull things out I sort of wondered you thought about that because it's just sort of seems to be quite sort of low level ipb based but so first off we're I'm doing ABR for we're using this approach uh the idea being is that the sender chooses which of these segments to send So based on the congestion window it can switch down a rendition uh you did mention SVC now very quickly switch to just in case somebody asks um this is an example don't overthink this but the idea is you could have a base layer like 360p and then you can have segments that build on top of um so SPC people don't know you have a basin layer 360p in this case and you could have enhancement layers on top so we're pretty much just sending SBC layers as quick streams is more or less this proposal but I don't know enough about SVC this is just like an idea right okay yeah I thought it should it seemed like it would naturally extend to it in a way but then in another sense it would then maybe uh you wouldn't necessarily need this would become the kind of ABR um aspect although I don't think we want to require SVC right and and the prioritization thing so that would be happening kind of in in a potentially some kind of switch relay entity or some kind of a quick relay kind of entity where so so then I guess it was a previous one about how that Maps down to the actual Network priorities we're going to cover that a good bit in the next okay so if you're going to hold that for the next fine fine okay thanks um and just uh the treasure about to lock the queue so if you feel like you want to be uh following up on this please get in the queue uh now Christian uh nice presentation Luke I mean I I"
  },
  {
    "startTime": "00:42:01",
    "text": "like the simplification it makes the thing easy to understand uh one thing that is missed in the simplification is the effect of these segmentation on relays because if you have a set of streams as you describe then you are going to get head offline blocking on these streams and if you have a series of relays then you are going to get head of line of blocking at each of the relay and the delays induced by head of line blocking will combine and that's the reason why I had been exploring how to do the same kind of architecture with datagrams so that effectively when you have three layers you get fraction of the streams in the datagrams and you can decide to for them immediately thus avoiding the end of line blocking yeah I'll let Cullen cover that more but just really simple response that assumes that you're reading streams in order and then writing them in order but you can get around that if you can read and write out of order to Quick streams uh beholden to flow control uh so that's that's that's that's yeah I I understand what you said there but that's extremely hard to do because the reading in I mean basically if you're doing datagrams your application being in control can do whatever if if you are doing streams the application is not in control and this business of reading steam out of order is really really hard I would say this is a general problem for quick relays like even HTTP suffers from this where a gap for packet loss will cause um on blocking but yeah for sure Victor"
  },
  {
    "startTime": "00:44:05",
    "text": "uh Victor vasilia for Google uh I just wanted quickly to say uh that uh supports this proposal I'm not sure if in a stage for adoption but whatever there is that's not how to do looks very good uh and thank you very much for working this foreign okay is it better now cool so thanks Luke for the presentation on trying to kind of combine things good things from all the protocols and I agree this this is a good start and we have quite a bit of things to cover to make it more usable uh especially uh thinking uh end-to-end protocol not just the transport but also the relays and consumption side how do you ask for things they all depend on how do you break the things to start with so that we can Define the internal protocol I think we need to cut some of those pieces in and I want to kind of talk on a couple of things that was raised uh some more on someone else uh on on the delivery on the priority I think there are two different concepts uh like priority is more like saying what how is one thing important in relative to other and delivery order is more like how my decode decoder wants to get it in so that it can do some good work and and probably we need to kind of spend some more time on thinking how uh relays would understand the data but not making too much in protocol but leaving it to the application space to Define how it is done that would be helpful and another point on the datagrams I agree uh at some point as a group we have to kind of make a decision on um how do we meet the use cases that we have mock chartered for HTTP does not have to do real time so hence it's okay but mock is not"
  },
  {
    "startTime": "00:46:00",
    "text": "constrained by that we have more use cases and we might not do datagram it's a a working group decision at some point but we need to say Can what are protocol be develop in water transport we do works for the use cases thanks Peter thatter my audio working yes okay uh first I'd like to say I I think it's good that you're building on web transport uh that solves a lot of like multiplexing issues um and it allows for I think you should allow for either streams or datagrams I think streams are fine to use but I think it makes sense to allow the client to do either and that's I believe the direction RTP over quick is doing um during your presentation you mentioned that you haven't defined it all the media protocol are you looking for contributions on that front I have ideas that I'd be happy to contribute yeah of course um if any if anybody wants to help either on the protocol or on the demo or just just want to talk to me go for it looking for contributions okay great thanks that's it Randall uh Randall we can't hear you yet that I think will do it there we go can you hear me now yes okay um uh I notice you have the SVC example here um and um for the case where you're not using SEC but you're just feeding multiple different um resolutions or bit rates up I'd be interested to see how that Maps here but"
  },
  {
    "startTime": "00:48:00",
    "text": "also I notice also that all the playheads you've defined in your other diagram appear to be at the same location I you know as in there all these things are arriving at that location and you have different playheads which appear to be running off the same buffer stream whatever okay is that actually what's what's anticipated here which would seem odd except for maybe like the recording stream uh recording head uh uh or is there something more complex going on here I uh I think like it is actually getting fed out to the different two different locations which I imagine it is which may be consuming different portions of a multi-layered multi-bitrate stream okay sure so first off for simulcast which is sending multiple Renditions that don't depend on each other uh it's effectively this SVC example you just remove those arrows between layers so in this case you'd send the 360p rendition I got the highest priority will be delivered first and then any extra bandwidth will be sent on higher quality ones so sending multiple Renditions works very similar to SVC there's just no dependencies between segments and layers and then finally for the playhead uh you would not be receiving those all that's just an um just a visualization the idea is that different viewers will have different network conditions so they all have different starvation congestion I just wanted to explain that even with the same network situation you could have different experiences based on the size of that Jitter buffer and the size of that Jitter buffer depends on the use case the desired user experience but everybody would get a different experience yeah I think it would be more illustrative and more better for discussion if if we use a slightly more real world case for that because that"
  },
  {
    "startTime": "00:50:00",
    "text": "affects some of these these questions and answers about how to do this thank you thank you uh I think that brings us to the kind of next steps question here the discussion has been great today uh we definitely have kicked off a lot of different issues both at the mic line and again I'll I'll point out during the chat uh almost a distinct set of issues we've gone through and Luke I entered you to go through the chat and anybody who's watching us uh post facto to do the same uh we're definitely not going to do a call for adoption today it's pretty clear there's a lot of stuff that people need to cash out first um but please do keep the discussion going on the list even though we will be talking about an interim uh later on in this meeting we really want to make sure that we don't have to have the pace of discussion only at these meetings there's definitely a lot of this that you can do in either uh mailing list discussion or on the issues for uh the GitHub repo uh so thanks very much uh our next thing on the agenda is a different presentation Colin you want to come up and by the way if you have not actually signed in to the uh the tool because you weren't thinking you were going to get up or something like that we'd really appreciate you doing so because this is a pretty tight fit for a room and we'd love to have a a little bit um uh more spacious space uh when we get to Yokohama so whoa I forgot I had to stand in the pink Square here okay awesome um thank you dust that working okay when I step away from the mic abuse me um so I'm going to talk a little bit about uh the relay stuff here and whoa huge lag"
  },
  {
    "startTime": "00:52:01",
    "text": "um the goals of this discussion is uh is really around um understanding some of the commonality of some of the questions that come up in every one of the relay drafts that we've had a lot of these drafts had relays or things are a lot like them in some ways so I want to try and hit uh some of the similarities differences there's lots of more nuanced things that even some of them came up in the previous conversation which which unfortunately I don't really talk about at all in here but I I think we're just you know it's a first working group meeting we're trying to figure out what are the the big things and and get them moving along and get us all sort of talking on the same page and having a good discussion about things that need to be I was really um thankful to like I like I love the last discussion lots of good ideas came up and it wasn't about like oh a versus B it was like ah this is a property that we need in the end system we're going to do and that's that's a great way to think about stuff at this stage of a working group so uh where do relays fit in here and you know in every one of these systems that we're talking about on the ingest side our producers are creating some content they're pushing it over some protocol up to something you can call it a server you can call it a relay you can call it a CDN but it's going up to something we're going to talk a little bit more of that second on the other side something up probably in the cloud and again it may be a CDN it may be a relay it may be however you think about it an SFU these types of things are bringing the data down to the consumers and the key Point here is we're not trying to Define and figure out exactly how that CDN relay internals Works what we're trying to make sure is that the protocol we're defining here which is going from the a to the X's or from the Y's down to the C's has enough information in it that the X and the Y's devices whatever they are have the information that they need to do they need to achieve the end end-to-end user experience that we want to achieve here okay so there's you know some things in there that you end up having to know you know if consumer C has twice the bandwidth as consumer D and producer a is making things in different resolutions or"
  },
  {
    "startTime": "00:54:01",
    "text": "whatever you know there's some information you need to know in here that helps works there's not much information we're going to get to that later now the other things is um we talk about the security on this stuff so I wanted to just sort of hit a little bit of the terminology in here on this slide to just there's not um I'm defining just by definition that the ends are the producers and the consumers things that have the ability to uh encode or decode the video decrypt it can see the media those types of things the relays in the middle here are not ends they're middles so they cannot do things like that um and when we start talking about the ability in our Charter we have a strong requirement for end-to-end security we're talking about the producers and the consumers being able to do it if there was a transcoder in the middle of this network it has to access the media obviously and which means it's an end a transcoder is probably both a producer and a consumer of different streams and is is in the end um you know what I think my audio is really messed up I'm gonna take a mask off here so so uh that that's what I mean by ends and and such next slide Maybe back a slide maybe okay so we do get the question of like well do we have these who's going to provide these relays and was one of the questions that was raised a lot in the beginning it will anybody build this type of stuff and so uh there will probably be a range of different people to do it there's some companies that have a CDN or something like it today twitch is an example of this right they have this they operate it themselves they can deal with this stuff they want to probably use some of the existing thing to have and don't disrupt about them uh there's companies like that you know WebEx part of Cisco that uh today does its own media nodes near the edge and pushing them out to all of these places but uh would would rather run that on somebody else's infrastructure and buy and there's there's companies like Akamai cloudflareoplasty of all uh talk to me about mock in various forms and those types of things this company likes interdigital that has drafts here"
  },
  {
    "startTime": "00:56:00",
    "text": "that are looking at relays that are tightly tied to 5G Network optimizations so they can provide a better experience uh for the media than you could if if you weren't so that there's there's a range of those types of issues so do I have a cube built up here warn me if I do because I won't I will stop if we need questions so what what really drives the need for these relays why do all these existing networks happen well there's a bunch of different reasons uh some of the sort of high so the two biggest ones for me are one is on the distribution side you you need fan out you need some device that can you know take your your content instead of just going to one person spread it out to hundreds of people or millions of people and scale that up and these are a convenient way to do that type of thing and the closer you move that you can uh have some bandwidth savings and some of those types of other issues as well then there's another point of you're trying to get these relays closer to the consumers or the producers so let's talk about in the consumer case for a second the closer it is uh the faster the less round trip there is between the device consuming it and the and where the device that has a valid copy of the media you're trying to receive and the more you can squeeze the the time like that that it takes to get a message back and forth between those the more you can use Quick streams and things like that to re-transmit data you lost and still not have it slide out and still not have it extend the latency of your overall delivery of the media from a glass to glass point of view so this is why you see so many CVS like any of the major media networks today just even doing Dash Ratio or something like that I mean you don't try and uh deliver video to users in Australia by using a CDN server um in London it's it that's just not a very effective way to do it you try and get it close to there so that when there's loss on a local land link it's a slow it's a much faster place that you can say I lost this packet please return it and get it back to me so bringing those things up now"
  },
  {
    "startTime": "00:58:00",
    "text": "some people like the 5G people want to extend that much farther and closer out to the edge than we've traditionally seen in today's networks and that's one of the things that's changing that allows more bandwidths so that's some of the things we're trying to hit here and why we have these relays at all so these operational requirements a few of these comes out um you know you want to minimize load and make them easy to scale you know normal sort of things and trying to develop develop any Cloud Server um you know you need to make them configurable but fundamentally they're making decisions about what to forward what to drop okay that's really what they do they take some packets in they send some packets out and you decide which one of those go in and out um a lot of the requirement is drafts we saw some sort of requirement to clearly hand off a client to a new relay to reschedule relays or reload balance take something out of service and this is the sort of go away messages you see in several of the protocols so this is a relay redirect type idea um low latencies obviously the whole point of this sort of thing uh and rapid recovery from buildings these are all requirements that we're going to have to figure out how our protocol will help what it what our protocol needs to be able to support those operational requirements the um again I'm just defining this as terminology by by definition not some logical sort of thing I'm splitting the stuff that relays see or this goes across the protocol into an envelope and a payload okay so the payload is the video content the audio content whatever the media is from it may be encrypted it may be DRM protected it may be anything uh or it may be totally unencrypted as well but the key point is the relay can't depend on being able to look into the payload to see it the envelope data on the other hand is really what this presentation is going to mostly be about and this is the stuff what is it that the relay needs to see you know in an IP sense like you need to see your source and destination IP addresses or a router can't route it it's like that type of stuff obviously our goal here will be put the minimal stuff we can in the"
  },
  {
    "startTime": "01:00:00",
    "text": "envelope for for a lot of reasons um and and do it so that that's just the terminology I'm using to define those two different chunks of sort of data that we need to deal with so I don't want to delve deeply into what all the drafts do on this but you know warpad you know an ID for a stream and some delivery Priority stuff perhaps dependency list uh the deploy draft had you know similar set of stuff with a bunch more data that might help it work in a 5G context uh you know quicker had some sort of different things with you know time to live or time stamps and discardable but the exact details of all those drafts don't really matter what matters as we start figuring out what our use cases are that drive the need for chunks of data that go into this envelope and and what they are and I'm going to talk about a couple of those in the slides coming up I can't see if anyone's on the Queue so we will see it up in the right hand corner oh okay cool I will see it then so probably the biggest one and we are talking about that earlier as this priority and delivery order type stuff and all of the proposals have some form of idea that there's there's sort of a base priority related draft and I'm struggling because different terms we you know Luke says it's terminology is hard here different drafts I've done this a little bit different ways but there is definitely a concept in all the drafts that if your audio if your application business logic feels that audio is more important than a video there's a way of indicating in there that relays should forward audio before they should bother forwarding video for the same thing and similarly different types of video might matter differently um I was thinking about you know your 360p videos more important than a layer than an adaptation layer that it goes 720p on top of it but you know as Luke pointed out too uh if you have advertising video and non-advertising video your business application may say business logic may be that the the ads are more important right so there's some"
  },
  {
    "startTime": "01:02:01",
    "text": "application layer of things at that type of um idea and then there's this stuff that has to do with sort of delivery order um some sort of numeric counter of these things and that was a lot of what Luke was talking about and helping to helping try and motivate and explain his draft of look if if if you have some data that depends if you have a p frame that depends on an iframe but you can't deliver the iframe there's really no point in delivering the P frame it's that data is useless there's nothing you can do with it so there's some sort of you know sense of of a natural ordering that way there's also data that is so old it is useless that's really what we mean by real time when we talk about real-time video right is is there's stuff that's too old to be of any use um and so the warp drafts you know took the approach of uh the the newest thing was always more most important in this sort of context that might be uh that might be completely right or mostly right with some fine print and hand waving and I'm sure the working group will will dig into that deeper as as we hit that but we we need some way of sort of understanding uh the the decoder order of these types of things uh of what's needed at the other end and what makes the sense to be delivered in that so this is similar to you know an RTP you had sequence numbers [Music] um we have different stream or segment IDs whatever we call stream segments layers fragments um we have you know that type of way we need some way of ordering that and we need to decide how that that ordering number relates to the priority of it being sent within the broader priority of uh the audio is more important than video so I see these as two different things um we can I mean both conceptually we clearly everybody agrees there's two different things and we need both of them here how they get mapped to bits may be one thing two things we can sort of debate that later but I think that the key thing for us is to understand what it is we want the the protocol to deliver and that we want relays to be able to look at this type"
  },
  {
    "startTime": "01:04:00",
    "text": "of information and start to to deal with it as they forwarded something on and remember if if if a video stream was sent up um you know in multiple Renditions one at one megabit and one at two megabits and there's a relay that has access to both of those there's we will be doing the normal thing of clients you know connecting and asking for the one that has the right bandwidth for what they need none of that's taking away from that but you still need to understand what's priority to deliver inside of that uh no questions okay so this is actually what I think is the most thing to land in the envelope paid actually uh dependency indication so this gets to how explicit we want to do get about this and I think this would be a great thing to get some feedback from the working group on today as mine yes so one way that we could do and one of one of the warp uh drafts had a proposal for this is I mean we could have a dependency list where we could say these uh chunks of data segments of data depend on these other um chunks like by named ID and that relays and things could look at that figure out what to do with them um some of the other drafts quicker being one of them use the idea that there'd just be like a priority like you know audios Priority One uh you know 360ps priority two 720p is priority three and that the application would pick those priorities but with a small number of set of numeric priorities they would be able to deliver you know get the the end effect they wanted on the relays so one of the questions that we'll have to figure out uh from the working group and is should we have a dependency an explicit dependency list or can we get away with just having this sort of you know General priority idea to be able to do this in the relays um and what you know what do we need to do there that's sort of one of the areas caching is a is another uh area where uh the quicker drafts did assume sort of this but we could or could not have this"
  },
  {
    "startTime": "01:06:00",
    "text": "so this is not very important on the Ingress side this is all about the distribution side where it's probably more important and on that side you come to the idea of is it useful for uh one of the relays to have a short-term amount of media data that's there available to it and this can help deal with tasks like when a client first joins there's already the previous iframe is there in cash and they can get the previous iframe instead of waiting for the next iframe to be transmitted and sent into the system it could be pushed a little bit farther you know some interactive applications Skype long ago even sort of did it but you know various applications playing with the idea of well if you missed some packets you could you could pause the stream temporarily roll back in time and then play quickly forward you know those types of ideas where you can catch up um we could or could not support those this could be something that the protocol if we did this at all I'm sure it's something that would be optional for the relays to ever cash more than zero bytes of data um but you know caching and whether we want to deal with that is one of the issues that we'll have to touch on on that side it can be done as completely extension draft right it's those types of things let's see uh security and privacy so as I said at the beginning um the relays can only uh read the envelope data uh the and other network elements can't uh you know routers along the way this is all inside a normal quick TLS connection between the consumers or producers and the relays okay so this doesn't change quick TLS in any way like everything's encrypted over the network with quick TLS exactly as you would expect and exactly like today's cdns as well um on on a classic CDN today yeah every connection might be encrypted with https and uh the media that's being moved around inside of that connection uh you know maybe DRM encrypted right so I think that we're trying to very much"
  },
  {
    "startTime": "01:08:00",
    "text": "follow that same model that's worked in that space uh and allow for this now one of the things about this that we do need to sort of you know hassle is like do we need techniques for dealing with Integrity of what happens of the envelope data my view is that there's no let me go to the next slide on this one oh sorry I took the slide out I'll go back one okay so uh my view is that the relays shouldn't ever change envelope information that they should they can read it but they shouldn't change it and if you were on a system that chose to use MLS group keying then you could get away uh with uh with actually providing real a a Integrity protection of the envelope data at the ends it's not like the relays in the middle would do the computational work to check it probably but they they could so Spencer you had had some questions on the list and I see you on the Queue now does this hit Does this answer my view anyway on the where we should go on these key items okay um Spencer Dawkins uh too long don't read or don't listen is yes uh but uh the thing to me that seemed helpful for me to be asking uh about this is um what a what a useful name for what are called relays now would be that would be clearer uh and the probably just the functionality kind of thing uh we talked uh yesterday recently about uh me asking so or you know is"
  },
  {
    "startTime": "01:10:00",
    "text": "this a general purpose relay I think not or if it's not what kind of a relay is it and uh I think after seeing that part of the discussion in your slides which which are incredibly helpful to me thank you but uh I think that uh what I what I think you were saying was that this would be a relay that relayed whatever uh Rush warp is it ends up being called is it would you agree with that it it it so I view these relays as a hundred percent tied to the the protocol that's coming out of this working group to relay the data you know that with the media that this working group's done that's what side now that makes it sound like a specialized relay it's only a relay for the mock protocol um however given the mock protocol is meant for moving any media and nearly anything you ever want to move can be considered media in some form or another it is an incredibly General thing so what this working group is really talking about building from from day one whether you think about it or not as soon as you talk about moving media over quick it or media over anything uh in some real-time way we are talking about building a a better set of ways to move real-time data around the network and it will turn out to be incredibly generalized if we get this right right okay right I like to say I think my biggest thing on on being able to explain to people what the scope is is you know number one figure out what the name of Russia is so that you can say we relate among whatever else we do we we relay that so um from from a church perspective uh thank you very much for the intervention um but I'm going to ask not you actually"
  },
  {
    "startTime": "01:12:02",
    "text": "spend a lot of time talking about names because right uh what what I think you were asking for is a description of relay functionality that's absolutely cool um but uh we don't want to spend a lot of time on on names because it's an enormous bike shed and I will note that the cube ballooned the minute you said names seven additional people who no doubt have great ideas of uh of their favorite name if you're in the queue to talk about the name please get out of the queue um if you're in the in in it to talk about relay functionality welcome we look forward to your insights but we can keep talking about this as the mock protocol good name coming future and the mock relay yeah functionality to be discussed no name discussion please right thank you just in terms of processing the queue I think Colin you only have one more slide after this is that right sure um or is it you want to try to finish it up and then take all the questions please do that okay to get back to where I can actually do that okay one more slide here uh so the this this slide under cells the connect the the problem here but uh when the mock client uh connects over the mock protocol to the mock relay um we need some idea of what it is that is used to authenticate and authorize and allow the relay to decide that is going to be willing to forward this traffic in a meaningful way okay and basically uh initially what I had written on this slide when I first started was um that you know there'll be an authorization token and some form and really I I think that we come around to that uh that the you know there's an authorization token that the application got that the consumer or producer got in some out of bound way"
  },
  {
    "startTime": "01:14:01",
    "text": "that it allowed it to pass it to uh the relays it probably also found in the same out of bound way what relays it might want to use um and that the relay say that now that is such a fluffy hand wave to the whole problem it's completely useless okay so I recognize that uh but I think we're going to end up something along those lines now option two which was I forget who proposed that to the list right now I'm just blanking I apologize to you over but but you know what oh it was will so you know will said hey look actually we need uh we've had this problem Lots we've thought about it lots in existing cdns and there's some really good approaches to steal from here that have a bunch of advantages over what was being discussed and so I I'm as an individual they're very much in favor of going down this sort of you know number two type option where we take the same type of ideas of what has worked to be very scalable uh in existing today HTTP cdns and apply it to these mock cdns so that was really my my last slide and so we can talk a little bit more about that Wednesday so let's go back to the question queue okay before we do I just want to do a time check for people it is 10 45 we closed at 11 30 and we have a pretty hard stop because they need to set the room for the next uh host talk uh so please make your interventions pithy um and uh be ready to uh have quick responses the first is Emma nice presentation so quick question running the last slide so I love you said minimize the information the envelope should have for the Relay but should not keep it as is because several developers love to have everything if we keep it as it would be vague so I think we should include what information should be available if it should not be there plus one okay yeah it's implicit in what you were saying in the intro but I don't think you we need to be able to have a relay that is doing a meet Echo function"
  },
  {
    "startTime": "01:16:00",
    "text": "this is completely unaware of any of the content that is being gated so you know I want to do me TECO but without the relay being an end point so I think if we could add that to the use cases it would be useful in making sure that we keep enough information to make that possible oh okay let me paraphrase what you're saying here and make it a little bit tighter than what you said but I agree with you so obviously if we're doing an application like meat Echo we can definitely make uh the relays not have access to any of the media content for sure and be used to help scale out something like me could Echo but mean Echo also probably requires an SFU somewhere and these relays do not 100 do the functionality in SFU you'll still need an SFU in some sense which also can be end-to-end encrypted but yeah okay so yeah that works for you yeah there's got to be enough metadata to to to to allow the switching to happen got it the other thing on the authentication piece um if I've got a large fan out and I've got Fred who's been creating um data and we're using Mac or whatever to authenticate I don't want Fred to have to have a million Association uh uh shared secrets with all the end points I don't want one endpoint be able to impersonate Fred so The Logical implication of that is the relay is going to Fred is going to authenticate to the relay and the relay is then going to re-authenticate to each endpoint in that scenario okay I think that is a hot and deep area you're not wrong of course and I think that'll be a great topic for the working group after we get past some of the easier stuff but we're not like but you're totally right but I mean we're this like people aren't even ready to It's Gonna explode their heads though"
  },
  {
    "startTime": "01:18:02",
    "text": "hello uh Robin marks Akamai uh I've seen Luke and you talking about priorities for streams and dependencies between streams and not doing the just at the origin but also in between and it starts to smell a lot like HTTP 2 dependency tree which is a notoriously bad idea that we're still suffering from today um especially people like me um uh I don't think I think this is too complex for the new extensible priorities that we have in HTTP 3. um but what I really want to say is keep it simple great feedback well yes I I will I'm on the same page mercenary so quickly on the dependencies uh I think uh if you look at um what we do in RTP in in a lot of us assumptions is that sequence numbers are always implicit dependencies on the things before them the difference here is that now we're talking about independent quick streams and is there going to be some sort of signaling of the dependencies across those quick streams because they may not truly be independent they may be in the SVC case different layers in in each of the streams or for various different reasons you may have different encodings that split things up across streams and you might be able to describe the dependencies of those streams I think it's pretty simple to have a simple and flexible way to to describe those without too much complexity we have the frameworking draft we have 81 dependency descriptors and you're talking about you know like a simple one bite thing that that gives you almost everything you need to know but within a stream I think it's just simple to keep the basic rule everything is in sequence what came now depends on everything that came before in that stream that's that's a simple assumption to start out with um and then secondly um I'd like to understand from um from the uh meta and twitch folks you must already have some relays assume that just your own built realize if you had to replace those with my cloudflare fastly whatever what would you do differently or what would you have to"
  },
  {
    "startTime": "01:20:00",
    "text": "encode in the protocol to work with a third party I think that's an important thing to get out of the protocol is for people who wanted to have vendor infrastructure it's not their own what would we need in the protocol to enable that yeah um it's interesting around this envelope information and the security if you're actually going to sign Etc it needs to be stable across the whole system which has been a classical case which doesn't work in RTP because you have leg by leg or basically and point-to-end point dependent signaling so you're gonna have to have an interesting actually ensure that you have a global namespace for identification Etc this type of information so yeah 100 agree and that's part of why MLS is in the charter the Piezo handled I'm just um going back to the um ABR stuff in that um you kind of have this very fine grain kind of behavior happening at the solves um like per stream representation level but then the ABR sitting in some kind of browser sandbox kind of trying to measure throughput on a JavaScript sort of API and making an ABR decision there there seems to be quite a big disconnect between the amount of information that's kind of you've got this very fine-grained stuff happening per per rep but then switching reps you've got sort of crappy information that sort of you're getting uh kind of quite messy like timing uh and data arrival stuff I mean the streams API for example doesn't even give you a timestamp when you get a call back so trying to measure that I'm thinking sort of signals or something like that from relays that could then help that but a cool cool idea to get some signals from the relay to help that I hadn't thought about that I had thought a lot about given this is all running in user space the quick and all of this is running in user space and application I"
  },
  {
    "startTime": "01:22:01",
    "text": "was hoping that this working group at some point would be sending some requests over to the quick working group to expose some API information that we can use uh for example the current you know bitrate estimate that the quick layer has which would be really useful for um yeah I mean that's a constant kind of thing net info API has been sort of like in and out of uh yeah like it's not really kind of flying anywhere apart from chrome and uh so that kind of thing is tricky uh it seems yeah but there's such a huge advantage to having a lower level feedback on the the information allows you to react much quicker which allows you to deliver a much better end user experience and I think that that's one of the things that we need to optimize the group is well you know yeah this information is all in the same computer but our layer stacking has caused us to have not have the information we need that would allow us to deliver a much better end user experience that's nuts yeah yeah it's true although sometimes in some ways there is actually information that's at the server the relay that the client doesn't actually have like the kind of effectively the sea wind um you know how to draw transport info that basically tried to do that um but uh yeah that we've got some stuff in cmsd that potentially does something like that but uh but anyway so we should drive all of that discussion forward it's a great discussion I think very important thank you okay so uh we started out with 12 we've had uh seven people at the mic and now we have ten so I would like to say we're gonna close the queue very quickly here so if you think you're going to have comments on this probably go ahead and get into queue well it's pure uh sorry will next yeah thank you uh will the lack of my I'll be quick I agree with the vast majority of the points presented here I I think Mark has the luxury of looking back at 12 years of Dash on hls which used relays in a very similar context and look at what would we have what should we have added to those formats in the very beginning that would really help with operations today and there's two aspects that that are"
  },
  {
    "startTime": "01:24:00",
    "text": "not the the attractive ones we like to talk about one is request tracing being able to debug as soon as you're going through components owned by different people in multi-cdns how do you know where a problem's occurring so request tracing tracking where it went and then also logging if you ask people today what is a quick or web transport log look like uh you're going to get 50 different answers so we might start thinking about standardizing that so that if you are collecting logs from five different cdns operating your relays that they're going to be consistent thank you I love it and I hope whoever's taking minutes got all of that Corey since he's our uh our minute taker you can ask him whether he got all of that I saw from his face that he was doing his best but not I tried yeah going first I was just going to say and we'll talk about the intermediary being a relay there's probably some interaction with the network that was that might be possible can we take that to transport area tsbwg or something and talk about that completely separate to talking about the relay but if you're exposing things in the real way then maybe something to the network might turn out to be the right thing to do 100 agree Mo touched on that earlier and you're the person that helps us do that all in the past with every other media protocol we've done I hope we can get you involved again yeah Stefan Wagner regarding the dependency indication question you post um this is very very tricky the one reason why previous dependency indications in the video coding standards nh64 uh had two bits for some priority thing never worked out was because there was no defined uh Behavior what to do with these two things and with these two bits and therefore people set them anyway if they wished and no one cared about them"
  },
  {
    "startTime": "01:26:02",
    "text": "at the end even if the encoder choose to do that so this particular problem here for this protocol is even harder because we now have um different media types some of which we don't even fully understand that the lifetime of this protocol is probably larger than the full understanding of what you need say for hat takes uh type of priorities or whatever right so the um my recommendation there is to have a single one-dimensional now number not any for complex dependency graphs with B frames or whatnot but the single number with a limited range and a defined behavior of how a Gateway reacts to this number and then the encoders uh and the the sending mechanisms can figure out how to set this accordingly thank you thank you that sounds like a really good advice for us Luke from twitch uh I was just on up there but um my day job is I work on the CDN team um so I've thought about this a little quite a bit um one thing that I'm grappling with internally is just how successful HTTP and HTTP cdns have been because they're stateless the idea being is we have millions of viewers across thousands of streams and we don't care like we you don't actually need to keep track of them it just kind of funnels all the way through so I'd like to have more thought about how much state do we really need in the system how much state do we need for media for quick and how much can we defer um like for example slide one the origin is the final decision maker probably wouldn't work at scale um for this uh point we that's that's one thing I want people to think about so look before we walk away totally totally agree but I want people to I constantly have people tell me it's stateless and I'm like oh how do I request something they're like oh you give us the URI and from the URI we know"
  },
  {
    "startTime": "01:28:01",
    "text": "given the state we store what to deliver to you I mean it's it's I mean stateless is a wonderfully bizarre term to use in the context of a file server right so it is stateless but I think that the thing that the point 100 agree with you on is we have to really carefully think about what the state is there's TLS State there's all kinds of State in the servers we need to really think about the state and how we make high reliability cheap scalable systems uh you know how we engineer that and get it um we decided the state at the start of the broadcast when somebody joins and then we just encode them the URL and from that point forward it just follows a fixed path more or less so yeah pretty simple yeah exactly which sounds like we could do the same thing here yeah foreign poly can you go back one side to the security sure give me just a second here there we go yeah I think the another point that the quick stream is one two one I think there is a drop about a multicaster quick you need to make a quick become the one to earn so if we use that we may be able to do the end to end quake and for the metadata maybe we can put it in the outside of the quick uh of course you need to encrypt it yeah thank you for correcting me then I was unaware of this multicast quick work I'd love to read more about it and it seems like this might be a prime use case for it if it all works I just this is a my mistake on this slide and I will pick yeah and and I think during the end to end quick may enable very nice Stadium uh kind of because Renee doesn't have to decrypt all the quick quick package you don't need to look at metadata it's very small amount of computation Maybe interesting okay can can you send a link to that work to maybe the the chat room or something in the mailing list yeah the mailing"
  },
  {
    "startTime": "01:30:00",
    "text": "list Randall Jessup Mozilla um uh plus one to what's uh Stephen Winger was saying I was going to say something similar um I think that what this does is it helps push the um the complexity on the choice of the priorities and so on out to the application which will allow for future you know changes and improvements in in what's going to happen there whether it actually comes down to a single number or not I wouldn't want to decide now but I do think we wanted to simplify and tightly Define what the relays are going to operate on uh so that so that then you can innovate at the edge at the edges thanks Randall 100 agree I I that makes sense to me and all the input from you and others here has really convinced me which way Direction I want to go on this so hello um Ian sweat Google um as a person who spent a ton of time trying to kill hp2 priorities um I actually would make an opposite argument here and say that I don't think we should immediately assume because HTTP 2 priority was like a complete dumpster fire um that it actually applies and my argument is as follows so one is in hp2 the client is attempting to guess what the correct prioritization scheme is here the producer of the content knows the structure uh of the Gap and it understands the premium dependency there is no it's unambiguous it's absolutely like it knows it or not um another thing is we had things like reprioritization there's no need for reprioritization there's no redirect re-parent things that do all the things that are very complex um and for example like in hb2 when you cancel the stream everything under it got like re-parented automatically hero would argue either you don't allow that at all or if you do allow it you just basically like kaibox the whole Stream"
  },
  {
    "startTime": "01:32:01",
    "text": "So you because they're all dependencies you can just like kill that entire tree you don't need to wait and like trying to re-parent it and all those things um so because we understand the use case I think it's a completely tractable problem to use trees however I also will say I'm not sure it actually provides a huge amount of value over like the stream per Gap approach um and so I think that's something for the working group to decide is whether the marginal complexity is actually worth it um but I don't think it's I think it's a completely different situation from hp2 because the problem is different um who's sending the information is different um like again a huge problem with hp2 was Firefox had a prioritization scheme that it kind of like invented based on like looking at the spec and trying to figure out how to use it but like there was no right answer like everyone did something different on the client side none of them were right and most of them didn't work particularly well here we have a fairly authoritative answer on what the right prioritization is at least within a gap so those are my thoughts um thanks thank you a quick question uh I showed that you were trying to to uh request the screens uh did you actually have something you meant to be showing or is that just a that was a fat finger okay no worries thanks thanks Karen for bringing everyone on the same page on what relay on some of the requirements we need here a couple of points uh I agree on the state argument that you and Luke made we need to make sure we keep the state as minimal as possible as it helps for the distribution uh anything other than that we need to kind of discuss in the group and see if we really need it or not uh second thing uh on I want to Echo what Randall and Stefan said on the dependency or prioritization scheme whatever we do relay should not be made to aware of each and every video Codec and dependencies they bring in they should be asked me agnostic as possible but should be able to give minimal information to make a forwarding or drop decision and if you can come up with"
  },
  {
    "startTime": "01:34:00",
    "text": "something that fits that requirement that would be easy to build scalable relays uh last part is that I don't see uh you're talking about how the clients would pre-warned relays uh in case whether they wanted multiple qualities so that they can easily switch between different qualities maybe have you talked about it but missed but something that we need to keep in mind on how can relays kind of of subscribe to various qualities so that when our client uh because of what our Network conditions goes to different qualities it would be quick rather than going through all the way uh through chain of fillers to get the qualities if needed at some point right um yeah I carefully just avoid it in my my um my slides the whole topic of sort of naming or or that that evil subscribe word or anything like that um but I mean one way or another we will need to be able like all of these things imagine that we'll be able to you know something could express it could get a a consumer could get a 4K stream and it could realize they didn't have the bandwidth and switch down to a 1K you know a 720p stream or something I mean I clearly we need to deal with that in the working group I just didn't feel we had the right drafts and agreements in place to really hit that problem yet so we'll need to figure out that that type of stuff down the road a little bit but I just sort of chose for that for the limited time to kick that can down the road a little bit uh here uh on the topic of congestion control there's nothing stopping a native or mobile app that's using a quick library from getting to information about the congestion control however for a web client which I assume you will want some web clients that do Ingress into the into a relay um you do need an API that can expose information luckily we are working on that in the web transport uh working group in the w3c so if there are specific things you need beyond what we're already doing there that might be"
  },
  {
    "startTime": "01:36:00",
    "text": "a good thing to uh ask for similarly on priorities I think we've mostly been talking about priorities that are sent to the relay but there's also the question of the local prioritization um of streams uh at the at the client and that also relates to a web client API and that's also a topic in discussion in the web transport to be 3C working group so um if there are things that need to be requested of another uh working group that might be the w3c web transport region group lastly um oh I forgot I'm sorry let me see if I can remember uh nope it's gone so Peter before you get to your last thing just 100 agree what you said and I might ask you if you could just send a little bit an email to the list here I mean I know you're deeply involved with all that stuff you can just send a little email to the list with with pointers to people where they might be able to read a little bit more about what's going on there um I think that that would be I mean this this group's gonna have to have a Synergy with you know transport uh web transport uh quick uh and so getting people aware that would be awesome I could do that oh I did remember the third thing the third thing is um basically everybody wants implementations of quick that have a real-time friendly congestion control algorithm and I haven't really seen one yet um and I think that in order to write one for example if I were to Port the Weber typical webrtc one called googcc over to Quick uh it would require some more time stamp for information in the feedback messages and that I've seen two drafts for that uh proposals for adding that as a quick extension but I don't think either one is you know progressed a lot so that might be another thing that might be a request to see one of those progress"
  },
  {
    "startTime": "01:38:00",
    "text": "so a 100 agree with that I think we do need to move that over along over time for this group to be successful I don't think it can be done in this group I think we probably need to take that somewhere else but the people here can and you know probably are the same people that need to do it to somewhere else and explain what our needs and why the current bbrv2 doesn't quite work but what could be or whatever it is that we're going to decide to do to have better congestion control for our applications I do think this application will that I do think that media over quick will work reasonably well with today's uh congestion control but it could work much better with slight changes to the congestion control that we should try and get from the appropriate working groups or something so that's another thing that we should poke on moving forward 100 agree and just you know maybe now that this is really moving and is real it's an incentive for the people who have those drafts to to resurrect them and get that going again next stanhurst BBC R D um so it's not specifically a comment on your on your presentation which is great by the way but more uh something which keeps coming up in other comments that people have been making and I wanted to make sure it got captured somewhere we need to make sure that we don't end up focusing on frames as a unit so I love the idea of walk being able to arbitrarily chop up gops and have different numbers of frames within one stream we've got to be careful because when you're thinking about audio what what's the application data unit there and is that a sample and if we end up just having a new sample per stream we're going to use up streams like there's no tomorrow um and there's also then other media types like what do we do with subtitles what do we do with haptics which is a new one coming through we just need to make sure that we don't specify that there's one for every Adu uh and give the ability to have more than one of those and then how do we synchronize them but that's an entirely different not that's an entirely different topic great Point 100 agree and we tend to over rotate on video and forget about other things but yeah"
  },
  {
    "startTime": "01:40:00",
    "text": "hi Jonathan Lennox I mean uh something that I was thinking about when the medical case was mentioned and generally like the case of if you're trying to you know essentially live stream a conference um is the case where you will have cases where we have in where we have media from Independent Media sources we probably want to send those over the same mock connection for prioritization reasons and we need to make sure we think about that both for prioritization then for the protocol to figure out how that works and I just want to make sure that's you know on everybody's mind as they develop this yeah and I think that that uh comes up and as you said in the the conferencing scenarios but it also comes up in the ad insertion scenarios um but yeah that we need to keep that in mind uh I think we have an empty queue thank you everyone much appreciated for all the people showing up here today moving this long and I will pass it back to the chairs for whatever happens next uh thank you very much and thanks everybody who was at the mic line or commenting in the chat I I will say if if we're seeing this level of engagement on the mailing list we would be having a weekly uh interim calls so please do uh bring these same uh issues up now that we've we've kicked off the working group uh on the mailing list or in the uh appropriate repos and we can go from there um uh pull up the get the chair slides back up so we can go on to the next thing all right uh so before we get to aob"
  },
  {
    "startTime": "01:42:00",
    "text": "which will uh which will possibly include uh Spencer's presentation which he gave us as a backup a quick question on uh possible interim uh and I think we can run this as a poll I'm going to try and do that in a second um we think there's a a lot that's that's happened here today in a lot of discussion that have been kicked off at the previous interim and so we'd like to think about having maybe an in-person interim over a couple of days uh What uh Alan and I have been discussing is late January of 2023 that's that's next year surprisingly um and um the U.S West Coast uh there are a couple of people already in queue so you have clarifying questions or comments before we run the poll okay please please go up Spencer anything we would do would of course be hybrid not only in person but Spencer you had a clarifying mode did you have a clarifying question I don't know if you can still get from it so uh mozamati I was going to ask in the prior interim we identified two major points of discussion that we wanted to try to cover either in person this week or you know in the in the near term that was the congestion control and prioritization aspects and then media formats we haven't started the media formats discussion is that going to be a focus of this in interim or do do we want to start that just on list and not uh so I think we can understand it a little bit uh today when people were reminding us and including Stefan and and others that we need to consider media formats Beyond video uh and not uh not only video but I definitely agree that we need to have uh deeper discussions of those on on audio video and potentially um upcoming uh it might or might not be at this interim we if we get agree in an interim then we'll start the agenda bashing so you don't have a specific agenda for this interim yet uh no we think there's plenty to talk about so we'll be list discussion to to hash out who can be there in person and in"
  },
  {
    "startTime": "01:44:01",
    "text": "particular which topics we people will have drafts or or similar to uh Spencer uh Spencer Dawkins uh I've made this coming in chat but uh I I you know I don't know the chairs have been able to keep up with everything that's happened in chat um the the you might want to also poll on how many interim meetings and I I'm I was thinking virtual interims uh between now and Yokohama um and it I had a couple of thoughts about possible uh topics and neither one of those was the one most suggested so um I you you all are talking about a in-person interim yes please that would be lovely but uh I I would I would suggest that you uh wait for the working group to make progress on the mailing list as opposed to when slack or you know something like that um before uh you decide that we only need one uh I could see multiple reasons why you might need two or three but let's let's ignore that for now okay I have suhaus and Cullen behind you in line and then we're going to run the poll um I think we can at least figure out whether we want zero or one and if it grows beyond one we can continue that discussion but uh just a clarifying uh thought here uh Luke's presentation had some of the next things that we want to do that might be a starting list we can think of the possible topics and and also depending upon the topic we might uh get a different audience and and we need to make sure that we have the Right audience in the meeting if not it will not be useful thanks Cullen uh love to have them but I think the really important thing asked us is January late January is incredibly close when you consider the holidays and"
  },
  {
    "startTime": "01:46:00",
    "text": "everything so I think we should nail down the dates soon if we're going to do this like before the end of November type soon that's partly why we want to to have the poll run today okay so the way I put this is if you if we have an in-person interim in late January 2020 through 2023 please assume I said U.S West Coast here because apparently I forgot to type it can you attend either in person or virtually in that time zone so please understand uh that there any place we have this there will be support for um remote attendees but that that time zone late January uh raise your hand now uh if you can do not raise your hand if you are not available there and we will see how it goes foreign so so far only about a third of the people in this room have either raised their hand or clicked do not raise hand so please go ahead and do one or the other as quickly as you can thank you okay the numbers are starting to change pretty slowly I'm going to close this off in 30 seconds okay so uh certainly quite a few people were able to do it the raised hands were 52 that do not raise hands 10. that's a"
  },
  {
    "startTime": "01:48:00",
    "text": "significant number uh so what we'll do is go ahead and come up with a concrete suggestion on the list Alan and I have the token to do that um it looked like there was somebody in the queue but who may have since dropped okay so we do have a couple of uh minutes there and there was a uh update on the mock use cases and requirement stock uh Spencer do you want to do a quick run through that temperatures so I'm Spencer Dawkins uh I want to call people's attention to the picture of this on this on the cover slide pretty maybe more than anything else you know the idea that use cases would interact with requirements and that those would uh Drive protocol specifications if that's that's if that's uh shocked anybody please tell us now our next slide please um so what we said we were going to do was uh work on basically dragging out a lot of stuff that was in the individual draft for um require for use cases and requirements uh that would not belong there some of them were talking about things were not in the approved Charter and some of them were just history and observations and opinions um and so what we did was update the use cases section well we updated the draft this way and"
  },
  {
    "startTime": "01:50:01",
    "text": "um went to update the we were thinking update the use cases to reflect working group discussion on the slide three questions from uh the from the uh for October interim there wasn't actually any discussion of those questions from the for children uh so uh we submitted Dash zero three and uh Forex for extra credit uh did a proposal of uh initial structure of the requirements section uh based on our understanding of the implicit requirements that they're into approved Charter which was based on uhas uh understanding all right next slide please yeah so uh yeah that um so and like I say added a little uh introductory text in each subsection of the uh the requirement section um next slide please um so this is uh I did talk with um James Michael author uh since we got the uh comments from Cullen uh that were that appeared on the mailing list thank you for those calling um so uh so thinking we would do a-04 revision before requesting that the chairs would uh adopt anything um and then basically after something that uh the working group can adopt we would be drilling down to the next level of detail in use cases index level structure in the requirement section next slide please um so I did have some questions there like I say which we will follow up on the Mike mailing list uh for all of these uh we were trying to"
  },
  {
    "startTime": "01:52:00",
    "text": "we were trying to categorize uh the live media of uh gaming and video conferencing use cases uh and then think about that some more again we'll follow up on the mock mailing list anyway and once we get to that point uh talk about any additional use cases that would need to use a simple low latency media delivery solution for ingest and distribution um as we mentioned earlier there's those are out there but the ones we would be we would need to know about is our use cases through all our own capabilities we have not previously identified uh next slide and so we had basically a high level structure from our understanding of zuhas's high-level structure from the interim meeting and uh which I really appreciate and they're basically agree on a starting line for that the requirements and then proceed in the usual issue PR emerge way on GitHub is that my last slide I try to remember no we are not um I like I said uh Cullen sent uh comments that were uh helpful enough to where uh we would need to you know James and I both agreed we need to do a dash 04 before asking people to look at it seriously and we're committed to do that in the relatively near term um and uh any other thoughts okay okay that's everything I know thank you uh thank you very much Spencer note that we will not be waiting to an interim meeting before issuing a call for adoption on this once we've gotten the mailing list discussion uh to a point"
  },
  {
    "startTime": "01:54:00",
    "text": "where we feel like the working group is ready we'll issue with a call for adoption on the list so please do continue to uh join us on the list so that you can do that we have reached the 11 24 Mark and the question is is there anything else for the good of the order sorry thanks David skanazi process Enthusiast apparently uh no just uh I wanted to provide a small suggestion to the chairs uh because um this reminds me of when we did a requirement stock in mask and it was helpful um but we uh ended up not publishing it which was quite disappointing for some of the folks working on it so just they make that clear at the beginning whether or not you plan to just have it and not publish it or if you're going to publish it as an RFC both are valid options but I would make that clear during the adoption calls so no one's surprised uh thank you for that point um certainly uh as people are commenting on in the run up to adoption uh you can consider that but um and and make comments on that then even in advance uh back to the previous question is there anything else for the good of the orders there was someone else okay uh thank you all for all of the great conversation uh both in the lines uh remotely and on the chat today uh we look forward to seeing you on the list thanks again different ly"
  },
  {
    "startTime": "01:58:42",
    "text": "foreign"
  }
]
