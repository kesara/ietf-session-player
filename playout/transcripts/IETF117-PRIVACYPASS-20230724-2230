[
  {
    "startTime": "00:00:05",
    "text": "So please thank you for the reminder, David. Plea please pass the the magical blue clipboard, clipboard, and put your phone out. It that will get you both access to the in room tools, and make sure that you are listed as attending. we've been told that There's been some some loss of accuracy in the on-site attendance records. are trying to improve that. We still need a minute taker. We cannot start the meeting until we have a volunteer to to note some minutes in the notes tool taking minutes at IETF, Sessions is a great way to participate if you're very experienced and already know the material or very new and want a chance. to familiarize yourself with it. Oh, thank you. smaller, Thank you to Phil Holland Baker for volunteering. As you can see, there are live there is a live transcription being made in some magical way that I know nothing about what what what on the screen there. In order to have your words correctly attributed to you, please make sure to use the on-site meeting tool to put yourself in the queue when you are speaking, we will and we will manage the queue. That way, the the system will attribute your words to you and not to somebody else. You yes. Thank you. You it's also important that you say your name at the mic,"
  },
  {
    "startTime": "00:02:00",
    "text": "because the that system is not a 100% perfect. I think we're we're ready to get started. This is Monday, so it's possible that not everybody here has seen the note well? Please remember that participating in an IETF meeting like this one got causes you to incur certain legal obligations and also other kinds of obligations and the note well documents those. in particular, in addition to the legal obligations, There are also codes of conduct and guidelines for behavior, I I think that as a working group, we've done well this front, but please continue to treat each other with respect and and with And let the chairs know if there's a problem or reach out to the ombuds team. Just a reminder. Yeah. Just a reminder to use the use the right tool for the job Lots of folks coming in. Please make all seats available. I remember to sign in. Yeah. And if you're you're coming in just now, please remember to register with the on-site tool. that's the only way that otherwise, next time, we're gonna have an even smaller room. Yeah. Okay. This is the chair's proposed agenda for our 90 minute meeting. If anybody would like to revised the agenda please let the let the chairs know right now."
  },
  {
    "startTime": "00:04:01",
    "text": "now is your chance to tell us that you'd like this agenda to be different. Otherwise, we'll get started. with the chairs, followed by a brief update on rate limit tokens. some proposed new extensions to privacy paths that are not working group items, Next, And then a discussion from the from the configurationconsistency design team including a a specific proposal. And finally, a a brief report on some related work from the w three c. one of the topics that was going to be discussed there has been struck and and will be discussed in a in a later time, I guess. not in this working group. So We wanted to take this chance to remind the working group that we've have several documents that we've successfully pushed past working group last call. Hooray, thank you to the authors and editors for moving through that long process. of course, the working group last call is not the end of the road, and so those documents have been substantially revised. Well, at least, notably revised. since working group last call. We're not going to take working group time to walk through the details of that. But if you are interested in those documents, please go and take a look at the latest version. there is some new and interesting text in each of those documents. We've previously promised to do an adoption call on the batch token's draft. That hasn't happened yet, but it will happen shortly. Also, there are several drafts on today's agenda that"
  },
  {
    "startTime": "00:06:02",
    "text": "are pursuing adoption. So as you listen to the discussion of these documents, please consider whether you'd like to adopt them. I think that's all for the chairs, and now we're going to hand over to Tommy Polly for a a brief update on rate limit tokens. think I'm asking first Let me see. Yes. Okay. Alright. Hello, everyone. This is a brief update on the rate limited token issuance protocol, which is is one of our adopted documents that's The main protocol on that is not gone past last call yet. we're still working on. I mean, as a brief recap, this is a variant of the publicly verifiable token that involves rate limiting that is performed on the attester The attester keeps state and you're required to have a split a tester and issuer model here We have a handful of recent changes that went in right before the document update deadline because we realized we should probably update it So we did. Some of the updates that went on here Mainly clarifications for some of the great issues that had been raised around areas that could be confusing, like the fact that if you have different time windows, upon which you need to rate limit your tokens such as you have one day tokens, 1 week tokens, 1 month token right limit windows, Those need to be different. issuer names to make sure that the token and testers that are keeping state aren't able to learn too much about"
  },
  {
    "startTime": "00:08:02",
    "text": "what's going on. there's some text in there around how you do rate limiting when you have token challenges coming from a 3rd party web context and making sure that the rate limit applies to the first party so we clarify that text. we talked about some of the cases where the tester could be penalizing the client if they are rotating their secrets too frequently to try to evade rate limits and we added some security considerations. talking about how the your effectiveness rate limited tokens is determined by your a tester that you select because the state for how many tokens have as this client used with this issuer in the you know, last week, needs to be kept on that attester. And so if you were able to use many, many different attesters, off with the same issuer, then the entire premise of the system breaks down a bit. And so is something that needs to be considered in selection here. Now the other thing is that on the next slide, m, we painted our bike shed. We chose green, everybody. So we we'd had one of the terminology points in here that was particularly confusing. essentially, there is a set of inonymized IDs, where the client is saying here, here's the pseudonym that I have for the origin that I'm trying to use. I don't wanna tell you what website I'm going to, but this is the pseudonym for it that I need to be rate limited on. and then the issuer had a similar pseudonym going back to confirm that that was unique. anonymous origin ID and anonymous issuer origin ID. we're very It should be issue or not issue. very confusing. And so we've renamed that to the origin alias. And so the client has an origin alias. and the issuer has an origin alias."
  },
  {
    "startTime": "00:10:02",
    "text": "Yeah. So if you're reading it and you were for some of the terminology that you saw before, it's changed. let us know if you hate the bikes it and you like to paint it blue instead. and that is it for the updates. Now we don't think we're ready for last call yet because there are a couple open issues such as aligning with the key consistency story, which is something that we'll talk about later and we didn't want to actually update it until we had had more work by the working group on agreeing on what we're doing for key consistency. So let's do that. We do have implementations. Apple's implementation supports this. I know some of the open source go version support So if you were interested in testing with this, Yeah. Please do that and let us know. Otherwise, Let's continue working on key consistency. before we wrap this up. That's it. Any questions? Okay. Thank you, Tommy. Next, we have a presentation on various proposed extensions to the privacy class protocol. to do Yes, sir. Okay. Great. Great. Alright. So I'm gonna be presenting sort of a reframing of something that Scott talked about time at IITF 116, which is this public metadata variant of privacy pass It'll be framing it in the context of extensions to the protocol. So so"
  },
  {
    "startTime": "00:12:02",
    "text": "As a recap, the the basic issuance protocol right now, it it it specifies, like, a number of things. Like, how you how you are the basic issuance and redemption protocol, specified number of things. For example, how you challenge a client for tokens, what a token looks like when you redeem it, what the attributes of the token are, whether or not it's, like, publicly verifiable or not as an RSA signature or, like, a VOPR output, whether or not support public metadata or private metadata. And the basic protocol documents I just went through working group last call. do not support any metadata despite us having, like, a registry entry for know, supporting these particular things. And so oh, jeez. the the the purpose of this talk is to introduce a new variant of issuance protocols that do support metadata. with a specific use case in mind. So to sort of give some people some context about what I mean by public metadata In basic privacy pass, there's, like, this from the origins perspective, there's a challenge that it sends to the client basically saying, please give me a token that is consistent with this particular challenge. and then the client after it runs the issuance protocol, construct a challenge or a token that's bound to that particular challenge and send it back, and the origin can just simply verify it. So In this interaction, the urgency everything in in the challenge as well as in the token itself. in all all as well. One of the properties of the issuance protocol for actually issuing that token, though, is the issuer sees none of it. it sees nothing that goes into the token challenge. nor does it see anything that actually comes out of Earth's put into the token itself in particular the signature. because it's a blind signature protocol or it's a VOPRF protocol or something like that. and And so this is perhaps some problematic if you consider use cases"
  },
  {
    "startTime": "00:14:02",
    "text": "or settings wherein you want the issuer to actually do some policy enforcement on, you know, that token requests before it produces a token. So, for example, maybe wants to ensure that that that a particular token is issued and it's only valid for, like, a certain amount of time. it could do so or could enforce this particular policy by, like, issuing the token by, like, you know, signing a a a token request that comes in And then just, like, discarding its keys after a certain amount of time effectively, like, expiring things. that's not a particular, like, great solution in practice. fortunately, that's what we had metadata for. another example, you might imagine, like, putting in geolocation information into a token saying that this particular token, it can only be spent in a particular geographic region. This is exactly what private relay does right now, and we do this in private by basically having a key that's bound to a specific geographic region. would be a lot simpler if in practice, if we just said, like, this is, like, the region that this particular token is bound to, we have one key that's shared across everything. So public metadata sort of changes this. by introducing public metadata. And what I mean by public metadata is from the origins perspective, there's this new field, which is some opaque metadata, Bob, that sits alongside a token challenge or sits alongside a token, and it's able to verify the verify a particular token with respect to that metadata. But the important difference is now in the issuance side, when an issuer is actually producing a token, Not only can it see the token type, which determines the issuance protocol, but it can also see this metadata So it can now what was previously hidden from it, it can, like, do do it can it can force whatever policy. It can put information in there. and the client can agree on what information is put in there. before issuing a particular token. So"
  },
  {
    "startTime": "00:16:05",
    "text": "that's fine. We have, like, issuance protocols that support this opaque metadata slot. And then you might ask quite naturally, okay. How do we actually put information into this metadata to make it useful in practice. so there's a document that we have that basically, add some, like, structure to this particular metadata field, and we and that structure is, like, a list of extensions because that's what we do in this particular community. We we want to, like, add things. We, like, do so via extensions. In particular, This is the this is the basic structure of the metadata. It is a a list of extensions that go in that are effectively bound to a token. and you can I'm whatever type of extension you want, put whatever extension data you want in there, and provided that all the parties in the sort of deployment agree on it, including the client, which is very important. That information will be cryptographically bound to the token. And so we have a way to, like, cryptic do the cryptography to actually issue these things in a way that the metadata is bounded token. We have a way to structure think. the information in a way that you can actually you know, implement it reasonably well. to actually send it on the wire there's yet another draft that extends a the authentication mechanism that we have right now and the base protocols to tasks. the metadata as an additional parameter alongside the token itself. So If you're using an issuance protocol that happens to have public metadata, would send the authorization header with a token, block, whatever. b. And then an extensions value, whatever that happens to be, and then the origin would could take the extensions and use it to verify the particular token."
  },
  {
    "startTime": "00:18:01",
    "text": "pretty easy. And it could if it wants to parse that extension list and use it as necessary, or use it as desire. And so a very concrete extension that we would like to have is exploration. And the expiration extension is very simple. It just basically encodes when a particular token would expire. So you could run the issuance protocol you know, at a certain point in time, encoding the information that when the particular tokens from the issuance protocol should expire cash them. And then you know, used into your heart's content up to the expiration, and the the origin is expected to enforce that expiration by checking the expiration time and you know, rejecting a token if it's later than it that particular timestamp. there's this is not super complicated or anything, but I don't think it's super complicated, so, hopefully, the the intent is clear. So that that is a simple, I guess, motivating use case for particular type of extension mechanism. course, we have to be very careful with, like, what we put in the extensions like, it cannot be used to partition the client and then we reset. Expiration is, I think, kind of a no op because you can already code them in other ways, and clients effectively can you reasonably agree on what the expiration information is without thinking distinguishing Yep. Yep. property to that. Any particular individual client But there are other extensions that people can imagine that could, you know, effectively be used to partition the client on the reset, which would be not great. So course, there's gonna have to be a lot of guardrails with each extension that's proposed, the architecture document, hopefully, already describes some of the considerations that women have to take into account for issuance protocols that support public metadata and became lean quite heavily on that text. but I would I think it's pretty clear that each extension document or each extension itself should make clear what the privacy implications are."
  },
  {
    "startTime": "00:20:04",
    "text": "So from a, I guess, a technical perspective, a number of things that we could know, I guess, consider asking. So, for example, right now, in the documents, It's not possible for an origin to say, to challenge a client for a particular token and ask a particular set of extensions to be included. but you could reasonably imagine doing so similar to how client hints work right now where an origin says, I will set the set of client hints. And then if a client wants to present them, it simply presents them. that seems like something fairly easy to do if you wanted to do it. I think we'd have to think through what the pricing implications of that are, especially if, like, users might make different choices about what extensions they are are not willing to support. But this is, like, not a new problem. We've encountered this in many different contexts already in this community. so so so so I'm confident we can work through that. And then there's, like, you know, bike sharing questions, like, do you encode these extensions? Should it be TLS style, Quick style, whatever? Those are things we can work through as people implement them and have strong opinions on them. But I I think, like, putting the technical questions aside and perhaps I shouldn't even put this on question slide. are reasonable privacy guardrails to have in place for extensions perhaps the most important thing for this class of protocol. is the is are is the guidance that's in the architecture document sufficient do we need to add more? And what is know, what is reasonable? an extension to to to include in a way that, you know, doesn't violate the intended privacy goals of privacy pass. Okay. So there was some interest based on Scott's presentation last time, There are some use cases for this. Like, for example, Cloudforships extension that has privacy pass. way we deal with key rotation right now is"
  },
  {
    "startTime": "00:22:01",
    "text": "quite ugly. It could be vastly improved if we had something like this, so I would like to implement it for that particular purpose. So at this point, given potential applications, and I think, like, you know, confidence in ourselves to put proper guardrails in place. I'd like to ask if there's interest in, you know, implementing and using these for whatever use case you may happen to have. And if so, if there's interest in adopting sort of this, like, batch of work, which includes The issuance protocol itself, it includes the extensions to the authentication scheme or the authentication mechanism, and it includes, like, the like, extension documents themselves. then Yeah. Tell me. Go ahead. Alright. Thanks for sharing this. the I I like how this is framed with the extensions. I think that makes it a a reasonable way to get metadata in their I do see the use cases think we need to look more at the crypto and, you know, what we're comfortable with for actually deploying it, so I can't speak to that yet. But I think it's It certainly seems within the scope of the working group, and it's something that we should continue to work on Just a thought around, like, the safety here. Since we're also having these discussions around efficiency. It seems like, you you know, the client wants to be able to guarantee that they're not being partitioned by this metadata. And one of the examples so, you know, today, as you alluded to, For private relay, we just have a different key per country and time zone. And there's a bunch of them, but, you know, you deal with it. And it seems like one of the motivations here is, like, we don't wanna have to deal with that many keys we just simplify this? But can we have at least the consistency"
  },
  {
    "startTime": "00:24:02",
    "text": "or partitioning guarantees be somewhat similar in that You know, like, if I have a consistency check today for I make I'm making sure that I have a key that's for my geographic region, and there's a bunch of those should we include Like, or is it viable to include, like, essentially, every version of metadata in the consistency such that you have, like, tuple of your key and a particular spelling of that metadata. and you make sure, like, this is a well known thing that's registered, And you could even say, yes. All the other people in my population see this same set of metadata too. essentially that it's essentially just a shorthand for a key, but that you have fewer private keys on the back. That's right. think that would be a reasonable way to go, but I'd be curious to hear if there are metadata use cases for which that would not work And if that is the case, are those metadata use cases that we should be more worried about because they're potentially more fine grained on clients. Yeah. That's a a great point. In fact, I think our thinking as a group has sort of evolved from that key consistencies to, like, configuration consistency, which would include, like, the metadata particular values. I think your suggestion that maybe there are extensions that You can't very easily sort of run through the consistency check being a bar to, like, acceptance or not is probably a good one. Like, If you can't reasonably ensure consistency, for this particular tool of key plus other configuration information plus your metadata value, then Maybe that's not great for deployment, but, of course, deployments can do what they want. that and so that I don't know. That seems like a good filter function for me. Watson. Oh, Watson Lytle. Akamai, one thing I noticed in your graft is it seems to be an all or nothing revelation of the of the metadata that was included Yep. And conceivably, you could take the different extension equipment, some sort of merckle restructure and selectively reveal. And that way, you don't need to make the decision when you're issuing the token,"
  },
  {
    "startTime": "00:26:01",
    "text": "about the privacy impact. The client can and the origin can make it later. So that might be invest a direction worth investigating. That is a really cute idea. so much. You please note that in minutes. Fantastic. Any other questions? So, Susan, I don't like Watson's idea. I think that's really a good one. simply because one of the primary things that concerns me about all these things is that you're often given the choice to use one of these systems and no control over its operation. And my reason that I got in line was to say, oh, how how do we deal with this problem where In order to access the service, you need to produce a token, and in order to produce a token, only option you have available to you has a whole bunch of things in there that you would rather not reveal so that the the thing that's consuming the token That's that's cute. It's moving cutest. Right? That's great. Let's just fix it. Right? that seems like a no brainer to me. I did wanna just double check. There were 3 drafts on the front of this. Yes. One mechanism? Please, So you you do the way that we structure things. You need to defined intuitions protocol. It's capable of producing metadata and to do the the way that we redeem things in the header, you have to define how you actually, like, express that metadata on the wire. then you actually need to having around the system. This is what the metadata is. And so this was that that's what those draws Yeah. I We could it seems like that could be something that could be one draft. Yeah. Yeah. I mean, we made it a very conscious way to keep authentication and issuance separate. Sure. Sure. We whatever the work group wants, don't care so much. I'm I'm willing to be I'm willing to let the people who are"
  },
  {
    "startTime": "00:28:01",
    "text": "doing the work, make these decisions. But it's just when I see 3 drafts on the slide, I'm going, If if that's at least of our concerns, I'm gonna be short. Yeah. fantastic. Any other questions, Scott, you're in the queue? Yeah. I just wanted to quickly follow-up. Yes. Can you hear me okay? You're good? Yeah. Yeah. It was just a little bit loud. them. Oh, it's a little and it's way delayed. Okay. Cool. I wanted to follow-up on Tommy's points with trying to create kinda like, how this fits into key case SSD, essentially? I just wanted to know that it can get complicated because of free. I think this works really well for a geo like approach where it's essentially a fixed metadata tag. expert you more need, like, a email of saying, like, the free is going to always be rounded to an hour. The expiry is always going to be something like this, and you may need to size it to your deployment. you may need to say, Or you you may just stay out of the protocol. We're looking for this much entropy. and only this much entropy, and that's how you have to size. But you you won't necessarily be able to fix the metadata values themselves. You have to be a little more creative and that can definitely be done. per extension. and and register on per extension basis. So I I think it still works. It's just not super simple. I mean, I think in general, what we found I I agree with what you said. general, we found the privacy passes at the privacy properties very much dependent play this thing. So this is not unique in that way. and it's just a different a different variation of the do you deploy this thing in a way that's still privacy preserving problem? So yeah. like, Can't hear you. It seems like the We're good. We can hear you. Seems like the advice on architecture trap is"
  },
  {
    "startTime": "00:30:00",
    "text": "maybe too much on that deployment question. Right? It it seems to note that, oh, by the way, if you added more metadata, it would be naturally worse for privacy. and that you should do some balancing. I I I don't think that's nearly enough. if if what we're talking about is 64 bit. Energizer is our or or even geolocation. I I don't think we're talking about just separating the anonymity set. We're also talking about revealing information about the user that they don't necessarily know if they are are intended to reveal. So I I think we would much more elaborate privacy considerations before we're just gonna open like, an open ended amount of metadata that might be passed from, you know, a task or issuer to an origin. Nick, is this something that you think would land in the architecture document or in these different documents because, like, as part of the architecture document, we made an attempt to without, you know, writing pages upon pages, concisely express what the problem here. I'm willing to admit or accept that, like, it's not perfect and it can be improved. and would absolutely love PRs to help improve it. I'm just I'm concretely, I'm I'm I'm wondering if your suggestion is to, like, But pull that document back into the working group before it goes to ISG. or to elaborate on these elsewhere. I I guess I'm saying that we should demonstrate that there's a good guidance we can give on a particular extension before we start adopting or making progress on extensions. if if it can't be specified safely, if it's just, hey. Deployers please consider privacy, then I don't think that's good work for the work group. That's not a privacy pass. That's just a pass and where you're asking deployments to consider privacy. I I absolutely agree. I don't think anyone is suggesting that we just say, like, that would consider private see to be, like, the criteria to adoption or whatever,"
  },
  {
    "startTime": "00:32:01",
    "text": "I I I think before publishing any of this stuff, or even considering the working group, we should have a very good understanding of what the privacy implications are in be written down in a way that's clear to, like, the audience. So absolutely agree with you. and I yes. Let's let's do that work so we don't you know, do the service to the users. device? the Yeah. I just wanted to go back to that to be authentication, you know, just to understand it. a little bit more. Is is that for the client when it's presenting a a a token or fonts, to an origin. Right? Yeah. So in in the basic scheme, like, Yeah. Yeah. Okay. So the extensions, though, I guess, there's there's a kind of trade offs here. Right? because you have a a token issuer that expected certain extensions to be present, in this model if the extension is is set by the client, they could omit. Right? So if that's the flag that tells the origin, expiry as present, the client perhaps has an expired token, Can't they just admit that portion of the header to to to trick the origin into saying that that token doesn't have an Firey. So the token type will without unwrapping this a bit too much. in this header, there is a token type that's expressed, and that token type says, there must be an extensions parameter or not. If it's not there, it would be considered malformed and the server should drop it. Right. I I just say, I guess, in the more elaborate cases where there may be more it's multiple extensions and, obviously, you know, factoring in selective disclosure, it gets a little bit more complex. But probably want the ability for the issuer to say, like, if you get one of these tokens And the following extensions are not present. then, you know, that that's an issue maybe. Maybe. I I I think we need to think more about, like, the the mechanics of it. Like,"
  },
  {
    "startTime": "00:34:00",
    "text": "how you how you express which extensions you want and how those are enforced on the relying party side. But yeah. I I agree. This is perhaps an overly simplistic presentation of what you know, what we were thinking, it can be improved. Chris Patent. Chris Patton Cloudflare. This is more of a understanding question. is I I could see Chris' eager for me to No. Just get out with it. Get on, Ruth. Well, I'm I'm trying to I'm trying to frame hold on. Hold on. Is there okay. So my question is so the the concern is that there's all of these metadata variants that are are the issuers issuing things with, and he's trying to they're trying to split up things into small buckets. you know, smaller in the mini sets. is there an entity in the protocol that is that to which that would be apparent. It it very much depends on your employment, but you imagine, like, in the the way that private token is deployed right now. There's this, like, a tester party that sits between client and issuer. that couldn't, like, like, help with this consistency check to, like, to see whether or not this, like, fragmentation is happening. Yeah. So yes, In in some cases, perhaps not in other cases, depending on what our the consistency story is there, but I think it very much depends on that. I think as far as figuring out the privacy story, I think that would be the place I would wanna start. is, like, for what deployments if you have because I'm sort of, like, pretty happy with non colluding assumptions, like getting privacy that way. maybe others aren't. But I think that's, like, good enough, and that's a reasonable place to start. Cool. Thank you. So I had said another just question or sort of thoughts around expiry. Since it's come up a couple times here as it seems like it it expiry is a bit of a special"
  },
  {
    "startTime": "00:36:01",
    "text": "Snowflake potentially in terms of, like, you know, how we would verify it, make sure it's not too granular, and it you know, it's if it's based on issuance, time, then it's, you know, it's always moving. And also to to the point that I've brought up before, dropping that particular piece of metadata, like, maybe quite bad if you're relying on expiry, like, So 2 thoughts are, like, 1, Could we ex would it be okay to express instead of, like, an expiry time Could we just have, like, key like, epics? Essentially, we're just, like, you just have, like, a epic identifier per hour or per half hour or whatever. Say, like, you know, this is the window that you're in and that potentially is easier to check consistency on. And then, you know, if it was simple, does it make sense potentially to have in these documents, like, is expiry something special? Or is it even expiry outside of extension system. I could imagine, like, you could have, like, a token where, like, the epoch or whatever is next to the key ID. in the token or something. Like, you know, it's just, like, always there. Like, you can't drop it off your Marco 3. Like, if if it is present. So, anyway, just the thoughts. Like, we may want to have if we're doing rotation or trying to have tokens become invalid but have longer live keys, that may be a very special type of metadata compared to a thing that's like, oh, I just wanna reveal where user is or some other property about willing to buy that. We got started with this because it seemed like a very simple case, reducing it even further to, like, out of an epic seems totally fine. and it but a couple is, like, the notion of what an epic is to, like, the key rotation and management like like like like mechanism, whereas with this framing, if they they can kind of be separate, That doesn't seem like a problem to me. It's just, like, that's an artifact of coupling or reducing it. But"
  },
  {
    "startTime": "00:38:00",
    "text": "Yeah. I have no problems with that. Yeah. I just I just wanted to highlight as well. Like, generally, this extension patent seems to work for for a lot for what I think valuable use cases But depending, I guess, on how exotic you wanna go graphically, like, things like expiry can be dealt with in in slightly different ways in terms of news things like range proofs and and the likes, that means you don't have to even give, like, an hour or a day sort of expiration. Right? It it means you go into far more specific, which probably is counted as other other design goals, which is keeping cryptographic agility in play, but just wanted to sort of highlight that those options are are at least cryptographically possible. Yeah. For sure. We often are I guess we have the benefits of being able to just, like, slap on a new token type and put whatever fancy The we want underneath this. So we could do something like that as an experiment. but we're could also keep the the crypto simple if we wanted to do so. you know, Code points are great. We can explore new things. Yep. Is it? Thank you, Chris. I hope you got valuable feedback. in the interest of these procedural questions, I defer to you to make a decision as to whether or not we would seek adoption or or to follow-up when talk about it. Yeah. I think we'll need a little bit of follow-up, but Yeah. I I don't think we're going to try to answer those questions. right now in the working group, but we will definitely encourage discussion of these these documents, it's clear that there's significant interest and I didn't hear any you know, objection to the the at least trying to understand these using these documents as the basis for discussion. Right. So I I guess on that basis, I'm at, like, asking, like, let's consider adoption."
  },
  {
    "startTime": "00:40:04",
    "text": "Okay. I mean, I I will note there was some discussion in the group that suggested that these documents are not ready for adoption or or that there are some It's not clear that these lead in a direction that we're able to adopt. So think we've you know, we we'd encourage discussion and and, hopefully, we can resolve those questions and And then we would be able to run an adoption call. Okay. Fantastic. That works. Do you want me to share the slides where you go? Yeah. I I can I can provide a few shares. I think so. I can Okay. I think where you share your -- I there's a yes. There was a little bit of a race, but Okay. I think I can yes. You should be able to Yeah. team you should be able Okay. Hey, Ron. I'm Matt Finkle, and I'm here on behalf of the design team that has been discussing how we handle consistency within Darcy Bess. over the last 5 months. 4 months, 3 months? Something like that. We've the meeting, and we come up with, actually, 2 new drafts and one update to an existing In terms of what we've been trying to accomplish, the chairs put forth a request that we look at,"
  },
  {
    "startTime": "00:42:02",
    "text": "defining what key consistency or configuration consistency are, and and what a protocol like that would look like. identify at least one major use case for this group. concrete proposal and you know, actually be able to describe what assurances but So that's what we've done, particularly, we we looked at both applications privacy best, but also OHP. as a a use case for it as well. we we came up with some definitions in general, the important ones are the key identifier, what actually identifies key so that you can compare 2 identical or 2 keys that should be identical against each other. We have a client, which is the entity that's requesting the the key or or you know, trying to understand if it has a consistent view. And then you have the source, which is actually the entity that that creates the key or supplies the key. We also have 2 notions, 1 of consistency, and 1 of global consistency. And then we also considered inbound versus outbound. implementations or or protocols for assessing because consistency. In terms of consistency, basically, if if 2 clients have 2 sets of keys, and they're able to create some set identifier based on those keys then they both have a consistent view if their their sets are equal. taking a step back, have global consistency, which is basically the idea that if all clients have the same sets of keys, then consistent."
  },
  {
    "startTime": "00:44:01",
    "text": "some words. Alright. So, pulling this straight out of the the draft that discusses various protocols for obtaining or or checking consistency. there are are 3 main ones that that work on. gonna highlight here. So the the first one is just the the typical share of cash, which is basically just a caching proxy where clients go to the proxy, request some configuration or or key or or whatever, If the proxy doesn't already have that, it goes out touches it. and then returns it. And for any subsequent clients that request that same resource. The proxy just returns it. you trust the proxy, then then all of the clients that are using that proxy will have a consistent view. Similarly, basically, if if you just add more dimensions onto this. a client can do this multiple times with multiple proxies. And if they all return the same answer, then You're not. You're not. likelihood that you're being targeted is very small. Kind of the inverse is basically a a a essential database, this could be, you know, key transparency service or or or something that provides a a central and transparent and auditable mechanism where clients can fetch information from it, but servers provide it and if if you can audit the database and ensure that wasn't tampered with or modified, then everyone has consistent view. And here, we have a combination of the initial shared cache design, and also one night in show, which is a basically the the the the initial"
  },
  {
    "startTime": "00:46:00",
    "text": "shared cache, but using it only as a proxy to to directly batch. And this this actually should look familiar. And for those who have following this space for the last couple of meetings. This is very similar. to the double check design then proposed. And, essentially, it's using a a shared proxy with which goes out and and fetches and works exactly as the shared cache We talked about earlier. But in in addition to that, the shared proxy can also be used as a intermediary between the client and the server where basically, you the the client just requests the shared proxy to to connect to the server and return the response. This basically allows the client to verify that the proxy is returning the same answer as the server itself. this this is actually the And motivation for the good. design that we'll be covering today. So in in terms of the the 2 options, that we've been looking at. Yeah. Inbound and out advanced. quickly in band which is actually another graph that that was published out of the design team. idea, basically. Can we use the existing protocol to provide consistency. and and not add an additional protocol on top of privacy fast in order to achieve it. And so specifically for the split origin issuer model where you have it in a tester, can the client just delegate the responsibility to the attester to verify that it has has that that the"
  },
  {
    "startTime": "00:48:00",
    "text": "the the key is consistent, for 1st some definition there. In in in other words, for all clients that are using the same attester, If that attester is trusted, then it will return the same answer to all the clients. and, therefore, all those clients will have can by consistent here. The client itself is not verifying this. But if you trust the tester to provide Yep. it's, you know, to to to be responsible for it, then to this is a very simple mechanism for obtaining doesn't seem You know, is it something that we want to understand is is is this simple design, something about the working group is interested in. or or is is that delegation not sufficient. In terms of Adabands, we came up with a design called KCheck. which which is is obviously, very similar to double check. And it's basically a generalization and and a describes an http based protocol that can be used to check consistency. So it in As part of it, we have Amir Prokel, which is basically a a re reverse proxy where if you've got your resource from that service, from that server? then if It is in the cache, then it returns the response And if it's not in cash, then it goes out and touches the request, or the the resource and then cache it in terms of to the client. Something that's a little special is that the mirror is"
  },
  {
    "startTime": "00:50:01",
    "text": "needs to be configured with some sort of minimum mobility window so that a resource that is fetched is not is valid for at least some amount of time. so that the the resource is actually cached. and that any client that subsequently request it, at at least have a a chance of of getting that that same key or same configuration? And so we're leveraging the the cache control max age value just to make sure that the the resource is valid for some math on. And just quickly, basically, the the mechanism that we're using is a URI template with a a target variable where can be a a query parameter or or a path whatever the the deployment wants to use. But, basically, there's a host that accepts a a target, and and knows what to do with that target. and the the target is is a 8 percent encoded URL of the the resource that was gonna be touched. in terms of how the does does mirror behaves, basically, if at any point, the request fails or validation fails, Amir just responds to the client with a 400 error. mirrors, may have some restrictions on what protocols it supports, or or what. targets it is willing to go and fetch a resource from, and and so it It does some validation there. And if the resource is already in It's cash."
  },
  {
    "startTime": "00:52:02",
    "text": "then it encodes that capture response with binaries to the key. and also includes captain control header in the response. So the client also knows when or so. the when that response expires. if the resource isn't in the cache already, then it goes and it takes that target variable gets the URL from it, That's just it. validates the response, catches the response and response. in the same way as before. terms of validation, In the cache behaves in the normal way with, you know, very header, and it validates the cache control header making sure that's present, making sure that it's at least as large as the minability window, and make sure that it that cat on is appropriate. The for KCheck, we're not interested. currently in talking about discovery. we assume that clients have these mirrors already preconfigured. 1 or more know, depending on what model or level of validation, validation, or consistency that the client wants to do. and in terms of what goes into that protocol, from the client, you have the resource. for example, if the the client is doing about a a consistency check. based on the HTA offered Yeah. authorization header, then new, you already have a a key that that goes into this."
  },
  {
    "startTime": "00:54:01",
    "text": "you know which issuer that you wanna target. So you have the target value. and then you have the representation. And just continuing through this yes, we've as the client, if if you for each mirror that you have configured, you request this, but the resource from each mirror, and if it fails, then That mirror fails. And otherwise, when you get the response, you compute the the first valid response And we assume that there is a known way to normalize these responses. such that any fields that are deployment specific or or not recognized can be ignored. this allows clients and you know, all clients to normalize to some to some standard format format. Format depending on the the the protocol. And so once you get this, then the the client computed the representation, and compares that with the value that it it received initially from the origin and make sure that it was error. for the same And once you get to the end, if all the years succeed and get the same value, then You verified that you have consistency. Otherwise, You don't. And what the applicant does at that point is up to it. So Yep. And as I mentioned before, basically, normalization is important, you need to be able to have some configuration and synthesize that down into a you know, something that is comparable."
  },
  {
    "startTime": "00:56:00",
    "text": "and once you have that, then then you can actually and that. In terms of of open questions, It would be nice if clients could learn how many other clients are using that same key. and This it's not entirely clear how we accomplish that yet. But the idea of having an an an enemies set and understanding the be in in the preset size. is is important here. But, also, if you can get that information It's not clear what the client will do with it, once they have it, So yeah, this is still something that we're we're we're working from. privacy pass, this is just a a simple For example of how the flow might look, So the the client wants to to fetch the issue dictionary from some issuer dollar example. And so it creates this HP request. with these fields. And so the mirror gets to the target and decodes it. and because the the say this is the first client that's requested this this resource, Sameer goes out and and actually fetches it and then gets the response and caches that response. And then"
  },
  {
    "startTime": "00:58:01",
    "text": "When it gets the response, it then Yep. Yep. Yep. Okay. Yeah. So the the cost that goes out to the target. say it gets 200 response, welds some bytes that are the directory, and then the mirror validates the that max age value, it is within that's minimum value. as as then encodes using binary HTTP. and sends that response to be clients, And so in terms of what the client actually does with that information, this is an example of the the issue a dictionary. And, basically, the the client as in in the normal protocol. it selects the first key within that token keys dictionary. that is supported and that is is valid at this time. and this is what it uses to actually compare for consistency. And so if the origin sends a a key. and the key that the origin sent is not the first key within this dictionary then the consistency check profile. And in terms of where we are with implementations, we currently already have a mirroring patient, that's that's it."
  },
  {
    "startTime": "01:00:00",
    "text": "some some Questions? Questions? Watson Lad, EchoI Technologies. I'm a little fused by why you have the encoding of the entire response versus just passing. passing back the resource it has device that client's actually gonna look at. That's a great question. Chris Wood? Chris, it seemed pretty simple to just wrap up what the service sends back and shoot it back down to the client. If client needed to check headers or want to check headers who could do so with a particular mechanism, We initially, we had it as you suggest, just, like, take the body and then copy it down, but then Mark suggested Marconia suggested like, you just, like, pass the whole thing back. We have this cool binary HTTP thing that lets you do that pretty easily. and and That's why we're here. I don't know. That that may not be persuasive, but, like, that's the that's how we got here. Eric? Yeah. Eric, or with Google. I'm I noticed the I think the biggest difference between this and the earlier double check mechanism is that double check mechanism do a lot of work to try to get the correct version, the key, and handle the key roll over and all that fun stuff. by asking the version time stamp and stuff like that. And this seems to do know that is am am I understand correctly that the assumption now is that the research being fetcher trees, multiple keys, and just whatever protocols using this handles the key rollover within that and the clients all working out because the client is just checking consistency between is first valid key and what they receive from the mirror. Is that correct? Yes. Okay. So so yeah. I think I think the doc will need a little more language around must or they should send multiple keys and stuff like that. But, yeah, it's that's all stuff we can work out post adoption. So overall, I think this is pretty good. this"
  },
  {
    "startTime": "01:02:03",
    "text": "Hi. Ben Schwartz as individual. So so so Thanks for thanks for doing this. I'm very excited to see the proposal here. I have a bunch of questions like I could probably too many to discuss here. like, how do you think about authenticity in this model and see didn't see an authenticity story. Why is the clients accept request header not the type of the thing that it's getting back since the type of the thing that's getting back is message, be HTTP or whatever that that type is. and I mean, a lot a lot more questions. But So I'm interested in those in those topics. But, really, I wanna know where you how you wanna proceed with this? Where do you wanna have those kinds of conversations? Great question. we're continuing to work within the group. We have regular meetings. I feel like mailing list is probably a good place, but also I think within reason, happy to have people show up at our meetings and discuss it, discuss If if there are concerns that would be easier to resolve there. Okay. Thank you. we could also schedule interim or or something of that nature. to bring it into more IETFs. procedure. Alright. Great."
  },
  {
    "startTime": "01:04:02",
    "text": "Okay. Next up is Steven Valdez. k. Hi. So I'm Stephen Wallace from Google. I'll be talking briefly about price pass and related technology that's happening in the w three c. So so Okay. So, first, the main crux of this will be talking about private state tokens, is being worked on in with you and the antifraudcTV, and then just a brief overview of some of the other w three c groups that are interested in related technologies, and that might we go to we go to coron connect with to see where privacy has confidence that model or with our privacy pest technology that would be useful there. Susan, brief update about what pro state tokens is. It's a web API that is built on a varied of privacy paths. where different websites act as issuers, This is effectively in the privacy pass where they joined the tester issuer. construct. issuers on various first parties or whether embedded as third parties issue tokens based on some activity on that page. you've solved the captcha, you interacted with that page a lot generally, we've heard use cases for this in the invalid invalid traffic world and the anti fraud world. Then on other pages that those same parties are embedded, they redeem tokens and use the as an extra signal that can be used to that you've already, like, proven yourself before or you've already done some activities elsewhere. So that helps you make decisions about whether you should show a huge catch again or if you're more willing to let certain traffic happen. Yeah. So captchas are the big one, and there was various embedded bucked companies that Armeditda's third parties that"
  },
  {
    "startTime": "01:06:01",
    "text": "run various metrics to figure out, like, if you need to rate limit or if you need to like, you don't wanna be posting something that's coming from a data center that's doing, like, 1000 or millions of requests. So the history of State tokens. It was originally called Trust tokens in the which is the web incubation community group Chrome ran a origin trial, which is basically a website sign up to use feature, but it's not widely available. through May of 2022, Unfortunately, given the complexities of, like, you need to be able to run this across lots of websites to be able to, like, do the sorts of traffic analysis in IVT across multiple sites, the organ trail had limited, usability. Martin, Yeah. So In May, we landed a PST version 1, which we're hoping to have at a 100 stable, which is base which is which is hopefully gonna be a way for people to play with API in a wider setting we'll hopefully get more results from that. We're hoping that once we get more learnings out of that, we'll AB standard version, which is a version of this that will be more based on the standard primitives process. Hopefully, if it's gone through WG lost coal, and of the extensions that are currently undergoing adoption here, and we'll update the API to be built on top of that. Yeah. No. I I realize this is probably getting very close to discussions that we should be having in the w three c. ideally in a working group, by the way. Yeah. Thanks a lot. that last point got me up. there's I think at the moment, there were 2 major uses of privacy pass on the web. correct me if I'm wrong, Tommy. Apple has a thing that essentially does the privacy past protocol with websites under certain conditions."
  },
  {
    "startTime": "01:08:01",
    "text": "using unspecified, maybe through a blog post, I think you had some details. but unspecified bounds on on who can be an issuer in those context and a very specific attester and and and various other things. One of the things that I think we need to talk about as a as a larger community is the governance structures that exist around the creation and approval of the the issue at a tester blob that sits in the in the system because of the nature of the of problems that problems continually come up this context. because because if the signal is this person is not a bot. then maybe that's okay. Right? But if the signal is This is a minor. That's interesting. If the, you know, if the this is know, this is a poor person or this person has a particular colored skin, then we've got a real problem. Right? And so working through that is gonna be quite difficult because the the ways in which they're used on the web. does create signals that transfers between websites. in a way that would rather not have happened on the web. So there's that other thing is this is very much a w three c comment, but what's happened here is that there's a protocol being defined in the w 3 c spec. that probably isn't necessary. you're using fetch. to to to to past parameters around and some like, you mentioned structured headers. Yep. something that ideally would come to the IETF. but I don't think it makes sense to bring it to the IETF at all because it's a web API. So we we need to have a longer conversation about the shape of the API. Yeah. I think there are some later slides about where we're don't have any compelling reason where we were just making decisions and figuring out, like, what if that should happen in privacy pass versus so right now, it's only in a community group where, like, we're trying to get feedback"
  },
  {
    "startTime": "01:10:00",
    "text": "eventually, we'll need to migrate to a working group w three c for the actual standardization process. That's what I would like to see happen. can have them. But we I think we still need to, like, work out what the use case these the the use cases that we think these APIs gonna, like, actually happen. Like, people can, like -- Yeah. And that's that's that's the government governance question that I was I was talking about. Well, part of the governance question at least. Yeah. Mark Nottingham, just to follow-up on that. I agree we're very much with Martin about the governance issue there. That's one of the bigger problems that think we probably need to get ahead of rather than taking it on at the very end. But I think just to to dovetail with that, I would strongly encourage you to 2 consider those other use cases because I suspect they're going to have pretty big implications on the API surface area, and, you know, the user experience, which is of course, the very w three seething. My intro is in you know, there are a lot of jurisdictions around the world right now who are talking about each verification. very seriously. And it seems like privacy pass could be a way to address those requirements with a horrific privacy, you know, issues that are inherent in all the other solutions that are being proposed. Think one of the high level, like, helps with private stick tokens is that, like, is a generic solution, but a lot of, like, age assurance and things like that. Like, don't know if we need this generic API to do all of those or we, like, cut a specific version of this out in other parts of LB 3 c. to handle those specific use cases. Right. Right. I I I yeah. I tend to think that that those use cases need to be thought through, and then we can come decision because if we don't and it's just, well, this is the fraud know, we're just talking to the anti fraud folks. Well, that's all it will ever be. Mhmm. Mark, real quick, actually. I I've kind of a question for you given that you're an expert on this topic. I agree also with Martin. I agree with you. what do you think would be a good way to, like, start the discussion about that governance model"
  },
  {
    "startTime": "01:12:04",
    "text": "because it does seem to be a bit larger than this particular working group. What is it, like, an appropriate venue to have that discussion Is it here? Is it in the w three c? Is it you know, somewhere else. I Name, please? oh, Chris would Mark again. Nottingham That's a great question. I think it's it's maybe we should have some folks go away and have a chat about that because It depends on the incentives of the vendors. It depends on the incentives of the vendors. It depends on you know, what regulatory forces if any are gonna come into play. there are a lot of practicalities, but I think we need pressure from multiple directions to make that happen. Tommy Polly, Apple. So a comment regarding kind of the governance stuff. So first to clarify for the privacy tokens, if I recall correctly, this is still model versus the split, a tester issue model I think is relevant when we're analyzing kind of the the governance and how easy is it for people to enter this ecosystem, how easy is it to kind of audit the properties of it It's like in the split model, which is what we're currently operating with today, you'd the websites that are validating tokens are only ever exposed to what issuers, and they do not directly see what testers are being used So there there's this transitive relationship which, you know, right now, you know, Apple has an attester that is on mainly ones used, but I would really love to see a growing diversity of those And in order to expand that diversity, and make sure we're not kinda, like, locking onto certain things."
  },
  {
    "startTime": "01:14:00",
    "text": "all you need is some of, like, the major capture token issuers to come on board with that. So, you know, cloudflare or recapture other people could just, you know, accept. Okay. There are other platforms that do we at the station, and then they all kind of fold behind one mail of tokens, So Alright. Alright. I would encourage like, I I don't know if we could kind of make you know, suggestions from the working this working group to work going on in w3c, but if they are going to consider those aspects it may be beneficial to move to the split model, that's gonna be potentially easier to reason about some of these things. Yeah. I think one of the difficulties are at least with PST as Since we want a wide variety of, like, the different things you're testing to, that would require, like, lots of different detesters and like, the I guess I'm not sure how well the split model currently works with, like, having lots of the testers, I guess, Like, for private access tokens, like, how do new a test like, if you don't want to have, like, 10 testers how do the origins and issuers? Like, add those attesters to the ecosystem. The org Origins, like, websites have to do nothing at all. So all it would be is just, like, the issuer. So let's say you know, like, Cloudflare has their turn style little captcha embedded thing today. and that just validates keys that come from their issuer. and So, essentially, Alright. anything any attester that they trust from their issuer which can be doing device attestation or, like, did you solve another captcha? Did you you know, do a little song in dance. Like, any of those things could fold into that check and essentially, it's only the issuer that needs to Say, yep. Yep. This thing falls into what I am broadly presenting to origin as Yep. They passed whatever my check was. So it it allows it to be a broader,"
  },
  {
    "startTime": "01:16:05",
    "text": "combination of things. Yeah. Though, I guess, in the world where, like, the website wants ask multiple questions and would need different issuers for each of those. and, like, those each would need different to testers. No. You don't necessarily need different testers. But I think we yeah. more details to get into. But I I think this is potentially a point for a dialogue between the bodies are, like, you know, joint emails or something like that. That's it. Watson Lad, Akamai, I think part of the issue you're having here is using privacy pass in the first place for these nonvarying stations we really are looking for. is I'm gonna massacre the name Carkermoislicenscha sort of credentials where you have a sign thing and you do a proof over to sign thing, and there's also variations, and that would solve then you are not really relying on the privacy pass issuer thing for the privacy issues that's really done in the client. And this privacy pass was designed for rate limiting. It's not transfer of an arbitrary signal that's indefinite over time. And so I think that it would change your conversation even to different property. I think that's spoke anyway, moving there's no more question in the queue here. unless So briefly going over some of the differences, For private state tokens, we need some way of consistently fetching keys so that you doesn't get prevented presented different keys, the work being done through paycheck and the consistency stuff, I think, would be variable revolumolar. We find on the web that we wanna be able to redeem lots of tokens, and, currently, the default state date is you've got one token. The VLP reconstruction only has one token. Chris and the CloudSoft folks have a batch token spec that I think was discussed last idea"
  },
  {
    "startTime": "01:18:02",
    "text": "that will hopefully come up for adoption or figure out what the next steps there are. And then we do have some of the public metadata and tokens. Currently, it's like 1 of values. This is currently done by choice of keys. the public metadata issuance doc that was just discussed, I think, would be valuable there to move to something slightly cleaner in that model. In terms of places where we currently diverge from privacy pass, and it's no compression whether this needs to be dealt with in w 3 c here or elsewhere. Given the key consistency thing, since we want all our clients to be getting these keys, would be convenient if we had a, like, centralized way of, like, getting like a bunch of different like, resources at once rather than having to fetch each individual issuer in a world where we have multiplichers. This could be something in the KCheck protocol, for example, for batching. bunch of research requests or having some sort of push model where there's some sort of service that pushes updates to the client. We also have some compatible leaders with a key commit in format, Most of this is because we're using multiple keys for public metadata. So, hopefully, with the public metadata, draft, we can fix up some of these. But one thing here is that We currently have different expirations for different keys that are in a key commitment. And in the current graphs, it's really the whole key configuration as a whole. like, expires all at once. I think moving in either way reason when that space And then there's a dot well known debate where we're currently not using a dot well known think it can land either way if we move to that or unfortunately, some of the issues that we've had experimenting with it find it easier to both like, run a bunch of the infrastructure on 1 shared domain. but I think this is mostly age. implementation detail. On the website, as Martin brought up, We have things where instead of using the we don't we authenticate an HTTP authorization. We use fetch and HR sorts of client code to do the private state tokens operations,"
  },
  {
    "startTime": "01:20:02",
    "text": "This is mostly because we have a lot of preexisting like code that various the key companies and anti fraud companies have that, like, are already doing fetch requests or already doing SHR requests. that's easier to integrate into that rather than adding brand new steps in the latency pack. path where it has to do a brand new resource call, I think depending on where we see people use this API whether we need to keep these sorts of client initiated things. or can have both of them is an open question. redemption record. That's a way of keeping like I've redeemed our privacy pass token. And I want to keep this information around for a while on the web space, so this is just an extra storage layer. If we standardize that, that would probably be a w 3 thing thing thing thing thing thing and Yeah. The other thing is that for operations with and privacy process is done by a independent requests to the issuer that has all the heavily pass stuff in the body. Since we want to integrate it with existing, like, calls out to refresh APIs and things like that in existing IVT. Currently, we're storing it as an additional header in the an preexisting request. I think there's ways where they're using privacy test method and instead store all the data that we currently would put in the body post in, like, a header or other things like running multiple requests in parallel with the HTTP 2 world. I think that's something we'll need to align on. And, yeah, here are some open issues where we do diverge, and this is mostly on the private state tokens API to fix. There's a bunch of token format and key commitment format changes that we need to align with. And, yeah, the adding support for the asking. No talk. Tell me, Polly, Apple. Actually, on the last slide, So when we're talking about just adding this to the existing connections, which which existing connections would that So refer it to. using capitalizing Namp."
  },
  {
    "startTime": "01:22:04",
    "text": "example, when you, like, submit the capture, like, there's already a, like, endpoint that it hits with the like, capture solution, And or choose the w w authenticate, you either need to have 2 parallel requests both for the case where, like, The privacy pass stuff won't work. or you'll need to, like, able to, like, integrate 2 of those to request the privacy pass request and the, like, capture challenge requests together in 1 e, p. Okay. So just to tease apart, you know, one of the differences. And partly, this is related to split versus joint at the station, but Yeah. At at least in the implementations that we currently do for private CPaaS, since there is, you know, some use of, like, device level accessation, and it depends what you're pulling into here in the attestation, like like like that's not available to the browser. And, like, there's other things on the platform they're having for this. So I I think when we're designing this you know, particularly if, like, PST is saying, like, this is always part of this flow then you're really tying it to something like, like, that the issuance must be happening within like, the web browser process itself as opposed to calling out to potentially some other system service. So that really does change what capabilities if you are forcing them to be the same as opposed to saying that you know you it's a separate potentially separate connection. you imagine, like, if I am in the case where I it's on the same origin, and I happen to be implementing it with my browser I can send them out at the same time as 2 different HTTP requests, on h the same two connection. Yeah. There is various ways to, like, these into the shape to use, like, the existing privacy pass 1 fit I think it's a question of trying to minimize at least is in the fermentation phase, minimize the burden of, like, rewriting whole stacks at currently, like, already rely on existing xhrfetch stuff."
  },
  {
    "startTime": "01:24:04",
    "text": "And, yeah, definitely, like I think that that is one of the differences between Pat and this of, like, In the attests slower test for model, the attestser can, like, have this in additional information, outside of the web layer. Well, error. Really, all the information is coming from the web layer. Yeah. Rafael Robert from Phoenix R&D. A couple of slides back. You mentioned the batch token draft, so that would be me. 6 presented it in Yokohama, said I would bring it to the lists for postproduction. I didn't do that. don't have a particularly good excuse for it. I could say I was really busy with Mimi and MLS. But, yeah, it just lifted up. So I'll I'll I'll do it. Okay. Right. And then finally, w three c and some of the other groups that are doing work there. There's the moving on to the or some of this work is happening, and various other related APIs are coming up with ways of in a privacy preserving way talk about attributes, There's Cutsigie, which has various advertising APIs that are have some concerns about how to authenticate and maybe some sort of privacy preserving token or authentication thing would work there. privacy CV has general privacy APIs, and the web often in web payments folks have also been interested in these sorts of technologies. it's not clear, like, where they will overlap, but there's been a few discussions there. might be worth having more with CE. to see how speed work. some of the That's all for my slice. Thanks. Chris Patton. Quick question about the batched issuance. is that agnostic to the issuance protocol? Like a okay. So you can only use it with a pure VoIPRF or r or RSA. Okay. Okay. That kinda sucks. For the for the minutes, the answer was it batch tokens can only be used with VOPRF."
  },
  {
    "startTime": "01:26:06",
    "text": "Okay. We've gotten through our whole agenda. Last chance to say anything at private basket. IITF 117. Okay. Seeing none. Thanks again to Phil Holland Baker for minutes. And See you all in the next session."
  }
]
