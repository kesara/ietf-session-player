[
  {
    "startTime": "00:00:24",
    "text": "um all right everybody we'll just give it a couple of minutes just see people are still joining"
  },
  {
    "startTime": "00:03:36",
    "text": "hello"
  },
  {
    "startTime": "00:04:12",
    "text": "all right uh it looks like the rate of people uh joining the room has slowed down so uh welcome everybody can i just check that you can all see and hear me okay all right uh in that case uh welcome everybody to the second of the uh applied networking research workshop uh sessions for 2022 my name is colin perkins from the university of glasgow i'm pleased to be chairing this special session on protocol specification techniques so i'd like to begin with the the usual reminders um the applied networking research workshop is organized uh by acm sig karma by the irtf and it's co-located with the ietf meeting as you all know and as a result that means we we follow the itf's intellectual disclosure intellectual property rights disclosure rules so by uh participating in this meeting you follow the rules which i'm sure you're also familiar with uh um you you agree to follow the rules which are all familiar which i hope you are all familiar with uh and that's a if you're speaking at the microphones um and you you know of any uh patents or an intellectual property that covers uh covers the topic you're discussing then you agree to to disclose that uh the the links on the slide point to the the detailed policies there in uh addition um this session is being"
  },
  {
    "startTime": "00:06:00",
    "text": "recorded uh and will be uh available online on youtube after the session and if you speak at microphones in the room if you um join during the session remotely then you you agree to being recorded and you agree to that recording being made publicly available uh in addition if you're participating in person and you're not wearing one of the red uh do not photograph lanyards then you uh consent to uh appearing in the recordings and in the um the the photographs of the session uh and uh online participants if you unmute your microphone and turn on your camera again you may be recorded and the recording may be published and as i say this recording is being published it is being recorded and i believe the closer being live streamed any uh personal information you give to the ietf to the irtf will be handled in accordance with the privacy policy and there's a link to that on the slide that mostly mostly applies to your registration details and by participating in the the irtf you agree to work respectfully with the other participants and you agree to follow the code of conduct if you have any questions or concerns about that please either contact me or contact the ombuds team and the link with the contact details for the ombuds team is on the slide and a final reminder before we start because of um the the risk of covet as a safety measure we are requiring all in-person participants in this meeting and in the other itf controlled areas of the meeting venue to where n95 uh or equivalent masks um at all at"
  },
  {
    "startTime": "00:08:00",
    "text": "all times uh what while they're in this meeting or in the the itf controlled areas the only exception for that is for the um the speakers in the session who are actively giving the talks at the front microphone if you're making a comment or asking a question from the floor microphone we do ask that you remain uh masked at all times let me ask you remain masked while you are in the meeting at all times okay so this is the applied networking research workshop the um goal of the a rw is to provide a forum for researchers vendors network operators and the standards community to to talk about uh to discuss uh interesting new results in applied networking research um one of the goals is to try and find inspiration uh from the topics and and the problems being discussed in the itf and the other is to try and connect the research community with um the iatf and to help bring new ideas into the itf community into the standards community the goal is to encourage this two-way exchange of ideas the a rw is very much there to encourage hopefully the submission of new new research results that could form the basis of engineering work in the itf uh but also to to help better specify internet protocols possibly change operational practices possibly uh influence future research and experimentation in the ietf and the irtf one of the"
  },
  {
    "startTime": "00:10:00",
    "text": "key areas there i think is one of the key goals for the workshop is to think about how we can better specify internet protocols and a lot of this has been in the form of ideas for new protocols themselves i mean as as we saw in in the talks this morning uh as we've seen it in the previous what versions of this workshop um we've had a lot of talks uh about how to to specify new protocols about new protocol designs improvements to protocols and so on part of the discussion though i i think is how it's not what new protocols we might need or how to improve protocols but how to specify protocols something we we haven't discussed so much in the a and rw uh which i think is one of the goals of this session is to try and start to think about how we describe and specify protocols um how we can you know think about how we can specify uh how we can ensure that the protocols we're specifying are consistent and correct and how we can verify the specific specifications we are writing as researchers or as engineers in the ietf are correct and how we can validate if the implementations match those specifications i think one of the key goals of writing a specification is to communicate and communicate in a way that we know that what is written in the spec is correct and that we can check that the implementations match that specification check that you know the specification is self-consistent the implementations match the specification and everything should interwork the way we have historically done that in the ietf community in the standardization community has been though to use natural language"
  },
  {
    "startTime": "00:12:02",
    "text": "we write rfcs in english and we use english prose to describe and specify the protocols and yes we occasionally use formal language right we've been using abnf for example for for longer and i've been involved in the itf which is far too many years more recently we've started using yang in a lot of specifications and there's a number of other languages of scattered around around the rfc series but predominantly we specify rfcs in english what i'm hoping we can start to do in the session is to start to discuss whether this is the right way of writing protocol specifications i'll start to discuss to what extent formal methods structured specification languages natural language processing or other techniques can help to describe network protocols the goal is very much not to think about how we change think about how rfcs are prepared right it was um what what i i hope was a light-hearted comment in the the chat earlier about is this about whether we use markdown or xml for for writing rfcs right the goal of today is not to start a conversation about the rfc series the goal is to start a conversation about how we specify protocols in general and whether english is the right way of writing these protocols to what extent formal methods or other techniques should be incorporated i'd like to use this session hopefully to start a conversation so that's a conversation about how we specify protocols and hopefully to begin to connect um academic researchers who are studying protocol specification"
  },
  {
    "startTime": "00:14:01",
    "text": "with the engineers in the itf the researchers in the itf and the irtf that specify protocol standards and design and develop new protocols so uh our agenda for today um we have we just finished the the introduction status update um and then we we have three invited talks first talk will be by max von hippel from northeastern university we'll be talking about um automated attack synthesis by extracting finite state machines from critical specifications uh that will be followed by a talk from jane yen from usc i'll be talking about some work they've been doing on tools for disambiguating rfcs and then finally chris wood from cloudflare will be um talking about some of the work that's been happening in the crypto forum research group and some of the the successes and limitations of the way that group has been um specifying cryptographic standards and then at the end we have a i hope some some time for discussion uh before we close um there is a note that there's a hedgedock for notes the link is on the slide here um if you have questions or comments either put them in the chat or put them in in the hedge dock i guess probably better to put them in in the chat there's also a session in the hedge docker in the notes if you would like us to get in touch about any of this work uh if you're interested in following up on any of these ideas then please put your name in there and we'll try and follow up afterwards and we'll talk more about this in the discussion session at the end so for that uh if max is present in the room uh"
  },
  {
    "startTime": "00:16:00",
    "text": "he could come up and i'd like to to hand over to him for the first talk right i'm curious i'm ready to go should i use this microphone or the other ones stay anywhere okay all right all right and okay and uh colin can you give me control over the slides yep i'm just uh trying to find the right button in me took got it all right thank you very much you got it okay over to you all right my name is max von hippel i'm a fourth year phd student at northeastern university and today i'll be talking to you about automated attack synthesis by extracting finite state machines from rfcs from protocol specification documents this was a collaborative work with maria lenoir pacheco from purdue who recently graduated and is now a professor at university of colorado boulder as well as her advisor dan goldwater at purdue another student from my lab ben weintraub and our advisor christina nieto retaro and this work was presented also at the ieee security and privacy conference so today we're going to be talking or at least i'm going to be talking about automated protocol analysis this is old news to those of you who who build protocols so everybody in this room but uh but the internet runs on protocols such as tcp udp etc and um these protocols create a sort of shared language for computers to talk to one another here we have picture the internet or specifically the arpanet in december 1969 at the time there were four computers talking to one another the internet has grown considerably since that picture was drawn i'm a formal methods researcher and in formal methods we call these things"
  },
  {
    "startTime": "00:18:00",
    "text": "crippy structures but to normal software engineers protocols are described by finite state machines and a finite state machine is actually a completely unambiguous specification of the language of a protocol and essentially when i say that a protocol makes us that two computers speak the same language i'm saying something both informal and formal the formal statement is that they run either the same state machine or state machines that accept the same language the informal one is that they know how to talk to one another in our group at northeastern we specialize in studying these finite state machines either using formal methods or other approaches and trying to find flaws in them so we take a protocol the protocol we define a protocol to be whatever is in these rfc we then formalize it using a finite state machine that we write in some language like pramela or upol or something like this and then we have a tool that generates attacks against that that protocol expresses an fsm so there's kind of a lot of hidden steps in between there right going from specification to fsm to attacks and i'd like to disambiguate that a little bit when we talk about protocols we first and foremost have to talk about rfc specifications right which is obviously especially salient in this context these are written by the ietf the written in english prose which i think is is the topic of today's conversation um and although they they may seem technical to somebody who doesn't write these documents for a living the reality is that they really could be quite a bit more technical than they are right um so for example they could be written in or agda they could be written in a in a programming language if we wanted them to um and there are advantages and disadvantages to their current presentation then a programmer reads these rfc specifications and implements them so on the left we have the uh well a tiny little screenshot of the dccp rfc and on the right we have a tiny little screenshot of dc"
  },
  {
    "startTime": "00:20:00",
    "text": "dccp implemented in the linux kernel and there are all sorts of decisions that the implementer has to make when implementing uh the the rfc um and that doesn't just include decisions about how to implement some functionality like oh this list needs to be sorted will i use you know merge sort or or bubble sort but also decisions about what it is that the specification actually says in the first place because since it's written in english that's not inherently clear so when the implementer reads the rfc specification and decides what she is going to implement um implicitly she's creating a mental model in her head and to me as a formal methods person i view that mental model as a finite state machine but it doesn't really matter you could view it as any isomorphic thing the point is that obviously a human being reading english text is going to to imagine what that text is supposed to communicate and that thing that that she or he imagines is some expression abstractly of what they intend to implement they then go and implement it and hopefully there's no errors in that process either so bugs in a protocol implementation can arise from any one of the steps in this process first of all there could be fundamental design flaws in the specification it could be that we specified something that was bad right it was a bad idea and the idea needs to be reworked it's also entirely possible that the mental model the finite state machine is wrong in other words that the programmer or implementer made some mistake when she read the specification and misinterpreted something now we could blame that on the programmer or blame it on the author it doesn't really matter but mathematically there could be a discrepancy between the finite state machine that the programmer intends to implement and the one that was abstractly described by the protocol text by the rfc and then finally it's entirely possible that the specification is great and the finite state machine that the programmer imagines is great but the programmer has"
  },
  {
    "startTime": "00:22:00",
    "text": "a typo or something and so there's some bug in implementation which was unintended right so the programmer writes something that does not do what she intended to do this last class of bug is an extremely well-studied problem and there are a ton of really great techniques for approaching it and it does not interest me today so we will be talking about the other two so this presentation is going to focus on fundamental issues with protocol design and ambiguities and omissions in the specification the first half of this presentation will be about how we can automatically extract a finite state machine from a protocol specification so i'm going to take an rfc document and i'm going to automatically guess or extract to find a state machine that should represent that rfc this is sort of like automating the first step that the implementer does when she reads the specification and tries to decide what it says before she goes and implements it in rust or go or whatever the second half will be doing something useful with that so in order to measure that the extracted final state machine is actually good for something i'm going to do something with it and by something i mean attack synthesis i'm going to automatically generate attacks if you're in this room and you're like well that sounds cool but i don't care about security um i'll tell you right now you can view these attacks as bugs so you could also view it as i'm going to automatically find bugs so this is relevant whether or not you care about security so let's focus on the first part first and then we'll move on to the second part when we try to extract a finite state machine from a request for comments document there are a number of critical challenges that we faced first and foremost there's simply no one-to-one mapping between the text and what we call this canonical finite state machine in fact it takes human expertise to read a rfc and interpret what it is that it says or should say for a number of reasons that we'll get into second and related to the first point rfcs contain omissions mistakes and ambiguities here's an example from the dccp rfc where a sentence if interpreted mathematically would imply the existence of a transition from part open to open upon receipt of a dccp closed packet but"
  },
  {
    "startTime": "00:24:02",
    "text": "of course that would be nonsensical if you receive a dccp closed packet you should close your connection not go to open and so um uh if you read the rest of the document it becomes clear that that transition should not exist but if you read this sentence and interpret it literally it clearly implies the transition that isn't there and i will tell you from now years of experience that rfc documents are riddled with sentences like these another problem is that off-the-shelf natural language processing tools are simply not suitable generally speaking they are trained on newswire text or books or things like this that are non-technical in nature and so they face the exact same problem that i faced when i tried to describe my research to my girlfriend who's not a programmer which is that i use all sorts of lingo that are kind of domain specific and and don't mean anything to people in other fields like in her case biology so um we need something that's trained on relevant text such as computer networking textbooks or stack page stack exchange stack overflow questions etc finally there's a ton of variation in the language and structure of different rfcs and this is one of the i think interesting results we'll show today is that spoiler alert the dccp rfc is more complicated than tcp one at least in terms of its english text so we have a multi-step approach the first step is that we create a technical language embedding um essentially what we're doing here is that rather than impose our own concept of what the most natural vector space is for the text that describes an rfc to live in we are allowing a machine learning model to come up with an ideal vector space that it wants to learn from um that embedding is going to capture all sorts of interesting things about the text such as uh the the punctuation of sentences or the use of mathematical symbols etc second we have a zero shot protocol information extraction that's where we're going to take the text that is now projected into the vector space of our technical language embedding and we're going to automatically extract an intermediary representation which i'll get into more later which contains sufficiently much information for us to"
  },
  {
    "startTime": "00:26:01",
    "text": "then in a subsequent step extract a finite state machine so i want to stress this point that we don't do end to end in one step from english to fsm we break it down into smaller sub-problems and we essentially create a structured intermediary representation which is an xml document that has enough data for us to then in a third step extract the finite state machine in a rather heuristic manner finally we do something useful with that fsm in our cases attack synthesis i'd like to stress that you can do other things if you care about other things so if you were like well i don't care about attacks i'm not a security person but i care about bugs i care about correctness monitors whatever well boy do i have other tools for you there are other things you can do with these but what we did was attack synthesis because uh frankly my dissertation is going to be about attack synthesis so i applied attack synthesis let's talk about step one in step one we want to learn a technical language embedding as i said i don't want to impose my own personal moral ethical beliefs about uh the natural vector space of rfc documents rather i'm going to let the computer decide this we use something called burt the idea of bert loosely speaking is that it takes a contextualized approach to understanding what words mean so the word reset could mean something different in different contexts because it is informed by the words around it we train on roughly 9000 documents and roughly 475 million words uh so our bert model is trained on quite a lot of data that includes um stack exchange stack overflow networking textbooks um all sorts of other things next we want to do protocol information extraction so just to remind you in case you missed it um i now have some text that is projected into my vector space and i want to translate it into an xml document i'll show you what that looks like momentarily which somehow captures um enough data for me to then be able to extract a finite state machine in a straightforward manner so i i essentially want to read the text so put differently i want to teach a computer how to read and interpret the english language rfc document now that it has it in its own natural language which is the"
  },
  {
    "startTime": "00:28:02",
    "text": "the burton coding so here's a little snapshot from the dccp rfc and i'm going to show you how we would uh put this into a intermediary representation first we need to know what it means for the computer to read the document so to do this we invent a grammar shown here in broadcast now our form the grammar contains a few different types of tags it has definition tags that are used to define states events etc reference tags that reference previously defined states transition events etc state machine tags that tag essentially the structure of the finite state machine in the document any references to transitions or other logic that relate to the state machine and then finally control flow tags that essentially uh capture logical structure in the fsm so when you read these documents a lot of times they have kind of complicated if else structure uh often that's related to the indentation of different blocks of text and um our nlp is going to capture that using these control flow tags so here's that same quote again but this time it's now wrapped in our intermediary representation structure in our xml we have a control block on the outside that scopes our search we have a transition block that contains an entire state machine transition including the source state and the target state and then we have an event and we know what exactly uh occurs on that event so from the perspective of a formal methods person i now have the source state the target state and the label on the transition in my cryptic structure but from the perspective of a software engineer i have you know an edge in the directed graph that is my final state machine in order to go from text to this intermediary representation without requiring a bunch of grad students to spend countless hours annotating things we use a machine learning approach uh in fact we use two different machine learning approaches we compare them the first one is a linear conditional random fields model which starts out by splitting text into chunks it then extracts features uh"
  },
  {
    "startTime": "00:30:01",
    "text": "from the the embedding that that the chunks are in and then feeds those into a linear conditional random field linear condition conditional random fields model which builds on the markovian assumption that the next state is predicated only on the previous one we also experiment with a neural analog to this model which is kind of morally the same but it uses a neural network instead of the the linear classifier so we evaluate these against each other and also in contrast to a completely rule-based heuristic program which we don't expect to do very well because this doesn't learn anything um the rule-based system uh in fact performs poorly as we expect achieving uh below 35 token level accuracy and and similarly bad span level accuracy uh in contrast the linear crf and neural crf models perform quite a bit better with the neural one slightly outperforming the linear one and you'll actually see in almost all the metrics we showed today that the neural model slightly outperforms the linear model which is another interesting result so now that we've gone from text to embedding to intermediary representation we now have these rfc documents that are expressed in xml like this you can imagine big long document entirely structured with xml tags that say what's happening where and now i need to extract to find a state machine from that and i want to be very clear this itself is not a finite state machine a finite state machine would consist of things like this actual transitions right i need to interpret the xml and extract the fsm so how am i going to do that pretty much in the most boring possible way i'm going to use an entirely heuristic algorithm that exploits the very structured nature of the xml to extract the fsm um and uh happy to talk about this more but if you want to improve on our work a good place to start would be on the fsm extraction algorithm because at the moment we're not using machine learning for that step and i think a fruitful"
  },
  {
    "startTime": "00:32:00",
    "text": "thing to do would be to try that once we've extracted the fsm uh we want to know how good is it right i mean it's it's probably not exactly what was written in the text so so how close is it so we compare against one that i wrote by hand and to get back to the point about why all this is necessary i will tell you it took quite a long time to write that by hand because there are all sorts of subtleties when you get into a model checking environment of the ordering of events and the semantics of the channels that connect uh to two processes that are not captured by just the diagram that you find in your average rfc document um so we expect to have 20 transitions in our tcp fsm we also hand annotate or hand create an xml for the tcp rfc and we call that our gold xml and then our fsm from that is the gold fsm so that one should have perfect annotations and any bugs are related to either ambiguities and text or problems with our extraction algorithm um so comparing these we find that the gold fsm does uh quite well we define a transition as being approximately correct if his source state target state are both correct and at least one of the events on the label is correct but it might have like the wrong order of events like um say you maybe receive scene and then send back instead of the opposite or something um and and in the case of gold we find slightly fewer transitions than we expect to but almost all the ones we get are are approximately correct with the linear neural models there's quite a bit of noise so although we find uh most of the transitions we expect to find we also get a bunch of incorrect transitions so comparing to our skyline we find that another interesting result here is that in the very particular case of tcp the neural and linear models extract different intermediary representations but actually all that is modulated away by the finite state machine extraction algorithm so any differences in intermediary representations do not matter in this particular case in terms of the extracted fsm although this"
  },
  {
    "startTime": "00:34:00",
    "text": "doesn't generalize looking at dccp we find uh deceptively more transitions than we did with tcp but we expect substantially more so fractionally less are found we expect to have 34 total transitions with the gold we only get 17. as you can see the linear and neural models do not perform as well as gold but again neural slightly outperforms linear okay so um let's talk about kind of why we miss things because as you just saw in these graphs we missed a substantial number of transitions so there's two problems one is missing transitions the other is incorrect transitions and we'll start with missing transitions here's an example from the dccp rfc for a transition going from close weight to last stack in which you send a fee this text contains mention of the message that was sent to the fiend and the state that you started in the close weight but it does not mention the state that you go to last stack to find that you need to search like i think four paragraphs up in the document and find something indented back three lines and it would implicitly tell you that it was last act but there's really no way that a computer could figure that out and even a human being like myself reading this would have to search around for a while to try to figure it out um also if we wrote a rule to tell the computer to look that far back it would cause problems when we parse other parts of the document so this is a good example of why you can have missing transitions when you extract from an rfc so we do an evaluation and try to figure out why it is that we miss transitions and we we do this by actually manually inspecting every single missing transition we find that in the case of tcp um most of the missing transitions are due to omissions or ambiguities in the text the thing i just showed you i would consider to be an omission or ambiguity i think it's ambiguous because you have to search in other parts of the document that are pretty far away paragraphs away to figure out where the transition's supposed to go"
  },
  {
    "startTime": "00:36:00",
    "text": "however there's also a number of extraction errors for tcp in contrast with dccp we also get some prediction errors which is interesting but that might just be a scaling aspect because there are so many more transitions in dccp so who knows if you guys added like 20 more transitions to tcp maybe i would get some prediction errors with tcp i'm not really sure uh it should be noted we missed very few for tcp and additionally this result with gold implies that the dccp specification is substantially more ambiguous than the tcp one because the uh fraction of of missing transitions that are due to omissions or ambiguity is substantially larger than the fraction that's due to extraction errors in contrast to the same analysis with the gold tcp uh uh fsm right so uh in both these cases we're using our skyline intermediary representation so any problems are at the extraction level or in the spec and again the the neural model is is slightly better next we look at incorrect transitions so i've told you about missing transitions and i've said that essentially they can happen for multiple reasons but they mostly happen because of omissions or ambiguities in the text another question would be what about when we make a mistake when we extract a transition that's just completely off it has the wrong start state or the wrong target state both um or none of the correct events on the transition uh in the case of tcp they're primarily caused by prediction errors so this would be something we can improve on right the prediction errors are are part of our and their their problems with our methodology so our methodology can be improved to not have so many prediction errors uh potentially even just by training on more rfcs to be honest um in dccp however looking at the gold we find that a substantial number of incorrectly extracted transitions are due to omissions or ambiguities in the text and so this again highlights that the dccp rfc uh"
  },
  {
    "startTime": "00:38:01",
    "text": "experimentally appears to be more ambiguous than the tcp1 along numerous metrics that we can measure we underperform on dccp in terms of what how many transitions we can extract um and and conversely how many errors we have and again this suggests that dccp is more complex okay so now i've told you uh end to end that we have a way to automatically read an rfc document and extract a finite state machine and i've shown you that the final state machine has a number of errors including missing transitions and also transitions that are wrong and i'm going to claim perhaps scandalously that despite these errors it is good enough to do something useful uh to me that useful thing is going to be attack synthesis so um that means the automatic generation of attacks against the protocol but you could also view this as the automatic uh discovery of potential bugs so uh i used an attack synthesis tool called korg which i created which is why i used it uh i presented korg at safe comp 2020 which was in lisbon portugal but i was over the internet because of covet unfortunately so hopefully next time we'll get to go to lisbon um cork takes his input and extracted finite state machine like the one we just generated uh a linear temporal logic property this is just a formalization of what it means for the state machine to be correct so an example of a property would be that there's no active close in dccp so i could have a property saying you never have both peers closing at once it then creates a system in which two pures are communicating over a vulnerable channel so it injects a vulnerability into the channel between them a man in the middle and it asks if in that threat model uh the property could be violated so we're not asking if the protocol's correct on its own we kind of assume it is in fact we don't even look at properties unless uh they're satisfied by the prop the model"
  },
  {
    "startTime": "00:40:00",
    "text": "in isolation but we want to know under attack is this still satisfied the spin model checker reduces this to a language emptiness problem uh by intersection of a book automaton with the system but basically it does fancy formal methods from the 80s and it says no the system is not correct when it's under attack here's a counterexample which is just a sequence of events that the system goes through in which the property's violated finally cork takes that counter-example and turns it into an attacker program that simply replays that attack so it's a pretty simple approach we looked at two case studies tcp and dccp uh the fsms shown here are made by me and have been extensively model checked for the number of properties and shown to satisfy all of them so these can be viewed as perhaps more canonical than the fsms that you'll find in the rfc documents or in any number of security papers where they do maybe graph guided search but they don't really care that they satisfy formal mathematical properties for tcp we use four properties the first three came from my paper when i presented the korg tool where i also used tcp as a case study and the fourth was written by ben weintraub all four though are written based off a close reading of the tcp rfc likewise we also have four properties for dccp i don't think these are incredibly interesting properties so i'll kind of skip them but i'm happy to talk about them afterward if people have questions we generate a number of attacks this is kind of a confusing table so i'm going to try to break it down into sub points essentially with tcp there's only one property with which we are successfully able to generate attacks using any of our extracted finite state machines we generate very few attacks with tcp but all of the attacks work meaning all of them are confirmed on my handwritten model so there's no false positives in contrast with dccp we're able to generate attacks using all four properties um however we get some false positives so for example with the third property i wrote for dccp using the fsm that was extracted with our linear conditional random fields model we"
  },
  {
    "startTime": "00:42:00",
    "text": "generate 13 attacks and all 13 are false positives so uh again there's sort of a noise to signal ratio thing here where i'm generating more attacks with dccp but there's more errors i generate very few with tcp but there's zero errors again to me this seems to indicate more ambiguity or more complexity in the dccp rfc because errors in the attack synthesis step are caused by errors in the finite state machine so this is fairly predictable for the results i showed you up until this point okay when i say the word attack it can mean any number of things and people use this word rather liberally so i want to give some examples what i mean by an attack so you know what exactly i'm saying one attack would be to inject an act to the first peer and a tcp active passive establishment routine in order to if i remember correctly get that pure stuck and seen received because it can non-deterministically decide to exit but if it doesn't decide to exit for example if it's configured to have a very long timeout time then it could stay there forever um or essentially forever uh they may sound a little silly but if you talk to samuel giro at lincoln labs he'll tell you that he's found tcp implementations in the wild to have this bug and in fact i found stack exchange queries even experts exchange queries from people who had this problem where their tcp connections hung forever because they'd configured the timeout incorrectly another example which is much more complicated which we also find would be to elaborately spoof each peer and a dccp connection in order to guide the other one into close rack this is sort of a silly attack because i'm not really sure what like an elite russian hacker would want with this however it is a violation of a correctness property because there's no active active close in dccp so if i can induce an active active close maybe i can cause a deadlock or something and i want to point out this interesting point that because our approach is based on model checking we'll find many many variants on kind of the same attack and um so indeed we find many variants on this attack and i can show you dozens of different ways to essentially do the same thing"
  },
  {
    "startTime": "00:44:01",
    "text": "i want to conclude with future directions so um the research i showed you today was presented at uh s p to a security audience and um uh you know there's kind of a certain set of things that interest them in terms of breaking protocols but i think for you you may have different interests in terms of maybe building protocols and won't be broken from the first uh first place so it'd be nice if we could automatically highlight omissions and ambiguities in the rfc text and i think that we're part of the way there if we can kind of take our tool and make it say i'm kind of confused about how to interpret this piece of text could you clarify it that might be a useful functionality for rfc authors to be able to know when something they wrote is not entirely clear another thing i'd like to do is automatically suggest bug fixes so if i extract the finance state machine and the rfc author looks at it and says yeah actually that looks correct i think that's what i intended to describe and then i find an attack against it well it'd be nice if i could use some uh synthesis techniques from the formal methods community to automatically suggest a patch that would resolve that problem and we actually have an undergrad working on that problem with us right now which is very exciting um a completely different kind of vein of research that we'd like to undergo is to extract logical properties right now i'm writing those properties from hand that's not easy writing properties from hand is maybe not quite as hard as writing models by hand but they're both much more difficult than they seem uh and it's very easy to write a property that for example is vacuously true and then you really can't do anything with it um so there's a lot of logical subtleties in these things we'd like to offer support for secure protocols there's a lot of kind of interesting problems to this like capturing the arithmetic that goes into the cryptography and such a protocol uh capturing secrecy requirements and we might have to take a different approach in terms of the back end for example maybe relying on pro verif or tamarin instead of uh quark um i think that finally it would be really nice if we could take all of these different ideas and turn them into some sort of cohesive software suite for an rfc author in the loop so an rfc author or authors are writing an rfc it would"
  },
  {
    "startTime": "00:46:01",
    "text": "be nice if they could run our code and be told hey this seems ambiguous maybe you could clarify this uh and and they could kind of have a feedback loop to do so until they automatically get an unambiguous piece of text uh and you could even imagine um perhaps mixing formal language with the english to make it so that if the rfc author actually likes the way they describe something in english they could just give little formal hints about how to change it and that's it with that i'll take any questions thank you very much [Applause] it's okay okay all right thank you very much excellent talk uh i see a couple of people in the queue um who's first uh ishii nishida web service so i think this is very interesting research thank you so much and then i this might be a part of a huge direction but uh one of the complicated things for tcp is congestion control and loss recovery yeah and then i'm wondering if we you know you can apply this technology to the congestion control gross recovery and how it's promising that's what i'm curious about yeah that's a fantastic question um my current project is congestion control uh however um i have not been applying nlp to the problem i've been manually modeling a congestion control algorithm of interest i think that congestion control is pretty complicated and it would be fantastic if we could partially automate the modeling um but i'm trying to get an intuition for the modeling to begin with by doing it by hand before we consider automating it but i mean you basically predicted the next year in my life so yeah that's that's that's what i'm doing thanks so much thank you"
  },
  {
    "startTime": "00:48:04",
    "text": "all right thank you uh michael um you said you use um you analyze the text to get the finite state machines um [Music] most of the transport rfcs contain a graphical representation of the finite state machine so [Music] did you also consider using that or is that too hard or too weak or that's a great question so uh i did write a little toy script that could read um the the ascii diagrams and rfcs and got it to work on quite a few of them but there was nothing very scientific about it i just kind of wrote a little heuristic python script i've spoken to maria about this and apparently maria did the natural language processing for the most part on this project i did all the fm maria says that there is kind of a field of nlp that deals specifically with diagrams and figures and it could be applied to this type of problem which is an interesting approach i will tell you however that the fsm diagrams are missing a bunch of information for example if i recall correctly i think that the tcp fsm diagram is missing a core component of the active active close routine and in the dccp one i think that there was again oh you know what i think in dccp there's a transition that's shown in the diagram but not in the text there's a bunch of places where the diagrams are missing things there's also this problem when protocols have multiple diagrams that have to be somehow merged like if you look at ltp which is a pretty interesting protocol that's used in space exploration it has i think like seven or eight fsm diagrams in it that all interact in uh i would say mathematically ambiguous ways even dccp has multiple fsm diagrams because it has one for the state changing stable and unstable and then another one for the communication so diagrams are interesting i'm not personally working on that problem i do think it merits attention but i don't think is a golden ticket to automatically getting"
  },
  {
    "startTime": "00:50:01",
    "text": "good fsm from rfc documents no but it would be very good but it would be very good if there is a disagreement between your fsm representation and the one shown on the graphic to to make that available to make this difference available to the authors because that might not be intended yeah that's a good point we we mentioned these things in our paper but i'd also be happy to summarize some of the differences uh in an email to whoever would find that interesting so maybe offline you can help me know who would like to know about things i found in the rrc that that maybe could be improved thank you thank you great work very interesting i want just two questions quick the first one you've shown two different methods one with linear one with uh i don't remember sorry anyway i wanted to ask if they are wrong in the same way or they actually could be used together to cross validate each other and say okay yeah that's a good question i haven't thought about that maria probably has yeah i'm sure that you could use them to you could do some sort of intersectional approach potentially right where you you have like a higher layer that takes as input both models and then finds kind of where they agree um that sounds interesting i i haven't tried that but i think it's a good idea thank you and the second one is if you are taking else into account not now my maybe in the future the problem of the length of the fields because sometimes the same transition is actually governed governed by some fields it might be either too short or too long or well usually too short and also that might be a problem for the state machine yeah that's a great question um i would say that prior work before i started my phd by my group specifically by samuel giro who as i"
  },
  {
    "startTime": "00:52:00",
    "text": "said is at lincoln labs now working with maria um applied similar techniques to figure out the packet structures of protocols using natural language processing applied to rfcs and you could potentially combine these approaches right so if you have one approach to find the packet structure and another to find the finite state machine and then you can go from just reasoning about kind of your atomic propositions of like scene and fiend to actual full-fledged packets then that sets you up to pretty nicely be able to make arithmetic statements about the contents of those packets right such as their length so that would be a next step however from a formal method perspective is complicated because you get into a state space explosion problem so you probably wouldn't want to use model checking at that point if you're going to reason about the packet details you'd want to use another approach but yeah it is interesting and something we've thought about yeah thank you thank you great work uh this is artwork from microsoft so i think there was one thing that i found a bit missing in the evaluation so obviously your current extraction don't have very good accuracy and also you have the additional loss from the translation to state machine so have you tried to actually uh do the work that you suggest in the future steps for the tl for instance for tcp or for uh gccp which would be essentially measuring how many trends you need to actually make in the original rfc until you get to the point where you actually don't have any missing transition anymore and you don't have any false positions anymore because it feels like with some that many errors you would need to essentially uh completely rewrite everything it would be like very frustrating for the rfc authors to actually do that methodology so do you have any kind of thought on that or did you do any experiments on that great question i have two thoughts on that first of all we have not worked on this yet because i think there's a lot to improve in um"
  },
  {
    "startTime": "00:54:00",
    "text": "in our methodology before we try to improve the rfcs so i mean just from an academic perspective i understand that as like the ietf community you are more interested in improving the rfcs but as academics working on trying to find cool ways to extract information from documents we're perhaps focused right now on improving our ways of extracting information from documents and as you can see with prediction error there's a lot that we could improve on right by training on more documents and improving our model structure um that being said i think actually that what we'll eventually find when we do get to doing this experiment is that the amount of changes necessary is not that great i think for the most part it would be things like you have a sentence in which the target state is implicit and you just add the words to scene received to the end of the sentence and now it's explicit right essentially taking things that to a human are implied and making them explicit because i'd rather have a slightly redundant boring spec that's unambiguous then one that's very cleverly written and i get confused by um so i actually do not think it would take that many changes in order to make it unambiguous good thanks thank you all right thank you uh a reminder that we we're using the meat echo queue so we can do the the local and remote people in the queue uh last question i think is by jonathan yes yeah um so i i'm uh i i spend a lot of my time doing tamarind modeling so um um i'm interested to know why you're using fsm because almost all protocols that i've come across cannot be expressed as an fsm you need tokens or they have states or you're actually very limited to maybe just tcp i i can't imagine what other ones don't have state yeah this is a fantastic question um so uh people who work in tamarin or pro verif or dare i say kryptol are um uh interacting with mostly cryptographic protocols or"
  },
  {
    "startTime": "00:56:01",
    "text": "protocols that use cryptographic primitives and they care about things like secrecy and privacy and and i care about those things too but um but because they care about those things they they need to reason about threat models where secrecy or privacy or confidentiality et cetera could be violated and attacks against those things and um fsms are are i would say probably not the right approach for that so i think we agree on that front and i i think that um maybe a more type theoretic approach like uh what what cameron does is quite good um there actually are quite a few protocols that uh have at least components that can be studied using fsms mostly communication protocols examples include ltp tcp dccp sctp sftp ftp and some also some newer protocols i've been looking at that are not related to itf uh which we'll release in future work so i would say that many things that are communication protocols that have handshakes in which you establish some sort of connection are are very well described by fsms but another aspect to your question is that we often will take a protocol and break it into chunks and say this chunk can be studied by model checking with uh an fsm representing that component of the protocol but this other chunk might be best studied in some other way so the cryptographic aspects are probably not suitable to to model checking yeah thanks for the question though great question all right thank you uh really excellent talk really excellent questions uh there's a bunch of discussion in the chat as well which you might want to look at uh uh so in a couple of minutes so thank you again max your talk thank you very much [Applause] all right thank you so our next speaker today is jane yen from the university of southern california uh who is going to talk about tools for disambiguating rfcs"
  },
  {
    "startTime": "00:58:01",
    "text": "yes thank you okay okay can everyone hear me okay great um so hi everyone um so today i want to talk about our work that i work with my advisors barack robin and ramesh govindam the topic is tools for disambiguating rfc let me start from giving you some numbers that i got from the rfc editor website here this table shows the number of published rfcs each year from 2016 to 2021 we can notice that the published number ranges from almost 200 to 300. so assuming each rfc is about 10 pages that means there are about 2 000 or 3 thousands of pages every year that should be reviewed and studied and then considered it doesn't sound like a small number to me it also indicates that great human efforts in involved so human efforts is involved in various parts of the specification production process and as an example here is a working group they will need to gather and discuss which uh and or what protocol needs to be standardized and there will be one or more specification authors composing the contents of spec although it might be obvious the content is full of domain specific knowledge and would require professionals to verify the content any unclear detail is picked out by human examination when we speak of the concerns about the quality of a spec it is hard to neglect the impact of ambiguities with the existence of the ambiguities in"
  },
  {
    "startTime": "01:00:01",
    "text": "a spec different protocol implementers could interpret the content differently and generate multiple versions of the protocols and here's one example which might cause different interpretations it says the checksum is the 16-bit one's complement of the one's complement sound of the icmp message starting with the icmv type at first glance it might not be obvious why the sentence is ambiguous however when we focus on the last part of this sentence it explicitly explicitly mentions the checksum starts with icmp type but it doesn't mention where the checksum computation should end so how could this affect a person's interpretation here i show the icmp header to help illustrate how we can at least come up with two different interpretations one is to check some only the icmp header part and the other is to check some both the header and the payload as shown the two different interpretations would lead to different protocol implementations so with this example we noticed that using an ambiguous specification could not only result in different implementation but also result in bug implementations and or security vulnerabilities so if we go back to our scenario of generating a spec the working group and the spec authors all know the importance of delivering correct message over the spec to spec reader so there are some guidelines to follow and they are useful to reduce the ambiguities but do we know whether we are close to near"
  },
  {
    "startTime": "01:02:01",
    "text": "zero ambiguity for the specification motivated by this question we presented our work in uh last year cycle and and our w following i will introduce what they are about respectively the first one is sage in this work we get to know what ambiguity exists in a number of long-standing protocols specifically we uncover by instances of ambiguity and sex instances of underspecification in icmprbc with an ambiguous specification we are able to generate executable code that can interoperate with the third party code and they generalize to significant sections of bfd igmp and ntp well as an introduction i leave some detail in our paper and aim to give an overview of it so in sage we apply natural language processing techniques on it to discover existence of ambiguities to do so we need to understand the semantics of the specification which is commonly termed as the semantic parsing and ultimately we want to leverage the results from semantic parsing to generate a low low-level executable protocol code since semantic parsing is not perfect and may possibly never be perfect in the future once ambiguous sentences are found we will need to involve human efforts to edit those sentences when a specification is unambiguous executable protocol code can be automatically generated and in this work we face a number of challenges"
  },
  {
    "startTime": "01:04:01",
    "text": "first semantic parser only parses generic terms but specifications could use domain-specific languages languages such as checksum once complement etc and second semantic parsing is not perfect and therefore multiple representations might be generated but only a subset is valid finally when we generate semantic representations we need to convert them into correct executable code in this work we make the following corresponding contributions first we extend the generic semantic parser with domain-specific syntax and semantics second we automate this integration of poor semantic representations with defined checking rules and third we compile semantic representations into executable code sage has three components that correspond to the contributions and respectively they are semantic parsing disaggregation and code generator components a complete rfc parsing workflow will look like this in addition to the already introduced three components human will be involved in the loop after the disability phase if more than one representation remains it means we discover ambiguities so we provide feedback to the user who is likely the specification author to resolve ambiguity by rewriting the sentences also if a sentence is unambiguous sage will proceed to map the intermediate representation to a code snippet the code snippets will undergo unit tests to check if it performs any underspecified political behavior"
  },
  {
    "startTime": "01:06:01",
    "text": "i will introduce the main idea of each component first semantic parsing component the purpose of doing semantic parsing is to derive a semantic representation therefore what we can expect is to take in a textual sentence and turn it into a chosen intermediate representation and in our work we use a semantic representation called logical form a simple logical form is composed of a predicate with a number of arguments the key observation here is that a logical form is a unifying abstraction for dissemination and code generation we do not change the functionality of the parser itself but for us to properly parse our text we have to extend the syntax and semantic parts for example the nlp parser wouldn't recognize all the nouns or noun phrases so we have to use uh so in this work we use a generic parser called spacey and we extend the species term dictionary with networking terms such as one's complement and for the semantic parts a specification might include some idiomatic usage for example the equivalent mark in zero equals to echo reply is not a sign that can be processed by the generic parser after we add the semantics for it the parser could understand that it is an association relations in the sentence next let me introduce how we leverage logical forms to discover ambiguities if the lp parsing yields something other than exactly one logical form we call it"
  },
  {
    "startTime": "01:08:01",
    "text": "a true ambiguity an example of a zero logical form case is that a sentence is incomplete due to missing subject in the sentence and another example of more than one logical form case is that a sentence uses in precise language such as using a terminology that can be interpreted as two different concepts so the nlp parser has limitation and could generate additional logical forms that do not indicate the true ambiguity cases therefore we define five different types of check-in rules to eliminate these additional logical forms the by-checking rules are type argument ordering predicate ordering distributivity and associativity respectively but for more detailed introduction of these five chicken rules you can find in our paper after disambiguation every sentence uses only one logical form as an ambiguous specification then we move on to the code generator the code generator works as follows it checks in a logical form and its contextual information and turns it into executable code this example is simple and straightforward to map a logical form to an executable code but this converting process can get far more complex than this example we include we also include more complex example in the paper so you may check it out later um i'm only giving you an idea of what kind of evaluation we've done in sage we um put the sage is on rcmp rfc we apply the sage on"
  },
  {
    "startTime": "01:10:01",
    "text": "and prfc and use generic linux pin and truss route on the center side and use the package formulation function to generate a reply packet back to the center the tested ping and threshold programs are able to process our automatically generated reply packets as for showing the efficacy of applying disambiguation rules we analyze how many logical forms are generated as the base and how the value goes down to one after all checks are complete i have focused on how we use sage to understand an rfc and but in our nrw paper we extend the discussion to cover what else challenges haven't been addressed and how the work can be possibly extended while siege takes a significant step towards automatic specification processing their much work remains in an rfc there are many components stages able to parse packet formats pseudocode but there remains other components it hasn't supported such as stage machine communication pattern and architecture in addition to the limitations shown there are other challenges that can also be considered from code generation perspectives thinking of stage as not only a method to understand the level of ambiguities but also a method to automate the code generation there is a lot of room to automate the code generation part there's for example sage mostly parses sentence by sentence to determine a sentence ambiguity and generates code according to the order of descriptions however it's unrealistic to assume that all specifications use self-contained"
  },
  {
    "startTime": "01:12:02",
    "text": "sentences every time that means we should consider the relationship between sentences and how a sentence can provide the context to generate the code for authors for another sentence another challenge could be discovering the mismatched or miscaptured behavior for example we have both text and syntactical components which might be a diagram what if the information in the diagram is not inc is not consistent with the textual description or what if a piece of information is illustrated in a diagram but there is no textual explanation should the protocol implementer implement that part of the diagram supposing that we are able to process a single rfc already what about some protocol describing its functionalities or constraints across multiple rfcs in some cases a protocol gets revised and added more constraints in implementation with the publication of another rfc how to aggregate the information from multiple rfcs then there's another type of challenge which is aggregating the information of multiple protocols instead of one protocol in practice we send packets with a stack of protocols when we want to generate the code of a stack of protocols how do we exactly organize correctly organize and categorize the information from diverse rfcs in addition how do we select the constraints or values while the stack of protocols is slightly different yet another interesting perspective is to consider the logical functionality versus the performance of a protocol"
  },
  {
    "startTime": "01:14:00",
    "text": "implementation a protocol rfc usually focuses on describing its logical functionality and lifts the flexibility of implementing code to any reader of the rfc that means we will know the political implementation should be logically correct but not about how the performance will be if the spec author suggests any performance oriented implementation will we be able to differentiate them and leave out the implementation as the flexibility or the freedom of the code of code implementer the last part i would like to present is our current work direction i have shown this figure previously it illustrates how we process an rfc document and involve a feedback loop to indicate the human user where the ambiguity exists i would like to point out there is still room to improve the involved human efforts such as how to assist editing ambiguous sentences or to extend sage to support more protocols this leads to another two challenges we need to deal with considering how we can assist editing ambiguous sentences we might also want to ask can we avoid writing an ambiguous sentence in the first place as for the other challenge we have only talked about a subset of specification components what else protocols are we going to support regarding the first challenge we are motivated to design a user interface tool that can guide the spec author to input only essential information for the protocol then we will use the extracted essential information to produce an ambiguous readable english text and"
  },
  {
    "startTime": "01:16:01",
    "text": "executable code as for the second challenge we are aware that stage 4 protocols require considerably more complicated operations and many protocols belong to this group unlike stateless protocols a staple protocol is required to keep internal states to integrate packet contents given the current state the ability to keep historical information could allow us to significantly cover more protocols so now let me piece together we would like to give the user a friendly interface that is able to extract the essential information we need without ambiguity as one of the options we might design interfaces similar to microsoft archive which leverage the web interface and puzzle-like shapes to generate executable codes but for us our output becomes the english sentences once we design a parser and parse the information received from the user interface we leverage the information to both compose easy to read english sentences and the executable code the last part we would like to fill in is what essential protocol elements are considered as i mentioned we want to deal with stateful protocol from our internal discussion we believe a reasonable stats should concern contains skymer less received packets output packet and the program's deeds this constitutes our vision of a useful tool to generate bi-directional unambiguous english text and precise executable code this is my last slide i'm happy to take questions and also interested in learning your opinions about the current work i leave my contact information and the open source code of the mentioned"
  },
  {
    "startTime": "01:18:00",
    "text": "siege on the side all right thank you train a real really excellent talk [Applause] uh does anybody have any comments uh or questions about this talk a couple of minutes to come up maybe i think one one of the the things which interested me about the work is is that it really nicely complements the the previous talk and that we we have you you're modeling aspects of the protocol which are not not perhaps captured by the finite state machines and so there's a nice complement between the two um [Music] one question for me i mean you um you know your you know but both this this this mechanism and the the mechanisms you've developed and the mechanisms and the previous talk are all based around natural language processing and so on um how how expensive are they to run how what was the sort of runtime for these models um the round-time is um longs for processing the icmp rfc it contains um seven oh sorry i forgot the exact number messages but to to process that whole rfc is runs for about five minutes okay so so so it's it's not instantaneous but it's not crazy long yeah okay yes good because um part of the complexity is due to the the lp parser that it has to analyze how many different kinds of way to generate the semantic representation so there are ones that you have a so for example if you have a noun phrase that"
  },
  {
    "startTime": "01:20:00",
    "text": "can be interpreted in multiple ways then all of them have to get enumerated um to generate the representation so that takes uh that takes the majority of the time yeah okay makes sense all right thank you uh folk i guess here in the room yes i am uh thanks well okay maybe i'm a little bit sidetracking um [Music] with the comments to start towards colin colin you said well okay we are used to do the specifications in english prose well okay kind of uh one some sometimes we enhance the specifications by some formal stuff i would not consider ascii art as a real formal enhancements of the rfcs but they are in there and in the iec icmprcs obviously and kind of i wonder how does the nlp actually deal with stuff like that are you are you kind of doing the manual transformation to reasonable data data structure definitions by hand and continue from there or the other way or ask around would it actually help you to have in the rfcs more formal and more modern ways of specifications of the relevant data structures okay so for the ascii arts parts um we didn't we didn't use the nlp"
  },
  {
    "startTime": "01:22:00",
    "text": "parts lv tool to parse that um instead we parse the ascii art of the packet header by writing another small program to parse it and it can automatically extract the structure the the packet header structure so that's one of it and um i just want to address in another perspective is as our current work is trying to extract the essential information out of it by by looking at those essential information we are thinking not only to generate the english text or the executable code we also are thinking we are also thinking to use those information to generate the diagram automatically so that's in the future we can we expect that the authors can directly compare the diagram or it can automatically use the diagram generated in the rfc so that it can help illustrate the idea so i hope that and answer your question yeah okay good good great thank you uh jonathan highland i guess hi and that was a really interesting talk thank you um i was just sitting there thinking wouldn't it be easier for us to formally specify protocols and then use some tool with nlp to generate the pros spec and then we can be sure that the pro spec actually implements the uh formal specification and then we can check the formal specification yeah so uh i agree that is also one way to do that um the thing is we have to gap we we have to fill in the gap of uh how to convert between the former specification um a formal language um former specific"
  },
  {
    "startTime": "01:24:02",
    "text": "specific engineering language to the english text so that part is still missing over there and um it's and i think there's one problem over here is there are too many different kinds of intermediate representations so formal specification language is sort of like another intermediate representation um you can always choose like different ones and then try to fill in the gap from the english text to that representation so i i would say that is totally a doable and actually a good way to go from it um but our first work stage is to just directly choose a simpler way to do the first first step to analyze whether we can find some ambiguities but for further specification i agree that's writing a formal specification language and then and then generate the english text might be also a very doable way um but i think we start from like a smaller um scope from the state machine to just have some like simpler probably some uh simpler language english text language and just to generate a very cool uh cruel simple english text um but we have to see whether that is a good approach first otherwise that we are not sure whether we should go from the formal specification language to the english text because formal specification specification language text um i believe it's more you need to take some time to learn and to pick up and then to do those things but if we just extract the essential information like just some key value pairs and we can just directly go from those information and generate text that might be also another good way to go yeah thank you"
  },
  {
    "startTime": "01:26:00",
    "text": "great great thank you i'm conscious that we we have one more talk and we're running a little little behind time so uh again thank you again jane was that really excellent [Applause] all right thank you the next speaker is uh chris wood who i believe is there in person is that right yeah hello um trying to figure out how to control this remotely all right i shouldn't theory have passed you control great i got it all right so to you chris all right thanks all right everyone uh my name is chris wood do some uh work a number of places in the ietf uh in particular in um the cfrg as a as of i guess recently uh do i help out producing technical specifications for that particular group and uh what i'm going to try to talk to you about is i think a problem that has been emerging in that particular group which is not it's not only in the cfrg it exists elsewhere um and uh it's on it's on this topic of you know uh how how can we improve the quality of specifications um in in in the cfrg and elsewhere in the ietf and irtf and uh i should say first and foremost that this is very much based on you know my own personal experience writing these things trying to like communicate technical things in a way that's clear and concise to people i've not not like talked about this with many other people i've not like asked for a review on the slides so i uh you know if you disagree that's cool we should talk about it um and uh this does not like consensus of"
  },
  {
    "startTime": "01:28:01",
    "text": "the cfrg in any way particular or any any way shape or form also i'm not trying to like point fingers or blame people and in fact i like i work on a lot of documents so i should just be pointing the fingers at myself i really want to see if there's a way a reasonable way forward for the itf and the irtf to improve the the overall quality of the outputs and so this is an attempt to do that okay so i'm going to start with what i think is sort of the the problem um or maybe it's not a problem but i depending on your perspective but i'm i'm going to refer to it as a problem so in in theory the cfrg is is uh is chartered to do um a lot of things it's pretty broads broke uh uh pretty broad charter um you could summarize it in the sentence that i have sort of coded here it's meant to sort of effectively bridge things that happen in in academia in theory and sort of bring them into practice um documenting things that like are written down in cryptographic like related papers or and published in conferences and whatnot and present them in a way that people can actually use them and to do so via what are referred to as informational rfcs an informational rfc just contains information does not go through the same sort of uh like editorial or or rather review process the same sort of rigor that other things in the ietf and elsewhere go through so that sort of bar to publication of an informational rfc has like historically been lower than that of like a standard track or proposed standard that's kind of a problem because in practice the cfrd specifications i think demand a much much higher bar uh for what they're actually being used for you you could"
  },
  {
    "startTime": "01:30:01",
    "text": "go back and look at all the many of this many of the specifications or rfcs that came from the cfrg and look at how often they're used and referenced in practice and it should be very clear that these are very very important documents like things like hmac hkdf uh crypto9 eddsa and hpke that's a recent one um these are all like fundamental to how the ietf and even other irtf groups design reason about and build protocols the security protocols in particular moreover these specifications that are developed have a huge audience they are meant to be useful to people who are designing protocols they want to be able to consume these cryptographic objects um they're also useful to people reviewing these particular specifications to make sure they're consistent with things that have been like analyzed and proved in practice and there's probably lots of other audiences as well but the gist is that uh the the stakes are very high i think for the things that the cfrg does despite what the charter lists um so quality of its outputs is is critical um and uh so i'm going to be trying to focus on like specification quality here um and uh what do i mean by specification quality so i i claim that there are at least three sort of fundamental aspects of a technical specification first of which is like the functional piece of it like what does this thing do and cfrg is actually kind of it's quite uh nice in that like typically the things that it works on are much much simpler compared to protocols that the ietf standardizes so it was only until i guess recently that we actually started looking at real protocols that involve like sending messages between different parties um"
  },
  {
    "startTime": "01:32:01",
    "text": "historically it's just been like you know an algorithm specifying algorithm that has some steps and an api and a syntax and stuff like that so i'm just going to refer to these things as objects and the the functional description of this object is you know what does it do what's purpose what is its purpose like i just kind of want to reason about how i might use and hold this particular thing the second part or a second part is what i refer to as the syntax specification and it's like how do i what does its api look like how would i interact with it if i were to implement this in like a type safe language what would that api look like and how might i enforce it and the third uh component is like the actual implementation like how do i how would i write code to implement this particular algorithm and to do so correctly um and keeping in mind the different audiences that were listed earlier it's important to make sure that each of these different components of a specification are tailored to the relevant audiences so for example the implementation specifications would be written such that implementers can actually use it and implement the thing correctly whereas like the functional piece of a particular specification needs to be written such that people who want to just figure out what this thing is doing can do so without having to understand the implementation details like you shouldn't have to be a cryptographer to understand like what an rsa signature is or whatever like that these are like simple concepts and people building these protocols in the ietf should not have to be experts to use them they should be like easy to use hard to misuse that sort of thing blah blah blah okay um and to figure out whether or not you know the the the specification or the the different components of a specification written satisfy these like these goals there's like there's like questions you can ask yourself as an editor or an author of a document"
  },
  {
    "startTime": "01:34:01",
    "text": "i think the the first and foremost one that comes to mind for me is like is the specification easy to use easy to understand and use like can i just like read it and figure out what the thing is doing can i reason about like the behavior of the cryptographic object could i reasonably like think about what the api might look like and what the exceptional cases might be if i try to hold this thing the wrong way second most important question is like if i'm reading this thing and i were to try to do a faithful implementation of it would i produce a consistent and correct implementation that i could interoperate with other people's implementations and that depends on you know the behavior being well defined it depends on the implementation description being clear and so on um and lots of other questions i'm sure would come up to to try to figure out whether or not you're on the right track so i want to look at a couple examples um that uh the the the cfrg has done um and to try to uh reason about these two high-level questions is the specification easy to use and understand and will it yield consistent and correct implementations and i picked on an rfc that gets picked on a lot rfc 8032 for addsa has this uh one component of the draft specifies how you like verify digital signatures um don't need to read this in fact uh i encourage you not to read it because you might get bored but i will just note that in this this final piece of this particular body of text they have something like you could do or we recommend you do this one thing for correctness and security and safety but maybe if you're feeling up for it maybe you do this other thing instead um and it turns out that has been the cause of tremendous amount of pain and practice it's been the cause of like security problems in practice um [Music]"
  },
  {
    "startTime": "01:36:00",
    "text": "people just don't agree on like what signatures are valid and that's that's probably not a desirable property for a digital signature scheme so i would claim um that uh to answer the first question like yeah the specification is easy to use and understand like it's actually a really well written technical specification um but was it written in such a way that it yields consistent and correct implementations unfortunately not um and uh just my own personal opinion like i said if you disagree with any of this that's fine we could talk about it okay another example is actually an ongoing specification that's being developed right now uh this thing called opaque um opaque is uh without describing what these terms are it's uh basically um it's a compiler for taking a bunch of different pieces or a bunch of different cryptographic algorithms gluing them together and the the result is a is a password authenticated key exchange protocol um uh conceptually like once you if you understand the object and understand the protocol it's pretty easy to think about unfortunately to like actually understand the spec and implement it correctly you have to understand and be familiar with all these other concepts like what the authenticate exchange thing is what this vo prf thing is what what hash to curve means what a prime primordial group means and there's other relevant specifications not listed here that are likely also in this dependency tree um that are like necessary to understand opaque um and uh so i to to to i guess answer the two questions i would say that this specification is not easy to understand and use um uh and will yield correct and consistent implementations uh their stress factors"
  },
  {
    "startTime": "01:38:01",
    "text": "and people have done it so maybe that that check is a bit biased but maybe there should not be a check there because it's still an ongoing draft so we don't know um but the takeaway is that like this is an incredibly complicated document it could be much potentially be much simpler if if you try to uh change up like what the what the the features that were supported are and and and whatnot but um it's not simple to understand and that's not great for security relevant protocols okay um so i think the the the the core problem here uh is that uh the nature of the work that the cfrg does is highly technical and it's highly technical and has to be consumable by like different audiences with different levels of expertise and different purposes for consuming that technical specification um like an implementer just wants to know how to implement the thing the protocol designer wants to know how to use the thing um and the the problem is like this is a hard problem so there's like no way around it other than just acknowledging that this is a hard problem like writing a technical specification that takes all of these boxes in such a way that it's maximally useful easy to implement and correct and consistent um is is not an easy thing to do and we've been doing it for trying to do it for many many years i do think we're improving but i think there's more that can be done and trying to shine a light on you know potentially more weight more things that can be done so i wanna i wanna uh call out one particular draft that uh is going through the process right now uh to kind of showcase how we chose to uh you know structure the specification in such a way that it does try to tick some of these boxes in what i consider to be a useful way for people who are"
  },
  {
    "startTime": "01:40:01",
    "text": "consuming the spec so this draft called hash to curve it is uh again without going into any of the math details it's basically just like a hash function uh or it's a specification that describes a hash function that takes as input arbitrary data a string and produces a point on a curve and that's basically it at the highest of levels it's a very simple interface it can be summarized in this one picture here it's internal steps are like quite simple and and and and pretty easy to i guess like if if you don't want to know more it's it's easy to figure out i guess uh what what the i guess high level functionality is basically from this so um as i go through next couple slides just i guess keep in mind and ask yourself you know is like based on the limited set of information i'm going to present like is the specification easy to use and understand and will it you'll consistent in correct implementations okay um so let's consider the the functional description of this particular specification first that is uh what the part of the specification that's written for the people who just want to understand what hashing to curve means so i'm if i'm reading this specification for the first time i might go to the part of the spec that describes like what this functionality is and i come across this this blurb here that describes hash to curve with some input produces an output and there's some there's just little there's these few steps here okay but maybe i want to know like what these steps are helpfully there's like similarly simple descriptions in the draft that describe each of these different steps without any of the implementation details like you don't need to understand them to figure out like what this particular function is doing like this this is sufficient i would"
  },
  {
    "startTime": "01:42:01",
    "text": "claim for understanding what like the function of hash to curve is doing um and i think that's like a good balance for this particular audience now let's consider the api designer so an api designer looks at the same you know section of text it sees like oh there's this uh there's this thing called map to curve as one of the steps that i have to you know invoke when like in an implementation of hash to curve what is this map to curve thing to do there's specific sessions that describe what the high level syntax and api for hash to curve would be for different like curves that you might target um i just talked about one this alligator two method for one map to curve thing it describes basically all of the parameters that you might need to configure for this particular function what are the constants that are relevant for this like particular function what are the exceptional cases that might you might have to specify when thinking about an api um and like this there's this other thing called sign of the output but don't worry about that um and i've truncated some of the like text that follows because from a like an api perspective this is kind of i would say mostly what you need um okay now you're now let's say you're like you're the implementer and you want to actually like try to implement this thing to make sure that you know it uh i i can use it for some protocol it interoperates with some other implementation again start on the left go to this map to curve thing and then i follow the pointers down to like the actual implementation description of alligator 2 in this particular case um and it goes into excruciating detail about every single step that needs to be done"
  },
  {
    "startTime": "01:44:01",
    "text": "with like comments as necessary for like the different parts of the pseudo code noting like for example on line three like what in exceptional cases um that was like described earlier in the syntax and the api and this seems uh and moreover it's not it's not actually included in this back but this is almost there's almost a one-to-one mapping between this implementation description as written here and the reference implementation that we provided in sage um not the sage that was talked about in the previous uh presentation um and so with the ultimate purpose to be like each step along the way minimizing potential source of errors and interrupt bugs or whatever so coming back to the the original questions um if you were to approach this document and ask yourself you know is it easy to use and understand and we'll yield consistently correct implementations uh i'm not going to like put checks or x's or question marks next to them you know i'll leave it to you to decide as an interested reader but i offer you offer it to you as a you know potential way to think about how we might structure these particular documents i think this one went through uh i spent a lot of time in the group went through a lot of refactoring and revisions and i think um it does a reasonable job at like striking a good balance so i may have just implicitly answered these things but anyways okay drawing on hash to curve i think there's a number of things we can do going forward to sort of improve the overall output of the things that we're doing in the cfrg and i'm going to enumerate just a couple high-level things without drilling into any specific details because a lot of it will vary based on like the thing that's being specified how involved it is like is it a protocol or not and so on"
  },
  {
    "startTime": "01:46:01",
    "text": "um i think the the most obvious one um is to just consider the audience uh it should be fairly clear if this is like written technical communication and that's obviously important for any sort of communication but that means like making it easy for the audience to like stitch these different pieces of the spec together so maybe that means like or that does mean being consistent where consistency matters like if you're using pseudocode to describe different implementations of different things in the specification use the same format of pseudocode at every single step of the way don't introduce like one different format for you know section a and then in the appendix use something entirely different consistency everywhere will help and will go a long way and also like hashtag curve does i claim that like any time you are using pseudocode try to make it as close as possible to the actual reference implementation that you are providing with your specification and we should require reference implementations for these things um but there's there's a lot of uh there's been a lot of interesting discussion in various like back analysis to in terms of like what this reference implementation format should be should it be say should it be rush should be something else uh maybe that's something we should all talk about but um try to make trying to make them as close as possible um as i think a admirable goal and on the heels of that i would say that consistency sort of uh beyond like sort of format consistency in concepts is incredibly important primarily because we want to reduce the cognitive load of people who are reading these specifications so not only like reusing formats but we're using concepts in draft itself but also across drafts so if we're talking about say elliptic"
  },
  {
    "startTime": "01:48:01",
    "text": "curves in one draft and groups or prime order groups in another draft that could be confusing to people because sometimes they're used interchangeably even though they shouldn't but sometimes they are so being consistency where you being consistent where you can is incredibly important using a shared vocabulary for the things that we're talking about is incredibly important um and uh i think for the most part the the some of the documents that the cfrg is working on right now uh are like certainly they're internally consistent i think the review process is such that like it gets a tremendous amount of review in the in the group before it goes out then it goes through like this formal the review panel step um and then there's a number of other steps but i don't remember what they are um and i think like we have reached a level of internal consistency but external consistency across drafts is uh it could be improved um and that's a hard problem because you know things and concepts change over time like what was like maybe you know we used to talk about the curves as you know the best thing since sliced bread but now we want to mostly we were talking about like primordial groups instead of elliptic curves we want to kind of shy away from elliptic curves in general um uh so maintaining consistency over time is is difficult but i think for you know batches of drafts that are being developed at around the same time it's it's not on a reasonable request and something we could do okay and i think the last one is sort of a call to action i guess to embrace formal methods and formality where it's appropriate there is a tremendous amount of work and referenced informally referenced and formally verified implementations of especially cryptographic algorithms like hacks back and hackle star and whatnot"
  },
  {
    "startTime": "01:50:01",
    "text": "whether or not they're suitable for the you know permanent nature of an rfc is not immediately clear to me but all of this work uh that basically lets someone take one single implementation and you know produce test factors from it produce other like implementations in different languages that they can test out uh and sometimes produce like optimized implementations as well that are just suitable to plop in production systems those those are those are good things and it removes any like choice chance of potential error from someone implementing these things themselves trying to you know interpret an implementation description or syntax description or whatever and potentially getting it wrong um i i was alluding to earlier it's like still not clear what it's like the right format for this uh this uh these like reference implementations be it like something based in python you know that a dsa rc8032 uses python to describe some of the relevant steps um more recently things have been using a lot of sage because it's convenient and there's there's like been a developing sort of library in which we can use or the developing uh i guess you can call it a library a library of like the the specs that are being developed the building blocks um and and drafts are like building on these building blocks and adding more building blocks and and and whatnot there's no reason you couldn't do this in rust or hacks back or whatever um i think it it'd be interesting to experiment with different reference implementations and see what is most approachable from an implemented implementer's um and what is what is good for really the you know inclusion in an rc okay um so i want to wrap up and lose some time for questions um"
  },
  {
    "startTime": "01:52:00",
    "text": "uh so i my core point here is that like the work that the cfrg does again is incredibly important um we don't call them standards they're not technically standards but i think um if it you know walks like a duck and quacks like a duck it is a duck these things should be considered standards in my opinion um they are incredibly valuable and important to what the ietf and irtf is doing um and i think we should treat them as such uh there's definitely a lot more work that we can be doing both within the group um even in the rtf for you know producing uh tooling like sage or the name that was in the first one i apologize um to uh improve the the quality of these specifications uh there's like ways that document editors can you know right now start improving the quality of their specifications uh through like better pros better better use of terminology better um more consistency what have you um and i think it's about time that we really took the question of uh or the the task of exploring formal method approaches for cfrg specifications seriously um and uh i don't know i don't know what that looks like in practice if it's like a task force that go off and like try to you know work through these different options if it's a group in the rtf to sort of uh explore this area i think you know anything is a reasonable start here but i think it's uh given all the work that's been done i think it's it's now it's now time to start uh actually pulling it into the cfrg um with that i guess i'll i'll close for questions um if you have any all right yeah jonathan [Applause] i think thank you very much chris uh jonathan [Music] so uh i was just thinking"
  },
  {
    "startTime": "01:54:01",
    "text": "if we are going to have reference implementations and pseudo code which has precedence right if the reference implementation doesn't agree with what's written in the spec which one do we consider correct do we have to decide on a case-by-case basis my personal opinion is that we should have reference implementations in no pseudo code um but uh like the that's just not the way specs are published right now we have pseudocode to describe the things and reference implementations sort of hang off the side of them it would be great if there was like one clear correct description of what thing is and how it's implemented not two possibly inconsistent representations so yeah all right thank you uh thomas i guess because you're in the room i i didn't sign in thomas is in the room but uh there's other people in the queue so i guess nick go ahead oh okay um so what about producing non-informational drafts within the cfrg is that completely out of the question no i don't think so for that i i i like does it i don't think so um whether or not that would i mean the topic of like should the cfrgb and irtf versus itf has come up before my general stance has been like if it's not broke don't fix it like from a like a iotf versus ietf perspective but um i i can't see any harm from like rechartering to potentially you know uh explicitly include like non-informational documents okay thanks i mean there are procedural issues there"
  },
  {
    "startTime": "01:56:00",
    "text": "but in practice uh we cfog should be producing as good equality documents as it can and yeah we can have the arguments about whether it should be an atf or an ircf group later it's us i guess um i'll go um siobhan said brave this goes to a discussion that we were having on slack chris but um security proofs and like do you think there's a conflict between this the proof and implementation complexity or simplicity um then like should you what should you do and maybe you can say in the draft like go and look at this other place where we did something slightly different and then proved it um yeah just wondering what i think anecdotally like the security proof should trump everything um if the implementation doesn't match there should be like very clear reason as to why it doesn't match perhaps with like justification as to why it would deviate from what has been analyzed in practice um uh but i think we should strive very hard to keep like all these things consistent and i would include like the the the security model and the analysis that's been done as part of like the description of the specification um uh so yeah keep everything consistent and hi again this is me thomas of pecorella and for real anyway i'd love to have a specification written like you said i really love it i have mixed feelings about reference implementations because i fear that for some cryptographic algorithms having a reference implementation would lead to more errors than on carefully written one meaning timing errors or stuff like that"
  },
  {
    "startTime": "01:58:01",
    "text": "possible timing attacks i mean somebody that just grabbed the reference implementation transform it in another language by some mathematic immediate don't use the brain and go on i would love to have however some test cases like if these numbers are there you must reach this point because i fear that in some cases uh this could be more useful to debug or test case your particular implementation um i mean yeah i certainly agree that the test cases and test factors are invaluable um and uh i think most of the the specifications that i'm aware of including probably would be the best yeah yeah um in terms of like whether or not reference implementation should exist uh that it was actually an explicit choice to use something like sage to describe uh these like hashtag based drafts because you can't just like easily take sage code and plop it in production we want to make sure that was like something you did not ever do and we furthermore we took steps in the reference implementation to try to describe uh you know uh or avoid sort of i guess implementation pitfall so for example if there you need to do like a comparison of two like byte strings and constant time we didn't just like in python say like does a equal b we would say like if a is equal to b constant time i forget like the extra actual string but like you make it very clear like you know this this is how you would actually implement in practice um but i mean it's a good point like should should reference implementation exist uh i i i tend to think they're useful but write a reference implementation that cannot be used as it is by um let's say an undergraduate student doesn't um actually have time to actually do something right that would be great and i'm all up for"
  },
  {
    "startTime": "02:00:00",
    "text": "it yeah i mean uh i guess the other other side of that coin is there are projects like uh i mean like the the recent nist pqc competition that all the reference implementations there were actually very high quality implementations and they perform very well um you could reasonably take some of those and plot them in production if you were you know if if you wanted to do so and i guess it depends on you know what sort of bar you hold for you know the reference implementation yes it also depends on how long you want to to have the rfc and how long is the code yeah yeah of course thank you yeah hey this is max uh great talk i just wanted to say i've found at least one protocol where uh not the pseudo code but the english language text did not match the reference implementation not an ietf one but one from outside so that can happen too um and i i do think it's maybe a good idea to work be worth considering to say what what should be um that somebody else mentioned like believed if there's a discrepancy right so the spec could maybe say if there's a discrepancy between this and the implementation you should always fall on the side of i don't know whichever right and and that way you have it be unambiguous so maybe there's something worth yeah that's not a bad idea all right thank you very much uh just to follow on from that last point i think it's very very clear that uh yeah uh chris had some some great examples but this is very much not a a cfig specific set of problems there are ambiguities and issues with all of the specifications uh and certainly looking at the the dependency graph colin jennings has for the web rtc specifications and the sprawling massive documents there uh i think it's it's clear this is a an issue of consistency that we we have in in many different parts of the community um so what what i've been trying to do uh with these talks is is to sort of try and"
  },
  {
    "startTime": "02:02:00",
    "text": "stimulate some discussion try and see if there's interest in your research on protocol specification thinking about how we specify protocols in in the in in the itf and the irtf communities uh it's time to think about what are the research challenges in ineffective protocol specification and how we can improve the way we specify protocols um there's certainly been a lot of discussion in the chat uh and a lot of really good questions to the talks and i think there are three fantastic talks there so thank you everybody for for both giving the talks and for the discussion in the chat um i i am certainly looking to to gauge interest uh to see if people are interested in potential irtf work in this area if you are interested in this pl please do talk to the speakers please do talk to me [Music] and if if you would be interested in following up on this put your names into the um that the documents that's linked on the slide and i i just put in the chat uh and we're you know not normally i would arrange a a lunch meeting or something at the itf um in covered times that's obviously a little bit difficult but please do put your names in the chat we'll arrange a call at some point if there's interest and see if we can take this further so um thank you uh again to everybody thank you to the speakers thank you to everybody for for paying attention um if there are any final questions i'm very happy to take them and if not thank you again all right thanks everybody look out for the nrw call for papers for next year [Applause]"
  }
]
