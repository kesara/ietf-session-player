[
  {
    "startTime": "00:00:53",
    "text": "it was Oh all right so I guess we should get started so this is the RM cats working "
  },
  {
    "startTime": "00:03:57",
    "text": "group I\u0027m Colin Perkins I\u0027m one of the co-chairs we have Anna hopefully one of the other co-chairs on meet echo and searching as well who will be giving one of the remote presentations there we go you expect me to work the Chromebooks can you hear me Anna churching yes we ride or at least okay excellent churching either yes very well I forgot to activate like okay get it since this is all working all right so let\u0027s get started so as I say this is the ITP media congestion avoidance techniques working group we have the usual note well hopefully everyone is familiar with this by now you have to disclose IPR and we have a code of conduct and the like today\u0027s slides are all in the data tracker if you want to follow along we have mute echo is anyone in the jabber room in the in the room Jonathan okay if you can keep an eye out for comments in there if I forget to press the red button someone shout at me I know all I am able to remote participation I will attempt to take notes if anyone else could take notes that would be helpful and then we can combine combine them to make minutes if you didn\u0027t take notes please send them to me so we\u0027ve got a reasonably light agenda today we\u0027ve got this introduction and status update to start with then churching will give an update on the NAD or implementation and some new evaluation results and then we\u0027ll finish up I have a very brief presentation about the congestion control feedback draft which will mostly be discussed in a BT core and I think Friday so looking at the document status we have a bunch of congestion control algorithm candidates and related documents the Google congestion control draft is has been dormant for some time and I\u0027m not expecting that to be updated although we would be open to an update if the authors care about that all of the other drafts the NADA algorithms cream the shared bottleneck detection and coupled congestion control refer in the RFC if it\u0027s a queue or have been published as RFC is already the requirements draft "
  },
  {
    "startTime": "00:07:00",
    "text": "the condition mutual requirements has been in the RFC at HQ for a very very long time I believe that\u0027s part of cluster 238 so hopefully will appear before too long the evaluation drafts the video traffic model was published recently recently the evolve test draft has been approved and this was the RFC editor when the eval criteria and the Wireless tests went to the IFG since the last meeting and they\u0027re waiting for processing the condition control feedback message I\u0027ll give you a brief update about that later today and it will be discussed in a BT car on Friday here my belief is that this is now ready for working group last call in a BT core and hopefully we will be able to wrap that up pretty quickly the RM congestion control feedback draft had a very brief update prior to this meeting which just fixed a bug I noticed to make the calculations I actually match the description in the text it turns out that you have to specify the dependencies in the make file if things to be updated correctly won the war for that draft once the feedback message has been finalized that will then get revised with the accurate numbers and the congestion control framework and codec interactions draft have both expired and we will I think we previously decided that we\u0027d reconsider those once the candidates had got the standards track and that may be some time so these are basically dominants we have a number of milestones I have omitted the the milestones that have been done for some time on this draft the others we had a milestone for December 2018 or she was to submit the requirements and evaluation criteria drafts to the iesg we\u0027re almost a year late but we managed it within within a year after the deadline so I consider that a great success and we we have succeeded in submitting those to the iesg are better than web RTC yes yeah yeah the milestone has been delayed a little obvious the evaluation results were supposed to go to the ASG in July the publisher first draft of the standards track congestion control in July and we are supposed to submits that the final version of congestion control and the interactions to the ASG by this meeting I think it\u0027s reasonably likely that those are not going to happen especially the ones in the past so the suggestion from the chairs after a little bit of discussion "
  },
  {
    "startTime": "00:10:00",
    "text": "was that we would spend the next 12 months or so gathering evaluation results and implementation experience and practical experience using some of these algorithms for the next 12 months or so and keep the working group alive and if there\u0027s things to reports then we can we can have meetings and have some reports and some discussion and if there isn\u0027t anything to report for a particular meeting then we can not meet at that time and then in a year or so we can evaluate that status see see where things are and see if we should continue in their pursuit of work if there\u0027s any interest in taking any of these to us done district I don\u0027t know if there any opinions on that from that the rest of the group jad yeah I think I think that\u0027s this sounds good to me but what I\u0027m I was wondering like what do you do with gathering information that is how do you gather it like is it like I mean we have been we have been showing results when we were developing the pollution control algorithms you have a quite amount of evolution results there so basically what exactly you would like to gather that was not already there and how you would like to gather that so where do we have the discussion well I think we shall have that discussion in in in this room yeah Mia Kulemin so what we usually do for congressional tour not that we have like a bunch of congestion control schemes sanitized in the IDF but like we\u0027ve been carefully about Senate I think I met proposed standard because you want to make sure it\u0027s safe to use them on the Internet so I think the only way to figure out it\u0027s safe to use them on the Internet is to actually use them on the Internet so what you want is deployment you want not only evaluation results you actually want like live four styles of deployed traces and whatever if you can\u0027t get that then maybe we shouldn\u0027t go for proposed standard which I don\u0027t think is like a bad thing because you can use congestion control without like having interoperability problems or whatever it doesn\u0027t really matter if it\u0027s experimental or proposed standard so but like that would be my bar socha heading and so do do we have an expectation of like what is the what is the spread of the deployment like like this amount of traffic need to use some condition control or something like that or how I mean deploy many I of course like what we can do we can what exactly for a scream we did and basically we have it open source we have a couple of "
  },
  {
    "startTime": "00:13:00",
    "text": "cases that we used it in different trials and tests and a light network and also how we know like some people are working on it it is on gh2 more and stuff like that but if you want me to call it all the more like instances of using scream it will be very hard for me to come and get an answer so what I would like to rely on like anybody using those not ice cream of Jesus they come and show show up here and that\u0027s also like and not really realistic as I\u0027m saying so what do you do here I mean that\u0027s my question basically I mean we for one year I mean we said like one year we\u0027re gonna gather a revolution reserves and experience but I mean this as a group I don\u0027t think we can force people to use these right if people are using them if people are doing experiments with them we would appreciate feedback if people are not then okay maybe we just leave them as experimental searching hi actually I meant to ask about it after my presentation but I think since we\u0027re talking about the plan for the working group I might as well make sure I have sort of similar maybe question as that he in sense of for the evaluation stage we\u0027re no longer talking about evaluating the performance right for that we have the test case we have all the collection of some preliminary set of our results now that we\u0027re talking about trying to gather a deployment results I was wondering whether that\u0027s a good expectation but I was wondering whether there\u0027s any sort of treats guidelines around it for instance the question I wanted to ask after my presentation was that we basically put in the implementation as kind of a experimental branch if you like as and the browser you know and buting an alternative method inside web RTC I was kind of curious to know whether any of the owners of either web RTC modular or inside Firefox is interested in trying it out on a larger scale so maybe those type of things so maybe this is more carful participation you know either inside this working group and I guess we can also reach out to people who are potentially interested yeah but then so that\u0027s my commenting may be trying to encourage evaluation of this method but the question is do we also have guidelines let\u0027s say if someone is interested in contributing to large-scale evaluation what do we tell them what type of stats to collect what type of how to run comparison things like that thank you I mean I think we I mean we have all the various evaluation drafts "
  },
  {
    "startTime": "00:16:00",
    "text": "which lists the type of things we look to evaluate now obviously what you can do in a simulation testbed is not the same as what you can do in a real deployment but you know metrics along those lines just generally experience you we have used it or we have seen other people using it it doesn\u0027t seem to break anything just some general experiences from from people actually trying it out and seeing if it really works yeah you could let me let me at one point so we also discussed that or like we decided long time ago that we don\u0027t need like to pick one that is the winner right we can we can document all of them and then we see what deployment which deployment will happen and so I think at this point it\u0027s also not about comparison anymore it\u0027s not to say this is this one is better than the other one it\u0027s just like if you can prove that there has been deployment and it didn\u0027t met the internet then we can move to stand up strike if not it\u0027s fine as well I don\u0027t think it\u0027s a problem so like I\u0027d really like the plan having the group open for 12 months see if there\u0027s like anybody who can report on these things and this timeframe and then if so we can move ahead that would be great if not I don\u0027t think it would be failure for this working group it would be fine as well hi this is Newton from Facebook I just want to add that we did some experiments and some deployments of one of these algorithms in a different use case and I would love to like my team would love to present at one of these working group meetings maybe next year great that that would be really helpful thank you John Lennox I think I mentioned on the mailing list that I\u0027m on the I\u0027m working for the jitsi open-source conferencing project now and I\u0027ve I think I mentioned the millions would be interested in trying out various algorithms in our open source SFU and we could even you probably you know we could certainly even try to try various algorithms in production I mean the thing about asset view obviously is your subject your own that you can only send what the browser\u0027s give you and getting but and you know which are which basically means GCC because they\u0027re all chrome but but you know we can certainly that\u0027s something that we\u0027d be open to contributions to try out various algorithms at our code even eventually in production okay thank you yeah I think I agree with Miriah like I mean this is fine with if we have the results we have a stronger if we don\u0027t have resolve so you have experiment and I think we have we have somebody saying here like they have dared I am doing "
  },
  {
    "startTime": "00:19:00",
    "text": "experiment and hopefully they are doing more than like GCC because I don\u0027t think like GCC is the experimental right now GCC is an expert internet after this expired internet drop but it is not deployed and so basically if you guys are coming up here and saying like we have we have evaluated GCC I mean that actually doesn\u0027t help us perhaps so BB so my expectation like you will do more than GCC so I just want to add that like like at the time when I submit the proposal like maybe to the cheer like you can decide whether it\u0027s fun or not but like our work is not just purely evaluating GCC like we actually applied it over a quick instead of UDP and we applied it we applied it to a different use case then video calling so there might be some interesting learnings for the group like I can submit a proposal and then we can I decide yeah I mean I think that would be an be interesting and you know we\u0027ll see we\u0027ll see whether this group is the right place to do it so ever whether we should possibly point you at so the the congestion control research groups that\u0027s yeah it\u0027s it sounds like interesting work and yeah it would be useful to hear it present it at least somewhere in in one of these meetings okay so how I\u0027m reading that is that there is general agreement that this is a reasonable approach and we will have to see how things go with the evaluation and it is clearly difficult to be prescriptive you know we\u0027re not gonna say you need so many hundred million users and minutes of deployment experience but if people are using it and getting some experiences then please report back okay so the only other thing I have I think that\u0027s the last of these chest slides I mean obviously it once we\u0027re talking about implementation experience we we had a hackathon earlier this week did anyone play with the erm catacombs in the hack fun Jonathan ik so I mean the main thing I worked on the hackathon is to sew is to try to get more visibility into tracing and debugging of what these things algorithms are doing cuz it\u0027s often it\u0027s very hard to say okay what actually happened here you know is you know is the fact that my band was just dropped you know some weird quirk about algorithm or really a profession of my network so there were there were no Sergio I mean three I think developed a bunch of visualization tools and so we\u0027re working on getting a common input format that our congestion algorithms can output such that we can feed these visualization tools to figure out what actually happened which is a very "
  },
  {
    "startTime": "00:22:00",
    "text": "important thing for an actual production system the other feedback we had you know more about not so much what we did but what we complained about not having was one thing that we useful to us is have a practical way to turn looks like some of the guidance in the content in the test cases into you know for instance actual commands we can type into Linux to turn on the various traffic shaping algorithms to say okay let\u0027s make something that looks like you know so we can actually test front on our real systems what\u0027s actually happening and you know there\u0027s these various tools there\u0027s this tool TC and Linux which is nearly incomprehensible to mere mortals there are various front ends to it but you know it\u0027s always clear whether the front ends are doing what you know something realistic because it\u0027s very easy to get a you know to pick the wrong you know traffic class or traffic shaping and get something that\u0027s completely unrealistic compared to what the test case they\u0027re trying to do particularly for things like the wireless test cases we have no idea how to make something look like you know a wireless base station using TC on Linux that is extremely non-obvious yes the that\u0027s not simple so if you had there\u0027s anybody if you if anybody has knowledge or know somebody who has knowledge you can sort of help turn this into something more practically realizable especially as you know you know if you get a lot of weird cases like when you change the settings have it not do something completely off the wall and like throw away all the packets that were in the queue when you change the settings or something like that which usually causes congestion value of them still completely haywire yes that\u0027s possibly something that it might be worth asking the math Hadji list because that maybe some other students who\u0027ve been doing evaluations and testing for this so some things there might have some experience the other thing we found is that a lot of both the algorithms and the test cases are written from the point of view of endpoints whereas you know the things that the people who are working on it are actually working on our SF use and so I think we need to you know sometimes you get things like where the algorithms will say okay when when this happens tell being coder to send more and I can\u0027t do that I\u0027m in SF you so I think sort of more you know clarity somewhere on you know how you know how it Asif you should behave and how it asks if you should be tested would be helpful to us okay thank you all right anything else before we move on okay so next up is cha-ching I believe okay "
  },
  {
    "startTime": "00:25:06",
    "text": "stretching that should be a slides up it in if you can see them yes I can see it well just checking if the room can hear my audio going through fine yes very clearly okay great yeah so and this will be an update on the NADA evaluation results following the most recent version of the draft and also somewhat updated implementation in the an open-source firefox browser and next slide please in terms of the draft the main update is really a recent sort of back to back to version updates to address and comments from both the gen art and sex sexy direct I don\u0027t know last car reviews as well as the tele chat and reviews basically right now the draft is in the editors queue and most of the changes to the draft are mostly editorial in the sense of trying to either be more inclusive or you know trying to eat and revise these discussions for more clarity and there\u0027s no algorithmic changes for the draft and also the details of the revisions were summarized on the mailing list one we have in version updates so folks are you know feel free to check them out and I think usually the diff view also gives very quick and concise comparison of revision updates and next one please and implementation wise and compared to the last round of our presentation we have updated our implementation to now include all the our following algorithm features and in this updated version of implementation that includes the mapping the nonlinear delay warping mapping in DES to the congestion signal as well as incorporating lost as part of the congestion signal penalties so this way the evaluation results I show today do reflect all the features that\u0027s kind of bells and whistles that\u0027s in the draft the other thing the other and things are mostly for inconvenience we basically added similar locking mechanisms the default rate adaptation modular and inside the web RTC modular so this way basically one we can do comparison we can have similar locking for for comparison of stats and we also enabled my colleague Sergio he worked on being an enabling on-the-fly switching between either running the rate adaptation in nada or in you know the default mode using the browser you know sort of a config panel so that just made our evaluation effort a bit easier instead of having to recompile most different versions of the same code and then I provided a link on kind of the current version of the code this is basically a branch that I fought off the web RTC the the Firefox entire browser code and we added the pasta the navigate "
  },
  {
    "startTime": "00:28:08",
    "text": "adaptation modular there we actually have two versions we have post the ground truth based version which is somewhat more like a fallback if you don\u0027t have the updated version of the transport feedback or the one-way delay based nada which reflects better all the features in the draft and it\u0027s become people feel free to go check out the code you know sort of take a look at the code to next slide please so in terms of evaluation results in this round we basically took the feedback we gathered from our previous presentation in the sense that previously when we presented results book will always have bi-directional cause and always compared the forward direction versus the reverse direction because the sender\u0027s were running different algorithms now that was is a chrome and people were having some concerns about the they don\u0027t necessarily follow the same path so the results I\u0027m presenting today although we still set up bi-directional calls and all the performance metrics basically all the monitoring logs come from the the direction from client a to B and we switch back and forth between the default behavior versus the nagas based great adaptation and as mentioned before for convenience with both added logging on the sender site to gather feedback from the receiver as well as per packet feedback information which is embedded in this kind of this new form of transport which Chrome already supports and the other may be side note is therefore nada the algorithm we do impose possib mixed max and minimum grades for the rate adaptation where as per my observation i haven\u0027t observed a clear rate boundary for the default option and we set the default resolution to 720p so with that and then only on the side and since we do gather all the logging information on the sender using our own code we also try to corroborate that information by taking screenshots on the receiver because chrome the web art web RTC internals tab do provide you with figures of various metrics of interest so with that we can probably now start taking a collection taking a look at some of the evaluation results question yes please so you modified Firefox to support transport so you see because I know by default it does not and that\u0027s right that was I believe that\u0027s the fruit of one hackathon maybe a few few meetings away it was mostly work from another co-author and Sergio may not he\u0027s due to timezone difference is not attending the meeting today he was mostly working with a neo from Mozilla if I remember correctly so they do have basically they enabled a transfer CC inside the Firefox yeah so we\u0027re building upon that yeah and chrome "
  },
  {
    "startTime": "00:31:09",
    "text": "already supports it so it works well for us yeah the other thing I want to call out is that this type of feedback the default interval seems to be at 50 milliseconds sort of a 150 millisecond would you get a feedback and the feedback information in that\u0027s per packet sort of acknowledgments yeah information so that\u0027s very we basically have all information we need for running the the another algorithm great great so we can move on next slide and I just want to mention as I mentioned before now we\u0027re focusing the comparison along one direction so at least hopefully you know between the same sender and receiver they follow the same path we still count really to idealize comparison so what we do is we try two different mechanisms one is back-to-back sessions so basically we\u0027re on one car using the default for immediately followed by another car of similar duration use another the other thing we do is that we also try a little bit about running two parallel sessions between the sender and the receiver one of them running nada the other one run in c4 so this way at least we subject them to the same path and any loss in delay pattern and we also get observed a little bit about how Nava competes against another default web RTC cop so in terms of evaluation scenarios are representing two different settings in one setting it is a car between I\u0027m based here in Austin Texas so the sender is based in Austin Texas and the receiver is based in California so it\u0027s a kind of a cross continental or putting the US units are domestic long distance car the other one is between Austin and Europe basically between my side and my colleague Sergio basting he was taking those costs from Spain so that you would you guys would see that it would involve a somewhat longer and wrong to delay baseline round-trip delay for the past so these are the two settings for each of the setting we tribals the back-to-back session and arrows worth it we\u0027re looking at four set of comparison results you feel like we can move on to the next slide so this one for back to back I\u0027m I thought it would be easier so even though we do default first followed by nada I thought it would be easier to show both of them side by side for maybe for ease of comparison so we have both the rates the delay and the packet loss ratio for the rate the blue lines show the target rate calculated the read adaptation algorithm whereas the blue the red dots are d and kind of the measured acknowledged rate based on the per packet feedback information and the read great are calculated using a interval of 200 milliseconds so they\u0027re "
  },
  {
    "startTime": "00:34:10",
    "text": "somewhat noisy or compared to the calculation of the target rate the middle two graphs show the round-trip time and delay granter time map and Ingrid and the queuing delay which is kind of derived from within the algorithm by subtracting out the long term baseline one-way delayed and is in blue and the the final two graphs show the in kind of the instantaneous packet loss rates basically we take the packet loss rate within a window of you know the 15-minute second the feedback look at the gap in any sequence number and also try to do a temporal smoothing but still we do see that even the losses don\u0027t occur all the time when they do occur they can be quite spiky so these are the three sets of metrics that we keep track of maybe the main observation and here in the bottom I list out the kind of the baseline are Kiki we observe which is the minimum our T key over the path which is around 16 in a second between Austin and California and the maximum are T key we observe is around 2.2 seconds so one things to queue up apparently some of our routers along the way have a deep use yeah so with that maybe the main observation I want to call out on this slide is that obviously they are back-to-back so we don\u0027t really have a exact comparison but maybe qualitatively we still see and the kind of the amount of losses the experience are similar they have spread the experiment sporadic and sort of hiccups over the network the default algorithm seems to be a little bit more so one day when they back off and taken ramp up again it takes a bit longer time and they also incur they actually incur a little bit less and delay so somewhat less aggressive whereas the nada core what we do notice is that it\u0027s very fast to ramp up and whenever there\u0027s a hiccup it recovers from the hiccup quite and fast and so and interestingly even though these two cars are between two sort of enterprise offices but turns out the Wi-Fi situation can be quite noisy and busy and between both offices yeah so next one please so firmed so this is from the same experiment I\u0027m just showing the screenshots captured from Chrome browser for for each of the sessions more as a corroboration and I guess qualitatively the kind of poster rate profile as well as the frame per second to reflect the rate we have measured internally at the sender so these are really all corroboration thoughts to make sure that our locking mechanism is not broken "
  },
  {
    "startTime": "00:37:10",
    "text": "yeah next one so under the same setting we now have two cores and side-by-sides and post the default and neither they actually share the past and over the same time that\u0027s why if you take a look at both the delay and the packet loss ratio we do see very synchronized behavior which is not surprising at all and again you know occasional delay spikes in this one so in this one and the I guess the red the red dots of the naba car in terms of how much weight actually gets through is a little bit hidden behind the blue health busy lines but it\u0027s sort of mostly there so there\u0027s one big dip between 0 and 100 secondly but after that it it must of the time it sustains quite well whereas the default car took a bit longer to thing uh sort of a greater share of the bandwidth it also does not seem to be a case where the pendulous is fully limited because most both of the cards are operating more or less as at the maximum sort of available bit rate of 2.5 micro per second for instance doings 300 and 400 during that period yeah next slide please yes searching is a little the red and the red dots underneath the blue is the the line stable funada and it is mostly stable after I would say after a hundred second so there was a big dip between 0 and 100 second we can see a little bit from yeah sort of uh seeing\u0027s and it\u0027s cool yeah after that it\u0027s quite stable we can also see those rate profiles in the next slide reported by chrome yep okay yes yeah one thing I\u0027m not sure is and what\u0027s the kind of the measurement and interval of these bytes received person by the way everything is price receives all the numbers would need to be multiplied by 82 corroboree with the numbers in a previous page I\u0027m not too sure about at what time interval day and they sort of measure rate yeah but other than that both kind of both the rate and the resolution and frame rate do sort of correspond to what we observe in the center side in terms of the acknowledged received rate next slide so for this round offer I thought it would be also interesting to since we\u0027re mostly interested in reducing latency or at least confining latency for conferencing course so what I did is I basically plotted out the CDF of the queuing delay from each of the sessions and put them side-by-side for both the back-to-back "
  },
  {
    "startTime": "00:40:10",
    "text": "session and the parallel session to form a comparison I called out some numbers here but I think maybe the main observation for this round is that there\u0027s not really a main advantage of the not a scheme with respect to the default in fact for the back-to-back session since it was a little bit more aggressive it does it actually in you know service EF is somewhat worse compared to the blood curve but that\u0027s sad we can also see that if you take a look at 95th percentile of queuing delay we can limit it below a hundred millisecond for sure and and that\u0027s that\u0027s a I guess that\u0027s a good news post for the default option as well as for another so next we can move on to the cross and cross Atlantic cars so first one is back-to-back back-to-back cars so again this is maybe as a reminder this is between Austin Texas and Valencia in Spain and here in general for this round of experiments I we observe a little bit less interruption sort of hiccups over the network ironically this this set of results I was sending from my home using Wi-Fi and then backed off by a Google Fiber so it might be quieter compared to an enterprise environment so with that we see the similar behavior as we can see before for the default algorithm well if it does experience a delay spike then it will take a you know sort of on the order of tens of seconds to blend up whereas in nada even though it is subject to similar types of delay spikes the the ramped up back is much faster because of the accelerated ramp up feature in the grid adaptation algorithm the other thing you might notice is that there is a kind of a floor of the rancher time around 190 mini seconds so then you know all the red curves are kind of floating above the blue ones for for this set of results and for for plotting purposes all the delay and graphs are kept at 600 but as reported below and some of the spikes actually reach as high as 4.5 seconds in terms of round-trip time and next one so similar kiryl screenshots to corroborate our observations yeah interesting for this round and even though we see okay I\u0027m not yeah I\u0027m not too sure the screenshots for the default behavior because it seems to be a little bit different from from yeah basically the "
  },
  {
    "startTime": "00:43:11",
    "text": "the previous well no I think it\u0027s right so yeah basically we have two tips in the and default algorithm and then I guess this could shopped missed the second second ramp up duration for for that one yeah folder for the second grandpa so it\u0027s not qualitatively corresponding as well yeah next next one now so this is in peril of the default comparing against them sort of competing against nada in parallel so again we can see that both the delay and the packet loss observations from both of the floors are very comparable and for the default algorithm even though their target rate can sometimes ramp up very high and go back and forth the actual received rate is quite stable whereas for nada we see very similar behavior as before and next slide is for the chrome observation and there was one figure that we forgot to capture so I no longer have a bit rate but other than that we see for instance the chrome reported bit rate for the for the default algorithm one thing to note is that it is actually kind of below basically typically the max trade is as 300 K bytes per second so this one the received rates for default algorithm is quite stable but stays somewhat lower than its full potential and the NADA graph we don\u0027t have the reach figure but judging from the from the frame rate and height we see that there was one sort of dip in kind of the beginning part of the call and for the rest it was quite an stay next slide shows the comparison of the queuing delay so for this round for both of the back-to-back session and the parallel sessions we do see an improvement in terms of the queuing delay distribution of nada with respect to the default algorithm I suspect part of it is sort of attributed to the kind of the big spikes and then the low ramp up time and so overall if we look at and of their personal numbers for the the blue nada stream and it\u0027s also quite good it\u0027s actually interestingly for this cross-continental sorry cross-atlantic car if we\u0027re looking at queuing delay just because the center side is in a more in a less noisy wireless environment this performance is actually better than the previous one so it\u0027s not really dominated by the past arty keys were mostly measuring to yelling here I think that\u0027s the end of our my observation slides so in summary I think maybe some of the consistent observations is that the proposed "
  },
  {
    "startTime": "00:46:11",
    "text": "another algorithm and do have a fast initial ramp up to achieve the maximum allowed grades and typically that\u0027s within a few second whereas the default algorithm if it does go down it can take on the order of tens of seconds and it can recover and the proposed nada or some can recover quickly from temporary losses and queuing delay spikes it seems to be quite robust to hiccups in the network and it can effectively limits the queuing delay build up sort of a yeah I wanted to put in some numbers like nineteen percent out and queuing delay is below a hundred milliseconds and it does not really start the complete competing WebRTC flows so what we put both of them side-by-side so at least I\u0027m not too sure I think the environment we test is probably not a bandwidth limited environment given that both my home Wi-Fi connection is quite sufficient and obviously enterprise network it can be busy but it\u0027s probably bandwidth limiting so in those environments we don\u0027t really starve a competing call so that\u0027s probably relatively safe to mirrors early point or concern about how you know how it works against existing traffic over the Internet and for further investigations the main things of interest would probably to try it out over bandwidth limited connections there were some suggestion about try it out over LTE links that we haven\u0027t got a chance to fully investigate on and we\u0027re also interested in understanding kind of coexistence of if we have multiple cars running our algorithm how it works that is that something we can we\u0027ll be able to get evaluation results on even using our existing setup and the other thing of interest would be to try it out with running some kind of a TCP like background long lasting the file transfer type of traffic so that\u0027s kind of the update I have for for this meeting for the matter of person posting terms of evaluation and kind of some additional ideas I\u0027m also here to maybe gather further feedback and suggestions thank you okay thank you any questions searching what are the plans for trying this out in real production traffic sorry yeah so that was the question I asked in the very beginning frankly speaking we we currently don\u0027t have any we haven\u0027t laid out a plan to try it out in a large-scale deployment yet that\u0027s something obviously were we would be very interested in gather results on but also like these are the type of tests we I drunk we can I wrong with my colleagues kind of manually and I find them to be here always surprising me thinking bloggers have you know a little "
  },
  {
    "startTime": "00:49:12",
    "text": "bit awkward yeah so that\u0027s another thing we\u0027re very much interested in yeah either like learning from maybe Forks like you guys and understanding how to set it up or or yeah or you know maybe collaborating this piece yeah so I\u0027m quite open to suggestions - yeah my other question is in slide 11 if you can go back to that I noticed that on the top left side in the default case the acknowledged trait is very higher as compared to the target rate do you know why that oh no I think you so the blue curve is the target rate the red dots are here acknowledged rate we basically measure based on per packet feedback and average kind of gear side entry levels are incorrect yet oh okay sorry about it yeah okay yeah I think I changed color later ah yeah this is I use from in u.s. have you tried evaluating different combinations of default and not applause so let\u0027s say you have multiple flows and 30% of them are default and 70% are nada no I mean to be honest this and the parallel setting is the only one we tried and mix well make sure meaning one versus one that\u0027s I guess that goes back to the question of a larger scale deployments if we get to let\u0027s say configure I guess the ideal case is that if we get to configure you know different browsers running different traffic then we\u0027ll get to look at the mix that would be much more interesting yeah but we don\u0027t have results on percentage wise comparison yeah I think the percentage wise comparison results would be helpful because they would actually help you see that if you have most of you a battery RTC traffic or changing to nada what can you expect a network to change in so I see I see so your so ok thank you thank you I am I guess is that churching would be interested in collaborating if you wish to try this okay thank you I think that was a very interesting presentation it seems to be pretty stable it seems to compete pretty well with the earth flows that seems to be pretty nice behavior so thank you very much thank you are there any other questions okay in which case I press the red button and see if you disappear nothing happens don\u0027t know there we go all right okay so "
  },
  {
    "startTime": "00:52:14",
    "text": "the only other thing we have on the agenda for today is a very brief update on the on the congestion control feedback draft from myself so this is a draft which started out in this group we had a design team in this group for quite a while and then it moved over to a VT core and has been developed there and it\u0027s work that I\u0027ve been doing along with ahead Varun and Michael Ramallah the draft seemed pretty stable that the format hasn\u0027t changed for a reasonable length of time and there\u0027s been only details for quite a while now since the last meeting the only update has been to expand the design rationale section we had an action item to explain how this relates to the the other congestion control feedback mechanisms so that there were two two two two changes that there was a small ones just a dimension of the receiver estimated maximum bitrate option as draft already mentions the the timber mechanism forth for signaling the feedback and the the arian be seems to be a very similar mechanism so so I just added that in the same section to say it behaves broadly similarly and that there are details but but they you know the level of comparison we\u0027re looking at here they didn\u0027t make a whole lot of difference and they also added a paragraph discussing the relation to the the transfer wide just control extensions in the whole my drift now what that graph does is it adds an a TP header extension to each packet to give a single consistent sequence number across all the packets sent and then reports feedback based on those sequence numbers and then differs from the mechanism in this draft which uses the existing sequence numbers in the packets and then reports per SSRC the loss for each sequence number and the the paragraph just explains that essentially the difference is that if you add this header extension you get an extra eight bytes per packet over overhead in that format but it reduces the size of the feedback packets slightly and if your goal is to do a rate calculation for the whole flow then this simplifies the calculation because you\u0027re getting feedback for the flow as a whole if your goal is to do rate calculations per SSRC then obviously the format which reports per SSRC is simpler so that\u0027s that\u0027s that the fundamental trade-off here it\u0027s trading off hetero additional header overheads for simplicity if you\u0027re doing your aggregate level congestion control it also we also note that the transfer white feedback on a per SSRC basis is perhaps a more natural fit for RTP since everything else in RTP is done per SS I "
  },
  {
    "startTime": "00:55:14",
    "text": "see where as this aggregate feedback in here in the header extensions it you know I mean it can clearly be made to work but it fits less naturally into the framework so we added that description and made the usual set of updates to references comes in details that sort of thing Jonathan yeah drugs I mean I think the way I just experiment that last point fitting more naturally is if we ever actually get to a point where we want to have multi-point congestion control obvious having you know not that anybody\u0027s working on that algorithm it\u0027s a really hard problem but had this the CCF V feedback is much more natural because if you have multiple senders into the then obviously their their sequence them and they have each have a transport wide sequence number though those sequence numbers will collide whereas druid next thing them per SRC it just falls out naturally yes yes process as he gives you more flexibility for topologies yeah yeah well just in general if you more flexibility because it makes it easy to treat each flow separately if you want that flexibility that\u0027s a win if you\u0027re looking to treat the aggregates as a whole then you have to you have to map it back to a single sequence number space yeah so that\u0027s basically all I had to say as I said we believe this draft is stable it will be discussed any AVT core on Friday the presentation for me that I will give an AV took in a VT core is identical to this one except that this slide will say please can we issue a less call anything else on this drift okay that takes us to the end of the agenda I don\u0027t know if anyone remote has anything they wish to bring up it\u0027s always finish there\u0027s anything from anyone in the room who wishes to raise anything no okay in that case we\u0027re done thank you everybody "
  },
  {
    "startTime": "00:58:14",
    "text": "you "
  }
]