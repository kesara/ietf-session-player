[
  {
    "startTime": "00:00:48",
    "text": "Oh, yeah. Alright. See you for lunch. 6."
  },
  {
    "startTime": "00:04:59",
    "text": "please come in and be seated will be starting the workshop in one minute Thanks."
  },
  {
    "startTime": "00:06:08",
    "text": "Okay. Great. Let's get started. Hello, everyone. Welcome to the ACMRTF applied networking research workshop. I'm Frances Yan from Microsoft Research, AND HERE WITH ME IS MARIA POSBLOCKEY FROM Princeton University. Thank you all for being here and for participating in our workshop. As you know, RWA is more than a workshop it provides a wonderful opportunity and a unique forum for everyone who's interested in applied networking research from the world to come together to share insights experiences, exchange obvious. It's also a bridge between IETF and academia. So We hope that by attending today's workshop, you will not only find it, intellectually stimulating but also professionally rewarding as you make connections with people like minded. WE HAVE A FOLD DAY OF EVENTS And with many insightful research papers, a keynote and a panel discussion. However, before we officially get started, we are required to present to you some IRTF policies. So first of all, since ANRW is not a research group, within IRTF. the intellectual property rights disclosure rules from IRTF do now apply to ANRW means that you're not obligated to disclose intellectual property relating to the presentations or the contributions made to ANRW. and the audio and video of this workshop. will be recorded and posted online. and you agree to be recorded unless you indicate otherwise. And finally, by attending this workshop, you agree to the privacy policy of Iutf."
  },
  {
    "startTime": "00:08:01",
    "text": "and you need to adhere to the code of conduct. Okay, so let's get back to the main agenda today. first of all, Maria and I would like to thank all our PC Members. So we have 22 people on the PC this year, And together, they have done a fantastic job writing 64 reviews for 20 one papers. And among the 20 one submissions, we have accepted 11 papers So that's about 50% acceptance rate. we're right on target. thank thank you all for their contribution and the service. We'd like to also thank our support team Colin, Amy, Alexa, and Greg. especially a big shout out to Colin who is the IRT of Chair and the chair of the steering committee of NRW, we couldn't have ported self. without your help? and next slide. Start working. One second. Could you maybe press the right button? It's not working if you press, like, button Does it work? we have some technical issues when 2nd, maybe it was Can we share"
  },
  {
    "startTime": "00:10:01",
    "text": "Sorry about that. actually. I could disable Wi Fi then. is coming back. Alright. It's working again. Let's go back to here. Okay. Next, we'd also like to thank our sponsors. Comcast Akamai and Netflix for their generous financial support. for the travel grants. Thank you. And here's the overview of today's agenda. We're in the first session, and we're going to start with the keynote. followed by research papers on IoT programmable networks and network measurement. And in the afternoon, the second session where going to have additional papers on network measurement along with divers on security and privacy. In the last session of today, there will be papers on D And S and a beach view. So depending on your interests, you could select to attend some of the sessions where you're encouraged to attend all sessions. and with more details. It's our pleasure and honor to have our guest speaker, Professor Phil Lavis from Stanford University. He was also my PhD Code adviser back there. He will deliver a keynote It's the end of DRAM as we know it. then there will be 3 research papers on IoT Location Impact programmable infrastructure for networking, as well as reflections on active network measurements in academia In a second session, we will have papers quantifying the effects of TCP options quick and CDNs on throughput,"
  },
  {
    "startTime": "00:12:00",
    "text": "met mapping the Ukrainian rapidgy crisis, using Internet measurements. and security and privacy research opportunities for IETF Verticals. After that, we will host a panel discussion moderated by Maria with Alicia Zhang from UCLA. Chris Wood from Cloudflare and the job art from TUM. and the panel will be about what do we want the Internet to look like. in 20 years. In the last session, Metaviber is on PGP and DNS. So we're going to learn how to learn the barriers to work with public ra r, level data, level data, and a call for collaboration DNS integrations. repeatable name resolution with full dependency prudence. enabling multi hub ISPhapergine collaboration. And last one, practical anomaly detection, a large b bgprelated VPN Networks. Finally, some logistics. The program the complete program agenda of today, and the PDF would be Each paper can be found on the ANRW23 website. and the presentation videos of today will be made available on the same website after this meeting shortly. And for qanda, we kindly ask you to join the queue by logging into mid echo. if you have a question for the presenter. whether you're in person or remote. SO WE KIND OF HAVE THIS SINGLE SHAREED Q FOR BOTH REMOTE and in person participants because, you know, for FARENOUS RASONS. AND As networking people, we care a lot about fairness and queuing policy Right? So we have the implemented this 5 oq with drop tail policy means if you are at the end of the queue, and we run out of the time, that presentation you will be dropped like a packet."
  },
  {
    "startTime": "00:14:01",
    "text": "but you should always, you know, feel free to follow-up with the presenter after the presentation. and the meeting links to each session, the meta call links can be found on the IETF agenda or the specific page for NRW. Okay. Now before I hand over my microphone to the microphone to Phil Levis, let's and let me just give a brief introduction about him. So Phil is a professor in the CS And EE departments. of Stanford University. And as I said, he was my PhD Code adviser there, and it was a great pleasure working with him for 5 years. And Phil's research generally spends, you know, operating systems, networks, software design, especially for embedded systems. and the results of his work have been running on hundreds of 1000 of devices and served as the basis for Internet Standard. So Phil is no stranger to idea. And moreover, his research has been recognized by 4 test of a time awards, which speak for the long lasting impact of his research on our networking community. And with that, let's welcome our keynote speaker, Phil, Phil, please take it away. Alright. Yeah. So Frances asked me for an exciting or interesting talk, so I tried to come up with an exciting or interesting title. So giving this talk to a group such as this, if there are folks of you who work in a hardware, I would love to hear your thoughts. Tell me that I'm wrong. Please also ask questions anytime. So As the titles are suggested, I'd like to apologize first, which is I used to research network."
  },
  {
    "startTime": "00:16:01",
    "text": "a long time ago. As Francis mentioned, for example, stuff that I worked on came part of Ripple. There's some RFCs. It didn't work in security. TLS Rahr, congestion control with Francis, video streaming and with Puffer with Francis. I actually haven't done much networking research in the past few years. So I don't have a lot of cool interesting cutting edge networking stuff to talk about. So I apologize. instead, I wanna talk about something which I have found really interesting. and has been very confusing to me as somebody building software at the edge of the networks, right, at the end devices, I THINK IT'S THIS INTERESTING SHIFT AND WHAT'S HAPPENING TECHNOLOGICALLY which is gonna have huge implications to the applications on the Internet. And so I hope You did 2. So here's the basic summary. Our networks are gonna get faster. processors are gonna get faster. but we're not gonna get more RAM. The cost per bid is flat. It's been flat for 10 years. you haven't noticed, and it's not gonna go down. The performance of RAM is also almost there, but it's also gonna flatten, probably the next generation. Both it's been flattened latency for a long time. It's gonna flatten in throughput unless they're some really huge shifts, and it's not clear where they'll happen. We'll see. What this means is if you look 10 years forward, computers are gonna be really different. and sold their applications. And so kind of the the tagline is it's the end of DRAM as we know Oh, there we go. Yeah. Okay. Great. So here's the summary for the rest of the talk. I'm gonna explain what's happening with scaling. Why is it that RAM isn't getting cheaper? Why is it not going to get cheaper? then I'm gonna talk about what's happening with this performance. what's happening with latency and throughput? Why is it Ram latency going down? In fact, why is it going up? If you have an L3 cache miss, it's going up. And also why throughput is gonna go up for a little while, but not much"
  },
  {
    "startTime": "00:18:00",
    "text": "longer unless there's some really big shifts where it's unclear they'll happen. What this means going forward is there's gonna be 3 kinds of memory. and our computers. and I'll talk about what those are and what their properties are. And at the end, I'll talk about a little twist. sort of this evolution of the PCIe bus, CXL, compute express link and kinda how that will change things in a bunch of ways. start at the beginning. Why is RAM not getting cheaper? So here's a plot of, you know, versions of DDR, 6 is still, you know, hypothetical. It's in work of JETEK. So here's DDR 1 through DDR 5. And what you can see is know, over the past 22 years, the throughput has gone up tremendously some latency improvements in the beginning. But For the past 3 generations, cost has been flat. cost per bit. If you look at sort of a standard sized dim and they map the dim costs and you compute cost per cost per gigabyte, it's been 2 to $3. Now there's huge variations in this in time because The RAM market is very sensitive supply and demand. But if you sort of average it out, you get 2 to $3. So, for example, a few months ago, even DDR 5 was very expensive because we're a shortage of it. Now there's plenty of it, so it's cheaper. but if you just kinda average out cost is flat. So one way you see this manifest is if you read recent papers from many of the hyperscalers. You know, they say, hey. Ram is 40% or 50% of a cost to the server these days. and it's going up. and it's been going up. And that's a big concern. You don't wanna build a server when it's 90% of its cost is a ramp. So why is this? is this happening? I think the basic summary is a DRAM bit, is a transistor. right, in a capacitor. That's That's it. Like, you can't make this circuit much simpler? It's about all you got. And so unlike with the processor or where there's lots design flexibility,"
  },
  {
    "startTime": "00:20:01",
    "text": "the cost of RAM is basically the cost of manufacturing transistors. And if you look at the cost of manufacturing transistors, somewhere around 28 nanometers, it became flat. The cost of actually manufacturing a gate stopped going down. so we continue to be able to make gate smaller and smaller dates You know, we can continue sort of Moore's law. You know, it's not scaling instead. We can keep on going to, you know, newest, smaller and smaller nodes, but the cost per transistor of manufacturing is not going down. So I can make you DRAM that's twice as dense. It just costs twice as much the same cost per bit. because, again, DRAM cell. is just the transistor. And so for a really long time, no. all of computing was able to chase you know, this exponential decrease in in cost. Right? So when a new process appeared, would move to the new process. because, gosh, suddenly, all of your existing things cost, say half as much to make. Right? You serve use the size. You get twice the transistors per unit area. great. Everything is twice cost half the amount to make. Therefore, you move to the new process. great. And that then gives you those savings on everything that you're selling. but this And, you know, you talk to architects, and they say, oh, yeah. Didn't you know? and it turns out lots of people didn't know. we're now kind of fighting this fixed cost per transistor. And so here's sort of the if we sort of chart, the the size of process nodes, you know, over time. Something happened, right at 28 nanometers. THAT'S WHEN SUDDENLY THIS SCALING STOP. So what happened? So 28 nanometers is when we went from planar transistors to FinFET."
  },
  {
    "startTime": "00:22:02",
    "text": "So, basically, things are getting so small. We need more surface there. I'd be able to actually control, you know, the voltage across the gate. And so this is when Gates started becoming three-dimensional. And so they started becoming more expensive to make. So these next nodes, you know, like, the next ones that TSMC is putting out. even FinFETs aren't enough. We have gate all around where something there's multiple fins or cylinders, because you really need to affect this surface area to volume ratio to control the gate. So making these transistors is getting really, really hard harder and harder. It's not just simple scaling, you need new processes. And so, basically, Right. above 28 nanometers, we used planner gates, now raising FinFETs. and soon get all around GAA. and and other problem is that as we make our gates smaller and smaller, It gets harder and harder to actually do the manufacturing. This is totally like a physical sharing engineering problem. So if you look at sort of the number of steps it takes to produce you know, a dye at different sizes. you realize weight. like like like just going as we're getting to smaller and smaller nodes, Just the amount of cleaning you have to do, the number of logic other steps and processing steps you have to do, is just going up. So can make put more transistors onto a dye We But gosh, we have to now suddenly run many more processes in order to actually produce them. It's not just turning a crank and scaling the process down. and this has all kinds of other implications as things get smaller and smaller. So Here's sort of a nice chart of you know, now allowable 20 nanometer particles per milliliter chemicals. And in 2022, it's going under 1. Right? So you can have, like, 1 you can anything that one 20 nanometer particle in your chemicals. So everything has to be cleaner. Everything has better it's getting harder and harder."
  },
  {
    "startTime": "00:24:01",
    "text": "and it's not just that. So, you know, we make dies and your chips through lithography. So, basically, you know, you put a mask and you shine light and then you do a process where, gosh, you know, where the the line the light shine, we can then strip that stuff off. And, you know, there's all kinds of steps hey, do this with etching and metal, etcetera, etcetera. So if we go to sort of patterning below 28 nanometer 28 nanometers. If you look at the frequencies of light Basically, there's a point where stopped being able to use visible light or ultraviolet light, We're now at the point where we're using extreme ultraviolet light. So 10 to a 100 nanometer wavelengths. So way out there. on the spectrum. And so one of the problems with extreme ultra violet light is that everything absorbs So the system is gonna stop reading the vacuum. You can't have any lenses. because the lenses will just absorb the light. us. Everything is through mirrors. And so this diagram on the bottom in the center shows you kind of the series of focusing that go on through a long series of mirrors within an extreme ultraviolet lithography process. And that's what you see also on the bottom right. That's an actual shot from an EUV lithography device. So the process to make these is pretty crazy. And I'll talk a little bit more about that. to give you an idea, like, what it even takes to make the light to make these Just transistors at this scale So so The system is dropping tiny droplets of molten tin, which then get zapped with 1 ultraviolet laser, which causes these droplets of tend to spend out into, like, a disk, which then they can get zapped by another laser to see if I'm a powerful which then gets them to produce 13.5 nanometer is happening fifty thousand times a second. So the process that goes even to make the light for this kind of manufacturing is understand why it's expensive."
  },
  {
    "startTime": "00:26:02",
    "text": "And I mean, you should if you for an opportunity, you should go read about how this works because whenever I do, it just boggles the mind to me. Like, the kind of engineering that goes into this is one person puts it the precision of these of this lithography is so good that we were shining a laser at the moon, like we could hit a quarter. Right? Because it has to be at these scales. For 20 years, people really wondered whether it was even possible. whether this is a possible engineering feat. it took, you know, tens of 1,000,000,000 of dollars, 20 years of research, but as another person puts it, there there is no plan b. if they couldn't do it, then we would stop being able to make chips smaller. the mail happen. But so the brief summary of all this is what the crazy engineering that's going on behind authority today is, We can continue to make smaller transistors, but the per transistor manufacturing cost is lot. because of the degree of engineering that's required to do so. So, If DRAM costs are flat, Why aren't CPU cost flat? We do see bigger CPUs, faster CPUs, So the short answer is the per core call is going down slightly. So here's just a series of AMD server class crossers Roam Milan Genoe and the Bergamo, Cores are getting faster, You have better accelerators, all kinds of stuff you can do in the design. It's also the case that design is a larger fraction of the cost of of of of CPUs. A lot of design work goes in there unlike, say, RAM, which mostly manufacturing and very thin margins. So There's stuff you can do with CPUs to use your transistor's more effective like like like in order to improve performance. Shiplet designs, for example, is what, you know, MD has been doing for a while. makes design a lot less expensive rather than designing one big chip for design little triplets and then stitching them together on an interconnection fabric. So they're coming up with ways to make the whole process of designing that ship cheaper,"
  },
  {
    "startTime": "00:28:01",
    "text": "which then means that, gosh, we can continue to have faster chips. sort of You can see sort of the the cost of the process is going up. So in summary, The price per bit of DRAM it's not going down soon. It's basically the cost of manufacturing transistors. there isn't much flexibility here. The cost of producing transistors spending is going up because it's getting harder. So what that means is if we want new RAM that has lots more bits, we're not gonna get it with how we make it today. We're gonna need new materials. You could do it soon through something like new fancy material. Who knows what? There's nothing on the road map. Right? So even if something appeared tomorrow, we're not gonna get it for 10 years. I was chatting to somebody who's talk talked with Micron on the way the Micron person put it was. Yeah. We we've you know the periodic table, we've tried everything. Right? There's no material coming down the pipe So there is Intel Optane memory, which, unfortunately, is just, you know, this pursued for us at 8 or 9 years, and Micron, they were just shut down. So that was an idea of, hey, maybe we can do things a bit differently. That gave you a 2xdensityimprovement. Turned out that wasn't quite enough for the market. you know, maybe it'll return as pressures get harder. It's also non volatile. There's a bunch of other things, Maybe Octane will come back. We'll see. So there are maybe some long shots So RAM, basically requires one lithographic step, you know, per sort of layer on, like, flash where you can kind of a magic of flash as you can make many, many flash bits with a single lithographic touching step, right, or single graphic step. Basically, you're making bits down in the in the z axis or on the y axis. Maybe somebody will come up with some fancy way of making DRAM where we can with a single lithographic step make multiple layers. that would you know, that will be a pretty amazing thing, and whoever does that patent sit"
  },
  {
    "startTime": "00:30:01",
    "text": "make vigilance. So People are trying. I don't know if it'll happen. But that would be something which can maybe change this. I We'll see. So one thing is that DRAM is flat because you think just how highly engineered it is. It's this very simple circuit. Let's just make it as dense as we can. Processers and networks still have some runway. generally, NICS are a few nodes behind. other processes, And there's still lots of room for acceleration in CPUs. if you look at a modern CPU like a Sapphire Rapids, There's a ton of accelerators in there. think we're gonna see that continue because that's how you get efficiency. terms of compute per watt. delta. Next. Alright. Great. Again, if anyone has any questions or thinks I'm wrong, please tell me. welcome to commodities. So We DataMax. There we go. Great. So, that's cost per bit. that's capacity. Let's talk about latency and throughput. So, again, let's go back to this table. Here's DDR through DDR 6. And so you can see latency is flat. Right? So, I mean, it's going down a little bit, you know, 79, 74, 72, So why is that? So first of all, Walt, the actual latency of the DRAM of the dim itself that's going down slightly, the observed latency of a processor is going up. So here's, for example, is looking at, you know, a series of 4 Generations of Intel Proster, Scarlet, Cascade Lake, ice lake, Sapphire Rapids, The observed latency, of a level of an L3 cache miss, hitting main memory is going up. from 87 to a 142 nanosecond."
  },
  {
    "startTime": "00:32:01",
    "text": "So the reason for this is these processors are getting bigger. Right? And they have more cores and they have larger caches. So the cash miss rates are going down. So the overall performance is going up. right, if you have good speculation or prefetching, whatever it is. But the observed performance of an L3 cache miss. So think like random lookups through a hash table Latency is going up. you've got lots of cores, your throughput still goes up. you know, in that case, but from a latency standpoint, It's going And so if you look at, like, what a Sapphire Rapids you know, sort of just came out a few months ago. Prosser looks like it starts to kinda become clear. So here's a Sapphire Rapids die. so it has a whole bunch of CPU titles on it. It's got its memory control tile, each CPU has its own L1 and L2, then there's the shared L3. You can see all the connections on it. There's the 5 for talking to other Dyes for, you know, for, you know, sort of the inter processor inter the intercore interconnect. PCI lanes, UPI, you know, all kinds of stuff. Of course, the chip is 4 of these dies all stitched together. And so know, if you're accessing L3, well, an L3 miss, you're gonna have to go across this entire chip, to see if it's there. And then if not, you might have to also progress depending on where the DRAM is hooked in on these memory control files. Right? you're gonna have to go prep potentially further. So there's no question that overall performance throughput is going up. but there is this throughput latency trade off. let's talk about the throughput of the dims themselves. So this is kind of the performance you can get from sort of the top end, you know, speeds of DDR. So say, if you have, like, a a modern DDR 5, you can get something like 57 gigabytes per second. out of the gym. Right? In served us a theoretical maximum, of course, in practice, to be a bit lower."
  },
  {
    "startTime": "00:34:03",
    "text": "So that turns out to be 7.2 giga transfers per second, So this gets now into sort of how these signaling on these dims actually work. So turns out turns out that DRAM data lines are single ended. Right? So a single wire unlike clock lines that you're differential, there's two lines. Bye. So it turns out, in differential signaling, we really know what the limit is. which is future I really, really hard and you put a lot of money into it, you can do 200 gigabits per second. pretty you can achieve that. A 100 is pretty straightforward with really good engineering. 50, yeah, you can just do But it's really tough to go past 200. at some point, and you're talking about 5 picoseconds, gates only switch so fast. Single ended though, really, it seems like maybe the practical limit is around 9.6 The reason is with differential signaling, you're just comparing 2 lines. Right? You can say which one is higher. Is it 0 or is it 1? if there's some interference or any kind of noise or some kind of, you know, sort swing on both the lines. Both of them see it, and you're fine. With single ended though, used to have one thing. You're comparing to a reference, and it's really, really tough to keep that stable. So 9.6 could give a per second for a single data line. So DDR 6 is trying to push this further with some buffering. Right? So maybe you can blow the signal stabilized, but that also increases latency. And so the brief summary is DDR latency is not going down. If anything, we're gonna see this latency going up a bit. due to more complex caches. DDR is also reaching its signaling limit. So the hope is DDR 6 can do 12.8, You know, I hear people say they think 9.6 is sort of the practical limit, to engineer this, Maybe you can go to 12 point 8 with dieting and buffering. Incy, we'll see. So there is one escape hatch on this."
  },
  {
    "startTime": "00:36:01",
    "text": "which is we could shift from single entity to differential signaling. on DRAM. So we could have 2 data lines and just do the compare. pseudo car completely new DRAM Designs, There are no, if you look at JETEK, there's no current plans to do this, maybe they will. We'll see. There's some historical concerns about this of, like, whether there's you know, intellectual property stuff. It's the IETF. knows how to navigate well, but there's also concerns about maybe going to differential we'll see see what happens. if that happened, oh, then maybe we get some more throughput. We're not gonna get that our latency will get better So In summary, right, the cost per bit of DRAM isn't going down anytime soon. I'd say at least 10 years, and then who knows how long. DRM latency is not going down either. DRM Bandwidth is gonna go up a bit, but there isn't much room left. you know, even if DER 6 or 7 8 does go to differential signaling, this is gonna be, like, 5 years out at the very earliest. which means, you know, earliest 8 years before you can buy it. And all this really just boils down to the scale at which these things are operating and really just, like, the lower level EE signaling and all that kind of stuff that's going on. Okay, so what does this mean? if DRAM is flat What what's gonna happen? to memory. So one way to look at it is that you know, when we hit clock frequency as a limit, And so the the beginning the end of the nineties, we couldn't really get much higher than 3 gigahertz in practice for the designs. What happened? So we hit some physical limit. and then systems just moves in another direction. And so What happens if DRAM's capacity latency and throughput are flat? But everything else, your network, and your processor continue to get faster."
  },
  {
    "startTime": "00:38:02",
    "text": "So just a little sort of back to the onboard calculations here. So today, it's pretty straightforward to get a 200 gig Nick. 400 gigs will be soon. So at 400 gigabits per second, a 4 kilobyte packet is 80 nanoseconds. So it's less than an L3 cache miss. And at 400 gigabits, a 64 byte packet is 1.25 nanoseconds cons. So these are really you know, from a software standpoint, these are really time crazy time skills. Furthermore, if you look at a modern server processor, how much aggregate DRAM bandwidth that has, 400 gigabits is 10% of that. So that means that if I wanted to echo just simply echo packets, Right? So write the packet into DRAM, then read the packet out of DRAM, that's gonna be 20% of my whole system memory bandwidth. And you see the implications of this and some, you know, high performance systems that people built. This is a nice slide deck from freebsd conference 2021, Netflix, on how they stream 400 gigabits per second. you know, in their open connects and their devices. And certain they make this interesting note. There's this 4 x memory ification. Right? If they have the DNA data off the disk in the memory, then they have to read the data from memory into the CPU, then they have to write the encrypted data back into memory and then DMA the data, from the memory back to the NIC. And that's just assuming there's no compression or other processing going on is like a 4 x cost on the memory balance. So don't think the 2 said before or 20% thing for, you know, 40% just to stream stuff. And, you know, I remember I was chatting with an engineer at Google, talking about, you know, some high performance stuff they do. And they said, look, we architect our pipeline so that the data enters the cache only once. because anything else and it's a bottleneck."
  },
  {
    "startTime": "00:40:03",
    "text": "So given this, There's this flipside which is if, you know, cost per transistors is flat, and we can't you know, new nodes are becoming more and more expensive. The way that CPUs are gonna give you performance, the way they do today even is through accelerators. computational accelerators. So you look at, for example, Sapphire Rapids, and there's a whole bunch of new accelerators, things to make analytics fast. or things to make memory copies faster, things to do all kinds of stuff with matrices. Right? for some preparation for machine learning. So, In the end, in silicon, compute is cheap moving data is expensive. so we can make things blazingly fast But then the challenge is, You know, how do you feed it? So you can, for example, add lots more ALUs to use under your processor their cheap. You can add all kinds of accelerators is really fast over large amounts of data. But at some point, you can't feed the accelerator fast enough. DDR doesn't have enough bandwidths. to match the computational speed of CPU that has a lot of accelerators in And in fact, you see this today. look at a GPU, right over a TPU, They don't have DDR. Right? They have high bandwidth memory, HBM. so what's hybrid? Memory is basically DDR with a lot more data lines. So DER 564 data lines, HPM3, which is like on the NVIDIA h100. has a 1024 data lines. So 1664 bit wide channels. The trade off here is latency is higher, right, like, 300 nanoseconds or so But, you know, if you're streaming huge amounts of data, Like, if you wanna stream megabytes of data or, you know, 100 of megabytes of data, you don't really care about those 100 nanoseconds leads So one of the challenges. And, again, this gets back to this idea. This is about, like,"
  },
  {
    "startTime": "00:42:00",
    "text": "manufacturing the physicality of these devices and how signals work, you can't run a 1024 copper traces from your memory module. on a printed circuit board. Generally speaking, So practical kind of finished traces you can make. today, you know, maybe 0.152 millimeters. So if you have a 1000 of those, that's, you know, like, a 155 millimeters of just trace without even spacings between them. Right? So that's much bigger than your chip, you can't do i. So instead, the way you integrate an HBM with a processor is in silicon. So you basically make effectively a PCB, a print circuit board in silicon. because then you can do things that lithographic scales, something called an interposer. So you have your processor that's built with its you know, particular node. It's particular manufacturing process. Here of your memory module made by say Samsung with the particular manufacturing process, they can be different. different nodes. It doesn't matter. Then they have, you know, Pads, then you join them onto this interposer just like a little silicon scaled PCB to connect them Right. So it works. It's great. it's expensive. Right? So we now have these two parts. We have this third part. We manufacturer, and we have to, you know, have to, actually assemble them and, you know, Turns out you can have lots of failures. But you can do it. This is also why GPUs are expensive. It's also why, for example, the know, SAFIRE up its max. that has integrated h Yeah. It's an expensive high end processor. So in fact, most recent Intel Processors. The they have a model of the SFR rapids with an integrated 64 gigabyte HPM module. which is kind of interesting. And so the idea is, hey. If you're gonna be doing something that's arithmetically really intensive over large blocks of data you can buy intel Prosser that has an HBM module and then stream out of that. So"
  },
  {
    "startTime": "00:44:01",
    "text": "coming to a processor near you. I my guess is this is gonna continue because at some point, if we put accelerators into our processors, The only way you can feed them fast enough the HPM. So HBM is interesting. because it allowed you to circumvent these bandwidth concerns per pin, etcetera, really get much higher bandwidth out of your memory, but it raises an interesting point. So if we look at kind of a modern memory hierarchy, you know, from registers to something hard disk drive. Where does HBM Sid. in this hierarchy And the short answer is it kinda doesn't. right? It has higher latency then regular DRAM, but it also has much, much higher bandwidth. than regular Gmail. So it's not in this strict hierarchy of you know, small and fast to big and slow. It's also a limited size to 64 gigabytes rather than, you know, like, half a terabyte or a terabyte. So I think going forward, We're gonna see, like, this pressure on DRAM and the need for higher bandwidth. to be able to process this stuff. It's gonna push systems towards 3 types of memory. The first is, you know, DDR that we know and love. So think of this as latency optimized memory. So this is the memory where random access has the lowest latency that you can get for something of significant size. We're not talking about, like, sram caches or something like that. So think 100 of gigabytes maybe a terabyte of ram, 100 nanosecond latency, you know, with on the order of 50 gigabytes per second. We also have capacity This basically flash. Right? So today, latency is in the 100 liked under a 100 microseconds I think that that can actually come down a lot. A lot of that is your FTL layer. maybe 10 microseconds seems feasible if we push hard enough. down within the tens of gigabytes. per second and the size of terabytes"
  },
  {
    "startTime": "00:46:02",
    "text": "So this isn't that strict in the hierarchy issue before. This is what we think it was like, oh, the smaller and fast and the the bigger and the slaw. But then the 3rd kind is bandwidths. So memory that's optimized for throughput. So this is HBM today. So think tens of gigabytes 100, so multiple 100 nanoseconds of latency. but bandwidth and say 800 gigabytes per second. HPM 3 module can do 800 gigabytes. per second. So be more than the aggregate memory bandwidth of all of the dims attached. to your processor. so if we look forward, And this pressure is gonna really push us to have machines that have this mixture of memories. latency, capacity, and bandwidth. Each of them is tied to a different compute element. For example, capacity elements, you know, usually are tied to the CPU. That's where our disks are attached. but perhaps it's attached to the IPU, the smart NIC, you know, the DPU. Our latency memory, of course, things like our smart have some, but it's mostly in the CPU. That's where, you know, say, terabyte of RAM lives. Now bandwidth memory, well, We're seeing it appear on CPUs. It's been on TPUs and GPUs for a long time. things from a networking standpoint, So networks are increasingly in wanna transmit data to get it into bandwidth memory basically, for accelerated processing. So you're seeing this today when you're doing training within a data center. You're shipping huge amounts of data to notes, then they can do their training on CPA And so what does our networking stack look like when really What's gonna happen is we're gonna load our memory into the DRAM on a smart NIC before we then offload it, you know, to GPU's high bandwidth memory. Is there gonna be a point where our Knicks actually need high bandwidth memory in order to be able to, like, get stuff back and forth fast as the mix get faster and faster. So"
  },
  {
    "startTime": "00:48:04",
    "text": "This is my my hypothesis about where computers are going. And I think there's one interesting twist maybe, you know, some of you've probably heard a bunch about compute express ring, link CXUP. And so So it turns out the speeds and the capacities that we're talking about, you know, the the workhorse of in the backplane for most, you know, servers and desktops and stuff and slaptops today is PCIe. PCI Express. So One of the problems with PCIe is that it's high latency So Basically, you know, depending on who you ask and speed up, the minimum latency you do, like, a PCIe operation is around 8 100 maybe 500, you know, nanoseconds or so. So when you think of this is if you got a 400 gigabit Nick, That's 40 kilobytes. of data. That's a lot. And so you could think if you have, like, some 8 kilobytes jumbo frames, that's 5 of them. It's just the time to even talk to the Nick. being said, PCIe is pretty PCIe is pretty high throughput. That of PCIe GEN 5, which we're now seeing, you know, roll out on you know, Genoas and Sapphire Rapids, 16 lanes, like, you plug a nick into. is 480 gigabits per second. So there's some great work. You recently served on the academic side looking at kind of what the limits are and finding, you know, 480 gigabits per second is not quite is really not enough to do really drive a 400 gigabit, Nick, So if you look at something like the NVIDIA BlueField Three, actually uses 3 32 Gen Five lanes. assist special, you know, open compute interface, Nick interface to do that. So that's fine, but we can do it. We've got the PCIe bandwidth. The thing though is that PCIe is just a data bus. So, basically, You can read and write memory across the bus, but the two sides of the buses are independent. Right? So there's no coherence across them. You read something. It might change. You'll never know."
  },
  {
    "startTime": "00:50:01",
    "text": "So this means in practice is that if you wanna check if a Nick has packet, you have to do a PCIe operation. You have to do a read. that takes 800 nanoseconds. So as people are pushing these super high performance systems, these latencies and kind of the amount of data that can be passing and are starting to become initiative. another way to think was, like, 800 nanoseconds is approximately 400 feet of cable and propagation delay, That's actually It's actually pretty big, right, say, within the data center. So what's CXL? CXL is a replacement for PCIe. Same physical layer, same signaling, same speeds, all that kind of stuff. It's great. You can plug the CXL car into your PCE slot, you know, should work. One of the things it does, it simplifies all the protocols down so that pay now suddenly the minimum is about 200 nanoseconds, is good. A factor 4 is nice. But the sort of more interesting thing, and I'll talk a bit about the implication for this for networking and network cards, etcetera. is that CXL supports cash profure and access to memory. So what this means is that two devices like your CPU and the NIC connected over CXL can have a cash coherent view of each other's memory. Really, in particular, the processor, you know, kind of the Nick. And so what that means is that when the processor reads something from the Nick, can pull that value into its cache, It And then if the Nick doesn't change it, it can continue to safely read it from its cache. And you don't have to do a PCIe operation or a CXL operation in order to see that the value hasn't changed. The other side, now the cost of this, the other side like to Nick when it wants to write that value, It has to invalidate the cash line on the processor, and this takes some time. So you can imagine depending on your expected read by trade offs, you can figure out how you wanna cache these. just to give you an example, just try and make this concrete. for PCIe. So I've got my CPU and my NIC."
  },
  {
    "startTime": "00:52:01",
    "text": "And then Nick has a variable in memory, which is what is the tail of the scriptor, Ring containing packets. Right? So what's the last packet I've written? and as it's receiving packets. And so the CPU, wants to see here the new packets, it'll go and it'll issue a read operation over the bus. or PCIE saying, hey, the value of the descriptor tail variable? Get a result back, Okay. Great. Then we actually do a memory read. gets pulled into cash. Awesome. Next time we wanna check if there's a packet, we have to do the exact same thing. to do a PCIe operation because that value in our cash Could have changed like this not in any way tied to what the Nick is doing. The Nick could have written to his script or tail, and we don't know. have to read again. This invalidates the value in the cash. We can then check, oh, yeah. There is no packet. Great. Great. There's nothing. So that means every time the c CPU wants check if there's a new packet, has to read memory over the PCIe bus. So think 800 nanoseconds since they're blocked. So with cash coherence though, Right? This changes a little bit. So the CPU can read the descriptor tail from the neck, Great. load it into its cache, So it's reading from, in its address space, Nick Memory, it doesn't go into the CPU memory. It's reading from NICN memory. Now the CPU cache references memory and the network interface card. Great, then it can just read from the cache. Now if the Nick decides oh, if the Nick receives a packet, it needs update the descriptor tale, it'll send a cash and validation CPUs. This takes some time because it can't sort of complete until that finishes that they remain coherent. But then later, when the CPU goes to read, it no longer has the value in this cache, so it knows it to do a read. And it does so, and it loads into the cache. And one interesting aspect of this is that"
  },
  {
    "startTime": "00:54:01",
    "text": "besides things like Knicks and GPUs and all kinds of peripherals, there's this idea that CXL and its lower latency, means that it can support memory devices. So, like, a device that just says memory in And the whole idea is, oh, wait. Like, if TDR bandwidth is a problem, maybe we can choose our PCIe link. they're differentiated signals. You know, they've got lots more bandwidth per pin. Awesome. If you look at an AMD Genoa, it has about 4.4 terabits per second of DDR bandwidth and about 4.7 terabits per second of PCIe Maybe we get more bandwidth this way. There's lots of buzz about this. People are writing all kinds of papers, folks in my of Azure exploring it folks at Facebook, preferring it. One idea is like, hey. Let's take a big box, and let's put Tens of terabytes of memory into it, and then plug it into all these computers with CXL. suddenly this cache coherent shared memory pool. I'm a little skeptical that this will work, But, I mean, I think from the perspective of networks and how systems interact with their network cards, CXL is gonna really change how things work. in the sense of the speed and the latency we'll be able to get from our Knicks actually means that the CPs will be able to keep pace with how much faster the networks should get. And so, as I sort of mentioned, you know, cash coherence essentially allows this low cost coordination between these independent compute device. is remember, compute is easy. Moving data is hard. And so the fact that we can have something like your NIC directly connected to your SSD such that then all the CPU can do is look in the memory of all peripherals, which are all cash coherent, And actually, you can take the CPU out of the data path, such that CPU can have the NIC directly transfer things to a disk. So imagine the like these network applications, storage applications that don't actually touch CPU, which is about data transfers. in transacts between these devices. So in summary, In 10 years, computers are gonna look really different."
  },
  {
    "startTime": "00:56:00",
    "text": "the, you know, the DRAM wall that we've hit, the scaling wall, means we're gonna push towards these 3 kinds of memory capacity, latency, and bandwidth. I think the CXL in this notion have actually been able to connect them through a cache to hear an fabric, is gonna be an interesting twist. Not sure how it's gonna play out, but I'm excited to see. It means we're gonna start building these much higher bandwidth applications. We can do things like start moving these large language into the large scale machine learning models. I think it's an interesting and exciting time. Since summary, Processors and networks can get faster for a while. It's great. but gramism. cost per bits flat, It's gonna be flat for a while. At least, you know, my guess is 10 years. unless, you know, some fantastic new manufacturing process or material appears. There's nothing on the horizon. performance is also flat. And so in 10 years, our computers are gonna look really different. applications they run are gonna look different, and it's gonna have really big implications to the Internet. So it's the end of the DRAM as we know it. I feel fine. hope the Internet feels fine. And, yeah, happy to take questions. out. name is John from Futureway. You mentioned about the scaling or the lack of scalability considering throughput latency, and one other factor cost in dollars. how does power consumption change or vary when you consider all these. Yeah. Yeah. Yeah. -- had a great point. So this is this is an interesting one. So it turns out that power is actually a really big concern on on DRAM. So if you look at DDR6 DDR5,"
  },
  {
    "startTime": "00:58:00",
    "text": "went from, like, 1.2volts to 1.1volts. need all kinds of, like, crazy voltage regulation where they're just trying to push the power lower and lower. because it's about charging those capacitors. Right? So yeah, I think Also on modern servers, RAMpower is significant. I think you can feed it in terms of, like, TCO least my sense is that's not like, you wanna store that data. certainly, in terms of individual DRAM elements, right, and heat dissipation, and just being able to, like, do I just have to put more and more dims? But I can't because at some point, it can't run the traces. That is definitely So I guess what I would say is I think that is definitely hard a lot of the design of DIMMs and DDR is governed by that. but that seems like a problem where we still have still have you know, leeway. We still have runway. Right? People can engineer that. There are other things where, like, we really we've got no We've got no Plan B. So so Yeah. Singapore back. Yeah. Yeah. My name is Mitch. Actually, I have two questions. First is there are recent, and it seems like quite intensive attempts to meet computing memory. On one extreme, it's something like computing memory, On the other things, Mark's team example is probably Travers is doing this. A lot a lot, of course. very fast on cheaper, and cheaper is really big interconnect. And some amount, small amount, but very fast memory near each core. So how does it fit into this picture? And the next question kinda couple of question is that it seems that a very significant part of computing, if future will be some kind of machine learning, and machine learning has very specific memory access patterns and, in particular, like, input and parameters, they have different access patterns, and probably there are some ways to optimize that and how it's going to influence memory."
  },
  {
    "startTime": "01:00:01",
    "text": "So let's go to the first one. So Great. Right. So I think, actually, like, an interesting example here is, like, Samba Nova. Right? So the SambaNova processors, like, they're really about, oh, and it's kind of like the and, like, the lightning bolt So a great idea there is, hey. Rather than have our memory in our compute, why don't we intermingle them working in intermingled compute tiles and memory tiles. Right? So what's important is in that, that works really well for streaming computation, so I'm gonna actually progress this data through the pipeline. So for lots of, like, machine learning and data analytics. So in that way, Yeah. I think we're gonna see all kinds of new architectures They don't get more memory that way. Right? So I think in terms of we're thinking about, like, optimizing machine learning. I mean, look at, like, what an h 100 can do or the GPU can do, like, Yes. We can feed these things. We can bring memory closer to the compute. that way, I think we'll be able to get and continue to get high throughput on those classes of systems. That's about really, like, optimizing the compute elements. Yeah. And and there's not point about the memory just gonna Sorry? I'm sorry. I'm trying to remember the I'm trying to remember the first part of your question. Yeah. Yeah. So let's so but I think with respect to, like, things like smart memories, like, I push my computation to my memory itself, if there's compute on the memory cells. That gets tricky, Rex, at some point, computation seed locality. I have to be computing locally. But look. memory actually isn't local. Right? I strike my 64 by cash lines across my dims so I can get full bandwidth. So it starts to get a little weird. We're like, wait. I'm actually having if I wanted to do a computation across say, multiple cash lines, either need to, you know, sort of strike my memory differently to support that. actually need to bring the data, you know, to the core. where the dims connect. Yeah. Thanks. That's very good point. To to do computation, you have to bring things together Right. Right. So and think for smart memory of, like, oh, I'm gonna do this on a cash line. Sure. Whatever your your"
  },
  {
    "startTime": "01:02:02",
    "text": "Scripping is. But then that gets in this trick of you know, you're stripping now can affect your bandwidth or or or what size data item can you achieve your maximum No, sir. Does that answer your question? The next question was that There are very specific access patterns machine learning applications because inputs and parameters they access differently. And are we going to see some optimizations for that? Yeah. I I think, absolutely. I think, like, the GPU and the GPU folks, Absolutely. They're pushing that also see it for different kinds of machine learning, like graph neural networks, really hard in all kinds of ways. And you have very different access patterns, you know, than, say, filters over images and convolutional networks. Yeah. people absolutely you know, it's not just about optimizing the compute or designing the compute, it's also even designing your memory controllers and your memory layouts. Absolutely. think we are seeing Thanks. Yeah. Hi. I'm Jonathan Holden with Cloudflare. not a hardware person. Is there ever gonna be a world where we just write data to the network. and round trip it just to store it. And well, I mean, I think lots of data centers do that. right, in the sense of, like, with the decoupling of disc. So when your network latency is a 100 microseconds and your disk latency is 10 milliseconds, right, for a seat. There's no reason to keep this is the whole disaggregate of, like, we have big storage units and then computers separated from that. Again, what I'd say that was just remember like like like compute is cheap. So storage is different for that reason, but compute is cheap, moving data is what's expensive. Right? That's what takes power, that's what takes time. AND SO THERE ARE TWO BASIC WAYS THAT YOU MAKE COMPETITION FAST. Number 1 IS YOU PAROLIZE IT. Number 2 is that you keep the data local. That's that's all there is. And so the idea of, oh, I send stuff out to store it, Sure. Sure."
  },
  {
    "startTime": "01:04:00",
    "text": "are you talking about, like, I just make it loops through the network? You you just have a fiber that points it itself. Well, so let's think about, like, propagation times, how much storage you get. You don't get a lot Now there are actually people who think this way a lot of supercomputing people think this way when they're doing, like, super high data rate, you know, think many terabytes per second. they actually think about the propagation delay along their fiber as that is the data in like, that actually counts as data that's in storage because it starts becoming significant. Yeah. So they they think that way. But I don't think you don't necessarily want it, like, circling around on this fiberoptic cable just to store it. Right. Right. So if it's easily paralyzed, but why not? Right? you have to build the ASIC to drive all You have to pay for this. So Thank you. Nellini Alkins, inside products. Now this is something prior which is probably way, way out there and probably more than 10 years out. I know there is some research being done at University of Washington and also Harvard. for putting what what doing what they call hard soft memory. which is using DNA to store memory, I mean, it's like and I don't even know how you do, like, Like, what's the manufacturing process here? You know what I'm saying? let yeah. Yeah. Yeah. I think there's So there's actually a really nice keynote by gosh. I'm for for getting his name. University of California Santa Cruz. who's, like, a total, like, storage, you know, Wizard and he talked about all the different media that are out there. any has this great thing of, like, latency, capacity, cost, etcetera, durability. and draws us, you know, sort of multi one of these, like, multi axis star things. He says, look. a memory ever encompasses another, then the old one goes away. It's just better in every way. And so he looks at stuff like DNA, but also there were some stuff that came out of Microsoft on, like, glossetering. right, etching and glass and the precision of that."
  },
  {
    "startTime": "01:06:01",
    "text": "tends to have better durability than DNA. And so he looks at all the different possible storage things. But I think you're really not talking about storage. Like, what's after Flash? Right? flash has that, you know, they're continuing to do more and more layers, right, that has some legs. We'll see where that goes. But, yeah, I I I can't say too much about the future of storage. Ethan Miller. the person. Ethan Miller is a good he's got a great keynote talking about storage. you'd be a good thing to look up. Yep. Thank you so much. We got a time. Yeah. We'll take one more question. You're the tail of the queue. Cool. Intel everybody Yeah. Yeah. Rich Saul Akamai. So thanks for this talk. it was really informative, and frankly terrifying, and I wanted to push back. Oh, it's so exciting for me there's so much research to do. So -- Well yeah. But I gotta write the code. So My question was the the I wanted to push back on the very last sentence you said, which is and I feel fine. I mean, I cannot just you know, when Moore's loss started to die off. I guess it's still taking a bit. It's like, okay. I know how to do things in parallel, that's a lot easier. But now I have to think More specific, I mean, I can understand accelerators, and that's easy. But now you're talking about while accelerators and smart memory and dumb memory. I mean, you really think it's gonna be okay? do Yeah. I mean, I I do. Like, people are smart. Right? They'll come up with. I mean, so One thought is not necessarily that any given application is gonna have to do all things because that's not true. Like, if I'm looking up things in a key value store, like, I'm not using a TPU to do that. Right? you know, might be that the composition of applications then does this. A lot that I think is gonna be we're gonna start becoming much, much more careful about our data structures. Right? Like, maybe we'll go back to the 19 eighties or, like, we really tried to make them small and tight, and it's not just ram is free. So in that way, like, I feel find these problems,"
  },
  {
    "startTime": "01:08:01",
    "text": "The challenge is that the engineering challenges this will raise for us are we've lived in roles like that before. and we've been okay. I mean, it means that, like, you know, the free lunch of everything getting better all the time is going away, but, like, oh, that's you know, you know, That's engineering. So thanks. Okay. Let's thank our speaker, Phil Levus again. Annette. please come up. through Yeah. Please. Come on. Okay. So our next speaker is atat Gremlin Barr from Tel Aviv University, and he's going to talk about it's not where you are. It's where you're or IoT location impact on m u d, m u d, Yeah. Hi. Thank you for the introduction. Yeah. Thank you. Okay. is title research So my not where you are. It's you're registered, IoT location impact. And this research was done in collaboration with and not women involved, from the Tel Aviv University, from with David High, the Heber University, and Sean Delina from Raiffeton University. This is also this research was also supported by Cisco and the Israeli Research authorically would like to express my gratitude today in our WHS, and the committee for allowing us the opportunity share our word. fine next today. So we started this research when we try to understand what are the factors that affect device network behavior IOT device network behavior, actually? Yeah. And the first scenario that we thought about is this scenario in which there is the same device in two different locations. And when we say, different location. I'm referring to IP based location, it means the the geolocation of the external IP"
  },
  {
    "startTime": "01:10:01",
    "text": "the device. And we found out that the impact of And the IP based location is on the network behavior of the device. and we saw an impact on the network behavior. So we are talking about IP based location, but Actually, a what it is that it's even more impactful is the user define location. we define the user defined location as the location that the user chose, chose, chose, through the registration process, of a new account And in that case, have a scenario in which we have a device that is located in the UK It's physically located in the UK. but the user chose to register it in the US. You can see that there are 3 options the American server, the European server, and the Chinese server, and k. use the laser. Yeah. And there are 3 options, and the user chose to work with the American server. So In that case, there is much more impact on the device network behavior and we will see it and resume. but when we have this kind of finding we thought, why do we care? So what are the implications of this finding that there are different device behavior, these different network device behavior. 4 at different places different locations. the first is The first implication is on the network security framework. moditfmadrfc8520, And in that security framework, we define a profile of the device, a network profile of the device, and if we would like to understand What is the profile? we should take into account all the factors that affect it. Another implication is on IoT identification. If we will learn our dataset will learn one location, in a single location, then we will try to identify the same device with the same firmware. another location, it just won't work. it might be potentially with errors. And, essentially,"
  },
  {
    "startTime": "01:12:00",
    "text": "each task that is composed of learning a normal device behavior and then extracting rules, such as the network secure framework, and extracting features such as IoT identification, is affected by these finding this to just to like, trailers for the for the rest of the presentation. So I go over briefly over the outline for today. we'll show you that user defined location is much more impactful than IP based location. and I'll show what is the implementation. How does a, user defined location is implemented. in the common case. I will cover background about mod about the ITFC security framework and then the implication on user defined location, on man. And then I'll show you a proposal that we suggested 2, improve the implementation using extension features of DNS. Okay. So as he said, location impact is very common. And when we came to quantify what is more impactful IP based location or user defined location we created a similarity measurement very straightforward similarity measurement in which we compare the 2 sets of domains for the same device with the same firmware, at 2 different locations. And we define 2 types of location. The IP based location which is the geolocation of the external IP and the user defined location. which is the location that the device that the user chose through through through its registration process. And as as you can see in the comparisons, You can see that in 90% of the comparisons that we made in our dataset that composed of dozens of devices in up to 10 locations for each device. we found out that 90% of the devices exhibited a difference a difference in the network behavior difference in the network behavior, but just 54% of the devices exhibited and network behavior, a change in the network behavior, when we change the IP of the device, the external IP of the device."
  },
  {
    "startTime": "01:14:03",
    "text": "So Then we raised another another question was raised. and is it was why there is a difference in the domains. Because the similarity measurement measure the in the domain the set of the domains that the device uses. and and Different domain names we we saw through our research to our dataset different domains allow different features and servers And for example, we saw a camera that as a feature that was available only in China, only when we chose the Chinese server the facial recognition feature was enabled. while you choose while you if you choose there, European Server with American Seven, server, this feature was not available, but There are other ways to implement different IPs or different servers for the same domain. For example, using IP based location, and in the next slide, I will show you how DNS can support IP based location decisions But understand that in order to allow the user to decide what is the location and what is the features that he wants to the domain names, must be different. and Here, you can see the IP based location decision using DNS, So in that case, you can see that the device is located in the UK. but registered in the US. In that case, their the device users and api.smartings.com. This is the domain that it uses. and he asked the reclusive resolver which is located nearby, the device in the UK. In that case, the request to preserver sent this request in his churn the authoritative name servers. And because the request came from the UK from the UK resolver, the IP is from Ireland, Ireland, which is in the UK, and and that is how IP based location can help DNS can help to make IP based location decisions. we call this kind of domains domains with No location identified within them. we call it global domains."
  },
  {
    "startTime": "01:16:01",
    "text": "And in the next slide, we can see. regional domains. In that case, it's the same case that the user registered in the US, the device is located in the UK, and the device initiate a and DNS request, to the DCDash US East It uses a US server, end, end, and despite the fact that the recursive resolver is in the UK, authoritative name servers knows to answer with an IP from the from the US. We call this kind of domains regional domains, domain suite and location identified within them? And that is how we that's it. Our user defined location is implemented. and this is how user define location difference looks like. In the right side, you you saw that you see the the same example in which the device is registered in the US And the device uses the us east.connect.smartings.com. and get an IP in the US. While on the left side, of the slide, you can see that the device uses another domain different domain, which is eus.connect.smartings.com. which is another location another domain, which with location identifier within them within them. Another thing that I would like you to keep in mind is that there are not just two available location. There are much more him you can capture it you can the app, you can choose up to 10 locations. In this case, we capture device in up to 10 locations, is the y i camera. We created a similarity heat map for the similarity measurements that we made in up to 10 location, each cell is compare to a different location. And as you can see, despite the fact that you can choose different places, you can see that there is a region, actually. So different places are addressed to the same region,"
  },
  {
    "startTime": "01:18:01",
    "text": "Russia, United Kingdom in Germany, example, and What we would like you to keep in mind is that there there are more than just to to to options. I I presented in the first slide just two options to simplify their slide, And as I presented, the common case is in the subdomain, the differences of the domains. are in the sub domains. Only 9% of the domains present the difference, the top level domain. The top level domain is the dotcom, the extension, of the domain. Okay. So now we saw that there is a difference that it is caused by the user decision. let's go over the mad, the mad profile, mad the IETF standard, RFC 8 520. This is essentially, a network framework and it will exceed the framework. that defines a MAD file. And MAD file is essentially access control list, a set of access control entries the composed of the legitimate endpoint protocol, the source code destination part of the device users. and the all logic behind Mod is reduce the attack surface on the device using a firewall or any network gateway that supports supports mud. The logic behind defining mod a mud file is that in IoT devices that there are there is a specific goal for each device. Think about a a light bulb, OA over yeah. a water sensor. These devices have a specific goal it uses set a small set of domains so it's makes sense or reasonable to create this kind of file it won't work for computer, for example. And now we can see what are the implications on demand framework. So if we have device in and if we will create a a a mud file for the device that is registered the US, it will compose of these 2 domains. in the US East domain. And on the UK,"
  },
  {
    "startTime": "01:20:00",
    "text": "will create in EU West. But the implication r is that the learning phase of the model will be much complicated. It will be we will need to capture the device in every single location that the device supported, supported, we need to combine maybe the all rules into a single large model, or we should use separate mud files for each location, and what's more the explainability? How can a security administrator go over this mud fine, understand is going on with this device, and how can the manufacturer or the security administrator can maintain even this kind of location. And as I said, there are more locations than just two. You can see that like, this smart file can be available for many more. So The next thing I would like to share is about DNS. So there is an extension for DNS that is called ECS. ECS stands for extension, the DNS client subnet, and Actually, the easiest way to understand it is through the figure. So in the figure, you can see that the device is located in the UK. it uses a domainapi.smartings.com. But in that case, that the course to resolve her is in the US. it happens sometimes that the device uses not new by recrucussive yourself when you use open resolvers. And in that case, when the authoritative name service will get request, he will answer with the US IP because the recursive software is in the US. So to solve this problem, and to get the IP that is nearby, the device itself, there is client subnet. In that way, the recursive resolver will add a field, an ECS field, that mentioned that the device itself in the red frame devices itself is in the UK. And this way, authoritative name servers will know to answer with and UKIP. and we also noted that ECS, the RFC of ECS, allows allows allows"
  },
  {
    "startTime": "01:22:01",
    "text": "the endpoint itself, the device itself, to add this ECS field. And that led us to our proposal. In that case, we want to solve the problem of having different domains for the same for the same actually server behind. And instead of using different domains, we can use a single domain and mention, the and use the ECS in order to carry the user decision. So instead of using the ECS by the authority the by the request of resolver, And the device itself will add that the ECS failed. And instead of mentioning the IP based location there relocation in the UK, will mention the user define location. in that case, the authoritative name server will know to answer with the US with the American server and not with the UK server, And, of course, we will use it for the regional domains and not for the global domains, order to m, preserve there. behavior of the device. and that is how mad looks like when we use this proposal with the ECS, In that case, the learning phase of mod be much easier, easier, will use we will need to capture it just in one place, and instead of, like, go over all over the locations that's available, we can use a single model as expected as predicted in the I've seen in the beginning. and it will be much explainable and maintainable. to to have it, have it, that way. So made it very quickly, let's go with the sum of of the summary. So user defined location, has an impact on the IoT device network behavior and on specifically on the domains. The domain said that the device users. IP based location. is is also an an impact but less than the user defined location. dataset and security measurement should take into account and the location."
  },
  {
    "startTime": "01:24:02",
    "text": "Otherwise, it just won't work. We won't be able to and different device or to identify it when we are talking about IOT identification. And user defined can be implemented using instead of using different domains, using the extension DNS And that's all for Now we have some more research as a we can scan the QR code about IoT, about IoT networking, about MADA, Fc, We have tools to generate mud anything about iotnetwork and Argentina for behavioral. is is in our website. you very much for listening. Any questions? There's no one in the queue, so you could come up. Oh, Hello. My name is from Research of Europe. Thank you for presentation. I have quick question related to what would be the impact confusing VPN, for instance, to would say, countermeasure this kind of solution. What do you think about that? Yes. So VPN can change the IP based location. So, actually, when we made this When we made this research, to to to quantify the IP based location. We use VPN in order to simulate, simulate, the device location. So even when you change the and and location, the IP based location using VPN, still the user can choose a specific location in the registration portion of the user defined location. And in that way, the VPN just won't have any impact. The user defined location is the impact factor. Is that answer your question? kind of. But if the user is is was the user who's creating this VPN in order to avoid like, selecting different servers."
  },
  {
    "startTime": "01:26:04",
    "text": "what I'm what my question is because I have some background related to new like technologies for IP based geolocation. that, it's like based on probing technologies. One of So I don't know if you consider this kind of approaches approaches So who who research is we measure the IP based location. So if you fake it using a VPN or any other way. we will take just the IP based that you fake. with the VPN. So we then try to like, and find that you're using a a VPN. So basically as a next step, could be interesting to consider detection detection of proxies something that that we can see. But, actually, what the manufacturer does here this is is allow the user to choose So they don't want to restrict the user. They want to you they want him to a lot to allow the user to to work with the device. So Like, I guess we can have this discussion later on because we do have topic, we are working on this kind of detection tools. I can share with in touch with you later on. Thank you. Thank you. Thank you. Hey, for love us, Stanford. So as you just mentioned, the companies want to allow someone to choose which server they use. you talk about the regulatory and legal implications of that because this actually seems in some ways counter. Like, I mean, of course, VPNs can always escape IP, but but just I mean, at some point, official you turn on your facial recognition in the US, right, is a little tricky. Yeah. So, actually, I'm from the computer science department. the legal department. But Yeah. But I know I've been I'm not asking for an answer. Just your thoughts. So thought about it. And Actually, I think that I don't need to explain when they Wote. Wote. Wote. Can we zoom to the to the There is no option. Okay. So what what you can see that is written over there is that you can choose the European 7 when you want to work with countries that support the GDPR."
  },
  {
    "startTime": "01:28:00",
    "text": "and you can choose the American server when you don't want to want work with the GDPR, and you can work in China when you into Chinese mainland. So I think that there. It's specifically mentioning that and you can choose. If you want the GDPR to be enabled or not, I'm not sure if it is legal or I guess that they have their own legal considerations. Sure. Yeah. I mean, I think there's yeah. So it pushed the liability to you. Right. Alright. Thanks. Thank you. Okay. Let's thank our speaker again. Roman, please come up Our next speaker is Roman vouchercorp, Vouchercop, from UC Santa Barbara. and his talk will be a penal programmable infrastructure for Net cracking Yes. Thank you for that correction. Can I get this? My name is Roman. I'm gonna present the peanut programmable infrastructure for networking. It's a joint work with my colleagues from University of California, Santa Barbara, and Nixon Enterprise. And I want to start. would start on the slides from credible statement that one of the problems in the academia for network research are representative infrastructures. often researchers end up with what they have in the lab currently it's like couple of laptops, several routers, switches, etcetera, etcetera, what they have, and they are trying to create something representative to collect the data from. But the desired infrastructure is something that you can see on the right on the right side of the slide. It should be more complex, more Let's use the word representative here. because it depends on the actual experiment that a researcher want to conduct. So non representative infrastructures lead to non representative data. bad data and bad data, especially if we are talking about machine learning solutions lead to bad solutions that we cannot apply anywhere. And if anyone interested, we have a paper regarding data leading to bad solutions recently. So the question is, like, we can do with it."
  },
  {
    "startTime": "01:30:04",
    "text": "fortunately, there is a number of existing platforms. This is non exhaustive list of different platforms like Rapatlas, Cosmos Metrics, Measurement Lab, And could those to them for allowing to do the research experiments, etcetera, but They are great, but there is simple problem. Imagine that we want to conduct some experiment. And I want to say that some experiments are hard or even impossible to implement on programs. Let's start with a pretty simple example. want to measure quality of experience for YouTube. For these, I want to watch the YouTube stream. I want to collect network data, like raw packets. And at the same time, I want to measure things like amount of health buffer health, how many times video was stalled, quality of the video, resolution, etcetera, etcetera, etcetera. Okay. That's very simple. But then it start to be complicated. example, I want to do this over wireless network. and not of this or not of such not of many such plat platforms allow you to do this over wireless networks. And I want to do it within a live network with the different users when I have a traffic aside. And I want to do it over a long time period, not just once, but, like, in several weeks or even months probably. And as well, I want to do it with a flexible and programmable client. dynamically change videos, resolutions, and what I'm measuring. And I also want to separate backbone problems from last mile problems. So and now it becomes very complicated. And the question is, what infrastructure do we need for all of this? And We we decided that we'll implement our own solution. And this suggestion, basically, I will present on the next slide. It's programmable infrastructure at university California Santa Barbara. I will describe on the remaining slides why we did it. how it's affiliated and how to reproduce if anyone interested in their compost compost infrastructure. So what are design principles for all of this stuff? At first, we want to be able to measure things actively and, basically, at the same time."
  },
  {
    "startTime": "01:32:00",
    "text": "That's very important. I'll describe benefits of it in the on the other slide. The next one, we want last mile collection to carry real world user track. And I want to say here that Compass networks are pretty interesting for this stuff because Campus networks allow you to have balance between unrealistic lapse scenario when you have couple of devices or maybe even 10 devices. And production network that academia usually don't have access to because it's production network. So a compass scenario could be very interesting in this case. Compass Network can mimic Deepgramterprisenetwork. We example, University of California, Santa Barbara hosts around 25,000 students not counting administrative staff professors and everyone. And the Compass networks Compass network are listed. You are currently for a Santa Barbara is vulnerable to different things like the normal provider network, latency spikes, peak hours, user and traffic overlords, especially during Some or there are no users during winter, for example, during different accents. There are a lot of users, etcetera, etcetera. you see different patterns in this network. What else we wanted for our infrastructure? We wanted localized deployments. So notes would be closed geographically and logically. We wanted to support arbitrary premiums, like literally anything based. And we want direct and fast access for our researchers so they can iterate on the processes fast, and do not wait in a queue or something like this. And in addition, of course, we want to do everything ethical. So minimal disruptions or no disruption to existing users, preserve privacy, and to make everything fully reproducible to use off the shelf components cheap components if it's possible and open source everything. So, basically, this one, principles of power play. A very brief slide regarding the overall architecture. Basically, it's deploy it's separated to 2 parts. the campus part and data center part. On campus part, we deployed those and software Raspberry Pis that are using UCSB Wi Fi. For structural, describe"
  },
  {
    "startTime": "01:34:01",
    "text": "a bit more on the next slide. and we do active measurements from these devices. At the same time, at the data center of the universities, before the Santa Barbara, there is a backbone, basically, traffic and infrastructure. And on the border gateway, here on the on the top of the slide. we have a live traffic mirror into our servers, let I also describe later on the next slide. So we basically have active and structures deployed together and measuring together what we want. And I want to start from active measurements, how we implement it. As I said, we do cross verify devices. These are single board computers with the Linux on top, basically, we use to optimize for Raspberry Pi. We deployed 60 of them. It's something on the campus. different locations, and we will deploy 40 more this month and no one can stops. in They are controlled from a central server by SALT Stack. they are deployed in public places such as libraries, dormitories, university centers, basically where students are located most. And very important point here that Universe California Santa Barbara has a single Wi Fi spend over the whole campus. and many universities have the same wireless infrastructure. And our devices use these Wi Fi. and that allow us to claim that their traffic is more or less representative as a user of this Wi Fi net. because they use the same act access points, and they suffer for the from the same problem as all users suffer around. Here is the example of of the device, it's it's even real world US. I mean, you can probably scan color code and get some statistics from real type. Regarding oh, yeah. Regarding deployment, we deployed them over the whole was mostly these are different dormitories and university centers and libraries, etcetera, etcetera. we try to cover almost everything. and more is coming for, basically."
  },
  {
    "startTime": "01:36:06",
    "text": "Regarding pace of data collection. So how implemented pace of data collection? there is a border giveaway of University of California, Santa Barbara. We agreed with the IT department that we all have a leave traffic mirror from this border gateway to our Intel to Finaswitch. It's a programmable switch. On this programmable switch we implemented on this program, it set before program for randomization of the trial. Basically, it analyzes IP addresses, market addresses, and everything to remove possible source of identification of the user. And this also allows ethical review committee to check that we are really doing this summarization in And then the Fintaswitch, balance this traffic to our servers. we just basically run speed up with additional start stuff. to collect all this data and save the data More details are available on the website up to the like like like, configuration and links and GitHub repository, etcetera. So why exactly active in passive measure? measurement. Many platforms implement just base of measurement. Some platforms implement active measurements. Active measurements are important because in the world of encrypted traffic everywhere, and I love encrypted traffic. but we still need some labeled data. especially for machine learning algorithms. And for this, we need extra measurements because we can control labels. We also need place of measurements because we cannot create live network traffic without basically observing what's going without the user traffic, basically. And the combination of active and passive measurements on our campus allow us to do things, very important things. The first one, it's pretty unique observation of the packet from multiple vintage points so we can look at the packets on the border gateway. we can look at the packet on the device itself. and we can find out if there are any problems on the backbone of the provider or there are problems on our campus. even can"
  },
  {
    "startTime": "01:38:00",
    "text": "get the information from access points, Wi Fi access points, and maybe find the source of the problem there at least. to sports. And the second one, if we have some data, that we initiated from the active measurements, from active devices, and we found some patterns, we can use basic observation to confirm these patterns to check that these patterns really exist in the real world user that would be easier to to find because we already into you need to fight them from the originally active measurements. So if you want to implement the same infrastructure at the other campus example, you can implement them separately because they are used more or less independently. But together, it's more in Some examples of current experiments as I mentioned in the beginning of video, quality of experiments for different platforms. You talk to each of you know etcetera. We tried and still trying to do quality of experience measurements for video conference platforms for Google Meet and for Zoom. We use this infrastructure for controlled speed tests where we can control time, whether it would be in peak time or in pretty calm time interface wired versus wireless location, whether it's busy location or pre empty, etcetera. We can use this infrastructure for application traffic collection for fingerprinting, and even for botnet limitation with different networks attacks, and the data partner was was not happy about. What else? Yes. There is a slide of different limitations of this platform. The first and very important one is that These theoretical data guarantees of the campus networks are only theoretical. So they're debatable. And the only solution we know for now is to measure and explore and confirm whether the data collected from this infrastructure is a representative the experiments that we want to collect this data for. But at least we think and claim that this infrastructure would be more interest in the the lab deployment."
  },
  {
    "startTime": "01:40:00",
    "text": "ethical review is important and required. So if you want to do the same, please do it earlier. And most of the other problems are either administrative problems, where you want to do stuff with university or security where you don't want your deployment to become a botnet without your knowledge And So blast You want to claim that we've created all of this stuff. We want to say that this infrastructure is pretty interesting and representative and interesting for research for network research in academia. It has live user traffic and many universe have many users, so it's if you are trying to reproduce it, we encourage you to reproduce it. it would be more or less useful for you for research. It's cheap. especially Raspberry Pis. They are pretty cheap and to deploy everything It's pretty simple. All hardware companies are easy to buy. Well, servers could be expensive. And on our website, we have a separate page reproced reproducible to where you have all links to Amazon and everything, all configurations, all GitHub repos, up to the code for labeling our devices as well. And important part. We invite other researchers to participate in this experiment to submit your experiments and you want to calculate video. provide the platform for you as well. So there is a email where you can go. Go. Go. Go. Go. Go. Go. Go. where it can mail us or go to the website and find the same information there. and that's it from my side. so much for the attention, and I want to ask answer your questions if you have any. Thank you. Greg Musky Ericsson, very interesting presentation. Thank you. So you mentioned that you used a combination of active and passive measurements. So can you clarify what active measurement method you use"
  },
  {
    "startTime": "01:42:00",
    "text": "exactly do you mean by measure like, method. We have Raspberry Pi devices. They have basic Linux onboard, Ubuntu, and This allows us to do any measurements that we can do basically on the Ubuntu device. So from speed tests to implementing custom scripts on Python that you can run basically anything. Okay. So, basically, it's not that you are using example, t 1 applied, or a stamp protocol. Yeah. Yeah. Yeah. It's not some defined critical. It's basically open platform for implementing Okay. but but Okay. Are are you using any particular measurement protocol at this time? No. No. No. Okay. We're using particular platform for submitting experiments, netunicorn details are also available on the website, but that's the only restriction that we have. And Most often, we do experiments in Docker Containers that also some create some restrictions for researchers don't have usually access to raw device and, like, Wi Fi statistics, for example. but it's for security prestartions. So I have been given a thought of using Okay. hybrid measurement methods, like, in c2om automate marketing method. Not yet, but probably now, yes. Yes. Thank you. If you don't mind, I'll talk to you later, Gregor. Sure. Thank you. Thank you. We can only take one more question. Royal, you know, Kita. Hi. Rio Yanagita from Universal of Glasgow. Thank you very much. It was very interesting. So I could see I think I see rational of, like, how it's difficult to do. Network research involves our large infrastructure, provide it more sort of real life sort of scenario situation. I I get that. But why why I'd like some clarification is on the reproducibility. What do you really mean by reproduce reproducibility in this context? Because it could mean a wide range of like, scale of, like, reproducibility. So, like, do you mean it in a sense that"
  },
  {
    "startTime": "01:44:00",
    "text": "you can do similar thing if you implement with the bits of code that we have? Or are you saying you can redo the experiments and then get a similar behavior, which I don't think is quite possible Yes. It's not quite possible with a live network. Regarding your reproducibility, I don't remember whether Ah, reproducibility here on this slide, means reproducibility over the platform itself. not of the experiments. I mean, we have a webpage on our website where we show where we describe van, our GitHub repos, all our our web sites are built, how the platform is built, what components are used, what servers, specifications and everything, everything, everything, everything, regarding the reversibility of experiments it's a separate topic, but, basically, docker based solutions, plusnetunicorn platform that we are using allows us to have reproducible code for experiments. and basically reproducible pipelines. and it's not reproducibility. in terms of having the same traffic each time you create you run the experiment, but at least reproducibility that you expect same behavior during each time you run the experiment. that your experiment will have the same behavior. Okay. you. So you have to take the other questions offline. Thank you. Yeah. next speaker is Toby's 5 from Max Planck Institute for informatics. and his presentation is crisis, ethics, reliability and the management dot network, reflections on active network management and academia. Okay. Good morning. academics engineers and other people from the Internet. I'm Pacific. He might know me from other fun research like Darling, I put all University IT in the cloud, and now Zoom tells us what to do in our seminars. or without feminist culture, you won't have IT security. But today, I'm talking about well, a little bit of ethics, a little bit of reliability and crisis and measurement network So first, network measurements. Well, that's basically what we are doing. It's an important"
  },
  {
    "startTime": "01:46:00",
    "text": "academics to get their papers. It's an even more important to practitioners to understand how things work on the Internet, and they come in or passive form, and usually, especially, the active ones are somewhat difficult. If we're measuring things For example, emails, these things got a little bit more complex if you were measuring email in 1991. You had rca 21822. A little bit of DNS, like 345, and you were really into funny protocols, you could also get a couple more from X Four Hundred If you're doing this in 2022, You have around 500 male RFCs, 300 DNS RFCs, HTTP. NTASTS, which is far too many. Of course, there is also all the stuff on TLS. And, well, IP 4, ipv6, welcome to the NTE world. So this got a little bit more complex. The other thing is if you do these measurements, you for this complexity to broad reliable measurement software. And little example from my own mail server. So one morning, I woke up and saw this. So I basically saw that I had no SMTP anymore that my mice LTL daemon was handling around 400 queries per second, and my open SMTP the mail server was running at 100 percent CPU. solution to that was that I had used MySQL back end with the default collate of Maersk Oil, which is lesson 1 Swedish CI, and some nice person did some active probing involving utf8 characters of account passwords on my server. which led to a funny loop of the authentication module of open SMTPD breaking off with this warning you see on screen and retrying them. which basically saturated to the open sntpd. And if you write measurement software, it's really easy to find similar things. So the Internet is full of corner cases. We have to account for them. There's a lot of unwritten rules about how you hold protocols. And ideally, you want for your measurement tools we use things that are already tested and deployed software. In general, if you write software, you"
  },
  {
    "startTime": "01:48:01",
    "text": "really benefit from being a good and experienced programmer and well, you don't want to break things that are not standard compliant. You have to do all the basic things like version traces, proper development, best practices. In addition to that, you also have to run your measurements, and here we have yolacola example of a measurement you might see, which involves some scanning, a recursive DNS server sorotative DNS server. And if we look at that, we see their their funny things because a person who set this up apparently forgot that DNS all Constanticep And there's a lot of things that can go wrong here. Also, RDNS 1 might stop resolving and maybe DNS stocks being delegated to our authoritative name server. or our recursors just stubbing against one of the port 1 servers, and we see queries from ourselves on our authoritative server and things that they are part of our measurements. So if you want to run measurements, you also have to be an experienced system operator. You have to have an understanding of all the tools involved. You have to monitor your stack. It's toric and real time. basically make sure that you set up a self contained on Well, you just have to do all of that. So then we also want to do ethical measurements, which means we have to consider all possible unintended harms. And especially with the Internet full of corner cases there is often the case that it's, like, Yeah. Mhmm. We know the Internet is made from duct tape and bubble gum, and this specific thing would be an issue, but we kind of decided not to talk about it because otherwise things break. But just something I heard, for example, about colleague's work on de aggregation of a slash 32 and putting it into the GRT. We have to get ethics approval usually from people who are academics in an IRB or a tech board who have no clue about the Internet, We have to do all the proper attribution things. So the reverse DNS, the URL, who is running web servers, etcetera, etcetera, etcetera. we have to do app use handling, and we have a maintained block list. So the PhD we need is somebody who thoroughly understands the protocol clicks there measuring. is burst in"
  },
  {
    "startTime": "01:50:01",
    "text": "bulk aspects of IT operations, experience program, experience system operator, and let's be honest, not page a student is basically a Hawaii department. And the thing is PhD students tend to be people, people, This this this is sometimes a surprise for faculty, but PhD students are actual people. And if if they are facing this world of requirements, In the reality of a PhD, they are facing a world of these requirements under 4 to 8 years, which they have flipped their PhD where they have to do 4 top tier papers Try to do knee research advancing the field. have done better with unrelated work usually do this directly after the batch show or their master. If you do the math 4 years 4 papers, the first paper should be under submission after 1 year. this leaves around about 6 months to basically get 2 decades 3 decades of protocol development into the brain of a single person. So what people then usually do is this They over caffeinate and they take shortcuts. because a very basic rule of everything is that people will do people things if you put people under pressure, they will try to cope. And if they cope, they cut corners. And that then makes bad measurements. The idea would of course be that faculty can help The problem is the perception of many PhD students is that the faculty is not necessarily helpful in giving good advice. other thing is if you think about junior faculty, there is The something very, very important, which is called a 10 year clock. and they tend to be running after that. So they are doing service because it helps 10 years. They are trying to grant grants because it helps 10 years. They are trying to get publication because I know Stania Waldi, get the idea. You get the idea. Profaces, on the other hand, we are still lacking any kind of evidence that being a tenured professor actually changes the amount of"
  },
  {
    "startTime": "01:52:02",
    "text": "time that is available to you. Usually, it's the same as for an non tenured person amounting to too little. And then now, of course, is the thing if you become a professor, you become a manager. and you basically get a bit removed from the technical world. So in the end, you might not actually even be versed enough to work on these things. an ideal world, of course, you would have some form of IT support In or former students handing things over. The problem is that research program is not really a well established position amongst universities And well, University IT often difficult, often riddled with middle boxes, funny forty gates on camp usually, you make a lot of acquaintance with your university's IT department if you run active measurements. the Christmas man. basically everyone crashed their 14 at once or twice Also, infrastructure a student left, it becomes undocumented, so the next student builds their own. what we are basically proposing or Hi. Hi. this is a single also thing is that we build a community governed infrastructure, which is available to researchers where measurements tools are reviewed by people that are not only academics but know what they are doing. basically take away the overhead of all this measurement running, like, abuse handling, blockless maintaining, etcetera, etcetera, etcetera. and enable basically people to focus on their research and make better network measurements. Before closing this talk, a couple of Q And A, so we get quicker 2 through the q and a, First question, of course, is why am I doing this alone? Well, it's it's a relatively simple reason because I have the experience with joint projects that it's better to have something running. And, otherwise, you end up in bike shedding, so I want to build this and then it over to somebody who can run this, There's, of course, a question why not some USR1 university group should run this, but instead trying to build this and give it"
  },
  {
    "startTime": "01:54:00",
    "text": "to a public body. Well, it should not be owned by an entity that published for a living. currently with me, that's, of course, the case, but I want to change this. Researchers tend to be a bit paranoid of being scooped should be running this. Well, entities would be the right NCC measurement working group or metallurgy. Or, of course, the RTF. But as soon as it works, I will start looking. isn't it easier to block if we give this entity of fixed prefix, well, it's kind of the point because usually in network measurements, you end up with a problem that you get scanned from Amazon. block that. And a week later, the learning management system of the university is at that IP, which is kind of kind of kind of kind of not nice. What if it doesn't work? So if I don't get this together and don't make it work, well, basically, I burnt my time and money, and no one else has served, which is also always kind of nice How will this be paid for? Well, for now, it's supported by the TobiasVs personal bank foundation for doing things that comes at a useful grant. by the way, not accepting other applications. and additionally to operators indirectly sponsor upstream. I got a slash 22 RPV 4 legacy from the info informatics for the project. for as soon as things are up, I will also try to motivate more, entities. And last slide, where can we find this? well, there already is a couple of resources, which are also in the GRT. there's basically 2 POPs already. This will open Berlin. there's 2 plant pops doing something in MPI Informatic. to have a bit more infrastructure going in something in Amsterdam. I need colocation hardware and layer 2 between POPs And on the web, there is a website which is currently a static file, but I'm working on something more interactive. and some existing services. And with that, thank you, and I happy to take any questions. very interesting talk. Any questions? So ff if you're all collecting your pitch works to find me during the break, that's also fine."
  },
  {
    "startTime": "01:56:13",
    "text": "Hi. Colin Perkins. so clearly you're not wrong. However, I wonder if the process of running this is easier than the process of running the measurement's infrastructure. the process of running The process of administering and managing the foundation managing how to get people involved and so on. someone has to do this and someone has to coordinate with all the the the people who want to use the infrastructure and so on, and you're you're bringing in a whole another layer of management over is this easier than actually building the infrastructure? So I will try, try, And I I will well, the government governance part will be harder than just building the infrastructure. But the problem with the infrastructure built by everyone that, not everyone can do this. and not everyone has the experience. So doing it in one spot once is probably easier than people reinventing the wheel everywhere. Even though for IT reinventing the wheel kind of standard contact. k. Thank you. Okay. no other questions, let's thank our speaker, Tobias. If Thank you. Thanks. Okay. So this this end of the first session, and we'll see the one in the afternoon. Thank you for coming. don't. 4."
  },
  {
    "startTime": "01:58:04",
    "text": "Yeah. Yeah. nights No."
  }
]
