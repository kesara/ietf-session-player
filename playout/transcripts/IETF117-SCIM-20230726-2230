[
  {
    "startTime": "00:00:07",
    "text": "Okay. It's not letting me do this. why is that not letting me do this? Come on. to Okay. I uploaded the new version. Check in it. Hopefully, it fixed. the typos. Did you want me to try and share? Well, I was waiting. Oh, oh, you want me to show? Yeah. It doesn't get a new one. No. It didn't"
  },
  {
    "startTime": "00:02:02",
    "text": "we should start. You can tell them. new one did get uploaded. You wanna share? I can just keep them You want me to start it? You wanna do it. Alright. Hello, everybody. Let's go ahead and get going. Welcome to the Skim working group session. Next slide. Reminder of the the note while applies here as it always does. thank you everybody for agreeing to this. And Go ahead. Next slide. If you are in the room, please do remember to the QR code to join the on-site tool. That's how we do the attendance of who's in the sessions as well. And please also use that to join the queue if you're gonna speak up at the mic That's important to make sure that our remote people also are in the same queue. if you are on-site that links to the light version of the Meet Echo tool, so you it won't ask for your camera on that main we don't get any feedback or anything like that. So please use that. and if you are on your laptop, just make sure you don't turn video on. And if you are remote, And speaking, please use a headset if possible, and it makes it a lot easier to hear everybody in the room here. Okay. Okay."
  },
  {
    "startTime": "00:04:03",
    "text": "Sorry. No. That's okay. A reminder of the key points of the code of conduct. on the screen here, just generally be respectful and keep the discussions professional and not not personal. Let me skip that one. That's from the last time. Oh, yes. Thank you. Notetaker. So Pam offered to take notes we do need someone who can take notes while Pam is presenting. Paul is doing that. Great. We're covered with notetakers. key thing for the notetakers is just make sure any decisions that r r i, decided during the session are captured. the recording is good enough for the detailed discussion notes. Okay. So the agenda we have a packed agenda. The 1 change that we just made a second ago is actually, we're gonna start with Elliot. Elliot has to be in two places at once. and then we will go on. So we are going to try to keep this On schedule, everybody except LA has 15 minutes. We'll have a timer going on the screen, and you should see it on the on the conference monitor in front when you are at the mic here. And that's it. So with that, I guess, must anybody else has changes unless anybody else has any last minute agenda requests, we're gonna go ahead and get started, with with with Elliot. k. So Yeah. Here's the question. Who shares slides? It doesn't matter. Did you want me to Sharon, Sharon, Sharon, Yeah. Could could you please, Nancy? And then I'll just call out when to change. Okay?"
  },
  {
    "startTime": "00:06:04",
    "text": "k. We're waiting. Okay. Next slide, please. Alright. So just to review, this is the Skim device schema. And the idea is to do with devices, that which we do with users We used core schema. We used group schema. and but just extend to for devices that means a little bit more than that and I'll explain that in a little bit. Next slide. Right. So this fits into an entire picture. The green line is what we're doing. today. We're actually looking at how we standardize the orange and blue lines. working also in the IETF. We'll be talking about more of that in the in Prague. The idea basically is to abstract out the different means by which you the different things that you might wanna onboard, whether it's a BLE device, Wi Fi device, a Xyngby device, who knows a thread device, and to be able to provide the provisioning information for each of those. devices. Next slide, please. Okay. So core schema the core device schema just has 1 or 2 objects in it. To start with, this is something that I think as we as the working group, moves on we'll have to get into. The BLE extension contains different forms of authentication for BLE. such that the devices can be onboarded pass key or with other information, either with a keys if they're available. for bonding, things like that. EZ Connect has the DPP QR code components in in the schema. Zigbee is similar to BLE, and then we have endpoint applications, which are means to"
  },
  {
    "startTime": "00:08:00",
    "text": "authorize communication into the non IP infrastructure. So if you're doing application layer gateway, you need to authorize the application layer gateway endpoint. that or the application endpoints. Next slide. So our status, we have update the endpoint application schema a little bit, just to make clear a a little clearer what's being authorized, At some point, we have the entire public key of a device in there, we remove that. It wasn't necessary, and and and it's generally not a good idea. because you want certificate. It wasn't just the, you know, certificates change and public keys change you need a naming structure for that. We had a number of issues to address. We we know we have a number of open issues to address. We we we received comments that the introduction could stand a little bit of a cleanup in terms of making the use cases just a little bit clear the next two lines are meant to be one line, which is probably good way of illustrating that we also have some some small errors here and there in the regular expressions for MAC addresses in some places. It's rather inconsistent. In some cases, it would put a dollar sign at the end of the regular expressions for matching, You know, in other cases, we actually added a dot at the end, which is just wrong. So we have a a little bit of cleanup to do there, and we have some formatting issues. We do have in the document as as we we promised narrative text, as well as open API and a little bit adjacent example. And they all and the JSON, in particular, has to be folded, and that gets really nasty in an 8868. Okay. We wanna get some hands on with coding. We're hoping very much that we'll have OSS code out the door by even before the end of this summer, I'm gonna say in just a couple of weeks, we're gonna try and get that out the door."
  },
  {
    "startTime": "00:10:02",
    "text": "so that people can play. And then we're pondering a hackathon with some key players in Prague, and we'll have more information on that. The working adoption according to the the email that was sent by the chair, working adoption had closed, but we are expecting at least one more comment to come through in the next couple of days. If if the chairs wanna wait for that, Otherwise, we wait your answers. We're holding back further changes in the draft until we hear about the call for adoption so that we can go through all the changes as as is. that's it. Elliot. And yeah. Sorry. I I The call for adoption is closed. There seem to be good support. would have liked to have seen more people interested in implementing, but I think you have a few Elliott, so I'm happy to see that you will be participating in the hackathon. So, At some point this week, I will close the adoption and formalize it that the document is being adopted. So once you see that, go ahead and submit it an ITf00 draft. Right? Okeydokey. So unless there's any more questions or feedback, Oh, I didn't see Hi. This is Danny Zelner. apologies for not actually, you know, providing feedback on the mailing list so far, but my only real concern, which I can, you know, sort of discuss more detail later would be the usage of the of the devices namespace. when, I guess, everything in the draft so far is aligned towards we'll have an IoT devices And there may be some other use cases just just about, like, actual, like, laptops, desktop servers. You you name it. And essentially could buy, I guess, consideration of"
  },
  {
    "startTime": "00:12:04",
    "text": "other folks who may also want to use that namespace in any design. So, Danny, I invite you to help us stretch that or constrain it. as we discussed this in the working group. Like, we can go in 1 or 2 directions there. we can stretch it in terms of saying, okay. What is needed for non IoT is use cases, and then just include that information, or we can constrain it by changing the top level names that we're focusing more on IoT devices. I sort of don't like the the latter because nobody can really define what an IoT device is. So if we can stretch it, that would be better, but I'm I'm open to either if we if if we have to go to the latter, we go to the latter. but but but but that alright with you? Yes. getting a thumbs up. Okay. Great. Good comment. I guess, Alright. Thanks, everyone, and thanks for the accommodation. Thanks. Alright. Next up, Tamela, Tamela, Hello? Hello? Hello? Oh, that's pretty good. you you're willing to Are you ready? Yep. Are I'm willing to -- Events. -- advance slides for you. Okay. Great. So We're just I'm gonna do 5 minutes on just the base concepts and then Palo is going to take over and go through some of the use cases we have. So if you wanna jump to our first slide, We actually did create a draft in the right format that we just didn't submit quite in time. And so this will be submitted to the data tracker as soon as it opens back up, which I think is pretty soon here. So if you wanna take a picture of it and look at it now, you"
  },
  {
    "startTime": "00:14:00",
    "text": "we'll have to type that in Uh-huh. Alright. Well, we'll just get that done then. Okay? Shows what we know. Okay. So, basically, what we found going through this process and what you'll see in the document is there's actually 3 main concepts when you boil it all down. The first one is the newest one, so we thought we'd cover that very quickly. is this concept of orchestrator roles. So when you think of what SCIM is, right, skim is a server and a client and a restful API where you're using HTTP verb to to push and pull data. But the question becomes what direction is the data flowing? You can push it you can pull it. So just being a server or just being a client is not enough information to understand how you are moving data around. So we've come up with this concept of orchestrator roles which define the flow of data. So if you're a resource creator, for example, Right? Then you are in some sense on the left hand side of a diagram, Right? And regardless of what skin verb you're using, Right? The data is going to flow from a resource creator to another one of these describing the data flow separately from from the actual restful, action that's occurring. And then this is the big thing we need feedback on, we need help with as to whether we're doing this correctly. So we have these 3 here, resource creator, which essentially is an originator. So your your actual object is being created by a resource creator, you may also be authoritative an attribute. Right? We have a resource manager, and that might be something that is sitting in the middle right, that is that is both accepting data but also passing data along. possibly while transforming it in the middle and resource subscriber is consuming data. Right? So obviously many implementations, you will be playing both roles."
  },
  {
    "startTime": "00:16:02",
    "text": "But what this gives us is language to say how the data is flowing. The resource is being created at a certain entity Right? It's flowing to a resource manager, and then it is flowing again. to a resource subscriber. Next slide. So we also talk about resource objects and resource attributes. Right? So obviously, a resource type is something that's defined in the SCIM specifications. Right? So A resource object is a thing that is that is typed to a given resource type Right? And it contains attributes. So that's the easy one. Keep going. So the other 2 are things that are already in the specification, but we've tried to boil them down. The first is triggers right? So there a trigger is essentially the thing that happens that's a moment in time that causes a skim action to be performed. Right? So if you if you open your mobile web app and you change your phone number, that could trigger a skin action, right, a push example of your data, to a resource manager because because you had that application interaction. Right? you might have a chron job causing synchronization to occur every hour. there might be a single sign on event where you've actually you know, tried to access a resource and therefore a federated assertion is being sent. So we define what we think are the set of most likely triggers It's not meant to be all inclusive, but we would like feedback on which ones you know for an implementer would be important them to understand exist. Right? So we think that events can trigger scheme actions As you can see, an administrator action might do it. A user action might do it. single sign on. So we think those are the big ones. again, looking for information. Keep going. And then the last one are the scheme actions themselves. Now the reason why there was some there is some confusion with skin is that You can actually"
  },
  {
    "startTime": "00:18:01",
    "text": "be on either. You can perform either a server role or a client role, And in that action of of being a server or a client from an HTTP perspective, Right? You can either be pushing or pulling data. And that's the stuff that Palo is going to talk about the examples that we talk about. But you literally, as a push, right, you can actually be the recipient of the push, or you can be the cause of a push Right? And the data will either flow to you or from you, depending on which whether you are the passive endpoint meaning the SCIM server or whether you are the active client. Right? The SCIM client. So we think that those three things, if we can explain them well enough, are going to move us forward in explaining use cases and concepts to users. And I if you have any questions, go ahead and ask because I know this isn't the most obvious, obvious like, it's not a picture, but does that make sense to folks that those are the 3 things Okay. Great. and each Yeah. Yeah. I got the I got a a shaking. What do you call that? The so so hand. Alright. Well, let's see if we get better when we go through the use cases themselves. hello, I'll turn it over to you. How do I do on time? About 30 seconds over. Not Batten. Okay. Next slide. So the first one, we are not even sure if we need it but we put it there because it was in the original RFCs. that is the slash me, So just a question. Anyone did the implement slash me that is defined in the RFCs. No. Does anyone know what is slash me. So we yeah. It's a mechanism that is defining the RFCs where someone can get information about the themselves. We still question if these there is a need to keep it in. Maybe we need to review it. but it's there. So we have to keep it as an example. Next slide."
  },
  {
    "startTime": "00:20:01",
    "text": "Next slide up. too much too much Mhmm. No. No. Okay. Next one. Yes. This one. Okay. So the next one, this is a typical one. Right? You have a resource manager. that is also resource creator, results of data, typical we are talking about an IBM that is going to push or pull depends on the trigger and depends on the action. to the resource subscribers. Think about it as a SaaS application. And typically, we'll have one IDM that supports multiple SaaS application. And, again, it does not need to be an IDM, right, because we saw devices or So so it can be anything. That's why we call it resource objects and resource attributes. Right? Not users or anything like that. Next slide. Leaf, do you wanna jump n about this in particular? previous. about the previous one. Okay. A bit slow. So the the the slash me thing was at the time when this was created, people were looking at activity pub. Right? And then they have this at me thing And it was it was a designed for a completely different envisioned completely different deployment model for scheme. Right? which never happened. Right? Never materialized. I would deprecate it. Yeah. That was our thought, but since it was there, we were not sure if someone implemented So it needs to be covered in the use case. Okay. Daryl? Carol Miller, Microsoft. As somebody who spends their day job work on a large API. that is centered around a thing called slash me. I wouldn't do it. Okay. it is a problem when you start to look at cross organizational effort, you need a slash userisy thing anyway. So you end up with to your eyes for the same thing. So caching becomes more of an issue. it"
  },
  {
    "startTime": "00:22:04",
    "text": "isn't relevant for administrator scenarios because usually administrators aren't working on their own behalf. So, I think it has more negatives than it has positives from what I understand for your use cases. Yeah. Yep. Yep. Yep. Yep. And maybe there is some application also for the devices. We are not yet because we are starting now. Right? So we are not sure, and that's why we cover it in the use case Sorry, Darrell. Can you please clarify it for me. He said, I wouldn't do it, and it wasn't clear if you meant, I wouldn't deprecated it, or I wouldn't implement it? My apologies. I wouldn't implement it. Okay. Thank you. Okay. Alright. Go ahead, Philip. Okay. Next one? Sorry. Wait a minute. We've got a -- -- couple of minutes. -- got a whole queue here. I go ahead and fill Hi. As far as I know, Oracle and a few others maybe Salesforce. I'd have to check with them. But it was heavily requested in the context of Owa. So the idea is to save the client from having to search for the first, they just say, I've got this oloftokenoruserm an ID token, and I just in the context of the current request, I just want the SCIM resource. So the shortcut is slash me because the authorization header holds the information as to how to locate me is there. It's simplifying and it saves a call Yeah. You don't need it, but then you have 2 calls to make. Also, when the server responds, it is supposed to respond with the true location so that if you're caching you're cashing by the true to not by slash me Okay. Thanks. Go ahead. answer a couple of Rodrigo. I sort of agree with Daryl that it's not such a good day. You probably have 2 year"
  },
  {
    "startTime": "00:24:02",
    "text": "for the same thing. However, I think I think there is probably 2 things on the slide. here. So there is a slash missing. But for me, there's also the use case that an individual might want to edit his or her own entry, which I think is probably an interesting use case. a self-service kind of thing. So we shouldn't probably if we get rid of so. me doesn't necessarily mean we should with also the whole use case. That's my point. And, again, I think that we could find something in the devices that also need So -- Right. -- we include it anyhow and accept if we duplicate it in that we'll keep it Right? Okay. We're gonna let you move on. Next one. We talked about that. Yep. Next one. Okay. So the next one is that now think about that you have that resource management, the IBM. and you have the SaaS application. And now you'll have an external source normally if you think about in the past something like an LDAP source, or through APIs, you can populate those these are subjects. in the resource manager. and the creation. It's not in the scheme protocol, but it's going to be created those object are going to be created in the resource management. Okay? Next one. So the next one is one that we already debate a couple of times that Now think about an HR application. Right? HR application, don't want to do any management, don't want to provide that information any resource subscriber, but it's the source of truth for those resource objects. Right? So these will big scheme to the resource manager, and that will provide me formation to the resource subscribers. Next one. Now you mix that with DERC, the external resource creators, that can also bring information, and it's the responsibility of the manager that can also create those objects to consolidate that and have the business logic to create the different resource objects and to push them to the different subscriber. Next one."
  },
  {
    "startTime": "00:26:03",
    "text": "Now think about the SaaS application. So we have many use cases where that SaaS application Sometimes, it's in charge for specific resource attributes. for the resource object. k? So they are just going to create attributes. They will not create the full or they might even create the full hacker, but they are not the authority for everything. So they just provide an accurate And we have been debating a lot on now the HR applications are providing that information to the all the resource subscribers, the since as applications provide that information back to the resource manager. Okay? And that is a use case that we need to discuss, and there is a couple of us to implement that. Next one. So now the next one is that think about the the HR application, that now wants to subscribe to the attribute that was created. in the size of the case. Right? So think about a telephony number was created inside the application that was pushed to the resource manager that now their HR application also needs that. Right? So these start to get a little bit more complex. And in the next one, It's the last use case that we can see that now think about that many times, we have very complex environments in our in our implementations where resource managers talk. to themselves. Right? Think about an Azure that talks to an Okta or talks to a Google or something like that, and they need to consolidate all their information. Now These are the use cases and what Pamela was showing you with that how does the data can flow through the push or the pull, and how does it trigger, so we are not describing that. We just need to describe the use case and try to figure it out if this is possible and what commendations we are going to give."
  },
  {
    "startTime": "00:28:04",
    "text": "for each one of them. Another part of the document that I think it's very important is based on their tactics. pool or active push, or now we are going to describe basic mechanisms for those that are always client to server based on all these use cases. So I don't know if we covered all the use cases, to be honest. if there are use cases that you think that are missing based on this concept of this or duration rules that we define. Or if there are other or station rows that you think that are missing, we really appreciate feedback, we create this basic schedule and to consolidate the ideas and to joint ideas and to try to allow us to have use case that are more meaningful than what we have today in the 7642 Any feedback? Yeah. We're a little overtime, so unless there's any fucks, we will Well, thank you. I think that was that was great. So while while we're bringing up the next presenter, it'd be good for Pam and Paulo for you to submit it in the data tracker. posted to the skin working group, and it would be great to get feedback on the draft. that we can do the call for adoption. So we'll have Sounds great. Great. Thank you. Okay. just who is presenting this 1, cursor pagination. updates. Both of you. Great. 99 here."
  },
  {
    "startTime": "00:30:00",
    "text": "Okay. Yeah. This is Danny. I guess I'm presenting this one. next slide, please. So I guess the the current state of things, we've I don't know if the past what 45. 6, however many IETF sessions we've talked about pagination at some level. There were various things proposed around requirements for the data and how I was returned and there was some some I guess, the disagreements others. Essentially, what what where we've landed on, at least, is proposing that the draft essentially remains how it's been written since 2017 and describes the over the wire mechanism, you know, the the protocol elements of of how to paginate a set of resources without defining any strict requirements as far as we'll always say that consistency of the data, you know, any any of those traits. that is to be inclusive of, you know, use cases that have not it's a little hard to, you know, please everybody and make everybody happy at the same time. And then if there are specific use cases that need, you know, x, y, and z, those can potentially be established via a PCP or know, a profile or something defining those separately. So we've published a couple of new versions since Yokohama, relatively minor edits. Most were in response to review from http DIRR And at this point, we're thinking it's about time to start working group last call. So the main question is really just Does anybody think otherwise?"
  },
  {
    "startTime": "00:32:06",
    "text": "That would have the only one with me. Yeah. You should note it in the notes. Great. in in the absence of any I guess feedback at this point Yeah. So what we can do is we can issue a working last call. I'm looking at rheumina as our AD. I can also ask for a sector Review. Harley, Charlie, I don't know. I don't think there's anything specific from a security perspective. Yeah. Hi. This is Robin. I'm just gonna I'm looking at you confused only because it seems like the normal processes apply. I mean, if you think we will accelerate things you could ask for a sector of you certainly. Okay. So we can do that. Okay. I believe this is our only slide because we're expecting to you know, have people with comments and returns are never so cool. good. Thanks. Yep. That's I think it's us for the next two. Yep. So -- Oh, great. Okay. Well, could we I made some time back on that. group. membership. Okay. It's no. It's me again. Danny. next one, we'll tag team. Next slide, please. Okay. Cool. So I was originally actually going to call this group membership pagination But just upon thinking of a couple of different topics that we're sort of trying to pieced together about, you know, change detection, and pagination, there's actually a little bit of both of those as far as group memberships go. So just quick run through the current problem for anyone who's not had to suffer from it. Right now, group memberships represented by multivalued attributes. They can have potentially 100 of 1000 or 1000000 of values in some cases. That's incredibly unwieldy."
  },
  {
    "startTime": "00:34:02",
    "text": "And, unfortunately, pagination and scheme today only applies to responses that contain multiple resources, and there is not a mechanism that is you know, adopted and standardized at least to paginate parts of a single resource such as, you know, multiple values and attribute. That potentially a large size of groups combined with the inability to paginate them has made large membership impractical to the point where just certain approaches certain use cases cannot be done. really anything consuming group membership for for, like, a million member groups distributed system, at least, acting as a client can't happen. Next slide, please. So okay. Yeah. I got a little ahead of myself, but, yeah, 2 problems this patched Asian, and tracking membership changes. So ties to the other part because what if some sort of, we'll call it, you know, change section delta query, whatever mechanism ends up coming into this came back. then we solve the problem of Assuming that we go the model of, you know, tell me all of the objects that have changed since a certain point. And don't try to you know, write the stamp in such a way that you can say, you know, tell me all the objects where a specific attribute has changed or, you know, something like that. That I don't even know if that solves this one. bias, diets, diets, hearing back from, you know, the same server. Yes. This group has changed. and not knowing what changed and just get having to look at the million members or whatever it is and do the diff yourself doesn't really. help. Next slide, please. So we've been, you know, thinking about some different solutions Phil Hunt authored a draft a few years ago on a I I guess you consider a protocol extension to introduce pagination for all multidiotattributes, There's a couple of other models that"
  },
  {
    "startTime": "00:36:01",
    "text": "or maybe take inspiration from other non skin APIs. So there's just a a new know, schema, etcetera, tech I guess, not even a schema extension is technically be a new resource type defined. and it's not plus schema, say slash group members, where it is specifically written to solve the problem of group memberships and believes any other large multi value attributes problems, you know, dangling in the wind, Or if we're already gonna start, you know, solving down that road maybe we're most of the way towards solving it for all multibillion attributes. and that brings us to the 3rd option of some mechanism at least, to represent multi value attributes as a, like, we'll call it, a subresource And in that way, whatever existing pagination models exist in the same standard. can be applied in the same way. versus the approach taken in fills draft of a new protocol. Like, essentially, a new syntax. Like, a new a new structure compared to the paginating other, you know, responsive resources. Like, So from an implementation point of view, right, if this is a just about group memberships. I'm wondering How much how hard it would be to just flip the relationship around and have a member of attributes on the users instead. And and I have that be what you implement instead of pagination. it makes detection change detection a lot easier, for instance. Right? Because typically groups It's a for at least for the systems that I've seen, In our my day job, the Like, what we typically see is that we get about as many groups as as users."
  },
  {
    "startTime": "00:38:00",
    "text": "in a in large scale collaborative platforms. So you have lots and lots of groups with very, very small number of members. And it's definitely more efficient to represent group memberships on the user object instead of as a group object. Now I understand that there are could be a process reason a good reasons to have, like, both Right? But if we're talking about something that is specific to groups, Maybe it's just simpler to to implement that than instead of pagination. Sorry, Paul. Yeah. That's something that's been discussed on the mailing list before. It's not a not a bad option. the the one of the flaws that I see in it at least, which it does have workarounds would be that's every so we're we that that sort of operates in the model that users and groups or I guess the main resource types that exist in SCIM. We just had Elliot present talking about devices, because other things that may come down the, you know, the pipe at some point plus anything that people standing themselves with the extensibility of the standard And you'd have to, I guess, do a query at the roots of the of the directory rather than on, like, slash users. It's achievable. Yeah. Phil? Yeah. I just had the question. have you considered using the groups attributable of the user resource for some of these cases. you look at a user resource, and it tells you what groups they're membering. So That's what I just said, Phil. I I would maybe argue that I complexity like this comes at a cost"
  },
  {
    "startTime": "00:40:01",
    "text": "and, like, pagination and attribute pag attribute value pagination comes to the cost and maybe it's worth thinking about whether there are actually use cases other than for very, very launch list other than group memberships. I I agree. That that's part of why the middle you know, option of just solve group memberships, seems not the best, I guess, future looking one But part of it is that we don't necessarily No. what other use cases are. Like, groups are it today, but Yeah. Go ahead, Phil. Phil, are you on mute? I already done. I'm done. Oh, okay. Okay. Yeah. And there's well, send a mail to the the mailing list on this one are, you know, matter what was write something and get feedback, and we're know, we tend it. uh-uh. the other. One of the other. But if anybody has And any outside, I guess, has any feedback on the topic. either here or online, feel free to reach out. Do we have any more slides? Nope. Nope. Okay. That is then the end of but this Topic. delta objects. Hello. Hi, everybody. I'm Angeli. So Yep. Okay. So we are working on the new introducing this new topic of change detection. Next slide, please. So what we are trying to solve is provide a scalable and highly accurate method of change detection that allows a scheme client to the tree"
  },
  {
    "startTime": "00:42:00",
    "text": "current state of all resources that have changed prior to a certain point in order to perform a recurring interval recurring retrieval of data. So there are various use cases, not constrained to the 2 mentioned on the slide where this might be used full is one is building a reconciliation system. So today in an most probable solutions that is an identity manager. who is pushing data into a scheme server. there may be some bugs or unidentified areas because of which some provisioning that the two systems get out of sync. And customers don't have today any mechanism to build these reconciliation systems apart from doing a full scan, which becomes quite expensive if the directories are huge. So this is an attempt to provide a mechanism where the system scan the SCIM API can provide a mechanism to provide changes using a get URL, which can be run based on a trigger or a certain interval of time. and retrieve changes and the system two systems can scan the data and find differences and do a redrive or reconciliation, either provisioning those discrepancies or even identifying the root cause and maybe fixing those issues. Other use case can be an incremental synchronization where clients pull data from the server. This could be an identity manager pulling data from an HR system and then publishing to provider. Next slide, please. So, typically, from a requirements perspective, what does this change detection query need to resolve is provide resources modified since a specific point. done by a query."
  },
  {
    "startTime": "00:44:01",
    "text": "should return current state of the record. We're not looking into a snapshot of data because our aim is to reconcile to the latest values. able to convey that a previously existing resource was deleted since this specific point in time. So today, scheme server can only respond to data that exist in their system. in order to build this kind of query mechanism, deleted records are also required. So there might be a need for keeping some metadata information about deleted records for a certain interval of time. to be returned as part of the query. able to convey changes on group memberships, and this is how it's linked to having a sub resource for group membership so that it easier to return the membership changes what has been deleted, what has been added, and what has been updated. And the next requirement is that it should perform on a large scale of data with frequency data updates. Yep. Next slide, please. So there are typically 2 approaches which can be taken, and we present both of them in this presentation to just bring out that there might be some existing implementations that may already be doing. So in case of approach 1, it is a simple change detection using timestamp based filter. So in this approach, as a first step, the client starts with a full tremel of data. So it's does a full scan aligns its make sure that everything is in sync. And then at a certain interval of time or through a trigger, it runs a query to the server which is including a filter based a last modified date. which includes when was my full sync done and give me records that was that has changed from the last sync time to now. and give me all the records."
  },
  {
    "startTime": "00:46:02",
    "text": "So this can be done today by existing scheme protocol. There's no engine needed. However, this query does not return deleted records. So The third point emphasizes that we may need to include A new parameter include deleted, which allows the scheme service providers to return deleted records as well. Next slide, please. Yeah. So why we aren't benefits of this approach is it's simple to implement It's built on an underlying search framework that scheme service providers may already have. there may be some existing implementations that are already doing this. maybe pulling records from the HR system or some other service providers. to build this reconciliation mechanism, which is based on timestamps. of course, because Kim protocol today does not allow for returning deleted records, those would request still require to do a full scan to to come up with the deleted right? Auto sync deleted records as well. So we want to keep this approach in discussion because because it is maybe it's built on top of an existing protocol. There might be implementations, and we just do an extension to include deleted records. However, this solution may have issues with time trip. that's a known issue. And this can be mitigated by having some overlap while you're doing a data sync interval. but in high volume updates and large directories, this might become a challenge So we propose another approach as well in this document. and we want to get feedback from this group. on"
  },
  {
    "startTime": "00:48:01",
    "text": "Should we keep both approaches? Should we talk about both approaches, or should be allowed to 1, and how do we handle if there are existing implementations built on top of the time based. Yep. And, hi. This is Danny. I'll discuss this part. So the second approach would be change detection using, we'll call watermark based approach. with a watermark we we can define it as an opaque artifact generated by the SCIM server. provides a point of reference value that can be used to identify the resources created updated or deleted after the point represent by the value. Flow is fairly similar, you know, same to, I guess, in order of operations to the prior one. You start with a full retrieval of the data either, you know, unfiltered or, you know, with some filter, you know, get users where department equals sales, and then you'll Roll that in forever. the highlighted delta query equals true parameter. essentially, it's just a bullion to say, I would like to have a watermark return to me with this request. So then the future, I can request only things that have been modified since this point. names are all placeholders, of course. And then on subsequent requests, you would, say, continue to give me new watermark, and also here is my watermark or the the delta check and to start, you know, just incrementally and let this over to provide the in scope results. as a clarifying question. So the the idea here is that the delta token is mine. It's an association between the client and the scheme server. I I what is it? A it's a global one. It's a is it more like a git commit, or is it more like a a a A cookie."
  },
  {
    "startTime": "00:50:01",
    "text": "probably closer to a cookie, I'm I am not terribly skilled with Git, so I It's hard for me to do the complete I mean, is it is it global? What with 2 clients get the same delta token for the same state. I do I believe the yeah. It it you know what? Some you said no. You yeah. I I I I believe probably, yes. It sort of depends because, like, the opaque value how it's being generated. I honestly hadn't thought too deep into that side of things. Jimmy, would you say that it could be implementation specific? it's not relevant to the spec. that. I mean, it it it goes to implementation cost, I guess. So so if the question is which of these two approaches is a is like the the the more the more reasonable one. Right? One of those one of the things to think about is how the how hard it is is it to implement this. Right? And it there's a big difference between having client specific state. My my my comment was i that it is up to you to decide whether to do it when you're implementing it because it doesn't matter to this protocol, which of those options you choose. That was my If if that's, like, Right. That's a relevant If that's true, then I I for I at I I don't think at any previous point, the thought had crossed my mind of the watermark being client specific, I haven't sort of identified or seen anything that that could mean. Or, I mean, it could matter. I mean, can I give it to somebody else and say, here. Go you know, go query the skin server for anything between that state and now so it I I actually changed my mind. I actually did does matter for the protocol, whether it's know, client specific or not. Now you're the you're not intending this to be passed to anybody else. Correct?"
  },
  {
    "startTime": "00:52:01",
    "text": "I I guess when you say anybody else, if it's -- Client holds onto the token and uses it in further requests and the scope of what you're proposing. That is I I think what would be with any intended use case that I've envisioned. I I guess also defining is the client or sort of the actor, you know, the personal organization or whatever, or is it a specific application But it's it's something that I don't I I it hasn't been honestly considered enough to give you answer one way or the other. k. Mike is on the queue. Yeah. You got it? Okay. Sorry. Danny, you know how it's coming. couple couple thoughts. 1, We've done both of this. at SailPoint, and and we've been putting a bunch of these. in different kind of formats. So couple of one, I think you're right to comment that a full ag or a full sync is the starting point for all of this. we find that in doing these kind of delta aggregations, historically, A full aggregation is also super helpful for a sanity check. because things things tend to get a little out of whack for various reasons. We can talk more detail offline, you know, about why those exactly happen, but sometimes full sync is just required. Right? So we do you know, there there's time change. Time stamps seems easiest to me on the outset. In other words, more common that Yeah. I can do a time stamp on that instance of that service provider, whatever can't rule it. Pam and Paolo were calling it, but I like that name better. basically, the the, you know, the real repository of information. and then I hand that back as as a time stamp on the target system rather than arguing about who the time stamp is. Right? Other times when the time stamp isn't used, we can"
  },
  {
    "startTime": "00:54:01",
    "text": "we do kind of what you're doing with a token approach where we say, look. We're gonna collect information from this system. It's gonna head back a token that it understands, I will save off that token and we'll we'll do a time we'll keep time on our own side to say, this token is gonna be valid for this number of days or this number of hours and then we'll go back out and reask for the next aggregation, the next delta aggregation kind of divide. So part of it is is kind of I think I meant at this point, subsequent later discussion, I'm kind of in favor of the This is, like, some for some system, it's gonna be easier to do time stamps. some systems, it's gonna be easier to it a little more sophisticated had token generation. another open question I have is a comment, and then I'll be quiet. is how common or how easy is it for systems to have a a soft delete or a staging area of deletions if you're making is deleted or requirement, how much of a a burden is that on these target systems to provide that back end. I don't know. I know that in a while, there's not a lot of systems hand back and is deleted kind of vibe. Right. I'm I'm open to that idea. I'll be quiet now. Thanks, Pam. Newark. Hi, Pam Dingle. I think with respect to the question of client specific, delta tokens, you know, it's the follow on to that is does possession of the Delta token convey any kind of access. Right? like, I think that has to be either way, it should get written into the considerations. I would suspect that we would want to make that choice and make it part of the spec rather than having that question of whether it conveys access to be and implementation detail."
  },
  {
    "startTime": "00:56:01",
    "text": "Yeah. And I would If if I was writing anything, at least, I wouldn't want the token to connect access purely to convey where new set of results should start from. Yeah. Darrell. you in your second query there, you have count 50. Does that mean that In that result, I'm going to get a new to token that would then allow me to retrieve those past 50. Or do I then do a skip with the same delta token until I run out of everything in that delta token, and then I'll get a new delta to token a later point. having not written a draft yet and intending to get feedback on approaches before doing so right now. haven't completely I I guess, ironed out that mechanic, but the there is a cursor based pagination draft that is, you know, probably about to go to a working group last call and the the the call would I imagine be to either use index or cursor based pagination to go through this set of results. And I imagine if you're providing any Delta token, it would probably be the same deltadelta token, but stating different, you know, pages of results whichever pagination method is being used. So to answer, so this delta token was provided by the server in the previous Scan. Mhmm. So if there is pagination required in this current scan, So the next so if you see the server provides a next delta token as a response on the last page. So that is the so this delta scan, a delta token is like query parameter, which needs to be provided same same in each and every page request."
  },
  {
    "startTime": "00:58:02",
    "text": "And on the last page, the server will return a next scan token. which the client needs to save for the next run of the scan after whatever interval of time. Does that clarify? I think so. Okay. Thank you. Thank you. Mike? you know, that's fine. two more things that we might wanna consider as you think about the draft. One is depending on what information you bring back, it highly influences how many delta changes you're getting back. In other words, if you bring back in last login as an attribute. then there's gonna be a ton of delta changes, and that'll impact how you approach in your best practice to doing these delta queries. Not all Furthermore, not all service providers are optimized to where respond really quickly to these kinds of of delta queries whether a time stamp or not. And so sometimes, practically full lags are actually just as fast as as somebody is filteredeltiquary. just an interesting thing to think through best practices and stuff as you approach it. sir. Yeah. Thanks for if if if comments. So is there some of those I I guess we have thought about already, And I I just like to clarify that we're not necessarily intending to write a mechanism that is honestly suitable for everybody versus say, just using a time stamp for instance, you know, even without the is deleted or whichever are just doing, you know, full import every so often. But that there are sort of use cases, that seem to be out there where there is a desire to efficiently and frequently retrieve"
  },
  {
    "startTime": "01:00:04",
    "text": "data of you have a current state of a system So, you know, human resources, systems, example, just, you know, I guess, in the overall identity, it's very common nowadays for people wanting to connects, you know, their true source of you know, truth for a lot of their user data into other systems. But even then just, you know, high value systems, which, you know, they may be able to optimize Maybe not. It allows to keep a sort of a steadier eye on the state of things, and helps to address any unexpected or unwanted changes, you know, just cosmetic to things that affect authorization. m, one's quick. So, in the question that Daryl asked, So are you presuming that it has to be cursor based pagination for this is this draft tightly coupled to the curse of pagination Draft or not. I have a preference at least of for for if the if there's a clear problem statement of, you know, requirements of I guess, high, you know, trustworthy accuracy and efficiency and whichever to write with one approach rather than too, just in providing more options than I guess, we'll say either absolutely necessary. or something to that effect can lead to interoperability challenges where Some pointers choose 1, some choose the other. And if, you know, 1 of the 2 options doesn't actually fulfill all the needs of you know, the other side of some of these innovations or interactions. than post investment in building, you know, the the one option that they will find out that it doesn't actually work and also waste of time."
  },
  {
    "startTime": "01:02:01",
    "text": "But but, Danny, you there's no actual requirement. like, spec wise. There's no dependency in the spec. this does not actually depend on course of pagination. Right? You could implement this with non curves, or pagination, I'm sorry. I completely process what you said as a different question than what you asked. That was a good answer, though. It was an answer to the question we didn't ask. But is this dependent on cursor based pagination? I Would want it to probably be at least in the scope of the problems that I'm targeting in, you know, the accuracy or liability, all that because I I think index pagination has its own problems with accuracy. and, you know, changing live results sets, but So probably. Okay. if there's opinions, you know, if people have different opinions, I'm sure they'll let you know. oh, sorry, Mike. Did you wanna jump in there? Oh, Yeah. I'm sorry. Like, how about Dan said. Yeah. I I think I I would prefer to to leave it independent, because I think it's it's a slightly tangential tend to do a problem set. I just wanted to go on the record with having saying that. That's all. doesn't mean I'm against 1st year based pagination, by the way, Danny. It just means that I see it as a tendential, thing. So that's all. Yeah. I I think if I step back, I they are separate, and I don't think anything from a technical standpoint requires cursor pagination for either form of, like, you know, pulling changes. just as far as targeting I guess, you know, trustworthy results at where you haven't accidentally missed something. That'll be a thing that, you know,"
  },
  {
    "startTime": "01:04:00",
    "text": "I would require positive planning, but I can understand that Yeah. I I Rob, by I get your feedback. And then, yeah, just talking through some of the I guess, analysis of this So so for just, I guess, context. I am not a software engineer. is to press my ability. I understand these things. a watermark based approach has some of the benefits of being more accurate by avoiding, you know, some of the time drift that can happen if just explicit, you know, like, get on meta last modified thing exists. It also eliminates any concerns over time drift or time differences even between the client and the server, and it at least, you know, isolates down to different systems or servers, whichever, behind the, like, the front face of the SCIM API. It also ultimately provides flexibility to server implementers where even if their internal system tracks changes with timestamp, and they don't have any sort of change log either directly, you know, whether or not we, you know, should or a must or whichever to describe using an opaque value. They can you know, either encoded or just put it straight in as, you know, in that, you know, container of, you know, next or what was it? Delta13equalsx. So it it's, I guess, syntactically or what protocol we would allow for 1 method regardless of how the implementer at least is. Are the the same server implementer was was handling any sort of change detection behind the scenes. some limitations. It is more likely to be a complex solution, for applications, especially if they don't have sort of a change log already? Already?"
  },
  {
    "startTime": "01:06:05",
    "text": "if the client loses all recent watermark values, then if There's, I don't know, like, a server instability, Internet outage, and pick whatever you want, then to go back they and, you know, re reobtain those results They may or have to do a a sort of a a full get or at least, you know, cover a lot more ground than they intended to. Depends on the lifetime of the watermark And it doesn't potentially provide if this is know, a mutually exclusive or whatever option where only the watermark is being, probably, to not say that, is diluted, Also, with what do you call it, with, like, a time stamp. then it renew removes some flexibility on that sort of, you know, correcting a known bad set of data saying, hey. Give me the last 2 minutes or just right in the last 2 days or whichever of changes. effort. I do wanna make sure we have time for the next presentation to keep this Quit. Yep. this one, the materials are on the data tracker is essentially just some open items we're seeking input on. some of that which we got here today, and we will bright up something and send it to the mailing list as well. of anybody with experience in this, we appreciate your help in figuring out how, if at all, to approach this. Great. Thanks. Alright. Skim events, and we got fill. Great. Can everybody hear me? Sounds great. Okay, okay, Let's go to the next slide then. So just a imich."
  },
  {
    "startTime": "01:08:01",
    "text": "couple of changes to this spec, most of the work, for me has been around implementation we had to restructure the event URI a little bit. Ayanna pointed out, I think it was Amanda. pointed out there is no actual event registries with security event tokens. So would we wanna use the skimregistry And so the new spec It may need some tweaking still, but it'll establish new 3 under the namespace, paramsevent So all the event nameswares prior, I had it in a different order assuming there was an event registry and there's not So All that is neither here or there into the actual spec other than that a name had to change So all of that should be more or less stabilized We had a good call a couple weeks ago on the asynchronous event because I was thinking is it time to punt that or is it time to go with them? We had really good example from a workday, and I'll cover that on the next couple of slides. along with that, and you'll see why soon I added a new attribute for service provider config so that clients can cover whether asynchronous events are supported Okay. So let's go to the next slide and see what that is. So an async scan request is basically any 7644 request, a skin protocol request. So, for example, if you're doing a put And all you're doing that's different is you're putting a prefer header that says you would prefer a respond async response. And when you do that, you'll get the the next slide, I believe I have it,"
  },
  {
    "startTime": "01:10:00",
    "text": "you'll get a response that it was accepted. There are 2 possibilities for the location. The location can be the actual reason location, And, optionally, it could be actually a place where you can go and check for whether the transaction is completed. So the transaction ID that you would look for corresponds to the set transaction value. So determining which response you're gonna get is also part of service provider config. So that gets clients, who are waiting for a SCIM server whose response might take 30 minutes. They don't have to keep the HCC connection open while they're waiting can just say, oh, I would like an asynchronous response. And then half an hour later, they can pull and check whether that specific transaction finished. So next slide, And the event response that's laid out in the spec is basically the async response And the values in that response. are simply the same information that you would have got had you had the normal skim response, in a synchronous manner. except you're now getting it as a security event. So the two ways that that event response can be delivered is either via doing an HTTP gap, as I mentioned on the previous slide, or you're getting it through the event delivery streams that you may have configured with the service providers so you can get it one of two ways. And that's it for the async, and that's how that works. If there's any questions, let me know. Okay. Next slide. So the discovery that I mentioned before. Oh, sorry. We do have a question from Daryl."
  },
  {
    "startTime": "01:12:00",
    "text": "Deeva's Go ahead, Joe. Graham, let me begin on the queue. Just a few comments. If you do prefer async, it is expected you return preference applied. as a response header, to say that the server did do the thing that you asked it to do. Asked it to do. Asked it to Is that one of the defined I can do we can do that. I didn't know they're there was a response header for that. I believe it says that in 7240. Okay. We'll update the spec. Yeah. because that was a question we had on the on the call is you get a 200 response, or do you get the 2 of them are accepted? And then you know, I'm not sure. In skin protocol because we normally return a 200 the 204 response tells you that information already. But if the convention is to give that it was applied. I'm open to that. We can I'll look at that up and refine it. Thanks for feedback. I noticed your location header value was quoted. That's that's Usually, location values aren't quoted. Okay. We'll fix that too. And I'll I'll I'll channel Mark here. Arngnoddingham has told we're not supposed to use the location header for this. but everybody does. So I'll leave that with you. I had questions about that too. Okay. Thank you. the discovery, this was to try and help So client figure out what's going to happen, and we came up with 3 scenarios, None, which clients should assume is the case that the servers don't support async request unless they've implemented it. or because of administrative centers, they there is no none there is no async response."
  },
  {
    "startTime": "01:14:03",
    "text": "long, and this is really the other two cases. Does the server elect to go ASIC if the request takes a certain amount of time if it takes more than 5 seconds, can respond asynchronously. And if the and if you want that to be a contract, and to say, I don't care how long it takes. I want you to respond asynchronously for other reasons, then you can say it's on request. So that was us trying to think of what are the possibilities, and we don't really wanna get too complex. I want to replay that against the the new feedback we just had on the call here. with the preferred response and I'll tie that in together. Danny. And in case if you Yeah. Go ahead. if you thought about about any, I I guess, you know, applications where I guess it would be a 4th sync request value of always. That's what request means. If if the client indicates that they prefer a sync response, then it always responds that way. Long to simply means, if you request it, I may still give it back to you if I think it's faster. Or sorry. I I realized the the big or the the end biggest part of my question. If the server would prefer to only ever respond asynchronously. would be a change to the protocol in my That opinion, and it it's up to the client to use the prefer header per 7240. There's actually a new RFC for that apparently. Okay."
  },
  {
    "startTime": "01:16:01",
    "text": "An event URIs was simply listing this these are the URIs, event types that the server can generate, which might mean it only supports the CRADA operations or it could support all the other events, there's the question though of just because the server is capable of generating. It doesn't mean it's willing to generate it. So We may have some more discussion there. have a question. Just a little bit of feedback. that setting threshold on long we've that Microsoft have spent months, years debating what that right value is. The other interesting option is in 7240. They have a weight. command which allows the client to say how long it wants to wait, before returning to 202, which it stops the service provider from having to go do it. And I'll give you one of the piece of data as we recently looked 202 responses that we get back from our API. and approximately 0.6% of clients actually bother to go polling after the fact. It's a pain for developers. actually use this stuff. So if you do it, yeah, it's cool, but you'll find a lot of clients just have code that say, It's a state successful status code. It's a 2xx. and they assume it works, and they move on assuming Yeah. That it has completed and the rest of the code may work or may not work. So Yeah. It's a double edged sword. the primary need that I heard was that the use case was that they had a lot of requests that take minutes to hours and holding the HDP connections open indefinitely a was"
  },
  {
    "startTime": "01:18:01",
    "text": "blocker. So they wanted to open up the doors and, of course, being the spec riders, we want everything definitive exactly your case come comments come to mind and Yeah. I I wouldn't wanna get too ambitious, and I fully expect most clients don't care But for those, I was giving we're giving 2 mechanisms. That actually would for one mechanism which would be the event stream, But if you're not implementing event streams, then the fallback is the location to go get it I think that that I I'd love to have more discussion on that. and see if you can how we can improve it. Alright. Okay. So I'll be on the next slide. Sure. So I have I don't know how much of it we're going to do. I have a demo We've got the i 2 SCIM server, it's we're currently working on code that we're developing as we as we're going, and I was making changes this morning. So this is not not even alpha grade stuff. but I'll comment that the implementation for RFC 89358936, which is set push and set pull went fairly straightforward. And then we have iTunes go signals, which is a store and forward system that handles event streams, deals with grabbing all the events and sending them to the receivers and letting you set up different deployment scenarios, whether you're having whether you're supporting communication through the cluster or communication between domains,"
  },
  {
    "startTime": "01:20:01",
    "text": "between different cloud providers and you need to have a hand off point that's the idea And it turns out that push and pull are both useful for different reasons depending on the layout that you're dealing with, The other reason I did all this work was one of the comments early on was that recovery was an important item. one of the nice things about having this kind of service is that If it's maintaining a stream context, then it's possible to reset the stream indefinitely. for a patient for cases like SCIM, you can also have streams only retain information for an hour because you might have a high rate of exchange because security streams are gonna be used for many, many different things. So a lot of the standardization for how that works. Is that to being done by the open ID shared signals. group, and they're sort of defining how those servers should operate from a management perspective what we've got here today is just 89358936 working with SCIM Servers and and a storm forward system called go signal. Go signal. Next slide. So what I actually have today, I didn't this slide is a little too ambitious. I have schedule server, 1, and it's sending events to scan server 1b, and back and forth. So they're they're essentially replicas of each other And the idea is I can put events to either side or skim transactions to either side an event flows through your signals. 1, and get sent to the other server And just to make sure we're doing things right, it actually go signals 1. We'll send it back to the originating server"
  },
  {
    "startTime": "01:22:00",
    "text": "as a closed loop and the originating server will reject it as a duplicate transaction because he had, of course, published the transaction to begin with. I've also got a polling monitor on GoSigning 2, so copy the event and how it goes to go signals 2, which would represent, for example, Could be workday passing events back to Azure and the Azure servers getting a feed of the events, and then they can decide what to do with it. which may be to send it to another SCIM server or just just process it in another event processor that decides what to do. So I'll share the desktop Pam has a question. I can mix this. Okay. Go ahead. Oh, Pam says she'll wait. Go ahead and do the demo, Phil. We're not seeing your screen. Is it I'm just trying to understand this tool. Oh, it's gonna do this. Is it going? It's gonna ask for permissions probably Yeah. Try and share something, and then we'll approve it. I did already. He I approved it, but it's not Okay. Okay. Yeah. Okay. So you So what I have is a It goes signals demo. There is a database cluster that that actually the 2 Go servers talk to. They each have a different database that they're using. I'll show you about a minute. So go signals 1,"
  },
  {
    "startTime": "01:24:00",
    "text": "receives of ads, you can see that prior to this demo, it received an event from one of the SCIM servers, which was to create an entry for myself, and then it checks the other events streams that are out there and sent those events on to the other partners. One of which was go signals to which then received the event and passed it on to my administrative client which I'll now turn on here. So this is running off of go signals to Hello? Okay. And I'm gonna update Let's see. First of all, Let's pull my record. make sure I have the current ID because I'd rather use a few colors. So So I'm gonna run this. I have a SCIM server running worked 9000 and another one on 9001. So we'll run this put And if I go over to Docker. We can see."
  },
  {
    "startTime": "01:26:00",
    "text": "So skin cluster 1 received that request and it generated a token here. And if I put that in JWT, it'll show you the modification, but just speed things up. getting cluster 2. It's received the event it's doing polling now to get the event on the server and it acknowledged that it received it. So over here, Remember I left this running as a polar, and it also received the event, and it saw the the 3 changes, and one of it was to change the profile URL to IT skin IO. So this is after 3 hops away, I'm now on a separate Domain. I've been receive notice of the change, and I can act on that change immediately. If we look at the database, just to give you an idea of what streams are light. So go signals too has received copies of the events. So this was the original Event, which created created the entry, and this was the Update if any There's put And then I actually break out for sorting reasons so you can pull out the event. One of the things that happens is that when an event gets received, the router goes through and looks at the streams that are defined"
  },
  {
    "startTime": "01:28:01",
    "text": "I have a bunch of streams defined and besides which which which stream should get what event and keeps track of it by putting in pending event. If the event hasn't been delivered, it sits in pending. And then once it's delivered, We have the list of events that have been processed. of which gives the the event number and the acknowledgment date so you can track what's there. what that lets me do then is if somebody needs to have their event stream reset because their database crashed or whatever. you can go back to this pulling provider and reset the stream And, basically, we're just rebuilding it from the events database we can rebuild the pending events for that stream So all of that's there. Sorry. I don't have a jazzy demo. Let me try one more thing. we only have, like, a minute left, Pam, do you wanna jump in and ask your question. Yeah. Why don't we take questions? Hi. This is Pam Dingle. I'm sorry. I didn't raise my hand in the app. and So in this case, it actually really matters that the client follows does check back for the async request, right, if the async request this thing starts with an async create and you send the SCIM event that propagates based on the assumption that it's gonna return, but it does but it actually fails 30 minutes later. you could actually get into a circumstance where where some of the clients think that this the request has succeeded and others think it's failed. Right? Because some of them received an event from a client that didn't bother to check back and some of them received an event from a client didn't bother to check back. Right? is that? so that I think we're talk we might be talking about slightly different scenarios. So"
  },
  {
    "startTime": "01:30:03",
    "text": "The async request starts with a normal SCIM operation. and they request they requested an async completion What I did with the putty was simply a straightforward skim request. and we immediately got back that it was created. Right? and what happened in the background was because I sent it to the skim server on port 90001, we wanna make sure that the other replica is updated. So what's happening is the ASIC Stream system is taking care of replication by pushing that event directly to port 9000 it consumes the event itself. So that's all done in the background asynchronously without client having to worry. So in this case, the client confirmed with the server sent the request to and got a response saying, yes. That request was captured and was successful And then in the background, all the other copies got synchronized. and also, my command line client also got a copy of that event because that's what I've set it TO DO Now The problem so now let's say this is a possibility. So say say your a company based on LDAP, you've received that event. It's a SCIM event, but you wanna take that piece of information and translated into your own processor and decide what commands you're gonna play on your side, So you can do that. and then you can go back if you find that you the system that was processing it failed somehow, you can still recover the event and go get get the old events back great thing is the SCIM servers are up to date. They don't have to keep track of"
  },
  {
    "startTime": "01:32:02",
    "text": "cursors and paging indexes. They just published the event it's no different publishing an event than say, doing logging because the the server just pushes out the event like, the same way it would log and it's done and it's gone into the signal's service. And it doesn't have to be go signals could be any shared signals framework. server implementation. So I'm doing a lot of work right now to make sure that SSF is fully capable of dealing with the scheme events in this way. So they're doing a lot of work on that, and pretty pretty happy with the open ID work on this. Thanks. Thanks, Phil. We are over time, we have to close this up, but thank you all. and see you. Oh, one quick note, side meeting for the SCIM is tomorrow. So if you it's on the Wiki tomorrow, unofficial side meeting for chatting about any of this stuff that we wanna talk about. that is in 11:30 in the morning at I don't remember the room. It's on the wiki. 1 of those type of interns. They're trying to get something. Yeah. Okay. Okay. work. We we're we're we're out Sorry. Yeah. I'm working on Interoperable state synchronization, And looking to do a a a a BAW for dispatch or working group, Next, IETF, I'm interested. There's a lot of state synchronization here. If you're interested in talking to me afterwards, I just love to share. Yeah. Yeah. hopefully see with the side meeting. Cool. Thanks, everyone. So I'm trying to see I'm trying to cancel tomorrow's meeting. Oh, okay. Well, happy"
  }
]
