[
  {
    "startTime": "00:00:14",
    "text": "you can still advance, right? It was going out"
  },
  {
    "startTime": "00:02:14",
    "text": "Okay, let's get started Audio's good Sweet. Thank you all for attendance this session of the congestion control working group, especially in our heavily conflicted slot with some other major working groups. A brief reminder that the session is being recorded and we're using the key slot with some other major working groups. A brief reminder that this session is being recorded and we're using the cues, so if you'd like to speak at the mic, you know the drill Make sure that you've scanned the QR code over there that also helps us make sure that we have a record of who was here so that they continue to give us nice large rooms to have these fun meetings in A quick reminder to actually please read the note well. Don't just look at it and nod your head and smile This is the terms under which we participate in the IETF, which especially covers certain IPR and patent disclosure requirements that you agree to by participating here Also note that we abide by a code of conduct, so please treat everyone with respect. And if you experience issues, you can come talk to us as the chairs or the OZBUD team Got a couple of helpful links if you're looking at a PDF version of the slides And with that, we have a super packed agenda today. So we're going to try to jump right into it. That said, you'll notice there's a rechartering discussion a little bit later on in here. So in our previous session at 119, we had a bunch of really good signals from the group about, hey, these are the kinds of things that we'd like to work on as we've wrapped up 5033 BIS. And so this time, we're going to go through a number of these different proposals and actually talk about, hey, the are specific drafts, how many of them do we think we can work? on at the same time, what would we like to adopt first? what do we want to move forwards with"
  },
  {
    "startTime": "00:04:02",
    "text": "So the context for each of these presentations is be thinking about is this something that the working group wants to spend time on? Are people ready to review and deploy and implement and actually read these documents? And especially of interest for us as chair how much do we think we can take on at once in a group? It seems unlikely that we're going to do like six different congestion controllers all simultaneously at the same time, especially to because that would deprive us of the opportunity to learn from one and apply those learnings to the next. So with that in mind, first off we, oh, good call, yes Would anybody, well, michael rosa getting set up to talk about rate limited sending, would anybody like to volunteer to take note? And as I mentioned in another session this week, you are distinct. Thank you, Gore, you are wonderful. Well, everyone else was distinct interested in their laptop. That is an awesome moment to make sure that you're on the Meetecho page in your browser so that you can participate in some of the show of hands and polls and things that we'll be doing Thank you so much, Gory. All right, with that we have a solid 15 minutes. Let's take it away. Okay thank you. This will be a short one. It's meant to be a bug fix of a strange oddity in the specifications that we have About when the sender is application limited. It all began with me looking at NS3 who were they were implementing the spec true to the spec and seeing a plot like this, which is just completely wrong and odd and strange in every possible way. And I said, oh, you know, NS3 must have, must have a mistake in there but it turns out it's a bug in the spec. I was back absolutely allows that when the application is limited or the receiver limits the sender but you keep on sending just not the full congestion window You can grow forever and then you can"
  },
  {
    "startTime": "00:06:02",
    "text": "you have that window that is ready to use So the primary motivation of this is just to fix this with a proposed standard RFC. That's what this is about Another reminder this is all unchanged since the last presentation We propose two rules. One is a must. That just says, you should, well, not allow what you just said right? Include the limit to the growth of the congestion window when flight size is smaller than the congestion window. And then we make a concrete problem but with the should, which says you should limit the growth of the congestion window, the flight size is smaller than the congestion window with a function call, which is a generic increase as a function of the maximum flight size, which which is a generic increase as a function of the maximum flight size. So the increase function will be whatever the congestion control algorithm would now use to increase for instance, TCP in Slow Start would do a plan one but not on the basis of the congestion window, but on the basis of this MaxFS s variable and max fs is the maximum flight size so far since the last increase and then in the definition, we have a rule saying, well, yeah I mean, if congestion windows come down, you reset it And at the very beginning, of course, that's also something you can take turns out that slow start the slow start part of the Linux implementation for TCP already does that with Wheel 2. That's actually exactly what's implemented there We presented that at the last IETF feedback that we got, we tried to incorporate It wasn't very much feedback One incorporation was that we have made new subsections on rate-based controls and page The guiding principle behind the rules that we have made is that when there is no congestion,"
  },
  {
    "startTime": "00:08:02",
    "text": "indication, a congestion control center, should always be allowed to increase on the basis of what it has sent in the previous round trip time So if I have sent a window of 100 packets and diamond slow start, I should in the next round trip time be allowed to send the 200 packets If the window could have been 100 already, but I only used 100, then that would be the basis to increase. And that logic holds irrespective of whether the sender is rail limited or not I mean, you have measured something on the basis of what you've measured, you should be able to increase Just to finish that slide, pace normally operates on sub-RTT time scales, so we think there's nothing we need to do to be true to pacing It doesn't actually affect it. Yes? A really quick comment, because I don't want to interrupt you, but looking at that slide I expect to agree with me in the absence of evidence absence of evidence is not evidence of absence The absence of a conjection in the case in the absence of an indication there's no congestion. It's just the cliche, right? Absence of evidence is not evidence of absence Okay but yeah exactly. I carry on. I think this is a wordsmith nitpicking. No, I didn't mean to nitpick. I'm sorry if it sounded like that and I must be not saying it clearly. Saying, I have sent no packet so I have no evidence of congestion. Yeah proves I can send whatever I want because there's no congestion, that there's fault logic there, which is your point. I'm not disagreeing with you I'm just backing you up that there's some faulty logic there Okay. Just because you don't, if you haven't tried to find congestion, the fact that you found none doesn't mean there isn't any. Exactly. Yeah yeah, I agree with that. But we were just trying to stay true with the regular congestion control logic, right? So we're not changing it anything about it really um next slide it"
  },
  {
    "startTime": "00:10:02",
    "text": "another thing, another feedback we got this that, well, we had kind of an open question in the draft of how many specifications should this update because there are many congestion controls somewhere in the spec here and there and as a result of the feedback we trimmed the list to TCP Reno congestion control quick SCTP and cubic and that's here and there and as the result of the feedback we trimmed the list to TCP renal congestion control quick SCTP and cubic and I think that's my last slide that that's what we've done. We would like the working group to add that Any comments clarification questions? We have Neil in the queue Yeah, hi So I think Stewart's remark is an interesting one to try to take on board and incorporate into this So one question I have about the way this back is structured is that it seems that the max fs is over potentially very, very long time period since I guess it's since the last packet loss or ECN mark or the beginning of the connection and so it seems like it's written so that we're using evidence from potentially minutes, hours, days ago to justify the increase of a sea wind now and so I'm wondering if we might want to structure it so that you can only use the flight size in the most recent round trip time, something like that That's how the Linux code that handles this kind of stuff is structured as it tries to look at just the flight size over the last round trip when deciding whether to increase now. And so I think like, so for example, in Stewart's point, I think is one way we can incorporate it is to say, you know, if we're not sending over, the last two hours, then that's absolutely of evidence of congestion, but it's not evidence of absence of congestion. And so we might want to say that we shouldn't be using a flight size that would"
  },
  {
    "startTime": "00:12:02",
    "text": "from before that very long idle period we should only be using, you know, a recent flight size something like that would that make sense or something like that I think my answer is that I think that new congestion window validation makes sense, which is an experimental specific that we have, which addresses this And yeah, I think that's my answer to that. So, so I think this should be left up to congestion in the validation and you know if you don't do congestion with validation, you could have a window of 100 packets and not do anything. And after 10 minutes, send it 100 packets if this is in slow start, we would allow 200 in that one round. So that's just staying true with the congestion control specs I believe either way you know this is just a shot it's it's it's yeah. ian swett, I want to ask that RC9, 2002 does not have the identity text that you put up there, but it has something that's attempting to be very much of the same shape of the normal text that you propose adding in section 7.8 about underutilizing the control window. In our draft? You had a you had some normative text at the beginning of these slides Yeah you're saying this is not matching the text from the draft I'm very sorry if that is to... No, no, no, no. No, I'm saying 9,0002 already has text that is quite similar to the two normative statements you have there, though not identical. Yeah Okay. So, like, I mean, it talks about page right, and you want, I think it requires the application to tell that you want to pace, which I think is unnecessary I mean, it has basically it says you either need to use the entire congestion window or not be pacing limited and you should not increase your congestion window. Yeah"
  },
  {
    "startTime": "00:14:02",
    "text": "yeah, okay. Is that not, I mean, minus the fact that it's a should not a must i think that's equivalent If, I mean, it requires the application to tell that it's pacing, right? No, you don't have, you can not use pacing as well with Matt doesn't you. Yeah, that's OK, yeah, but if you paste I think then. Then it's just, you need to use the C wind, otherwise don't increase it It will law out in the same way, I think, then on that, that right? No, you don't have, you can not use pacing as well with that doesn't do. Yeah, that's okay, yeah, yeah, but if you base, I think, then it just, you need to use the sea wind, otherwise don't increase it. It will law out in the same way, I think, that I agree with, yes. All right think then it's just you need to use the C-wind otherwise don't increase it it will law out in the same way i think then i'm on that that i agree with yes so we're just trying to find simpler words for the same thing that's basically. Yeah, yeah. That's fine. I guess some of my points, I'm not sure you way, I think, that I agree with, yes. So we're just trying to find simpler words for the same thing that's basically. Yeah, yeah, no, that's fine. Okay, I guess my point is I'm not sure it's worth bothering to update it Thank you, Ian All right. That looks like the end of the queue. So this is going to be a deeply interactive session, so I hope you all have your buttons ready to click on poll answers. We're going to do a whole pile of them today So for our first question who's read the draft? Wonderful. Click yes, please, on your left another second and then we will go on to our next fun question So this is good. So we've got about 10-ish people who've read it We'd love always to have a few more, send comments to the list and or file GitHub issues life is good for our got about 10-ish people who've read it. We'd love always to have a few more, send comments to the list and or file GitHub issues. Life is good. All right, for our next question We've got three for each of these. Excepted eight months my question"
  },
  {
    "startTime": "00:16:14",
    "text": "A quick start I mean, I got you No. Just, that's the question noise All right, our next question is, is this a problem we actually want to solve? You see my difficulty is going to be fun if questions don't work Yeah we've each clicked to start who would like to actually raise their physical hand and say this is a problem that we should take on? Very nice All right. It's kind of sad not to make a humming noise if you want to make a humming noise at the same time, go forward All right, no worries. We'll get some connectivity soon Wonderful, thank you so much That is, what would you say? 30-ish you want to put them up again and I'll count faster Wow, estimating is hard 18. Who thinks we should not work on this? anonymous shows of hands are more fun All right, and if anybody would like to chime in in the chat, that is one wonderful as well. We have one extra from the chat so make that 19 for the notes The Q tool is also stuck Gotcha. Fun time Welcome to the Q. What would you like to say, Matt? mathis. I think one of the things that wasn't really touched on directly, but there does need to be a very clear boundary between this work"
  },
  {
    "startTime": "00:18:02",
    "text": "and a new experimental C-win validation work because as Neil noticed, there's a slippery slope between the two. Exactly. That's what I'm trying to do So we're trying to keep this very tight to just this Yeah, thank you Ooh, magic for our third question We'd like to use this document as a starting point for that solution Questions can anyone respond successfully? Oh, we have one lucky, one lucky individual I had to refresh the whole Meetecho interface if that helps anybody Yes need some pacing, I think Thank you right, if there's anybody who can't get to the poll and wants to just raise their hand, that's also okay, and we can do some back in the napkin math, but it looks like it's working now Fantastic. All right That is a very good signal. Thank you all for your button clicking and your excitement and willingness to work on cool things Thank you, Michael. Thank you Okay, next up, yes, we'll refresh the slides in a second. Next up, we have Neil to talk to us about BBRV3 I think you should have slides control, Neil. Okay, yes, okay great um thanks everybody um i wanted to just give a quick update on the status of the"
  },
  {
    "startTime": "00:20:02",
    "text": "bvr draft and the nature of the changes this with this new incarnation of the BBR draft, this is a CCW draft. And there are so far three co-editors my myself and Ian and Joseph Bichet from Meta and then so just a quick overview of what we wanted to talk about today. We just kind of wanted to outline the changes in the draft and provide a roadmap for readers and maybe implementers of VBRV3 who are reading the draft and just wanted to invite folks to read it and offer comments and gain interest in making this a working group item So at a high level, you'll find a link to the new draft, which as I said, is in CCWG. These are the co-ag co-editors. Obviously, this is work-based on a lot of work from a lot of people over the years but these are just the folks who are working on the editing of this particular draft And the high little changes since the previous version of the draft include the following. One big change is that we've moved to the draft to GitHub to make it easier for folks in the IETF community to collaborate and offer pull requests and issue file issues things like that We have incorporated the delivery rate draft which used to be a separate draft we've incorporated the text of that draft inside the BBR draft just to make it easier so we're not having to maintain two separate drafts with dependencies from one to the other And you can click on that get commit your just to see exactly how we did that The other changes other main changes are"
  },
  {
    "startTime": "00:22:02",
    "text": "several bugs fixes and parameter tunings that we talked about last July in San Francisco And then you can find those in the Git commit log by, you know, just looking for the string BBRV And then there's a fair amount of just editorial wordsmithing for clarity and readability and updating references So I have a couple I eye charts here for folks who want to drill down into the detail of what changed. This slides summarizes the BBRV3, the main changes that are interesting which we already discussed last July in San Francisco And then you can find here the correspondence between these slides that discuss the changes and what those changes look like if you visualize the dynamics of the flows and then the course corresponding get commits that update the various area you the draft that are describing those changes to the algorithm and there are two books fixes there and then a set of parameter tuning that are basically inspired to reduce loss rates while keeping throughput pretty much the same And then the is a collection of more minor changes that are less interesting various small tweaks in bugs fixes and changes that were made over the years to the code, that we wanted to update the draft to reflect those as well And yeah, and that's pretty much it this is just a reminder so this is code that's running at Google all of the internal when TCP traffic at Google is using this, and then all of the TCP traffic from Google.com and YouTube to users over the public internet is using this, and we do"
  },
  {
    "startTime": "00:24:02",
    "text": "have some experiments on matthew quick side that are running code that is pretty close to this, although not exactly the same. And I think the goal is to converge matthew quick NTCP code to be using pretty much the same algorithm But we're just not quite there yet due to developer resources resources And yeah, so that's pretty much the update We're inviting folks to read it and offer comments either in email threads You can file GitHub issues, you can post GitHub pull requests, whatever feels most convenient And yeah, we just wanted to engage interest in moving further with this. Thanks everybody Thank you, Neil John is in the queue. Yeah I had a question at the AANRW yesterday there was a presentation that claimed BBR3 went backwards in terms of fairness, being closer to BBR1 and BBR1 And I don't really know any details about which version of BBRB3 that used but do you know anything about that? paper um i'm not sure if i know anything about that particular. I'm not sure exactly which paper that was i have seen some papers go up in various places that that um that have that kind of result and i think if you think about the dynamics of the changes, you can understand why that would be And we've been able to reproduce issues like that in the lab as well I think you know, this is an area where we'd like to continue to make improvements We're open to suggestions and you know I think we do hope to make, you know, continue to make improvements over the years in coexistence. It's a kind of a thorny problem but, you know, we'll continue to work"
  },
  {
    "startTime": "00:26:02",
    "text": "on it. If you want to shoot, if somebody wants to shoot me a pointer to the exact talk, that would be great Yeah, I'll send it to you We have a job that's almost in the queue i am in the queue i just can't get to the QR code because i think it's behind Neil on the screen Hi, Neil. Hey, good to see you. I wanted to see that backward fairness sounds like a really exciting thing to think about. But I was going to say here that the this seems like a perfect going to say here that the this this seems like a like a perfect candidate for like an experimental i don't know what the goals are for CCWG in terms of, but to the chairs at least what I'll say that seems like exactly the type of thing that you want to be doing here So to the question of making it a working group item I know it's not the question that chairs have asked but I think they should absolutely be it's, it's relevance and it's importance to the community is well understood i'm glad that um that are contributing to it. This, however, shouldn't mean that it's a standard that we encourage everybody to deploy It should just be something where we are able to document the currency state and be able to drive it that way Thank you. Yeah, I think so we're going to talk about this a little bit later on in the session, but we absolutely have the ability to choose between experimental standards et cetera, and a lot of that we've talked about kind of having our bar be something that is widely deployed, provably safe all that kind of stuff. And so as we're talking about, you know, can we meet that bar with either the current form of these things? or changes to any of the congestion controllers? With each one, I think a lot of what we were doing in 5033 biz was defining that criteria for where do we fall on either side of that split based on the evidence that we can gather from each algorithm Matt matt mathis. Um, one of my worries in a bunch of different places"
  },
  {
    "startTime": "00:28:02",
    "text": "is that there's much too much emphasis on what I refer to as bug compatible with Reno And this is something that this community is the right community discussed, is to find out whether or not where to draw the line between if the scale is bigger than such and so, Reno has no hope of doing what so don't worry about it And that, Neil has drawn that line particularly conservatively although it's probably moved since the last time I looked at it And I would rather move it back, but we can do that. Neil can't do that And so that's why I want it to work on here here Jana? I think Neil can do that Neil, can you push a commit to do that? Sorry There was a joke but I think what I will say is that, yeah, I mean, I think create a fuckery, no working group but I think this might be it actually That's the point. No, but I do think that there's the relevance of real world data from deployment is very useful. And I think we should bring that into this room but that's the point of an experimental to me is basically published something and then you get data on it and you and you share that so yes christian huitema basically you publish something and then you get data on it and you share that. So yes. Yeah, I mean, I think I'm with math on this because the change, one of the changes in BBRP is to space out the prop up transition much more than the previous version. And that's nice from the point of view of not being worse than cubic, but it's bad for us because that means that it takes a long time to detect that the bandwidth of the link has increased"
  },
  {
    "startTime": "00:30:02",
    "text": "And so that's really a trade-off and yeah the working group shall opine on this kind of trade-offs because is it okay to make your solution worse because of cubic? And I don't know That's exactly the problem I was referring to that BBR has been made sluggish and somewhat cautious in environments where it wouldn't need to be for technical reasons. It's that way for policy reasons because it would not be appropriate for Neil or Google to unilaterally say, well, we're going to deploy a congestion control that's going to hurt everybody else This working group can make the decision of value judgment at what level of features we're willing to tolerate Yep, and I think that's one of the reasons that we're all here together to talk about this stuff. And I think one of the other things that we're talking about is we consider making BBR working group item is as we hand over change control to the IETF and to the working group where we go with that debate is something that we all need to be on board with At the same time, we all are realistic humans who recognize that the thing that we write down and what people choose to ship are the same because we all have goodwill and we understand the risks associated with diverging from that, not being we have any real authority to force implementers to do something martin duke, I mean, obviously, 53rd BIS is a lot about this. So we have a guiding document for it, but I would like maybe echo I would like to also encourage Neil to maybe be aggressive because I think it's easy for the working group to rein him in in terms of being aggressive about stuff than like for us to make BBR more aggressive. So I would air on that side to the extent that you have data that you can support it, thanks Yeah, I guess I'm next in the queue"
  },
  {
    "startTime": "00:32:02",
    "text": "So yeah, I just wanted to say I agree with the you know, the, everything that I everybody's been saying so far. I just wanted to also add that you know, definitely it's not my goal to be bug compatible with Reno And I agree we don't want to spread out the bandwidth probing too far and give up the potential benefits of probing more quickly. Where I think BBR as an algorithm has a lot of room to improve is that I think by being smarter, about potentially using more signals like RTT signals as short-term indicators of conjunction it might be able to do a better job of both keeping cues low and reducing loss both for the sake of the BBR flow itself and for the sake of the throughput of any Reno or cubic flows that might be coexisting with it. So I think there's a lot of room for improvement both in the general health of the cues and in the speed of the acceleration and in renal and cubic coexistence. I think we can make improve all of those together and get some nice results but it's just going to take some work Cohen is up next and then I think John actually in the queue. Yep, can you hear me? Yes okay. Yeah I think there are opportunities to indeed improve let's say the Reno behavior, or at least for the classic the loss scalability or the responsiveness to loss But like for dual pi square, it is important also to have a kind of a reference to measure what is now"
  },
  {
    "startTime": "00:34:02",
    "text": "the assumed rate in the in the classic category and how do we drive L4S Alvarez to match more or less that rate So just throwing away everything and of course it's good to to avoid also loss in the class category, but as long as the are loss-based congestion controls there should be some way of coordination and signaling let's say what is the assumed rate here, and I guess also delay-based congestion control at a certain point will experience laws and need to match this. That doesn't want to say that we should be Reno loss compliant I think there is an opportunity to make a a certain point when the rates are higher and the GDPs are higher, we do that already with cubic to go a bit farther because also cubic becomes unscalable. Is there a way that? we can make this loss pattern more scalable? That means there will be more loss potentially on high link or we can sustain also more loss on high BDP parts But I see there an opportunity to let's say, correct that curve from that point but I wouldn't want to throw away the whole mechanism because I think it's important mechanism to signal in a loss-based classic category, what is around the right rate or the fair rate that we can use Thank you I'm again. I don't have anymore jokes this time I'm actually trying to understand what Christian and Matt said means Because if you say that I think it's perfectly fine for Neil or Google or"
  },
  {
    "startTime": "00:36:02",
    "text": "anyone for that matter to deploy something that is more aggressive, they do it all the time. They're not asking us for permission anyway to begin with But if we only document behavior that we find except to our brains, that's a problem. I want Neil to document the stuff that's happening in the wild because it's a way of understanding how to imagine the protocol that's running there There's a difference between experimental and recommendations, right? If we are saying that by recommendation, we recommend that you can be x times more aggressive than Reno I don't know where we stop because then it's not just Reno, it's cubic, and then it's L4S, and then it's what else and then it's different conditions and situations so i don't know that we can really go down the path of saying, is it okay for Neil? to be more aggressive? Well, Neil's not looking to us for permission to be more aggressive at Google. But I'd like him to document the work how it's actually deployed here. This is why I said that experimental to me seems like the right way to do it, because that says that there's an experiment there's data that can keep coming to the working group from it but ultimately we are documenting something that's happening and the decision is that are being made elsewhere. We can engage with those decisions and try to understand the consequences of those decisions and try to influence them But I don't think that we can really say how much more aggression is OK okay or not. Nobody's going to care No Mozanetti, on the fairness and aggressiveness topic, just to catch Neil up, I think the ANRW paper, it took like about an order of magnitude of of Reno or cubic flows to achieve parity with, with BBRP3 rate and that wasn't alarming to me because while the, you know, the research methodologies typically just flow in an abstract sense if you look at the operational deploy you know, a lot of this is"
  },
  {
    "startTime": "00:38:02",
    "text": "geared towards eventually going over quick and that's a very heavily multiplex transport usually So it's not unreasonable to think that what we're calling flow is actually, you know, 20 or 30 subflows. And so that begins to muddy the waters about, you know, the traditional, you know, research approach where you want things very controlled and so you want to you want the variables to be controlled and not have, you know, these, these know, research approaches where you want things very controlled and so you want to, you want the variables to be controlled and not have, you know, these degrees of extra degrees of freedom to have all this multiplexing and everything else. But in the real world deployments, I don't see a problem with having an understanding that this heavy flow could be eight to 12 or 15 times of a reno or cubic flow. And way back in IETF 84, when we had the congestion control work that founded RM CAT, that was the first conclusion that almost everybody arrived at, that TCP friendless would not be a goal for a single flow, but it may be weighted fairness for in you know, TCP flows. And that was deemed okay for a single media flow back then. So now, today, when we look at a single quick connection flow that may be 20 or 30 sub streams I think it's okay to veer off from stream flow friendliness compared to other congestion controls Thank you, Mel. Zahead would like to make a comment So, uh, just want to understand a bit like what Yonah you're talking about. I mean, I mean, bringing, so when I was designing construction, I was designing, I was designing everything, the test case test bed and everything on my use case, right? Then I come to ITM and show something and I got a feedback, like, hey, yeah, you have not looked into this kind of thing, that kind of thing and all these things. That improved the congestion for that improved how I was testing my"
  },
  {
    "startTime": "00:40:00",
    "text": "congestion control and all this thing. That's the value value basically nil or whoever comes with it congestion conglum to CCW will get the benefit of like looking into these things so my thinking is like yeah, it might actually start with experiment some things, but at some time we need to decide, like, is this good enough for our internet? It's just going to go for like a standard or like more than experimental thing uh i i understand like they're like b s one because enough for our internet? It's just going to go for like a standard or like more than experimental thing. I understand like they're like we're one, we get two, we're three, maybe four, five might be coming But somewhere we need to say like, okay, this is it. This we have done on enough, and we're fine with this one and this working group then produce. Is that something that aligned? with what you're saying? John, or you'd like to see like this all the condition cultural work happening in CCW actually the outcome is experimental Because for me, condition control, improvement is a never-ending thing But sometime, we need to just decide, like, this is good on enough for our internet. We stamp that one ship it, hope internet doesn't melt down but we keep improving after that. Right, we're never going to stop it, right Yeah, go for it, John. Just to respond quickly to that, what do you just say? to me is an experimental. I don't want to get into the document type conversation, but I think that literally to me is, I mean, yes, it has to still go through. Otherwise, it will be in informational or something, or independent stream, right? If you're just probably, what you're engaging in the working group, you're taking feedback but ultimately when you push it out there it is an experiment and we hope the internet won't melt down, as you said but you come back from that and you do reps of the document later. But to me, it is still an experiment I want to be careful that this is the gap between recommending what people should do on the internet versus the experiment. That should be clear I mean, the thing is like, I think I understand. I understand your thinking but i mean the one of the reason that we created this"
  },
  {
    "startTime": "00:42:02",
    "text": "working group to actually have a standard condition control algorithm If that's never been, never will be achieved I think then we need to rethink what we're doing here because you can do this kind of things with ICCRG and stuff like that. So I mean as I'm saying, at some point, the community has to decide what's good enough and then ship it, and keep doing the research I mean, every time, every, I mean, today I saw something like, okay, they were saying like, hey, BBR is bad in satellite geos geo-satellite network, but that might be their, they're looking at a different perspective than Neil have been looking at it So, I mean, we can learn from each other definitely, but sometimes we need to say this is good enough. We have improved our experiment with our condition control algorithm in the extent. Like we have touched upon the coroner cases, we are satisfied with what we're doing we're satisfied with the fairness and everything we're talking about and we are happy to shape it as a more than experimental perhaps. That's my thinking I mean, that's what I'm saying. Thank you. All right. We need to keep moving timing wise so we're going to start a poll um this isn't just have you read this particular draft, but have you read any of the recent BBR's in general? And while you're responding to that, a couple of thoughts on the discussion we were just having, one of them is very much, the value that we bring as a working group is a place to have some of those ideas to think of what is this edge case that we should test. But at the same time, one of the things that I think we've all said we explicitly don't want to do is ask authors and people who are bringing congestion controllers to go fetch infinite rocks of infinite cases. And so a conversation that we are like, to have, we talked about this a bit at the hackathon we'll keep talking about it more in this room, is how can we as a working group contribute to helping make these pieces of data obtainable, right? So how? can we help people have a test bed? How can we help people do hackathon style? participation where we come and you say, hey, I want to try a data?"
  },
  {
    "startTime": "00:44:02",
    "text": "different tuning for my congestion controller, let me test it and how does it interoperate? How does it compete with different flows? So a big part of this is also what can we as a community do? to enable that kind of experimentation and enable that kind of data collection likely in simulation to help provide that input Then I think we talk about especially CCWG as an IETF working group we build on top of that by using actual real world data from how is this going, how does it behave in the way at scale on the real internet and things like that So to Zahad's point, I do think we are trying to have actual proposed standards in some cases. It doesn't have to be everything that we do here but we are never going to stop iterating That doesn't mean that we don't say that, you know, we've run this through our suite of criteria and we think that this is sufficient for us to go with right now All right. So we've got 34 folks with a yes on having Reddit and up next we get to talk about is this a thing that we should work on? How is our refresh going? Hey, it works. All right, we get one poll per refresh I was going to say you may off you may also need to refresh every poll It's a real test to see if you're awake So this is really what should the working group take on BBR as a congestion control algorithm that we think is something that we'd like to standardize? we'll discuss exact standards track for experimental versus not christian hopps a clarification question. Yeah a clarification question. I mean, that I am not"
  },
  {
    "startTime": "00:46:02",
    "text": "sure about the rules of the game because BBR up to now has been a Google project So is it clear that adoption means that it be becomes an IETF project? That is part of the plan, yes So the working group, the congestion control working group here would take on change control for this BBI. That doesn't mean that we're going to ask Google to stop iterating or stop making improvements, right? And you confirm that's exactly what Neel is proposing. But that is the conversation that we're having, yes So I think a big part of this comes to you know, when we shipped TLS1 we did not decide that that was the end of it and we weren't going to make any further improvements ever, right? So this isn't quite the same problem domain or space, but this is are we willing to say, the IETF is going to recommend, ooh, nice refresh the IETF is going to recommend? that we, if you would like to use BBR, this is a good form of BBR that should be safe to use and we've just spent a lot of time talking about what that safety criteria is in 5033BIS right? Oh, it was so close to putting up the 30 Just to check for the notes. Did you say that the intention is to give IETF change control? to the document? Yes, correct All right, and for our third poll and our fourth refresh the current draft is a good starting point for this work, i.e do we take the current BBRB3 and evolve it? to meet our criteria?"
  },
  {
    "startTime": "00:48:02",
    "text": "One of the things that we are really good at is a community is coming up with random weird edge cases that we'd like people to go test. A big part of that is we need to I think, help test that and help bring that data But when we talk about, hey, we can't force people to deploy changes because this is the IETF and we write documents we are also asking people to have implemented ideally more than just one, right? So we're looking for multiple implementations, interoperability is a little bit different in a congestion control space than with many IETF protocols. But we're still very much looking for can we prove? that these are safe and use real-world data to motivate? changes that we're asking people to make Can I add something? Yes, absolutely Thank you. So again, for 233BIS has a list of test cases and use cases So this is not strictly a question to bring me a rocks. I think we have protections against that. Now, that said, it's meant to be a somewhat timeless document and so it doesn't like specify, you know, link specifications and all that. So there is some wiggle room there, but this is not just, like, whatever we kind of so it doesn't like specify you know link specifications and all that so there is some wiggle room there but this is not just like whatever we whatever we kind of improvise as a test case there's actual like a battery thing we're going to look at absolutely yeah and i think operationally we do really be interested in chatting with the group about how do we turn? that into something that is approachable for people especially researchers and folks in academia who'd like to propose changes to one of the algorithms that we're talking about. Would anybody who said no to this document is a good starting point for this? solution like to come to the queue and tell us why? We had two folks who were uninterested in this particular starting point All right, we've got Michael in the queue. Yeah I think it's very long and has a little Michael in the queue. Yeah, I think it's very long and has a lot of pseudo code, a lot of pseudo code and I'm, I'm, uneasy about that. I mean, I'm you know, there was not yes, no, maybe. I mean, uh, i'm unsure if this is the ideas"
  },
  {
    "startTime": "00:50:02",
    "text": "of starting point. I mean, I have a feeling that if we, if we want, if we would start with a shorter one from scratch, we might end up with it perhaps simpler and cleaner document that doesn't have so many heuristics nailed down and built in as a hard-coded suggestion but thank you so some of the concern that the pseudocode contains choices that the actual text does not? not? In that the pseudocode picks particular things that are not otherwise specified in the text, you must use this value or something like that. Yeah, it might be too concrete as a table of all these parameters in there and there are I don't know, I'm unsure you asked me to come so i'm you know i'm unsure about it don't want to say it a bad starting point necessarily i'm not convinced it's a good one thank you All right. Moving on forwards in the interest of time, but this is definitely a discussion that we'd like to continue both around starting point, and I think you'll see the meta discussion of how do we decide that something's good enough to cut a version and actually go with it. It's a beautiful thing. Next up we have christian hopps talk to us about some changes we could make in BBR Is that on the close? We have some flexible cameras. You can stand anywhere talk to us about some changes we could make in BBR. Is that on the close? We have some flexible cameras. You can stand any one. Okay. Good afternoon. I'm christian huitema and it's a bit of a continuation of the previous discussion Pardon me? Okay good afternoon, I'm christian huitema, and now I'm speaking into the mic Okay, so this work is work we have done with my colleagues, Sua and Cullen, on doing youth this work is work we have done with my colleagues, Suez and Cullen, on doing, using BBR to control real time communication And basically what we are testing"
  },
  {
    "startTime": "00:52:02",
    "text": "is sending media over quick and the media is structured in a way that is very similar to what people may know as hierarchical encoding It's actually a simulcast in which for a given media stream we are sending several versions say 360p, 720p, 1080p, 4k and then we are using the congestion control in real time to make sure that in case of congestion, the low, the low gets priority over the high bitrate So that is done inside matthew quick packet scheduler to know which stream are sent first. So in case of congestion, if I don't have enough banners to send everything, I would send the priority packets that belongs to the stream for 360p or 720p and not necessarily send up Wait a bit before sending the stream for this higher definition. The goal being that in case of congestion, the user sees continuity of the video without extra latency at the price of a loan definition So basically the point here is that to do that, we have to sense continuously the state of the network And we sense the state of the network by looking at the state of the congestion control which in our case is implementing using BBR to answer the previous question, I did actually implement a structure the state of the congestion control, which in our case is implemented using BBR. To answer the previous question, I did actually implement a BBR based on the draft that Neil published So we do that. So in theory, you see it's perfect. In theory, if the network gets congested, we stop sending the high definition video, we send the high, the low definition one. And when the"
  },
  {
    "startTime": "00:54:02",
    "text": "capacity comes back, we send the high definition again and we are always i mean the best quality without overloading the network In practice, there are a few problems There are a few problems at least here, which are problems with BBR The first is doing the start phase. BBR obviously was designed to facilitate web traffic and traffic which is being streamed and things like that. The start phase stops when we have for three years no increase in the measured bandwidth And at that point, what BBR does is because you are almost at the right value you have actually almost estimated the capacity of the link So it's starting sending data at 2.77 times the bandwidth with the sea wind which is set to twice the BDP, twice the product of the mean RTT by the estimated boundaries What that means that basically your era will be bannwis limit, will be sea wind limited It will law for two BDPs, so it will law for two RTTs. And do during that time, the Q will raise two BDPs, effectively So you are basically getting twice the delay for three eras, which means six RTTs If you are doing real time, communication, twice the delay for a long time is not good So we'd like to CBBR do that as probably kind of stuff will do in the working group, discuss that we have a big dependency, however,"
  },
  {
    "startTime": "00:56:02",
    "text": "on BBR is because of the fixes for cubic the initial estimate matters a lot If the initial estimate is too low, because it will wait a long time before coming up it will remain low for some time, which is not good What did I do? We'll need nothing wrong We'll come back. Keep going. Okay We have then other issues which are linked to the fact that we are working a lot with Wi-Fi networks A lot of the video is sent to computers that are sitting on Wi-Fi networks. And Wi-Fi networks have issues of their own like the suspension, or the fact that they can go bad very quickly, which I will show in the next slide. And then we have two issues linked to the bandwidth limited nature of the video. Because even if we send 4K, in most cases, when the video codec is just sending differential encoding, the battery is not that high so the bandwidth will be lower than what the link actually permits, and the application will tend to be bandwidth limited That will change every five seconds or so when all the videos are getting refreshed, because at that time there is a peak of traffic But so we have this alternance between peak, rare pick of traffics and relatively low bandwidth And there are a lot of issues there, like we observe a downward drift of the bandwidth over time because of interaction between bvr and the application and we also observe that in some cases, the state will remain in the probe bandwidth state because when the application is bandwidth limited, BBR effectively never exists the problem with state"
  },
  {
    "startTime": "00:58:02",
    "text": "so next. So first let's go back to Wi-Fi. The one thing we really hate about Wi-Fi is that every second or two, your Wi-Fi, driver decides to stop the radio And they stop the radio for we don't know exactly why, but we believe it's because they are switching the radio to other frequency to scan those other frequencies And the net effect is that you will see the transmission just stop when there is no radio left Packets are being queued. You are not going to lose any packets. Those packets are being queued. They are being queued before the driver on your station or at the router in the Wi-Fi network because when the driver does that it first sends a Wi-Fi control message to the router saying, hey, keep all the packets for me for some time, please, so the packet are being kept And when the driver decides to come back, when the radio is turned on again, all those packets will law very rapidly and situation will back to normal That doesn't play well with BBR because what you have is you'll get an RTO because one other and situation will back to normal. That doesn't play well with BBR because what you have is you'll get an RTO because one hundred milliseconds is larger than the RTO You may get several Rtheos in fact So your bandwidth will say, hey that link is almost broken. Benwheels will fall to very little, even when that comes back it will take some time to come up again So that's not effective very little. Even when that comes back, it will take some time to come up again. So that's not the effect. The other effect is because it takes an RTO to detect the situation doing that PTO, the station continues to send packet And those packets that continue to send are typically the wrong"
  },
  {
    "startTime": "01:00:02",
    "text": "packet. You will continue to send the high-definition packet wen lin fact, given the low bandwidth, you would like to send the other ones there height of them. Basically, you send like to send only the high priority packet but you get a part the high demand you basically would send like to send only the high priority packets but you get a priority inversion so you have a minute and a half Okay with a question you get a priority inversion. So you have a minute and a half. Okay, we have a question Yeah. If you want to take a question. Okay next problem is that even if it were not for the suspension, Wi-Fi can go back very easily Basically, it's suffice that you move your device a little bit and the antennas come out of a light or somebody moves in front of the antennas. And you can easily, we have cases that are very easy to produce in which the transmission drops to very low bandwidth The low threat increases a lot. So basically you have the link has changed. And BDRV tray kinds of deal with that but it deals with that slowly. It deals with that after the first PTO, he decides to reduce the window etc there are mechanisms but net net is that you have all the same problem as in suspension at the beginning, like priority inversion. It takes a long time to realize that you should only send the low bandwidth packet And after the benefit is bestore, you have a very slow ramp up which means it doesn't go very well so all these are kind of stuff we observe in practice and I would like to see fixed are we are running our time so if you read the draft, there is a lot of proposed figures basically make the startup more like I start plus plus exit the proof"
  },
  {
    "startTime": "01:02:02",
    "text": "banaries when the deal increase, etc And I'd like to make a another slide for one other suggestion we have Done Oh, okay. It's basically of detecting the loss of feedback a lot of the issues we have is that it the link disappear or gets back, or get bad, it takes us a long time to find out basically you have to wait for a full PTO to find out and what we do there is that we observe that in normal condition you are going to receive a steady stream of acknowledgement And if a suspension happen, or if you're trans transmission goes really bad, that stream of acknowledgement will disappear And we want to basically notice that that stream of acknowledgement disappears because we can notice that much faster than notice that we don't jared mauch faster on a PTO and that's as the diagram shows. Okay. And so we have added that signal as a control signal for BBR, that if we have that signal, if we have this loss of things, okay? And so we have added that signal, as a control signal for BBR, that if we have that signal, if we have this loss of feedback, we basically freeze the window get into what BBR calls the conservant mode, so that we don't pile up more packets in that thing, because piling more packets when you should not be sending them is bad for it mode, so that we don't pile up more packets in that thing, because piling more packets when you should not be sending them is bad for the whole time. So that's what we do and we go back to normal So basically, we can match the issue at least with the values solutions that we have and our goal is to basically make our BBR colleagues. Our goal was to make our BB colleagues at Google aware of that And please consider updating the"
  },
  {
    "startTime": "01:04:02",
    "text": "BBR draft. Now, if the BBR draft in fact becomes an IETF draft, I mean, that becomes, make our IETF colleagues aware of it the BBR draft. Now, if the BBR draft in fact becomes an IETF draft, I mean, that becomes, make our IETF colleagues aware of that and get some proposals change on the BBR draft. So that's what we're doing now socialize the issue Suas has prepared a video that shows the experience the change in results changes We can have discussion all the details. I'm sure that's Squat has plenty of ID and will tell me I'm wrong Oh, no So we are at time. So please, we're going to drain the queue, but try to keep the comments quick Stuart Churchill from Apple, uh, I don't remember every telling you you're wrong, but I don't know, maybe Anyway, I came to the microphone because I just particularly wanted to 100% agree with you about Wi-Fi suspension. It's funny how the time works out just in the last few weeks. I've been working on this in the context of L4S and just basic TCP retransmission time that when TCP is trying to measure the round trip time, or quick or anything, and compute the retransmission time out if you're sending a bunch of packets to the Wi-Fi layer and then it just sits on them because it busy doing something else, it's doing a channel scan or something then that can result in spurious retransmissions And our conclusion was you really should be made measuring the round trip time from the time the packet leaves the device not from the time you hand it to the firmware and then sometime later it actually leaves the device. That's a partial solution Sios. There's a partial solution because it's the other side as well absolutely yes um yes i i just came to say i think for all of us in this community better cross-layer communication between the Wi-Fi driver and the IP layer and the transport protocol"
  },
  {
    "startTime": "01:06:02",
    "text": "about what's going on would be really valuable. If the Wi-Fi layer is going to sit on the packets for good reason it should tell the transport layer, you know what, I haven't sold, sent those yet and you can keep giving me more, but I'm just going to cue them up And that would be really valuable information, especially when latency matters and doing any real time the stuff we didn't care about 10 years ago is really critical now So same conclusion, different background Yeah, severe stuff we didn't care about 10 years ago is really critical now. So same conclusion, different background. Subir? Yeah, thank you. I'm Subir. I have a question on the Wi-Fi site. Is it what kind of which version of Wi-Fi did you use in the experiment? Is it Wi-Fi 5 or Wi-Fi 6? We have measured that Suez, can you... I think we did measure on Wi-Fi 5 and Wi-Fi 6 and different platforms. So the next quick question is that when you are sending this video, are you also listening the audio? Yes Okay. So then I understand that why the suspension reason is there because audio is coming through Bluetooth, right? No, audio is blocked as well at the same time. It's blocked Okay, okay, that's fine. But I agree with Stewart that if there is a data available on this, the behavior on the Wi-Fi, I think it would be very helpful. Then I can actually take it to the IAA. Yeah, I mean, that's true Okay, let's move on to Suhas So just a very quick comment about that, because I don't know IA yeah I mean that's true okay let's move on to Suhas so just a very quick comments about that because I know not everybody's an expert on Wi-Fi there's lots of reasons your Wi-Fi radio like if you're walking around the house, it might want to scan the other channels to see if there's a better action point to roam to. There's a lot of complicated stuff going on And there are good reasons for that. We just have to deal deal So I'm from Cisco and Mark can suggest. So one of the things with the mock working group is to make mock work for real time"
  },
  {
    "startTime": "01:08:02",
    "text": "calls as well as the streaming. And we started playing with BBR and we realized that BBR is really good algorithm but it does not work very well for the real-time use cases. And that's a gap that we want to fill so for adapting mock to support real-time use cases some of the fixes that we've been working on for a year and half of experience it has shown a clear difference in making it work for the real-time requirements We would like some form of the solution to be discussed in this working group and consider for the next version of the BBR. Thanks. Thank you Magnus. Yes, I would think about this based with and the issues with the application limit and the codex. I mean, that's why if we're in scream has some rules that saying, okay, you can actually send faster than the congestion window within limits, but just so that you can get your variation due to, for example, video codex out in a reasonable time and try to maintain the pacing, et cetera just so that you can get your variation due to, for example, video codex out in a reasonable time and try to maintain the pacing, etc. So I wonder how pragmat we want to be in the future here on these things and saying how for certain type of media or data streams, how strict are you to do for following the congestive controller? when you actually have data to send and you sometimes application limited limited? no it's a comment okay actually have data to send and you sometimes application limited. No, it's a comment. Thank you, Magnus all right we're going to do a quick poll here of I have read at the draft. I think it's sounds like our goal here is to kind of fold this into some of the BBR work that we're doing rather than have this be a separate document And I think this is a really interesting mechanism so definitely worthy of additional discussion. We've had some good comments so far Cool, okay, looks like we've got a couple folks who've read it, some folks who might go read it soon and then we'll kind of fold this in as we talk about some of the other BPR"
  },
  {
    "startTime": "01:10:02",
    "text": "topics. Thank you. Thank you, Christian All right, next up, we got Rui to talk about some HPCC Plus Plus Thank you there? Hmm All right, we can see that you're in the session, right anybody, but you're not sending any audio or video Give it a second to give going, and then we can always come back Okay, let's come back and do a search for, oh, there's where come back and do a search for, oh, yes, sorry, just rejoin Okay Sure is yours. Yeah, so we just want to give updates on our progress So this is a joint work from meaningful And so it's want to give updates on our progress. So this is a joint work from Minifox. And so HBCCC is a mechanism that leverage the precise information you can get from network devices to understand the condition status along the path, along the devices so that you can make a timely and process decision at the center side So next please You should have slide control if you hit the right area"
  },
  {
    "startTime": "01:12:02",
    "text": "Oh, really? That's great. Fancy. Yeah So this figure showed the working scenario while targeting. So we have focusing on those data center hyperspeeds networking for those applications like AML training and storage database applications. So there, the requirement for bandwidth and latency is very, very high especially for today's network in the backend the bandwidth is huge for the vacations and any latency laws will cause significant loss in the network capacity. So that's why we want the ultra low latency in the meanwhile we want to have a high throughput. So that's the scenarios we are targeting I just want to give a quick overview of our mechanism how it works So HPCCC is a precise condition control mechanism to leverage the features in today's network devices where the networking ASIC can provide the inband tennematory capacities. So those in-bank telemetry capacity can provide can insert metadata in the packet to tell, hey, when the packet goes through this switch, what's the current cool lens? of this device? And also, what's the link utilization of this device? when the packet goes through? So actually, the package, can carry out those information and hop by hop and to the destination and this destination can send the information back to the system So the center actually know when my packet go along the path for every single devices I get a precise information, so the center can make a"
  },
  {
    "startTime": "01:14:02",
    "text": "very timely and the precise decision and how to either increase the sending rate or decrease the sending rate accordingly So privacy, we present our evaluation results and take I want to give a summary on our deployment sites to support the SISIC So this is a condition signaling that has been deployed in one of the largest cloud provider and also we're working together with the, so it's been proposed from another working group at PPM So that draft is focusing on how to define the the telemetry format so the different vendor can be implement the same, format. And what are this? draft? We're focusing on the algorithm part to make sure that we are along the same condition control algorithm So to support the sysic, they have a fixed size, aggregated MAC data You can see that in that packet header fields. And currently this support the compact mode and expanded modes But right now, in our draft, we focus on the compact mode. And so in HBCCCCC, we have two parameter that calculate the link utilization is the U1 plus U2. So the U1 actually calculate the condition detected by the Q lens so actually is the cue lens divided by a link bandways. So that were determined what's the extent of condition this link experience. And you choose actually capture the link utilization. So"
  },
  {
    "startTime": "01:16:02",
    "text": "the idea of SISC is that they only leverage one of them, not both at the same time. So HVCCC will leverage both and they only leverage one of them, not both at the same time. So HVCC were leveraged both, but the SISC only leveraged one of them at a time because the update of the observed is that when when you have a queue like you want larger than zero so the u2 should be always 100% I mean, likely 100%, I mean, there are some transient behavior So, so that's why we, in that case, if we already observed, there's a Q-in buildup in the link, we don't necessarily need the YouTube because YouTube, most likely, will be 100% On the other hand, if you don't have a Q build-up, which means that U1 is equal to zero there's no Q buildup in the link, then we don't have to collect U1 information We only need to U2. So in this case, U2 will be less than 100 because if it's above our the link. Then we don't have to black U1 information. We only need to U2. So in this case, U2 will be less than 100. Because if you're above 100, you will certainly build up the Q So this is the key idea, can save the space when you load the data into the packet header fields fields The second part, we also handle a couple of age cases where when the traffic switch the pass, either due to routing changes or due to low-ban balancing, so we only need the two concerns backheads collected in the center side to identify there's a new pass so we can get rid of the old condition starters and start to build the condition status for the new pass and we can use the So here is a slightly different mechanism when we quantify the Q lens In HPC, when the packet is a"
  },
  {
    "startTime": "01:18:02",
    "text": "about to eject from the IGRA's Q. And we look the Q lens saying how many packets are behind this packet are already in the Q That's our Q lens definition, which we'll get a very immediate response when the condition just build up behind this packet. The difference here, for the C-SIC that the only leverage the synch package to see how much delay this particular packet experienced in the queue. So that's a- Time check you have three minutes. Thank you So that's a slightly different. I mean, that is includes QA time So that's a... Time check, you have three minutes. Thank you. So that's a slightly different. I mean, that includes the QA time? Yes, technically, yes Okay, okay, I can quickly ramp up here So that's a technically study slightly different because if you look at the, how many people just behind me when I, when I about to send from my egress queue, they will give you the early notification versus if you allow a packet go through all the way from the tail of the queue to the front of the queue and then calculate the sojourn time of this particular packet, it has some delay and then our algorithm would not recommend to do that but that's the estimation or approximation that SISIC is doing, that's a slightly different So in the algorithm perspective, we can see support SISIC with the format defined by PPM and also we we put the cc format in our information draft as well. So we have two draft, one draft focus, the main draft focused on the I algorithm, and also second information drafts focused on different implementation in terms of how you're gonna inject the tenement tree in the packet Yeah, that's all from presentation"
  },
  {
    "startTime": "01:20:02",
    "text": "and appreciate your time And I think that will be working. Awesome, let's have Hi, yeah a nice talk. I saw a couple places where it seemed to be talking about taking the QA length and dividing it by the link bandwidth one thing I wanted to mention is as far as I know, most cloud deployments, typically are going to have multiple cues in front of every link to support quality of service or differentiated service what do you want to call it. And so I think in cases like that, we don't really get what we want if we just kind of take the queue link divided by the link bandwidth, because there are potentially many other cues that might have cute packets And those would be contributing to the latency of the packets as well anyway thanks yeah yeah that's a great observation And actually, that's the case for the cloud environment as well. So we have two takeaway from here So, I mean, for those traffic, control, by HTTPCC, we have some other traffic controlled by TCP right so we assume that traffic controlled by HTTP is under a high priority because people care more about the latency there And then they typically will assign higher bandwidth share, which is a minimum bandwidth guarantee of the link. So in most cases, we got enough bandwidth. The secondly, we talk about the multi-Q support in our draft and we have a pseudo-agrozone also ready. And basically, the key idea is when we see the condition, we can always fall back to the minimum bandwidth guarantee That's the mechanism what people are using is a waited around the robin We can always fall back See this particular queue, although it's shared by other cues, other traffic, but the"
  },
  {
    "startTime": "01:22:02",
    "text": "device is guaranteed for this particular queue that's your minimal bandwidth. We can always save to fall back to that rate And going further, it's a, it's a we can do additive inquiry to try, if we can grab more bandwidth, if there's no other trippy in other cues, we can do that Thank you Wonderful. Thank you very. All right, we're going to do our quick little interactive portion here So first off who's read the draft? And one of the things that we're thinking about is kind of, you know, as we talk about getting deeper into some of the discussion for these, how much do we have bandwidth to take on? and so one of the questions that we're about to ask is is this kind of data center and really high bandwidth congestion control something? that the working group should work on and that can be, do we want to work on it right now, that could also be that's something that we think the working group should work on, but later, so yes is a valid answer for both of those to the next goal We will have a separate conversation at some point about the now and the ordering That'll be when we talk about charter. So right now it's just the, we would like to see CCWG talk about this at some point. It does not have to be tomorrow All right, we're going to close this down We're going to refresh and then do the next one So this is we're interested in working on this kind of topic at some point in the life of CCWG And if anybody who has responded with no as opposed to no opinion, would like to come to the queue and talk a little bit about why that is always welcome but not required"
  },
  {
    "startTime": "01:24:17",
    "text": "Stewart is in the key All right, closing this pull down and opening the next one while Stuart talks. Go for it, Stuart. I said no opinion but I think I should say why My first reaction to this was we work on the internet here, and all of these things where like the buckets have to be configured, there's clearly no way that on the global internet, you can't have them all configure differently And then at the same time, I'm thinking, we develop this technology here, IP and Q Quirk and TCP and things, that have use beyond just buying stuff and doing online shopping. And I do that myself We use IPV6 for home automation with thread. So on balance, I'm thinking IP is a good technology to use in the data center And we should be making that possible. So as long as this is clear that this is for a very specific use of IP technology, I think that is very much in the charter of the IETF to make our technology applicable and useful in difference scenarios beyond just web browsing and email that we all do day to day. Thank you, Stuart Stuart Yep. All right, and our last question about document being a good starting point and Gori wanted to make a comment as well Same question goes for this, and then we're going to move rapidly through the other things because we've got a few more agenda items to talk about in only so many minutes. Go for it, Corey so gory fast yeah i yeah i agree with what Stewart said, but I also want to know who is the customer. So, I mean, when we do an internet protocol"
  },
  {
    "startTime": "01:26:02",
    "text": "we're expecting multiple parties to come and want to use that technology. Some people are providing a service. Some people are developing implementations and we need a standard to it between them. So I think it's important when we look at data center cases that we have multiple customers for this That means multiple different vendors using it, but also different people developing things then maybe it's worth us investing in the standard otherwise this is going to take our time up and who needs the work. Thank you Gory. I saw a couple of thumbs up in the room as well. It is worth noting that our charter explicitly mentions data centers in real-time protocol but does not require us to do anything in that space. So I think perhaps some of the feedback that I'm hearing here for you Rui, is also, you know, let's make it clear to folks what is the expected use case for this, who are the kind of intended audience, and if we can get an number of different proponents who all say, like, yes, we'd like to use this then that is a really good signal for everyone else as well in terms of where we allocate our time. Hello, can you hear me? Yep. Yeah, so that's a great question And for the customer, you can see that this is one of the biggest, is Google, right? is already deployed and supported and also some other company also supported like it Alibaba, my previous employer is already deployed and people on board because people can more about, really, really care about the latency for those emerging applications like storage or AI training. And for the, for the really, really care about the latency for those emerging applications, like storage or AI training. And for the reason why we want to have this draft is because many a different vendors, you can check the author list that different people from different companies they want to have alignment how to support this customer, what's the data format, what's the algorithm you want to use? So what? companies. They want to have alignment, how to support this customer. What's the data format? What's the algorithm you want to use? So we have been put into discussion, many, many discussions on this. This is why we want to have a draft ready so people can on the"
  },
  {
    "startTime": "01:28:03",
    "text": "same page to align the algorithm and data format so they can work with the customer, work for the customer more easily. So that's a in initial motivation to do this Gotcha. Thank you very much for the clerk clarification. In the interest of time, we're going to keep going through our next talk, which is search a new slow start algorithm Mark's going to, we should have some slides going. Beautiful. All right. We've got a slightly abbreviated timer here to make sure we can make it through So let's keep an eye on time and take us away. I thank you everybody. Yeah, Mark Claypool, Professor at WPI, it's near Boston It's a small engineering school, Jay and Funger of engineers at Biasat. Merriam's a PhD student that's worked on this So we jump right in. So again, if you're looking at slow start ramp it up the sea wind, you'd like to hit the capacity point before you exit slow start, so near where you want to be you do it too early especially for links with high RTTs, you're going to have underutilization. So we say that's kind of a too early exit point. Conversely, if you sort of overshoot, you end up going past it causing a lot of loss, exit too late. Ideally, we'd kind of hit you have underutilization, so we say that's kind of a too early exit point. Conversely, if you sort of overshoot, you end up going past it, causing a lot of loss, exit too late. Ideally, we've kind of hit this sweet spot near capacity, exiting slow start we call this a choke point makes for a nice acronym so a choke point as we start looking at, so how does current TCP with high start enabled or the default in Linux, how does that work? And how does that work over wireless networks? and spoiler alert it doesn't work too well Let me show you a few things. So if you take a look at Linux, how does that work? And how does that work over wireless networks? And spoiler alert, it doesn't work too well. And let me show you a few things. So if you take a look, we started looking at, of course, Viasat's geosatellite link You're looking at the congestion window ramping up exponentially, doubling each time, roughly each time every RTT. You want to hit that capacity in a narrowband geosatellite link. You're going to hit that you know, right sweet spot. You're going to hit it right there and you'd like to exit there if you can. Turns out with, um, uh, uh, narrow band geosatellite link you're going to hit that you know right sweet spot you're going to hit it right there and you'd like to exit there if you can turns out with the geolink it's about 12 seconds in it can again higher RTT is about 12 seconds in. You're going to hit that"
  },
  {
    "startTime": "01:30:03",
    "text": "You turn on high start, you run out TCP, hit it way early about 1.3 seconds. So it exits slow start really early. Causes underutilization, takes a while to get up to this speed. If you turn off high start, of course you're going to hit the other side. You go way past it. And again, some of that because there's these large queues that build up. So there's a huge air that you keep on going without knowing what's happening and have a lot of packet loss. So it doesn't work well And this is not unique, though, to Geolinks. This happens for Leo Links. So starlink did some measurements in starlink same idea numbers are different you know you hit the congestion point about 400 mil milliseconds in. High start always exits early About 100 milliseconds in. High Start just causes it to say, hey, I'm done exit slow start it's on the left on the right view if you don't do it you overshoot uh 0.6 about 600 milliseconds in happens every time For GLTE, we did some Verizon be the provider. We did it for this And again, some of the same, you always are exiting before that congestion point with high start on. It's almost about, you know 100 milliseconds or so averaging about 3.3 is so of where you're going to hit the sweet spot for our test bed here, where we're running this uh big cues so you go over it in about two seconds worth it at the end so all these are working pretty poorly we works for more typical networks so land transfers across Wi-Fi. This is done in our two seconds worth it at the end so all these are working pretty poorly we works for more typical networks so land transfers across Wi-Fi has done us on our campus network at WPI sort of the same it's not oh always under but it averages high start on again before the conjoining point, just exit early, caused by those timing issues in these wireless networks So what we're looking for is something better. So that's search let me let me tell you a bit about search how it works, and then a little bit on performance So the acronym, slow start exit at right choke point, so search, and then again, starts with the premise that if during slow start you're sending roughly doubling your bite every RTT and say you expect that to kind of continue. And you can sort of see in this picture on the left it surpass the capacity So when that happens, you actually don't deliver the bites. So the delivery rate what actually comes back to the server you get this sort of flat"
  },
  {
    "startTime": "01:32:02",
    "text": "flattening right can't deliver more than the link capacity So you can compare those two. And you could keep trying of the set bytes, but you can also just do this with the delivered bytes, And you can actually take the delivered bytes and say, hey, I'm just going to keep track of those and then you can go ahead and compare them and so that's what the algorithm is going to do, is trying to try and look at the delivered bites and the scent bites So, um, taking a look, a bit of the code, this is a few things to talk through here. So if you try and predict those sets, bytes, so in roughly let's say it's T4, say, hey, I know what I sent last time at T3. It was like 2A, so it should be 4A so I can actually compute that, and I can compare those two things. I can say how does my current delivered bytes compare to the ones that I sent? So I can compare a T4, I can say, hey, ours are the same things. I can say how does my current delivered bytes compare to the ones that I sent? So I can compare a T4, I can say, hey, there's ours that's the same. The same, cool, looks pretty good, life's good, keep going But then if, I can see it, time T5, if I do the same computation, oh, I'm sorry, I have to normalize it too, because I need to normalize it because of the different link capacities we have to handle so if i take a look at that normalized difference and it's greater than some threshold, select at T5, there's a difference there Then you say, hey, I passed my capacity limit And so you'd want to exit slow start at that point So that's a quick intro, high level idea motivation There's some challenges, of course, to making it work beyond my PowerPoint slides, right? So how do you build this? And the first one we had to face is sort of the ones that were facing us in the geosatellite links, which is variable RTTs. Here's a picture of the roundtrip times in the geosatellite link over time, so this is just a download over links of 12 seconds of time, and the RTTs on the XX x-axis, those are hundreds of milliseconds, again, base of 600 but they go way up. All of this is before concession there's no congestion in the downward link here this is not a congested link. These are just variable RTTs and they're they're not caused by congestion in the forward link they're caused by ax scheduling on the backward link So lots of RTT variability, and that just messes up"
  },
  {
    "startTime": "01:34:00",
    "text": "high start, but it also makes timing and using round trips times hard. So holds for high start plus plus, for example And so the uh uh solution is to smooth over this variation with a window so use a window of data so don't just look at a single RTT but look at multiple RTTs to try and smooth over that variation So that's sort of the first challenge. And then if you're trying to keep track of delivery rates, you have of course, limited memory. We know that you can't track every ACC and store it as a history because you keep this per TCP connection. So we have to limit memory use. So the way we do that is we allocate sort of a we aggregate over multiple chime blocks you take that window we break it in blocks. Think about an array call those bins so we have a series of bins and so now you're actually aggregating keeping track of these you from made it more granular and then we actually also only process, you only do search processing at the bin boundaries so you don't have to process every act but only at these bin boundaries so save processing also so those are two challenges that make it practical to deploy that we've overcome with the wind and then the bins This is the pseudocode. It's in the RFC. I don't expect you read that. I'm not going to actually talk through it. I think it's fairly readable, but take a look yourself At a high level, I'll just do a few bubbles you know, initialize stuff at a TCP connection. And then every time you receive an act, you do some updating of the bin You update what's been happening so the data's been arriving at the bin boundary, so when you actually get to the end of it, you see okay let's see if we should actually slow start so you can compute the delivery rate, the predicted previous delivery rate you compare those to a threshold normalize and compare to a threshold. If they're above the threshold, then you exit slow start right so that's pseudocode take a look at the You want to talk about a break or something happy to. And let me tell you about a few key things in any kind of code there's some parameters that matter and we spent a fair amount of time working on these. We have some theoretical, some measurement stuff justifying the parameters. And there's a few that make a"
  },
  {
    "startTime": "01:36:03",
    "text": "difference. The window size matters, so how much do you need how long do you need to make that to smooth over that variation so we did some for your analysis on the signals and you to pick the p peak and then you use that to drive how large the window could be Some details are in the paper we presented at Wow Mom. Just last month. And, you know, again, large enough to make it to respond to smooth out, but not too large so you can be responsive. Window factor 3.5 size that's three and a half times the initial RTT. That's our window size that we based on all the measurements we've done of all those networks I mentioned. The second parameter that matters is the how many bins, so that bin size, so how many bins you need to use. So again, you want to, enough bins that you have aggregated and reduced load but small enough that you have greater clarity to represent the network so again another thing we've done some sensitivity analysis how much to the number of bins matter over lots of different conditions And our current recommendation is 10. And you need extra bins because you have to roll back that window, 15 extra bins so 25 total, and again, there's possibly could be adjusted, but those are make decent recommended starting points and then the third one is what's that threshold factor. And so we did some theoretical analysis of C wind growth and did that based on noise and currently you know it has to be something positive but 0.3 works well for most of the network conditions we've been testing. So those are parameters selections and again look at details in the paper the RFC doesn't have too many of them, it waves its hands. We can look at the paper for details Okay. Thank you So I'm going to do one performance evaluation piece, and then I'll wrap up since I know we're sure on time I have a couple in this slide deck but just the GEO link and the Wi-Fi link. And let me just show you a little bit how it's working So if you take a look at, again, this is the C-wind growth with high start on. It looks like it's growing well but that's actually the cubic growth right there. It's actually exit slow start. And then it doesn't cause you a loss, it's under-utilization You turn off high start and that window shoots way out Great, better utilization and yet causes lots of package"
  },
  {
    "startTime": "01:38:00",
    "text": "loss. This graph is packet loss here. Search works pretty well has the same ramp up, exits Then there's ingestion avoidance kind of probes causes very little loss also So we get sort of the benefits of the throughput. Under the hood this is sort of what search is doing. That blue line is the predicted sending rate. The green line is actual delivered rate. And so they diverge. And so kind of when they diverge, that's really like, hey, something's going on. And if you normalize that day, difference and compute the threshold that's this graph I'm sorry, normalize a difference to compute that, you know, that normalize difference graph, that's the blue line. When it crosses the threshold, it's when it exits, it's indicated there So yeah, that's, I know that's quick. I won't show you the Wi-Fi results. I've just pruned this. Works well over Wi-Fi 2, sort of a different set of analysis. Let's go on We've built Linux kernel modules for it and test it. Sorry, I didn't need to go through that slide. We've built kernel modules and tested it. It's worked well. We have a couple different ones out there. We've actually also built it in OpenStars quickly library, a quick library that's the quickly HTO library and have some results there works works well also for that we've done a bunch of different tests on the network i mentioned we're turned to upstream it get in the kernel but happy to have people grab these as is and use them if they want to do library and have some results there works works well also for that. We've done a bunch of different tests on the networks I mentioned We're turned to upstream it get in the kernel, but happy to have people grab these as is and use them if they want to try them out Um, and that is mostly my wrap-up, you know, again high start is clearly broken over these networks. It just does exit way early it doesn't do what it's supposed to um you know search has some reasonable properties. I think that makes it work pretty well. Preach utilization reaches packet loss versus turning off high start. And we have a, if it's helpful, if you search dash SS www.edu will take you to the project page. That's where we have. Again, the code modules plus there's all the papers that we published on it and document documentation And that is all. Thank you. Wonderful. Thank you so much Gory is first in the queue. Then we have Ian and Neil. Let's keep it snobing so we can make our way forward. It's very interesting work. Yeah, thanks for this talk. And of course, when we were doing the"
  },
  {
    "startTime": "01:40:00",
    "text": "careful resume thing, we also looked at high start because it happens to play well after careful resume But yeah, all these problems like this. Was this 904? you were using as the basis? Was it high start plus plus? or was it high start? Because I think that's a major difference Yeah, this was high start. So it's in the Linux default Linux kernel's high start. We have done some preliminary work with high start Plus Plus and High Start Plus. Also exits early, or geo-settlement, very early It has you know high start as you probably know has an extra stage so it takes longer before it really decides to enter congestion avoidance and it has a slower growth rate so it has some slightly different properties but still under utilization for some of the links we looked at Would look to follow up on the High Start Plus Plus. Yeah, thank you Ian's but yeah I was actually just going to ask if you were comparing to high start or high style plus plus in the original slides. That was all, but this looks in interesting thank you very much And Neil Yeah, Neil got over here. So one thing I think if would be worth incorporating into the document is consideration for exiting spurious based on receive window limits or application limited behavior That's something that we found is critical to incorporate in BBR's code you know, heuristic for exiting. So they'll start because those are very common issues to run into and you don't want to exit spurious because of those. Another thing is that my reading of the draft, it has 25, bins of data, and presumably that's about four bytes maybe a, you know, U-32 for each, so that's about 100 bytes worth of state, which would kind of double the congestion control state on a Linux TCP connection and so there might be some concerns about it incorporating that into mainline Linux. So we might want to it would be great if there's a way to iterate on this algorithm and reduce the state concern the state use, and maybe, you know, this on as a working group item when there's a version that is incorporated in a major transport stack, maybe"
  },
  {
    "startTime": "01:42:02",
    "text": "Yeah, cool. Thanks. Those are great comments. I mean, just real quick The application rate limited, I think we handle that that, that's a case we do handle, I think we're okay, we just don't, search doesn't kick in, that's the first one, the memory one's a good one, and we're working on that I think we can go down in bytes for each storage plus it perhaps is fewer than 25 and yeah that's a important consideration We're considering that also. Beautiful Thank you so much. So similar questions as all of our other interactive segments So I have read the draft always a good question, or the paper That's totally fair. Or seen the other presentation But yeah, I think mostly we're looking for signals of engagement and willingness to review and contribute. This is also really nice because it kind of contributes to our toolbox of things that we use as we develop congestion controllers So as we talk about in our chartering discussion, both the do we want to take on particular congestion controls as well? as, you know, can we spend working group time on helping to the I think it's really great work. So thank you for bringing this here All right, for our next attempt at a question Is this something that we'd like to spend working group time on? Is this something that we want to contribute to, come up with ideas, help? test, brainstorm ways to poke it, prod it, and other? make it great? I will give that another second as you convince your client to let you click buttons All right, thank you, and then"
  },
  {
    "startTime": "01:44:02",
    "text": "is this something that seems like a good starting point for? taking on work around slow? start and how do we actually achieve? a good estimate of an initial rate to There we go, beautiful I think it's also one that we will talk about more as we kind of move towards a doctor of various things. So this isn't necessarily will we adopt this tomorrow but more of a, is this, it sounds like this is a thing that folks are interested in coming up with solutions for and it seems like a valuable problem to solve, which is really encouraging Beautiful. All right. Let's hop into our next presentation but thank you so much, Mark I'm sorry, can I agenda bash? Absolutely presentation. But thank you so much, Mark. I'm sorry, can I agenda bash? We have 15 minutes. I think it's really important we have rechartering discussion rather than another paper or draft. We can do that. One of the reasons that we deliberately put that at the end rather than the beginning is because we believe that our current charter covers the all of the work that we've talked about here and our potential to adopt it. And so the rechartering discussion is mostly a editorial or people happy with clarifications that we finished 53 bis. Okay. Plot twist But that said, I think let's try to keep this to a super speedy five minutes so that we make sure we can 5033 BIS. Okay. Plot twist. But that said, I think let's try to keep this to a super speedy five minutes so that we make sure we keep a solid 10 to go through those editorial changes Make sure everybody's happy with it. Thank you, Martin All right, let's bring up Kuhna So, thank you We cannot hear you or see you yet"
  },
  {
    "startTime": "01:46:05",
    "text": "Okay, yeah going to go for an express mode five minutes here so let's go Zippy Zippy It's a short presentation, so just want to give a quick update on the Prague congestion control and maybe in general a bit about L-Forest Should have control somewhere how can i move to the next page Yeah, okay. So first, quick Alvarez standard status so you all know Doxies has adopted it since a while 3dbP also has adopted it in release 18 It has Alvarez in the run, CUDU in interactions, low latency deer bees support LFRS, and in the next ongoing release 9 also targets for you to user playing notice supporting AlphiS LFARS. And in the next ongoing release 19, also targets for you to user playing notice support in LFARS. Then also interesting is that there is in a WBA a lot of activity around implementation guidelines to do twice square and why is in WBA a lot of activity around implementation guidelines to do it by the Wi-Fi Mac. So there were a few comments before leaving with the, or coping with what the Wi-Fi is doing. So one of the good things is that Alvarez can work with the Wi-Fi knows if there is an outage, it doesn't mark packets So only the window blocks, then your throughput And when the axe come back, you continue with the right rate without the reduction so all of these things can be done if, let's say, you have an explicit signal in the device itself So that's, for instance, one of the work to have optimal connectivity between an a QM and the Wi-Fi Mac. Then on the other hand, of course, you say we have to cope with all those Wi-Fi"
  },
  {
    "startTime": "01:48:02",
    "text": "outages. Well, actually, it's not necessarily because a lot of these scanning can be done with side radios so more improved Wi-Fi chips will avoid these outside and also one important thing is the EDCA and the MAC layer, they distributed Mac More and more people start to realize that WMM is kind of a triple play legacy and it's not future proof anymore and want to move on and correct these things. So that's also interesting Like Alvarez, removes the queuing latency, but not necessarily Mac latency And so in that respect, it's good that we can work on improving the mac latency for future and maybe existing Wi-Fi devices Also in BBF, Alvarez is an Alvres project is launched You can see there will be working on PON and access networks, fixed access networks So that will also start Next slide. Time check, you have two minutes you can advance yourself yeah so maybe also to give a bit of an overview without the Prague objectives. So from the Prague congestion control itself, it's to have an end to low latency serve and to make that deployment of these services easier and scalable. So you have to provide low latency end-to-end and all your network devices need to support low latency, but Alvarez you should think makes it easy to deploy it and more scalable because you don't have to guarantee back bandwidth. You can adapt the application and typically interactive applications they are very adaptive and they want to be adapted before they see any latency or blockage or drop So also it's important to know that we first want low latency so all the these"
  },
  {
    "startTime": "01:50:02",
    "text": "objectives want to to control the real speed and not the throughput which is just the quantity. How much can I send for another? adaptive application the quantity can be adapted but the speed, how fast the interaction is is important. Okay then and also elvarez offers an interactive survey to apply applications next to the classic and buffer traffic. So it's good to have two in parallel. I'm not going to go in the detail but maybe in interest of time the next slide click myself um quick uh quick update on the the practice status itself so is deployed in Apple quick product it's apple quick falls back to cubic by the way it was released in the latest macOS and iOS Linux TCP Prague We have improved a few extra features there are going to be a presentation in ICS on Friday. If you want to know a bit more details on that. We also have a lot of kernel versions available now and we're working on the upstreaming and release 6.11. We hope to get it in there now Then also new is that we work on an udip prog implementation so there is a link to a repository it's still under construction it's to do able to do the Prague congestion control for UDP-based applications and of course targeting interactive applications which immediately can update the source generation of the data, and not necessarily have a stack blocking the throughput So it's a single source C++ file that you can kind of incorporate in your applications. We are working on some examples further. It's being currently"
  },
  {
    "startTime": "01:52:02",
    "text": "introduced in I-Perf 2. So it will be platform independent running on all on all platforms that's our support by IPRF2, and you can do then UDP-based Prague, which has the advantage that you have more precise application level measurements without socket buffers and flow controls and all these things in TCP stacks and even in quick stacks. But of course, quick stacks can be made also more application data source interacting um okay we will also add further real-time product and also the cubicon log interacting. Okay, we will also add further real-time product and also the cubic on loss. It's the idea to have that in UDP Prague, but also in TCP Prague in LIN England. So there are also udp prague or prak related applications like in VD and G-Force now they are deployed and, okay, the draft in ICS exists, which we need to update to the latest status of the newest um enhancements let's say in TCP and also in the future you Prague. Okay, that's it for me Excellent. Thank you. We've got our poll running. We'll go quickly through our other questions and pop into our charter discussion For this, I think with regards to Prague, it definitely be good to update to have it match kind of the current reality and ship a note to the list. I think we've seen some pretty good discussion in the past, and there's definitely some real-world deployment from which we could gather data So let's get things updated and then we'll rediscuss from that All right So to clarify, they're seeking a adoption in CCWG and not ICRG? So we're having a conversation amongst the chairs of both groups to figure out where we think we should take this work yes So this is not a, we think that CCWG is necessarily going to adopt it versus"
  },
  {
    "startTime": "01:54:02",
    "text": "ICCRG, but the question is more, is this something that the folks in this room are interested in working on? Yes, it was a request from the chairs here to give a presentation on the project. It was not necessary an adoption call, so for us it's not for at least authors here, it's not, we are fulfilling our promise in our charter to coordinate with chairs it helps that one of our chairs sees the same chair in the mirror Yeah I know Matt we had an as time permits, which will not permit, but we can bring it back next time around Friday in ICCRG All right, folks, now we've already had all these wonderful proposals. Let's bring it back to the high level if I can drive my own slides refresh that. But as Eric already reminded everybody, CCWG is already chartered to adopt proposals in the space so we can already adopt proposals in our current order and still we wanted to reflect the fact that we're done with 50 to the business and add some introductory text about what congestion control is why it matters and then also refine or focus on like now we're doing it now we're specifying congestion control proposals and potentially reflect some of the themes that we on like now we're doing it, now we're specifying congestion control proposals and potentially reflect some of the themes that we've talked about a lot already since we started the working group a year ago. So this is just a proposal for some introductory text to start our charter and we're not trying to ward smith right now we welcome discussion on GitHub and the mailing list list Can we get a thumbs up, thumbs on maybe for adding this high-level motivation because conjection"
  },
  {
    "startTime": "01:56:02",
    "text": "on toll, sometimes people will misunderstand who are not in this room, what it is? I'm sure we will happily word smith exactly how we want to define congestion control on GitHub, so this is just without any words, just see reasonable to people to have an intro at the top. That's just like congestion controls the thing. Sweet, cool All right, thank you. You got some thumbs up And then here is two focus topics that reflect themes we've discussed a lot in CCWG so far. So we can always add more of those And then we already heard all these great proposals but we might have limited bandwidth to take on everything at the same time so people have thoughts on so what to do first, prioritization, how much can we take? on, and review, and also make sure we help out folks of evaluation right how can we make a evaluation a collaborative effort? instead of only the offers? all right Matt did you want I actually had a comment on the first paragraph, which I don't have a word smith for the unsight tool as being particularly lame in this meeting The I don't have a word smith for, the unsight tool as being particularly lame, this meeting. There's also a missing thought here, and it is the conjunction control properties of applications And what I mean is there's a lot of application designers who say, oh, transport worries about congestion collapse, I don't have to. And if you write an application, which every time it retries, it's starts a new connection, it defeats the safety measures in the transport. And so, in fact, there are application design issues which are congestion control properties that have to be subject to congestion control properties. And this is all looking inwards into the protocols, but you actually have to look up the stack as well right thank you Matt. Any other thoughts?"
  },
  {
    "startTime": "01:58:02",
    "text": "Before we dive in the comment, I think the thing that we're most, if we move, the slides back forward, the thing that we're most interested in, so there's a pull request that's current open on GitHub, has a bunch of text, please come help us wordsmith it. It's easy to find. It is the only poll request on the working group materials repo. So we'll send out a link, but please come help us word smith it. It's mostly just update to reflect that we've finished with 5033 bits and adding that paragraph So, you know, propose all the texts that you possibly can please. I think the thing that we're most interested in right now is how much do we think we can take on at once and what should those things be? All right, with that. Yes, thank you greg mirsky Erickson. I think that, well, improving the latency is bounded by was a physics But what congestion control can achieve is minimizing the latency variation, delay variation. And I think that that's something that can be mentioned explicitly. Thank you Thank you gory thank you for typing and running we appreciate you gory trying to think about how we do potential work. I mean, one of the things in 5033 is somebody needs to get data out there about how well this works This isn't a great venue for discussing that, but some that data has to be there and we have to see it And if we take on that broader scope, that's a lot of heads that have to look at all this different things. So I wonder whether we have to actually just adopt them slowly and actually evaluate them, then I don't know what happens I'm going to be very short now, I know what happens when we adopt something in there we need to evaluate it and then we need to progress it So this kind of like, I don't understand how the mechanisms might work do you have an idea how it might work? We have a pile of ideas that would be interesting in feedback on. A big one of them is how can the working"
  },
  {
    "startTime": "02:00:02",
    "text": "group help contribute some of that data? Because I think one of the things that's made this kind of work really hard in the IETF in the past is as somebody who is a proponent of an idea, you end up with infinite work to satisfy a questionably action set of questions that are asked. So as Martin reminded us earlier, like we have a much nicer scoping of the set of questions that we're asking, but I suspect we have some appetite for drilling a little farther. So one of the big questions is, do folks have interests? in helping both in the hackathon and some of those kinds of things, helping build this evaluation in environment? Can we make it so that when we adopt one thing we can I think our most concrete proposal right now is let's pick one thing and adopt it. And as we run through that process, build the tools that we can then apply to the next one So our, the chair proposal is a width of one for the first one rather than a width of six Yeah, gory can. Yeah, I love that. We might find this small stuff that comes up which is actually more obvious and doesn't require that sort of process but that's it sounds good. Do you adopt it and then? do that or do you just kind of note it and get that work? done before you adopt it? And don't overcommit at the mic in response to that I think. Yeah, definitely. So the idea is that we have to have a scientific study of the pros and cons when we ship the proposal to IESG. So they're working group's job is to make sure that that, you know, evidence exists. It can come from various places proponents, authors, academia ICCRG presentations, academic papers, all the sort of thing. And we also, we had a hackathon table this time to get started on testing congestion control more broadly"
  },
  {
    "startTime": "02:02:02",
    "text": "And we will come back in Dublin with that. So happy to have more participation on that topic Yeah, I think that's where we come back to adoption is where we essentially take over change control of that proposal and we take on responsibility for helping gather that data, right? So there's the initial bar of, hey, you know, have the authors has the initial thing, have enough, um, show that it's a real serious proposal and a plan with these cases and people are interested in working on it But as we go forwards, I think a big part of the reason that we're in this room is because we all want these things to succeed and we'd like to be able to make progress in this area rather than just to see things continue to stall. And I think when we want to see that progress, part of it is now our responsibility to help make that progress happen. So all right, I see Mirias in the queue, Matt, is also up. Pick your order but So, all right, I see Mirius in the queue, Matt is also up. Pick your, pick your order, but yeah, I'm a little bit concerned. So I think the biggest benefit of having an RFC for a congestion control is that you have like a stable reference. I mean, that's why we published QBIC So I think the biggest benefit of having an RC for congestion control is that you have like a stable reference. I mean, that's why we published cubic because otherwise you have like doesn't different papers and everybody implements things slightly differently or whatever The reason why we never published something else was because we are afraid to say this is what we recommend, and then we break the internet, right? So I think we should very clear that this is not what we do, what we want as a stable reference. That's a benefit And by putting the testing burden to high I think it can be easy to drive people away who just want to have this stable reference so I think that is much big value. We just have to like put the right disclaimer on the RC saying, you know, this is not like the best recommended congestion control. This is what you should test on the internet and this is how it works And I think we've tried to write down the litmus test for what are the things that we would recommend and what are the what are the bars for thing we would recommend? versus, by the way, this exists, please be consistent consistent At the end, I think the evaluation burden shouldn't be there high, and therefore I think we can actually adopt multiple things as long as"
  },
  {
    "startTime": "02:04:02",
    "text": "those people who work on it, come back and actually do the way I think we can actually adopt multiple things as long as those people who work on it come back and actually do the work, right? So why should we limit it to one if there is enough insurance? in doing something? Got it, thank you All right, Matt, and then I was going to suggest that we have two different statuses, one of which is adoption, which means we want to have change control, and another one, which is monitoring or supervision or something, which means we want periodic reports from the authors but aren't willing to take it on as something that we're responsible for. Interesting for what it's worth that something along those lines has so far been our limous test for the delta between ICCRG and CCWG of ICCRG as kind of that incubation place where we ask questions and ask for the reports on how the results went and CCWG is the place where we say okay we'd like to take this on it ask questions and ask for further reports on how the results went and CCWG is the place where we say okay we'd like to take this on and make it a make it a real thing so 5033bis has guidelines for standards, experimental, and some information and there's like no in terms of just having a stable reference, you could do informational, and there's no test bar for that. Yep, so we have built the litmus test. Now is the now is our opportunity to run a real world example against that litmus test and learn from our mistakes Beautiful With that, thank you all so much for being here. Have a great rest of the week, and don't forget to come to ICCRG on Friday so we can incubate some stuff Thank you Yeah, on that"
  }
]
