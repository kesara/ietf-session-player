[
  {
    "startTime": "00:00:10",
    "text": "Yes. Hello, everyone. This is Meaning for the quick working group. If this is not where you expect to be, you can stay if you like. We're nice here. But Otherwise, we're going to get started. As a reminder, There is it."
  },
  {
    "startTime": "00:02:01",
    "text": "there is a clipboard that is probably circulating but that clipboard really just has a QR code for get bringing you to the on-site tool, which is how we track attendance. it would be nice if you did that. so we can add our numbers and make sure that everyone knows that quick is important. and Yeah. Yeah. So moving right along, this is a reminder that this is an IETF meeting and thus is covered by the note well. If you have not noted the note well, please do so Also, the all of con all your conduct is covered by the IETF code of conduct. we take very seriously here. and some more tips in addition to the QR code thing I already mentioned. you are joining the queue, will need to use the the on-site tool. It can be a little bit funny, but Just keep with it, reload it if it's not working, but that is how you join the queue. The shares will do our best to dequeue people from the queue as needed. From remote participants, Please do the normal thing of muting yours audio and video when you're not speaking. generally, people do have have a pretty good job of that. Also, we can just mute people that are not doing that anyway. And, of course, using a headset is is strongly recommended. And We will also be reading any questions from the chat. that is either in the know, the the Meet echo chat tool or on Zulip And we already have a notetaker. So thank you again for that. and This is our agenda for today. So we have brief chair updates then we have a good number of working group items. And then we have some if we get time, we will have a couple other topics"
  },
  {
    "startTime": "00:04:03",
    "text": "that are not currently working group items, but may have be of interest to the working group So update since last meeting. We have 2 new RFCs so And that is compatible version negotiation. Yeah. Some people may have forgotten that we have compatible version investigation, and we have whole new version of quick, So Thank you, everyone, for your contributions to that work. And that that is it for our our slides. So Now Who who's presenting from multipath, actually. Okay. Should we send Okay. Hello, everyone. I'm Yumming from Alibaba. today, Clint and I will introduce the topic of multi pass extension for Quick. slides, please. Okay. Today, we'll introduce the difference between the row 4 to the row 5 and of course, we have a hackathon of internal task we will do the in graph reports, and we will discuss us the open issues and of course, the next step of this draft Okay. Also also we have merged loss of poor request on the GitHub, but the most important change in this version is that we update the new transport parameter and frame types. first important thing is that we update the new transport parameter value, we"
  },
  {
    "startTime": "00:06:01",
    "text": "as we have don't need to support their lens connection ID anymore. we don't need the value of this transformed memory parameter. So we just introduced the enable multi pass as a Netherlands transport parameter and we update the version 205. And by the way, the enable multicast transmit her must not be remembered and stored with any session tickets. y, Yossi. Yoss. with their entity because all the multipass features should be used with one entity package. that's just since works. And next size, please. Okay. The set second important thing is that we update the frame types and error code. we replace the frame time with random numbers, and you you can see the new types here. And we have applied the an iron reduce situation for these new franchise. we will get shorter runs before the last call for adoption. and And and by the way, we update the error codes for NPE protocol violation. and we use the framing coding error while it receives multipass specific frames. in packets except the 1 RTG package. For example, if you receive an ACKMP in the handshake packets, you should update and frame encoding error. is the most fit specific error. next lines, please. And we update these, say, the usage in the model as as we have remove the DEROLAND CRD support in in the current draft. We remove all these sales statement about this. And we suggest that implementations should"
  },
  {
    "startTime": "00:08:01",
    "text": "use should have at least one available CRDs unless they don't care about refining, Net size, please. I already added some guidance about the RTT compute patient and ICMP scheduling. And then the point is that implementations can choose different CKMP scheduling strategies. such as the deconsending SDKmpframes on the past the package was received, I think has something they have some ASICMP frames on the shortest path. But for most considerations, they need to encapute most RTT and RTT variable per pass. using the algorithms, specific has specified in quick recovery. And some congested control functions may rely on the estimate of the mean RTT And so we given 1 solution here, the endpoints might remember the path over which the SDK MP that produced the Meaning. IT team was received. And what you find that task is abandoned or changed, can restart that mean ITDcomputation. That's fine. So please And we add some educate I do agitonal guidance in this in this draft, and people can check the details through the diff. We claim that there are RTT and package should be acknowledged ASKMP friends. although you can just send ackndpference in yattacking packets. and we clarify that past abandon and past failures is a elasticity"
  },
  {
    "startTime": "00:10:02",
    "text": "Also, the easy incomes are per pass. longper connection that's different from quick version 1. And they we clarify the effect on the transport machinery, When people receive passive abandon, and after 3 PTO timeouts. And, of course, about the announced usage, analysis, and please feel free to check the details next slide, please. Okay. The the last important thing is that we an update, the decay update mechanism In the Last version, an endpoint need to send packets on each path when the update happens This is sometimes a waste. and it's not very efficient. So in this version, as MRT has pointed out on the GitHub that endpoints remain we may wait for at least 3 times the largest video among all the passes before initiating a new key update after receiving the acknowledgement that confirms the promo's key update. We also think that's an allegant solution, so we update this and merge this English draft. And it's different from what what Quick, version 1 has claimed. that may just use straight times the PTO. They only one active pass. Just feel free to check the diff Next slide, please. Okay. And this is the interoperability part, before Quentin introduce the report, I really would like to say that we already have 5 implementations, which supports multi pass."
  },
  {
    "startTime": "00:12:01",
    "text": "And I think we have 6 on the client side as our Apple friends has claimed that they support multi pass on the client side. So And we have this nice highconcern interoper reports. So I think this is really a big step of this draft, and thank you for everyone Okay. So it's quarantine. we can keep the slide, actually. Yeah. So, actually, it was during the accountant we did some interoperability tests between the different implementations and some were implementing version 4, other were implementing version 5, somewhere implementing both. And so, basically, what we did is just check if can negotiate multi paths, if you can create paths, and then if we can just split that across several paths. And so the bottom line, actually, of this implementation interrupt is that actually we manage to to multi pass between all the implementation in some way. Of course, later, we will need to do, like, performance interrupt as well, but this will be for later. So next slide. Yeah. Fine. Can you just stand on the Oh, Sorry. So Next slide, please. So now we'd like to discuss some open issues that we have. So there is this error. So let's start with the first. Next slide. So we still are some discussion at the last IETF about what to do with the past status frames. So currently, what you have is that you have one frame And then that frame define a value to define the status of the frame, and we have defined 2 value, the value 1 for done by, I think, and value 2 for available. And the question was do we want to keep"
  },
  {
    "startTime": "00:14:04",
    "text": "this frame with a value, or do we want to instead Split the frame. into several frames, such as a bus available and pass standby. Knowing that, we need to keep a sequence number between this different frame. So in the case of the split frame, we need to have one sequence number that is between the different friends because the status of a frame may change. Or if we find we prefer to keep the pass status frame as its current form, we need to define what to do if you have some value, which is unknown, not unexpected. So if you have any point on this. Yeah. Yeah. This is a Kazuho, guys. I think this is a bag shake regarding how you send the either in a frame type or it has a distinct property of a field. And I don't think we have had new information since the previous ICF meeting, And if minus If that understanding is what I'd prefer, pulling the previous consensus of using different frames. k. Yeah. I agree. And I think it doesn't matter that much. I think I prefer the first solution here. And as Kasu pointed out in the In GitHub, like, this is not the first time we're actually using this between frames, so I think it's fine. Okay. Christian, And the the main advantage of using multiple rands that we do not have to create a specific registry of status. in for the Ayanna. So the the it's slightly better. The multiple firm is it better because it's"
  },
  {
    "startTime": "00:16:02",
    "text": "means less administrative burden for the INR. The the only question is that you do have to maintain this sequence number across all these frames. but they get Right? That's the only constraint. Alessandro Guerini Cloudflare I agree. The the I think the freight the framing to 2 is probably better, and it's also going to be easier towards additional statuses that might require additional information so that we won't need to actually change the path such as frame itself. also just on a new friend. Okay. Thanks for the inputs. So Next slide, So, yeah, this is also small issues so currently about the error code that we have, we only have one error code which is very specific to multipath, which is multipath protocolviolation, And the current rationale behind only having one value is that you can then in the connection close frame, indicate of additional value to indicate precisely what you have as an error. So do we want to have further our code, or do we want do do we just keep the current question out of only having one? And then if in connection close, that can just f4400Codes. I'll assign to get any Cloudflare. i e, the think so so the the arrow codes might be useful for, like, interrupt I guess. But as you said, like, you can choose the the reason Fields. I don't think we need more, but I don't know if anyone else as opinions. I'm trying to remember myself. Is this"
  },
  {
    "startTime": "00:18:02",
    "text": "I think we're only using this area code in 1 in one place at the moment, which is, like, if you try to negotiate and you don't have a connect likely than you're putting this error code in. Right? And then, like, I think there's also echo conditions where something is wrong with the frames, but that's why we use more generic frame type error codes, I believe. So this is kind of this is only useful. The handshake, we don't have another Yeah. used. And the point of this issue was to, like, more holistically look at, like, where we will use switch error codes and if this is actually the right approach. I think, like, I think the only question here is really we could rename this error code because we only use it in one place. But other than that, I think we are at the right point point now, but, like, it it kind of because we just changed it in in the last version, so we don't really, like, have, like, and we changed it at a different point of time. So we had we didn't think about the whole concept. So we should make sure we do part Thank you. at Actually, we did think about the whole concept, and the there is a thread off there you can if you have multiple error codes, Then it will be present whether you're in debug mode or not. wise, if you rely on the recent code. You can have a programming switch your implementation that only set up the reason if you are not in debug mode if you are in debug mode. And it has implication on the overall security issue because effectively, by disclosing more information about the could you disclose information about the internal state of your application and that can be used to values at tax. And so it's it's generally preferred to say less and rely on the recent code and mcdouble put the recent code behind some kind of debug switch"
  },
  {
    "startTime": "00:20:01",
    "text": "So that you can be very precise about what's happening in debug but not in production. Chris, Christian, stay here for a second. So do you even think we don't need this error code we should use a more generic error code for the handshake problem. I mean, I think one is fine. for the the head. but that's we we certainly don't need to have 2000 Well, actually, in the next steps that we are going present, there is the security consideration that we need to write. some entities related just one thing about this. We had a discussion over the weekend about error codes and What it is that when when it is that you can send them and and that sort of thing. I think the it's probably worth airing here a little bit about the conclusions that we've reached privately. there was a concern that someone would send this particular error code in the in the case that one side attempted to do some multipath things during the handshake, but it was you hadn't sent the enable multi path transport parameter, And A conclusion was that No matter whether the other peer has sent you any indication that they support a particular feature or or what have you. you can still send error codes related to that feature. which has implications for the use of Vericos, more more more generally. But I think that that's probably something worth thinking. about here as well. just simplifies things, I think, in this case. than anything else? Alright. So excellent. So this one is about non validated pass and I can be. So"
  },
  {
    "startTime": "00:22:00",
    "text": "What we do currently in multi pass is that when you want to create a new pass, you need to do a path validation on it. So you send a path challenge. And then the server will need to replay response. And the goal is we want to acknowledge that packet. So can we just actually, but the I can p in the task response packet. So, currently, what we can do and that may that works actually is we can just send I can be framed on a different validated path, but And this is because, actually, the m p frame is a non popping frame. So if you want to make it work and but I can pin the plastic ponds, bucket. actually, we need to make the i m p frame approving frames. So do we want to make this or is it just fine to keep it non probing and send back in ponadotas. Castle Hawke. I prefer simply removing the concept of probing and non probing frames in multiple. fiscal part. The reason we have the distinction proving and non probing frames is that non probing Oh, is that non probing frames have to be tracked by loss recovery. And because with quick pre run, it is hard if not impossible to handle loss recovery or not, but multiple pass concurrently That's the reason we have used the transmission probing frames as a signal of migration. But if not as big, we have lost recovery on every path. So I think we can simply say that any frames regardless of being probing or non probing can be sent at any moment. and call it a day. k. in favor of Alibaba? Actually, thinking about this problem, we need to think about why we need it monetizing the first I think one of the most important reason is to offer like redundancy and the communication robustness. Right? So in in that sense, if you rely if you send ackmp on another validated has that's a coupling between 2 different passes or more different passes. I I think it's not good for redundancy."
  },
  {
    "startTime": "00:24:02",
    "text": "purpose. So I really think that we should make ACMP a probing probing line. and and allow it to send directly from the new pass. k. Christian, on this one. If we have documented support on MultiPASS, then we we can just ask what happened on the concept of probing packets. Because we have other ways to validate the path. I mean, the way to validate us for expressly clear. by by the by the client is to send a pass status saying that the pass is available. Now If the client doesn't do that, the server has to guess. But if the client wants a specific handling, it can send a pass status and set it to available or standby. and there is no ambiguity. Business of trying to infer from some type of pass that the client might wish to do something. is value of a phone I must say, for example, that in my implementation, I always then I can pee. with the change response. Because, I mean, you have to bytes, you have to check it, you can just send it. And I always process all the packets we see from any pass because If they have been sent by the other hand, and check some matches. They are good packets, so you can just always use them. So I think it's Kazoo is completely right there. Yeah. Yeah. Eric, can you hear Apple? One thing to think about if we're doing this is kind of what Christian was saying."
  },
  {
    "startTime": "00:26:03",
    "text": "at some level, validation is is proving that you can respond and arguably echo some some data. Right? So either say this is fine to make a probing frame and we figure out how that plays into some of the amplification limits. Right? because if you start sending much ax and chewing up that data, you may not be respond to a challenge. or we say, no. You have to keep those very strictly separate, and the only way to validate is is one thing. But, like, the point of validation isn't that you responded with a particular frame. The point of validation is that you exist and you have keys and you can respond and you can see the data that comes in. Yeah. Alright. Then maybe the next slide. Oh, shit. Yeah. So it's it's also tricky to this one. So we might be in a situation because we use the connection ID. Yes. Passidentifier. And if we are in the case where you experience not rebinding, for instance, while a change in protocol at the same time that the client change its connection ID. Actually, you will observe a new pass at the server side with a new connection ID. but at the client's side, actually, it refers to the same sportable. And so how should we actually advance this case because it will appear as a new for total without saving proper consolidation, but as we discussed previously, just to check. So to create a new pass. And so there are the found option to to actually cover this case. So, Heather, we let the server figure out to which for the polyposic response. or we can tweak either on the return connection ID, the password done, or define a new frame, which is very specific to that case. So Maybe"
  },
  {
    "startTime": "00:28:00",
    "text": "would be nice to see your opinions, wanted to point out that this is Just would be solved by introducing explicit path ID which is the -- You have a slide have a slide, and we'll discuss this later So maybe get -- Yeah. So, actually, we can actually move to the next issue, which is actually a good one. So This is actually, 1st of my ground strike, to explain what we want to do. So currently, this issue currently, we have that what connection ID correspond to 1 cane number space, so it has its own quick mass. So it means that if you rotate the connection ID, even if you are using the same for totally correspond to a new Greek pass and so with a new packet number Because you rely on connection ID, this means that you can have at most the maximum number of connection ID that you might have on your pass, the number of pass that you you have. So if you can add up to a connection ID on a on a connection, you can add up to 8 pass on your connection. we have that all these paths are showing the same key. And so we have the key updates stuff that's where you need to synchronize your path. And so that's what we actually fix in the in the summified version. also means that you may have gaps in the sequence number of the connection IDs because you might still be using connection ID 0, but then 1 to 3 is retired. And then you have connection ID 1000 83, so that's possible. Because we do that as well, we override the an ID feature of our earth's 9000. We change it. and we use the app button pass to request you to close given for Next slide, So the idea of This issue is to say, okay. Let's press assign the connection ID to some pass. Pass. identifier. And how can you do that, actually? Instead of using the new connection ID frame, you use a MP new connection ID frame stating that"
  },
  {
    "startTime": "00:30:03",
    "text": "This connection ID belongs to that as identifier. And so you will have a set of connection ID that will share to send back in the box space. And so it means that if you rotate the connection ID of our optical if you use connection ID of the same pass ID Actually, you still use the same mechanism space, and there is no ambiguity there. You can also explicitly state the number of paths that you want to be able to simultaneously use. So we can define a neutral parameter stating the number of maximum active paths that can f. And then you specify the number of connection ID that you can add for each path identifier. And so this actually, the couple the fact that we have 4 to 12, the network pass from the quick pass, which are actually corresponding to back end of those spaces. And with that design, actually, we can also have we can also define different keys, and we can have mechanism just to f specific keys for each pass. Identifier we keep The sequencing of the connection ID sequence number of pass This also means that we keep a connection migration feature, but on a per pass at 11, And we can also define then a frame to say, okay. We want close the pass? Okay. Let's close the pass ID anytime. All the connection ID and that's it. And so we do not need to pass up on some stuff that we have So that's a quite important is unchanged, so maybe it is would deserve a lot of discussion. cattle hog. I think the downside of this proposal is that it is a very disruptive change. statistically, it changes how path are identified by the 4 tapples. Mike Mike, please. because there was -- Sorry. are managed. In quick viewer, I'm not far draft. Each path is a direct property the connection state. But in the proposed approach, each path as identified by pathside is"
  },
  {
    "startTime": "00:32:03",
    "text": "will be managing multiple paths identified by the 4th double. This is because natural binding happens within the past ID space. If that happens, within the path ID space endpoints have to keep the old photo while probing the new topic. The power of rates will depart will be departing from the 1 of the approaches that we have had in PICTORY 1. moving to a 2 layered approach, and that's going to be a complexity to the endpoints that already support past migration. to The bright side of this approach is that this proposal is by far the SIM us approaching propels, So I think there's a bright side, but it's complex. Yes. So so it's true that you need to be you need to be a little bit careful to think about the when thinking about net revising. because a net rebinding will now appear as So so so as a server, receiving a packet from a new four couple, can can can be the result of a net So you need to probe that you passed now. I I So so so it's it's it's difficult because we don't have the right terms here. Like, there's the one path identified by the path ID, But now if there's a natural binding, there's this other 4 tuple that now needs to be probed. So we need to be very careful to distinguish those. We don't need have a new suggestion controller and and and loss loss recovery context. we will need to run run a probe on the on the new Yeah. Exactly. So, actually, you would you may have different vegetables on a same pass identifier sometime, but given to that pass identifier can only use 1 at at a given time to send non probate frames. Yes. So so this is this is basically in line with what what RFC 9000 SaaS. Yeah. that you only have one congestion control. And here, we have only 1 congestion controller per path."
  },
  {
    "startTime": "00:34:04",
    "text": "the the upside of this proposal is that it's immediately clear that a net net rebinding happened, and then that it's not a an intentional migration event because the server can see which path ID the packet belongs to, because it can just look it up by the Mady. from Alibaba. I remember that we also had this discussion a a very long time ago. I think at that time, the conclusion was it's very difficult for you to pre assign CIBs to the to a different path. because maybe in the first place, for example, if you have lot of paths. And if you have application, that keeps connection running for maybe 1 week. and you actually in the first place, you don't know how many pass you are going to use. so the assignment becomes quite difficult. I think it's a it it is a challenge here. And the second thing is we are talking about a net rebinding and net debt rebounding. I I in our experience that if you keep sending ping frames on on the passive. and you can I I think network binding is not that frequent. I I think but but but here, I I really think that we should more measurement to verify how frequent network spending can affect you yeah, I I I think that's the second point. And the third point is so I I I'm I'm Also, trying to understand why we do not want gaps in the CID sequence member. I think in operation, it doesn't affect me that much. What what it was is there a strong reason? We we don't want that. Well, actually, the point is that it's not only specific to not migration and so not rebinding. It's actually There's also a way to limit and force the maximum number of paths that you may simultaneously use if you want to allocate some specific resources because you want support up to, let's say, 2 or 3 different pass at the same time, you may have more network pass"
  },
  {
    "startTime": "00:36:01",
    "text": "But you only want to support up to these number of parts can be simultaneously in use because each passed in use, actually, maintenance state. so if you increase if you have a lot of network paths using a not of the current connection IDs, you need actually joint it keep state for each connection ID. So each back in the most basic sequence control itself for it's condition it. we're we're effectively at time, but I'd like to to drain the key on this topic and then see if we can draw any any kind of conclusion. Okay. So, Chris, please go ahead. I I really don't like that propose change because I think it may of many problems in a single thing. And the first I mean, whatever we do, I think we have separate the decision about encryption keys and the decision about pass. It we we know that having multiple encryption contexts increases the memory of an idle connection a lot. And so I would be very reluctant to see that. The other thing is that limiting the number of paths goes into the problem of having temporary paths doing not rebinding. I mean, if you have only 2 paths and one goes to natural bending, suddenly you have 3 pass, and you need the 3 of them. Otherwise, you cannot resolve the tree. They're not rebinding. So there is a plan there. if we solve that problem, we can have a transmit parameter that states the maximum number of paths that one node wants. that we can do that regardless of how we do it. and so we could separate it. And I also think that this business of managing loss, packet loss and managing congestion control. I mean, My implementation has only 1 control per pass even if the"
  },
  {
    "startTime": "00:38:01",
    "text": "connection ID changes. That's trivial to do. And the same thing is true for the rest recovery, you can absolutely do it to it's a bit more complex and for the concession control based it's not an actual problem. I try. Maria? Let me go keep it brief, please. So having a knowledge of a past is, like, concept that you can also apply with the current setup we have. It's just, like, it's not baked into the protocol. It's not that you have to signal it. Right? But, like, can you hang out of this concept. And I think that's what Christian is doing by saying he keeps the congestion control. The only case where it fails in the current in the current setup is the case that you described on the previous slide, which is when you get a net rebinding. So you have 5 double or 4 double changes, and you happen to have a user ID at the same point of time, can easily happen after an idle time at work. Like, you as a senator just decide, you know, I haven't sent so much for so long time, so a new a new set. So it actually can happen, but this is the only case it will not happen very often. And this is also a case where actually congestion control is not a problem because just had an idle time. Right? So your congestion control should anyway be low, and you should be careful. I don't think congestion controls the problem. The problem we've been solving trying to solve on the career side is just, like, you On the other hand, you don't know that it's the same path. So you believe it's 2 paths, And if you try to send on the old pass, which was not actually a pass, you just fail. So you don't wanna send on the old pass anymore. You wanna Like, at some point, you will fail. You would take nothing goes through, and I have to close the path. Right? the only problem we want to solve here is to close this path as quickly as possible. so you know you have to send on the on the new pass. So this is for me the only open problem we have. And I think there are other solutions to it that are actually more lightweight. then introducing this whole concept of a path into the protocol and actually put it the on the wire format I'm just gonna join the crowd of people against this change."
  },
  {
    "startTime": "00:40:03",
    "text": "having having 2 layers of concept of path where we have path and a addressed topple or something. I don't know exactly what it is. is is particularly problematic. the idea that you might get an at rebinding within within this and not treat this as though we're completely a new pass. I I find a little difficult to to reason about the the other thing But I think sort of came up in this was if you allow NAT Reminding, you also allow the client to simply change the path on which it's operating completely. in in this context. So in in in quick question. 1, the client can just move to a completely different address and that's fine. it's a different path. and one of the great advantages of this particular revision to the protocol is that We are treating them discreetly. at and one layer. rather than having this 2 layer mechanism. So, again, I'll throw in the others. Okay. thanks. We're at times. We need to to move on. I I'm not sing any real strong conclusion there. There's some pros. but there's cons too. I think maybe seem to go back and maybe take take some of the things that people want and come up with a slightly different approach. Yeah. So in any case, we we should continue discussing on ship. which is on GitHub. Yep. was gonna have to move on. Yeah. So in the next steps in the end. So Great. Not not not very surprising. So Thanks. Thank you. Martin? Come up. Come up. we're moving on to the next agenda item of reliable stream reset. Yeah. No. I'll try to keep this short."
  },
  {
    "startTime": "00:42:02",
    "text": "Next slide, please. So updates since since Yokohama. Kazuho joined as a as a coeditor you for that. Can you upload the new version of the slide? the one I felt. Yes. Okay. I'll I'll I'll keep talking through the slide. the chance to update the slide deck. we in in in Yokohama, we we realized that naming the frame reliable reset stream is a little bit confusing because you implement the extension, it behaves more like a like, a fin with an arrow then as a reset. So we settled foreclosed stream for this for this revision of the draft. And also requested that we add a variant that doesn't have an error code. So now there's 2 variants of the of the closed stream frame. How are we doing with the slide deck? yeah, we're trying to change the slides to Martin's updated ones a chair that will not be named may have uploaded the wrong deck. It's"
  },
  {
    "startTime": "00:44:34",
    "text": "The silence is intentional, Christian. Lorenzo, if you're listening, We do need a slide update. Okay. You can. it doesn't it doesn't do anything Not when you're Yeah. For anyone that's curious, the reason that this is a problem with that. Let's just go go with the 1st. Yeah. You can. Yes. Correct. Yeah. Yeah. Meet echo."
  },
  {
    "startTime": "00:46:28",
    "text": "First people are starting to leave the room. Hi. I'm Altana from. In the meantime, could you basically describe what do you mean by reliability in the reliable reset I I had to slide for that, but okay. I'll I'll do it without the slide. So in in in in quick version 1, if you reset the screen, you have, like, a bunch of screen frames that are still outstanding. you will not retransmit those stream frames if they are lost. You will only re if if you if you send a normal thin bit, Right? you will make sure that delivery of the stream data is is reliable. But as soon as you send a reset stream frame, you just say, like, okay. If any of those packets are lost, then I won't reconsolidate it. And this extension to to Quick is intended to declare a part of the screen at the beginning as reliable and then reset the stream and say, like, okay. Everything up to the reliable size will be retransmitted. and everything after that. I will not retransmit. Yes, there we go. Nice. yeah, this is the slide of to what I just explained. Next slide, please. So now we have the the closed screen frame. which contains a field with a reliable size. And an optional application error code field. So depending depending on the frame type, the location error is there or it's not there. Next slide."
  },
  {
    "startTime": "00:48:01",
    "text": "It was pointed out by MT on the list that this this this might be problematic to have to have an a variant that doesn't have an error code. and I basically copied this from the email that he sent. So we have we we have the following three cases. It's like, you you send data and you want the peer to keep none of this just send a a reset stream frame. Second case is you want to half. a part of it being delivered reliably, but discard the rest after their reliable size. you use the close frame frame. And the 3rd case is you want to keep all the data then you send a a stream frame with a with a thin bit. So depending on your on on the API, that you on your and on the stream API of your of your quick stack. this might or might not be a little bit awkward if you receive a closed stream frame. without an error code, So imagine the case where You've transmitted. let's say, a 100 bytes. and they have been received. And now you receive a close stream frame with a reliable size of 50. So if it does have an error code, that's totally fine. That's basically your reset stream code path that you already have. But now what do you do if there's no error code? Like, how do you signal to the application. And as I said, like, in in some APIs, this might this might be complicated in some other APIs. this might not not be a problem. The that that that that general so the the the the editors of the draft Can I struggle with this a bit because while we have a very clear use case for closed stream with an error code. we don't really have a concrete use case for closestream without an error code. and it's always"
  },
  {
    "startTime": "00:50:01",
    "text": "difficult to design something if you don't have. don't have a clear use case in mind. So unless there's a unless we get feedback from the room that we really want to keep the close stream without an error code. we would remove that for the next version of the graph. next slide. Then there's 2 more things I would like to get a would like to get feedback from feedback on. The the first one is is the is the frame type. And in the current version, we are using a 1 byte frame type There's only 64 of the of them, so we need to be a little bit careful Quebec. using them. The the justification for using a 1 by frame type here is that this frame is basically equivalent to A reset stream will be sent in in a lot of cases where otherwise you would send a research stream frame which has a one bytecodepoint So it would be nice to to have the same property and to to save some bytes on the wire. when you send this frame pretty frequently. but just wanted to check if there are any any objections to that. The second one, next slide, please. It's the name of the frame. Should we keep close frame? Should we if if we remove the the the no error variant, we go back to reliable reset stream? Should we do something else? don't think we need to discuss this here. But if you have opinions on this, then maybe send them to the list, and we can We can rename it if that's the conclusion. Yeah. I just wanted to point out that's a bike end of shed, not necessarily a bike shed, I'm sorry."
  },
  {
    "startTime": "00:52:00",
    "text": "Let's go back to the previous slide, please. It's it's only Tuesday people. I I've no objection to to to one. I I would have been fine with 2. But I think I think We have plenty. And we're not really using them up at any great rate at this So good with 1. seems like it seems to me almost like the default position for this particular working group should be to look at the the 1 byte space. unless we're who's starting to take a large chunk of of the space, and and that may be that may be true of some of the multi path once where we start to look at the the the 2 white ones. But, otherwise, this this This is this is core. functionality, Okay. Victor. I wanted to say it till 1 by the fine, especially in quick where we don't have as much problem with colliding numbers, because you have to renegotiate all of the extended frame times regardless. Yes. But they are also Ayanna start. That's a so we can double couple of yeah. But Would you like to respond, Martin? Will you wait for me? Okay. Take me off my chair hat. I I think one bite is fine for this. I do agree. maybe putting the hat back on that we we should try and think a bit more about like like, the things in this group get priority for the lower spaces than others. maybe not. I don't know. But that's it's a good idea to keep in mind maybe in, like, a few years' time where we are getting close to the end of those things to be a a little bit more discretionary about"
  },
  {
    "startTime": "00:54:00",
    "text": "that. And we have the designated guy. experts on that? but but but but but but but but but but but but but but but but but but but but but but but but but but but It's also a working group method. So what I'm what I'm hearing for myself, know there's there's one by probably seems fine. the question I have though is are we gonna do that now? we gonna keep using some kind of tbdcoploint until we're happy that the the frame is good to go in case there's any interrupt issues, etcetera. the current draft uses as your X Twenty. So -- Yeah. It's fine. We'll bust it anyway. Okay. I I think it's perfectly fine to continue to use the the low water CodePoints. because the transport parameter will differentiate. between the the different uses. The only concern there is if someone to implement multiple versions of this particular specification. and also like like can currently use them. We end up in a in a bit of an interesting We can deal with that problem when when when when the code points or or or or when when things change. So if somatic's changed, then we we may have to pick a different code point just so that we can deal with that particular problem. particularly since people are actively relying on this right now and looking to looking to start building it. where transport looks to use this. That was my other point, which is that in the web transport setting, there's the potential when people are using something like mock that this is going to be used a lot that sort of argues for and the one point one byte code point as well. I I I just respond. I I was gonna like, make a similar point too, but wasn't sure. So it's good to hear it from Debit. daviskenazi, you know, Verint and Zuzius, I guess. So What are you planning on doing for the transport parameter? in terms of how many bytes that is."
  },
  {
    "startTime": "00:56:00",
    "text": "It's currently a random value. as it should be. Yep. I would like to get a small value there So -- You only send it once. I don't really care. that makes perfect sense. That's what we did 1 or 2 bytes. So for version negotiation where we waited until quite late in the process. I think from memory, we we decided end of working group last call was that point. when we moved to lower values. And so You'll wanna do that for the transfer parameter no matter what. I would say do that for the error code at that same time. because you're gonna have to make a normative breaking change no matter what with the transport parameter. So that kinda solves all of your what if we change it problems. and then you can resolve it then. Yep. for Yeah. So so the plans until Prague is is shipped a a 02 to incorporate the feedback that that we got here. I don't know if after that, maybe we are ready for for working group last call. There's no no other outstanding issues. would be nice to get some more interrupt. we currently have interupt between QuickGo and quickly, but it would be nice to have a third implementation at least. We don't So, yeah, just on the interrupt note, I I mean, mine from sync to imply there's some he's aware of. But if if fix the raw. and and and and People please please let us know. Let us know. Let us know. this Yeah. Yeah. I'm not seeing issues in the specification that would require us to delay a working group last call beyond what's discussed in potentially resolve today. Obviously, we can take those things back to the list, but Yeah. Yeah. Yeah. think we're making the progress that"
  },
  {
    "startTime": "00:58:02",
    "text": "rupted Progress. and and and and what was effectively requested of us to do to help out by transport and the lock. working groups. David? it's Ganazi. We're transport enthusiast and chair. David, I wanted to thank the working room for moving this one through this quickly. Like, this has been, like, not hasn't been rushed, but it still has been done quicker than we've been, and that's really good because We expect not like this is the only issue remaining web transport, but this is we expect this to kinda be the the long pole. So from my read of the draft, there aren't any open issues on how the session went today. I think our only limiting thing for working with blast call is implementation. So I'll I'll take an action item to, like, follow Victor in the hallways to make sure that Google's implementation is ready. to as well. He stood right behind you in my eye line, and he's smiling. So Right. Right. Right. Okay. Cool. Cool. and through and through Thanks. yes, cool sign up. stop. kilocalo Robin? Go ahead. from Yes. I assume I'm Hurable, Alright. Welcome all to another Q log updates and to celebrate the latest box office hits. This will be the barbie edition You see, can I Can I move the slides myself? Yeah. You just have to ask for it."
  },
  {
    "startTime": "01:00:00",
    "text": "Uh-huh. I did that. It's a slice request. Right? Oh, yeah. There you go. So update since last time, We had a little bit of work, especially editorial updates. We did have some breaking changes So we up to the version number, more on that in a second. One of the biggest changes is that we now have to board for all of the quick n h three extension RFCs that we decided last time that we would include in the documents. Additionally, we also had an early sector review. Thanks to legally blonde Barbie there and especially Dan Harkins as well. No issues refound specifically around security and privacy for now. all the recommendations were pushed through in this version. as well. So breaking changes might sound big and bad, but especially relatively simple. It's just some renaming's. So in QROC, every event has what we call a category. For example, you would have a transport category. And in there, you would have packet sent vent. categories turned out to be a bit too high level. once we started adding more stuff. And so we decided to rename those to more vertical specific names. So, for example, HTTP became H3. and transport specifically became quick So this doesn't actually change any of the of the for functionality, but it does mean it's a breaking change because all the implementations need to change, how they call things. We do have a few other issues that also had our breaking change is in a minute. So for those and in general, I would I would say people who are currently implementing using QR code off, in renaming all of these things until maybe next drafts or even the ones after that until things settle down. a bit more there. Then in regards to the RFPs that we now added. So as we agreed upon last time, we would"
  },
  {
    "startTime": "01:02:01",
    "text": "add everything up to and including the RFCs that were finalized in 2022. So, for example, we have the grease bit extension. We have the extended connect rep sockets and HTTP 3. And then we have the extensible prioritization for http 3 helped by our Betalfield triage Barbie on the right. All of these were relatively straightforward put into q log very minor changes. One thing we did add explicitly for the prioritization one is to have a separate priority updated event even though most of that can be inferred from other things that we log. It's always nice to have a separate event for these kinds of things. I think most people would agree. But these were easy. The one that was slightly less easy or the 2 that were slightly less easy were the Datagram ones. So for quick and h three, or previously proposed these as a separate extension draft, as you might recall, last time. We then decided to all of these into the main documents, so that was that's what happened now. Important to note, again, this this is only the basic data claims sent things around that, so we do not have anything about the capsule protocol in there. just as an FYI. And as part of this change, we also made another breaking change where the data moved event is now split into 2. stream data moved and datagram, data moved because now we suddenly had 2 types of data that have been moved around. that's kind of what has happened. That's when the current draft the stack. So the next parts are are things that need to happen or things that they've closed before. starting with this last thing because that has further impacts because now we suddenly have Datagrams. obviously, to deal with. The question becomes, what is the data grant? because we also have minor support in QLOG for UDP, level logging."
  },
  {
    "startTime": "01:04:03",
    "text": "We used to have this as part of the transport Datagram's sent. event. Well, you do things like Back of coalescing inside of UDP Datagrams. That made sense, but now the transport has been renamed to quick. We now have quick datagram sense. but we also have a quick datagram frame. that you can lock. And so it might become confusing what exactly is a datagram. would level the stack. So the current proposal is just to rename to have a new category, basically. to duty to duty Datograms the actual UDP Datograms, the defense is actually UDP. category name there as well. I don't think this is very commercial at all, but it's, again, another breaking change that will happen in the next set of drafts. That's the main thing that would that would happen there. the the next thing, is that we want to have a better approach that we first and q log in how we actually say which which additional queue log documents are be using in this particular q log instance. Last time, we had the proposal on the left. where we wanted to explicitly link to different schema that we're using. was a lot of discussion about how you actually link to the individual schema. We propose just using RFC names or draft names that was not well received. So the new proposal is the one on the right. where we will use universal resource names or URIs that are actually registered through Ayanna for individual documents. well, and that can then be used to all list which exact defense you're using in it in a specific q log file. As long as you're just using drafts, of course, you don't need to register with Diana just like we do with parameters and everything there as well, which is what we're also doing in the current draft. Just having these as placeholders. This is"
  },
  {
    "startTime": "01:06:02",
    "text": "only just a proposal. This is also very new So any any feedback on this is more than welcome today, We do have a a precedent for this, which is also linked on the bottom of the slide. for some RTP parameters. They're using the same approach there as well, which is something that was suggested to us last time. Then we have the same thing. Sorry. Can I just dive in? just to add bit more context. So, yeah, the The just on the slide, I wanted to, like, avoid doing that at your end or of the HP stuff, but hence the old design. But, you know, based on the feedback in Yokohama and some of the suggestions from Jonathan Lennox and looking at the RFCs of how our does this. It it doesn't seem terrible, I mean, if if people have strong opinions against that kind of thing, I please go and go to the issue or the PR. But I wrote this last night at, like, 2 AM. So Robin made this light because my eyes are going cross, so I couldn't anymore. So it's very raw, but I think I think this can actually work. Maybe there's 224, not But this is really the whole thing is really key to making sure that Q log is an extendable format can be extended properly for people to use So, we can actually hunt on new things that are coming constantly and get this work done. So it's it's important the shape of it. valuable, but but we'd well, I personally really appreciate us getting this fixed for the next pikes, Go ahead, Dror. Bobby. I agree with all of that, Lucas. Alright. So for a very long time, we had this well known URL in the draft there as well that people could use to request Q logs from a from an endpoint. It turned out that made just about as much sense as the barbie telephones shaped as high heel shoes on the bottom right. So we asked the the list."
  },
  {
    "startTime": "01:08:01",
    "text": "very recently, and they all decided, you know, or or all agreed would be better to remove this from the q log graphs. However, We do have some people that that would like to see this general concept of having a way to extract logs from endpoints especially in a privacy and security sensitive way to live on in some form probably not q log specific. We also know that there are existing So how should I say? Conflicting proposals on how to do that? I think, fastly or Kazuo had one of these for their current approach as well. So it might be worth keeping this around, but not within the q log or maybe not even within the quick working group. So we are looking for people who would like to take this up. for today or so. So I suggest we take Martin's Yep. Sure. Hi. I'm Martin Duke, Google. is a few slides ago, but there's no need to go back. on the so the with this Datagram's this Datagram counter that's are you, like, introducing a new UDP namespace? specifically for that in the quick document. Yes. So, like, This is maybe just a a document, like, inside baseball thing. But we were later than to decide to do a UDP If pqlog, I don't know, module or whatever you call it, would you then have, like, separate draft for that and have to reference this thing because you got this bit of UDP over there. Yes. likely. Okay. is similar to the splitting the queue back in the h three draft. We also have TLS related events. within the current quick drafts and so on and so forth. Well, yes, there is some overlap, but that would be the there. Alright. If if you are you've already gone down that road, then that's fine. I was gonna suggest maybe, like, having a UDP's not that complicated. Like, you could have a little UDP module and just, like, the extra work to flush that or I don't wanna suggest another document, but have another document. If you've already got this stuff scattered everywhere, then mind. I I think"
  },
  {
    "startTime": "01:10:01",
    "text": "we're probably stuck. Thanks. dotdot just to respond to that quickly. I mean, if you're volunteering to write that, yeah, and do it in a transport area, then we could help support you. But but, actually, what was nice, what I realized yesterday looking looking at the URLs stuff, there's no these URLs, you can actually have single documents include multiple schema, and and reference them with fragments and do all kind of stuff. So The the additional schema here would be affected via partial UDP, like, 1 or 2 events here, and then, you know, to a full UDP support, you would then link to some additional components in another dot someone wrote that. I I think it could kinda work even if it's pointy. Yeah. I mean, like, I mean, I'm not gonna write it. I would I would support it going to teaser if someone were to write it. I guess it's a little bit of a bummer somebody who wants to go do a TLS model or wants to implement a TLS key log or a UDP keylock model in the future. They have to read, like, 7 different RFCs. But like I said, I think we're already down that road, and I don't wanna you know, tip over the cart that badly. Thanks. Alright. And, yes, we've we've been down that road, I think. current approaches to better one. the main remaining thing, and I I just in a flash, updated the slides, specifically for Ruby and the chats to include Ken. Because we're once again for your help with multiparts and connection migration. While they are definitely linked and important for quick as a whole. they are so much strange to us as queue log editors because we have very little implementation experience. So they are kind of like, to us, Barbie, Conversecers, we kinda need some Ken experts to to kind of help us out. If that will not arrive, We will just kick keep a very, very simple concept there just having We call it path ID, which is basically just a string."
  },
  {
    "startTime": "01:12:00",
    "text": "string, you can do whatever you want. It is very application specific. and that you can use then to log any kind of parts and multipots information in there. till someone actually makes a multipart extension down the road. We believe this spot ID can support everything we currently don't have. even things like logging IP addresses for whatever policy is trying to probe, which we currently do not support. Just something to be aware of. But it would be very very rough and very implementation specific. However, again, if we don't get any more concrete feedback, hopefully, from people currently implementing Multipart, doing the intro from the multipart, I would be very interested in knowing how are are you how you are currently debugging this. internally and what we might learn from that. If it doesn't arrive by next draft, we will just have the part ID. and move on to to other things. Finally, one of the big open items is the QPAC event. We work Kings. We had hoped to get something proposed for this time as well. Robin, for that. Robin, sorry to cut you off. Myrias joined the queue. I'm I'm I'm presuming to speak about the multi path. That's correct. So So I guess pass ID is in a pack field. So, okay, you can put whatever you want there, but, like, Yeah. given where the draft is right now, that you consider just putting the connection ID directly in there? or, like, actually both of the connection IDs eventually. Yeah. And those are you can possibly log those in many different ways already in in quick in q log currently as well. either as part of what we call a group ID or individual packet events. as long as it's at the quick layer, it's fine. put it as a top level feed to kind of all the other locks you have, basically. instead of the pass ID field. definitely an option. I don't know enough about multiples. I know if that would be enough or not. I think that's"
  },
  {
    "startTime": "01:14:01",
    "text": "mostly what we're doing right now where we hacked it in. Yeah. Just yeah. I was hanging lurking at the hackathon on and not not strictly hacking on interop of MultiPaths. But I was I was speaking to a few people, and came over with some ideas of maybe how something like you guys could help visualize the the comings and goings of paths. So a bit like a stream multiplexing view. or a waterfall model, something like that. I can I can visualize how it would work, but I don't know what the events are. So let's take that that part of the discussion offline. because maybe, you know, we have a use and and that can help drive the design. Christine, go ahead. Yes. I mean, Mark, and Excuse me. You you're you're asking for health on volunteer to help you handle that. And I'm I'm waiting to start helping there. The the main issue with multi pass is not so much the pass ID, but enabling tools to match past specific packet numbers, past specific acknowledgment and things like that. And, for example, currently, when you use QVs with a multi pass log, It's kind of we are there. And and that that's what we have to to do, basically, make sure that we have enough information in the q log that a new version of QVs could handle multi pass properly. Yep. Exactly. And I would I would love to work with you on that. So, yeah, I mean, I I I I mean, I'm very busy, so, I mean, I'm may not be the best person if someone else volunteers. I'd be very happy to help them to Yes. We we need to move on. Robin, can you have you got just the last couple of slides? Can was doing"
  },
  {
    "startTime": "01:16:00",
    "text": "So As any proper movie, we end on a cliffhanger for part 2 next summer. So the QPAC stuff is coming. We just didn't get it ready. And that was basically it. This is the end, hopefully, not of all things. is just a summary of what we talked about, if anyone has Any more comments right now or otherwise on the list? on the GitHub issues of force. Thank you. Thank you. Great. Thank you, Robin. Thanks, Dave. being in a weird hour. for me for me for me I I don't even know what it is. Lake, I guess. Okay. Up next, we got Ian. There's no slight But with gonna just gonna share the slides we have. want. Yes. Please. After what yes. might not be precisely what we Well, there's not much difference. I I only have 10 minutes, though. So two of the slides we're gonna skip over because I don't think they're actually critical to cover. So here's an update on act frequency. I'm in sweat. coauthors are Miria and Jana, So thank you for up there. Let's skip this slide. but it's here for reference. So the change is since 04. They're mostly quite small. Even the normative changes are are quite small. So we removed a must that really just didn't need the be there? removed to should upon migration because a kind of unnecessary recommendation. So things like that. There was a new error code added They're speaking of error codes. for an invalid, request, max actually value so you can tell that from other error codes. So people are error code nerds, then they should check out that. issue, but these are pretty small changes. There's some other editorial changes, of course, but those are even even smaller. Next slide. Let's go through some open issues."
  },
  {
    "startTime": "01:18:00",
    "text": "So this is the big one. So I would like to get this settled. I Martin, I believe, opened this issue. Is that Okay. I think Martin suggested that though immediate act, worthy of having a 1 byte frame type, which we kinda spent some time on in the past. act frequency was going to be sent much less often And given the number of one byte frame types we have, maybe 2 bytes is just totally fine. I don't have a particularly strong opinion on this. But I would and I and I think it's completely true that frequency is going to be sent a lot less often. Right? I mean, at at the most, I would expect it to be sent. you know, every maybe 10 RTTs or something like that. and even that seems unlikely. like or, like, what's in the connection? Do people have a strong opinion on this? besides I believe Martin, do you still have a strong strong opinion supporting Can I ask you a question? Yeah. I'd like you said, I think there's it it very likely that this is gonna be sent 1, 2, 3 times per connection, and that's it. And so it seems like using using a prime real estate for this is error. bit of a waste really. Question. Maybe I'm doing it to own, but one is sending I I see logs in which the accuracy is sent much more than once per IT. Bell connection. The act frequency as a delay component which depends on the evaluation of the RTT. and the OTT will change I mean, quite quite often. So it also the accuracy also has a component that depends on the evaluation of the concession window. because that's how you compute how many markets you want to have."
  },
  {
    "startTime": "01:20:00",
    "text": "And so those things 1st, they changed rapidly during the beginning of the connection. And then, yes, they change slowly, but they do change. And it's not faulty, but most connections don't go which falls out in the start up. So, yeah, I'm I mean, I could do with 2. I don't care. that touch, but it's will be sent more often than you believe, especially if you call the other connections. I think all Christian's points are true. I think the question is, Does it bite matter? On the other we still have what 60 more code points to give out or something. so, like, Okay. I'm gonna change it to two bytes unless someone is throwing objects. Is that okay? This is not a strong opinion, but I kind of prefer using one by because when you add a tail, I mean, I mean, it's just easier to have have that space fulfilling in the bike rather than having to check that there to buy. That's a strong Yeah. Yeah. Act frequency is already a variable length frame, do you have the nature of it? Like, it's not a one byte frame like immediate x is. So I guess that would be another argument why maybe the extra byte matters less because you already throwing some buttons in there. Is that a convincing argument to people? I would say just just just move it. Just change it, and we'll see if anyone really wants to fight it. I I just don't wanna I mean, we're getting pretty pretty close to the end, so I'd prefer not to revisit this. Ed Martin has a suggested code point. That's two bytes that he'd like to suggest. Feel free to do that. Skip this. unmisha. Great."
  },
  {
    "startTime": "01:22:02",
    "text": "There's a question or an issue opened by Corey. about sending an immediate act soon after an athlete limited period. I think the the specific concern is if for some reason, we created settings where we only sent 1 RTT, which is, of course, on very low number of experts to see and you come out of full quiescence. you can actually go 2 entire RTCs without receiving a single act. while sending, which seems like a very long time without any sort of acknowledgment feedback. I think that's a valid concern. And so, you know, the question is, do we wanna add some sort of around when coming out of quiescent's you know, in including, like, an immediate act frame or something in one of the earlier packets coming out of quiescence to make sure that there's some fairly timely feedback Or I'll alternatively coming out of Chris since we can also say, maybe you should send that frequency frame and, like, drop down the accuracy, so it's a little bit more frequent than it was before. Like, I mean, there's a number of and technical solutions to this issue, but I think it's worth probably adding some sort of tax would be in my inclination, but I'm feeling like maybe we could just make an editorial comment. But if people I'm really getting looking for feedback on Do people want an actual normative statement? Like, it is recommended you do blah. Or are they happy with just here is, like, issue that could happen you may want to be aware of it and adjust your you're, you know, responding accordingly. Myriad is technically 1st in the queue. Okay. Yep. I said, I can have priority. I'm sorry for hers. So I don't like do nothing. And the this is could be much, much worse when you get the RTT wrong because the RTC was varying a lot because you wait for 1 RTT you previously thought was an ITT and know the RTT's worth shorter So this can actually reduce the amount of feedback, you've got quite a lot. So I would like to see recommend generating at an somehow"
  },
  {
    "startTime": "01:24:03",
    "text": "And I really don't think we should be prescriptive about what way you do that. So I don't know how you find that text, but what I would like to see. I I'm happy to write text around, like, ways that one could, like, accomplish this So I think this is again, it's very much for me a brown, like, do this normative or not? That's that's the feedback I want from the working group. So I I understand your opinion, though. And I I think if we wanna go for another the BCP on RTT estimation said you should do it once per RTT. of the current RTT. expecting that you send every other racking TCP I know. But so it kind of feels close to what we should really be doing to be a sensible citizen on a varying Internet. Okay. If there's an think I mentioned an issue, but if there's a specific informational reference like, a section we should reference that, like, kind of really, like, informs this you point to an RFC, but it was fairly long than also I think would be the me towards, like, something normative. But here you Yes. I'm I'm want to disagree slightly, but I think we're still agreeing at the end. So think the issue you're talking about to this draft, this is a general issue. Right? If you wanna, like, talk about it, then, like, You can talk about it, and you dropped it about it. However, the connection to this graph is that we have didn't get a mechanism here, and that can be helpful in the So I think it's good to, like, mention the situation and recommend that we we could be used, but I don't think we need a normative reference our recommendation because it's not an interrupt issue just like editorial text is fine. Yeah. Yeah. No. I didn't want a normative reference. I just was ending a there's an informational But yeah. Sorry. I didn't I didn't meant reference. I don't think we need to use so much language -- Okay. Thank you. That's what I was looking for. Christine? You're pretty much what Mirya said. I mean, You you definitely don't want to have a lease nominative statements saying this is when you send the immediate act because, I mean, it can be for a ton of reasons"
  },
  {
    "startTime": "01:26:03",
    "text": "And it's really something that will vary from implementation to implementation may send one after a PTO. You may send one when you are doing a contrition bandwidth estimate. You can send one in in many, many cases So I would hate to have a list of normative statements saying do this, do this, do this, because it will be interpreted by someone as protocol error, if you do it that outside of the list of prescribed recommendation, And so that's there's a pitfall there. So please don't put any kind of normative language. And, yes, and be very careful when you edit it to say, well, you may do this or you may do that and blah blah blah, but it's it's that it doesn't look normative in any way. Okay. Martin. Martin, do Google. I'll definitely do something. regarding editorial versus recommended, I guess my question would be, like, how sure are you? about this. I mean, certainly, my intuition is that this is a good thing to do. But only thing I really know about guest controls that often intuition is wrong. So, like, if if if we if we have if if if we have data or, like, or, like, model this, you know, formally or something like, yeah. This is definitely what wanna do, we should recommend it. not, I don't think we wanna have recommendations that turn out to be bad. when we actually deploy it. I I don't think we have data that's solid enough to say that this is definitely be there's no right answer here. I think this is a consideration one should take into account, but I I do not think anyone can substantiate data. when Yeah. I think we have the wrong end of the stick. If there's any recommendation to be had, it's on the receiver. you don't have to send a frame in order to get the behavior that we want. if the receiver receives a packet they haven't received a packet for what they think is more than an RTT. Maybe I should just like, send the Mac. And then we don't have to worry about what what it is to be saying about an immediate act. Now that's probably something that RFC 9002"
  },
  {
    "startTime": "01:28:02",
    "text": "should have said. PTO, by the way. I'm Also, you spoke for a Oh, yeah. Okay. All all of you. Fine. But this document can say it, but I I think it we should be somewhat careful about sort of detaching from the immediate act semantics here and and be very clear that this is A gap. mistake that we made perhaps in 9 1002 rather than rather than something that's attached to the frame that we're defining. Technically, acknowledgments are 9000 still. I'm sorry. Oh, god. Yes. Well okay. So it's been my my r c Oh, no. No. I you should believe me anyway. But anyway but I agree with you. I will this this draft just exacerbates an an existing Right. And I I think that's I think that's that's worth highlighting there. Nope. Mike, please. I I really I've nearly asked whether or not we should have some sort of normative text. or or non normative or whatever. I have no opinion on the distinction between the latitude points on this one. We should have text. Yes. But the but I'm saying the shape of the text is about the receiver behavior, not the sender behavior. Sure. Gory is gonna respond So, Gary, first, quickly rebounding on that. I think we actually got it wrong because we didn't think the dilution of the act that would happen afterwards when he published this. So I think probably we should describe it well And I think Martin was right. He should have been a receiver behavior. rather than something the sender says because they're receiving those So let let let's try and write that paragraph because that might actually have no normative text on this document. Okay. Happy to happy to do that or other people can give you a text as well. I'm out of time. do I get extra time for, like, one more slide or Yes. You do. Okay. Thank you very much. Oh,"
  },
  {
    "startTime": "01:30:01",
    "text": "Skip Nope. the slide. deployment experience. Yay. Okay. So Matt has been kind enough to contribute some production experience. here you see some some interesting information. I'm gonna skip this slide, but I wanna move to the next one because I filed 2 and I want feedback. So two takeaways and matter. We should be presenting this is one you really, really need to really PTO with the MediaDAC, well, or you could do the thing we just discussed in Immediate Act after an RTC of questions. if you mess with the reordering tolerance. that that's critical to performance. So if I have an issue for that, And you need to be really careful with MinrTT here because minRTT can be enormously smaller than smooth RTT. and you can end up with cases where I say, like, the minute RT is 10 milliseconds. smooth RTTs, like, a 100 milliseconds and you end up you know, getting a act every two and a half milliseconds you know, on a connection that really does not 2 and a half milliseconds. intuitively, that seems obvious to me because of mess with congestion control a lot, but I don't think that's obvious to a typical reader, and I think it's probably worth adding some text. So here we go back this this class question of, like, These seem like just performance considerations that I can add editorial text on. and maybe it overlaps with some of the previous discussion. But if people think I need normative text about other of these issues Please don't have to speak up now, but, like, please at least comment on the issue. and and say so. because otherwise, it'll probably air towards non normative text. Did you wanna add anything else Matt, given this is your data. No. Not really. But also as a follow-up to their our nice bike shed earlier about the the size of this."
  },
  {
    "startTime": "01:32:01",
    "text": "In practice, I don't think it really matters what whether we do 1 or 2 bytes also in deploying it. So just take that as you will. Yeah. Thank you, everyone. No. Really? Thanks. And and I think, you know, what was saying it's And the draining of the open issues on this document. There's a bit follow-up work to do, but that's you know, we're we're probably getting in a position of working group last call and readiness. But, again, implementations, enough interop would be good. So if there are other folks who are experimenting and can comment on the lists whether they have data or just interrupt success, that would be useful information to help us decide whether to the the 1. the process and accelerate it. But thank you. Martin. Hi, Robert. Me, remember this draft. Oh, we've been we've been quietly beavering away. it's taken a really long time to deploy it. This turns out Google's big. Next slide. No doubt you're all constantly thinking about the minor details that's been case Some of you have not been. in the very first so, of course, quick OP is about a structured connection ID that does couple things, but mainly, it's about encoding routing information for for the benefit load balancers. then in the very first bite of the structured connection ID, There's 2 bits for config rotation. which is, of course, for code points. 1 of those code points you reserve for the server saying I don't have any configs. So this connection ID does not contain any routing information, so do something else. And so that leaves, of course, 3 configs, which generally would just be different keys. assuming you're doing encryption. And so what the outcome of that is I mean, a sensible way to deploy this would be heavier the bouncer have 3 sets of keys, essentially, like, the next the next one and the previous one."
  },
  {
    "startTime": "01:34:00",
    "text": "And then when you do when you try to roll new rotate keys or roll a new config of any kind. Essentially, if you have these 3 good points, you've kinda worked out the logic You can update things in any order, and everything will still work during that update process. And real condition is that Everyone, all the load balancers and all the servers have the config before you try to roll a new one. That is kinda condition condition, Next slide. Okay. So the proposal TLDR is to do this. So the other thing I I mentioned here so the rest of this first bite has to has that, like, an additional purpose? and that is although this is separate from the fee and flow balancers, it is like a a a schema for, like, a structured connection ID and the other thing that is out there. is that the the hardware people that wanted to crypto off crypto receive offload need some way to have a self encoding length in the connection ID. So if you're doing that, you can use that field. Otherwise, that's just random stuff. Next slide. So why do we wanna do that? Well, The point of holding this draft was to get some deployment experience and talking to people actually operate, Google's network, which obviously is pretty big. say that they really can't guarantee that condition that everything everyone will get the config we roll a new one. And so So right. So they're asking for a third bit that has some modest benefits and has some modest of Downside's next slide. Okay. So what are what are the downsides and they're not big. One is that, of course, connectionee cannot be any longer than 32 bites and be softencoded, there. Now, of course, quick v 1 and v 2 limited to 20 bytes anyway. I don't anticipate that getting a much bigger unless we end up in a post quantum world with like, gigantic connection IDs, And and if we we end up in that situation, then, like, there'll be a lot of silicon out there and"
  },
  {
    "startTime": "01:36:02",
    "text": "these hardware encryptors would have to be replaced. And that would be sort of sad, but not certainly not a immediate problem. And then the other thing is it's kind of an attractive nuisance next slide. And so just here's the situation. You have a you have a CDN, and you've got multiple customers on the same IP address because you're trying to help user privacy, so you're running ECH or something and doing s SNI switching. next slide. They're also doing quick LB. and Well, how do you get all these how do you, like, decide what back end to go to And one out, answer is to get everyone the same keys. which obviously has some trust problems. Next slide. Another option is to just try to use these all these config ID code points, like, give everyone a different config ID. But if you think about this for a little bit you realize, this completely defeats purpose of BCH because then every packet identify what customer it is going to, and that is pretty easy for an attack to determine. So, like, there's, I believe, already language and security considerations about this don't need that. But, you know, people don't always read security considerations, so we are kinda giving people foot gun if they're if they're not, like, thinking stuff through carefully. So this is not a huge problem. I they are minor problems, but I do wanna call this out. Next slide. Okay. So Few quick so, you know, I know the I know the set of people that care about this is is not that large, but I do wanna get some I I so the the the I I didn't mention in in the other slide the the effect of having this misconfiguration where the The load balancer of the server out of sync is that you will get that the server will end up generating unrivalable connection IDs that are not known to be un routable. They'll be misrouted. And that, essentially, what what that means is you will have the first flight go somewhere then the second client flight will go somewhere else, which"
  },
  {
    "startTime": "01:38:02",
    "text": "can result in status resets, which might not work because you're in a little handshake. Ian and I made a pass at figuring out what this would mean Chrome, and I think we decided that under many circumstances, you would end up with a significant increase latency, maybe handshake time out. I can't speak to other clients and and how they handle, like, his speed fallback and so on. that's the that's the so, like, the severity of this problem is also not that large. So I would be interested in, like, hearing from anyone who thinks that those drawbacks are much bigger problem than I do, anyone a client where, like, this is where this this, like I don't wanna say black hole, but this this one flight successful, second flight, not successful problem is gonna really ruin your performance. And then thirdly, anyone else with a big deploy says, no. No. Your guys are really missing the obvious thing here. We have a much better fix for this. And why are you having trouble syncing your your stuff? and I'll open the floor to comments at this point. if there are one's in a queue. Okay. Well, I guess that's ours. No 0? just do it. Okay. -- safe. Alright. Sounds great. Then we will go a couple more rounds to our ops guys to make sure that we really need this but I'm fairly confident that's the case. So I've expect to land this PR. in the next month or so. Thank you. Thank you. Thank you. Up next, is Quick and make yes. the pay. Hello, everyone. Yintay from Alibaba. and this presentation is about clicking enable service differentiation for traffic Engineering. And then I'm here speaking on behalf of my collaborators, Geelong, Yame, and Miriam. next size piece."
  },
  {
    "startTime": "01:40:01",
    "text": "So the first question, what is the problem we are going to solve here? Here, we have a backlink network between 2 edge routers, there are 2 passes. indicated by different color. and the blue one is called the premium pass. the premium passage hater for low latency traffic, a more interactive traffic, But the the downside is it's could be more expensive. and the bandwidth resource may be limited. and the green one is called a regular pass, and the regular pass is titled for a bug traffic transfer. that is extremely low cost and the bandwidth capacity is very large. So the goal here is to balance a cost and a performance. What I mean by that is we really want within a quick connection, the a critical data to go through the premium pass ensure good performance while the rest of the data to go through the regular pass, to reduce cost and save money. Next size, please. And so to do that, we need a service differentiation, So within a quick nation, there are basically 2 types of differentiated surveys for And the first one is for prioritizer streams. So basically, here, you have 2 streams. Stream 0 is a high priority stream. which carries interactive traffic or command or signaling so that you want the the string 0 to go through the premium pass Well, stream 1 is not that latency sensitive. So it can you can it it's better for it to go through the regular path to reduce cost. next size piece. And the second type is a differentiates a service for a prioritize a package, For example, for loss recovered packets, because you already have a loss, you so that means you have latency. you really want the loss recovery packet to go through the faster pass as much as possible, And also another type is the handshake because we want to reduce the handshake latency. So we really want those 2 specialized packet to go through the prioritize premium pass"
  },
  {
    "startTime": "01:42:00",
    "text": "while the rest of the packet a Yeah. Because is there maybe they are not that latency sensitive, So it's better for them to go through the regular path to reduce cost. Next size piece, and how can we implement this To increment this, we really need our backbone network to understand the priority information but quick is end to end encrypted. So it seems that the only option for us to do this is to embed priority information in past ID, So the pro what proposal here is to leverage Modpass quick plus embedding priority information in past connection ID because right now in multi pass quick, we have different paths have a different CID. So it naturally provide the opportunity for us to embedding this information And in the action, you have a client and you have a server running quick, quick, quick, quick, And the CID 0 here is for the high priority traffic. and CID 1 is for low priority traffic. So in order for the server to send traffic on high priority cast, it's basically, it uses a scheduler to schedule the packet on the high priority pass. And the edge router will try to parse the CID and decode the information. Now it looks at, okay, it's a CID 0. So it understands the semantics, it then can then apply traffic engineering policy to the packet. And in doing that, it can simply apply srv6 c or MPIS label to direct traffic to the desired path. And what is different of this proposal from the current multi pass quick is that typical multi pass quick application scenario you already have multiple last mile connectivity links. For example, set it on the Wi Fi, But here, we could only have just a one last mile connectivity for example, you only have one LTE link but you'll have multiple passes in the in your backbone network. next size piece."
  },
  {
    "startTime": "01:44:05",
    "text": "there are a number of benefits of this proposal we think The first one is it allows you to achieve a stream level and packet levels service differentiation, at a verifying granularity, whereas traditional DSCP you can only control your queues at a 4 triple or connection level And the second thing is that you use the space application has a direct control, And in in this way, your background network with service differentiation capability is directly exposed to the application, and your the application can assign of priority to packets, using a multicast scheduler, So it is is quite flexible to for the application to to use this feature. And the sec servicing is that the the packet reordering, architecture, mass estimation, and the pass congestion control are all automatically handled by multi pass quick framework So, really, we don't need to do any extra work here And the last part, not the least, Right now, the priority information is embedded in the connection ID. So it's a fuse marker and which will not be easily interfered by middle this? whereas for the SCP, it's quite often is that the the media box can change the DSAP value or remove the DSAP value. But for this proposal, we don't have this concern, concern, Next slide, please. And and this is the last size. so we're thinking about why it should be standardized. The first thing is that we need to think about how to the algorithm and encoding rules for the CIB, The first thing is that we because due to the privacy concern, we really need to some of classification because we don't want a a a a third party network or a public network to understand your priorities, semantics, The second thing is because quick LB also use a server ID encoding in the in the connection ID,"
  },
  {
    "startTime": "01:46:01",
    "text": "we need to think about that the code exists with server ID encoding. here. And the second big reason is that to use this feature, your kinda server need to negotiate. for example, the kind it needs to understand, does our infrastructure support this feature? And if so, how many a priority levels in total that the infrastructure supports and further also, we the application also need to know how many priority priority levels that application I want to use. And after the negotiation of we need to take care of the CID issued by the new connection ID frames. because each CID then issued by the new connection ID first must consider CID encoding rules and the number of priority levels negotiated. And I think that's all. we really if if anyone find this an interesting use case or if you have any feedbacks, please let us know. Thank you. thanks thanks for the presentation. We have a lot of people in the queue. We we have a little bit time in the agenda until the end of the session. I would ask people to be brief and concise, we had a a slight IT issue. So Watson's first in the queue. Come on up. We're gonna put Ted in second so that he doesn't lose his spice and then we'll just run the key 3. You wanna actually physically queue. might help a lot. like, the old days. But Watson, going? Watsonlad, Akamai, a person after me. Wait a moment so I can get back, take down the notes. So with this proposal, it seems that the diagram you drew shows one network between the client and the server. On the Internet, that's not how it works. You have multiple different network And it's very unlikely that the path that's being traversed is known. or that it's that it's even have the relationships that allow the BSC to be exposed here. The other thing is by having to sit encoding You know, the obligation for privacy"
  },
  {
    "startTime": "01:48:04",
    "text": "if you're differentiating by stream, you're now breaking now exposing a lot more about what kind of traffic's flowing over the connection, that is we carefully analyze. Oh, okay. Yeah. Yeah. Yeah. So so for our application, because we like a cloud provider, we also have this global backbone network. So it really is like a to the traffic to the edge part and through our backbone network and to to that our data center. Yeah. But but I I understand. So in in other cases, you have so many AASs Right? And the the the traffic will traverse. So that that's a different applications in quickly run deep fine. Sorry. Sorry. Ted Ted is next. type, type, No? No? No? No? No? Come on, Ted. You can do it. Good, Hardi. I actually dropped to the bottom because of a missed click. But thank you for taking me and my original place. So there's about 18 different things wrong with and I really encourage you to go back to the drawing board and start over. One thing I would encourage you to start with is explaining why you want to do all of this in a single quick as opposed to using multiple quick connections with different CCP markings. It's really not clear at all how you would use this outside of single SRV 6 domain. This is terrible for leaking information on potential privacy concerns. And frankly speaking, if you can't trust the DSCP markings from a single client, why would you trust this? And lastly, the way you would have to coordinate this between servers and the orchestration system so that you would be able to do this kind of modular system is just going to be a gigantic operational pain, and you won't want to do it. Thank you. Thank you. Thank you. Thank you. Thanks, Ted. Martin. I had a lot to to add to this list. that's I think Ted had made a good point. Yep. Alex. Hi, Alex fromowski Google. I wanted to point out 2 problems. One is that you really wanna do your prioritization at quick frames"
  },
  {
    "startTime": "01:50:01",
    "text": "not quick packets. So this is not going to work very well if you're doing a prioritization at packets. 2nd of all, control the entire network, why don't you just use, like, GRE or plus MPLS end cap or something if you use DSCP Bets. Also, Why is your server so far in your network? Please move your server closer to the user and, like, do multiple quick connections to your core back end or something like that. I agree with everything Ted said. David? Hi. David. It's Kenazi, a architect enthusiast. plus one to what's been said before, and then I'll add this sounds a lot like the application aware networking work that was proposed, and that is still ongoing on the list. So I encourage you to go there. I think they're trying to refine their problem statement. And I still don't see this recurring need to push this information onto the network. It still seems like there are too many privacy problems. but I would say maybe go there, get started on, you know, understanding that. And then we can talk about encodings. I agree with others that Quick is probably not the best place. but let's first see if we can reach agreement on whether this concept is a good idea. before we look into encodings. Gary? Gary Firthhus. Everything Ted said sounded good to me. But I'm on transport enthusiasts, so see concurrent multicompath. the way in which use concurrent multi path degree retransmissions and maybe congestion control, so it bundles and even greater set of things in here. I think there's too many things in one draft. Thanks. Martin Duke, connection ID enthusiast. clarifying question in a comment. clarifying question, is your intent to have like commonly known code points for this, like, DSCP, or is this on, like, a per network? completely custom mapping. It's more like a I I think custom Yeah. mapping. It's it's different than the SAP because the SAP is connection, but here is, like, you have multiple streams."
  },
  {
    "startTime": "01:52:01",
    "text": "Right. And you really want different stream to go through go with different paths. Yeah. I I yeah. I I got that. I but I just wanted to know if there's a, like, a for your answers now. Okay. So the other thing I'll say is, like, Modular webinars is a good And I think people have had a lot of feedback on that. QuickOB is probably a flexible enough framework to fit this in. mean, obviously, at some level road load balancer is essentially a router. And the the architecture of the structured connection, he does allow for multiple tiers. of rounding/load balancing, you could absolutely If you, like, configured it, right, you could absolutely use the QuickOB framework support something like this. So if we decide to move forward on it, I think there's a discussion on how the documents work. this is an informational thing on how to, like, tune quickly to do this thing or whatever, but we can we can get there. Alright. Thanks. Bye. So, Richard Jafonega, unfortunately, everybody in front of the here in the line has already stole my thunder. It is quite frankly not really clear to me what the what the problem statement here is. and how you want to negotiate this with the infrastructure as you have here on the Slide 12. again. all the other points that have been erased before me, I fully support. So I don't think that this is No problem. Thank you. k. Same thing. There's a lot of statefulness in this structure. One question I wanted to ask is, For every application, their own stream is most important. Like, there could be a web page with 100 WebRTC connection, then they'd say, oh, our stream is the most important one. when there are so many things with the same priority, how do you handle the multi priority level priitization, I mean, Oh, so so so, basically, your application understand the priority for -- Application will say everything has highest priority then. Yeah. Then you go through the high high premium pass by but then you you you actually you increase your cost. Right? Because the bandwidth resource there is is is scarce. So a back end service provider, you don't have control what the application demands. bread,"
  },
  {
    "startTime": "01:54:04",
    "text": "So so so here here are the the the idea idea is like like that for example, if you have a server, right, so for us, like, the our server side and the the infrastructure side, we're together. So we actually have some knowledge about which stream, we should prioritize And and if you get that that information, yeah, sure. You can leverage this information to improve your performance. But if you don't, you just go back back to your default fall back to your default settings, settings, Okay. So like a rule based Friday engine, which is with the service provider. Yeah. Yeah. Yeah. I don't Okay. Spencer Dalkins Ted Hardy enthusiast. and also learning from the past enthusiasts. The path of our networking research group did a RC9049. that talks about obstacles to deployment, and I think if you do take Ted's advice about rethinking this at a fairly fundamental level. There are several that are listed in that RFC that I think apply here, and it's not it's not that they're for most of them. It's not that they're fatal but it's that you need to be aware of them and you need to think about we're gonna do this anyway, how will I overcome this obstacle because the obstacle is real? So I was the editor for that RFC I'd be happy to talk to anybody about about applying that to anything, but especially to you Okay. Thank you. And finally, Christian, I am a third party enthusiast I 2,"
  },
  {
    "startTime": "01:56:01",
    "text": "I mean, the basic view of the application is that the the network is a womb that is full of arrogators and middle boxes And you definitely don't want to tell the avigados in the middle boxes of your information is important because you're pretty sure that I mess it up. Now if you are going to do multicast there is a much simpler way to do what you describe. and it's basically assume that you have one path to an expensive network and you can define that path with a particular fault. then the only thing you have to do is add the past status To that pass, it says it's for priority stuff only. And then once you have set up the status to priority, it's up to the endpoints to decide which of the application data goes on that priority path or not? Okay. So it was over time, I think the discussion now is it's been in in useful for you. You know? the feedbacks. Yeah. I think you you can have a lot of discussions with some of the people and see maybe if Yeah. Take on both the feedback. button? Martin might we would have to keep this to kind of an FYI with that queue since we have set up time, but Martin is going to talk about another nice topic, which is Natural. Yeah. I I expect it to have 20 minutes for this. So 0 0 Try to be quick. Can I have slides, please? Lucas is clicking buttons, so we'll see. Yeah. Start talking. Okay. I'll I'll make it very quick. version 1 defines that in in in quick version 1, the service always public irritable. client might be behind on that. So we need to consider things like net rebindings, We want also defines how how a client can migrate"
  },
  {
    "startTime": "01:58:02",
    "text": "from from one path to to another, like, from a cellular interface to a Wi Fi and back Next slide, please. So that's that's quick. Ice, there's another RFC called ICE, 8 5, which can be used to to get through nets if you have 22 nodes that are behind behind that next slide, please. the way that works is So you have you have 2 ice calls with agents. basically as a client and a server. both of them gather candidates, meaning they cover what addresses they might be reachable at, then they use a syncing link server to exchange those address those those those candidates. they run a matching matching algorithm, to form pairs of candidates that might might be a path to commune a direct path to communicate. then ICE runs connectivity checks. it's called it's basically sending and retransmitting packets on those candidate pairs that go through the nets, puncture holes through the nets, and allow the packet from the other side. to make it through you will end up if you're lucky with 1 or multiple paths, So you need some logic to to to nominate the path that you'll actually be using. and then then you do some some key to large. So this is all handled by ice. The purpose of this draft, next slide, please, is to make it possible to use quick in a in a peer to peer setting. there's a lot of use cases for this. Like, one of it would be a building block for for WebRTC over quick, which will be discussed in other working groups. And there's there's a lot of other peer to peer protocols that this would be interesting for. Next slide, please. So my document defines 3 different modes, and I won't have the time to go through all of them. But the first one is their"
  },
  {
    "startTime": "02:00:02",
    "text": "basically just for completeness. It's it's a very, very simple you run the ice algorithm that I just They just told you about to completion. ice that tells you here is a four tuple that might work as a path you run a quick handshake on that path, and you end up with a direct connection between two peers. I'll skip the second mode and jump to the 3rd mode. Thanks. So when you look at the list of what what ICE does. you will see that there's a lot of stuff in there that could potentially be handled by quick first of all, the exchange of candidates between peers this could be done in a quick frame on a quick connection. Quick already defines how to how to probe path that could replace connectivity checks. any quick stack that that handles connection migration will have some logic of selecting one path out of the out of the multiple paths that are available, So that one is handled by quick as well as well. And then quick does keep a lives. So the idea is, like, why don't we just take use quick to do to do all of these things. Next slide. So in specific, we can use next slide. we could use a proxy quick connection. We could, for example, use connect UDP for that, but we don't have to. So we start with a proxy quick connection. we then do use the the the the algorithm described for a quick connection migration to probe the path. we need a slight slight change to RFC 1000 here because the server now also needs to send a pro packet to to create the net binding that allows the pro packet from the client to make it full. And then we use we can use connection migration to to move to"
  },
  {
    "startTime": "02:02:00",
    "text": "to move to that path. This is the advantage that the application can start start the application protocol right away on the on the proxy connection. and doesn't have to wait for the whole ICE procedure to complete before being able to exchange application data And then once once we have punched the hole, we can migrate to a better, like, direct connection. and continue the application protocol. Next slide. one thing that came up on on the list and on on the issues on on GitHub is does this require Quick Multipath? and the answer is Probably not. It's beneficial to use it, but we don't need it as a dependency because in in in both quick and multipass, it's the client that props the path a path. the server does not. The server on your response, where where quick v 1 and multipath differ is that you can send data on multiple paths. you're using multi path, but this is the the map doesn't care about it because it's it's all encrypted for UDP packets anyway. Next slide. So I'd like to see if there's interest for this work in in this working group. Obviously, this is a o o version. So there's a lot of work to do especially around the the the ice parts Thank you, Martin. And, also, it came up in the chat that there's related work being presented in a b t card by Peter Thatcher that it's it sounds like basically mode 1 that Martin briefly described here. So I think anyone interested in this work would be good to follow Martin and then maybe also attend ABT core. and that it concludes the quick room group meeting. We are over time. Thank you, everyone. Thank you, especially to our notetaker Watson. And Hope you all have a great rest of your day."
  }
]
