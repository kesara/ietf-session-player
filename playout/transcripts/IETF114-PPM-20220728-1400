[
  {
    "startTime": "00:00:32",
    "text": "uh so"
  },
  {
    "startTime": "00:03:40",
    "text": "welcome to privacy preserving measurement you've seen the um note well it's thursday i'm not gonna read it to you next um thanks i see i don't see any nostrils in the room i'd like to keep it that way if you need to take a sip of water put your mask back on afterwards"
  },
  {
    "startTime": "00:04:06",
    "text": "you know how to use meat echo make sure if you're using the full version here you don't turn on your microphone because we'll get feedback in the room and if you're speaking at the microphone keep your mask on um i think we're ready for siobhan shawn's gonna talk about star and then we're gonna move on to that yes i can just say next slide chris and chris are you in queue for agenda bashing [Music] uh yes i mean q to bash the agenda there was no opportunity to do so um i would i would like to request that this be moved to the end of the session so we give uh appropriate amount of time for all the dapper-related issues that we have um we need to make sure we get through those prior to discussing non-working group items at this time thank you the cheers have decided to do otherwise chris chris cotton"
  },
  {
    "startTime": "00:06:11",
    "text": "thank you i i for one i am remote i'm having trouble hearing the room it's very choppy is anyone else having trouble hearing oh all right um is it is it either you're having trouble with or siobhan would you say a word of that mike hello hello hello is that okay chris okay it's just me uh chris wood can you cover me if i am not able to uh to communicate with people um i'll i'll figure it out on my end i'll be i'll be back but um yep okay go ahead siobhan hey chris would i'll entry offline hello um my name is siobhan and um alex pete and i have been um talking about star for a while star is was a research paper um it's gonna come in the uh it's gonna appear in the upcoming ccs i think and um yeah looking forward to discussing with folks the central idea is that we would like to have k anonymity for clients reporting potentially sensitive measurements to an untrusted server um and the pretty important goal that we have is that it should be cheap because brave is a small organization and there are other similar small organizations who would like to do privacy preserving measurement but don't have um infinite aws um you know money to spend on that and so yeah so low computational overhead and network usage for clients and servers is pretty important it should i guess similarly it should also be easy to implement using well-known"
  },
  {
    "startTime": "00:08:00",
    "text": "cryptographic techniques um and obviously should also be private um the central idea here i won't go into too much detail i think alex's presentation at the last itf did a good job but essentially we're going to use jameer's secret sharing so you get a symmetric key through some deterministic function operation on the initial measurement and then you encrypt the measurement using that key and um so the idea now is that you can only get x um when you have key when you have k um so you generate a secret share of of k and you send the measurement and the secret share to the server and the server gets a bunch of these but it doesn't know what m is until it can perform the recovery operation um on k on the secret share of k and it can only do that once there are n of those so in this way you get like uh key anomaly where n is the number of uh minimum number of shares you need and then you can use k once you recover it to decrypt them to and get the original measurement back and it's really important to have an anonymizing proxy here um so i'll talk a little bit about this later on but essentially once the measurement is decrypted you want you want to be sure that you still don't have access to the the ip address of the people submitting this and also you have to use a randomness server um because the you could also do this locally but we decided that um it's just a lot better for privacy reasons if you use this so the idea is that you would the client sends a blinded input value to the randomness server and it gets assault back and if you have the same input value then all of those clients would get the same salt pack um and this is to do this is done to like mitigate the server brute forcing all possible input values because if the if the space for the"
  },
  {
    "startTime": "00:10:00",
    "text": "if the initial measurement is not except doesn't have enough entropy then the so the server can very easily brute force all possible values and then just see what the values match up with the encrypted value uh and then yeah we also use an opr to make sure that the randomness server does not learn the input value as well this is the architecture so there is a randomness phase like i mentioned where the client talks to the randomness server and gets back the random value that it needs to generate the message and then there's a key rotation and this is done so that when the aggregation server gets the message the randomness server has already already rotated its keys so the aggregation server can't ask the randomness server at this point um and yes and then the client sends the encrypted message over an anonymizing proxy to the aggregation server and then there's an aggregation phase where you can reveal the original message if it's sent by enough clients right now yeah we're all allowing only 10 minutes for this now so the questions are nice okay cool um so the implementation status is that we're shipping this in brave for some telemetry um there is an open source rust implementation with some razon bindings and we right now using a tcp proxy for the anonymizing proxy bit but we're experimenting with we're using an ohio one yeah and we made a bunch of changes given the feedback that we got at the last ietf on mailing list and through conversations so we don't use punctual uh opr apps anymore and um we just simply rotate the keys like i mentioned um so the randomness server the client can only sends the encrypted value to the aggregation server um in the subsequent ebook after the"
  },
  {
    "startTime": "00:12:01",
    "text": "randomness phase and yeah we also require the use of a randomness server um just to make i guess keep things simple and um you don't have to like then make that decision yourself oh should i do i need one do i not need one and also apart from that just a bunch of documentation of the risk of collusions um between various entities um and just a lot more like details on leakages and the security consideration sections and stuff like that and um yeah and we think it's ready for adoption um we spoke a bunch to chris wood and um he had some good comments and i think we will be making those comments but i think at this point it would be great if that's a working group um if the if the draft is like a working group thing that the working group works on instead of just a couple of people so yeah so happy to hear any comments about that thank you so um first question um how long do you imagine the epochs being is it again sorry how long do you envision the epochs being how long do we envision the airpods because now that you have because you've diverted the keys every so often right and you say you can't upload the data until after the so how long do you vision the epochs begin right so right now we um use it the the the time period we use is one day okay but that's what we imagine okay my second question is what happens if um if i as a client generate a bogus share what i mean is i generate a share that corresponds to a different encryption key right um yeah so this is something that chris would brought up as well and so you mean like when you finally decrypt it you get a value that is not the same as the other ones right well that's not but that's not what happens right no no no no no i generate a secret share that corresponds to a random value"
  },
  {
    "startTime": "00:14:01",
    "text": "so that when we construct something similar right any endpoints right allow you construct right and so imagine that imagine the amount like a modest civil attack or 20 so you need 500 so you need 500 500 shares and 10 of shares are bogus and correspond to and correspond so what happens then so but it is a secret share um you're trying to attack a particular well i want what i want is to make it impossible for you to construct a given value so like so so for instance like you're collecting the top urls right and i want to make it to impossible for you to impossible for you to collect the google.com on the top urls so what i do is i generate a description because i generate so i generate a thing that has the encrypted value for google.com but has a secret share that corresponds to a random point i think generally we are keeping civil attacks out of um out of scope because i think that's a common problem with like many systems but the one thing that we do that we were talking about with chris is that if on decryption like that's one aspect of it is that on decryption if you see a value that is different from everyone else then you you know i think you're misunderstanding me no one will be able to correct right i see that i think in general we will be not considering simple values for okay but this is like this is like the most trivial analysis attack in the world like basically like so so maybe they try to hit the point if like because of the way your super sharing works if any input value is bogus that put key is bogus right so if i can convince you to take as one of the quorum one of the quorum of n keys the user construct a point of mine and my point is bogus the consequence will be the key is bogus and and and now you can't charge any of the values because you're because your corresponds to a random key and so the aad fails but you still wouldn't be able to you would still be waiting for enough number of no no no no because so if i understand it you inventory the you sort the keys so you sort the inputs on the encrypted value right good so you bucket them up on encrypted value you take the corresponding sql shares right now you can start the key and now you get a key and you try to"
  },
  {
    "startTime": "00:16:00",
    "text": "decrypt them and none of the values will decrypt and so because if you consider one key you've considered a random key right so you've got to say so you've got to find somebody to reject the bogus key the book is inputs right and so if i remember correctly there was some technique that apple used in their um in their in their system system for this but like i don't know how to do it with this design so um so i think this actually doesn't answer for this i think it's not just i mean so civil i mean like it's like one thing to be like we don't have grand civil attacks but like the situation where you know the situation is that like if i can get any small fraction of the keys that you can't decrypt um that's like actually very serious so i think you think you need a way to reject bad input shares okay um yeah i got to think more about that um but did you have another i don't know i don't know i don't know how to fix it um i think as i i said i said um my pleasure would be that there was a um that in the um in the binet um you know um lester meyers apple ccm thing they had a secret share a secret shimmer secret share scheme and they had some way to reject broker's values so maybe you can steal that maybe you can't yeah the queue is closed and uh alex it's your uh you're the last speaker hey uh can you hear me okay yes alex yeah uh yeah i'm alex i'm one of the authors of the star draft a co-author of the star draft so in answer to eric's question that's what we do in brave is we take the threshold a threshold number the threshold number of ship the minimum threshold number of shares and tries to reconstruct and then if we can't reconstruct we keep taking random sets of shares so that's like one thing that we don't work if there's any substantial fractions of bogus uh no say your threshold see a threshold is zero threshold is 500 shares right and one percent of the shares are bogus what is what is the chance that not what is the chance that 99"
  },
  {
    "startTime": "00:18:00",
    "text": "so 0.99 to the 500 is like an incredibly small number so like it doesn't work it doesn't it doesn't work it can't attack with any kind of power agreed but the adept secret sharing framework that we currently have has like a mechanism for deciding whether shares are authenticated or not and that's the thing that we use dedicated there's mechanisms in the adept secret showing framework to decide whether shares are valid or not and whether they agree with a certain policy okay i i i i do have to write this down because like i'm sorry i i think the uh thanks for thanks for uh highlighting that point uh i think we're out of time we need to move to the next speaker which is tim thanks for mentioning the the question of adoption uh we we might come back to that if there happens to be time at the end of the session uh chris uh is uh i see tim is in the in the q2 tim this is your uh presentation do you want to present slides no go ahead if you have them uh oh i see a slide request i'm happy to approve that here we go okay let's see and i got buttons all right great let's get started uh okay so um so in this uh particular deck we're going to cover uh the current status of implementations of jp then i want to talk about a small number of the notable changes in the most recent draft 001 of bap and then we're going to use that to segue into the discussion of a couple of"
  },
  {
    "startTime": "00:20:01",
    "text": "one or two open problems that the chairs have encountered and that we're interested in discussing in the working group before i move on i just want to check can the room hear me everyone can hear me okay chairs we hear you clearly on site sounds good all right so implementation status so as of right now we have two implementations of draft ietf ppm dap01 uh that are up on github uh first there's daphne which implements a dap leader helper and a collector and is written in pure rust then yanus is another implementation of dap server components uh also written in pure rust um so dafty and giannis are independent implementations of dap though they do share some common dependencies which we'll get to in a minute and finally there's a divi up ts which is a client so it only has enough of the protocol and cryptography bits just to do report uploads uh and it's written mostly in pure typescript although some of the cryptography dependencies are transpiled from rust uh so as chris patton would have explained hedigon before me um so the dnp protocol is defined in terms of vdaf which is a verifiable distributed aggregation functions uh so that's being standardized through the cfrg um and draft zero two of that just dropped a few weeks ago now uh so we have an implementation of vdaf draft zero one um in librio rs which is up on github and also published as create prio on create.io um the significant thing that's missing from librio in order to be an implementation of vda f02 is an efficient um which is to say scalable implementation of popular one which we don't currently have but we're hoping to get to in the coming months uh so as i mentioned before daphne and giannis are independent implementations of dap but they both use libprio to implement um vdafs so it certainly would be nice to see more implementations and especially ones"
  },
  {
    "startTime": "00:22:01",
    "text": "in um some languages besides rust then you want there if anyone out there is interesting so yeah all this stuff is up on github uh please you know go check it out uh maybe deploy them let us know how it works out for you uh so we have been doing uh some measure of manual testing of interoperability uh between daphne and giannis which has gone all right and going forward uh we're looking at designing an interoperability test framework inspired by the quick interrupt runner uh the uh the aim of which is uh you know in a nutshell you could take a dp implementation stick it inside a docker container and then besides the endpoints uh specified by the protocol you would have um a handful of extra sort of control endpoints that allow the automated um setup and execution of interoperability tests so the idea being that uh hopefully at some point we'll have some tests running in like a continuous integration setup somewhere that uh you know gives us like ongoing results about whether implementations are working and can talk to each other okay so that's it for implementation status and let's look at some stuff interesting stuff that's changed first up there's a course grain report timestamps so in uh dap a report gets uniquely identified by its knots and a nonce consists of the time at which the measurement was taken and then a random component that's intended to make the nazis unique so nonces have to be unique uh because they are used for anti-replay by the aggregators and they are time stamped so that the aggregators can decide whether a given report falls into a particular batch interval um so up until draft zero one the notch was the number of seconds since the unix epic and then eight random bikes as it turns out that timestamp is high enough resolution to leak some meaningful information about the client so as of the most recent draft we now expect clients to round the timestamp down to the minimum batch duration which is one of the long lived task parameters and we widen the random component to 16 bytes so this allows a task to tune how identifying a time stamp is because"
  },
  {
    "startTime": "00:24:00",
    "text": "bigger mid-batch duration takes entropy out of the timestamp but we can still ensure that noises are unique and meet the requirements so you can take a look at the linked issue and pr the ones linked on the slide and the chairs want to thank shan wyang for championing this great idea moving on uh but okay next big idea is aggregation jobs so first uh let's recall what the aggregation sub protocol is about and i suppose here i should take a brief parenthesis to note that um dfp uh consists of three sub-protocols upload where clients are transmitting reports to the aggregators then aggregate where the aggregators jointly prepare prepare inputs and aggregate them and finally collect where the aggregate shares are transmitted uh to the collector so that it can get the eventual aggregate result uh chris patton is going to get a into this a little bit more uh in just a few more minutes uh okay turning back to the aggregation sub protocol let's just unpack a bit more what it actually does so at some point uh aggregators are going to be holding some large number of input chairs that have been uploaded by clients and i mean they want to aggregate them together but in the taxonomy of dap you can't actually aggregate input shares you first have to obtain an output share an output share that is from each input share and that process of going from input to output share is what vdaf refers to as preparation so preparation is generally going to require multiple rounds of interaction between the aggregators exactly how many depends on the vdaf in use and that process is stateful so what actually happens when we prepare an input into an output depends on the vdaf once again so in the prio family of vdafs we are only evaluating the zero-knowledge proofs of the input's validity and no actual transformation occurs from input to output so it'd be tempting then to call this process verification or proof evaluation but we"
  },
  {
    "startTime": "00:26:01",
    "text": "know that in vdas like poplar one or other vdafs that have yet to be specified there will be some meaningful transformation such that the input and output don't have the same type hence we end up with the somewhat unhelpfully generic term of preparation but one thing that we do expect is going to hold across all or most vdafs is that preparation is embarrassingly parallel since verifying the proof of one input's validity should generally be completely independent from another so we want to enable the leader to be able to schedule the preparation of lots and lots of inputs in parallel for efficiency and this is why we introduced this notion of the aggregation job into the protocol okay so as we discussed at some point the leader is going to want to schedule the preparation of a big set of shares uh in the prio family vdafs preparation can begin as soon as the aggregators receive inputs from the clients because there isn't an aggregation parameter uh in those vdafs so maybe in that setting the leader every time it receives a thousand inputs uh it'll dispatch an aggregation job right but in something like poplar one you can't do preparation until you get an aggregation parameter from the collector so there maybe the collector will have gathered 10 million client inputs by the time it gets the aggregation parameter and at that point it's going to want to schedule say a thousand parallel aggregation jobs to get the collector the aggregate results sorry aggregate shares as quickly as possible so either way what the leader will do is generate random aggregation job ids and assign a set of reports to each job that mapping one job id to many shares gets transmitted to the uh to the helper in the aggregate initialize request illustrated here on the slide and that job id is going to be referenced in subsequent messages in the aggregate protocol um right so then the helper can use the job id to index into its own storage to fetch the state and execute the next step of edaf preparation uh so this enables many helpers to work in parallel provided they can share some storage like you know a database or key value store or what have you"
  },
  {
    "startTime": "00:28:01",
    "text": "uh and the other virtue of the scheme is that since the job ids aren't secret um the they excuse me the job ids aren't secret and uh neither do they need any uh complicated anti-replay protections because all the sensitive state is in uh either aggregator's trusted data store um so again you can check out the linked issue and pull request on the slide if you want to learn more about like the context behind this change all right moving on uh now let's discuss how the aggregators authenticate to each other in this aggregate sub protocol that we've just been discussing so in in dp aggregation is coordinated by the leader aggregator though actually in the aggregate sub protocol the leader is acting as a client to the helper's http server now this channel between the two aggregators has to be mutually authenticated uh to prevent network attackers from impersonating either aggregator we might assume that they talk over tls which helps to an extent with server authentication uh but we also need client auth here so what's in the spec now as of the pull request linked in the slide uh is a requirement that the leader has to set a um a bearer token under this uh dap auth token header that we invented in the request it makes and the value of that token is a secret pre-negotiated between the aggregators before the start of the protocol now we did this because it enables the deployments that we have in mind right now but this isn't really a workable solution for the protocol uh so first off you know long-term shared secrets between the participants is not desirable we should do our best to avoid that of course um and more to the point because it's such a specific prescription uh it makes it impossible for deployments to use any number of existing well-established offense or offset mechanisms uh used widely used in http apis so this in particular is something that we definitely want to change in a future draft but at this point we should take a step back from the specific interaction between the two aggregators uh and look more broadly at how protocol participants are authenticating to each"
  },
  {
    "startTime": "00:30:00",
    "text": "other when they communicate in dap um and that's we have a summary in this slide here so um so right so here we have the different uh communication interactions between uh dap participants so in the first row we're looking at uh the uh how the client uh interacts with the aggregators to upload its uh input shares so uh in this case the design requirements are confidentiality which we achieve by having uh the client encrypt either input share to an hpk public key advertised by either aggregator of course this is necessary because if a network observer could see the input shares in the clear then that would defeat all the privacy goals of the protocol what are the colors maintain the colors are intended to match up the requirements to the specified mechanism that satisfies the requirement and then red highlights uh just like stuff that's going to change um hence the motivation for discussing this with the working group today um okay where was i right confidentiality uh right so we protect the input shares uh in flight by http encrypting them to a public key advertised by either aggregator okay then of course this has to be uh we need server authentication in this setting um because we want to make sure that the input chairs are being transmitted to the act the authentic aggregators participating in a dap deployment uh so we do this by having the client um fetch vhpke configuration that it's going to encrypt you over tls so that the server identity can be verified that way um excuse me where was i all right finally uh we have the dap allows but does not require the client to authenticate to the aggregators um so it's tempting to require klein off as a mitigation for civil attacks and in those deployments where this is possible that's going to be extremely effective but but it's not going to be the case that every deployment will be um will be able"
  },
  {
    "startTime": "00:32:00",
    "text": "to have a meaningful client side identity uh with which you could authenticate so for that reason um we don't want to require it in all cases some deployments are are going to have to allow on authenticated input uh input uploads and we'll have to figure out some other means of mitigating civil attacks um we we also i in my view don't want to specify how a deployment would do client authentication if it chose to which i'm going to come back to all right the next row is um the communication between the leader and the helper during the aggregate sub protocol which we just covered so i don't want to spend a ton of time on it again but yeah confidentiality is generally cheap because they are communicating over tls and mutual authentication through that um through this current pre-negotiated barrier token scheme and the server's tls certificate uh and finally we have the interaction between the collector and either aggregator uh to deliver aggregate shares at the at the end of the whole protocol during the during the collect sub protocol so these aren't exactly the same um because while the while the collector makes direct http requests to the leader um it never actually talks directly to the helper the communication between collector and helper is uh tunneled through the leader which coordinates the collect and aggregate protocols so we achieve confidentiality in both cases um here here i mean the transmission of aggregate shares from either aggregator to the collector uh by having the aggregate shares hbk encrypted to a public key advertised by the collector but we don't currently have a good story for how the collector is meant to authenticate to the leader uh what's in the text right now is the same uh bearer token scheme as between the aggregators and we currently have nothing to specify uh whether or how uh the collector and helper should authenticate to each other which is necessary in the one direction because we don't want the leader to be able to present forged collect request parameters to the helper and in the other direction because we'd like the collector to have confidence that it's receiving an aggregate chair from the"
  },
  {
    "startTime": "00:34:00",
    "text": "authentic helper aggregator okay so clearly we have a bunch of cases where what we do say about authentication needs to change and others where we say nothing at all and maybe we should um oh excuse me before i move on i forgot one interesting piece of red text in the slide which is in the collector helper case as it turns out both of those actors already advertise in http configuration and public key so maybe that uh the problem of mutual authentication there could be solved by using hpke's uh mutual authentication mode okay where was i right so clearly um we have some inconsistent guidance and some missing recommendations about authentication in this protocol uh so the question here broadly is what should dap say about request or response authentication so to advance the strawman claim and stimulate some discussion i'm going to claim that as much as possible we should say nothing and stick to enumerating requirements for the security of the channels rather than solutions dap is built on top of http and one of the virtues of that is that we get to rely on existing mechanisms and implementations for all sorts of things like caching uh error handling or authentication so in my view we should be aiming for composability with existing authentication schemes widely deployed with http apis with an eye towards sort of integrating nicely with with the schemes already deployed by vendors who might want to operate uh dap servers so stuff like aws request signatures oauth2 or even tls client certs which a lot of people do use for authentication um now of course the exception there is the cases that i discussed where we mandate the use excuse me where dap mandates the use of hpke the distinction to keep in mind there is that we mandate that in those cases where we're channeling a secure channel through some protocol participant and so in those cases we can't rely on an under excuse me on a security property of an underlying transport all right so that brings me to some of the goals that the editors have for the next draft of dap some but not all we're going to discuss"
  },
  {
    "startTime": "00:36:00",
    "text": "other goals in them in a couple of later presentations so continuing from the topic of like good use of http um we're thinking about uh rewriting the http the api mandated by dap to be a little more resource oriented if not full-on restful so for instance instead of the upload endpoint uh being just upload with all the meaningful parameters being encoded into the body of the request you might have a path where you have like tasks and then encode a task identifier and then encode a report identifier you know into into the uri that you're uploading to um we're also interested in looking at the relevant best current practices documents and aligning with their guidance where it makes sense uh for instance uh to get to make use of better http semantics and maybe doing something like extending the http config and point into something like acme's api directory uh and as we were just discussing or interested in revisiting what the requirements are for authentication and what if any prescriptions we make um okay so that's it for me we're looking forward to discussing all these topics in the working group here today and uh you know in the coming weeks and months on the mailing list and so on tim can you go back um or something back to a few slides to this authentication point thank you nope but next so knowing what i know now which is not much um um well i know i know how this document works but i mean more generally um the um this seems like a good approach um but i think perhaps we should do is reach out to the hdp api working group because they are specifying best practices for this and i think they can like this is just like straight up http web app right our http web service and so i think we should take their guidance on how we do this which i think would quite likely be this but i think we should get their guidance on that rather than reinventing the wheel so i don't know who will be responsible for that is um you know i suppose we could do it privately and i said not but um uh that would be my recommendation for this um as i said i think these are like i think you're i think that your your intuition here that like"
  },
  {
    "startTime": "00:38:01",
    "text": "people are going to have their own mechanisms and we don't want to interfere with those like i think it's entirely entirely correct um i think that's also true for the for the next thing you said about like the acme you know um the you know um the directory and stuff like that those are also questions which like that one might help us with um so starting our recommendation yeah yeah i agree eric thank you i think we should also talk to some of the uh prominent operators of acme i i know a couple of them um to see what their experience has been with like this you know acme specifically mandates the use of jwts and the directory um and like i know the people who run let's encrypt have opinions about those things i'd love to hear from other acme operators how they've what their experience of that has been chris patton yeah i i just wanted to mention i'm a little a little apprehensive about leaning on hpke for mutual authentication um i kind of see the motivation though because we have basically this collect request from the lecturer goes to the it goes through the helper via the leader and if the leader is is attacking privacy then this is this is a problem um you should also stipulate though that the collector is also part of the threat model so privacy should hold as long as one aggregator's honest um that said uh i think i think off the authentication would be useful ecker once suggested that like we you know change the protocol shape so that the collector communicates directly with the helper that might help so i just wanted to suggest that as a possible alternative yes yes i think that's a good point chris um and yeah and on the topic of direct communication either aggregator that's also something we've been batting around in this on the upload side of the protocol right like at the moment um the way uploads work is that the client"
  },
  {
    "startTime": "00:40:00",
    "text": "sends one message sorry chris we can hear you we can hear your typing you wouldn't um doing my muting thank you um at the moment clients will create one message that contains both input shares transmit that to the leader and the leader is responsible for relaying the helper's chair to the helper uh so this has some problems like the the the main problem with that which i think we discussed the last ietf is that it means a leader may incur like significant uh costs for network egress uh but yeah but it also forces us to deal with like this tunnel channel through the leader so yeah i think there's a few things that are still like that have the potential to change right about whether we introduce direct communication between protocol participants which might significantly change how we approach these authentication questions rich saul's in this case http api co-chair um it'd be great to come and ask for advice we have done no work in this area so it'll be like asking economic economics people you know you'll get 10 people giving you 11 opinions um i i uh want to reinforce the concerns tim that you're raising about the dependence of this on uh uh proper behavior of some of the centralized players in particular the leader has worried me um with this design you know the goal here is to make it so that as i think chris said it should be the privacy should be preserved as long as there's one aggregator who's playing fair and i worry that the leader has a tremendous amount of control here um and could potentially uh de-anonymize uh or remove the privacy protections"
  },
  {
    "startTime": "00:42:00",
    "text": "based on it being capable of uh just controlling which messages get rounded where both from the collector to the other helpers um and from the reporter uh to the collectors so i i i would appreciate more thought about distributing the routing itself message driver yeah i agree this is why i highlighted the problem of a collector the collect request being authenticated all the way through to the helper um otherwise we do have a threat model in the back of the document that tries to enumerate like what exactly the leader can do but the helper can't i think it's out of date though and it certainly i think it needs some attention so on overscroll um if we do have that problem however um if the protocol is lines away the leader can't leader can independently break the privacy protocol then then there's a protocol design failure because the um because the collector if you take the collector and leader and you split them apart the collector talks directly to the helper and then the collector closes the leader you're back in the soup so the protocol must resist that must resist it must have but really designer design was that neither case so i actually don't believe it's like so like so while i'm open to having the open to having the collector talk directly to the helpers um i do not believe they address the problem the gtg is addressing um so um one thing that uh sorry i'm finding something there's a bunch of backup machine guys we have the secretariat on it great okay um so um with that said uh i i think i'm not not averse to having the um collectors directly to like the uh um the helpers um the uh uh you know in our implementation um you know we looked we literally like to send to um both helpers independently um and that made us pretty sad so i think if we do decide to do that we have to have a mechanism"
  },
  {
    "startTime": "00:44:00",
    "text": "that also allows you to have a an ingest server um because like otherwise there are all kinds of problems where like we send like only one chair not the other and you could deal with that so um i think there's less of an issue for the helper though it is like a lot of burden on the helper to like you know make it happen sorry i'm the collector how to make the logic of like what if this guy's name this guy doesn't answer but it's obviously much more important on a plan [Music] so i also want to echo some of uh dkg's concerns um but separately i noticed in the draft that you specifically say that uh only one collector is or sorry only one helper is supported is that still the case um because it i did not pick up on any real blockers for that but i'm curious what's providing that thanks all right so my understanding is that there is nothing in like the underlying uh crypto constructions which is to say the vdas precludes additional helpers although chris patton is about to because i think that's not very popular but in prior you can have arbitrarily many helpers um dap kind of makes the soft assumption but there's uh exactly one helper though um we're a little inconsistent i think throughout the draft about whether there's exactly would help or not but yeah so in my view there's a trade-off between if you add more helpers you get in some sense more privacy because more actors have to collude to defeat uh the privacy of the protocol but that's a trade-off against like the complexity resulting complexity of the protocol because you have so many more actors um to coordinate uh so i think we're where we're at at the moment i suppose when i say we just mean the editors of the document um is that exactly what helper [Music] is yeah i think that's where we're at common"
  },
  {
    "startTime": "00:46:04",
    "text": "if anyone's ever played that video game um okay uh so yeah um just to echo tim's point um right now we don't support more than one helper however we uh we intended to design the protocol in a way that we can we could go in that direction if that's what people wanted to do um one liter one helper is kind of the simplest thing it adds protocol complexity uh to add additional helpers but i don't think that complexity uh is impossible to address um so if folks want to um add support for more for more uh aggregators i think we can do it um i wanted to go back to dkg's point um collector to helper authentication doesn't have anything to do with the power the extra power that the leader has the extra power that the leader has has to do with civil attacks because the leader gets to pick the set of reports that are aggregated um we don't have a generic we don't have a generic defense for civil attacks uh that would probably be pretty hard but something definitely we should we should uh find solutions for um i wanted to point out that the helper also has can do sybil attacks by anyone who can upload reports to the leader can can mount a civil attack they have to collude with the collector um because the uh as tim pointed out the aggregate shares are encrypted under the uh collector's public key so as long as one server's honest they don't they don't actually the attacker doesn't see the result um but we want you know we want to be able to deal with the case where the collector is malicious um so because reports are unauthenticated there's uh anyone can do a civil attack and i think the leader's relative strength is kind of minor uh and i would i i would like to see defenses be more generic uh and not just apply to that particular"
  },
  {
    "startTime": "00:48:00",
    "text": "situation nick nick you're on here um i i wanted to go back to the request authentication unless unless other people have an immediate reply to that okay um i i would be concerned about not specifying or only specifying requirements not specifying um an effective way to do uh request authentication um uh apologies i didn't introduce myself at nick dodie center for democracy and technology um my concern would be that uh that there would be like an extraordinary or like silent spec for actually enabling interoperability between clients and uh servers and we would like to make that easier and so it would be good if we had a sort of recommended way do that um even if yes there are going to be cases where someone would deploy this with their own custom authentication scheme thanks chris wood um on on the topic of uh one helper versus multiple helper i'm sorry to keep bouncing back and forth between different things um i wouldn't be surprised if we find out that popular in practice is just like too expensive to run given how many rounds it requires for every single bit of input that you're actually that you actually want to aggregate um so uh i don't know what that says about the fate of the dap as a you know generic thing for all vdos versus dap as a prio specific or uh bus or whatever specific protocol but i i could see a future wherein dap kind of gets less general more specific to prio and maybe a heavy hitters like solution gets"
  },
  {
    "startTime": "00:50:02",
    "text": "it's its own thing maybe that star maybe that's something else but um if that were the case then accommodating multiple helpers would be um rather straightforward and dap for prio um but uh if it's like super general and we have popular with its constraint that it only works with one particular helper um it's it's not clear like what the the result of in terms of complexity would be on the protocol but i just wanted to note that like we're not set in stone here we might see that things get less general or not as we go forward i think uh so on the topic of the leaders uh power and and control over over the system can you just clarify just so i i know i i'm understanding it so the leader has the ability to uh reject shares and therefore they are never processed by the helper and therefore a colluding leader and collector can basically single out individual um uh uploads is that correct uh yeah but then you're going to select yeah which shares get paired like as we saw earlier right it's good to assign a reporter to job ids however uh the helper also can do this on a per share basis because um the responsibility helper delivers to a leader's aggregation request is going to include like a list of um essentially a list of like per input preparation messages so the helper could simply choose to like fail to prepare any individual input right so i'm not saying that that's good my point is that in this respect the leader i don't think has a power that the helper doesn't um the other half-baked thought i have in response to that question is that i think injection servers anonymize congestion servers uh are probably a"
  },
  {
    "startTime": "00:52:02",
    "text": "helpful mitigation here right in that one of the issues is that um if you have a in a deployment where clients are uploading directly to a leader uh the leader gets to see all sorts like interesting metadata about a report you know client ip stuff like that um on which basis he could choose to to to drop reports so we anticipate that a lot of deployments are going to use some kind of intervening ingestion server um hopefully you could just like stick an ohio server in front of um in front of the leader such that the leader uh i don't want to say to be impossible but certainly it ought to be harder for it to be able to selectively drop reports i i think that's the last word on this topic and we're going to move to the next presentation chris patton good morning everybody all right um so uh you planned for me to go first i was going to start with kind of an overview of hold on let me turn off this there we go um we were going to start with an overview of the dap protocol i'm curious if people would find that useful at this point chairs can you can you basically just tell me if i should uh do like a four like four to five minute overview of dap thumbs up uh some slavery [Music] four or five minutes ago yes all right cool um well so the so what this what this talk is going to be about is about how people use dap um recently since adoption um a couple of use cases that have come up that we don't support very well so what i want"
  },
  {
    "startTime": "00:54:00",
    "text": "to ask the room about is what protocol changes um should be made if any to accommodate these use cases um and as i'm talking i think it would be helpful if folks would sort of think about how they intend to use dap and whether the protocol really suits their their needs so as tim mentioned um dap centers around a particular class of multi-party computation schemes that we call vdfs these all have basically the same shape we have a large number of clients each with the measurement clients split their measurements into what we call input shares and upload these to a small number of aggregation servers the aggregation servers interact with one another in order to verify and aggregate the reports at the end of this process each computes a share of the aggregate results then later on a collector comes along and pulls aggregate shares from the aggregators and computes the final result um so yeah uh vdas are being worked on in the cfrg uh there is a link to the document there if you'd like to learn more but this broadly covers things like prio poplar and other schemes um that we've we found in the literature and we hope um this becomes a target for cryptographers to go design solutions for uh the problems that we have in this working group so what dap is basically is a way for executing a vdf over http and as tim mentioned we have we sort of think of this as three three protocols in one all of which are being executed simultaneously so the first is uh the upload protocol clients uh uh take their measurement generate input shares and then um upload these to one of the the aggregators the leader in a report the input shares are encrypted under the public key of each of the aggregators in order in order to protect them and at the same time the aggregators are aggregating reports"
  },
  {
    "startTime": "00:56:02",
    "text": "leader whose gets all the reports it picks some set um and takes the uh pulls out the encrypted input shares of the helper and sends these to the helper in a a an http request and then after some number of rounds um they have uh computed aggregate shares for the set of reports they were able to verify and finally we have the uh eventually the collector comes along and grabs data um it does so by sending this thing called a collect request and um this is where the uh uh yeah so um the uh the in general the leader is not prepared to respond to a collect request right away um in general it has to interact with the helper first in order to compute the correct aggregate shares to return to the uh collector so uh what it does immediately is it and sends the the leader sends to the collector this uri that the uh collector can pull later on in order to get the result so that's kind of the the shape of the thing that we're working on um the problem i want to talk about is uh this how do we choose how do we choose a set of reports to aggregate um and when you think about it the most basic requirement for this is well the batch of reports needs to be sufficiently large that um the measurements the set of measurements remain private um and what this means is kind of application dependent but at least intuitively the larger the batch the more privacy that you get but think about this from a sort of a usability perspective what are the expectations of the collector who's grabbing data this basic requirement doesn't say anything about like whether reports should have anything to do with one another um so how should we how should how do we"
  },
  {
    "startTime": "00:58:00",
    "text": "like kind of automate the process of of grouping reports um so uh here's what we do today today um basically uh reports are assigned to a time window a discrete time window based on a time stamp that's generated but generated by the client and included in the report so um what a collect request specifies a batch interval which determines a sequence of time windows and what the collector expects is that the reports aggregated all fall into one of these time windows um now we have certain restrictions on the on on batch on batch intervals on the one hand this is about like operational stuff like we want it to be possible for uh both aggregators to be able to efficiently pre-compute aggregate shares in advance of getting a collect request and also there's privacy considerations here basically what we what we what we what we do what we say today is that batch intervals must not overlap in order to avoid leaking uh small batches um and then chris wood is going to get into this problem a little bit more in the next talk um but the problem we're working on right here is um there are a couple of use cases that this this scheme doesn't support very well so for starters you might want to select a batch based on some client property um so uh this was brought up in in issue 183 um basically reports uh you might what the collector might want is that the reports are grouped by say user agent or location um so the collector would specify some predicate that defines a set of reports uh that go in the batch uh basically the properties of reports that go in the batch um and you can imagine this this could be quite simple like give me the aggregate for all chrome users or all safari users or a little bit more"
  },
  {
    "startTime": "01:00:02",
    "text": "complicated like give me the aggregate for all chrome users in the us or or all firefox users that aren't in canada or something like that and um yeah so the problem that is the problem though is that even a very simple version of this kind of grouping strategy is not well supported in the protocol you can kind of hack around it but we don't expect that uh any solution that we have today uh will scale very well now uh issue 273 brought up even what's arguably a simpler use case um maybe you actually don't care that reports have anything to do with each other um maybe what you need basically is that reports are the the batches are disjoint and that they all have the same size or at least approximately the same size and this is this is important yeah yeah go ahead do you think it will help to have discussion on each of these piecemeal or do you want to finish and um i think it would i think it would be good to uh finish because i want to i want to sort of talk about the generalization of this um but we can talk about these i mean i guess up to you go ahead okay um so so fixed size batches are useful for like you know a statistical analysis where you need to control the sample size um and for applications that want to compose dap with differential privacy this is also going to be important for tuning noise um and then there's also uh you know waiting for the current time endo to expire before you compute in aggregate can add latency uh to the system that might not actually be uh sort of necessary um so this sort of fixed size chunks uh use case which was brought up in this issue is not supported at all today because the collector has no control over the batch size beyond specifying the minimum"
  },
  {
    "startTime": "01:02:02",
    "text": "um okay so um all right so uh where we are we think is that we need more flexibility the question is how much um and we need to stipulate the fact that collectors in dap are going to be more constrained than uh in a more in a traditional database or telemetry system um and this has to do with some privacy privacy issues uh which chris will talk about in the next presentation uh lots of open questions there but even from like a functional perspective uh we need to figure out what we need uh one question is what are all the query types so i've talked about three here um basically this time series things that we have today uh grouping by client properties um or or or partitioning things into fixed-sized chunks uh what else do we need another question is uh do we need to be able to compose different query types um this can get quite complicated i imagine not all query types would necessarily compose and finally would every dapp deployment need to implement all query types or is this something that we can allow folks to implement incrementally or not at all so um yeah so i i guess i'll leave this slide up for the discussion i can also go back and forth as needed um my proposal for draft two would be to take uh an incremental step that is minimal but is sufficient for our use cases and i think this would involve enumerating all the possible query types that we want to support in a way that's extensible um uh and then i you know i would add some additional requirements to this um basically the idea would be that the collector would include in its collect request a query uh that the leader would use to um choose a batch of reports that satisfy that query um will uh there are some additional"
  },
  {
    "startTime": "01:04:01",
    "text": "requirements here to think about um uh yeah so my question i guess for the room is uh does a protocol change that satisfies this these requirements uh fit your use case do you think we need something else and i see eckerd's in the queue uh in person yep go ahead i am impacting person not a sybil so do not believe well i want to just talk this over before i start saying what i think we got to do um so i guess i want to make two observations um one i know what's going to talk about the pro but the privacy implications um but i think we're already like kind of out of the zone where we can plausibly make um privacy assertions so like the the the property that's nice about the current design is that you can look at that look at the possible and queries these trips don't work the possible outputs and draw conclusions about the privacy properties of the system right you got some key and enemy conclusion you draw some like conclusion about like interception attacks like you can just conclude it's safe or unsafe but like you can just analyze it right um and so i suspect that the minute we get to the point where so and the reason you get the properties because you guarantee only process one each each input submission once and so as soon as you get out of that mode and you allow any other structure which i think we're going to likely have which i think it's not quite clear to me that this allows but if it doesn't but i mean sorry you know although screwing around we've done is that trying to make that single processing requirement easier to implement on the helper run later right um and so um as soon as you get out of that mode and you say you can make one query multiple queries um on the same submission which is not quite creative this allows but like it implicitly might allow um and and certainly we're much harder to implement um um you know if it doesn't allow um then like the situation's much more complicated wise and then some other way to think about it um i'm not saying you can't generalize it but i'm saying that trying to analyze what the political"
  },
  {
    "startTime": "01:06:01",
    "text": "commitment doesn't permit it will be almost impossible you have policy construction instead um to look at a different example and so that's like the first thing i want to say um look at a different example the way ipa works on the meta missile proposal for interoperable private uh aggregation um the uh instead of having the um the the selection of the submissions is entirely within the the unit of the effectively what's in this case the collector namely that the collector collects all the submissions and then then shoves them into the in in the helper for analysis right um and so we can do anything or anything anything it wants uh modulo whatever mechanisms are provided for privacy and so um what i wonder is whether or not that kind of design even not that about that specific design is what we want here um in particular allowing essentially instead of trying to create some language here um that is sort of like uh restricted for what you can say i wonder if we want something like much more fancier instead right um and so what i need a fancier is effectively to say well um the collector's like any subset it wants to be any mechanism at once and um uh and then they usually analyze and we have some other mechanism for ensuring privacy into those conditions um but anything could be i'm not sure quite sure but to say once we have any kind of query methods that allows overlapping queries we're already in the soup and we're going to have a flexibility question analysis so um so like here's like my dumb version of this um which is effectively that the um that the collector gets to upload a piece of javascript that the the leader and help the coverage execute the term whether it'll include a given section um another version of that would be for the leader and helper to um provide the collector with the entire inventory every possible submission and and the collectors simply say aggregate these ones these ones these ones right um and so the reason why the reason the reason i'm saying this is not it's not make the"
  },
  {
    "startTime": "01:08:00",
    "text": "problem harder but to make it easier um and to sort admit the fact like admit the fact that we already are sort of like off the fairway and try to solve the problem on the far end of the fairway instead of trying to solve my immersion camera here so um one thing one thing i'd point out though um [Music] so this this this fixed size chunk use case um here's kind of it's it's it's it's simple in that um there's no reason to ever have overlapping batches what we want is every chunk is disjoint um i would love to be able to support something like this that is simple and already kind of constrained but i like your point though is well taken like whatever we do here i think at a minimum we can try to uh prevent overlapping batches um like that i think that could always be defined um although with like the grouping thing like the what this is what this is kind of about is i want to i want to explore the data yeah yeah well well they could be overlapping because uh the intersection between uh chrome users and everybody else that would be an overlapping batch might tell you something about everybody okay that's what i'm saying that's what i'm saying yeah so i think so i think like one so again like i don't know um i guess what i'm saying is like instead of having like we initially designed this the kind of idea was we designed a grammar that would basically not let you say things that were illegal right um and the grammar like it wasn't even really a query grammar just like this is how it works right and so um and it was and that grammar inherently enforced one query per match right one career submission and so i think we need to go back and say what's the underlying rule that guarantees guarantees privacy and then let's implement that rule on the helpers and uh and not worry about like and not trying to minimize state on the helpers"
  },
  {
    "startTime": "01:10:00",
    "text": "and leaders and just be like this rule and then within the limits of that rule you can do any queries any query pattern you want and we won't try and we'll try to shift that so that's probably a better way to go and so like so if that means if we say right now the answer is one query per submission then you like a counter on every submission and like you know and i don't know about one query right and these are some other rules and some other rule but i think like and and then i think that what basically says like and then i think the question is what's the most the cheapest way to allow arbitrary queries um and not design a whole new language for that um rather than trying to design that language for that so now we might put on this on this i think dkg is probably going to say like the answer would be to live no queries in the problem we were simpler we're going to close the queue on this topic soon so if you want to hop in all right daniel gilmore um so uh i wanted to say something similar to what ecker was saying but maybe looking at it from a different perspective um the reason that people are comfortable participating in this scheme is because they want to give feedback that will help the person the the group that's developing their software or they want to report some telemetry without risking their own privacy right and some of these types of disaggregation mechanisms require me as a user to report some specific things that i don't actually know how they're going to be used to differentiate me from the rest of the crowd yeah so if the goal is to convince people that they can do this safely and not everyone's going to do the full analysis here but they might read analysis from other people then the more complicated you make this the harder it is for somebody to analyze it and say you cannot be disaggregated right yeah totally and that seems to be defeating the purpose of all of this right if people are willing to just throw their hands in the air and say well we trust that the telemetry collector is not going to disaggregate me then we don't need any of this protocol so i would be very wary about uh the extent to which we are asking people to"
  },
  {
    "startTime": "01:12:00",
    "text": "tag uh or to tag their submissions or opt into their submissions in some way that uh that has something complicated here so so you know ecker's point of like if we can say that you know the helper that obeys the right rules has this limit which is like each query can only be put into one aggregate response that's much easier to analyze and much easier to convince someone that they should participate um than this kind of like well you might be disaggregatable if you happen to use a browser that you know more than 75 percent of other people don't use or something like how do i know that that's going to happen whereas yeah the the simpler thing is better that's what i'm saying simpler yeah i i totally agree with that um i think like i think where we might be heading is uh i would like to be able to support at least use case number three but maybe we sort of um so so one one like in in the in the original design one thing we were we were contending with is what if you don't have enough data so like say your batch interval is t0 to t1 and you don't have enough data to actually get an aggregate to actually uh get over the minimum batch size well if i have enough data t0 and t1 then that's good enough like then that's good enough as long as uh i'm prevented from ever like doing an intersection over over that that larger interval but i think yeah i think this is something a problem that we can deal with um simplicity is key here sorry let me just add one more thing um the fact that this is looking like it might be proposed as uh an in protocol negotiation also makes me more worried right so if you could say if you're doing ppm you make a decision whether you're doing this kind of grouping or that kind of grouping and the whole the whole deployment makes a decision so it's not in the wire format right yeah you know as part of the"
  },
  {
    "startTime": "01:14:00",
    "text": "configuration of your of your system that you're going to be doing this type of grouping that makes it easier for someone who's considering do i want to deploy this do i want to participate as a helper do i want to report as a client to know what they're getting themselves into instead of being like well it could change up while we're going on and it looks like here this proposal for adapt2 looks like you've got it in the wire format which suggests dynamic transformation of any particular collection over time and that that seems much harder to believe in yeah yeah totally um i think that initially we would just say that the query type is uh is is configured out of band and as part of the task configuration that's like it's kind of punting but um i think that at least kind of addresses your concern jim i also agree with the simplicity point i think for that the most important thing is we deliver a badge that satisfies some privacy guarantees how the analyzing of the badge is totally up to the user and it doesn't have to be um part of the the initial requirement for that because like others have said there are you know infinite possibilities of how a user wants to define these selecting groups or query groups and also another point is these predicates you define for grouping the data it can also be achieved by encoding your input shares or your measurements in a way that every client participating will subject to the same predicate so everyone knows what kind of a task they are participating in but later there's no way you can slice the data to pick up some user lab but you could like encode whether a user is using chrome or safari"
  },
  {
    "startTime": "01:16:02",
    "text": "without actually picking out the the group of users that actually using safari chris wood i agree um uh a couple things so uh i think chris you already mentioned this or someone mentioned it but the the current restrictions that we have in the draft right now for validating our um verifying password basically is present forces limit that echo is adjusted where you have basically one query submitting to a given report there's no intersection allowed um and that allows us to like very reasonably conclude like certain privacy properties about the the resulting scheme if you're using that in a specific way it is overly perhaps overly rigid particularly because it doesn't enable like the um the chunk based variant here um so i would be in support of you know perhaps looser enforcement that did enable that use case um but i guess what i'm concerned about is whether or not those two different or that or rather the the enforcements that we put in place whether or not it would yield a protocol or a system that's like useful in practice um there's certainly like a large gap that exists between dap with like all these sort of query constraints and like other general purpose data collection systems that are used today it's like daf is not a drop in replacement for these things um and if it's not a drop-in replacement for these things like what is the incentive for people to to use this protocol um the the drill down use case that was mentioned um i think echo originally brought it up a while back is you know particularly interesting to consider um and i i don't know if that's something that people will want to have in order to"
  },
  {
    "startTime": "01:18:00",
    "text": "like use doubt um to enable privacy preserving collection um so i i'm kind of conflicted here i i very much support you know guardrails where appropriate and more reasonable such that we can reason about the resulting privacy but i'm worried about the inflexibility that that yields for the resulting system um and i don't know how to square that right now yeah i mean i i i maybe there's a way i mean if anyone is insisting on supporting this it doesn't sound like anybody is um i i'll just say that uh i i have a i want to be able to use this use case um i think it's going to be really important for in particular for differential privacy which is something we haven't totally worked out but um uh i would like to at least take a step in draft two that that deals with this use case go ahead wes no thanks um so i always hesitate speaking about a protocol that i haven't read the draft but that's never stuck to me before but i'm going to speak at a really high level i mean one to rephrase i think some of the things that other people have said um that are extremely important is it's not even so much the complexity but the instant you add two parties into determining whether you're getting the proper privacy aspects you know or not it greatly changes things right so i think about this protocol being deployed in a wide range of circumstances everything from you know in my house collecting data about my wife and then you know we have this mutual agreement that's all fine and these extension mechanisms would be great to the flip side of you know industry that have agreed legally on a binding you know contract and therefore only one side can pick it and it's got to be static hard-coded configuration so if you do end up putting this document i greatly suggest putting in some guidance on"
  },
  {
    "startTime": "01:20:00",
    "text": "when it shouldn't be used like when you know what the error message should be if i refuse to actually you know resolve this this conflict in this negotiation i just don't do negotiation it's hard-coded and you know there's legal auditing and representation behind it thanks for that perspective [Music] eckhart go ahead we'll let you in since you're insisting well i'm insisted but on rich madrid for you but the um i mean the drill down case is important but when we're like able to drill down based on client demographics is like like i can tell you is all the time um and and yes you get to the point where you're like hey the failure like some statistic is like bizarrely high on you know overall and now you want to know where it is like that's like absolutely important and it's not just a matter of time windows but that said um i think it'd be okay to like roll out like certainly be okay in this version of the draft and the next version of the draft to only only cover a smaller set of use cases um and it might even be okay if we had a sensible system that um allowed for drawdown later um but like at the end of the day this is going to be necessary for like a lot of correction um cases so thanks hacker um yeah i mean i it would be great uh yeah um given the the known unknowns i think we we should try to take uh as small of a step as possible i think differential privacy is something that we need to figure out the story of uh pretty soon um i would say it's like it's higher priority than other things but i mean what would you say is higher priority figuring out differential privacy or um or drilling down at this point do you have a preference aren't this the same aren't this connected they're connected yeah i think if you pick i think if you just did 273 like for the next six months that'd be fine so i keep it sure yeah no let me give you a concrete"
  },
  {
    "startTime": "01:22:00",
    "text": "example um we take measurements um regularly of the fraction of of tls deployments of the fraction of connections that potentials um and so we have a graph you can see that's like you know how much how many connections are https versus http and like that was like up to the right until about nine months ago and then for some reason it started going down for the world's whole and so we're like what the hell and so um i asked somebody to go and like look and they were like let's bucket by country and they discovered that there was like two countries where it was like they had bizarrely high numbers of reports and what was going down and if you remove those back it up to the right so like there's a great case of drill down we'd like to demographic drill down to figure out what's going on in a statistic and so it's not you can't do it temporarily you've got to do it like by the demographics and so you know now again we have to like we have developed preserving privacy which is the complicated part but like it's like and that requires overstep because we're pretty sampling on the same data set to solve that problem and i guess if we cancel that problem i guess we'll like have a less useful protocol than others would but like it's like a really important use case we do all the time chris patton do you have the guidance you need on this um i think so i think so i think uh the connection to differential privacy whether differential privacy is going to be necessary uh or or if it's sufficient or even necessary i'm not sure about um but we we can we can get into that you are the next presenter um [Music] well i i just wanted to summarize my way uh for chris and i love to meet you as well i think like addressing 273 with equivalent constraints that are um currently in the draft um one report poor query um is a good next step for the next version and then we can sort out separately how we want to deal with drill down and the related differential privacy issue perfect i'll i'm going to file an issue and then i'll uh i'll start working on a pr to discuss thank you everybody"
  },
  {
    "startTime": "01:24:02",
    "text": "thank you chris okay um so uh talked a lot about privacy in the previous presentation specifically for uh uh the the colexa protocol and what that means for dap so um attempt here is to sort of take a step back and and try and reason about what the what the threat model is for dap make sure we have sort of agreement there what irrelevant attacks that we want to consider what are possible to consider in the protocol itself is like first class thing and what are uh attacks that we need to sort of punt to deployment specific uh mitigations um uh just a reminder um you know we just we just saw this but i'm gonna i'm gonna repeat it anyways the collector protocol basically allows the collector to issue a batch predator cut for a particular um for a particular query and get an aggregate result as the output um the details of like what happens internally are not really that important beyond the the stuff that was talked about in chat like the leader can choose which reports correspond to a particular batch that satisfy the batch predicate um and that relates to you know civil attacks and stuffing attacks and whatnot but uh at the end of the day the collector issues a query with some predicate and gets back and i agree results and the question is you know what is the right way of validating that batch credit kit right now in the draft we're extremely constrained in terms of what is permitted as a valid batch predicate there's a number of conditions i should have linked to this specific section but the the first and foremost most obvious one is that the number of reports must be at least the min batch size so you get the gain energy guarantees that you want from the particular um instantiation of dab uh you have that a a report has not been included more than"
  },
  {
    "startTime": "01:26:00",
    "text": "uh max batch lifetime we need to change that particular variable name that constant name or whatever but um hasn't been included in more uh reports than is allowed um and importantly we uh to deal with intersection attacks um which chris was sort of alluding to previously we require that no batches can intersect they either have to be exactly matching so two collect requests they have to have the same start and end time interval um or they must be completely disjoint otherwise the aggregators are supposed to reject the leader supposed to project that particular requested but as noted this is not really flexible it doesn't allow sort of the other use cases that chris was going through in particular group based or maybe we need a better name for that but whatever more chunk-based collection um and the the motivation for this uh restriction was uh primarily doing an abundance of you know safety we wanted to make sure that it was not possible to interact with that in a way and that could lead or compromise the privacy guarantees that you want from the protocol and this seemed like the safest safest and simplest thing to do at the time i think since this landed we've had lots of discussions about what are um what are reasonable ways to enforce it uh you know the the underlying fundamental requirement um ecker just proposed a new one that we might move to in the next version um but the gist is that you know we we had this huge gap we plugged the gap but um we plugged it with uh perhaps too big of a van date or too big of a patch um so it's probably worth uh you know to identifying towards all right let's take a step back so um if we wanted to identify what was sort of the minimal um the minimal enforcement needed to take place and the minimal patch that we needed to apply in order to allow dap to be queried correctly it's worth like"
  },
  {
    "startTime": "01:28:02",
    "text": "taking a step back and looking at what dap is doing under the hood um so chris already already mentioned this but dap is a multi-party computation protocol um it computes some aggregate function f uh that is parameterized by a query in this particular case that's provided by the collector and then a bunch of inputs a bunch of plain inputs x one all the way to x i whatever all these inputs get fed in collector issues a query and gets an aggregate as output and the uh privacy goal that we want is that the aggregate output does not leak anything more beyond the aggregate itself so in particular the person who views or is able to interactively and adaptively query the system and get aggregates doesn't learn anything about honest client inputs beyond the aggregate that is computed based on those honest client inputs so as chris said sort of that means you want the batchman batch size to be high uh higher the better for more privacy um it's an application specific parameter or a deployment specific parameter but um that this isn't sort of intuitively or fundamentally sort of the privacy uh definition for for dap the threat model that we consider as a reminder um for privacy not for robustness is that there's some fraction of clients that are assumed to be malicious um and others that are assumed to be honest so if ever every client was malicious uh the system wouldn't really make sense so you assume you have some number of honest clients that are contributing to the protocol contributing to individual aggregates and the number of malicious entities are bounded all of one of the aggregators are honest uh or dishonest rather um so we assume that every single there's at least one honest aggregator that is you know implementing and abiding by the protocol as as specified"
  },
  {
    "startTime": "01:30:00",
    "text": "um and everything else is malicious and we also assume that the collector is malicious from the purposes are from from the perspective of actually interacting with the system uh as it adaptively queries it um this is kind of interesting because you know in practice i guess we in practice i can see you know scenarios where the collector is the one actually configuring the system deciding whether or not to use dapp in the first place so a malicious collector could just easily not use that or you know configure the system with parameters that are pretty awful um but we're sort of assuming that you know that was done in an honest way clients are actively or we're configured with good parameters and we're configured with um or are actually using dap and we want to protect against now a collector that wants to subvert this honest bootstrap or an honest configuration for the purposes of learning individual information about client inputs um and as noted at the bottom the robustness start model is different which assume it does assume that all aggregators are honest okay um so there are a number of tacks that we've already identified and that we have either text in the document to deal with or open issues to address the first of which is a stopping attack um your classic civil attack wherein uh the attacker which could be either a combination of leader helper or compromised clients is injecting things into the system into the aggregate to basically skew the result and allow allow the attacker to learn information about an individual client inputs so in this example we have all but one of the clients contributing to a particular aggregate are malicious and the honest collector is or the honest client is submitting its honest value it would be very easy for someone looking at the aggregate to determine what this honest input was which is obviously something we want to protect against in in actual deployments of the system it's also um i'm calling like an over sampling attack um in the differential pharmacy"
  },
  {
    "startTime": "01:32:00",
    "text": "literature it's like uh continual release up to exposure um or i don't know maybe that's not the correct technical term but basically the idea is like you have clients that are uh contributing honest inputs over and over and over again up to a point where they've revealed or contributed that input too many times and have sort of uh it's been folded into an aggregate and the the intersection of the combination of those aggregates therefore reveals information about the honest client's input um so in the the sketch here i have like multiple instantiate or multiple runs of the aggregation function f uh the honest client x one is contributing at the same value each time but every other client is contributing a different value um uh maybe either honestly or maybe maliciously um so this is kind of related to the stuffing tax but kind of different in that it depends on how you're using tap and specifically what you're using gap to measure as an aggregate so this is i think important to consider the other attack which is um referred to as an intersection attack uh was also discussed um as uh in the previous presentation which is why we have the sort of very restricted uh query uh batch or the batch predicate enforcement right now it's where the collector is adaptively querying the system with different query parameters trying to like yield different aggregates that may have uh overlapping underlying batches and then using the aggregate results to compute some you know some aggregate that is based on less than min batch size client inputs which is a violation of the privacy goal or definition that i sort of sketched out earlier the current query enforcement mechanism does not prohib or does not allow this because it ensures that every single query must be disjoint or exactly the same but this is obviously"
  },
  {
    "startTime": "01:34:00",
    "text": "i think something we want to protect against um okay so uh i don't know if this list is exhaustive in fact that's one of the questions for the group like have we sufficiently identified you know all the relevant problems for a privacy relevant problems for dap um but the question that we're asking ourselves now is you know what are reasonable mitigations for these particular issues uh so a stuffing attack as an example uh it might be reasonable to say that this is a very deployment specific problem um that you could deal with if you had for example client authentication that ensured that every single client input was honest and not maliciously generated you could address it with some application of different privacy local or central or otherwise um i think it kind of depends on you know the specific deployment and i don't know to what extent dap wants to mandate to require anything um about dealing with this particular problem and the same goes for oversampling as well because that's very closely adjacent uh to uh the stuffing in civil attack the intersection attack however we can deal with in the protocol in fact we do deal with in the protocol right now but it's uh as 273 or issue 273 sort of uh talks about it's um it's not it can be improved and and we aim to do that okay um so uh we i mean we kind of already talked about this fortunately in the previous presentation so uh and we kind of have like a proposed solution for moving forward but um so this slide was meant to say like we're trying to ask the question what is the fundamental requirement that we have for mitigating intersection attacks um and the informal goal is to basically not allow you know the privacy definition that we described earlier to be violated so all aggregates are based on the minimum batch size um and the enforcement is to basically ensure that uh you know every single report contributes to um uh some number"
  },
  {
    "startTime": "01:36:01",
    "text": "particular case as we were as we were describing um and uh the the question that was raised previously was you know what is uh what is a reasonable way for expressing queries such that this price equals meant but i think like the the conclusion that we reached was you know maybe don't uh constrain ourselves with how we express queries just enforce the fundamental invariant for query validation or batch predicate validation and allow whatever sort of queries make sense in the time being and separately we can figure out what the drill-down solution would be um yeah dkg but i assume now's a good time to take questions so i'm thinking about these underlying uh constraints right that that echo proposed in the previous talk um yeah um sorry there's there's also something on a few slides back that uh i just don't want to go into the record if it wasn't if it was miswritten and you might want to update the slides or maybe it's right and i'm confused one more back one more back that one all but one of the aggregators is honest yeah i i realized that was wrong as i was saying it out loud so there's a typo here okay yeah please update the slides um in in whatever archive we have i just want to just make that should be at most uh at least one of the aggregated zones right yes okay just wanted to put that in on the record okay we can go back um the um in the situation where everyone submits one report on their own then the types of constraints that ecker was describing sound pretty plausible to me i mean i don't have a clear analysis of it exactly i don't know for sure but if we say you can only use each report in one query uh and each query needs a reasonable size batch then i'm fairly confident about those protections"
  },
  {
    "startTime": "01:38:01",
    "text": "in the event that each client might report more than one more than once over time i am much less confident in that defense in the event that the client's reports might themselves be aggregatable um then i don't know how we i don't know how to evaluate this so yeah i mean the same thing with a lot of differential privacy questions right right you know yes if everybody's doing it once it's fine but then once you think about it over time and you don't know what the aggregation possibilities are it's a lot harder to evaluate yeah i was chatting with martin thompson about this earlier in the week in the concept of ipa um this is fundamentally related to the the concept of like over release or continual release in that in that setting um it's not clear like what is the best way to deal with some practice like you do on the client collector side you bound the number of times a given client can contribute its input across different aggregations or different tasks um i so i don't i don't know the answer there or the best answer there but i i agree that the current query enforcement mechanism and even the one that was proposed is only helpful in the context of a given a single like aggregate or a single task rather it does not consider like leakage that might occur across tasks that have as input the same client values yeah go ahead yeah um so just just to that point first um i believe that actually do anything about that um sorry another constraint the claims are continuously reporting i suppose to the clients you know report only a small number of times i don't need any way to do anything about that that doesn't also require um uh some some sort of reasonably strong client identification um like if you do know if you don't miss the clients obviously there's no way to person say that you can't bucket up all the clients you know you know other clients over a month right um"
  },
  {
    "startTime": "01:40:00",
    "text": "so um so i'm not sure i'm like i don't have fix but that's like that's my initial observation um the second is um you know the version that the constraint the dkg just suggested which is to say you know one query per submission and you know um and no submission maybe and no no batch size is more than n it's like playing an easy mode right um and um the and i guess i don't i don't actually know whether there's an algorithm that provides the variant you describe up there informally given a like if you just give me the you know the matrix of like which queries are in which batches um i actually don't know if there's an efficient algorithm to determine whether it conforms to this requirement um yeah maybe you do know but i don't know do you know no i don't think i don't think we have one either um and this is something we were talking about with some folks in the slack channel um about uh especially as you start adding like multiple dimensions to how you uh express certain queries how do you enforce this like non-overlapping well i'm actually suggesting something different i'm saying give me for each query give me the list of submissions involved in a query like in normal form rather than like you know in enumerated form rather than rather than degenerative form and can you simply determine the term validity of this of this constraint right um and if we had an algorithm like i guess where i'm going with this i think i sort of indicated earlier right is that i think the easiest way to do whatever we do is going to be to require the um to require the uh the servers to maintain an inventory of exactly which queries each client was involved in and then to prescribe an algorithm that determines whether with the m plus one query is a valid query based on the previous end queries and it doesn't require it doesn't say anything that other queries are expressed because the query because thing that the server is required to express is membership in that query membership right so and so i think that then if we had that then we'd be able for the case of this to ask the simple this like math question of like is there a way is an algorithm for looking at a matrix and terminating conformance right um and i"
  },
  {
    "startTime": "01:42:00",
    "text": "suspect there is um but i just like i don't know because like i'm not a math guy yeah i mean i think the the thinking that i have right now is that each aggregate mark reports uh that contributed to batches is dirty or not based on whether or not they were included in particular queries and then we would express the enforcement criteria based on like that dirty bit for every single report um uh that seems like the simplest thing right now but i i agree as written i don't see an obvious algorithm but the the simpler one that i think you're describing just makes sense chris we can find a solution um i think i would like to be able to can you you uh can you start over like there's you're cut off at the beginning yeah um i i also don't have a solution for this but i i i suspect like we've been thinking about this uh at on my team for a little while and i think i'm a little more confident that we can find a solution um but yeah so so i i don't think we should rule anything in or out at this point but uh just work on the problem uh one i actually wanted to add something um for folks interested in working on differential privacy um we are definitely gonna have to make accommodations in the dap spec itself but over in the cfrg uh we have uh we're we think that we're going to need to say something in the vdef vdf document itself about how to how to compose differential privacy so if anyone has expertise there and wants to contribute we would love to have your help on the vdf document and that's all i got okay cool thank you um all right so uh just to kind of wrap up uh i wanted to circle back to the the high level questions that i was trying to identify and um uh hopefully get some"
  },
  {
    "startTime": "01:44:02",
    "text": "discussion around um first i guess is the threat model clear with the you know the edit that dkg pointed out and that we discovered during uh presenting it um second of which is are there attacks that we're actively not considering um uh or just not considering not actively um that we we think we should address either as first class sort of citizens in the protocol itself or um or as you know a deployment specific uh thing and the third one uh the folks agree with sort of just right now constraining ourselves to mitigating the intersection attack using the the the sort of proposal that has now uh been floated and end up and sort of punting on the other ones for the time being right my sense isn't based on nothing in the room but seeing chatter and hearing people talk is that uh yes uh like let's let's deal with the intersection attacks um and let's let's like separately in parallel talk about how we might consider these stuffing attacks and uh you know over release of data across tasks or the over sampling attacks separately i think that's the right answer um i think so i think one yes two i don't know of any but i'm sure we'll find some um three um yeah so i think that right now we should do is effectively you said what i said and what dkg said which is like any minimum batch size and any submission can only be one query it could ever be only one in one query and then like maybe you could last requirement later but like that would get that will get you pretty far and i think it's compatible with like a very a very flexible set of uh actually quite a flexible set of queries a little drill up um and we'll get you pretty far and then like you know and then like once you have some experience or anything like i think we're gonna like this is a complicated enough thing it's a new enough thing that we're getting some experience either like pre rc or post rfc so i think this would get us far enough to make some real progress and then we could like as and then you know and like if they the coop and i were discussing earlier right you know we could have addressed that problem by"
  },
  {
    "startTime": "01:46:01",
    "text": "like doing doing that aggregate on like like day one and then drill down for day two and like that would have also sold a problem a slightly clunky one so i think i think this is already useful functionality and as patent was saying like let's get something there please try to keep remarks brief so we can get to the last presentation tim you're muted or you're not muted but i can't hear you right click the button uh okay sorry we were talking now about the notion of tracking how many uh queries a given report had been used in so um i think actually this motion does exist in the draft there's this concept of that lifetime in there um and that's the that's intended to accommodate uh poplar in the popular setting it's expected that the collector would make multiple iterative queries um against an aggregate in order to like like essentially it would be longer and longer string prefixes to eventually figure out like what the heavy hitter in a population is so yeah so one like i think we already have this and uh i'm pretty sure having the code that the honest implementation already handles it and we should keep in mind that like i'm pretty sure that in order to make poplar useful uh you have to allow multiple queries against the same set of reports thanks tim nick yeah thanks tim nick dirty seriously um i think this is a great start in the privacy right model um i i don't think we should be um uh confident that we've considered every privacy threat and in particular i wanted to raise something that i think has come up in the uh chat or um"
  },
  {
    "startTime": "01:48:00",
    "text": "sophia had mentioned in another presentation there might be some privacy threads that are about uh groups rather than individuals um and i think that'd be particularly important with small groups but uh if i don't learn that uh yes this particular person had this particular report but i do learn that you know unfortunately we're not able to to get your audio very clearly um maybe you can write your question in the chat and we can uh invite sean to uh come up and and show his slides so we can get started there thank you chris thank you chris good day everyone so this last talk is about a task enforcement and configuration this is somewhat related to the staffing attack chris wood was talking about um so this is a list of task parameters uh defining the current uh that draft so today we are focusing on the parameters that are particularly important to a task so here we have parameters like the vdf verified key which is uh not necessarily related to a task and it's also a shared secret between the leader and helpers so what we are talking today doesn't necessarily apply to these kind of parameters but it definitely applies to saying like minimal batch size so when we say task enforcement what we mean is how do we make sure the parameters we configured for a particular task"
  },
  {
    "startTime": "01:50:00",
    "text": "is used by the client and how does the client know these parameters are enforced and actually used by the aggregators and when you see task configuration we mean the process of creating the task with such parameters and today this is handled auto band there is no detailed specifics assumes its deployment specific but there are some potential issues and privacy issues with this so first of all what if the leader that configures these parameters are dishonest this is not necessarily just the leader it could be the collector or sometimes the leader and the collector could be the same organization if the when the task is constructed either the leader or the collector they define a minimum batch size that is too small for the task and then communicate that with the helper it's impossible for the helper to know if this size is too small and also it's impossible for the clients that participating in this task to know the privacy guaranteeing in this task is compromised now this applies to other parameters especially if you adopt the differential privacy there could be parameters like the different currency epsilon and so on these parameters can also be redef specific and like i mentioned how how does dap earn clients trust so the client know the task of the participant is indeed configured of the the right parameters and is enforced on the server side so what i think needs to be addressed in depth on the protocol level is first the transparency so the client or the user they shouldn't know the privacy guaranteeing they are getting they should know the parameters that defines the privacy guarantee of the task"
  },
  {
    "startTime": "01:52:00",
    "text": "secondly there needs to be some enforcement on the server side in the aggregators this should make sure the parameters configured for the task is indeed used on all the clients that participate in this in this task so what we mentioned in issue 271 is a proposal for inband task enforcement it has basically three things first we let client know about the task parameters being used this will help better auditing so the client can see what parameters the the they are participating in deca also develop policies to either obtain a task or not or have some way of auditing the tasks they have been joining and the parameters being used there this can also use this can also allow client to have some of these parameters hard-coded on the client side for example if you are using differential privacy the client could decide uh i want to use the local different privacy of a particular epsilon or lower [Music] and the second thing is the client needs to send these task parameters back to the server we want to use the extension in the current report because the extension is supposed to be extending what the the report is providing so this is a a way to make it extensible for different read-apps we can have one particular extension data type for a re-dash with a particular dp guarantee or a particular privacy guarantee but also because the extension is used in aed so a malicious aggregator cannot change these parameters later on they will fail to decrypt the report and thirdly the aggregator should check the parameters coming from the extension"
  },
  {
    "startTime": "01:54:02",
    "text": "match what they have stored for that task in this way a malicious client cannot pollute the aggregation by permuting the parameters in the extension so as a quick example here you have a leader that sends the task parameters to client helper auto banned the client each client will seal the input shares with the extension that contains the task parameters and then send those back to the leader in the report the leader will verify the parameters from the extension and when the aggregation flow starts it will send the same extension in the report share to the helper which will do the same verification now once we have the this kind of inband task parameter delivery we can go one step further this is what was proposed by issue 290 what this does is we can actually create the task on demand automatically when we receive a new combination of task id and task parameters so in this diagram we no longer have the auto band task parameter distribution between leader and helper so the parameters are only given to the client the client does exactly the same thing in as in task enforcement but when the leader receives the report if it's uh an insane task id and task parameters tuple the leader can create a new task on demand similarly helper can do the same when when it receives the same combination of task value and parameters and in this way we avoid this autobahn task orchestration between leader and helper and also the there is a nice side effect that any malicious client permuting the parameters will cause their reports to be aggregated in a different"
  },
  {
    "startTime": "01:56:02",
    "text": "batch than the the good reports from normal clients because in this way essentially a task is identified by not only the task id but also the group of parameters it was defined with this also from a implementation point of view is easy to do with any frameworks that implement a group by operator so you can essentially group by the report by their id and the task parameters in that case your task object becomes kind of a index that just groups reports together now in this last slides uh this is a small optimization uh so like i said in this scheme the task is identified by tuple of task id and parameters we can optimize that further by creating the task ids not randomly at the uuid but as like a hash of some shared info among all the clients participating and the extension that includes all the parameters in this way the id can be sent to the server and as one thing the server have to verify as a genuine task um but this does have some implications on whether you know the task id can be defined on the server side before the task starts so this is really just now optimization just say at this point i want to end the the presentation and take some points okay there we go uh thanks john for the presentation so i just have uh one note on the notion of like dynamic configuration uh based on"
  },
  {
    "startTime": "01:58:00",
    "text": "our experience finding the um notifications describes in that um like you uh the tasks effect the task analog um are configured dynamically just based on like what inputs are getting uploaded um by in that system's uh ingestion servers anyway and it turns out that has been quite valuable because it has enabled like the mobile os vendors to add new aggregations to the system uh without needing to explicitly coordinate with like the three organ organizations running the other servers um so yeah i just wanted to you know put up that comment that it is kind of valuable to have this dynamic task configuration capability oh great thank you uh chris patton um i also think this is a great idea um i'm a little i'm not very clear on some of the uh security considerations um i would like to see those fleshed out a bit more before we consider a pr like before we consider a change but like uh it would hopefully be nice to like look at apr for this for me personally like thinking through the security implications and like what we have to do on the to implement on the server side so if you wanted i would say like uh if you wanted to start drafting for for people to look at i think that would be a great part yeah we can start preparing a pr for this thank you i appreciate that you're thinking about this uh because i think it pushes us to really think about how things are going to be deployed um when i think about how things are going to be deployed i like to ask a couple of questions by taking the perspective of some of the participants as a client"
  },
  {
    "startTime": "02:00:00",
    "text": "i don't know how we would expect the client to set to to to be able to choose uh the consent options here like well i'm okay with this kind of parameters but not that kind of parameters that seems very complicated and difficult for clients to do and i'm wary of asking clients to do that kind of configuration choice when i think about asking someone to operate as a helper the helper's job is to keep the system honest as far as i can tell and if my job is to keep the system honest then i'm going to need to make some very specific decisions about what types of parameters i'm willing to accept not just um not just you know take oh here's a new set of parameters i'll just i'll just adopt them and that means the helper is going to need to actually make some constraints on their system about what what requests they're going to accept and now you're asking the helper to make some pretty sophisticated decisions as well i think the way we want people to step up and say yes i'm willing to be a helper because i think this measurement is valuable and i also want to protect the client's privacy um it seems i don't know how how you're going to offer those kind of constraints if this dynamic configuration is happening yeah can i just quickly address the question so uh i agree for clients to understand all the privacy parameters and decide to open or not uh is not realistic for most of the clients but i think the transparency needs to be there so for the few that do understand i do want to see what kind of a collection schema they are they are participating in i think this is very valuable and sometimes these kind of obtaining opt-out policy could be defined by uh the organization that provides the client side to the to the individual clients"
  },
  {
    "startTime": "02:02:00",
    "text": "that's one thing and then the other thing is for the uh you mentioned the photo helper it has a responsibility to you know know the parameters that you receive and make sense privacy-wise uh that is true and for uh in issues 271 we also said something about uh like if you have differential privacy guaranteeing then we could implement some uh sanity check on the helper side uh and this can be extended to to extend it to the client side as well that you know the privacy differential privacy parameter you received indeed makes sense for the kind of batch size you are defining um but i think here that the key is one the transparency to the client today we we simply don't have that on that side and two is like you said when you deploy something you have to worry about how do you communicate these parameters between need and helper you could find any secure ways to deliver these but i think it's better that you don't have to worry about that and there is one option for you to actually configure these using the same route that you uploaded the report to the aggregators and and you achieve both transparency and enforcement in in one solution thank you sean uh that concludes our meeting of privacy preserving measurement thank you everyone for participating and for getting through our whole agenda i noticed continuing discussion particularly about star in the zulu room feel free to take that to the list we haven't issued a call for adoption but feel free to comment on that on the list because i see that some people are interested in if there are topics that are warrant an interim feel free to come to the chairs we could schedule such"
  },
  {
    "startTime": "02:04:10",
    "text": "okay oh you"
  }
]
