[
  {
    "startTime": "00:00:13",
    "text": "jonah that's this that's the same one as the as the one in mid echo right it should be yeah yeah it's the same one here okay i got it thanks thanks zeke thanks lucas all right it's a minute passed um so let's get started um welcome to iccrg at iit f112 uh we are meeting after about six um oh after about eight months so i hope you had a good eight months and i'm looking forward to our session today it's a pretty packed agenda um and i hope to get through all of it so i'll start off by um with the with the irdf note well which is pretty similar to the ietf model if"
  },
  {
    "startTime": "00:02:01",
    "text": "you're not familiar with it you should you should read this carefully at a high level i will simply point out that you are simply following these idf processes and policies by participating in terms of your the intellectual property of anything that you share um you are expected to file ipr disclosures on anything that you file that might have ipr that you're aware of otherwise read the note for more details generally i will just point out one thing on the code of conduct please work respectfully this is a group of people from many different backgrounds and many different uh with many different views um be respectful and have respectful discussions and finally the goals of the irdf specifically pointing out that the irtf conducts research and it is not a standards organization so as you as you engage as you participate and as you discuss items bear in mind that we are not trying to standardize anything here and with that let's get to the agenda so um basically we have we have a number of things uh uh today we're going to start with a a a different uh item than usual which is uh uh source priority flow control and data centers um from jk lee who's at intel and i will let him talk about this this is different in the sense that we don't normally or off in anyways talk about data center stuff and we have sometimes done that and we have a presentation here which seems to be very much uh on target for iccrg so we're going to talk about that first after that it's going to be uh"
  },
  {
    "startTime": "00:04:00",
    "text": "more things that you are used to we're going to start off with we're going to go on to a ccid for bvr for dccp followed by a presentation from ayosh mishra on the game theory behind running cubic and dvr you've seen ayush in the past at iccrg and i'm looking forward to this presentation and then we have uh updates on bbr v2 from ravine and neil and ian and praveen is also going to give us an update on our like that so with that i'm going to get started um jk do you are you here yes hi oh there you go wonderful so you can uh for for all the speakers you can you can present your own slides you just have to go to the little start slides share button which is right next to the hand the raised hand thing uh on the left there we go and i will allow you to do that which you should now be able to i'm gonna try to grant try to stop sharing first apps ah and now you shaw skin should be okay i mean excellent the flow is yours jk thank you very much everyone um this i'm jk lee a principal engineer from intel uh this is my first time attending ietf so um i'm glad to be part of this forum um yeah i'm going to talk a little bit about source flow control or a little bit more degenerative form which is source priority flow control source pfc"
  },
  {
    "startTime": "00:06:01",
    "text": "and this has been a collaboration mostly between inter and recently we started to talk about you said i3 a report condo and then lilly probably oh sorry i need to click here good so um i don't think i need to go through what kind of conditions are there in data center there are multiple different types but one type we are looking at right now is incase condition which is mostly caused by the money to one traffic pattern and it uh mostly happens that the last of switch is the last of course for our second half switches and it drastically uh goblins affects the uh tail latency because of the large queen delay or even it can cause packet drops and absolutely this tail latency is well known to have a large impact on the application performance for our scale metrics and especially when incase happens due to the line rate rdna senders because they are really starting to send at the right line rate or the messages and uh connections um the the same size the message can actually finish really short amount of time which means that it doesn't really give large uh rtt time for the condition control to detect and then converge and react to the condition so fast reaction possibly at the sub rtt time is more preferable with rdma incas in data center so yeah obviously for congestion we the community has been working on entering condition control for many many decades and at hell level yeah it's uh end-to-end signaling from four direction data packets and echoed by the receiver so that the sender can adjust its transmission rates and condition window"
  },
  {
    "startTime": "00:08:01",
    "text": "and one thing we kind of notice here is that the condition signaling itself is carried by the the data packets either explicit condition notification like ecn or rtt measurement so they happen to be copper with ongoing congestion and um if for example there's a new sender arriving at the the tail of the heavily congested queue then the the new the pocket the first packet of this new flow has to wait until um the all the other congested packets are dequeued out and then finally hit the receiver and then can be echoed back and also the nature of aimd or the typical um rate control mechanism it takes multiple rtts to actually flatten the curve like if uh if the rate adjustment mechanism is a cut rate by half open the reaction architecture of congestion that means that 16 to one in case will take another rtt for eight to one four to one down to one to one and eventually we want to really cut the rate down to zero if there is a having cast so that we really flatten the curve that means that many rtt times will be required um at the same time there are a number of flow control mechanisms mostly layer two and well known for i3 pfc and they are really mean to uh prevent congestion practicus from the beginning by ex employing something like uh exonyx of uh low latency reaction mechanism that is guaranteed to be happening within one microsecond detection and reaction can should happen within one microsecond as required by the standard and but it's a hopper hub flow control so although it can actually avoid congestion packet drops from the beginning"
  },
  {
    "startTime": "00:10:00",
    "text": "it effectively slows down the fabric because of the many of the hard drive line blockings happening at the inter-switch links and it can actually back pressure from the um transition point to towards the upstream switches and towards the eventually to the senders and it is well known that some side of operational side effects like a pfc storm and deadlocks can happen at the same time so um here this presentation we are going to kind of stress on the needs for the new layer 3 flow control mechanism which can give us immediate detection and reaction and um and the layer is better to be layer three so that we can reach out um across the data center so this is the one slide summary of the our proposer so the key idea is very simple at the congested switch um which we just first compute the minimum time required to drain that target in case q where you can expect is something like a expected surgeon time of the queue and the signal is signal packet is generated and carrying the information backwards for the incas senders and there are two different ways for us to consume uh this information the first one is um the sender side top of rec switch can convert this layer three signaling packet back to standard pfc to directly pause the sender nic cues so we call it source pfc or um the signal information can be forwarded back to senders and then sender link hardware or host networking stack"
  },
  {
    "startTime": "00:12:00",
    "text": "can directly consume that information and pause the search flow and we call it source flow control or sfc with that um the next slide is about simple diagram depicting the uh the behavior of search pfc so what you're talking about here is that um simple cartoon data center with two senders and one receiver there's an interest happening um so typically when we enable pfc and 2n that means that the the destination top of rec switch will create pf's priority flow control and this will pause the upstream aggregation core switch links instead of that what we do here is that we assume some simple mechanism at the switch ingress which can learn about the ongoing congestion at the switch egress so even before when we fold the package from ingress to egress the ingress pipeline has some capability to generate a signaling packing back to the senders and here for source pfc we are assuming that i actually implemented that the source side topper work switch can simply convert this layer three similar packet to pfc frames so that it can immediately pause the uh in case senders so the entire detection and reaction can happen within sub-rtt time where the rtt is congestion-free based rtt and here we are not really aiming to replace entering condition control this is more about kind of emergency brake or reaction to having cast because we are not really pausing any of the inter-switch links there is no head"
  },
  {
    "startTime": "00:14:00",
    "text": "of line blocking happening between the inter-switch links and no phd side effects are expected um of course we can deploy this mechanism every switch in the data center but from our simulations and test beds top of rack switch only upgrades can actually give us mostly bang for the buck because i think mostly happens at the last stop switch and the signal link between this these couple rack switches can still give us pretty good reaction to most of the heavy intestine meanwhile when the heavy congested queue is being drained but finally the entry and condition signal will be received by the receiver and then echoed by uh the receiver back to sender so here the point here is that um the this source pfc reaction can be much faster than uh the heavy condition control reaction to the heavy incase when the especially qdap is pretty large um this is the very simple um test battery experiment to show the benefit of source pfc here uh we have two switches and multiple senders and receivers senders on the right hand side mostly and the receiver on the uh sorry senders and the left-hand side receiving the right-hand side there are two in-case flows happening at the same time here the the experiment is designed in a way such that the the link between top of rack switch 1 and to r2 will be a head of line blocked mostly by the uh in case happening at the receiver 2 will pfc pause the omnics port and which will in effect uh create head of line blocking also for the flows of sender one to receiver one"
  },
  {
    "startTime": "00:16:03",
    "text": "so this is all the rdma write request so first we measure the q-dabs over time using our some telemetry mechanism and here we compare the q-dabs uh between the two mechanisms on the upper hand side we have a the case where pfc is enabled everywhere versus the and the lower handler side um we have a case with the remote pfc or source pfc enabled and you can see that the q that's at the order three congested links are pushed down to drastically um with more than sometimes tens of order of magnitude difference with that you may wonder what would be the throughput performance so with so this is the measurement of the flow completion time of this um more than the thousands of flows uh or rdma write request and yeah we see that in the cdf of the flow completion time the source pfc performs uh better than the traditional pfc here we actually didn't enable ecn because it was not easy to find the good tuning of ec and dcqcn and actually when we enabled this sequence we found that the tail latency was worse uh in case of tfc so we just wanted to simply compare the operation between pfs and source pfc so you may wonder what information we need to carry for this layer 3 signaling packet um so key id is very simple um so we have some more detailed backup slide at the back down the road um how we can convey these signaling informations in the i3 802.12 cz which is the draft"
  },
  {
    "startTime": "00:18:01",
    "text": "cim here means that conditional isolation messaging excuse me um so in the source pfc mode uh the we just need to mostly swap the uh source and ip addresses of the data packet so that the the newly generated so when we generate a signal in packet we can just swap the source ip from the data packet and then use them as the user's destination ip of the signaling packet with that the packet will be forwarded signal packet will put it back to the sender and we can still carry the original destination ipod the incas traffic it can be optionally used to cache some of the post time at the sender side top product switch i can talk about that more later and we can also we should also carry something like a dhcp uh or vlan pcp whatever um qs priority information that is needed to identify the actual pfc priority queue to pause at the center nick and most importantly we we carry the pulse time duration or expected surgeon time it should be the smaller or the equal than minimum drain time to reach the target shoot ups so here the target q depths could be something like an ecm thresher or uh slightly over uh lower than that when we tried to try with different values they didn't really make big difference because our reaction was the spx reaction is really fast and optionally we can carry some additional condition locator information like a switch for qids but that's really optional even without the information the entire protocol behavior should be the same the additional information can have if you really want to consume that for more network-wide"
  },
  {
    "startTime": "00:20:01",
    "text": "monitoring and then traffic management etc but that's a kind of beyond scope of the initial use case and for the source flow control we want it to be consumed by the sender side transport layer so we need to carry some additional layer 4 port or maybe qpid like information so that the the actual sender side flow and connection can be identified which flow and connection to be paused as a reaction of the signaling so when we look at this source flow control um a bit more kind of advanced version of source pfc because we are pausing the flow uh at the flow level at the transport um you may wonder how is it how does it look different from something like icmp source crunch so just as a data point there is actually recent paper nsdi this year something called onramp which also implements such a similar flow level um connection level flow control mechanism implemented at the linux q disk so yeah in order to make a reaction and consumption of this information we need some uh changes on the software stagware uh we need to modify the rdm hardware stack um and there's one example uh in the nsd paper back to this question how does it uh differ from source crunch which is actually duplicated rfc i think more than 10 years ago um as i understand there are multiple reasons why source quench has been deprecated first it didn't really specify which"
  },
  {
    "startTime": "00:22:00",
    "text": "information to carry or how to consume and react to that information at the sender side and so we in sfc or as tfc we clearly specified that we just carry the pulse time duration for the drain time duration and we promote that uh the immediate flow control so that incas senders can really stop sending immediately rather than aim this style of transition control especially for data center and uh yeah source quench was really designed for or promoted for when internet transition handling but we are promoting this sfc for data center with single administrative domain and in case of layer two data center there what has been something called i3 qcn uh it's actually quite similar you know message that it also promotes the back to sender signaling from switches to the traffic senders but this is a layer 2 transition control with similar aimd still requiring multiple time to flatten the curve and rocky dcq cn is the l3 adaption of gcm answer is one of the questions and some additional questions are shared by the itp community uh by separate emails so um at high level how do we secure the protocol um [Music] we assume this will be for single domain data center with trusted switching devices and i can make some argument that the signaling between switches for source pfc it could be similar to error dp and bgp and then i understand that there is a bgp encryption mechanism but in reality it hasn't been really used for"
  },
  {
    "startTime": "00:24:00",
    "text": "many regions it cannot really serve the problem of malicious or poorly implemented router and it can actually cause additional headaches so as i understand no one not really heavily turned on the bgp encryption mechanisms and for the signaling for sfc source flow control for the sender transport to react um we can see that this is quite similar to ecm marking where the data center switches for intermediate switches and routers are uh provide some information in the actual data packet and consumed by the sender side transport here we are generating the new new signaling packet instead of modifying or marking on the inband data packets but you know such that the information is provided by the switches and then directly concerned by the end host is pretty similar in my opinion and then ecn is has been uh heavily used in data centers these days something like a dc-tcp at the end uh we can simply uh implement accur at the domain boundaries uh like a topper rec switches or maybe gateway switches so that this new form of signal packet cannot really come from the outside of the domain and another question was yeah is it only for rocky yes rdma is a primary use case and then rocky b2 is the most popular transfer of today but we see more new type of transport for rdma or rma mechanisms are rising and so we believe we can have many different rdma transports in a similar way to scale on standard ethernet fabric and uh we have some some argument how this can be a good fit"
  },
  {
    "startTime": "00:26:01",
    "text": "for the machine learning training in the backup slide so if you want you can take a look and sfc can also be applied to non-rdm use cases as similar to on-ramp paper from nsdi and we are currently performing some evaluations with the tcp traffic and you may wonder that the ig-28 signal link is still sub-rtt but it can be still proportional to the network rtt if the network size is growth so then isn't it too slow um so we have a simple mechanism that we can catch the uh pause time per destination ip at the sender site top project switch and this information can be used to instantly pause another sender's coming from the same connected same switches and then uh handy gearing for the same destination ip can be immediately paused without waiting for them to be trend uh reach out to the sender side receiver side tour and their account back with that um this is a simple history um of the mechanism yeah we first talked about this idea starting from uh last year april under a number of presentations at the public domain and also recently more in the iit pre there is a suggestion that ietf should be aware of this activity so we are having this conversation here in iccrg thanks for the opportunity and the planet the i3 is simply extending the existing in 80 2.1 qcd uh conditional isolation mechanism it already has a layer 3 mechanism so we can simply extend it to enable something like a source pfc but i can easily imagine that if you really want to do source flow control uh for the transport to make use of this information then ietf can be a better forum to discuss"
  },
  {
    "startTime": "00:28:05",
    "text": "yeah i think for the sake of time i'm going to stop here and then we can i can take some questions should i stop sharing my screen hey jk let's keep your screen on in case people have questions about any other slides um there is a question from jonathan in the chat room if i understand correctly yep go ahead take that and then praveen can go on after that uh i was just uh pointing out that the um the basic uh multipli but multiplicative decrease mechanism is an order log n mechanisms that was from quite early in your presentation i see yes sorry sorry if i misunderstood but yeah it still takes more than one itt i guess that's the key question thanks for pointing out growing europe the question um so there was another work recently presented in iccrg called hpcc uh can you compare and contrast approach with hpcc yes hbcc i'm also part of the effort it's still four direction signaling so hpcc you can imagine this as a really multi-bit ecn so instead of just one bdcn it carries the"
  },
  {
    "startTime": "00:30:00",
    "text": "multiple information about the condition like qdaps and then link utilization but still in the four direction data packet and echoed back by the uh receiver back to sender so um it still kind of suffer from um the condition signaling path is coupled with the ongoing condition um so we do have actually simulation results um demonstrating the effectiveness of source flow control or sfc with hpcc so when the incase ratio is not that small then new condition control like httpcc can actually do decently better but here in in this case if we actually created an in case a little bit higher than uh user um let me go back uh 88 of traffic is in caspers with hundred twenty two down to one so it might be a little bit kind of severe in case condition in those cases and uh we still kind of generate up to orders of magnitude and it's one order of the magnitude better improved our flow compression time compared to uh sfc and uh we could also maintain the buffer occupancy smaller and then slight improvement in the good but overall because of the queuing is better managed that that leads to the reduction of the flow completion time i hope this answers your question png you're on uh yeah okay here we just want to double check hello"
  },
  {
    "startTime": "00:32:01",
    "text": "hello yes okay hey john i have a particular question regarding your slide 6 the example in the normally in data center of the tor switch or for the other aggregation switch from the downlink to uplink you're going to have some ratio not just like the one you're showing in the slide six like the downlink 100 gig and the update also 100 gig normally just like a photo one ratio it's like not uh not so con congested as you're showing here so like you are using 100 gig for the downlink you probably are going to use like 400 gig or even combine up to 1t so in that case are you still seeing so much uh uh improvement from your yeah experiment thank you yeah that's a fair question thanks yeah this this has been intentionally designed the topology has been intentionally designed to uh just nail down on the head of line blocking issue and uh when we simulated the larger scale simulation for 320 servers yeah we definitely created a full bisection than this uh topology without any um um oversubscription and here there we could still see that pretty good flow completion time improvement even for the dc qc which is the uh actually improve the cqc and also hpcc and um there was another presentation from uh at i3 so if you sorry yeah this particular highlighted link if you can click on this one this was the measurement study done by huawei um a month after our initial presentation in i23 and then they actually quickly the prototype and then uh demonstrated when their rdma traffic is mixed with tcp"
  },
  {
    "startTime": "00:34:01",
    "text": "uh with i think normal of a subscription or maybe small over subscription uh the benefit was still pretty good there in uh in terms of tail latency so yeah my key answer is that there are multiple data points that you can still have thank you thank you all right i have a question from the queue from the flow so i'll ask you this question jk um i'm familiar with some work in the past and this is probably a few years ago timely and from from google folks did some work on basically using rtd rtd as a congestion signal for within data centers and it was if i remember correctly they did some work on rdma fabric have you compared your work to theirs or have you looked at that book somebody else from google might be able to shed more like on the specifics of the proposal but um have you had a chance to look at that yes yes um we are very well aware of such a new condition control algorithms designed for rdma so hpcc is one of them timely and recently swift i think near carville is here today and um many of them really make up way more better position control than something like a tcqc or dc tcp because um it's really designed to for low latency high bandwidth but i think the fundamental difference is that um incas can still happen because you cannot really perfectly synchronize all the senders especially with rdma each centers uh blasting at line rate if somehow they happen to collapse in the within one or two rtt time maybe"
  },
  {
    "startTime": "00:36:02",
    "text": "more than just three or four senders they can easily just fill up the queue very quickly right and the entry condition controller still has to kind of have the signal uh passing through the congested queue and reaching the receiver and echoed back so this will naturally will take endless more than two to three or maybe more rtts depending on the level of incas so um this source flow control or we colleges back to sender signaling mechanism um provides that incast information directly back to sender within uh one rtt time and so they can handle such a unintended synchronized interest or it may also handle the case where um the timely or the rdma condition control uh happen to coincides with non-compliant congestion control or maybe when when condition control reach has a way larger rtt time that they wouldn't really react to this uh incase congestion control uh signaling within uh soon enough so that the back to sender is really handled such a different cases understand well thank you so much for that and with that i'm gonna uh uh that we do not have any other folks and we need to move on to the next presentation so thank you so much for your time jk your presentation to the folks in the group i'll say that this is work that's going to the ieee and if there's any feedback that you'd like to pass along i think it will be welcome either on the on the research group mailing list or directly to jk and with that i am going to move to the next presentation"
  },
  {
    "startTime": "00:38:01",
    "text": "um natalie i hope you say i'm saying your name right i'm not sure yes i'm here yes i can hear you um i'm gonna ask you to request uh presentation okay and you should be able to choose your slides now yes excellent uh take it away then yes okay so good afternoon everybody my name is natalie roman and i work for the strategy and technology innovation at deutsche telekom and today i'm going to give a short presentation about an implementation of the vbr congestion control algorithm for the ccp which we have named sincerely so uh first of all the motivation to bring bvr to the tccp protocol relies on the fact that right now for dccp there are only three congestion control algorithms standardize it and all of them are lost bases so we thought about bringing vbr precisely because it is a non-loss-based acc algorithm apart from that we also wanted to use vbr within the multipath dccp protocol that means to bring it into a multiple scenario where the latency difference among the paths is a key factor to achieve a good performance and we thought that in this case bbr might be useful last but not least a will note that bbr has proven to have quite good results for tcp in terms of having a low latency high bandwidth and also avoiding problems like buffer blood so we wanted to verify whether all these characteristics also apply for this ecp"
  },
  {
    "startTime": "00:40:01",
    "text": "on that basis a we started the development of bbr version one for dccp as cc85 within the linux kernel this implementation is available as open source and of course it is based on the existing implementation of bvr for tcp however the main difference between tcp and dccp relies on the unreliable nature of dccp which means that all the functions related to the acknowledgement generation and their processing are part of the ccid definition not of the protocol itself which means they are going to be part of the ccid code apart from that a we also wanted to adopt this existing and mature a tcp vbvr as a new ccid profile for tccp and for that we have already submitted an initial draft and we will be happy to receive some feedback and comments about it so once we finish our first implementation of bbr for dccp we started some evaluation in a controlled environment using a single path and a multiple scenario in this evaluation we compared the performance of our implementation of bvr which is cc85 with the performance of cci2 which is the default congestion control for the ccp the results show that for both cases for single pad and multipath cc-85 that means bbr has way better results in terms of latency when the pad has a limitation in the bandwidth apart from that in the case of the multipath scenario ccid5 also helped improve the scheduling performance moreover the main achievement of this test was to prove that the conceptual basis of tcpvr was also applicable for dccp which means that all the"
  },
  {
    "startTime": "00:42:02",
    "text": "existing studies and results available can be extended to dccp as well in [Music] the whole analysis and also the description of the test environment is here in this paper which i have linked in case anyone is interested to check about it so after executing this first test we move to a more realistic environment and we started testing our implementation in nlt link and we compared the result of cc 85 and the standard tcp vivia in this results we found that a during the pro rtt phase the drops in the bandwidth were quite deeper for the cc-85 in comparison with the tcp vr and also the duration of the property phase was a little bit longer in some cases in comparison with tcp so we started an analysis of this problem and we figured out that what was the cause the point is a bbr requires the restoration of the congestion window when it leaves the property phase so it restores the congestion window from a quite low value to the value it had previous entering this phase apart from that the dccp requires that when there is a big change in the congestion window there has to be a synchronization between the sequence and acknowledgement validity windows this synchronization uses a feature negotiation function which is described here in this figure basically the sender starts this negotiation and once it receives the confirmation it can update the local buy this brings a brings us to a problem and is that the property federation acquires a latency dependency because this negotiation"
  },
  {
    "startTime": "00:44:03",
    "text": "update the local values and proceed to the restoration of the of the congestion window when leaving the property phase so to solve this problem we apply the temporary solution which is that we trigger this synchronization but we don't wait for the confirmation to update the local values that means as soon as we trigger the synchronization we can update the local value and proceed to restore the congestion wind after applying this a temporary solution we managed to achieve a similar performance comparing ccid5 and tcpi now the question is that as i said this is a temporary solution and we would like to start a discussion to know what is the best approach to solve this problem so maybe a new or an enhancing feature for the sequence with the negotiation is necessary or maybe there can be a different approach that help us to solve this problem so to summarize this whole thing we have implemented bbr for dccp the initial test that we have made have proven that a the conceptual basis of tcpvr applies as well for the ccp of course taking into account the unreliable nature of dccp and the difference that that brings us and on that basis we would like to adopt vbr as a new ccid profile for that as i said we have already submitted an initial draft we submitted it initially to iccrg but we have been told that this has to actually be part of the usbwg group so in in the further version google submitted that but we will be happy to receive any comment or any feedback about it the second question comes about this sequence window negotiation so we have"
  },
  {
    "startTime": "00:46:00",
    "text": "described the problem and we would like to receive some feedback about it and to start as well a discussion there but if we are not sure what should be the right place to start the discussion either here in ccr in iccrg or in tsbwg so that's the reason why i'm making this presentation now and that's it for now thank you natalie that was a wonderful uh and short presentation so uh and thank you for that last slide in particular i think it's uh gauri's in the queue so i'll i'll let him have a take as to www that we'd be able to discuss this um on the mailing list and make some progress with the algorithm and the proposal i'm not sure where the home would be um but it lies between these two groups and whatever the home you shouldn't be discouraged please please discuss how to fix it and please discuss the issues um it was a great presentation i don't know what the outcome will be so i'm looking forward to hearing more from you about these methods okay i want this stuff thanks thank you gauri i had one uh question here so you refer to bbr here as mature right my understanding is there's quite a few presentations today talking about bbr v2 which is the next evolution of bbr with like coexistence with cubic etc right so my question is if you're going to place you know create a new standard um would it be better to wait for bbr v2 to mature before doing this i guess that's"
  },
  {
    "startTime": "00:48:00",
    "text": "my question to the group yes i mean from my point of view we are also planning to implement vbr version 2 and tested it in dca in tccp that is in our scope of workforce next year and i think yes it will be better to use bible version 2 as it is a more mature version all right so we're going to take more discussion on to the list and when i say the less i mean i see crg but um we're going to talk to the chairs about oh i see the ad is in there so martin go for it well not the id for this but um yeah i like i i i support this work as well but i am a little concerned that this is racing a little bit ahead of the actual tcp bbr work um which i i think probably has a little more data to support it um so i mean it seems like most bbr discussion at this point is happening in iccrg um and you could certainly do a drafting iccrg it would be experimental but given the bb like we'll find out more i guess in a minute here but given the bbr is still a little uh is a moving target maybe that's the right designation for now yeah that's my sense as well um but we'll take this discussion offline uh with the chairs of tsuwg and um we'll get back on that david you'll have the last comment and then i'll let them on to the next discussion among chairs is fine um i think icc as as i think iccrg is the right place to discuss the technology and it's also the right place to figure out appropriate timing when the timing is appropriate tsvwg uh is almost certainly uh the uh the venue to work on the directness here and standardization but uh need to get the timing right and make sure that it's it's it's well"
  },
  {
    "startTime": "00:50:01",
    "text": "coordinated with bbr as a whole yep thank you for that comment david all right thank you so much natalie and i hope to see you on the mating list um people please engage it's good to have this mapping for bccp as well i will now move on to the next presentation and that is ayush are you sure you're here i see you up there uh can you i i see you requested i am wait a minute all right take it away okay uh so yeah hi everyone i'm ayush i'm a third year phd student at the national university of singapore and today i'll be talking about uh some very interesting work that we've been doing on studying the game theory behind actually you know choosing between running cubic and bbr on the internet uh this work has been done in collaboration with my uh collaborators jinxie melties sean raj and my advisor ben leon okay so since pbr was introduced in 2016 a lot of websites have made the performance driven decision to actually adopt it and use it to send data for their websites and companies like google and spotify and dropbox have reported seeing lower delays and better throughput especially in lossy networks where act locked loss-based algorithms like cubic are known to suffer and clearly this trend has got on since um we did a measurement study in late 2019 we found that close to eighteen percent of the alexa top twenty thousand thousand websites are already running"
  },
  {
    "startTime": "00:52:01",
    "text": "vbr and this 18 metric actually goes up even more when you consider the more popular websites or websites that uh contribute more to downstream traffic like for example video streaming websites so the question we want to ask is where is this transition uh really heading so this transition in the internet's congestion control landscape is definitely not a new thing we've seen in the past that renault dominated internet in the early 2000s slowly transitioned into an internet that was mainly cubic dominant and much like gbr does today even back then cubic basically gave you better throughput and better utilization guarantees on the internet which is why people moved on to it but there is one key aspect in terms of which this transition from cubic to bbr is very different from the transition that we've already seen which was between renault uh to cubic so the transition between render to cubic was essentially between two window-based loss-based algorithms so they were they both had the same congestion control philosophy they both uh reacted to the same congestion uh signal and that's why you know we didn't really face a lot of problems but right now as you have more and more websites replacing uh using bbr to replace the existing loss-based algorithms what's that actually doing is it's creating a paradigm shift in how congestion control is done on the internet uh now we have websites um now we have floors that are competing with other floors and we have all mixed set of congestion signals that everyone's uh responding to so the question we want to ask is uh given this performance improvement uh"
  },
  {
    "startTime": "00:54:00",
    "text": "that bbr has given us so far uh where do we actually expect this transition to move or in other words if you're seeing such good performance benefits is it reasonable to expect everyone to switch from cubic to bbi at some point in the future so this is a question uh that we discussed in a recent short paper at epnet 21 it was titled conjecture existence of nash equilibria and modern internet congestion control and the main insight we found while writing this paper is that you can really model this entire problem of choosing between cubic and bbr as a normal form game because we have some players or in this case websites that can maximize some utility which in this case is network performance and all these players have a fixed set of strategies which is either running cubic or bbr available to them to maximize their utility so the approach we had uh to analyze this entire system was actually to calculate uh the nash equilibrium in the network where the senders have the freedom to choose between cubic and bbr to maximize the throughput so just as a refresher a nash equilibrium is basically a strategy distribution where none of the players have the incentive to switch to the other strategy or in other words in networking terms it would mean a distribution of congestion control algorithms over the network where none of the senders have the incentive have the performance incentive to switch from cubic to bbr or vice versa so let's look at the example on the slide here let's say we have a network with seven senders and of the seven centers four of them are running bbr and three of them are running cubic and given this network configuration and congestion control algorithm distribution"
  },
  {
    "startTime": "00:56:01",
    "text": "each of the flows are getting some share of the bottleneck bandwidth now let's say one of these senders alex decides to switch from bbr to cubic and while making the switch he essentially changes the internet's congestion control landscape and now he sees um different throughput on the network so in this case uh we're going to make the assumption that when alex does this switch if he sees a better throughput he's going to switch to the algorithm that is giving him better throughput or basically all uh the agents in our network are going to make a performance driven decision on which algorithm they want to run so in such a network if we are able to find a congestion control distribution where everyone where for everyone making the switch to the other algorithm gives them strictly worse performance that essentially means that this uh conjunction control algorithm distribution is the nash equilibria for that network or basically this is the fixed share of cubic and bbr flows we have in the network uh there's really no incentive for the number of bbr flows to increase or for the number of cubic floors to increase uh now a conjecture in the paper is that we think this nash equilibrium equilibria will exist in all kinds of networks where you have senders and senders running cubic and vbr flows and this is actually quite a big claim to make which is why we still say that it's a conjecture but we have good reason for making this conjecture so in the paper we go over the exact observations that we made based on how cubic and bbr interact and how these observations actually guide us towards making this conjecture but in the interest of time i'm only going to"
  },
  {
    "startTime": "00:58:00",
    "text": "discuss the key observation uh over here which will hopefully convince you guys that yeah there might indeed be a nash equilibria when you know n number of flows compete at a common bottleneck so um over here i'm going to plot a graph for a system where let's say we have uh symmetric senders so all my senders have the same rtt and they only differ in the sense of which congestion control algorithm they choose to run now we know from other measurement studies that when you have a very small number of uh bbr flows in the network they can get a disproportionately high share of the bottleneck bandwidth so i'm going to plot this as point a in the graph on the slide so on this graph basically on the y-axis i have the combined throughput of all the bbr flows and on the x-axis i have the percentage of bbi flows in each congestion control algorithm distribution so we can plot point a based on the observation made by other measurement studies we can also plot point b which basically says that when all the flows uh at the bottleneck are bbr flows they will basically use the entire bottleneck bandwidth which is really a no-brainer so we have two points point a and point b and we can also say that all uh all the data points between point a and point b will lie on some line connecting uh the two points and these different possible lines i've just depicted using the different gray squiggly lines of the slide so the interesting thing about this graph is that when you actually plot out these values every point at which your gray line intersects the fair share line that essentially signifies the nash equilibrium point in the network"
  },
  {
    "startTime": "01:00:01",
    "text": "so the fair share line uh i'm sorry i didn't go over it earlier but the fair share line is basically the line at which all your flows get uh the fair share so if bpr was getting the fair share in this network the the data points would follow the fair share line so let me actually go over why uh we actually claim that this intersection point in this graph is going to be the nash equilibrium so to do so let's zoom into one of these intersection points so at this intersection point basically what's happening is that the average bandwidth of all the cubic flows equals to the average bandwidth of all the bbr flows which is why neither of them wants to switch to the other kind but why do we say that this is actually the nash equilibrium well we say this intersection point is the nash equilibrium because let's say we move to the right of this point which would signify that a cubic flow in my current configuration wants to switch to running dbr so when we do this we will actually be transforming the entire system into a regime where bbr flows perform worse on the other hand if um there's a bbr flow in the network that wants to switch to cubic that would move the distribution to the left into a regime where cubic flows perform worse so in both cases the cubic floor does not switch to bbr because that would mean meaning to moving to a region where bbi performs worse and similarly the bbr flow does not want to switch to cubic because that would mean moving to a regime where cubic floors perform worse and because there is no incentive for any floor to switch to the other strategy at this intersection point this by definition becomes our nash equilibrium point now the graph that i plotted earlier was uh theoretical but we have uh validated these predictions through actual experiments so we had 20 flows"
  },
  {
    "startTime": "01:02:02",
    "text": "running through different length speeds and different buffer sizes and across these different regimes we plotted the normalized bandwidth for bbr and we did actually uh observe that at the intersection point where the line crosses the fair share line to the right pbr actually performs worse than cubic and to the left cubic performs worse than bbr so to so in the paper uh beyond the observations we also use these um observations to write down a couple of uh equations which we use to come up with the exhaustive proof for showing that a nash equilibrium will always exist when two floors are competing and the two floors have the choice to run either cubic or bbr but in the interest of time i will not go into the details of this proof but i will just say that the nash equilibria in this case depends on the buffer size unsurprisingly and the nash equilibria also exists independent of the fact whether your two floors have similar rtts or distinct rtts but beyond the exhaustive proof we also wanted to empirically validate some of the claims of our conjecture which is saying that the nash equilibrium will always exist so what we did was we set up networks with six nine and 12 floors where all these flows shared a common bottleneck bandwidth and in each experiment exactly one third of these flows had uh 2050 and etms rtts and this was basically to simulate flows of different rtts competing with each other and then we wanted to see how this actually impacts the existence of the nash equilibrium"
  },
  {
    "startTime": "01:04:00",
    "text": "so given this network configuration we basically ran all the two power n combinations of different flows running cubic or bbr and then we recorded that throughput and once we had the throughputs uh we used these throughput values to validate if any of those uh congestion control algorithm distributions were the nash equilibria so just to recap if we have a three-floor system and we say that cbc or the first floor running cubic the second floor running bbr and the third floor running cubic again is the nash equilibrium that basically means that uh when your distribution is bbc the first flow gets worse throughput when your distribution is ccc the second flow gets worse through part and when your distribution is cbb the third flow will get worse throughput so before i actually get into the graphs there were a couple of interesting properties that uh we found while uh actually calculating the smash equilibria in our experiments uh interestingly enough we found out that in all our experiments there was exactly one nash equilibria so there was one fixed distribution of congestion control algorithms where none of the flows had the incentive to switch to the other algorithm and we also found that in each of these nash equilibria um the congestion control algorithm distribution fell in such a manner that it was always the smaller itt flows that chose cubic and the large rtt4 flows uh decided to opt for bbr so later on in the graphs when i say that let's say that a sixth floor system has a natch equilibria where 50 of the floors are running cubic uh that basically means that uh 220 ms flows and 150 ms flow is running cubic and 150 ms flow and two atms flows are running bbr"
  },
  {
    "startTime": "01:06:06",
    "text": "so while actually calculating the nash equilibria we experimented with different link speeds and different uh buffer sizes and the entire point of this was to see how the link speed and the buffer sizes impacted where the nash equilibria lied uh predictably buffer size had the biggest impact on the on the distribution at the nash equilibria so when your buffer size was deeper you're more likely to have um floors opting for cubic rather than your buffer size when your buffer size is uh shallower and this makes sense because cubic is a buffer filling algorithm and it's likely to be more aggressive when you have deeper buffers uh we also tried changing the rtt distribution to see if that made any impact on where we earlier saw the nash equilibria and we found that there was very little in impact on where what the distribution of algorithms actually was at the nash equilibrium point so to summarize uh the findings of our short paper is that despite bbi's current throughput benefits we think it's unlikely that cubic is going to disappear soon and this is because we think that dbr's performance benefits that we see on the internet today are going to wane as more and more people on the internet start running bpr therefore we think that the internet is likely to remain a heterogeneous mix of congestion control algorithms uh we also think that a lot of our results are a good lesson in understanding that tcp performance is highly contextual so how your algorithm performs not only depends on the characteristics of your network but also who you're competing with and lastly i would like to know that you know we can make all the fancy predictions of uh having different kind"
  },
  {
    "startTime": "01:08:01",
    "text": "of nash equilibria but the internet actually does not uh follow economic game theory exactly so it's not a given that the internet will move towards nash equilibria but given uh the fact that a lot of people on the internet are likely to make the decision between cubic and bbr based on performance we think it's likely that even if we don't reach the nash equilibrium we are going to move in that direction now obviously there's a lot of future work to be done uh in this paper we want to come up with a formal proof uh for general inflow game we also want to look at the effect of more complex network utilities so in our paper we assumed a very simple uh utility function where every flow wanted to maximize its throughput but obviously that's not true on a real network uh flows are likely to care about both throughput and delay and the utility function is likely to be a combination of these metrics we also want to look at the effects on the congestion control algorithm distribution at the nash equilibria in the presence of bbr v2 uh multi-hop paths and eqm's and also how things change when you have very very deep buffers and much larger number of flows so in terms of very deep buffers and much larger number of flows we have been doing a fair number of experiments and so far what we've found is that the trend still exists that at higher bdps the the share of cubic flows at the natural equilibria will be greater but um there is one aspect in which the large flow experiments differ from the experiments that we've done in the short paper uh and that aspect is that when you have very deep buffers and very large number of floors things are not as nice and clean as having one nash"
  },
  {
    "startTime": "01:10:01",
    "text": "equilibrium point uh generally we found that there exists a region or a window within which you're likely to get a nash equilibria okay so thank you for your time uh that's all i have for you today and i'd like to take questions if there are any now thank you so much for your time irish this is a very very interesting piece of a number of people in the queue already but i'm going to uh ask a question before i get in there i actually took two quick questions one of them is that you seem to suggest cbb and bbc as two different uh um experiments and i want yes that seems to me i said the order in which uh flows entering choose makes a difference um so it it's not really the order in which the flows enter but it's just the fact that you know we have given all the flow some cardinality and we're treating all the flows separately uh so basically we are in this notation we're not assuming that flows are symmetric or there are a bunch of flows that have this that have same rtts so each flow here is distinct you can assume each flow has different rdt and therefore is a separate entity i see yeah okay well maybe i'll ask you later but i can't see the difference then between the first uh set up there the first experiment then the third one because to me they seem to be this okay so basically the first second and third floors might have different rdts understood which is why bcc and cbc are different handsome all right i'm gonna i'm gonna get out and allow the others to ask questions but i'm gonna close the queue here because we don't have a lot of time um all right go for a dn hi thanks uh this is this is quite interesting um i i do have some kind of um"
  },
  {
    "startTime": "01:12:00",
    "text": "extra complexity to add on to this whole thing which kind of um at least my thoughts on how this replicates or does not replicate the real world so um it's it's worth it noting that the most valuable flows are commonly the short ones that are very accurate uh the conical example is things like search and ads and in the case of my employer you generate a lot more value you certainly want more value provide something like youtube video um and given the amount of value it's actually kind of incentive compatible to make sure your uh your long-term flows are not too aggressive and that they move out of the way quickly uh when something that's high value like start your ads comes up um and the the other thing to note is it's not uncommon to have between 10 and 20 connections for a single page load on the internet um and when you're dealing with that kind of chaotic environment where nothing really gets out of startup or very rarely um it's it's very difficult to reason about the congestion control performance right like like the congestion avoidance phase is basically like irrelevant um you can largely like remove it from the congestion controller and it would like largely work the same for search um and a number of other major websites um it obviously matters intensely for youtube that matters intensely for you know a large flow like an uploader or download um but but i guess even for a given provider it might actually be instead of a compatibility like make your congestion avoidance scheme not too aggressive to make sure that like smaller flows which are higher value like our favorite um and so i think i think it's complicated um made sure that this was not a problem and make sure that there was no negative impact on search latency when we launched pvr originally um and we did a bunch of studies and couldn't find anything so as an anecdotal yeah yeah uh i i think all those are fair points and i completely agree with you that this is uh"
  },
  {
    "startTime": "01:14:00",
    "text": "extremely complex problem in fact you mentioned the flow durations and how flows of different iterations might have different metrics and they might want to optimize for different things so yeah all those things definitely complicate uh things a lot but currently from um what we are working on is the assumption that all your flows uh that all the flows that you care about are substantially long such that they enter congestion avoidance mode and then we want to see um you know how how performance is going to change for these considerably longer flows all right martin duke uh thanks it's a very creative um way of approaching the problem but um there's a little discussion in the chat uh because of course the term rtt is a little overloaded in our in our field right where sometimes it includes the buffer sometimes it's not so there are two ways to look at it that if that if the two two intuitions that that are that we're applying in the chat one is that um that like if the path latency aside from buffering is low the cubic is favored and therefore like low flow latency flows will low latency pads will just use cubic forever um the other the other one is that um uh as more and more people adopt cubic towards the nash equilibrium that that buffers that buffer occupancy drops and therefore that the the benefit of adopting bbr instead of uh cubic lessons and so the the nth person to adopt cubic um has no incentive because the other bbr people have already you know reduced the buffer occupancy so i don't know if you can speak to either of those intuitions if they're if both or neither or one of them is correct in your view yeah so generally we have seen that there are diminishing returns in both directions so whether it be for more and more people to adopt cubic or more and"
  },
  {
    "startTime": "01:16:01",
    "text": "more people to adopt bbr and we are in the process of actually um coming up with a model that can reason about these diminishing returns and i won't get into the details but at a very high level basically why we think this is happening is that when you have uh both cubic and bbr flows uh competing at the bottleneck what they do is that they section off different they basically section of different regions of the buffer so there you have one box that belongs to bbr and one box that belongs to cubic and as you put more and more flows into the cubic foot box or more and more flows in the bbr box the boxes don't increase in size linearly compared to the number of flows you're putting into them which is why we're getting diminishing returns and which is why you know the the rate of acceleration when you reach the nash equilibrium point keeps on reducing in terms of the the performance benefits that you get i just wanted to thank you first off for this work it's really interesting and it seems super useful um i just wanted to um amplify some of the discussion here about the workload that's being tested here i think a lot of us are thinking it would be really useful to um in future versions of this work to include a sort of mix of short and long flows and in particular the kind of effect i'm interested in is that if you have a dynamic mix of entering short flows then often every time somebody enters the"
  },
  {
    "startTime": "01:18:01",
    "text": "bottleneck they'll cause packet loss because they sort of try to figure out you know what what bandwidth and how much buffer space is available and if those flow entries all cause packet loss and those flow entries are um close enough together then that can basically prevent cubic from reaching its fair share you know because obviously it's going to be very sensitive to how far apart those lost points are so i think you might get very different answers to in the question of what cc is incentivized if there's sort of a mix of dynamically entering short flows so i'd love to see that in a future version of this yeah yeah so we're definitely considering uh different kind of workloads uh i think it's a great suggestion that we should look at a mix of short flows and long flows and uh see how things are changing uh in fact another thing that we are exploring currently is if you remove um right now we were just experimenting with long flows but we also want to experiment with what happens when you have video workloads and when you're dealing with video workloads ideally the utility function that we'll be looking at would not be the throughput but actually the qoe that your client is calculating so yeah thank you for your suggestion uh i mean all these uh all these different aspects of the problem uh i mean we have been also trying to reason about it a lot and in fact the biggest problem we're facing right now is really you know to come up with a nice systematic way to explore all the different things that can happen in this space hey uh great work hi hey uh this is great work uh uh so i'm assuming this work was done uh measuring bbr v1 with cubic it would be also interesting to include vbrv2 and"
  },
  {
    "startTime": "01:20:00",
    "text": "the second comment i had was yeah you're right that the evolution here might not be just based on maximizing throughput as a utility function so i think certainly reducing latency is one of the goals on the internet right so for all the players here the utility function might not be just maximizing throughput so that should be taken into account and the other thing here is that um there's certainly a benefit to standardizing on one algorithm in the long term so when you look at it from purely you know engineering efficiency point of view there's one algorithm that can give you better throughput and lower latency that's what everyone will eventually convert to um yeah yeah i agree but actually from uh from a design point of view i think it really it's quite a hard problem to convince everyone to you know switch to that oracle algorithm uh just based on performance because that would mean that uh okay in this graph we basically want our designed algorithm to always exist north of the fair share line and uh i mean that you can obviously design based on whatever utility function is you can design you know a utility-based algorithm that only maximizes that utility but yeah i i think it becomes a very hard problem to you know incentivize people to switch incentivize everyone to switch to just one algorithm because there are different because performance is contextual and also because generally the performance gains are diminishing as more and more people use it so from um from a design point of view i definitely agree uh that the best thing is to have everyone on the internet run the same thing but uh realistically we feel that it"
  },
  {
    "startTime": "01:22:00",
    "text": "that's something that might never happen and we might have to you know figure out a way to work around with these zoo of algorithms thank you again ayush we seem to not have scared you away that he came back and presently the second thing with us and we look forward to seeing i look forward to seeing more of this work in particular the last thing that you just said which is to uh i'd be very curious to see how you can if you can extend this to the zoo of algorithms that you found either on the internet and with that i'm going to move on to the next presentation thanks again ayush and uh please i wish it was on the iccrg mailing list so if you want uh uh to have any discussion on this please take it there i encourage that um i'm going to ask um praveen to come back on so that he can start his presentation next proving we are running behind so i'm going to ask you if you could try to keep it tight um hello everyone so um this talk is slightly different i'm going to be talking about implementation experience uh hopefully you know talking about two drafts um that have been presented to the iccr in some form uh we have an implementation update on on both both of these algorithms uh so a quick recap on our led bat uh so early bite is basically trying to bring the benefits of lead battery led by plus plus to the receive side of the transport connection uh the draft currently only talks about tcp but it could be applicable to other transports the key insight is to use the flow"
  },
  {
    "startTime": "01:24:00",
    "text": "control mechanism to throttle the pure uh in the tcp case it's basically shrinking the tcp receive window uh and and growing it based on uh running sort of an equivalent condition control algorithm on the receiver side um i missed books there when i said shrink so yeah we don't shrink the advertise window which is we just reduce it by the amount of bytes we received but we do tune the window over time depending on the observed events from the network like loss now why is this important uh so one of the reasons why just a center side conjunction controller is not good enough in practice is because uh a lot of a lot of software uses cdns a lot of cdns currently don't have for example that bet plus support it's harder to update all cdns to have the right uh congestion controller proxies can prevent effective use of led bat on the end to end path also if you have proxies on the path then effectively uh from the server side you're not actually measuring the right bottleneck and are able to basically throttle your sanding rate and the receiver has a very clear information about which download it things are background downloads compared to foreground download so there's advantages logistically in doing it on the receiver side um and this work is based on this draft which is currently active in iccrg so we did implement this in the windows tcp stack so we have a single api currently that enables both the sender and receiver side scavenger algorithm so all right let met plus plus an outlet bat uh our implementation on of our led by is based on lightbed plus plus so it includes all the uh additional mechanisms that were introduced in ledbet plus plus like uh using rtt measurements instead of one-way delay uh slower than reno"
  },
  {
    "startTime": "01:26:00",
    "text": "condition window increase with the adaptive factor as well as the multiplicative condition window decrease with the adaptive reduction factor uh the slow start is also modified to be slower than reno and then we have the periodic slowdown so we did simplify this compared to uh ledbet plus plus so uh we currently are doing one slowdown period uh per um basically a measurement interval so this was deliberately done to simplify uh the code uh we haven't compared one approach to the other but this just a simpler implementation and we also simplify the base delay implementation uh based on the outlet by draft so we do require negotiation of time stamps so what this means is that if the application did request or let bat and the timestamp negotiation may fail with the server in that case we need to reflect that up to the application so it can implement its own fallback logic to throttle for example using a fixed rate um currently we don't take an action if a data packet is received without timestamps after establishment so for example a middle box is stripping timestamp options uh we are currently not reacting to that that's actually a the standard says you know the receiver should drop those packets but we currently don't um this is an area where we would like to continue some investigations to see what the draft should recommend but we are collecting data on this to see how prevalent this is uh in the wild the other problem that we haven't mitigated is that the uh rtt is measured could be inflated because of bursts during slow start on the sender side uh there's no uh effective mitigation we can think of this on the receive side uh if there are you know this might be an area of research"
  },
  {
    "startTime": "01:28:03",
    "text": "so uh one of the things we observed while we started experimenting with this in in production was to find that there are several cdns which currently do not enable timestamps so we have worked with uh many cdns to enable timestamps when the client requests timestamps uh and i believe i think the coverage is much higher now we are currently doing these measurements with with the windows update downloads uh both for operating system updates as well as uh store downloads and we are aiming to share some data by the next iccrg i'm i'm leaving your question open here i'm a co-author on the outlet red draft this work is this presentation was about the implementation but now that we have an implementation based on the draft uh um i wanted to ask janna and and the group whether you know we should consider publishing drafts as an experiment uh we can take that during the q a um i will go on ahead with the presentation because limited time so i'm also going to talk about bbr v2 and our implementation of bb rb2 um so bbr v2 a quick recap is a model based condition control algorithm uh the goal is low q occupancy of the bottleneck buffer low loss uh and also some form of bounded aren't cubic coexistence so the way the algorithm works is uh continuously measure bandwidth round trip time uh packet loss and then ecn markings from the network and basically figure out a rate that the sender should be sending packets at there are some notable additions in v2 compared to v1 so the bandwidth probing time scale is adaptive uh loss and ecn have been incorporated into the network model um and then even when even when we are"
  },
  {
    "startTime": "01:30:00",
    "text": "application limited uh we want to basically adapt to loss and ecn information and finally uh because there is significant aggregation in networks uh we want to adapt this event based on estimating the amount of ack aggregation that's happening in the network and finally the the computed rate uh the sender will basically paste the packets at the computed rate uh so a brief uh overview of how we implemented this uh so basically the code is actually open source um it's available at this link so that's what we based our implementation on uh we integrated it as a conjunction control module in the windows tcp stack um it's currently available as an experimental knob in windows 11 insider builds um the raid based pacer was built into tcp so we're not using a pacer that's outside the tcp module and the way this works is that basically on each send uh we're computing an allowance uh based on the time since the last send and effectively if the allowance does not allow us to send us the send the package at that time we schedule the pacing timer to send the remaining data um so one of the challenges for us was because this code is also integrated into the linux kernel one of the challenges was that and because the code is not final it's still evolving uh we wanted to integrate this but still leave most of the code intact so that we can enable direct comparisons between you know future versions and be able to you know pull in those changes easily one of the simplifications we did was we currently don't do any ecn handling so we currently assume there's no ecm marking happening on the network um so that's just a simplification but eventually we'd like to add that in i would like to say that this sort of we called it a reverse engineering approach basically just looking at code and trying to"
  },
  {
    "startTime": "01:32:01",
    "text": "implement a conjunction control algorithm is something we've done for the first time and it was very very hard um lack of spec was uh a significant uh problem for us while developing this uh but thankfully you know we had good support from new and yuck on on email and we were able to get most of our questions answered but you know longer term it's probably not sustainable uh some of the early data uh so significant improvements in latency so uh particularly with cubic you know we see uh latency overshoot uh a lot beyond uh the base rtt uh but in this case we're seeing up to like 10x improvements in many cases um and some throughput improvements as well and these are like test cases in the lab that are doing uh wide area network emulation so um one of the interesting things was hey you know this this is primarily aimed to you know reduce latency let's run it uh at you know ultra low latency test cases where you have like back to back systems in the same rack and loopback test cases and we see that there's actually a cpu usage bottleneck so the algorithm is executing more cycles compared to cubic so this is something we would like to address by doing software optimizations and we also find that there are interactions between uh placing an lso lso is basically you know a tso or like sending a large segment out to the neck to improve the efficiency and we find that because of pacing there are fewer opportunities to do so and the and the size of the lso is actually smaller um we also did an inter region test uh in the in the azure cloud and we see about 20 throughput improvement and not much difference in latency and this is a low loss sort of uh not uh you know over saturated network basically so there's ample headroom and we see"
  },
  {
    "startTime": "01:34:00",
    "text": "basically throughput improvements but not much difference in latency in this particular test case uh significant fairness issues still so uh in all our lab tests we see that cubic dominates bb or v2 across a range of test cases so i think in bbr v1 we sort of had an opposite problem with some of the shell buffer cases but in this case uh we find that uh i think maybe we have over compensated a little bit and cubic is dominating bbr v2 um and currently it doesn't seem incrementally deployable of course if you have if you have a network in workload where you can guarantee that it's only going to be uh bbr v2 then it's certainly deployable but otherwise uh for any sort of incremental deployment uh currently we have significant finance issues so uh next steps here so uh neil did promise he'll bring a draft i think his talk is next so i'm looking forward to that but basically we would like to help review that draft and adopt it and take it forward and change our implementation according to the community feedback uh we'd like to resolve the fairness issues when cubic shares the bottleneck link and of course the cpu usage optimizations that need to be looked at and finally deployment production a big shout out and thanks to neil and newton for all their help on this work uh questions thanks so much praveen um i uh i want to say i don't we don't have time for questions there are negative time for questions but if you have a really quick uh question asked here or i would recommend that you take it to the chat uh if you can because we're already behind way behind on time do you have a burning learning question that you want to ask in person um not really just very quick comment so about our light pattern like that possible it's the gain factor and the"
  },
  {
    "startTime": "01:36:01",
    "text": "the target these are the two things i i just wanted to say that we shipped these two last year um in our tcp implementation and the gain factor that we are using and the target um we probably played with it a little bit because it wasn't working uh the additive increase wasn't going as fast as i was expecting and the throughput was really suffering so that's all i want to mention and maybe i can speak to praveen offline yes um and i would encourage by the way i would encourage even for conversations i would strongly encourage using the iccrg as a place where when you're having conversation across uh on clarification of implementations or various things if you're able to identify ccrg a lot of lot of good things come out of it because the community gets to see it there's a record of it it's available for others to see later when they're doing the implementation work so not just for this but for praveen and neil when you're having your conversations if you're able to have it on the channel they'll be very very welcome as well um thanks for your presentation um praveen that was very it's exciting to see this work move forward and i want to now hand it over to neil uh neil i know that we are uh behind so uh the rest of the time is yours so managers manage it as you see best but uh go for it thank you let me uh just get set here i will request permission to present the slides let's see actually get the right one okay okay uh let's see is everyone able to see the slides can you guys hear my audio yep visible okay great um"
  },
  {
    "startTime": "01:38:00",
    "text": "so yeah so i wanted to give a quick update um about uh bbr work going on uh inside our team um and uh ian will also be presenting uh some updates on the quick uh side as well um and i'll make this super quick um so i just wanted to talk a little bit about the deployment status code status and then give a super high level overview of the internet drafts that um i updated um the site last night so you can you can find links um on the iccrg list and the vbr devs list um and the goal here is basically to um uh you know talk mostly to talk about the drafts which are responding to requests from uh the itf community and other transport stack maintainers implementing bbr v2 that um you know obviously that would be useful to have a draft documenting the algorithm and you know this is always part of the plan and we apologize for this not happening sooner and we also of course want to invite the community to read the drafts and offer any kind of feedback both low-level editorial feedback algorithm ideas or bug fixes test results anything is useful and welcome so uh in terms of deployment status of vbr at google right now google internal traffic is either using bbrd2 as a default or it's part of a pilot program or we're gradually rolling out the rv to the swift variant which we discussed at a recent iccrg so the default right now is bbr v2 um using ecn and loss and bandwidth and rtt is as signals but we are doing a pilot that is small but growing of bbr a swift variant that is using a sort of network rtt estimate as a primary congestion signal in the manner of the swift algorithm"
  },
  {
    "startTime": "01:40:00",
    "text": "that was published at sitcom in 2020. google external traffic is still using bbr v1 by default but we're working on transitioning that to v2 um looking at av experiments qoe and latency data and iterating to to improve that for the launch and of course we're continuing to iterate on some of the areas where we know we want to improve including the issues that praveen mentioned about coexistence with cubic i also wanted to mention we have praveen mentioned cpu usage and we do have a apache set that introduces a fast path for ppr processing that we did find was useful in bringing the cpu usage um between to parity with cubic for our at least for our production workloads which we will be sharing when we get time um let's see the status of the code this is just a sort of repeat we have open source versions you can find the links in the slides so the bbr functionality is sort of split between two different drafts as it was in the original um release for pbrv1 so the first draft is a delivery rate estimation algorithm and that covers a bandwidth sampling mechanism that's used by both pvr version one and version two and it's also just generically available in linux tcp you can use those bandwidth samples no matter what congestion control is in place and being used the algorithm is largely unchanged since vbrv1 times there was a significant bug fix though that we've folded in and described basically the you realize that when the loss detection algorithm decides to retransmit something that is another point when you need to look for bubbles of silence before that event just as you would for cases where an application decided to"
  },
  {
    "startTime": "01:42:01",
    "text": "send something um anyway so you can take a look at the draft and give us feedback we'd be appreciated um so the other piece of this puzzle is the bbr congestion control draft itself that's also been updated to cover the current pbr version 2 algorithm right now it just includes the aspects relevant to the current public internet so the core model and the loss response and the strategy for coexistence with cubic and reno um the ecm part is only missing due to time limitations that's still used uh at our site and still part of the long-term roadmap we just haven't had time to put that in the draft so i'll do that you know we'll work on that as soon as we can um and you know the the algorithm is sort of documented um in its current state and of course there are some known issues um the one i mentioned here it corresponds to the the issue proving mentioned in terms of um cubic and bbrb2 coexistence or cubic winds too often um and then i'm just going to zoom through some pictures and well actually first in a quick outline as you would expect um the draft sort of covers first an overview and then a detailed rundown of the algorithm uh network path model how it sets the control parameters and then the state machine as the algorithm decides to probe the network um during its lifetime um and then then i posted a couple pictures that i think you know as pictures by themselves they won't have enough context but i am hoping they'll be useful to folks who are making their way through the draft reading the content and they could um you know use a little picture to help uh put everything in context and make it a little more clear so this is a sort of high level block diagram if you will about how the"
  },
  {
    "startTime": "01:44:00",
    "text": "bbr algorithm fits in with its inputs and outputs and the basic structure of the algorithm where it's ticking various input signals feeding it through a model and a state machine and then generating you know the three control parameters congestion window pacing rate and quantum or burst size um and then the next picture that i think might be useful to people is just a picture of the parameters in the model and how they fit together and we don't have time to go into detail but all of these are defined in the draft and at a high level you can say there's basically a set of parameters about the data rate that the algorithm thinks is appropriate and then some parameters about the data volume or you know amount of in-flight data and a key part of that of course is the bdp estimate but there are also other pieces that are discussed in draft and then there's just a little colorful picture of the state machine diagram that might be helpful as you're reading through the draft and then here's also a just a picture of a typical life in the day in the life of a bbr flow that sort of starts up and this shows its evolution through the state machine uh with the level of in-flight data superimposed on top so you can sort of get a sense of how these things interact and again we don't have time to go into it but this might help visualize what's going on in the text of the draft um so yeah in conclusion we've updated the drafts to cover pbr version two and uh we look forward to uh feedback if people have time to read um and high level feedback low level feedback anything in between it's all welcome um so uh yeah thank you very much and um if there are any quick questions i can take some but let's also leave time for ian to give his uh update about the quick side neil do you want to take questions now or wait until the end's done"
  },
  {
    "startTime": "01:46:01",
    "text": "yeah maybe let's wait until ian's done so i'll um see is there a way i can yield do i to just give it up uh folks line up in queue if you want but uh we'll wait until the end is done before we take questions go for it [Music] how do i uh oh there you go all right i was just grabbing my slides okay let's do this all right um so i i've been working on a variety of small changes to bdr p2 that don't actually they don't substantively change kind of the core algorithm and the approach that neil outland but they do make some tweaks kind of around the edges and some of those tweaks um may or may not end up being particularly relevant particularly in the public internet so i'm going to walk through three today um there are a number of others that i had time to outline and also a number of others that i do not yet have a good qe experience for um so yeah let me go for it um so the the tljr is basically that uh bbrv2 is very very close to achieving the same uh youtube video qe as well as search latency as bbr b1 um with these tweaks and a few others but these are kind of the most substantive ones probably the pilot um and you know there are still some some differences between the two in particular we we expect that there will be a bandwidth regression between the two algorithms just you know because that's kind of the intent um and i think that's deemed hopefully acceptable but that the key issue is that rebuff rate and those other metrics are not seriously harmed uh as as a result um consumer related research uh search actually has been a little bit"
  },
  {
    "startTime": "01:48:00",
    "text": "easier really so far it looks like um and possibly due to the fact that bbr2 is a little bit less aggressive uh search latency seems to be pretty robustly uh pretty close to neutral um so whenever we launch it we'll release full details but um you know it's it's not a major concern at this point so the the video queue is more the problem um so uh one challenge is uh today when you start up due to loss you set imply high to bdp um that that means that unless you are extremely nicely out clocked in an extremely smooth manner you are going to be sewing limited uh before you ever achieve the max bandwidth uh that kind of has resulted in bdp um and similarly once inflate high is low it can be very difficult or even impossible to grow it substantially um so this can sort of result in this sort of a bandwidth crash thing where you know you have a certain bandwidth and then you each crank down the invite high um and then you know the future bandwidths actually keep going lower because in flight high is so low that you can't actually achieve the bandwidth that you first achieved um so my my proposed fix is relatively simple which is you track the maximum bytes delivered in a round um not the maximum fight to be clear the max bytes actually delivered um so that should indicate that the pipe is at least that large because actually to look though when you put that many guys in lounge around um and as a result you have a lot less of a bandwidth crash uh when aggregation is present um and as well as like when you have excessive loss um so this seemed uh the change at least in our experience improved qe and had almost no downside the three trans change and retransfer it was extraordinarily small so i think niels actually has plans to like start experimenting with us at some point but it's kind of one of many smaller changes um there are a few other spots in the code that one could potentially start using this max"
  },
  {
    "startTime": "01:50:00",
    "text": "delivered in around or rely more on bytes delivered rather than like um inflight and other metrics and i have some experiments to work with those but the results are less clear than they are with this one where this one kind of was a pretty clear one uh so the next one that's an issue that neil's talked about a few times is early provo exit um and similarly like the lack of inflight high growth during promo which is kind of related um so probate up connects it early due to queueing uh the queueing criteria is you exit probe up if it's been at least minority in probe up and to be clear probe up in these slides means probe bw colon going up from needle slots uh and the bytes in flight are greater than 1.25 times bdb plus 2 mss um so if you're not in profit you don't really increase in flight high there are some ways it can increase but typically it's quite rare um again you never kind of re-achieve the max bandwidth so the the simple solution that we currently have in our code default enabled is you wait at least one round instead of min rtt uh in cases when there's a lot of aggregation the min rtt can be an order of magnitude um smaller than the smooth rtds that kind of ends up being necessary and then you put on the extra act um and the for the in-flight check um this isn't perfect this does increase pre-transit rates somewhat measurably it's still massively less than dvr1 but um this extra criteria is in some cases a little bit aggressive but it it at least is proof that um there are solutions out there that kind of like avoid this early exit um and don't cause like you know a huge amount of collateral damage um a newer idea that i wrote relatively recently is instead of looking at the extra act what about looking for a persistent queue of the course of the round um and so the code says if you've been in profit for at least a round and your"
  },
  {
    "startTime": "01:52:01",
    "text": "main bites and flight are greater than kind of the 1.25 bdp number that we're checking against um then then you exit in theory this might allow us to skip the application limited check in various spots there are some spots in startup is otherwise that we do have limited checks um if you were app limited and you still couldn't get your queue under the target um probably something's not going great um and so uh making code a little less sensitive to checks it's kind of a potentially side benefit of this um and yeah the last one was uh excessive time and probe rt this one's pretty simple basically when you're coming out of quiescence and you're in pro rtt you don't leave until like a full round has passed because you need an ack to kick yourself out of probe rtt um and so that means you know you're sending it like well less than half the bandwidth for a full round um we independently discovered this apparently anticipated quick um so apparently it was a good idea and um yeah tcp already has this this fix in as well but it's kind of worth noting just because it's um it does it can increase the amount of time and probe rgt when you're doing app limited traffic and we noticed there's a number of youtube flows where you'd get a chunk and then stop for a while and get a chunk and so on and so forth and when you finish the last chunk you'd still be in pro partsd um cool so that's it um i want to open up the floor for questions we have seven minutes left sweet thank you all right uh jonathan you're up neil do you want to join yeah i um i i was wanting to ask about the ecn since it isn't in the draft yet uh could we have a brief summary of how ecn information is incorporated into the algorithm and how it differs into how loss information is incorporated"
  },
  {
    "startTime": "01:54:03",
    "text": "right the um ecn information is interpreted in a manner that's very similar to dc-tcp [Music] is so i think that tells you all you need to know right now there's no it's not l4s specifically just because there's no you know it's not tied with the ect1 code point and it's not integrated with uh accurate ecn but the intent is to allow it in the future to be for us compliant down the road right and that differs from how lost information is incorporated right uh yes yeah did you what well i don't know if we have time to go on the specifics but the the um the reaction to the econ is very similar to dc tcp so there's a every round trip where there's a where there's ecn marking um there's sort of a multiplicative uh decrease thoughts proportional to the exponentially weighted moving average of recent ecn marks um so yeah so hopefully that's a quick summary there are also um details about it i think in in previous iccrg uh slides but i'll also try to update the draft to discuss the ecm part um as soon as we get cycles that's probably a good idea to put in the draft um video you're up next i was actually going to ask about the which form of acn are we using but uh neil already answered that um"
  },
  {
    "startTime": "01:56:00",
    "text": "and you said there's plans to use accurate ecn right if accurate dcn is supported so that's that's good um oh so the second question i had is oh sorry go ahead no i was just gonna confirm that yeah so i think if and when accurate ecm makes it into um linux or other os's then yeah the plan would be to use that signal yeah okay the second question i had was regarding uh the the points that ian noted about in flight high so it's not is is the in flight high not set again after the loss like i was assuming it would be said again you know once you have a last year you set it to bdp but then you have probably another stage where you would increase the in-flight high or am i understanding it yeah that that's that's right but i'll let you know yeah yeah you're correct that the flow probes again um but the tricky issue is that there's sort of a coupling between the in-flight high value that's that's bounding the amount of uh the volume of data that you're willing to put in the network and then your bandwidth estimate which is the rate the delivery rate that you can achieve and once you've decided you can only fit a certain amount of data inside the network then that implicitly bounds the rate that you're able to achieve and then because it's bounding the rate you're able to achieve that um bounds the your estimate of the bdp and then that in turn bounds the amount of data you're willing to put in the network to probe for bandwidth and causes you to not put enough data in the network in order to achieve a higher delivery rate and raise your bandwidth estimate and so you can sort of with the current logic that's in place you can sort of get stuck sometimes if there's a packet loss early on that can"
  },
  {
    "startTime": "01:58:02",
    "text": "limit your sense of the safe volume of data which limits your bandwidth estimate limits your probing and you can kind of get stuck in these cases um and uh you know there are a couple of different ways you can fix that and uh you know ian and i are both continuing to experiment with that it gets a little tricky though because then once you fix that which i think is is fairly straightforward to do then it impacts your coexistence behavior with cubic and reno because then you end up causing packet loss more often and so i think the algorithm might need to be a little more shrewd about how it schedules it's been with probing once it's more robust about pushing up to encounter loss in these cases thank you yeah that's a great insight thank you yeah halloween europe yeah hey thanks neil uh really appreciate the new drafts uh i will certainly go over them provide my feedback um one quick question on the logistics so do these drafts reflect the code that's uh in the sort of uh out of kernel repo that we are using as the basis or is this a reflecting work there is not part of that code um so that was one question and and i also had a suggestion for ian is to like you know whatever enhancements you've made you know bring them twice as dirty to the draft uh having quicken tcp converse to the same thing would be extremely useful so that's that that is the intent um as soon as these things are kind of proven to work well in both tcp and quick we're gonna make sure they make it into the draft but um we're we're so very far been a little bit conservative and tried to make sure that we're we're very sure that like any change we make really like works well in a variety of robust circumstances neil has a variety of simulations and uh test scenarios that i don't easily have available to me and and kind of"
  },
  {
    "startTime": "02:00:00",
    "text": "vice versa to some extent um and so we kind of need to make sure that something i changed that like works great for youtube doesn't like destroy like data center applications so we do need to kind of go back and forth and i think that'll take some time right well and then this is a question about uh which version yeah this should correspond very closely to the code that's currently on github there may be one or two small differences where there are two different behaviors available in the github code and the draft documents the one that we currently recommend but otherwise it should be very similar thanks i'll short my question um so uh you're implying that um the quick and tcp inflation is diverging a little bit and obviously there's a practical issue there but is is the different um are the differences solely related to the applications involved or are you seeing like protocol specifics that are driving these divergences i i mean i'll let ian give his perspective but my perspective is that we just have a team with multiple people who are both doing experiments um on a continuous basis and just in different code bases and you know quick is probably able to move get experiments pushed to front-end servers faster because it's user space code we have some different tools for the tcp side that can help explore other scenarios so i would view it as a collaboration and both both of us are running experiments to sort of move you know figure out how to improve the algorithm and then as ian said once we gain confidence with one particular approach we're going to make sure that the tcp and quick versions of the algorithm agree and likewise if there are other people out there doing bbr v2 experiments we want to make sure we're incorporating any good ideas that are out there in the external research community as well the only thing that's a really protocol"
  },
  {
    "startTime": "02:02:00",
    "text": "difference i can think of that we do observe some of is um as you probably know martin ack decimation act frequency whatever you'd like to call it is quite widespread frequent similar approaches are fairly it's read for tcp um but they kind of the the way they manifest themselves are a little bit different um and that along with like some of the scheduling being user space means that i think the traces and the ack lines tend to look a little bit different and i think kwik suffers a little more from the aggregation effects on the public internet due to a variety of these factors so uh my experience with pbrv one is quick was impacted more by aggregation yeah that's that's definitely all different yeah although it's interesting because uh inside in the data center case um tcp also does see massive degrees of aggregation because when your rgts are super small in the data center then the aggregation that the the nics are doing or your software is doing uh then becomes massive in the in the traces again at these very tiny rtt's so um so yeah so i think there's plenty of aggregation on both sides and and i think um yeah i'm sure we'll be able to coalesce on a single um final algorithm thank you for that and i want to i want to uh thank anil and others on on pushing in new draft and i will also say that if you don't i'm going to start requiring you to have all of your discussions on the mailing list so that somebody else can copy them and turn it into a draft but thank you for for for all the presentations and uh thanks neil ian and praveen for shortening your discussions and allowing the the the first timers to take some more time to present their stuff but i want to thank everybody for being here um this is a fantastic session and i look forward to more discussions i've just created a slack channel so use that"
  },
  {
    "startTime": "02:04:00",
    "text": "as well and [Music] we'll see you next time"
  }
]
