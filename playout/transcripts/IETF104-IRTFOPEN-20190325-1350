[
  {
    "startTime": "00:00:06",
    "text": "[Music] [Music] okay is there anybody who is if you\u0027re if you\u0027re not here and you\u0027re supposed to be raise your hands because we\u0027re waiting for you and we we didn\u0027t mean to put up art RTF we really still are internet research task force and with any luck I have now updated slides so welcome I am NOT going to give you slides I know you\u0027ve memorized the note well so we\u0027re not going to show it to you but there will be a test at the end "
  },
  {
    "startTime": "00:03:07",
    "text": "of the session and and just so you know I RTF abides by the ITF Snowbell so if you\u0027ve seen that multiple times you\u0027ve seen it for us as well we just changed it so Society F this is the day the week of a change of command in the IRT F and I want to introduce you to Colin Perkins if you don\u0027t already know him and welcome him he\u0027ll be running things from here on and we have three presentations of a NRP speakers the applied networking research prize they there are normally two but one of our presenters was unable to attend last time and they\u0027re going to be terrific talks about large-scale heart problems so we\u0027ll start out with Brandon SHhhh linkers talk Brandon is with Facebook and University of Southern California and his award paper is called engineering egress with edge fabric steering oceans of content to the world and take it away Brandon thank you for the introduction good afternoon everyone my name is Brenda shrinker from the University of Southern California today I\u0027m going to be talking about edge fabric a system we built at Facebook to deliver traffic to end users around the world so let\u0027s start off here with a brief overview of Facebook\u0027s network Facebook has dozens of points of presence around the world and interconnects with thousands of network now that rich interconnection with those thousands of networks offers Facebook two distinct advantages first it provides us with short direct paths to end users which means that we can bypass transit providers that have traditionally been part of the internet hierarchy second it provides us with substantial path diversity meaning that for any given end user we often have multiple distinct paths that we can use to send them traffic now let\u0027s dive a little bit deeper into what goes on in internet connection at a pop and I\u0027m gonna go into the challenges that exist for interconnection so first at every pop around the world we have one or more edge routers we establish physical interconnections or circuits between those edge routers and other networks so in this case we\u0027ve established an interconnection within end-user ISP and also with a Tier one transit provider next we use BGP or the border gateway protocol to exchange reachability information with those networks so in this example the end user ISP we receive routes to their end users across the network interconnection that we\u0027ve established with them and we also receive a route from that tier 1 transit provider third BGP ad our router 2 selects which of those routes we\u0027re going to use and in this case it selected that route through that from that end user ISP directly so what are "
  },
  {
    "startTime": "00:06:08",
    "text": "the challenges to using all this rich interconnectivity well our key objective here is to deliver traffic with the best performance possible but the challenge to doing that is that BGP doesn\u0027t consider demand capacity or performance in this decision process so let\u0027s take a look at what problems that creates we have here a simple example Facebook on the Left is trying to deliver five gigabits per second of traffic to the end users in the ISP on the right now our router is configured to use those short direct paths that we prefer and so as a result it puts all that load onto that upper path and everything\u0027s fine until later on in the day now demand has risen we\u0027re now at 12 gigabits per second of demand and again bgp at that router can\u0027t be adopt can\u0027t adopt a demand or capacity in real time it\u0027s simply not possible to express that with Beach of these policy terms so as a result the router continues to make the same decision and it ends up overloading that link waiting the packet loss and degrading user performance likewise BGP doesn\u0027t consider performance in its decision process the simple example of that can be seen here that upper preferred route now has a securities route on it so it\u0027s added 50 milliseconds of latency also some piece of equipment downstream is miss functioning or malfunctioning a DeMoss so in this scenario the route through that set that second route through that transit provider would actually be preferred it offers better performance but we can\u0027t configure bgp to adopt performance in real time and so we end up still poisoning all that traffic onto the West preferred poor-performing route now despite all these problems with bgp and how it doesn\u0027t account for capacity or performance it\u0027s still fundamental to interconnection and it\u0027s not going away anytime soon the thousands of networks that Facebook and other large content providers connect with all expect for us to use the BGP protocol so what that means we need to do is we have to sidestep B jeebies limitations and what I\u0027m going to talk about next is how we do that by shifting control from BGP at our edge routers to a software controller so I\u0027ve briefly gone over Facebook\u0027s Network and an overview over the challenges next I\u0027m going to dive deeper into our connectivity and the challenges I\u0027m going to talk about how we sidestep BGP limitations with edge fabric I\u0027ll then talk about edge fabrics behavior in production and finally I\u0027ll talk about the evolution of edge fabric and some ongoing work so back to those points of presence that we have around the world at each of those we have three types of connectivity first we have transit providers and transit providers can deliver traffic to the entire Internet at each pop we typically have two or more of these redundancy and we connect with them through a private circuit or sometimes known as a private "
  },
  {
    "startTime": "00:09:08",
    "text": "network interconnection then we have peers and we separate peers into two different categories I\u0027m going to go into detail and why we do that a little later but in general we have private peers on which there are on the order of tens per pop and again we connect with them through private circuits and we have IXP or public appears that we interconnect with via internet exchange points and those are on the orders of hundreds per pop and we interconnect with them through a shared fabric which means we don\u0027t have a direct circuit between our routers and ours so how do we prefer across these different routes what does what\u0027s our router is configured to do in general we apply this very simple policy we prefer routes from private peers over internet exchange point peers over transit providers now we prefer peers over transits because peers provide a short direct pass to end-users and we prefer private over internet exchange point peers because we prefer circuits that are dedicated to dedicated capacity between Facebook and the peer so as a result of that routing policy the vast majority of our traffic actually egress is through these private peers but that creates a problem because we cannot acquire sufficient capacity with our private peers to satisfy all demand we always try to do this but due to logistical constraints and business constraints is simply not always possible for us to establish the phys and capacity and as a result you can end up with scenarios like the one i illustrated earlier where during a peak time period or perhaps during a failure you have a link that becomes overloaded due to pgps decision process so how big of a problem is this well to understand that we did a 2-day study of 20 points of presence which is a subset of a production network and we identified circuits that would have been overloaded with B gp\u0027s default routing decision process based on the other policy I described earlier and overloaded here means that demand would have greater than the circus capacity what we found is that it\u0027s 17 out of 20 points of presidents they had at least one circuit that would have been over voted and 18% of all circuits across these 20 pops would have been overloaded at least once now the further dive into how big of a problem this is let\u0027s take a look at what the circuit peak demand is to its capacity for circuits where we predicted that the demand was going to be greater than its capacity at least once and what I have here is on the y-axis a CDF of circuits where the demand exceeded the capacity on the x-axis is their peak demand relative to their capacity so a peak demand here of two indicates they had twice as much demand as a circus actual capacity so two key points I want to pull from this first 50% of circuits had peak demand that was greater than one point one 9x their capacity and then 10% of circuits "
  },
  {
    "startTime": "00:12:08",
    "text": "had peak demand that was greater than twice their capacity indicating that some circuits ten percent in this case are drastically under provisioned relative to their peak demand so going back again bgp doesn\u0027t consider demand or capacity as a result in these situations where demand exceeds capacity we\u0027re going to end up with packet loss integrated user experience bgp s decision process in general doesn\u0027t meet our needs we don\u0027t want to created user experience and that\u0027s why we built edge fabric so next i\u0027m going to talk about how we sidestep b gp\u0027s limitations by using edge fabric and stepping back that again involves shifting control from bgp at routers at the edge of our network to a software controller so before I dive into what our implementation actually is I want to talk about our two key design priorities here first we focused on operate on minimizing op sorry maintaining operational simplicity which means minimizing change and minimizing system complexity second we wanted to have ease of deployment which means we want to interoperate with our existing infrastructure and tooling we have BGP routers at the edge of our networks like most network operators do we already have existing tooling for interacting with bgp so we wanted a system which could interact with that existing infrastructure so in general there\u0027s two key extremes in terms of how you can do routing on a network on the left hand side here there\u0027s what most network operators do today which is traditional routing I have my routers at the edge of my network perform our configure routes a per destination basis based on what they\u0027ve learned from BGP on the right hand side I have another extreme which is host based routing and that\u0027s where each host makes a decision on what the route of that packets going to be and then uses some signaling method such as MPLS or GRE to signal to the routers at the edge of the network how does how to handle that red packet so edge fabrics approach balance is balanced between these two extremes we have a controller that overrides B gp\u0027s decisions at the router and when our hosts provide hints on packet priority but don\u0027t precisely specify how the package should be egress from our network so what does this approach look like well first routers at the edge of our network keep selecting routes like they do today using BGP we still have all of our BGP sessions with other networks terminated at those routers so in this case our router based on all the information that\u0027s received have selected route a edge fabric also selects ideal routes but in addition to all that bgp routing information it also has access to other inputs in this case that means advanced policy information such as for instance us configuring based on business reasons or reasons provided to us by a peer prefix traffic "
  },
  {
    "startTime": "00:15:09",
    "text": "rates circuit capacities and brow performance measurements so edge fabric takes all that additional input and it also makes a decision and in this case is decided to use route B so our router and edge fabric have chosen different routes we need to resolve this the way we handle that is in this case edge fabric injects an override to that router using BGP which I\u0027ll go into a little bit later and forces that router to select the route the edge fabric prefers so edge fabric can perform two types of already it can override BGP decision in order to move traffic for a set of end-users so for instance we can say on a per destination basis override what BGP would typically do which is perhaps send that traffic via appearing link and instead send it via a transit link it can also move a specific class of end-user traffic so for instance I can send low priority traffic which is perhaps non video traffic over which would have traditionally or by BGP been routed over my peering link and I can instead shift at to a transit link so let\u0027s take a look at how the all of this comes together to prevent congestion in our network and we\u0027re going back to that example I showed earlier where we have facebook on the left trying to deliver 12 gigabits per second of traffic to this is P on the right and BGP by default is gonna put all of that traffic onto that upper link because we always prefer their short direct paths from peers and as a result that link is going to become overloaded so what edge fabric does is it understands that this 12 gigabits per second of demand is actually composed of two prefixes and in this case it understands that if it shifts one of these prefixes away and shifts that traffic to an alternate link in this case the path via the transit provider that it\u0027s going to prevent congestion on the peering link without causing congestion anywhere else so how this is work at the bgp level well we take that transit route that we\u0027ve selected we injected via BGP and then BGP at all of our routers is configured to prefer routes from edge fabric and we do that by configuring local pref on the BGP sessions for edge fabric such that the local prep of its routes is always the highest and less preferred so edge fabric monitors BGP decisions and overrides them as needed to prevent congestion in our network edge fabric is able to support a variety of traffic engineering policies because it operates over a variety of inputs and it can perform overrides on a variety of granularities and more importantly it\u0027s compatible with our existing bgp infrastructure which means that what we\u0027ve truly achieved with edge fabric is centralized control over the traditionally distributed BGP decision process going back to those design priorities I "
  },
  {
    "startTime": "00:18:09",
    "text": "introduced earlier edge fabric meets our goals of operational simplicity because we can always fall back to BGP at the routers of edge fabric fails it allows operators to continue to use our existing tools because routes are injected to those routers via BGP and synchronization is only required between edge fabric and routers likewise it meets our goal of ease of deployment bgp sessions with external peers remain at those routers we don\u0027t need to shift them elsewhere in our network and we use industry standards for route and traffic information such as BMP IP fix and s flaw so let\u0027s take a look at how edge fabric behaves in production and in particular criteria that we use to evaluate its performance edge fabric entered production in 2013 with the primary objective of preventing circuit congestion that we were seeing on period circuits at that time it runs per pop and it executes every 30 seconds meaning it takes in both route information and current traffic information and then determines based on its decision process where traffic should go and then injects routes via BGP it controls a hundred percent of Facebook\u0027s global egos traffic so one of the things we have to decide when a link is projected to be overloaded which means that edge fabric believes that the demand would be greater than its capacity is how much traffic edge fabric should move off of that link now if we move very little traffic only enough to get down to a hundred percent utilization then we\u0027re gonna end up with packet loss during bursts likewise if I move a significant amount of traffic and now I\u0027m at fifty percent utilization now I\u0027m getting poor utilization of those short direct links and I\u0027m not making good use of my capacity so in general we strive for based on operational experience is achieving 95% utilization and this allows us to have high utilization with tolerance for births and traffic now the key question here is can we maintain that utilization without any packet loss so we\u0027re gonna look at now is two key questions can edge fabric prevent circuit congestion and packet loss and can we keep the utilization of circuits at that 95 percent threshold what we did here is we measured across our network during that two-day measurement period and what we found is when edge fabric is shifting traffic away meaning that it believes that us link would be overloaded if it didn\u0027t intervene 99 percent 0.99 percent of the time there was no packet drops on that link likewise when edge fabric wasn\u0027t active which means it wasn\u0027t shifting traffic away from a link there was no packet drops so this means the edge fabric intervened when needed and it successfully prevented circuit congestion now the next question here is can we keep utilization at that 95 percent threshold and to analyze that "
  },
  {
    "startTime": "00:21:09",
    "text": "what we did is we looked at the circuit utilization against the threshold every 30 seconds for circuits where edge fabric was actively intervening and in this figure what we want is as much around that zero mark as possible because that means we\u0027re keeping the utilization right at that 95 percent threshold anything to the left means of the utilization is lower anything to the right means that it\u0027s higher and we end up with potential loss during bursts so what we find here is that the vast majority of the time we\u0027re able to keep the utilization of these interfaces or these circuits within 2% of that threshold so edge fabric is able to successfully prevent circuit congestion and packet loss and it can do that while keeping circuit utilization at this high threshold so now I\u0027m going to talk about the evolution of edge fabric and some ongoing work so I talked earlier about those two extremes of how you can have routing decisions made at the edge of your network at routers or you can have routing decisions made at your hosts and when we actually started off with that fabric we were using the other extreme routing decisions made at our hosts that\u0027s called host based routing so in this model what edge fabric would do is it would inject its decisions directly into our servers and then our servers would use MPLS DHCP or GRE depending on the generation of edge fabric this was to signal to routers at the edge of our network send this packet through circuit X now a key challenge there is maintaining synchronization you have to keep routing state maintained across all of your hosts and if what\u0027s a circuit X disappears my servers need to know that now that\u0027s no longer a valid option for them to route traffic via in comparison what we did today this edge based routing approach described as red fabric inject its decisions into routers at the edge of our network and overrides are enacted by those routers hosts don\u0027t signal the precise path that they want to track our pack to take instead they just signal to the router information about that packets traffic class such as this is a video packet so this means that we don\u0027t have any hosts synchronization which in our network drastically reduces the complexity of the system like edge fabric further we have flexibility with DHCP signaling because we can account for different classes of traffic and we can always fall back to BGP at our edge routers so both of these approaches provide the capabilities we want today but in general edge fabric is best aligned with the design priorities I described earlier next thing I want to briefly go over is about congestion beyond the edge of our networks and for this example I\u0027m going to talk about internet exchange points so internet exchange points allow networks to interconnect through a shared switch so in this case Facebook and another content provider may both "
  },
  {
    "startTime": "00:24:09",
    "text": "connect to this big ixp shared switch and downstream end user networks may connect as well so internet exchange points are often seen as removing barriers to interconnection I don\u0027t have to provision cross connects between me and all these other networks as I want to interconnect with well they also create a key challenge and to see why let\u0027s take a look at this example in this case both Facebook and this other content provider have hundreds of gigabits per second of capacity to this internet exchange points which in this case Facebook wants to send 8 gigabits per second of traffic to those end users and the other content providers 6 gigabits per second now the problem here is that is px they only have 10 gigabits per second of capacity as a result we end up with the same problem that Iowa\u0027s traded earlier demand here is greater than the available capacity or any up with congestion and packet loss now the key problem here is that these networks on the Left Facebook and this other content provider have no visibility past their network edge they have no understanding of what that other networks circuit capacity is downstream and even if they did they can\u0027t see each other\u0027s traffic from Facebook\u0027s perspective a gigabits per second is fine for a 10 gigabits per second link and likewise for the other content provider now this isn\u0027t just a problem for internet exchange points it\u0027s anywhere past your network edge you simply lack visibility I can\u0027t see into what a private peer or an internet exchange point or a transit provider has as problems downstream so what can we do to identify congestion beyond the edge of our network well we\u0027ve looked at a few different signals before we were looking for instance of prefixed traffic rates so I could figure out how much of Facebook\u0027s traffic is going to go on the circuit again that doesn\u0027t work here because trough cross traffic beyond our edge from other content providers is being mixed in and we don\u0027t know how much traffic they have circuit capacities oftentimes you aren\u0027t going to know downstream how much capacity does my transit have with the end user network I have no idea and what that means is you have to instead use route performance measurements you have to infer congestion from these performance measurements but that can be particularly challenging because you can see things such as latency increases and you aren\u0027t sure as to whether that\u0027s due to a path change or a change in client population or due to actual congestion likewise you don\u0027t know how much traffic to shift you have to continuously probe for capacity as downstream a failure may occur reduce capacity for 20 minutes and then be resolved so it requires a trial and error discovery process likewise those interactions with other networks also create complexity they may also respond to congestion signals and thereby reduce the amount of traffic they\u0027re putting on those links and you may increase your traffic and you may "
  },
  {
    "startTime": "00:27:09",
    "text": "oscillate together so it\u0027s very difficult to get a signal here as to how much traffic should I put on this link even if you know the current status of congested or not that doesn\u0027t mean that five minutes from now it\u0027s going to be in that same status so stepping back from all this what\u0027s really new here these problems in general have been known for quite some time BGP hasn\u0027t considered demand capacity or performance ever in terms of the inter domain setup and what we see as new here is the scale of connectivity traffic and quality of service demands that both content providers and end-users have and that brings new challenges and opportunities so stepping back again those rich interconnection I described earlier it offers content providers like Facebook a number of advantages providing short direct paths that can bypass transfer providers and substantial path diversity but in terms of meeting our goal of delivering traffic with the best performance possible we\u0027re challenged by BGP limitations because BGP doesn\u0027t consider demand capacity or performance and as a result we built edged fabric which has allowed us to sidestep BGP limitations by shifting control from routers at the edge of our network to software and the result has been a more efficient network and better performance for our end users and with that I\u0027d be happy to take any questions thank you there\u0027s a question that came in from the remote participant Bruno asked how does the controller tell a router to redirect traffic for a specific traffic class of traffic for example to use BGP flowspec so in this case what we have actually at each router there\u0027s multiple routing instances and those routing instances the DHCP marked packets arrive at each instance based on the dscp value so for instance gscp value 50 will arrive at Rowdy instance 50 and we inject routes into each of those instances if there\u0027s no route injected the router will fall back to the default route instance so this allows us to customize on a per destination per classic craft per classic traffic class as to whether or not we\u0027re gonna override the route he says thank you crystal clear don\u0027t forget to say your name\u0027s Cathy Aronson I\u0027m just curious do you have any mechanism for ingress traffic or is it just that your traffic is so heavily egress that this is the most important yeah I would say in general or traffic is is primarily egress so it ends up being a much larger problem for us traffic for ingress traffic it comes down to more of load balancing using DNS entries or other other things like that to send between pops Aaron Falk so it seems like one thing one of the effects of this mechanism is that it increases "
  },
  {
    "startTime": "00:30:10",
    "text": "sort of the dynamics of route changes for that a packets experience and I\u0027m wondering if you\u0027ve looked at the impact that this has on individual flows I mean my experience with Facebook is that most objects are pretty small but I but it it\u0027s unlikely that both paths are gonna have the same latency and so for a particular flow if you get switched you\u0027re going to have things like reordering and you know are there have you looked at sort of the how frequent traffic is shifting when you make these decisions how many flows are interrupted that kind of thing like what\u0027s the where how much of an impact is this sure so the way the decision process works today is it\u0027s likely going to contain you to select the same routes or the same destinations to shift as their vote increases so what say I\u0027m a hundred megabits per second over my capacity I\u0027ll choose X to shift now I\u0027m 200 megabits per second I choose X and Y and what that means is that once we\u0027ve shifted something over or likely to continue to shift it it\u0027s not always that we will there is some level of optimization there where we can change what we\u0027re shifting over time but as a result we don\u0027t end up with with rapid oscillations for the same set of flows and do you have any statistics on how much shifting is happening I don\u0027t have any statistics in terms of how much shifting is happening in general I expect that for TCP in terms of the reordering problem it would be very brief on the on the flows that were actively transmitting at the time and yeah we can take this offline further right hi Brendan I\u0027m Dave Blanca need idea about injecting the BGP prefixes and I guess the failure mode then is if the edge fabric doesn\u0027t work it falls back to BGP I was wondering about you gave an example where you showed two non adjacent v4 prefixes aggregating to more than the bandwidth on the on the 10 gig and you selectively chose one to offload say two and a half gigs of traffic or something um where did you get those prefixes from do you synthesize them from what you know is downstream or are they pre-configured the you know what what degrees of freedom do you have to make more specifics or things in the when you have a pure that has a bunch of aggregate prefixes sure so the the general aggregation here is we get samples from IP fix or s flow we aggregate them up to the most specific prefix advertised by a BGP and then we do break those prefixes apart again further so let\u0027s say I have a slash 20 which is one gigabit per second of traffic will break that slash 20 up into smaller prefixes / 21 or / 22 until we get down to a certain granularity so in this case I think we discussed in the papers splitting up until we get at least 250 megabit per second granularity which would mean that then when we\u0027re fifteen traffic we can shift in 250 megabit per second buckets so that allows us to keep that utilization at "
  },
  {
    "startTime": "00:33:11",
    "text": "that high threshold okay and that one other thing about the prefixes that was curious to me is you showed a V for example but you guys are heavily v6 and do both do you make any effort to find the corresponding v4 and v6 prefixes and move them together or do you treat them independently no the decision processes are independent we actually prefer to move v4 for v6 and that\u0027s because v6 we\u0027ve seen cases where you shift it to a different route that route is actually black hoing the traffic and then you end up oscillating because you shift away and back each time the prefix and this is likely just because of v6 routes being less chromed than before yes my name\u0027s Stuart Cheshire from Apple throughout the presentation you talked about demand as being a fixed thing like we have 12 megabits of demand or 12 gigabits of demand going into a 10 gigabit pipe right but if you\u0027re all the transport protocols I know like TCP and quick adapt the throughput and if you send a sustained 12 gigabits into a 10 gig pipe and lose 20% it\u0027s not going to continue losing 20% the sender\u0027s are going to slow down that rate so I I didn\u0027t understand why the normal congestion control algorithms to adjust rate did not slow down when they\u0027re too fast and conversely speed up when they\u0027re too slow if there\u0027s excess capacity TCP will speed up until it uses all the capacity because there\u0027s there\u0027s no such thing when I\u0027m looking at Facebook is loading a picture too fast right I want it to load as fast as it can which should be all the capacity that\u0027s available so to be clear here when I say to all the gigabits per second of demand that\u0027s what our controller sees as the demand to that prefix at that moment in time the reason that it can be greater than that links capacity is likely because we shifted traffic away from that link on a previous iteration so I may have let\u0027s say I have a single prefix that if I was to send all through this link it would be congested I would shifted it away on a previous iteration now it\u0027s it\u0027s utilization has been able to continue to climb and so now we\u0027re actually above what the Winx capacity is in terms of the transit report protocols reacting you\u0027re right but you\u0027re still going to end up with a poor user experience as you\u0027re still gonna end up with packet boss in order for those transport protocols to to react also many of our shorts our flows are very short which means that you have a lot of flow is constantly going through a slow start which means that they\u0027re going to end up interacting poor way when you\u0027re ending up with a lot of congestion in the link thank you I was cyllid a lot of krypton eat it\u0027s not a question it\u0027s quite a remark you are struggling with the old good old problem of a congestion link and informational about congestion so you stopped just one step before "
  },
  {
    "startTime": "00:36:15",
    "text": "reinventing frame relay and means of struggling organization in frame relay I hope it will be a result of your ongoing work and you propose something like bacon from the BGP Thank You Jo a plea this might be in the paper I haven\u0027t read it but you you implied I think when you were looking for congestion off net through exchange points or in remote networks the earing active probing till the folk ingestion conditions so I was wondering if you\u0027d considered pulling those kinds of insights directly from TCP when you already kind of have in a with a passive observation some indication of whether transport protocols are being throttled beaten before packet loss exists so we have an extensive TCP pipeline that we look at at these events on that\u0027s other work that were looking at publishing volodya for telecom I wonder that the primary and first control that you have for directing your traffic seems not to be mentioned and well okay feedback are not explained and the first thing that you I guess are doing is the server selection deciding to which of your server clusters at which location you direct the queries of these other customers and I guess I guess some of well okay essentially the predictions of how much traffic will be generated this way from each of a server clusters goes into edge cast into edge fabric as the estimation of the required or of the generated demand for the former volume but kind of I wonder is there no feedback that actually feeds back we are having difficult situation in what for one of the connections of server clusters and please read please reshape the distribution of traffic between the servers so to be clear there\u0027s there\u0027s two controllers here I don\u0027t talk about the other one there is a global controller which decides which point of presence around the world and end users traffic will be sent to and then there\u0027s this local controller which each point of presence decides how we\u0027re going to egress that traffic those two systems do have some cohesion between them and the interactions that you described do exist in terms of how we decide what the demand is for each point of presence that\u0027s not based on the "
  },
  {
    "startTime": "00:39:15",
    "text": "global load balancer that is based on IP fix or s flow measurements at that local pop so that allows us to get in near-real-time every 30 seconds exactly right now how much load there is at that location ask for questions by the jabber if you like but anyway thank you Brennen for a great talk [Applause] our next speaker is another NRP 2019 awardee florian struggle who is at max punk institutes our Brooke and your top his topic is bgp communities even more worms in the rowing camp thanks for the direction and thanks for having me here I think I should adjust that ok so this talk is on even nearer well so this talk is on a paper at IMC last year as already mentioned even more warmth in routing can and full disclosure part of this work was presented at last ITF meeting by Randy it grow but now you will get the full take so we were looking at BGP data that we collected and if you follow this this development you see a large increase of BGP community is being used over the last eight years we have seen more than two hundred ninety six percent of increase of BGP communities being used so individual values in BGP communities and I looked up yesterday is further increased so last year around 5,000 I guess we\u0027re using BGP committees and now it\u0027s up to 10,000 and we see 74,000 individual values for short communities so for me as a researcher this this means I should probably take a look what\u0027s actually happening there so what are we talking about we are talking about the short bgp communities you probably in earlier defined in our steen1997 they are a 32-bit value usually split in half so the first 16-bit being an AF number the lector 16-bit being a value where each PHAs is agreeing upon values where their peers what they should mean or what they are being used for so there is no strict semantics in it peers have to agree upon it on themselves and as you have noticed it\u0027s only 16 bits so we now have a number two which are larger than 16 bits finally we get the large community is defined in RFC 1892 and they are now at 12 buy it well you so know you have three three fields each with significant space to use them so 4-byte asn a SS can actually use communities now here the first four bytes are now defined to be a "
  },
  {
    "startTime": "00:42:17",
    "text": "global administrator so it\u0027s now clear that this is actually an a s number and this is the the network defining the meaning of the community so this is actually a good thing besides the confusion of the naming if it\u0027s a long or large communities we spotted other problems when we try to do our measurements the large communities were not really used in in 2018 we only found fifty one global administrator actually using them so nothing we could actually measure on internet scale this has has been become better and if you\u0027re interested in the uptake of large communities a meal from ripe has set up or have published an article where he looked into the development of large communities in the uptake so now we have around 120 global administrators that are using large communities so but how are they being used at all or in general communities can be split into two groups we have informational communities that have passive semantics they are used for location tagging where has this prefix we learned in which a pop RTT tagging we have seen and on the other side we have action communities that carry active semantics they are used for triggering back holding or actions in other ESS for example past prepending the problem here is without documentation of these P of these values you cannot see if this is an active or passive community or if the semantics is active or passive because it\u0027s already mentioned the peers decide themselves what these community values mean there is no bid indicating if it\u0027s active or passive or an action community and this leads into several lots of problems so given the increase in the popularity of bgp communities that we have seen and the ability to trigger actions in other assets with communities and relay information between ayase\u0027s the first question for me as a network researcher or science guy is what could actually go wrong in using them and we found out several things can go wrong and the first thing we we noticed is when we looked at how communities propagate people seem not to expect them to propagate widely through the internet although we have to RFC\u0027s actually defining how communities should propagate or should not propagate our 15 1997 states communities are transitive optional attributes so they should be forwarded to your peers an RFC 74/54 says you should scrub communities you are using inside your network so he cannot be manipulated from outside but forward for any communities by other users so it should be expected date that they are actually propagating through the internet still a lot of people do not expect this and a lot of trends providers don\u0027t actually forward them we only found 14% of transit providers propagating received communities and yes "
  },
  {
    "startTime": "00:45:20",
    "text": "this value seems to be small but the Internet graph or the EAS graph is highly connected so you actually end up in communities traveling quite quite a lot but still many people do not expect them to propagate it widely and the problem here is that this leads to some potential for misuse as they are propagating through the internet and can trigger actions multiple hops away and there is no way for an operator to find out if this is intended or not this leads into a problem you cannot say well this is traffic management and this is legitimate or this is an attack and we ask ourselves the question if there are also unintended consequences in this combination of b2b communities being transitive and forwarded and used for actually changing routing decisions and our assessment in the end is yes there is a high risk for attacks as we already see some attacks as well so what we were looking at of course we took all the publicly available b2b data we can find and in the end we find that 75% of BGP announcements that we looked at heavenly\u0027s one BTP community set and in 2018 it were five to six thousand a ESS now it\u0027s more than ten thousand yeses that make use of is short communities now taking a step back and looking at the propagation again what we can actually measure or what we cannot measure we have this very complex topology of four ESS where a as one is announcing a prefix P and this is recorded in a s4 which could be a collector or just a simple peer with the is path for three two one as expected and now a s2 is taking the prefix P where the community in our case 2 colon 3 or 3 so 2 is the a s actually defining the meaning of this community and this will be transported finally to a s 4 so it\u0027s 4 is recording this community in its routing decision in its rip so a s2 has added this informational community now a s2 is also adding a community for signaling it or triggering an action in a s3 it\u0027s upstream this is also for what is 2 a is 4 so both of these companies are now present or visible in a s4 but a is for cannot know who actually has added these communities and so can we but we needed this for our measurements so we had to come up with a solution for us we can only death in fear infer which a s is adding a specific community by assuming that if the S value or the s number present in the community is actually the as adding the community we will get a lower bound of the travel distance or of the a s hop count this will lead us for the community to 2 to 3 or 3 with the correct travel distance of 2 a s hops and with the other community 3 1 2 3 with the wrong assumed travel distance of one of also 2 but it "
  },
  {
    "startTime": "00:48:24",
    "text": "actually off sort of one because it\u0027s just one s off although correctly it would be 2 for us this lower bound of the distance of one hop is sufficient for our for our work so if we plot these values which again is the lower bound of travel distances we end up with this EC DF on the excess pieces you see das hop count and we find that 10% of communities have an Aes hop count of more than six so they Traverse more than six different guesses from where we assume them to have been added at more than 50% of communities still traverse more than four ESS and if you compare this with the mean length of a s path that we have observed which is around 4.5 or followed 7 this actually means it travels almost through the whole internet and the longest community propagation we have observed were 11 a s hops so they do propagate through the Internet now looking at another very complex escapology I use one again announcing a prefix to a s 2 and adding a community 3 1 2 3 to inform a s 3 or execute path depending there you will notice that this community value is also propagated to a is 4 again and although it\u0027s only intended for signaling something towards a 3 I use 4 is also receiving announcement with this community so we end up with 2 different s paths and in the first case we for our for our research call this community be on path because the a s value from the S community from the community is present on the a s path that we record in s 3 in is for recall this community being off path because the a s number 3 is not present on the S path it could also be that the a s that is being signaled for is further hops away behind is 4 but in both cases this would be called off path because you could a s number is not present on the record yes path and if we now take the right part of these community values separate them by unpadded of path and plot this we end up with this distribution on the left side you see a quite a number of community values in the off paths communities that are related to like holding remote record black holding and on the other side on the right hand side you see very even numbers that look like provider like operator assigned and easy to remember and we think that this comes from the fact that II that a essence that are not implementing black holding will just forward black hauling with communities compared to yeses that do black holding which follow the a black hauling RFC and do not further propagate his communities so they will not be visible of path now coming to the "
  },
  {
    "startTime": "00:51:29",
    "text": "experiments that we did to show that there actually are some problems out there in internet all of the experiments were done first the lab environment and then validated on the internet with of course operator consent and and I will show two different scenarios in this talk there are more in the paper and the configuration of our others are available publicly so first going back again and giving an intro how does remote trigger black holding is supposed to work so we know we are talking about as one is announcing prefix to its upstream is to and then receiving traffic this is expected we have here sometimes you have the problem that you receive more traffic than you actually you want to attract we call this a denial of service attack and one mitigation this is one signaling to as2 that wants to black hole prefix usually this is done in band in the same bgp peering session then the normal VDP announcements are being sent but there are also cases where it\u0027s a special bgp session which has other problems but not the ones mentioned here so as one is announcing the prefix p tagged with the black hole in community to signal a is to that it should drop traffic is to is of course still announcing the prefix p to all its peers but without the black hole in community now what happens is is to is now dropping traffic and all the routing the traffic towards P and his body routers and the link between a s1 and s2 is released from y\u0027all of service traffic and is usuals usable so you sacrifice parts of your network or parts of the prefix IP addresses to you still keep only of the other prefixes and servers reachable this is how it should work what we noticed is that for this to be used in a secure fashion you need to employ some safeguards of course the provider that is providing black holing has to check if the customer is actually allowed to black hole these prefixes so if these prefix are owned by the customer or the customer has permissions to black hole them and this leads to the fact that you need different policies for customers and peers different access control lists and leads to a lot of configuration overhead for for a secure usage of remote trigger black holding and of course receiving such communities you have to add no advertise and not export to the announcement so you don\u0027t propagate it further and we also noticed some providers translating black holding to the black hole in communities of other upstream so you were not even able to do selective black hole because they were translating it and announcing it to their peers as well translating the actual rail user now what should not be possible is depicted here we have the same topology but now s2 isn\u0027t the role of an attacker and is to should just be a backup path to to the prefix of a as "
  },
  {
    "startTime": "00:54:30",
    "text": "one but a s2 is able to actually at the black holding community of though it\u0027s not on the best path so as to is announcing to a3 that prefix P should be black hold and we notice a3 is actually doing that although the best path is through a as one and a as one as the origin for P is not actually requesting any black holding and the other problem that we noticed is that this is even possible if a s2 is not involved in in any connection to a here\u0027s one at all so as long can just hijack the prefix P and announce the prefix P with the black hole in community said and we noticed that in some cases we are able to circumvent ACLs and prefix filter lists because the black holding community is checked before any prefix filter lists are applied so we were able to confirm this on the Internet it works multi hub and it\u0027s hard to spot because the community values are usually on one Accord reasons for that we found is the black : prefix is more specific so you need exception rules in your configuration to accept I was left 32 so essentially everything that smaller than / 24 and some providers to check the black : community before applying any prefix filters and we even found some configuration guides on the internet which had this problem and they were the example configuration provided and the problem here there is no validation for the origin of the community every is on the path can add the black hole in community for the upstream provider now yesterday yup Snyder\u0027s gave a talk at the IPG where he presented the mitigation for this if you would check that the period is announcing the black holding or the prefix with the black holding community is on the best path and only then accept the black holding this is one possible mitigation for this attack so if you are interested in that you should check the recordings of that talk so if you only accept a black holding if the period is announcing the black holding for a prefix is your current best path to their prefix these problems go away the second attack we were able to do was a traffic redirection attack again as one is announcing its prefix P and you see the current best press path from a or 6 to a as one is through a is 3 on the lower side of the topology now a s 2 is our attacker announcing prefix P with community to do path prepending in is 3 which leads to the longer path over a s 4 and 5 to be the preferred path for a o6y this attack could be interesting well one thing could be there is an effort between a s 4 and 5 and if even if you would identify that a is 2 is your attacker and you would screen the "
  },
  {
    "startTime": "00:57:30",
    "text": "network of a s 2 you will not find any network tab there because they on purpose redirected to traffic to a is 4 \u0026 5 Grady actual network campus could be a s 2 is being forced to cooperate here and the other thing it is it could just be a denial of service attack because it\u0027s known at the link between is 4 \u0026 5 is a very thin link with less with not as much bandwidth as would be needed so by redirecting traffic there you could actually fill that link there and after I gave this presentation at the right meeting we were actually approached by dying and they appointed us to an article where they found our attacks that today are actually already using communities to foster propagation of Heydrich so the attackers found out that by setting specific community values their hijack will actually be propagated more in the bgp network so we already see attacks using communities so what now we identified several well topics where discussion may might be useful we found prominent isset e transitivity standards documentation and monitoring of community usage starting with authenticity I mentioned several times that every hey s that is on das path is able to modify add or remove community values on announcements on on in BGP and there is no attribution possible it means even if you found out that there is an incident you cannot find out who is actually responsible for that we all know rpki but intentionally this is not able to secure communities because we do want guesses on years path to modify routing and to add pathway pinning for example on the other hand we also see that operators rely on the correctness of community values because they are basing policy decisions on for example where a route has been learned and large communities are there but they only partially improve the situation because all of these points still apply to large communities they only fix the first part of being an IAS number so the question is how can we achieve authenticity or at least attribution so after an incident you know who you have to talk to to prevent further problems in the future another thing that could probably - big discussion series we now communities are very helpful in debugging because you know what is happening in the network and why certain networks are forwarding traffic in a certain way and they are indeed a very easy law over to communications channel and widely news we still only see them being used one or two hops away you usually do not signal black holding five or six hops away or you do not usually need to inform peers "
  },
  {
    "startTime": "01:00:33",
    "text": "six or seven hops away so on the other hand you have a high risk for abuse in communities being transitive so the question is do we have a high risk here or do we have more benefit do we need a discussion between benefit and risk using full transit or allowing community values or communities to be full transitive monitoring is another field full of MIS misunderstandings we know there is no global state in BGP it\u0027s a highly distributed system and even if we look at all of the rod collectors that are available we will only see the end result recorded at these collections we do not know what has happened on the path between peers even if you are able to look into lookingglass as many earlier it\u0027s very hard to spot differences so inferring modifications between the origin or das setting communities and the collector is almost impossible and even if you would be able to record all of these changes you still have to probably do not know what these community values actually mean and there is no general way for attribution of changes or recording who actually change anything so monitoring community values to detect abuse is extreme be difficult in this in this field and we have the other great feel of standardization in the short communities ASM : value is still just convention we can we cannot really be sure that the the first value is actually an IAS number and there is no defined semantics I know there have been a lot of discussions in the past that all came to the point that well we cannot define semantics here but this leads to these problems stated and by by communities being used both for signaling and for triggering and not being able to distinguish what is what you cannot even filter communities in a sensible way and until now they\u0027re only a little it\u0027s only a limited set of standardized communities and you cannot be sure if for example some AES is not actually using some arbitrary value for triggering backhauling so standardization here might be a thing that that should be further further pushed forward doing our research we found another very large problem this is with documentation because all of the yeses can define their communities themselves there is no need for documentation there is no central point of documentation we found that some of the guesses are the are documenting then in who is now your are databases on their websites some are only providing community documentation in customer portals or not even at all so if you see a community you cannot find out in an easy way what they mean and even if there is documentation it\u0027s often in you\u0027re not "
  },
  {
    "startTime": "01:03:33",
    "text": "touring in natural language and parsing this is impossible we tried we failed if you have a very limited scope for example trying to find out the community is used for geolocation or for for geo tagging of prefixes you can of course look for city names airport codes things like that but parsing community documentation in a general-purpose for a general-purpose application is not really feasible so documentation is very limited and fragmented it\u0027s very hard to actually find out dictionary or fight the dictionary for community meanings again things are happening on that on that I learned that the tag is is has internally developed a system what they call for community structuring they are only using string representations instead of community values internally and these string representations are then translated to short and large community values for their auto-configuration an example would be tact at origin dot country dot de where de is a parameter for the community definition Tec origin country so you see it\u0027s a hierarchical hierarchical a system very it says well this community is a tagging community it\u0027s a it has passive semantics it is taking an origin on the country level and the country is Germany and their system allows the definition of parameters to communities and these parameters with the committees together are documented in one\u0027s system they have working code and they are using this in production already right now they have an internal internet draft like document and if you are interested in that you probably should should talk to Whittaker who is sitting there and laughing so I think this is a great way for actually starting document communities in a sensible way because you don\u0027t have to operate with metrical numbers and you can actually distribute these documentation and talk about policies and filters with your peers because you have to you can talk with with with strings and no magical numbers and even if you have other router configurations you can still use these extreme representations and you know what you\u0027re talking about [Music] then we came up with some recommendations for Pareto\u0027s based on our work of course as RFC already States you should filter all informational comment community values that you are using that carry your a s number so if you are using communities to check where a prefix has we learned we should scrub these communities when you receive them from your peers because they are defined by you internally and used by you internally it might be useful to come up with agreements with their downstreams so to define what they are allowed to do with your upstreams if they are actually allowed to do path prepending with your upstream for their prefixes of course publicly documenting "
  },
  {
    "startTime": "01:06:33",
    "text": "the communities you are using is key to enable other a SS to filter action communities to you so if I have a customer where I know he might be playing around with with BGP I might want to filter things so he cannot trigger things in my upstream but you need agreements for that if you are able actually allowed to filter this or not or define what they are allowed to do and one one thing one request if you are providing public looking glasses which is a good idea please also show the community values that we that you receive otherwise it\u0027s very hard to debug things so coming back to the general problem PDP communities are currently the only feasible way to realize signaling between a asses but the problem is that is secure usage requires good operational knowledge and diligence we do not think that a very over complex system is really suitable to secure the shortcomings of bgp communities but we have to be aware that there is a problem and while everybody in this room is probably able to handle this and do everything correct all the time we cannot rely on that on a global scale there are a bunch of people out there who do not know what they are doing and there will never be a world in in which everybody is doing everything correct so the question is do we still can rely on or do we still want protocols that allow people to make mistakes that will break other people\u0027s network or do we need an evolution for the calls here that are less fragile and trot in and more usable or other or with other to to to prevent people shooting themselves and others into the in the food so wrapping up communities are widely news they are used to realize policies they are needed but they heavily rely on mutual trust between the peers because there is no less enticing and security in place there is no attribution attacks are very hard to detect and one take away from our experiments we did some prefix hijacking that was reported on Twitter but nobody actually spotted our agree direction attacks based on communities so we cannot be sure if there are other people already doing attacks using communities if you are interested in more details the paper is available here\u0027s the link and with a picture of my cat I\u0027m happy to take questions and I would start with a question to what extent has the people who done who did this research brought forward and these standards and the working group proposals to to make the changes again have you have your have the group who wrote this paper brought "
  },
  {
    "startTime": "01:09:34",
    "text": "forward any work to the routing IDF side of the world we did not yet because we are in the state where we well assessing the problem and well showing everybody there was a problem yeah okay and I don\u0027t know how to fix it but I thought this might attract Randy to the mic oh one of the co Rep army Randy Bush iij anarchists I\u0027m one of the co-authors strongly objected both in working group last call and i etf last call to the black hole well-known community okay anybody else have a question well not not so much question but one comment is when you say there is no authenticity that is not completely true the thing is one has to recognize here that Allah cave a community system is offering a range of stuff where people where operators can be creative and you have in the global system you have to control the creativity and the interaction of the various create creators but in the end you have to see that yes you always have bilateral relations that are mapped into BGP neighbor neighbor neighbor ship relations and yes what is exchanged there should be seen as something that is essentially just bilateral and yes if you if you want to be a responsible actor in the old system you have to really control what you are doing with your neighbors and if you really take that understanding you can actually start to build stuff that says well ok you and me are peering I\u0027m a responsible person I make an agreement what we are doing on our relation and for that if you and me are doing a decent effort at controlling at doing the right policy for implementing our agreement we actually have a chance for using that as fairly trustworthy and I might even I might even go out and offer you an agreement in which I in which I "
  },
  {
    "startTime": "01:12:35",
    "text": "promise you that I am doing stuff where I\u0027m related where I\u0027m relating to you in a controlled manner the communities that wendy is sending to me is something that does not work recursively overview wall topology but with that very limited and closed understanding one actually can do stuff and yes having better documentation have a having more tools for doing it will help a lot to do this right because a lot of the spreading of dubious information that you have observed is related to the fact that many of the operators just at the time when they\u0027re 10 years ago did their policy they didn\u0027t have good tools and they did not really take care of their responsibilities but but that was what I meant with you have to talk to your downstreams to make fear what you are allowed to filter because if you don\u0027t have any agreement exactly and you just strip all of their communities yes this could lead into other problems if not only and that has that has to go that has to go with a very strict understanding into the BC piece that we are using before you start deaf people should forward around the blue sheets and this will be the last question for this talk thank you for bringing it let\u0027s spell known and you\u0027ve been facing this for like 15 years probably since now if you start doing cream of blood calling the mitigation today is mostly basic hygiene you take care of what you accept and you take care of issues and there are some gentle consequences and very eautiful stuff things like banded communities that are very useful in data centers they were made non-transitive just voices issues while most data centers use ebgp we can propagate this community so there\u0027s work definitely needed that would allow us to use very stuff thank you thank you okay and last but not least is our a NRP 2018 awardee from who would have been in Bangkok but for visa issues oh and that\u0027s Arash ma la vie hockey and he is at Thousand Eyes now and the work was done it at Northeastern University and taking a long look at quick oh can you click on the size for him it should be "
  },
  {
    "startTime": "01:15:35",
    "text": "in there is it not okay hmm yes this is the case where we talked about at lunch where you give the talk without the slides this is where the a research group chess can work we\u0027ll add one note I\u0027m gonna go to some background material and history of quick oh no to bring up to speed those folks who might not be super familiar equate perfect so if you are an expert done quick and you know the background material I apologize in advance please bear with me as I go to it all right so I\u0027m gonna be talking about taking a long look at quick this was a measurement work that appeared at IMC 2017 and I don\u0027t think I need to convince anyone in this room that interconnectivity is important but just to set the stage and put things in perspective in 2015 3.2 billion people had access to Internet obviously that number has increased over the past four years but in that same year the number of people with running water was less than now these two numbers next to each other are I find them depressing but for reasons that are out of the scope of this talk but it it emphasizes the importance of Internet connectivity use it in our personal in our perfect professional life virtually every business depends on the internet and their viability is tied to the performance of the network that they\u0027re operating on so naturally there\u0027s a lot of effort to try to improve these networks and make them more reliable and more performant right EF is one of those efforts and we do a lot of things we come up with new protocols we use traffic management techniques to make sure that our networks are utilized in the way that everyone\u0027s demands are met and we even design our applications to adapt themselves to the underlying network so we increase the user experience improve the user experience and well quic is one of those effort it\u0027s it\u0027s a transport protocol it stands for quick UDP internet connection and it started in Google and it was basically a transport protocol design with today\u0027s needs in mind quic was designed for a bunch of main reasons the first one was to facilitate rapid deployment what does that mean if you think about HTTP you have HTTP hopefully you have TLS underneath and it\u0027s running on TCP which is your transfer protocol and as you all know TCP is part of the it\u0027s implemented in "
  },
  {
    "startTime": "01:18:37",
    "text": "the kernel so it\u0027s in the kernel space what does that mean that means if whenever you have a big change let\u0027s say for TCP and you want to deploy it at scale everyone needs to update their operating system and we all know that could take a long time Windows XP that are still around as an evidence to that effect so quick solves that problem by implementing in the user space so now what this means is that is that whenever you have a new version of quick all you need to do is let\u0027s say if you\u0027re browsing the web and you\u0027re using a browser all the users need to do is to update their browser and then they have the new version of quick obviously this means that a lot of things like a lot of guarantees that the people provides like reliable delivery now all of those also have to be implemented in the user space which I get to in a little bit what that means in terms of performance but overall this helps with rapid deployment another main reason was quick for quick which Google never shied away about pointing out was to avoid ossification by middleboxes we all know there are many middleboxes in networks these could be nuts or security firewalls or could be web caches and many other applications a lot of them do claim that they improve performance perhaps in some cases they do but there\u0027s also a lot of evidence that they actually do more harm than than good one of the examples that I find very interesting this is this was a joint work by Google and team a while it was a few years ago it was presented at velocity conference where they basically looked at YouTube\u0027s traffic over t-mobile\u0027s network and how does it interact with their web proxies and this is summary of findings from their slides they basically found that it\u0027s better that YouTube traffic does not go through the proxies because they\u0027re hurting their performance and I don\u0027t want to point any fingers to t-mobile YouTube this is this is not an issue isolated to that another example this is taken from a CloudFlare blog post where they were basically saying we had TLS 1.3 enabled for a while but no one was using it because the browser\u0027s were not supporting it and they were not basically support turning it on because middle boxes were breaking it and to be fair it wasn\u0027t just middle boxes there were other other issues that prevented TLS 1.3 from being deployed at scale but middle boxes were not helping TCP fast open is another example that a lot of folks believed it didn\u0027t it never got deployed at scale because of middle boxes and the list goes on so and all of these things can happen because in TCP all of your headers are in the clear so middle boxes can see them and act upon them they can modify them draw "
  },
  {
    "startTime": "01:21:38",
    "text": "and add headers or break your connections into to all the things that you\u0027re familiar with whereas in quick pretty much everything is encrypted so you take all that from mailboxes they can\u0027t they can\u0027t do any associations or meddlings and finally quick was proposed to improve performance I just a side note here I have performance for HTTP traffic I should mention that quick is eventually going to be a general-purpose transfer protocol but it would start it started with HTTP in mind and it\u0027s that\u0027s its biggest usage right now it\u0027s very integrated with HTTP so throughout this talk whenever I say quick we are basically gonna focus on HTTP over quick so whenever I say quick I mean HTTP over quick so quick improves performance by a number of optimizations the most famous one is zero RTT connection establishment if you\u0027re familiar with TCP you have that three-way handshake to establish a connection before you can send any data if you have TLS on top of TCP as you should welder there\u0027s gonna be more our titties now and quick tries to achieve zero RTT connection zero RTT connection what that means is that you can start sending data from the very first packet obviously that doesn\u0027t always work you should have contacted the server before and have valid keys for zero RTT to work if you don\u0027t then it\u0027s gonna be one or two our titties but after that everything else is gonna be zero RTT quick preview instead of log head-of-line blocking what is that if you have a HTTP stream if it\u0027s HTTP 1 you have a stream you have to open a TCP connection if you have more than one stream then you have to open more TCP connections and we all know that\u0027s not that all those connections have overhead they\u0027re competing over bandwidth so it\u0027s not a great use of resources HTTP to solve this by multiplexing HTTP streams into a single TCP action this is great it gets rid of a lot of overhead however if any of these streams is blocked for whatever reason then all of those the streams are blocked and the reason for this is because TCP is agnostic to the HTTP streams as long as TCP is concerned you have a stream of bytes that needs to go from one end to the other end and quick solves this by basically mapping http streams into quick streams now having those logic that logical streams in quick if one of the streams is blocked the rest of them are not going to be blocked and can be can proceed normally quick has an improved loss recovery it helped the mitigates the I can big unity problem that TCP has it has better RT t "
  },
  {
    "startTime": "01:24:39",
    "text": "and bandwidth estimation a lot of this good loss recovery comes from the fact that you can easily change the congestion control as well so for example if you have BTR a new congestion control you can easily replace your old one with the new one and that comes from the fact that the first point that I talked about you can easy everything is advocate in the application layer so you can easily update things and deploy it and at scale and there are a lot of other optimizations that I\u0027m not gonna go into all of it but basically quick try to learn from decades of transfer protocol evolution and take the good things that worked and put it into a single protocol a little bit of history quick started in early 2010 at Google as I said I think it was in 2013 there was publicly announced and Google started using it soon after there was a spec draft and towards the end of 2016 the ITF working group started and the working group has been very active there are many implementation of quick around quick google\u0027s quick is at version 47 now and the working group is working fast and hopefully soon we\u0027re gonna have a standard version of quake and everyone is gonna be using that all right so that\u0027s that\u0027s why quick started and a little bit of history of it and but as I said quick would start one of the main reasons for quick was improved performance so Google has been reporting on quicks performance they\u0027ve been using it heavily and they\u0027ve been putting our reports that helps with page load time with YouTube rebuffering and all these great numbers that it\u0027s perfect and it\u0027s very promising however the issue with these is that they\u0027re all aggregated statistics and not really reproducible by anyone else on this your Google and you have access to that data and they don\u0027t really record any control-x tests again everything is aggregated statistics there at the time that we started our work there were other evaluations of quick in their research venues however most of them were limited environment networks limited tests and they used old untuned versions of quake which I will get into in a bit what that means and the results that they provided were not necessarily statistically sound neither they provide good causes analysis for the performances that they observe so we basically wanted to reach those gap like filling those gaps and provide them more comprehensive evaluation of quick and how does it compare to TCP so as I said we\u0027re going to look at HTTP performance and we\u0027re gonna compare quake in TCP we have a very simple set up we have a client on one end which could be a "
  },
  {
    "startTime": "01:27:39",
    "text": "desktop client or a mobile client we have a server on the other end which supports both quick and TCP and we have at the network in between we can emulate different conditions and see how the two compare to each other our servers host a bunch of web pages and objects with different sizes and pages with different object sizes and different number of objects and we fetch them using quick in TCP and we compare the performance and I must point out that even though I\u0027m not gonna go into the details we once we get all the results we want a statistical test to make sure any difference that we see is not due to noise or network variations or things that are not really differences between the probable so whenever we report a difference between the two protocol we are confident that this is the difference in performance or not noise or anything else so the setup is pretty simple but in 2016 when we were doing these tests we had this big issue of finally having a server this supports quick it\u0027s not like TCP there wasn\u0027t a quick module for Apache servers the different many options around so basically our two real options were either use Google servers because Google at the time had quick and basically hosts our stuff on Google servers and run our tests against Google or use a server that comes within the chromium code base well the first option Google servers didn\u0027t really work for us for the first obvious reason that we had no control over it so if you want to change something in the server or get some logs from it well we couldn\u0027t because because we didn\u0027t own those servers and the second issue was that we actually started seeing some unexpected behavior when we were testing in school and so here I have one example to give you an idea what I mean by that we uploaded a 10 megabyte object to Google App Engine and then we downloaded it using our client and this barplot is the red part is showing the time to first byte and the blue part of it is showing the time they taste for the rest of the download and as you can see there\u0027s this huge wait time like it\u0027s it\u0027s half a second so basically one third of our download time is wait time and we did some tests we realized this weight on kinda exists in Google App Engine we wasn\u0027t sure we weren\u0027t sure why it\u0027s happening obviously we didn\u0027t have any control to the server to investigate this more and this was not good for us because if we\u0027re down if we\u0027re checking performance and comparing millisecond times a half a second wait time is is not okay so we decided to use the server in the chromium however so this is the bar on the left is doing the exact same experiment but with the "
  },
  {
    "startTime": "01:30:39",
    "text": "chromium server the server that is part of chromium now you can see that huge wait time is gone that\u0027s great but now our download time is much bigger compared to quick to Google I\u0027m sorry and this is problematic because this is basically these two plots next to each other are telling me that the server in chromium cannot provide the performance that quic is able to provide because we clearly see that Google is doing better so we had to try to basically infer what are the configuration that Google servers are using and and basically fine-tune our chromium server to make sure it matches the performance that Google gives so we did that along the way we found some bugs and basically we fixed it I\u0027m not going to go into the details but happy to talk about it offline but after we did that the plot on the right the bar on the light right is basically the same experiment using our chromium server after adjusting it and not only we don\u0027t me don\u0027t have that big wait time now our download time is similar to what Google provides and this is obviously not the only test that we run we did a bunch of tests and there\u0027s no great but we used Google as our baseline and matched our performance to Google and I spent time on this slide to explain it because there were a bunch of research work before us that they did a lot of great work with quick but none of them went through this step to optimize the server and pretty much all of them reported poor performance for quick at least in some scenarios which for fact we know it\u0027s because they were using a server that was not performant all right so now that we have our setup complete our test bed complete we did some tests so I\u0027m going to start showing you results from a desktop client and I\u0027m going to show you some simple results where we\u0027re downloading different object sizes from five kilobyte to 10 megabyte and we\u0027re downloading them at different ballin like bandwidths and we\u0027re comparing how quick in TCP perform compared to each other so in this case the RTT is 36 milliseconds the loss is insignificant and those numbers so 45 44% and what that means is that when we\u0027re downloading that five kilobyte object using quick in TCP the download time for Quake is forty-five percent better than TCP now to avoid bombarding you with a lot of numbers I\u0027m gonna replace that with a heat map so just think of it as red means quick is doing better blue means TCP is doing better and white means there\u0027s no statistically significant difference between the two protocols so if I complete this plot you can see "
  },
  {
    "startTime": "01:33:39",
    "text": "pretty much in every bottleneck bandwidth and for every object size click is doing better than TCP we\u0027re able to download the object faster so this is great we added we threw in some loss into the picture and we saw a still quick is doing pretty much better than TCP in all cases we increase the RTT time we did worked with different artists in this example RTT is 112 milliseconds again quick was doing way better than TCP so so far everything was great and we were very excited and then we did this experiment where we added some packet reordering and as soon as we added packet reordering things started to change and we actually so we actually saw a case as cases especially when that\u0027s covering the plot but the Blues the right side of the plot are big objects the last column is a 10 megabyte object so when you have a packet reordering quic is doing worse in CCD so we want to see why this is happening we looked at quicks code instrument instrument that the code look at TCP to see how it\u0027s a coping with packet reordering basically what we found is that TCP has this mechanism when you have packets reordered it increases it it\u0027s nice and it can cope with that reordering whereas quick didn\u0027t have that mechanism in place and when packets were reordered deeper than its night it was basically thinking that those packets are lost so it was going into loss recovery and we all know what that means only was that performance was going down so we wanted to see if quick can actually benefit from the same mechanism that TCP has our guess was yes but we wanted to test it so I\u0027m gonna I think this is not sorry I wanted all right so we did not threshold the default night ratio for Quake was 3 so we want to see and I\u0027m looking at the example when we\u0027re downloading a 10 megabyte object so it\u0027s a big object it\u0027s a sizable transfer and we want to see if quick can benefit with it from the same mechanism as TCP so we started playing with the neck and we actually saw that there\u0027s a big latency between my clicker and the slides and we saw that as actually as we increase the neck quick-quick spur formance actually gets better and when we let the neck to increase up to 300 which is actually the number the tcp the upper bound the tcp is a lot to increase its nag then quic is able to recover it\u0027s able to cope with the packet reordering I actually "
  },
  {
    "startTime": "01:36:40",
    "text": "starts performing better than TCP alright so next thing that we want to look at what zero RTT because that\u0027s that\u0027s a big improvement that about the improvements in quick so we want to see how much zero RTT help with performance and i\u0027m gonna go to back to our base example where there\u0027s no loss and we have a 36 millisecond RTT as I talked about quake is doing much better than T this is quick versus TCP so this time we run a test and instead of comparing quick with TCP what we\u0027re comparing is quick with zero RT t and quick without the RT t and red means quick with you Artie T is doing better and as you can see in the plot most of the plot is red which is great but also as you have noticed this benefit of 0 RT T is really you can really sense that when the object size is small and when your object is big naturally because your transfer is is longer and your connection time is a very small fraction of your transaction so it doesn\u0027t have a big big effect which is it still great because if you think about web most of the time you\u0027re actually requesting very small objects so so this 0 RT t can help a lot in that in those scenarios sorry so comparing these two plots together as we said 0 RT t only helps for smaller objects but we can see that quic is doing better for bigger objects as well so we want to see what is it that the quic does that helps it to perform better so I have an experiment here which is a little bit extreme but I like it because it helps visualizing things a little bit better so what we have here is we have a case that the bandwidth is changing between 50 milliseconds F I\u0027m sorry 50 megabits per second 250 megabits per second so it\u0027s a little bit of an extreme of an example but and we\u0027re downloading at 200 megabyte objects so it\u0027s a very long transfer using the TCP and quake we\u0027re doing it back to back that\u0027s what this plot is showing on the x-axis I have time and on the y-axis I have throughput and as you can see quic is able to achieve a much higher average to put compared to TCP which explains why it\u0027s able to get such a better performance especially when there\u0027s lost so basically the takeaway is quic is way more aggressively and better adapting itself to their changes to to the available bandwidth which is great but also made us think if quick is so aggressive in adapting itself to to available bandwidth how is it gonna play with fairness to other traffic because as we know we want different flows to be fair to each other so now no flow shuts down other flows so we made TCP and quic compete with each other over a bottleneck bandwidth and we "
  },
  {
    "startTime": "01:39:41",
    "text": "actually found out that quick is not fair to TCP we found out the quic is taking more than share share bandwidth we repeated that experiment with when quic is competing with multiple TCP flows and we still got the same results and to make sure this is not our environment we made quick and quick compete with quick things were fair TCP con competing with TCP everything was fair but when the two protocols were competing with each other quick was not being fair to TCP we want to dig in a little bit deeper so here I have the congestion window size for the two protocols yep in this example they\u0027re both using cubic and as you can see they start from the same congestion window size but quickly quick increases in congestion window and takes a unfair share of the bandwidth and causes TCP to basically slow down and to zoom in you can actually see that quic is way more aggressively increasing its congestion window all right so I have one last thing to talk about before I run out of time and that is mobile devices so everything I talked about so far the client is a desktop device and again going to my base example of no loss and 36 millisecond rtt we saw that quic is doing better than TCP in most cases however we redid the same exact experience experiment but this time the client is a mobile phone and what we saw is that well while quick is it still doing at least as good as TCP do you don\u0027t see any blue in there but the performance gains of quick started to diminish so quick is doing better than CCP but the gap is not as big is for a desktop client so we want to see why this is happening and what we did we instrumented the quick code to try to infer a state machine and see what\u0027s happening in and in quake and what states the the protocol is in at every time so I\u0027m gonna show you this state machine wait for the case where we\u0027re downloading a 10 megabyte object at 50 megabits per second and it looks something like this it\u0027s a classical state machine you have different states the percentage of time that you spend in every state the probability the transition probabilities this is a little bit difficult to reach I\u0027m gonna replace it with a table and as soon as I do that hopefully things are gonna become clear as you can see when when we\u0027re using a desktop machine quick is in application limited state for only 7% of the time and that\u0027s the state that the that the client is receiving data faster than you can consume it but as "
  },
  {
    "startTime": "01:42:43",
    "text": "soon as you go to a mobile device where resources are more scarce now quick is in application limited state for 60% of the time and this is exactly the price that quic is paying for being implemented in the user space you\u0027re constantly context switching between user space and kernel space which is fine on a resourceful device but when you\u0027re on a mobile device things are not that great so that\u0027s all I had to talk about to sum it up we looked at the protocol that was rapidly evolving and honestly sometimes I felt like a measuring moving sand we did tests in a variety of networks and environments there are a bunch of other tests that I didn\u0027t have time to talk about but I encourage you to read the paper if you\u0027re interested in and we instrumented the code extracted some state machine and that helped us to provide some root cause analysis for the performances that we were seeing and the finally I just want to point out that this work was done two years ago so at the time quick was at version 36 as I said now Google quick is at version 47 however nothing stops us from doing the exact same measurement on the new versions we actually did that in the paper we looked at quick from version 25 to 36 so we had that evolution of quicks performance and we can do the same thing for for newer versions and future versions and with that I\u0027m happy to take questions [Applause] Jake Holland could you go back to slide 19 please sorry that clicker in there um I wanted to ask so this is very interesting and thank you for presenting it I wanted to ask did you measure the retransmit rate in this in this bottom scenario and measured the bottoms are the returns memory I think we did but honestly at the top of my head I don\u0027t remember I have to go back and look at the paper okay and okay thank you so that would be in the paper if you I believe so but if it\u0027s not I\u0027m more than happy to dig in and find it because we have all the data I didn\u0027t remove anything great thank you um and one follow-up I believe it was two slides forward perhaps three the fairness question "
  },
  {
    "startTime": "01:45:53",
    "text": "and I have a lot of another in it board as though it slide 20 21 or 22 I think it was so if you want it yeah it\u0027s about the fairness so what I wanted to ask about was you notice the difference in the fairness word where and I assume you mean that quick was consuming a higher proportion of the bandwidth did you compare that to sort of the expectation as observed here that that quick performs better than TCP normally so TCP will prove that this must mean presumably that TCP will leave some of the bandwidth underutilized or less utilized and is it the same proportion here or how far different is it I\u0027m sorry I don\u0027t think I understood the question you\u0027re saying if the ball make them with was underutilized right so my my maybe this is too complicated to ask it might but what I was trying to get at is that we expect that quick will perform better than TCP based on the prior observations even when they\u0027re not competing right which means that on the same kind of link TCP is leaving must be leaving the some bandwidth under unutilized in order for quick to be able to beat it right so how how much is the fairness difference sort of disproportionately it is it different than the difference between and by how much than the difference between their performance when they\u0027re not competing if I understood your question correctly you\u0027re asking that if TCP when not comparing was basically leaving solve the available bandwidth on the table and if the difference was one so it\u0027s I guess is this really a competition fairness question or is it just a yes it is indeed running faster no it\u0027s definitely a fairness issue because in life this answers your question but we did this for very long so it we let them both get to that equilibrium and we could see that when there is no competition TCP is able to utilize the bandwidth on almost fully if okay okay that makes sense thank you yeah so like you so they all went to that 10 megabyte sort of learns when did the transactions is is long enough yes excellent Thank You Evan sorry one last one the TCP was both of them are running cubic you said yes yes thank you that was one of my questions so I have a clarification question a larger question Claire facing question is what\u0027s the queueing discipline you\u0027re running in your bottleneck what\u0027s the word what\u0027s the queueing discipline you\u0027re running in your bottleneck link uh "
  },
  {
    "startTime": "01:48:54",
    "text": "are you running PBR were you running red aqm dropped ale oh I I\u0027m not sure okay I think it\u0027s in the paper I believe so yes I\u0027ll follow the larger question is sort of going back to your very initial remarks I\u0027m really kind of first great work very interesting nicely presented thank you this is a good paper thank you for coming here and presenting it I\u0027m interested in the 200 million users who have internet no electricity right and so I think that there\u0027s a lot of attention being paid to quick as you know higher performance and you know better utilization of of congested resources but I rarely see performance numbers when you look at things like you know very heavily multiplexed long RT tt1 links that have dial-up at the end and you know end users who are you know desperately trying to you know load simple web pages and I\u0027d be really interested in seeing some comparison I mean we really want to make that sure those users don\u0027t get screwed if the world migrates from TCP to quick we you know it\u0027s not great now but it ideally you wouldn\u0027t want to be worse so I\u0027m wondering if you spend any time looking at it you know there\u0027s a little bit here you can sort of take the worst cases of all the things you presented and add them up and you know maybe that\u0027s what it is but so I guess closest to that that we experiment that we looked at some 3G mobile networks so we run some tests there and we actually found out that our results showed that quick is actually doing better than TCP and it\u0027s in the paper the reason I didn\u0027t put it in here because we kind of the things that I put in here I want to be cases that I can isolate and show where the difference is coming from but and like in networks that and as I said we found in 3G networks and the poor networks that quick is is still doing better than TCP but most of our experiments were in controlled environments that were hard bandwidth right so I would just add to that that you might look at adding like very long RT T\u0027s in there and then looking at things like fairness and seeing how that compares that I don\u0027t have any intuition about how that would yeah I believe in our test we went up to 300 milliseconds of RTG and one satellite hop but it\u0027s a good start yeah let\u0027s turn what off you always say TCP what I\u0027d like to know are you using TCP segmentation offload are using TCP fast open or using TCP from the 70s so true so we I believe the segmentation offload is is on it\u0027s in the paper we basically wanted to take a Linux box and use the TCP as is and don\u0027t change anything and "
  },
  {
    "startTime": "01:51:54",
    "text": "we did this on a purpose on purpose because obviously you can optimize any protocol to a Linux box I can turn TCP fast open on or off very easily use the kernel option I don\u0027t know what you did that\u0027s why I\u0027m asking well all the default values they came with it they\u0027re all in the paper hey goggle Montenegro Microsoft so thank you very much for this work and I think I heard you say that there may be some ongoing work going on some more research so if that\u0027s the case could I add a suggestion that you know G quick where Google quick is so fine and good but the whole focus of the IETF ever is i quick write the idea ITF quick it would be great and there\u0027s several implementations out there that you could use if you would use those for the next phase of the testing because that\u0027s a pretty different protocol really by today and the best part of it is if you find something egregious or something that might need to be quick then there\u0027s still time to go back to the working women actually have an effect on the on the protocol if that\u0027s one of the of your findings and that would be potentially more relevant for the future than India quick that\u0027s possibly everybody at some point will be on ITF quick so that that\u0027s one one suggestion and the other one is more of a comment you indicated that since quake is implemented in userspace that\u0027s one implementation hours for example runs kernel or user doesn\u0027t matter so you could run it in kernel you could run in user space it\u0027s not part of the Pirkle itself right so I understand that for the pesky needed to do of course it was easier to get the user space but that\u0027s not really part of the of the protocol definition so you could run it in in kernel and then maybe some of the disadvantages that you identified would would go away and in the mobile case that as you mentioned yeah that\u0027s it that\u0027s great yeah at the time we were basically working with Google\u0027s quick and that was pretty much the only option yet you understand oh thanks okay let\u0027s have a hand for rush excellent and our pink box so to pitch for the remaining remainder of the year there\u0027s four more great and RP toxic um if you want the links and you can\u0027t find them for some reason I did put up a an agenda slide set that is in the tracker so you can find that and also a humorous "
  },
  {
    "startTime": "01:54:54",
    "text": "prog related picture but in any event um thank you for being here and thanks for the great questions for our folks and end of IRT F open actually I have another thing since we\u0027re changing from one leader to another I\u0027d like everybody to really thank out Ellison for the work she\u0027s done in the past few years thank you so very much [Applause] [Music] horrible yeah I\u0027m glad you uh you know stick with that one yes things are better now with the PDF slides than they used to be yeah well Collins do you want to go yeah I probably have to but I think it would be worth your while to take a look at the network tutorial after the fact yeah "
  }
]