[
  {
    "startTime": "00:00:05",
    "text": "foreign you know good morning everyone the word good afternoon and uh and so some other places and uh we're going to start just in a few minutes this is uh Computing and a network we're going to give a few minutes to people to arrive in the room so welcome and we'll starting soon thank you"
  },
  {
    "startTime": "00:02:23",
    "text": "oh let's just do the touch foreign good morning everybody uh I'm not one of the chair I'm the Buddy stand in they need a physical person to be here my name is Cedric Westfall and I'm in the future way and I have no idea what I'm supposed to be doing to start this I don't know if the chairs is marriage Jose or somebody on the on the line I think it's you uh if you log into meteco you're going to be able to follow us I mean the instruction of the of the computer on the desk are very clear it says do not touch ah okay okay you have your own computer um anyway you can just stay there and uh we have everything loaded you should see us could we hear you okay the case that at this space it should be the case at this point"
  },
  {
    "startTime": "00:04:00",
    "text": "um that you are a delegate once you log in maybe not on that computer but on your own computer you will appear yeah I'll log into the computer and uh you go ahead because we I think it's uh 9 34 so we started so um if you want to start okay then that will start uh okay so this is um um the coin Computing in the network uh I would like to uh really really thank our proxy Cedric who introduced himself from future way to be there in the room since uh none of us could travel cool um obviously there's the usual note well and policies against harassment and everything a reminder that this is the irtf we do not do standards but we do research and you will see today that we have a very interesting research papers presented plus a number of updates and new drafts um Jeffrey you want to take over me yes so before I introduce the agenda maybe I'll remind you two things physically in London people in London so you need to follow the masking policy so we have to pass your mask during the meeting during the meeting in the room"
  },
  {
    "startTime": "00:06:00",
    "text": "second is that we maintain just one single queue in the meteco so if you want to go to the mic please also join the queue in the tackle so as for the agenda so we have three research papers today to introduce first it will come from [Music] Oxford he's the from the team of Noah Zero Man he will introduce some lessons learned from their electrical in network classification implementation and then second Ike will introduce their paper about the end-to-end transporter layer and that he will discuss the very interesting topic related to end-to-end argument and then they've from Theo Munich he is working together with Dr chosen he will discussed the DOT on a private provider Network and after that we have uh Pasco to introduce one new draft about the secure elements implemented in the internet and how the data plan the programmable data plan can be used in this architecture and if we have time then we can discuss some future of This research Loop and there are some initial considerations from cheers and we can discuss yes would you think um yeah if you want to continue though"
  },
  {
    "startTime": "00:08:07",
    "text": "sure um you are all here so thank you for being here those of you who are already signed on um there there's the pointer here to the meat echo which has many um good features uh not least of which is that it has many things integrated including it automatically keeps track of who's participating so it generates a blue sheet for us but for those of you inclined to do so we'd really appreciate your involvement in the shared note-taking and there's a both a pointer here and then in the tool itself there's an icon along the upper um along the top that also will take you to the note taking the integrated note-taking um we will try to also monitor the chat and um the physical cue for questions and um uh as was mentioned we're going to use the Meade Echo um to maintain the queue uh if everybody of course will default to everything being off unless you're speaking uh that will be great and in fact uh in terms of the masking policy if you are a speaker you are allowed to remove your mask when you're speaking um otherwise please maintain the policy um we welcome you to participate in our mailing list and um you can you know to subscribe we include the pointer here but also welcome you to peruse the archives for ongoing conversation and all of the materials for the session um are available through the agenda page and through the link here as well as again if you go to the top icon and look"
  },
  {
    "startTime": "00:10:01",
    "text": "for the folder um you can look at all of the materials and peruse the mature leisure we have many documents that have been contributed um to the discussion um we are uh for our next meeting we are very likely to ping you if you have written one of these documents um in order to refresh those that we would like to still consider as under the charter and maybe even advance to being adopted by the working group um uh but for today's session we have um One new draft that we will um discuss uh uh during this session and with that go ahead with that we go to presentations and uh I would ask the first presenter to [Music] um load um oh maybe I can actually preload I actually can pre-load stop my share and share preload oh our slide is being shared okay it's fine that's this one foreign excuse me"
  },
  {
    "startTime": "00:12:04",
    "text": "I think all the slides were pre-loaded and now I can't find them do you want me to show it for you I can't find the presentation that the first thought no here we go better yeah which speaker are you um the presenter from from Oxford could you please load your slides or share your slides because I can't find them I can't find them on the pre-loaded I mean the the yeah the author is is the author on on Deco you could actually load for me so they both heard we're gonna is gonna uh share from his laptop the presentation I think or do you have different because somehow I can't find the slides on the pre-loaded material yeah did you yeah uh do you have the slides marriage Jose actually um yeah but if you look at pre-loaded material I can't find them now what is your suggestion it's all right I have everyone else except that one I saw that one well then load them then"
  },
  {
    "startTime": "00:14:04",
    "text": "load them yeah because I I don't I can I can load it maybe I can try share the screen my screen so it's gonna take me a little time I thought it was okay let me take off one of the things there's a full client the video so I don't think any of the devices though because that's going to make somebody cool see then share your screen okay and if your mail Advance the slides for you I see them on the data tracker but on the uploaded material I don't see them so so you can ask it to reload maybe has a okay whichever window you want yeah okay so can you show your slides"
  },
  {
    "startTime": "00:16:05",
    "text": "uh this is not the screen there oh yeah here we go okay use on the uh thing that so okay so good morning everyone my name is and today I would like to talk about our research related to practic practical in network classification and the license we've learned during the past three years I'm sorry yeah I would like to talk about the lessons we've learned that during the past three years and this work is a joint work with many of our colleagues and from several different Institute and it can be shown here yeah that's scary and begin with 2019 we have begin to do the network machine learning research and at that time we have a research named easy which also named us to switch stream of machine learning and this work has been presented in how night 2019 and a mailing demonstrates the uh mapping of trained machine learning model to programmable network devices and for that work and mainly introduced four type of machine learning models which is decision tree support by custom machine k-means and knife base and for that work I'm mainly focus on two type of Target which is bme2 and net fpga Zoom and next page after three years we have many progress and currently we have a work named automating Network machine learning and it provides us a plantar framework that can help us to realize end-to-end automatic uh uh e-net machine learning deployment and with this framework we are able to Auto make it automatically"
  },
  {
    "startTime": "00:18:00",
    "text": "train the machine learning model and also generate Auto generator P4 file which is different from previous work easy because previous work only supports static P4 file and for the framework is also able to Auto generate the P4 runtime file and table entries that use for the generated P4 file and before the real deployment of our algorithm the the framework will auto test and validate the design on by using both Python and the software switch and finally after all this process the design will be finally Auto loaded to the hardware selected Hardware and currently our Frameworks support more than 12 models and supplementation of models and it support besides the easy support for type of model it also supports for example season 3 actually random Forest actually boost knife base k-means or an auto encoder and and actually for for this model it's not all the model that we can support it and we can even support more but we think currently is enough for this stage and we also generalize all these mapping Solutions into encode-based local space and direct mapping Solutions which can help us to uh use it to support new type of network machine learning algorithm and most importantly we support many different targets for example we support Commodities which Asic which is purely Commodities which with basic result uh modification for example Tofino and tofino2 and we also support for example targets like Papi which is running people program so Raspberry Pi and we currently support two types of compiler on it which is tapas and bme2 and we are also working on Nvidia spectrum and fpga so uh the whip here means working Pro progress process yeah the next slide so in this talk I would like to mainly talks about the challenges and solutions we face when we implement this"
  },
  {
    "startTime": "00:20:00",
    "text": "in-network machine learning classification algorithms so the first challenge that I would like to talk about is about limit number of stages and for example for the Commodities which Asic they usually have a limit number of stage for example Tofino only have 12 stages and for this figure on the left hand side it mainly shows the typical realization of this stream model in the data plan and we can show that uh for each steps of the tree it generalized into different levels and for each level it will consume a certain number of stages so which means that uh for for the data applying program with limited number of stages it will limit the types of the tree model to be deployed and our solution is use parallelization which means that we can parallel execute the independent functions in our Network machine learning classification algorithm and uh yeah and for example for this example tree models for example we can parallel the uh use lookup table as feature tables so for example when feature inputs will be mapped to codes uh for each feature and then for the next stage which are three tables and these three table also execute data paralleling and for each tree table it will just collect the input from each feature table and just output the adder for the both of this tree or the probability or the depth of the tree and in the third stage for the decision table it will just combine all the volts from previous three tables and output the final classification results and the next slice and the Silicon challenge that I would like to talk about is about limited memory and for the limit memory we have two type of solution and first is use more"
  },
  {
    "startTime": "00:22:01",
    "text": "efficient mapping solution for example on the first figure on the left hand side we can see that uh compare for the k-mean solution compared to uh for for the easy implementation versus classroom implementation in terms of table entries consumption we can see uh if the solution use a static number of table entries well the classroom table entrance consumption increase in order of magnitude as the model type increases and for in terms of the accuracy we can find that the easy solution is independent of model depths well the uh classroom solution is dependent with the model tabs this is mainly because the classroom use the tree like structure to store the table entries and well they either use lookup based solution so under these circumstances lookup is solution seems more efficient and the next solution is about using it seems a little bit obvious is using we can use RPM table or ternary table or range match table to replace the table that previously used exact match and actually when you realize it is not as simple as it looks like because it's not simply like using LPM in for for IP table because the input of the machine table of e-network machine learning classification algorithm as I mentioned in previous slides is for example for the tree table it's a concave codes from the feature tables so which means that we need to use our self-designed algorithm to map the exact match table to either Luca based table lookup table uh LPM table ternary table or range match table and we can even use Smart rope to further reduce the number of table entries in the uh in the table so for example we can just drop those less"
  },
  {
    "startTime": "00:24:01",
    "text": "significant table entries for example there are some single thing stable entries that are different from others that stop a large range of table entries from merging into a single LPM table entry then we can selectively remove those table entries and from the figure we can find that we can reduce around 20 of total table entries without significant influence of the machine learning classification accuracy and then next page and also when implementing Network machine learning classification we should also make sure that it should co-exit with the normal functioning of the network device so as shown in this framework we have a Content P4 block here which can help us to generate P4 file not only with the classification logic but also with the use case and this use case can be for example switch.p4 which is the L2 L3 reference switch and it is designed by Intel Tofino and this program is currently used as a reference program uh for your network network computing algorithm and actually our uh our our United machine learning classification algorithm is parallely execute with the normal uh this L2 L3 switch and uh it do not consume a lot of resources so when we compare our Network machine learning realization model realization uh the resource consumption of these models we can see it's only five percent to 65 of this reference program so which is uh relatively small and makes it possible to coexist with the normal switch function and also we can see the latency of our e-network machine learning algorithms and we can find that uh if we only implement the machine learning algorithm and and it shows in the pink bar and we can see the latency is"
  },
  {
    "startTime": "00:26:01",
    "text": "relatively small compared to this reference program and if we co-exist with the machine learning algorithm with uh with this reference program this L2 L3 switch then for the for the the implementable model we can see as shown in Blue Bar the level of latency is same as the standard lung reference program next page yeah next and so no matter how we for example we use mapping techniques or use LPM different types of table types of table to reduce the table entries to save memory or we can do the parallelization to save stages still we need to trade have some trade-off uh parameter selection when we try to use machine learning algorithm for e-network classification and for example for the as we shown in this radar graph so if we want to make sure that the generate model have a same level of memory consumption and for example if we want to use more features than the number of trees is and depth is limit and if we want to have more trees then we can only use a limit number of features and or if we we want to have a large number of tabs and uh feature and trees allow this limit so uh actually one using this framework actually we can play with it and um"
  },
  {
    "startTime": "00:28:02",
    "text": "it stops presentation here yeah I don't know connection uh are you connected okay uh can you stop and start again maybe oh you know I said you wanna try to ATF network instead of anymore uh are you gonna get through your phone okay is it back oh it's back okay so no wait oh please this is where you should say which slide are we on okay I have them yeah thanks a lot yeah so actually when you have the framework you can play with it and based on the use case that you've select and you can select the set of hyper parameters that that best have the best performance on your use case and when we look at the max number of features that we are supported for example for the three models for example the century accessibles and random forests or random Forest hybrid we can see that if we use cosmetics headers and we can support more than 60 for some cases we can support more than 60 features and if the features are stored in ASCII format inside the package then we can support uh only around 30"
  },
  {
    "startTime": "00:30:01",
    "text": "features and if it has joined Implement with the reference program reference which program then we can only support around 15 features and if we use the model that use different mapping techniques for example lookup based solution uh like a model like subtract machine base and k-means and it can only support less than 15 features and this is also use case dependent sorry the the the next slice thank you and also when we deploy machine learning inside the network we should also make sure that we can update the model and we should do the runtime retraining and updates and our popular projects mainly focus on this problem and we mainly solve this problem by using digest and shadow updates so which means that when e-network machine learning is deployed in the data plane and it will auto continue sending digest information to the control plan and the control plan will collect this information and combine with existing training data and use on supervised learning algorithm to to to relabel this data set and it will also unserver to uh you and this generated data will be faded to the planter framework to retrain and regenerate the table entries and actually this generated table entries will be loaded through the data plan by using shadow updates so when loading this it means we only update the table entries and we do not touch the P4 program and by using Shadow update we can do the runtime update and without interrupt is normal functioning of network function and as well as the normal functioning of classification service and the next slice uh next next thank you and also we should make sure that our in-network machine learning classification I have a good performance"
  },
  {
    "startTime": "00:32:03",
    "text": "and so this is mainly guaranteed by the power design which fits the commodity programmers which is like well so for all our design we do not use recirculation or resubmission and we have no control plan dependencies and we do not use special modules like uh customize which is we purely use commodity switch ethic so this means that we can achieve a full line rate and with where we've tested on a 64 ports Tofino switch and verify that and we can also achieve sub microsecond latency and our model have same or less latency compared to the reference program also as shown in previous slides and next slice and uh even though the model of e-network machine learning classification the model size is still be limit and it's impossible still impossible for us to implement some model like random forest with 200 200 trees and input 200 features or with 100 apps but what we can do is we can use hybrid deployment to achieve a high inference accuracy so which means the hybrid deployment means we deploy a small model inside the network in the data plan we use a large model as a backhand server and in the switch we use based on the decision confidence we can decide if the decision is directly been made on the switch or the packet will be forward to the backend for further process and next slice next slice and for example for a normal detection use case for the right hand side upper right hand side figure we can see if we select the x-axis for the switch confidence threshold to be 0.9 so which means that the uh decision of more than"
  },
  {
    "startTime": "00:34:03",
    "text": "70 percent of traffic can be directly made on the switch well we can see that the system hybrid system accuracy in the blue line in the left hand side figure is almost close to the Baseline so the Baseline model is deployed on the server without the resource constraint so this means that our hybrid deployment can achieve close to Optimal classification result will significantly reduce the amount of traffic flowing to the back end and the next slide so in summary so our work shows the in-network machine learning classification is feasible and we can run these machine learning models and Community switch with full line rate and we are also be able to make sure that this machine learning algorithm is coexist with the use case and the normal functioning of this network device and most of the model is scalable and if you want to have a extremely high accuracy or very large model then we can use hybrid deployment to deal with this problem and we have where I've been uh for several use cases for example the anomaly detection use case the iot Gateway smart iot Gateway and we also uh try the high frequency high frequency control trading use case and we are also looking for new use cases and if you have any ideas we are really happy to discuss about it so that's mainly all for this presentation and thank you I'm happy to take your questions um thank you very much for this very uh interesting uh Lessons Learned uh any questions uh I don't see anyone on the uh yes I keep please we have somebody physically here that's me"
  },
  {
    "startTime": "00:36:01",
    "text": "uh thanks for the talk I have a rather specific question so on I think on one slide you mentioned uh that uh so with the resource consumption and all that stuff that you had like that you could Implement more features for this one approach than fewer features with the other approach and then if you combine it with the switch program then you got basically on the same level for both uh versions that you had and there I was wondering what kind of resources or resource constraints there actually is the limited limiting factor so is it then basically the sequential numbers of steps that have to be executed or other than specific constraints in your program that yeah limit the number of features that you can Implement okay thanks so for the constraints so actually for our solution of e-network machine learning algorithms there are several types of constraints and the most common one is stage consumption so if we increase either for example the number of features used or number of three numbers uh I just use random Force as an example if we increase the number of feature input and if we increase the depth of the tree and if we increase the number of trees then the number of stage will increase this is because for each stage there is a limit amount of supported tables if you use more than this number of tables then it will cause extra stage no matter how you paralyze IQ execute these tables and also the Mind Race is another constraint because for example for some of the table for example as I mentioned in the uh uh The Ensemble three models we can see there is a decision table and usually if there are too many trees and the decision table will cause loads of memory and if there are too much memory then it will also cause extra stages and also uh we can see that for for if"
  },
  {
    "startTime": "00:38:02",
    "text": "you use customers hide us for the three models we can support more than 60 features but for uh ask if the feature is stored in ASCII format then we can only suppose 30 features this is because the parts also have constraints resource constraints and we cannot support on limit amount of protocols or headers in the header field so these are the stage memory and uh and uh and the head of uh and parser constraint are mainly constraints for our Network machine learning algorithms of course for the general speaking of e-network machine learning classification realization there are more constraints for example uh for for most of the switches they do not support and floating Point numbers and they do not uh support uh for example some type of mathematical operations for example multiplication and division operations these are other constraints but our solution can can can just do not use these operations any other questions uh we can send the questions on the list I'm particularly interested by the way by the iot implementation so I'm going to read the paper for sure uh thank you so very much very Jose we have one more question in the room oh one more okay because the person was not on the queue okay uh thank you for your good presentation and your good paper so in South Korea we had a similar project at the case we utilized the team learning or CNN RNN to classify traffic and we found it works so I'm wondering if You Hinder the new or three people are new picture of the traffic because for example headquarters were some the or better person make some new or traffic"
  },
  {
    "startTime": "00:40:00",
    "text": "for violating the network so in this case how can you handle this new 3p one new picture over data uh so which have type of data I'm sorry so you use the uh machine learning so it means that uh it or or training data and find some the wisdom from existing data so how about the new data or new picture new traffic so in this case can you handle the same picture yeah feature or picture uh currently we didn't pass the use case with features but generally speaking because inside the network the feature is not you know for if it is not packet level but flow level or and also in the real use case that the the you know the the the packet will go through different routing paths so that's the reason why I didn't we didn't touch it yet but there is some work that's that's I remember that using a design tree and run for it and neural network for your network classification that have some explanation about how to deal with how to do the image classification and you can try to find them and I remembered what they use is what they currently implemented is only on PM between environment uh dma tool a software switch so this is my my understand about the image okay thank you okay um for for moving around with time so thank you thank you again and uh we'll [Music] um make sure follow up and and great work"
  },
  {
    "startTime": "00:42:03",
    "text": "so like you want to share the next presentation if you find it this is being shared are you sharing uh no you're you're sharing okay I think it works uh it's um Ike is doing it so just just a quick reminder if you want to ask questions uh use the metico uh queue just so that Mario Jose is managing the queue remotely it's not done from here so uh so they know who is lining up to ask questions thank you okay uh hi again um I'm icon sir and this is Joint work with Dirk and Klaus and for those of you who have been following the research group a little bit longer this is actually something that has come out of our transport issues draft that we had had at this research group earlier and what we now try to do is basically um yeah have a bit more thoughts on how we can actually combine or what is the interplay between transport protocols the end-to-end principle and Computing in the network and this has resulted in the paper that we've presented last week at the new IP and Beyond Workshop and yeah today I would like to give you um basically also the the main ideas that I presented there as well as some additional thoughts that came up in discussions there at that Workshop so basically an improved version of the talk that I held last week in mixing okay um I think in in this context where we're here uh it's pretty easy to say that we can see that the networks are evolving from being dump networks to"
  },
  {
    "startTime": "00:44:01",
    "text": "being smarter networks uh previously or in the early days we could assume that if we had a packet coming into the network um maybe uh send from host a so this yellow packet maybe um that it would also come out on the other side mainly unchanged and this is then often seen that the network is just a gunpipe that forwards the packets and this is somewhat encompassed in the end-to-end principle and um is it typically also used as a basis for transport protocols so for example in TCP when we think about the reliable reliability aspects it's simply that our TCP assumes that the packets are unchanged in the network and now with coin this changes or this can change as we can do more stuff in the network so maybe change the color of the packets and thus we can no longer speak of the network as being just a dump pipe and this also breaks then assumptions for the transport layer and that we can now make changes in the network and this has already been discussed for example in the recent hot Nets paper so we are not the only ones thinking about this topic um but what we now try to do basically is to think about a more general or have more General considerations regarding this topic so basically uh what we would actually need to have a coin-enable transport protocol that ideally also respects the end-to-end principle and just to get your uh thoughts right for the stock here so I won't provide a lot of answers but mainly I will raise a lot of questions that perhaps we as a research Community have to answer eventually and in the following I would first like to go back to the end-to-end principle and then talk a bit about a few considerations that we had how um such solution or such a transport protocol looked like and then afterwards"
  },
  {
    "startTime": "00:46:00",
    "text": "also have a few more thoughts on this whole topic so starting with the end-to-end principle it goes back to a paper from the 1980s so a bit so the this principle is actually older than myself so um probably some people in the room longer nowhere perhaps longer than myself um and the end-to-end principle basically states that a function can completely and correctly only be implemented with the knowledge of the end of of the applications at the endpoints so basically saying that the endpoints have to know what's going on inside the network as well and this is then seemingly at odds with the coin with coin because um yeah in command we would assume that something might happen in the network however there's also a second sentence actually two sentences further down in that paper which then states that an incomplete version of the function can also be useful as a performance enhancement um if it's provided by the network and now if we think about coin as being such a performance enhancement then this could again align uh coin with the end-to-end principle and yeah in this context we then thought about or wondered a bit more about the relationship between coin and the end-to-end principle and in that context mainly focused on two aspects and that is first the location of computations and then as a second aspect what kind of computations can actually be performed and regarding the location I think they are even in this room if I would ask you what kind of computations would you consider to be coin computations I would get probably let's see maybe 30 different answers probably um so a strict definition of coin would be that we can for example only compute or perform computations or networking devices so really on switch Hardware uh basically as we've seen in the previous"
  },
  {
    "startTime": "00:48:00",
    "text": "presentation for example however there are also more free definitions of coins so basically seeing coin as a subset of edge Computing or cloud computing um maybe only enriched with additional functionality in the network but basically having some computations between the hosts between the end hosts and in our paper we actually try to get around making a strong statement at this point and basically just generalized to coin elements so just saying okay we now consider any capability that we have there between the end points as a coin element and yeah it's up for anyone else to decide what exactly is now coin or not um what we did distinguished though is where we perform the computations so with relation to the endpoints um so here we have in red the typical or the fast end-to-end path and we then distinguish between two types of computations one are on path coin elements so directly on this shortest path for example and the other computations would be of past coin elements where we then would need to reroute the packet or where the packets would need to take us to take a slight detour then as a second aspect we thought about what kind of functionality can now be provided by the networking device or by the coin elements um and here we took a rather functional view I would say um so also stemming from the end-to-end argument where our assaults at all we're always talking about the function that is provided and here in this example we have a function capital F consisting of a few sub functions that is computed between host a and host B and we then thought about what kind of functionality can now be provided by the coin element in the middle and the first option would be for example an F1 Prime functionality so an"
  },
  {
    "startTime": "00:50:02",
    "text": "incomplete version of the original function and then maybe also a bit uh tweaked so that it can actually be performed here on the coin element and as this is now still part of the end-to-end functionality we call this an end-to-end function internal computation so basically we have a functionality that was originally part of the function and we can now also place it in a on the coin element and then the other alternative would be to have a function that is not part of the original functionality and this then here uh symbolized as a function G um and here we were then and we call this a function a end-to-end function external computation and here we we were then wondering whether this is then still something that we would like or that can be end-to-end compliant or not and here we thought that if we now place a new functionality into the network then this is rather having a new endpoint in this overall functionality and thus we think that this is rather some form of edge Computing or cloud computing but not really coin at least if we want to ensure the end to end principle compliant variant of coin and with that being said let me quickly summarize these two aspects so what we think can be end-to-end compliant coin computations are end-to-end function internal computations that we can then either place on on or off path coin elements and in the following we then thought about how and where we could actually Place such functionality because right now I've only discussed a bit about the general aspects regarding these things but not how we would actually practically use this stuff and there we basically came up with two design principles principles as we called them and the first one is"
  },
  {
    "startTime": "00:52:01",
    "text": "regarding the location where we can place it so considering we have here two locations um where we could place functionality so we have F1 Prime and F1 double Prime and then the question is do we use F1 prime or do we use F1 double Prime and the first two aspects that we hear considered are rather straightforward I would say um so we think that if we use additional coin functionality then we should still adhere to the original requirements of the functionality so basically don't break anything that has been working before and then as a second aspect and that's basically now um belongs to the second part of the end-to-end principle that I mentioned earlier we also think that it should then also enrich the original functionality but then we could still have the case that both of these function placement that we have would um yeah be valid for both of these aspects and then we thought about what kind of tirebreaker could we have there or how we could then actually derive the decision that we want to have and for um the inspiration we'll then look at the Simplicity principle which basically states that we should always Thrive to the simplest solution or always try to reduce complexity as much as possible and we then basically translated this into that the com the coin functionality should optimize functional complexity against a key communication requirement okay fancy words what do we actually mean with that so if we have now here these two functions and we then for example consider the latency as key communication requirement then we would for example use the lower functionality if it's possible to deploy the function that we need there because in that case we would if we consider that the"
  },
  {
    "startTime": "00:54:00",
    "text": "rerouting to the upper function would take a little bit longer time then we would have the lower latency at the lower path and thus would choose this location however if the functionality is too large for the slower functional function National deployment then we would take the upper path and so we think that this is at least the starting point which people can build upon to these uh to consider where we can place functionality we then also had a different view on this whole topic um basically regarding the first part of the end-to-end principle so the knowledge of the endpoints and here we again have the overall functionalityf and now we place a new functionality in the network so it's also the incomplete version of the function but we place it there without the knowledge of the endpoints and in this case it could very well introduce a lot of problems because now the endpoints are still Computing the whole functionality but there in the network we also compute something of the functionality and then stuff can break and I think performance enhancing proxies for example are an example where placing something in between the nthos without the knowledge of the end hosts which is an example where this has caused problems before and thus we think that the second design principle in this context would be that we should always insert the coin functionality in transparent in full transparency to the endpoint so that they know that something's going on in between them and with these design principles then also in mind we then thought about considerations for the transport protocol obviously mainly coming from our previous work that we collected here in the context of This research group and why did we actually choose the transport protocol well it's a function that is traditionally implemented in the endpoints only and it's also the one layer that is translating from the network to the applications so it will also be the layer that is affected by"
  },
  {
    "startTime": "00:56:02",
    "text": "any changes that we have in use that are induced in the network and in this context we then considered several aspects so in this paper we focused on addressing so basically how can we choose which function or computations to execute in a network um as a second part flow granularity um so deciding do we need a stream notion as for example in gcp or are we okay with the datagram notion as an UDP and then finally also more evolved communication Concepts like Collective communication so basically not only having two endpoints in the communication but a few more of them and for today I would like or yeah for today I would like to focus on the addressing part and if you're interested in more details on those three aspects uh have a look at our paper I will also have the QR code in the end so you can scan it then and also we have still our now expired draft where we even consider more aspects than only those three okay then starting with the addressing um so typically even if a host a wanted to communicate with a host B we then add basically some kind of tag to the packet in the form of an IP address and a transport transport protocol part to address a specific application um and now the question is how can we do this if we want to have a function F1 Prime somewhere in the network and the first option that we have is some form of implicit integration so we actually don't really um address it explicitly but just place the functionality somewhere in the network so in this case we would try to guess maybe where the packets will go and then place the functionality strategically on this device so that the packets will actually be processed by that computation so this works in a lot of cases especially in smaller networks it's actually also something that is"
  },
  {
    "startTime": "00:58:01",
    "text": "typically used for research prototype so I've done that myself and quite a lot of projects however if we now scale up the networks then at some point this becomes really hard to maintain and especially in some networks we don't even know where packets really go and thus it's really tricky to do this to do it this way and additionally it also only allows for the on path notion of coin as we yeah just place the functionality on path so off path is not possible um yeah the second option is then an explicit steering mechanism so here we then really apply some kind of tag to the packet to for example say okay we would have we would like to have the F1 Prime functionality up there um and in this case we wouldn't also have the off path notion however it's really unclear how you would actually like to do this addressing and I will come to a few possible solutions later on as well but this is then really something that we we need to think about how we would like to implement this um and then assuming that we have some form of addressing um so mainly perhaps focusing on the explicit addressing for a moment if we then have two different locations where we have functionality then we would might want to decide which kind of uh this or which of these two functions we would like to execute and there are then the question is how would we like to do this so would we like to always specify the exact location so for example say okay we would like to have the upper F1 Prime functionality to be only specify some kind of constraints so for example saying we would like to have the one with the lower latency or with other requirements or can't we as an end host do anything about it but just let the network handle it and the network decides which function is selected so there's really I think a large spectrum of possible solutions or answers to these questions"
  },
  {
    "startTime": "01:00:02",
    "text": "um so as I said we mainly raise questions and don't answer them so quite a lot of them here and then if we have some form of instance selection mechanism that we might also want to keep Affinity through to those instances because for example if you would like to build up a certain level of State at some point and then we would always want to go to the same service instance at that point and here then the question is how do we actually realize this Affinity uh do we set that up already during an orchestration phase before we actually have the first computation is this done on the Fly um so again a lot of questions that arise here and I think in the paper we raise even more of them so if you would like to read a lot of questions or get a lot of yeah ideas for questions then have a look at the paper yeah then maybe trying to summarize this a bit and we also had a look at existing Solutions because obviously there's a lot of internet technology already out there that might be applicable to uh to these problems um and one is for example this Source routing so here we can as an end host already Define which path we would like Pekka to take through a network however it is not directly something that we can use for defining the functionality so for example considering that we might have different functions on one device that we would need yeah multiple IP addresses for example for each of the functions that we have which might become quite a lot however there are a service function chaining which allows already for steering traffic through functions so if we now interpret coin as some form of network function or service function then this might be applicable and similarly we have information centered networking as well we are we now address information rather than endpoints and thus again if we now interpret coin as some form of"
  },
  {
    "startTime": "01:02:01",
    "text": "information then the these might also be applicable Concepts so overall I would say there are many possible solutions to the many questions that we've raised but I think um yeah these questions are still open so we haven't provided any concrete answers for that um and I think the main takeaway from our paper is that there are ways in which coin can be aligned with the end-to-end principle but we then have to really carefully think about what Solutions we actually pick for all the questions that we've raised so and this is the part that was basically the same last week as well uh the uip and Beyond workshop and now maybe something more provocative that was discussed in the context of that Workshop um so there were actually also or wasn't other talk there as well so um this Tintin protocol and we then thought about um yeah how would you actually do this in practice now so we phrased a lot of questions regarding transport protocols but we now see already that there are two rather specialized protocols and the question that we then discussed was yeah do we now want to have one Global protocol that basically solves the end-to-end principle problems of coin um it's all basically having yeah the one transport protocol that we need for all the applications or do we rather have a lot of specialized protocols for different Limited domains um do we then have perhaps a couple of core features so basically a core protocol that we then extend for the different specialized domains that we have there to these protocols that have some way of interacting that is perhaps standardized or do each of these protocols really only apply to their"
  },
  {
    "startTime": "01:04:00",
    "text": "specific limited domain and then maybe you know as a really provocative last question um might it be also be possible to somewhat bend the end-to-end principle in these limited domains because after all if you for example think about industrial networks then we have basically that the whole network is in the um in the premise of one entity and so we could already think that basically this end-to-end principle aspect is already covered because if someone deploys Solutions there then they basically are the endpoints as well as the network um so yeah really trying to be a bit provocative maybe here in the end to uh yeah Inspire some thoughts and arguments uh on these all on all of these topics and with that I would now like to wrap it up so in the beginning I showed you why uh many people think that coin is adults with the end-to-end principle but then I hope that I've shown you a way on a way of thinking about it um how we can align these things again um so with the end-to-end function internal computations both on on and off path coin elements um we then talked about two design principles for coin and then finally I had a brief discussion about considerations at least for the addressing part and as promised here are the two QR codes and now I thank you a lot for your attention and I'm happy to have a discussion with you or to hear your comments on this thanks thank you you raised a lot of the questions that we've asked ourselves in the past few years there's three people on the um actually now four people on the cute Dirk you want to start yes my pleasure um hey Ike um nice presentation"
  },
  {
    "startTime": "01:06:01",
    "text": "um you are raising many um questions here I think this isn't um very comprehensive approach that you um present here and I was thinking you are probably making your life unnecessarily hard so um so the question so you know how should you design this or what should be the services by certain protocols really depends a lot on what you want to do and I think it's really hard right now to um you know um conceive and um hypothetical um you know coin protocol without actually knowing what you want to do and um so this the end-to-end principle discussion is a good example for that so um I mean these principles were formulated with a certain goal and um so they might not necessarily be the same goals that you have when you think about Computing in the network so when your objective is to build an internet work based on packet switching and statistical multiplexing um yeah then you need to think about um the the control Power of the end systems so and to enable different kinds of applications transport functions and so on and so this is a model that fits very well to the internet and to IP and so on but for computing in the network um so we don't necessarily um have the same goal and that means we don't necessarily have to constrain ourselves by by these principles so just one example um you could say conceive um something like a protocol for for computing in a network like a data flow system where you connect functions and each function does"
  },
  {
    "startTime": "01:08:00",
    "text": "something produces uh new data and so on um that would be of course very much at odds with this within the end-to-end principle but sometimes it's just what you need and um so um I I think we it's probably a bit um too much or maybe also premature to think about um like the the grand unifying Computing in the network protocol um for all kinds of applications I think this needs to come from experience um or yeah on experiments for them especially in different areas and then you could think about um so what does use case a need use case b and so on and um is there a need at all for an internet level protocol that's another question yeah uh thanks for the uh for your thoughts on this so actually our uh goals were not to have this one unified protocol but we just try to basically as you said uh without having a lot of practical experience in the large-scale deployment of these things um to think about from our standpoint of today how we could actually align these aspects maybe also to um give a lot a bit of guidance maybe for the first larger deployments of solutions like that um so that we can then afterwards maybe in a few years come back to these conservations considerations and then think about whether they actually made sense um or whether we actually need such a large scale protocol and this was actually then maybe also why we had these discussions that I presented on the second to last slide um so do we even need this large scale protocol or are we fine with having specialized protocols for specific applications because those two papers that I referenced there they actually"
  },
  {
    "startTime": "01:10:00",
    "text": "provide solutions for specific problems and they I think um yeah solved them quite well and could be used as a first step towards this direction already yeah thanks um let me just just quickly um I think it's also a question of how you frame this problem um so if you have the mental model of say getting data from one end to the other and then doing some computation in the middle um this may give you some kind of TCP like framing or something um I'm not sure that's that's necessarily the best um say mental model for thinking about Computing the network um so again thinking about like data flow systems where um it's all about you know carrying bits with some modification in the middle from from one to the other it's more like you are you having like discrete steps of computation and each step produces something completely different and um so maybe then this this whole connection or transport um metaphor is not not exactly helpful yeah definitely okay thanks um next in the line is is rolling yeah hi thanks uh well I like the approach to to try to think about how to maybe more align with the end-to-end argument um to me the end-to-end argument has has basically two advantages if you apply it one is the robustness because he has less less functionality inside the network so one aspect is maybe to think about how those partial functions influence or may influence each other so that that"
  },
  {
    "startTime": "01:12:01",
    "text": "could be I mean there could be some kind of interdependencies or side effects that should be avoided I don't I didn't read your draft and and you know all your work so that's kind of new to me um so maybe you discussed it already the other thing is innovation uh protection of innovation or I mean the the argument is is it's hard to change the network and when we have functionality in place that allows that like P4 and what have you um that's maybe not the let's say obstacle anymore that it when when it was the time of writing the end-to-end argument or um but robustness is the thing we should care about and uh so it would be good to to think along the lines um trying to to not break things uh one more observation uh when you were talking about addressing I think it complicates things a lot if you require applications to have knowledge about your network so one thing is that the network tries to figure out what's um what should be done uh in order to support the application but another aspect would be to to require that the application has uh some some knowledge of uh where to locate the functions or where to invoke the functions and I think this is maybe not a good direction to do that right because it makes things more complex and I can remember discussions here in the igf where the application developers said um please um no don't do that yeah thanks a lot of so that's a lot of interesting thoughts maybe to quickly answer to a few of them uh so regarding the robustness aspects"
  },
  {
    "startTime": "01:14:00",
    "text": "or the interaction between the functions uh I'm not sure if we have actually included it in the paper but at least in the draft we discussed stuff about this so for example regarding um regions so basically having re-transmission or if we would like to have some kind of reliability maybe um how would we then handle for example if we have two or three functions and then the packet get lost gets lost after the second function and we've already changed state in the first and second function maybe how can we actually handle this so we have at least I think discussed these problems in essence and as I already said we mainly raise questions and not don't provide much answers but that's a very valid point um and then yeah I actually have lost the track of the other aspects but maybe we can discuss that also offline yeah yeah the other aspect was addressing and requiring applications to have knowledge about functional support in the network which complicates application writing and makes it complex also on the other side not only complex in the networks on the network side but also on the other side yeah exactly that's why we've basically discussed these different possibilities that we have there um and I think the so at least the source routing aspect would would require knowledge by the endpoints to Route it through there while the um yeah if we do the the information Centric networking aspect maybe and only know we would like to have this functionality then this would of course then again reduce the the complexity so of course there are different that's why I said we have different solutions and we now have to think about which we would actually like to deploy but I think your considerations there are really helpful in that context um we have two more people in the queue uh Lars it's largest let me jump in this is Andy"
  },
  {
    "startTime": "01:16:01",
    "text": "Reid I'm sorry I couldn't work out how to raise a hand on the ins on the on-site Tool uh firstly thank you very much a very interesting paper and I think the connection between the transparency of the end-to-end principle and addressing seems to be a really Central issue and it's very similar to some of the aspects that we address in the paper presented at last irtf I'd be very happy to very keen to develop that further I think we started from slightly different start point that came to some fairly similar conclusions uh and I think understanding how you define your addressing and how you define your transparency is particularly important and I think of it developed that idea both potential getting some real Clarity of what in network compute actually means yeah definitely yeah let's discuss this then offline or later on hi so um Lars Eggert um first yes Public Service Announcement if you want to be in this room you gotta wear a mask You Gotta Wear it over your mouth and nose and it's got to be an ffp2 and 95 or better mask if you don't want to comply you can leave the room and join the session from your hotel room or somewhere else so please comply I know everybody's ears are falling off I know it's not consistent with what's happening outside the session but it is the community consensus that this is the policy that we have so thanks for complying and uh provide input for the next consultation what we're going to do up masks that's it nice talk thank you um I've sort of two points that sort of correlate to the two parts you had in your in your um slide so it's a work for a company and and we do uh you know the very early stages of of coin that we put stuff on fpgas and we're thinking about what might we want to put further into"
  },
  {
    "startTime": "01:18:01",
    "text": "the network and so the outline here in the beginning about the end-to-end principle sort of matched sort of what I thought intuitively but I hadn't really written it down or thought about it sort of formally in unstructured way so that was very useful thank you I think you got something there um that that makes sense so so think further in that direction very good um and second on the transport protocol side so again from the sort of practical view that you know one I want to put stuff that's expensive on the CPU somewhere else where it's cheaper um so I want to do the minimal thing um to make my current workload my current application faster that means like I'm not I don't want to bother with new protocols I don't want to like I want I want to put stuff that costs money now and put it somewhere where it's cheaper and have everything else be the same and specifically I want to do it for stuff that is sort of internet related right so the more you change and the more it becomes custom although it might be optimized in some way the harder it gets for me to actually start doing this and so so since this is the irtf which is close to the IET if I would sort of encourage people to maybe think about the low hanging fruit that has sort of easy to get to provide a bunch of benefit already and that sort of move us into that direction where later on we might be able to pick up some things I do understand this research I do anything that's where the hard problems that lets you publish paper so excellent but for sort of us here that try to think about the ITF the things that we can deploy soon are much more interesting to me thank you thanks Eric not Mark yeah I really like the the restaurant I would say last but not least yeah Eric Norman so I really like sort of going back and thinking about the stuff in terms of what what are the implications and sort of framing it the way we've done it one thing I didn't see here which ties into robustness is this notion indeed the issues around state in the network right and and the sort of"
  },
  {
    "startTime": "01:20:02",
    "text": "paid sharing aspects we have when all of the session transport related State lives at the endpoints um so it might be useful to to to add that to the sort of list of things to consider in this picture you raised an interesting question about sort of uh a common transport protocol whatever that means in the this context right as opposed to something specific I think we already have examples where people are proposing things that you might argue not really compute but there's this thing things called in inbound inline oam where your compute is just extracting state from the the routers as you pass through them right so you can actually now measure things in more interesting ways but but it said we have that and then we have ICN at the other end right yeah what what are the things what is from a research perspective sort of ignoring Lars you know we would like to deploy something tomorrow but but you know what's the sort of spectrum over there right yeah I think that's something that's interesting to consider continue to explore yeah thanks a lot for your comment uh so um yeah there is a lot of or a large spectrum of possible solutions so from something with only minor tweaks maybe and deploy tomorrow to having something really brand new and taking a lot of years to to deploy um so really interesting to see where this will go perhaps we'll need more uh practical experience first as Dirk mentioned in this in the first question and then regarding the robustness aspect as I said to uh olan's question earlier I think we have some discussions on that in in the draft I'm not sure if we had discussions about state but we had at least had it on in our mind when we wrote about it uh about the stuff uh so it was there but they are also definitely something that we need to consider when deploying things in these directions yeah so thanks again for the for the comment for questions or both"
  },
  {
    "startTime": "01:22:00",
    "text": "okay um so for sake of time we'll go to the uh distributed Ledger thank you very much um Ike and uh and and Dirk also who was part of this um so yes um next presentation them uh okay can work this speaker is not here so it's is it remote or um yeah I'm talking today hi everybody I'm David RW David can you share the slides or you want me to emerge with it I I will actually do it thank you yeah um so uh hello everybody thank you for uh attending this session so I'm talking today about insights on the impact of the elts and provided networks is a joint work with Derek trust and Mike McBride and seen Farm Xin Xin um next slide please so we have been working before already so we published in this ISE white paper um some some related material we started with a simple experiment trying to measure how was the dlts behave in our internet we realized a sort of a passive measurement we did some basic analysis and we also wrote this this draft that is published there and the upcoming paper has a little bit more analytical instructory analysis We compare with the studies and is about to be published next slide please um so for instead of understanding on a high level and how it is permissionless distributed consensus systems are thought and how the designers thought"
  },
  {
    "startTime": "01:24:01",
    "text": "about this how to approach the permissionless approach they take basically the so-called distributed hash tables that are nothing else that a place where where files are they close and receive content and the goal for these distributed hash tables was in the beginning for two decentralize and distribute the file system a neighbor file system in a permissionless fashion or faulty Network then over these network files someone can agree on the status in a state machine so the goal here on the distributed consensus systems is to decentralize and distribute State machine in a permissions fashion as well over 40 Network and after that it came the distributed later technology that is nothing else that a consensus oriented system that is agree on distributed content it resembles a voting system and it often execute the consensus or not replicated Ledger and currently is built over internet Scale based on peer-to-peer Networks um so our next slide please we identify in these systems in these three basic interaction we call the DLT service interactions where a client for example commits a transaction or a request to the distributed consensus system a miner on another peer can commit or a found block um for the truth as a voter in the distributed consensus system using previously discovered clients and any client at the end can read the blog how these interactions are realized in Nexus live please um actually we identify a key mechanism"
  },
  {
    "startTime": "01:26:02",
    "text": "to realize these interactions it's the so-called Atomic broadcast and for the case of these larger scale system these Atomic broadcast is randomized over a set of receivers mainly to avoid possible collusion of fires of a stable receiver set and to ensure distribution of Ledger information across all over all over the peers over the time it comes with permissionless approach but also deals with the scale nature of these distributed system and it's been data set before as a peer-to-peer system on top of Ip networks using GDP TCP and quick next slide please we identify in these build systems these communication patterns and we notice in the left hand side for example we identified the discovery part of the protocol and on the right hand side the pool establishment a part of the protocol the pool is necessary to execute the randomized broadcast as said before that is the key core mechanism to diffuse information over the entire system to agree on the state of the of the of the system and the discovery part starts with a load of bootstrap nodes from a list of the ltpers that is housed on a specific IP addresses all around the world we randomized this list and we tried to contact them executing UDP ping and pongs we identify which peers are reachable and which ones are not and to the ones that were reachable we sent queries to request more more nodes with these notes we again randomize this list of DLT peers to establish"
  },
  {
    "startTime": "01:28:01",
    "text": "upper layer communication something based on TCP Transport Security we execute capabilities exchange and we end up adding this peer to the pool that we are going to use for executing a broadcast or try to execute broadcasting the entire system next slide please so what is changing what is challenging about this we identify mainly a cost for pool maintenance for example peers need I said before to continuously establish and maintain these reachability information to discover new peers and to send information about the latest state of the blockchain each peer needs to maintain constantly changing this pool so it means that it's always constantly exchanging information about signatures TLS TCP and session in general we identify also because for resilience and reliability how the failing nodes are causing latency on the pool establishments hence is also delaying the distributed consensus and for the case of TCS and content retrieval 40 cases of distributed hash tables we also identified the timeouts that are inducing removal of peers and that causing the replenishing of the pool that's where we identified that matching capabilities in in these peers at the scale is very costly um and we as well identified the unicast replication for the DLT to work and as well as some issues with the IP address privacy because when you try to join these networks your IP address is exposed to the entire system next slide please so we set up um that would scale"
  },
  {
    "startTime": "01:30:00",
    "text": "experiment and we classify the peers that we look we compare and we also Identify some geographical distribution we identify some centrality things but for today we are exposing more Network oriented results and we will start with full establishment time this is the time that a single peer from a local computer will use to build the pool of peers that is willing to broadcast information to so uh in the first plot on the left we identify for a single sample how long did it take to um yeah to uh to complete the pool establishment why we um and we identified the one-third of the the total number of the pools at TN over 3 and we identify this as a single random variable and we analyze uh our entire experiment and preaching this probability distribution We compare and approximate with a log normal scale and with a power load distribution for the two random variables uh next slide please uh why this time is um so some sort of huge uh and what are the components we try to analyze from the point of view of identifying what is happening to discover a peer uh for outgoing and for incoming request as shown in the plot for example uh on the left side we identify the number of attempts that our node tries to execute over the Internet out of these how many were reachabled it is in the purple line and out of this how many Discovery and attempts I"
  },
  {
    "startTime": "01:32:02",
    "text": "executed and how many success we got for outgoing and for income in request next is like this yeah um after we discovered the peers that we want to communicate we are willing to communicate with we start by executing an attempt of TCP socket initialization and that's plotted for outgoing request on the left hand side as a red curve out of these for example we we were not successful in a certain number of Transport Security negotiation or capability checkpoint of capability protocol this capability protocol for example is an upper layer protocol for ethereum that it requires to run in a specific version or the checkpoint makes reference to for example the latest blog that was agreed in the entire system uh next slide please so we were thinking uh so with these observations um like for example that miners provide a service capability to others binders um the communication is some sort of constrain it and so we need to negotiate uh TLS capabilities certain sort of hardware and we need to identify the blockchain checkpoint to to get the right minors um and what is more is this group of peers are instantaneously randomized to ensure protection against collusion or what is so-called Eclipse attacks um so um the put creation is done at every peer and is the core mechanism to enable this this operation trying to execute a broadcast to the entire system based on unicast operations"
  },
  {
    "startTime": "01:34:00",
    "text": "these these operations is done on a fixed group size and is identified it's defined through heuristic there are some theoretical balancing this heuristic based on to Define when the system is going to converge and when it's not going to converge so can the network help with these put these observations next slide please um so we think for example that the previous slide yeah um that programmable in network compute capabilities can help with realizing some some of these aspects that I just showed like for example use service Centric abstraction where miners are service instances to DLT to a DLT service the routes become a pool of service instances to enable instantaneous randomization on the end point um the reachability can be improved through the encoded constraints or in in an email structure and we can replace the randomized unicorns with a forward multicast capability that is built in the network for a fixed side of of peers we can ensure as well that every requests leads to a randomized set of peers which includes the use of IP multicast um some thoughts are very welcome some some questions as well I'm happy to discuss and take your questions thank you very much foreign are there any questions uh if not actually we're getting late so"
  },
  {
    "startTime": "01:36:00",
    "text": "maybe we want to move to the next presentation which I'm going to load right now um I think it's certain do we have yes we have pascalsan so there we go uh pascad please hello can you hear on me hello okay uh uh so um and so I subject of this talk is a presentation of this draft which is called ioc and I use system for alternative secure events and it is an architecture of secure element in the internet whose resources are identified by Yuri so next slide please oh okay you can control it I don't know uh so two words about secure events uh secure event contains 35 microcontrollers and embedded software so they have it evaluation Assurance level up to El 6 Plus even a square rooting from one two two to seven according to Common fetalia so you all know Security man because the second element are used in a bank card SIM card passport and so on so there are a lot of secure elements produced every year so 9 billion last years and uh small CPU with a very modest quantity of S1 and a non-volatile memory there's the Next Generation and in the Next Generation you have a more RAM and"
  },
  {
    "startTime": "01:38:01",
    "text": "more fresh and it's welcome to notice that all the cheap include the crypto processor so it means they have a modest Computing resources but they are able to compute a usual cryptography with accepting acceptable performance Legacy communication news cellular interface normalized by ISO 1716 standard but you can find a to c interface or SPI interface they exchange a small packet which are named apidio by ISO 7816 the small packets means about 20056 bytes they have open programming on their own months for example Java car the 6 billion jerichada produce every years deploy every years let's say that most of Simca for example use it means that you can write for program in the java Cloud language which is a subset of Java or you can another use a usual programming language like like C and so on and at last but not at least uh there are a secure software management framework which is a standardized by the global platform Consortium and that is supported by quite all securement and this is used to list the delete and upload application in a secure element for example mobile operator use over there technique in order to download application in SIM card so next slide please um so in in this graph we want to connect"
  },
  {
    "startTime": "01:40:00",
    "text": "the security element to internet and uh why we want to do that we want to deploy a online crystallography cartographic resources for internet uh you you user the ID is uh well when you need to to store some key or cryptographic resource in an offline mode you may use secure element and so it may be useful for internet user to have the same level of trust but for online resource and so we want to identify these Resources by a uniform resource identity shares the issue is that obviously we will need an additional processors to to do that with a network interface and DCP Eco connectivity uh we need to ship our Global platform for on-demand applications not mandatory because you can use a pre-loaded application but for on-demand application uh the first mature the user will ask the provider to download a new application in a secure element so we we need this support uh we need a protocol to access to secure element resources and this draft we we chose basically TLS as a protocol for user interface service interface we need to Define secure element naming in order to identify this uh secure amount of our internet and we need an attestation procedure for on-demand application the goal of attestation procedural is to give the user a sufficient level of trust that is really using the secure amount you believe it is using with the right application inside and the right Hardware or provider so next slide please foreign draft you see the the kind of uniform"
  },
  {
    "startTime": "01:42:01",
    "text": "resource identity here so the secure element is identified by your name actually server name which is called scl it's used at this moment uh pressure key that's to say a symmetric secret is associated to a server name and import and according to a given scheme in current GitHub open application we use simply command line and ask online it's a kind of shell which is secure by your TLS and according to this game is some query to the secure elements and get the the the response so in the drive the way you see Server component at first for administration plane the protocol both racks which was designed let's say a few years ago and which used Speed Demon for the server plane we use the TLs for secure remote which is TLS server 1.3 with pressure key and and if you manage change for computing as a sheer CE grid and this use also TCP demand and the attestation procedures makers the staff need to transfer the element control to to user so it relies on two properties first the second element can be close and second they manage only one utilization at a given time so next slide please uh so this is a short uh review of the administration plane so rocks basically is something that works over TLS using uh certificate both for server on client it's been PTI it's a picky IMO model and so rocks is able to transport ISO"
  },
  {
    "startTime": "01:44:02",
    "text": "7816 packet and in order to use something called secure element identifier which can be for example a slot physical slot on an i2c address or a name and because the Iraq's transport ISO 1716 with policy access uh he is able to transport a global platform protocol and so it is able to uh to to to perform uh the delete and upload the application operation in insecurements and so next slide please for on the sales plane uh we use some things called the TLs for secure elements that you say a particular profile of TLS server at this moment you using a pre Pusher key and uh server name TLS says server name we put the server name uh in a field called answer to request which is obtained when you reset when you reset physically on the when you use the the reset pin on a secure element you collect some things calls an answer to request and there's an API to to put a in this answer to request you have something called historical bytes up to 15 bytes and uh you have some apis that enable you to put whatever you want in the historical back so really at this physical level we put the server name and so after we Define an interface to transport Terrace packet for the iso 7816 interface so there is a client facing server"
  },
  {
    "startTime": "01:46:01",
    "text": "and this client first things the other according to service name indication find the clientele uh the server name of the Beckham's server and so if this server is present the security mode is present and on the system after a while it wrote the incoming and outgoing packet to and from this TLS back in the server and uh on the client size you can to to access so on the client side as you see on this Dragon everything is based on TLS and TCP it means there is no it's pure network interface and you may use some identity more module in order to compute the procedure or required by the appreciate use in TLs but it is not mandatory so next by please and finale this is the undermined application illustration and an attestation sorry so you see on on the on the left you have the application provider and on the right uh the user and in the middle the iosc server which is the infrastructure that all the set of security elements so first the application provider or use racks to download the application in the secure elements and then binds the security name to the secure element identifier and at this step the secure element as an application and this can application can be remotely used as a TLS server and it store the pressure key defined by the application provider when this application starts in the secure element is create a pair of public and private key and the public"
  },
  {
    "startTime": "01:48:01",
    "text": "key is the identity of the secure element then after a while the application provider delivers the public key of the component and deliver a certificate and this and the the pressure key of the component and the server name of the component to use the user the user open till this connection with this component with the public key it checks the certificate and after it verifies that the secure amount both know the unshake secret of the TLs connection and the pubic key and if the secure amount knows this both parameter it means that there is not a man in the middle because only one Terrace session can be managed at a given time and because the second element cannot be close is the only component that stores its uh this pair of cubic and private key and at this level the user can modify the pressure key and so it's mean now it's the only user only entities that can remotely connect to this secure element so next slide please so at this moment all this is a available on the GitHub there are no patents and and all the code that is open uh so the code for tlssc for General care this code works with many Java cards current level of java cars that you can buy on the internet or is a 3.04 or 3.05 this is the level of the Java card API so if you go to to GitHub you you will find these implementation that were"
  },
  {
    "startTime": "01:50:01",
    "text": "quite should work with most of java candles on the market and as the scheme as a syntax it simply use a common line so it's mean when you want to create a key or to perform a signatures you just open a TS session using open SSL and whatever you want with the secure element and you just send the column line that created or sign and whatever uh the good source of the server is at this level at this moment is V5 uh it's a commercial so it works with Windows it works with uh like Unix and the Run months and things like Raspberry p and this is an open implementation of the server that includes the two TCP Diamond one for racks and one so TLS and inside the software there's something when you use a secure element on a PC or Linux and whatever use an API called pcsa we we each mean uh pcsa means smart PC and inside the software there is an enumeration of this API so it's mean doing that the software can be adapted very quickly to many kind of communication interface with secure element like obviously pcsc or i2c or something score similaras that exist today in the markets in a real array of SIM cards that are used for for roaming purpose and so they have specifics so so get the interface so next slide please and that's it so smart question here you see there's a list of papers that's describe this and and more and uh because you do to cover it and due to the fact that most of conference were"
  },
  {
    "startTime": "01:52:00",
    "text": "online this last time the video on YouTube that explains the the paper and give illustration of the process and so on and so my option is that this is this is what we become working with item I believe it's open it's something it's not it is in the I believe it is in the scope of the coin Energy Group and um and that's it I am done thank you there's one person the queue is I'm just going to Echo what was um just asked us a question on the uh in the chat which is can you help us particularly those of us who are in other time zones so we're only half awake um but you know the talk was very interesting thank you for your talk but kind of the fundamental question is help us connect the dots between what you were talking about and how this relates to in-network compute and is it I mean for me I I definitely appreciate that there are um uh potentially constrained devices that need help or processors um that help to secure compute and transmission um but I'm not sure if that was sort of how you would connect the dots so can you can you help explain to us how um more pointedly this relates to coin thank you well when we use a security mechanism in the networks when you compute cryptography I I'm thinking to the produce um presentation for example that was speaking of uh"
  },
  {
    "startTime": "01:54:01",
    "text": "of blockchain issue and so on when you skip the Laughing the network you used to have some such place to store Key and compute typographic procedures and as it is today when you want to do that you could use stuff called Hardware secure module in in the crowd for example but let's say that as a user level you have no Trust Insurance about that and as it is today you have no open features uh so the the relationships to to Corn is that when you need some this is a way to deploy uh some secure procedure in in the internet so it means each time you you need to to do some things to perform more complications to to perform signatures this could apply and with some open stuff it's not so simple to to get open stuff today and provable stuff it's mean what is important with secure events these are you have some ear level and this level are certified by a national Security Agency usually managed by governments and this means this is your root of trust and our manufacturer manufacturer from"
  },
  {
    "startTime": "01:56:01",
    "text": "that and many standards that apply to to to to this kind of components so by natural you have a lot of of this component you have 10 billion securement deployed every year so it's very huge and the level of trust everybody knows that it's not so easy to hack your banking card with a chip inside and so if you do that you you will get some money but in the reality this not happen it's very difficult to download the software in a bunker it's very difficult to recover the car the the keys that are stored in one car and so relating to coin energy it's mean it's a way to have a procedures Computing for procedure or still in the internet with I believe uh not so bad the level of security and Trust for the user another person in the queue and um in the interest of time perhaps we need to take that to the list um Emmanuel but Charlie uh if you don't mind uh suddenly not either in the chat or to the list or both I think we have Okay um yeah oh my God uh yes I will just like put the next slide I will go back to well maybe we don't even need to have slides in dress up time uh yeah I'll load the chair slides um"
  },
  {
    "startTime": "01:58:06",
    "text": "okay uh we're at the last slide okay so um it's um the um some of the root so troop topics uh actually it the the first one the question about the interim goes to the um the second bullet actually which is uh we wanted uh today to have a presentation about uh the chair Reflections after three years um what has been the evolution of the group I think today we had uh you know some uh presentations that went back to the original intent which was like looking at transport looking at security but this field has um it you know exploded in the past uh three years and we wanted to have this maybe this reflection and we're thinking uh we should have maybe uh an interim where we would uh first go through uh the uh the whole list of um Publications drafts and and related and see you know where we want to move things uh Pascal asked for uh you know should we have uh his his his draft as a working group uh we actually put that on there on the list by the way and you know go through the the the current Publications there's some that are um expired that probably needs to be uh re um re um kindled so we'll do that probably um maybe in in January uh obviously there's going to be ietf 116 in Japan"
  },
  {
    "startTime": "02:00:01",
    "text": "and March and we're going to most likely hold a meeting there um and now my phone is telling me that it's 6 30. uh and then um there's this uh 5G uh net app lab proposal solicitation there was an email on October 25th which is a 5G um I think it's an EU project and I looked at the program at hot Nets next week and there's a number of interesting papers that are um related to this community and we're out of time and we're going to send Eve to bed which is like 3 30 a.m local time for her and I thank you so much for attending we had a bunch of people online we had a bunch of people in the room uh thank you for people who presented in particular uh thank you for your dedication for taking the time to do this presentation thank you for the people who asked questions because that shows that you follow what's going on and it's really great thank you very much for Cedric to have been our proxy in the room and thank you so much for having done that and uh thanks to Jeff but Jeff for you it's uh it's indeed well it's early early evening now so you're probably okay uh thank you everyone and uh we'll uh have this interim so we'll probably see you remotely sometimes in January and um thank you and have a good rest of uh the week thank you so much thank you very much since"
  },
  {
    "startTime": "02:02:29",
    "text": "foreign"
  }
]
