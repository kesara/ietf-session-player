[
  {
    "startTime": "00:00:38",
    "text": "um good morning hi miriah yeah good morning everybody hi dave good to see you i think i'm gonna stop sharing this slide in that you take the the intro slides okay um yep that's fine so for some reason we uh i see that slides and make acre but i don't see them in the room so let me try again"
  },
  {
    "startTime": "00:02:03",
    "text": "ah now it works okay david should we start yeah okay then um hello everyone and welcome to the maverick session i'm happy to be here and actually see some faces uh and i also see dave's face so that's great um and let's start right away luckily we got a two hour slot and we got some really nice presentations so we should have plenty of time today yeah so this is an irtf session but we also have a note well it's very similar to the ietf note well and if you're not familiar with it you should look up the respective rc's maybe this session will be recorded and will be found on youtube later on so please be aware of that no matter if you're in the room or a remote participant then also the irtf follows very similarly the private c policy and the code of conduct of the ietf so basically this is just a reminder to be nice to each other to work together in a friendly way and also for the presentations state your questions clearly stay friendly and most usefully for this session you know provide productive feedback if you have some i said this already this is not an ietf session isn't it's an irtf session so what we do here today is mostly inviting researchers to present their work and building a bridge of between these communities to the itf okay um this slide is usually just here so if you have the slides in front of you you have a quick link to everything you need to know or you don't know yet um just um as a reminder because this is our first hybrid meeting the people in the room are also"
  },
  {
    "startTime": "00:04:02",
    "text": "supposed to use the virtual queue and you can join the queue over the meat echo interface either this light interface or the full interface post is accessible over the agenda and more importantly is you have to please join even if you're in the room one of these two ways um to join the meat aqua session because that's also how we generate our blue sheets so if you're in the room please join the meat echo as well so you're noted in the blue cheese yeah and like dave if you don't have anything else basically we start our presentation from here perfect um then our first speaker is actually in the room uh just arrived first time attendee and we're very happy to have you here so how much if you want to come in front and i'm getting up the new slide set for you so if you if you press this it should do something which it doesn't oh wait i have to acknowledge that oh no okay wait sorry i did this wrong with your knife yes okay hello everyone can everyone here be"
  },
  {
    "startTime": "00:06:01",
    "text": "fine uh i'm hamas bentanvir and today i'll be presenting my work called uh glowing in the dark so this work is in collaboration with uh raji singh from microsoft research paul pierce from georgia tech and rishabh nitham and who's my phdwell so just to give you an overview of where this work fits in so as everyone knows we have ipv4 we have ipv6 but then we have scanning in v4 and scanning in v6 so scanning in v4 is very well understood there's been a lot of studies about it and people basically know how to measure it and how to do it but things are not the same in v6 so we know that v6 is a much larger address space and scanners basically need to figure out where these active spaces of addresses are before they start scanning so there's been previous works on trying to figure out how to measure scanning in ipv6 but there's been no work on measuring how scanners in the wild like what techniques they use to scan active regions of ipv6 added space so i'll get it bent into what scanning is and then move to ipv4 and mpv6 and then go on from there so yeah so scanning is basically sending unsolicited communication to an ip address in order to draw a response and it can be done for a lot of different reasons so reasons both malicious and benign but in this talk we are more concerned about not concerned about how what scanning is but how scanning is different for ipv4 and ipv6 and why why is that the case so scanning in ipv4 so there's there's a lot of tools for that so for example if you use zmap you can scan the entire ipv4 address space within a matter of minutes obviously given that you have a good enough internet connection but in ipv6 things are not uh quite so simple because the address space is now 2 to the power 128 total addresses you just cannot brute forces so what you have to do instead is you have to come up with newer scanning techniques so you"
  },
  {
    "startTime": "00:08:00",
    "text": "can go through the y2v6 address space trying to find addresses that you previously didn't know of and these newer scanning techniques is exactly what we characterize in this paper so just to hammer in the point that how bigger the ipv6 space is in relation to ipv4 so this uh image helps me hammer it down really well so let's say if all of the ipv6 added space were equivalent to the 4.5 billion years the earth has been in existence the ipv4 added space would equal two trillionths of a second or the time that light would take to traverse the period at the end of this sentence so basically you just cannot boot force it it's it's not really possible so given that this ipv6 space is so large how do you even start to scan it so right now there's two major techniques that you can use so one is called ip scanning and the other is called nx domain scanning so in ip scanning what you do is that you go to these public sources of ip of ip addresses like dns zone files or tor relay consensus data or ntp public servers you get all these addresses you try to figure out patterns in them and then you generate newer target addresses to scan so basically what ends up happening is that by learning patterns in these ip addresses from public sources you reduce the search space where you want to look for new ip addresses but nx domain works very differently so this is so nx domain scanning basically involves exploiting semantics that were described in rfc 80 20. so i'll go into a bit of how uh what what is defined in rfc 80 20 and hardworks but just to be just to make sure that although uh the the search space is reduced for both of these uh both of these scanning techniques for ip scanning uh the results are still probabilistic so you might find 10 you might generate 10 more addresses but you you still might you still cannot be 100 sure that they they are actual addresses which are allocated"
  },
  {
    "startTime": "00:10:00",
    "text": "but for nx domain scanning you will always get to get an ip address that is allocated so what is rfc 8020 so rfc 2 8020 is called nx domain there really is nothing underneath so and so one of the main clauses in this rfc is that an nx domain response for a domain name means that no child domains underneath the kuwait name exist either so when you apply this rfc to dns reverse trees it unintentionally presents a side channel for efficient scanning of the ipv6 address space so let's say over here if you if you have um the ip 6. which is the uh root for dns ipv6 reverse uh zone trees so let's say if if if my dns uh resolver is basically set up to reply and to uh reply next domain for if something does not exist under a t and let's say i look up zero dot ip6 dot arpa so i will end up getting an nx domain this means that i no longer need to care about zero dot ip6 or arpa so i will go to zero till e and when i reach f i'll finally get a no error but for all others i'll get an nx domain which means i can just cut all of those uh sub domains out i don't need to care about all of them so now i'm only left with f so now i can proceed with zero dot f dot ip6 dot arpa and i can just go on until i've reached the whole slash 128 ibv6 address so we have somebody in the queue but usually takes questions at the end or do you want to take it right now i'm happy to take it right now okay go ahead peter hi it's peter from the esec actually i don't mind asking in the end also but so my question is um this reverse um inference technique only works when um the ip addresses actually have reverse delegations exactly the dns i wonder if it's known what the fraction of that is so there has been prior works which kind"
  },
  {
    "startTime": "00:12:00",
    "text": "try to learn new addresses using these techniques and the amount of addresses that they generate so i'm not sure about the actual number but it was a lot but there's no kind of study about how many of the total ip addresses have a reverse domain attached to them but that's the really nice points we can kind of like incorporate it in future studies okay yeah so just to put this into perspective of a slash 64 i subnet so this is the subnet where a host resides in ipv6 let's say i am a scanner and i want to find that little green dot that you see at the top and that's the whole that's the 128 ipv6 the whole ipv6 address so now i have two 65s over here so one on the right and one on the left so i know that my address resides in the slash 65 on the left so i end up sending queries to both and what is going to end up happening is that the right part is going to return nx domain but the left part is going to return no error so now i know i only need to focus my efforts on the left part of it so i'll do the same for the slash 66 is on the left so what ends up happening is that i get an x domain for the part on the bottom but nowhere for the part on the top and i do this for slash 67's eights nine still i reach one slash 127s and in the end basically i reach the whole slash 128 ipv6 address space so what this ends up doing is that it reduces the number of potential probes from 2 to the power 64. so basically one for each ip address in this whole space down to just 64 total probes to get to the actual address and this is a significant exponential decrease in the amount of probes that you need to send to find a host inside a slash 64. okay so now i'll talk a bit about the experimental setup so there were a few goals that we had in mind which uh which were motivations from the previous studies that were trying to study at scanning in ipv6 so most of them were basically based on studying"
  },
  {
    "startTime": "00:14:01",
    "text": "ipv6 scanning in darknets or from the vantage point of an authoritative dns server but the problems with that was that most of the scanning activity they got was as a result of ipv6 misconfigurations or they could not link the activity inside the ipv6 address space to the scanning activity that came in so for this experiment we wanted to mimic an active ipv6 address space capture actual scanning traffic so nothing from its configurations and link scanning activity to the services that were deployed in this uh in this ipv6 address space so so just to give you an overview of how my experimental setup uh so the grid that you see so this was supposed to come up one by one but i don't know what happened so the grid that you see so this is divided up into four slash 58 so one of them is used to run our services and the other three are controlled ones so nothing was ever run in them and these are all the services that we run for the entirety of our experiments so the reason for running these services is basically to incite uh scanners to tell them that yes this is an active address space and you you should look for stuff in here because we kind of give them a signal that yeah we are running stuff in here and you're more and you're likely to find more addresses here because you just simply cannot brute force everything so we capture all dns and reverse dns lookup logs and all the incoming traffic from a router that was appointed here and then we had two types of address assignments for each of our services so each service was deployed had four instances and they were never deployed in the same 64 subnet and not in this in adjacent subnets and each uh service had two lower byte assignments and two random assignments so lower byte assignments are addresses that people put up manually just to remember ipv6 addresses so they will have a lot of zeros in the start and like a couple nibbles at the end with a value like one two just to remember the"
  },
  {
    "startTime": "00:16:01",
    "text": "address and the random assignment does not follow this pattern so it's a random string of numbers throughout the subnet throughout the interface identifier okay yeah so some of the key observations that we saw that uh so we do so our method of measuring scanning activity so we did see scanning activity even before we ran any of our services but the main point was that scanning activity increases significantly after services were deployed and this increase was not only not in terms of the number of scanners but also in the number of probes that we see and the main uh observation that we made was that nx domain scanners are using the side channel very very effectively so in in this graph you can see that we have the duration of our experiment on the y-axis and uh the subnet id is on the x axis so 0 0 to f 8 so it's 256 in x so the green highlighted portion basically shows our slash 58 which had services running in them and the other part is the control part so this is the part before any of our services were deployed so right now you can see that nx domain scanners are kind of like guessing even the ip scanners are guessing but nx domain scanners probably think that there is something in the start and the end of our subnets and ip scanners think that there can be anything everywhere but the important part comes in when we deploy our services so as you can see in the for the nx domain scanners they never go outside the slash 58 line so they know exactly where the services were deployed and they stay inside that they because to basically cancel out every other subnet they just need to send in one request to each of those subnets if that returns an nx domain they can just cancel that out so they don't even need to check them so for the entirety of our experiment they only stay within that slash 58 and on the other hand ipv6 scanners they constantly search the entire 256 uh subnets trying to look for"
  },
  {
    "startTime": "00:18:01",
    "text": "newer addresses so this basically tells us that this uh the rfc 8020 semantic is actually being exploited by a lot of scanners right now which reduces their search space by a lot and uh this is our results for all of the services that we ran so just to give you uh just to like explain what this means so for example if you see wget and 511 that means that the mean number of scans per subnet that was running the service that you get got 511 more scans uh after the service was run as compared to before so delta diff basically means that the increase in scanning activity was within the treatment subnets so where the services were running and delta c says that this increase in scanning activity was in uh control subnets where nothing was running so some of the key takeaways we have are that that nx domain scanners target treatment subnets for almost all the services they target control subnets much less and ip scanners kind of exhibit mixed behavior they target both treatment and control subnets because they don't really have an idea every time they want to scan something they have to go deeper and deeper inside the subnets themselves to figure out if there's an address in it or not on the other hand nx domain can just send one request if it gets an nx domain it can just wipe out the entire subject and nx domain scanners target different different services than ip scanners so as you can see all of this services are kind of targeted by nx domain scanners on the left but iep scanners only seem to care about dns probes ntp servers and dns zone files so some of the key takeaways we have are that analyzing dark traffic is really not the best way to study scanner behavior particularly in v6 where you need to give out a signal that yes there is something active going on in this address space to invite those scanners and then most scanners aren't really"
  },
  {
    "startTime": "00:20:00",
    "text": "using uh nx domain scanning but we think that this is only a matter of time and one question i pose here is that is it is the efficiency from nx domain responses worth the loss of defense against scanning so so rfc 8020 was initially introduced to improve the efficiency of caching of dns trees so if it if site channel gives you some much easier access to scanning is it really worth it and then added discovery methods are very different than we expected we expected that people will go to uh scanners will go to these public lists of ip addresses like tor ntp get addresses from there and then scan but they don't really do that they mainly rely on open dns resolvers and then neighboring networks should expect scanning activity so let's say for 64 subnet has something running in them we expect that the neighboring 64s are much more likely to receive scanning traffic uh due to something running in that 64. and so we have a lot more uh takeaways and a lot more results but due to time constraints i couldn't put those in so if anyone wants to discuss those i'll be happy to do it and if anyone has any questions and comments i would love to take them thank you yeah thanks a lot and we do have a cue you can see it over there it all virtually stays so alexander thank you thanks for interesting presentation um alexander mayor from nikkito jt i was wondering did you also look at whether all name servers actually implement rsc 8020 because that has been a big discussion in the dns working groups and actually you would prevent that malicious activity by having a dns server that is not compliant to rsc8020 do you have any figures on that so the only thing that we made sure for our experiment was that our dns server was implementing rfc820 because we want to figure out whether people are kind of looking at are exploiting the semantic but we have but we did not look into how many servers out in the wild are using this yeah okay because like i"
  },
  {
    "startTime": "00:22:00",
    "text": "i suppose that many name servers for ipv6.ourpub would actually be some kind of database generated yeah scans that could easily fake things that's true that's true that can be done interesting operational uh yeah yeah thank you that's good i thank you uh rashford so also on 80 20 um pretty much uh it was a long time to get this actually done to have the kind of answers be more kind of uh the next domain answers to to cover what is underneath and if you don't want to have something in in the reverse three either just don't put it in or put everything in what's that because that's what a lot of people do exactly so i mean i would not go back to not having 80 20. yeah so this is one of the defense techniques that we actually want to propose in this paper that just if you want to comply to rc 80 20 just make sure you put off the scanners because this technique has been also discovered in many other papers that you can discover a lot of ip addresses using this so one of the defense mechanisms against scanning would be that just everything that someone requests for just give an answer for that just given no error for that and yeah that's all right thank you um this is peter koh hi uh again on 80 20. um i'm i'm wondering a bit the the scanning technique was actually described as early as the what 2005 and maybe before 2005 was when roy irons and i had a paper about this and a couple of other things and also we described some mitigation techniques but this was much much before 80 20 80 20 suggests that resolvers implement the semantics but conceptually these semantics were already there so if you buy pathways over that scanning technique is available anyway um so any any recommendation in dampening or or mitigating 80 20 um is probably not achieving uh"
  },
  {
    "startTime": "00:24:00",
    "text": "very much because you can always bypass that and do the scanning but happy to take that uh yeah yeah sure so just to like talk a little about it so what uh the previous speaker suggested that you can just basically return everything something for everything that someone asks so that can be one of the defense techniques but obviously you can still kind of bypass that but i'll be happy to take it off right uh petersburg the thing is that the defense is actually one line in the zone file because you can define wildcard in the reverse tree so everything is compliant but because you define the wildcard you can return yeah you know just an i don't know txt record saying well nothing is here but this world card will prevent synthesis of the next domain yeah so please don't say that rfc 8020 is wrong it's correct but you have to configure the zone file correctly so there's a study by tobias feiberg and he basically discovered that think addresses in the range of a couple million that were previously not known from other scanning techniques so i would imagine that in the wild this is something that people are not configuring in a way that you just suggested so in this paper we want to basically tell them that please just do that it's very simple you can achieve a lot of a lot more uh defense against scanning yeah okay thank you a lot all right thank you so much nice to have you here um and we're switching over to some dns related topics our next speaker is morocco muta and you should already be set up and can start right yes uh can you hear me okay yeah we hear you very well perfect um yeah hi and welcome to my presentation about uh the dynastic deployment metrics research this is actually not a measurement study but a study about measurement studies and this uh was initiated by icann and we are carrying out the study currently together with"
  },
  {
    "startTime": "00:26:00",
    "text": "the folks from anatlapse in short the goal of this research is to identify metrics and techniques that can be used to measure dns deployment now and in the future and finally to recommend an icann a metrics that should be measured to obtain the most comprehensive view of dnsec deployment on the internet and today i would like to give you some background about this project share some of the challenges that we face with categorizing and inventorizing all these different metrics and finally to hear from the measurement community which they think are the most important metrics to measure dns deployment and which are the most adequate techniques let's dive right into the challenges the first challenge is that dns deployment is a very wide field if you start thinking about metrics to measure dns deployment and you probably come up right away with two things how many domain names have are signed with dnsec at the root top level domain names and second level domain names and so on and how many resolvers actually validate the nsx signatures but if you think about this only a little bit longer then you already come up with many other metrics on the signing side that could for example include what kind of algorithms are being used for assigning a domain name or is the domain name does the domain enroll its keys frequently is it using nsg and x3 um or does it support some other kind of dns automation on the validation side you again might think about the algorithm what kind of algorithm does the resolver support what kind of trust anchors does it have configured does it support some kind of signaling protocols and so on and so forth"
  },
  {
    "startTime": "00:28:02",
    "text": "additionally the dnsc protocol is still being extended or at least related extensions are being deployed and developed this could for example include cds and cdns key records where operators can automate the deployment of dns sec so do these metrics do these attributes also should be measured in order to get a idea about dnsec deployment and finally we have the challenge that there are also other protocols related to dns sec first thing that comes into mind is dane should it also be taking into account when measuring dynastic deployment the second challenge is related to the measurement technique each or almost each of these metrics can be measured in different ways uh when we think about for example whether resolve is invalidating or not and then we could use active measurements for example from wipe atlas and issue queries to the results of these web atlas clients and then see what kind of responses they get another approach would be to look into named traffic collected at authoritative name server for example see whether resolver is trying to is requesting dns records and from that derive whether resolver might validate or not and both of these approaches have its uh pros and cons then of course we can not only look into different recursive resolvers but we might want to once you look into the client side whether a client is using only validating resources or not other metrics might include whether we rely on a measurement platform like the mentioned by"
  },
  {
    "startTime": "00:30:01",
    "text": "battleless or we use our own measurements setup where we use the standard libraries and use our vantage points that we have control of or we use some kind of hack where if we for example use a proxy network and then use this proxy network to issue queries from these clients other aspects might include whether we have access to raw data this or whether we actually have access to aggregated data this for example includes our asset data collected from the root service and whether we have full control of the measurements or whether we rely on a third party to collect the measurements and all these different attributes of these measurement techniques have again influence on the metrics that are being collected so for this reason um in order to address these challenges we have a the following approach the first is to get a very broad overview of which metrics have been measured so far by the community and there we mean not only the academic community where we look into academic papers at more the high level high tier conferences and journals but we also look into more conferences where industry partners are also presenting so that we not only cover the people that actually publish their studies at academic conferences and journals but also other contributions as well these conferences they include live conferences of course icon dnsec workshops work and so forth and then we want to carry out a gap analysis where we want to understand whether there are still metrics that have been not covered by related work so far that we use basically our own expertise in dns and dnsec measurements"
  },
  {
    "startTime": "00:32:00",
    "text": "and finally to develop an assessment framework to assess the different measurement techniques and to understand which of these measurement techniques are most useful to measure a certain metric i want to say a few words about the assessment framework that's probably a bit a big word but in the end we of course want to focus on the coverage how many for example resolvers can we potentially cover with a certain measurement technique or how many domains we can we potentially cover but we think in order to have measurements that's measured dns deployment that are also useful for a broader community we also have to look into other attributes of these measurement techniques these include in our opinion the representability to what extent are the parameters known of the different measurement techniques and can we reproduce the results actually independently and also whether these measurement techniques are actually feasible to roll out a larger scale so for example of course it is not very hard to measure the dns deployment and domain names if you have access to all the zone files but the chance that you have access to all the zone files is not very likely so we also want to take this into account when assessing the different measurement techniques and with that i already come to my end of the presentation where i would like to collect feedback from you as a management community and would like to understand which dns metrics do you think are most important to assess dynastic deployment now and in the future does that only include well how many domain names are signed for example and how many resources validate or do you think that also more advanced so to say metrics are necessary and finally if you would think on which basis you would"
  },
  {
    "startTime": "00:34:01",
    "text": "select a different measurement technique which would that be would you only sort of focus on on coverage or do you think that other aspects should be are important as well i'm not sure how much time we have for questions but i will hang out in the chat and you can reach me at this email address if you want to thank you yeah thanks a lot mart yes we have a little bit of time we have one person in the queue already thank you more interesting uh presentation a lot of interesting aspects um to answer your first question i think that what i would like to know is on a global scale how many dns transactions are actually being protected or what is the percentage of the nsac protected transactions on the public internet that's my because that's what in the end actually counts it's a single figure combines popularity of a domain name with the dns sec population and validation population but that in the end is the end-to-end metric to me thank you okay thanks alex yeah i mean um actually thanks for bringing this presentation here and starting discussion um feel free to have more comments on the mailing list have a discussion there um but i think that's also very interesting to maybe um take a similar look at different measurement studies and more broadly figure out these kind of aspects so um very nice to have the discussion in this group thanks okay but if there are no other questions then i think we just move on"
  },
  {
    "startTime": "00:36:01",
    "text": "okay next we have little mao um great to have you here uh i've just set up your slides and i think you should have control and you should be ready to go uh yeah thanks so can everyone hear me yeah we hear you well oh sure thank you uh yes so this is jerome from cwru and uh today i'm going to present our results in uh measuring the support for dns over tcp in the internet um right so here are the topics um i'm gonna cover today so i'm gonna look at the dns over tcp support on two sides of the dns infrastructure the recursive resolver side and authoritative dns server side so um [Music] we just lost your audio for some reason can you still hear us jerome hm that's a problem can you hear us we lost your audio and you lost our audio oh i i can't hear you ah okay now we can hear you again so we missed a little bit of what you said so i think you have to go back a little bit oh and you're gone again jerome might be accidentally muted i'm not sure if that either meet echo problem or problem your site think it might do be worth doing a hunt for mute buttons both hardware and software oh yeah i"
  },
  {
    "startTime": "00:38:01",
    "text": "i was muted and i don't know why yeah okay did you unmute it again no okay okay yeah okay well yeah so i think i'm back right so at the end of the tag go ahead i think we're fine yeah so at the end of the talk i'll just before you talk about the race condition between the race recursively hours and we lost you again okay um what do you think it is brian yeah can you can you hear us we lost you again and maybe it might be your um local airports or something that mute your oh let me take off my yeah let's do that and you have to start again at the top of the slide we didn't hear anything let me take off available we can't hear you again we let's actually switch the presentations um and we come back to you and you can figure out if you can get some other headphones because if we don't hear you that doesn't help um mike would you be ready to just take your presentation first uh yeah perfect"
  },
  {
    "startTime": "00:40:14",
    "text": "sorry i'm just trying to find your slides and have problem about that as well so okay so as screen sharing also an option you can give it a try yeah okay oh no actually i have your slides if that's easier sorry sorry about that so if it's possible i would like just to check if screenshot is working and fall back with yes okay now we see an empty screen okay perfect let me check and welcome thanks thanks for having me today um so i have some echo here maybe from the room i'm not sure yeah maybe the easiest thing is just mute the room for yourself ah okay okay now the echo should be gone yes perfect okay um yeah thanks for having me today um so i'm mostly blind and i can't hear you so um maybe we take questions following the presentation all right let's get started so hi welcome i'm mike from technical university of munich and i'm going to present our paper on dinosaur quick which was a"
  },
  {
    "startTime": "00:42:00",
    "text": "study we performed and was accepted at pam conference which is scheduled to happen next week so let's start with the brief history of dns transports in the 80s we had unencrypted dns with dns over udp and then s over tcp so those have these obvious problems problems with eavesdropping and on path manipulations and then we have a shift to the encrypted dns with dns over tls and dns of https which suffer mainly from the drawbacks of the tcp protocol like multi-rgt handshakes and head of line blocking and this is addressed by dns overclick and dns or kick is in its final standardization stage and the main feature is the feature of quick which combined the connection and the encryption into a zero or one rtt handshake so there are a lot of experimental implementations for clients and servers and they are also used in production systems for example companies like edgard or next dns already offer public dinosaur quick services however there are no studies focusing on genocide quick which we did in our study where we first looked at the adoption on reservoirs worldwide and then on the response times of dns so starting with the adoption we performed weekly scans over 29 weeks of the ipv4 address base and we also scan for dns over udp to have this as a baseline as for dns over quick we support the dns over quick versions draft 06 to draft 0 0 and for quick we have support for the rc version and the three different draft versions which are stated here in our adoption scans we collected different metrics first of all the negotiated dns overclick as well as quick versions and the common names of the certificates so this is a distribution here where we have highlighted in the different colors"
  },
  {
    "startTime": "00:44:02",
    "text": "different combinations of dns over quake as well as quick versions so let me guide you through this figure we see that the adoption rises slowly but steadily so from the first week of our measurement to the final week we see an increase of around 46 to 1217 reserves but what we also see is a high fluctuation so only 52 of the resolvers which were available in the first week are still reachable in the last week um if you compare this to dns over udp we have around 97 percent which are still reachable in the last years also we only observe seven pairs of dns over quick as well as quick version and it's important to note here that we added support for version 1 and week 43 only so the ones with quick version 1 are highlighted here in the dark blue bars we can also see that in the final week of our measurement the dns over quick as well as quick versions are dominated by this combination of dns so quick draft co2 as well as quick version one we also see an uptake in the dry and the weeks which are highlighted here of quick version one in combination with dns or quick draft zero two and we could track this to an open source dns server implementation of edgard home so they changed their d4 quick version from draft 34 to quick version one uh in this week and we could verify this also by looking at the common names of those certificates so we find something like edgard does something but something which hints at the usage of the edgard home open source dns server implementation so next to their open source dns server implementation edgard also offers a publicly reachable dns over"
  },
  {
    "startTime": "00:46:00",
    "text": "quick service and they already implemented dnso quick to f03 in combination with quick version one which is highlighted here in the yellow bars and we find these for around 25 reservoirs in the final week of our measurement with the common names dean asterodetkat.com and edgar ch next we come to our response times so for our response times measurement we performed hourly measurements over the course of one week and we use the other stuff ipv4 addresses from the adoption scans we performed a single credit protocol and it's important to note that we have a location bias where we only measured from one vantage point and this is why we did comparative measurements to dns of udp tcp dot indo so in total we find 246 resolvers which support all those targeted dns protocols then we perform two subsequent queries the first one is cash warming query to warm the dns cache and then the actual measurement we collect different metrics we have the handshake time which is the time from the start of the connection establishment until the connection is established be it secured or unsecured then we have the resolve time which is the time from the time we state the dns trivia until we get a successful answer back and the sum of those is the so-called response time other than that we also have a protocol specific rtt so we performed protocol specific rtt measurements with different payloads per transport layer protocol to get an estimation of the rtt which could be different based on the payload you are using in the different transfer programs as the limitations it is important to note here that we do not support tss session resumption or early data in the"
  },
  {
    "startTime": "00:48:01",
    "text": "study which is what we are currently working on starting with the response times so first of all we have to check our expectations uh the resolve time um as it's just a dns query um then the answer from the cash record and the dns sensor as well as the rtt which should take roughly one rtt over all protocols and if we now look at the resolve time here in the cdf we can see that all different protocols overlap so we find that resolve times are identical as expected moreover if we now look at the protocol specific ltd measurements which are shown here in the subplots we also see that all protocols overlap so we find no protocol specific path influences here and if we would overlay those figures we would see that the resolve times and the rtts are actually identical so our expectation of one rtt for resolve time and for the rtt measurement is confirmed we next look at the response times also starting with the expectations so for dns or tcp this is just the three-way handshake we expect one rtt for the dough the tls handshake is added to the tcp handshake which should take one rtt on tls 1.3 so in total two rtts and for dns so quick we expect one rtt due to the combination of the encryption and the transport handshake into one rtt and if we look at the handshake times here on the right hand side now cf we see that dns over tcp here highlighted in green on the left hand side actually shows the fastest handshake times if you now look at dot and do on the right hand side they are overlapping they show the slowest handshake times"
  },
  {
    "startTime": "00:50:00",
    "text": "and then also kick is somewhat in the middle so dinosaur quick falls short of dns over tcp but it improves on. what we can now do is we can have a look at the handshake to rtt ratio so um i said we uh also measured the protocol specific rtts um on every handshake time measurement so we can divide the handshake time measurement by the rtt measurement to get the handshake to rtt ratio which is shown in the support here and what we can see for dns or tcp to the left hand side here shown in green is that this nicely aligns with the one rtt as expected so this is confirmed if we now look at dinosaur materials and https again overlapping here on the right hand side we see that it follows the distribution of two rtts up under the median and then it converges into a long tail so this is somewhat confirmed and for dns so quick we see a very weird distribution here so around 20 of measurements follow the one rtt distribution and then it converges into a long tail where we see around 40 percent of measurements have an handshake to rtt ratio of more than two rtts so analyzing this we find that this is an interaction with the quick client address validation which is a mandatory feature of the quick standard to prevent traffic amplification attacks so we actually did perform client address validation while we reused the token which was issued by the dnso quick server issued in the cash form query and our subsequent initial of the actual measurement so according to client as well as server state the client address validation is actually fulfilled however what we find is that the handstake handshake is still limited by"
  },
  {
    "startTime": "00:52:00",
    "text": "the traffic amplification so the server stops sending if three times the amount of data received but the client is reached and depending on the size of the x 509 certificate the certificate either fits into its limit or it exceeds it and if it's fit if it fits everything is fine and if not we have one additional rtt for the handshake this is actually not specific to the inner sole quick but the quick implementation pack of the resolves we taxed it so as a conclusion we see a slow but steady adoption with high weak overweight fluctuations as for our response times measurement we find that crick's full potential or dns over export potential is utilized in only 20 of measurements where 40 of measurements show considerably higher handshake times due to the client address validation behavior however despite the still unused optimization potential dns or quick already outperforms and this is why we conclude that performance-wise dns or already the best choice for encrypted dns to date so we have a lot of more detail more analysis in our paper we open sourced our code and data sets so please have a look there thank you for attention and i'm happy to take questions yeah thanks a lot we have laurence on the queue uh can you hear me now if you are muted the room yes yes uh hey lorenzo coletti google um super interesting and i'm part of the android networking team and we're actually uh in the process of rolling out um something that you didn't test actually which is d d0h3 so basically doh over it quick right"
  },
  {
    "startTime": "00:54:00",
    "text": "and one thing that's related to this is that that was that seemed to be like easier to implement because there's more server support and one thing that we noticed is um there's a substantial hit in terms of bandwidth due to lots of http headers that really substantially increases the bandwidth used for the queries and uh it would be interesting if you had that type of measurement as well because metrics on android are you know a few thousand dns queries per day and it just it sort of adds up so i'd be super interested in seeing what metrics you have well my what metrics you could get with um you know with in terms of bandwidth size because you measured rtt and that's super interesting but also like how much bandwidth is used by these queries uh because that's uh uh that's a that's a big deal so that yeah just wanted to say that it's also super interesting work i'll obviously i'll share this with the team that's working on this and uh in particular like the quick bug around uh i i will be looking at the paper to see if there's a um if there's anything we can do about that um we didn't i wasn't aware of this of this issue so we're going to be looking at that thank you thank you for presenting okay thanks for yeah thanks a lot um from my site again like you can always have further discussion on the mail list i don't see anybody in the queue um i think we still have network problems with jerome so i think we just move on in the agenda and that means europe would be next up uh can you click the slides i can check the slides or you can click it there if you want to click it okay"
  },
  {
    "startTime": "00:56:05",
    "text": "so hello everyone um my name is deutschmann and i'd like to present uh some performance measurements of quick implementations over geostationary satellite links which we have obtained using the quick interrupt runner this presentation is based on a pre-print which is available on archive and i have to emphasize that this work was actually done by sebastian andres but he has finished his thesis recently so that's the reason why i'm presenting this here next slide so geostationary satellite networks heavily rely on tcp proxies or so-called performance enhancing proxies these are not applicable anymore uh in the case of encrypted transport so like vpns or quick and so far the performance of quick overview stationary satellite links has shown to be very poor there's a draft which summarizes the most important aspects we have talked about this in previous map rg meetings and there's also a literature overview however so far tests were limited to specifically selected quick implementations so the question remains how do other quick implementations perform over geostationary satellites next slide please um you're probably all aware of the quick indoor of runner for those who are not aware here's a screenshot of it it was developed and is maintained by the quick working group and especially martin siemann and i have to say a big thank you for providing this framework which helped us to integrate our satellite scenarios next slide so the quick intro runner is obviously mainly used for interoperability tests"
  },
  {
    "startTime": "00:58:01",
    "text": "and standards compliance but there are also two performance tests which use a very non-challenging link a symmetrical link with low data rate low rdt and no packet loss in the goodput scenario a 10 megabyte file is downloaded and the performance is quite good for almost all implementations there is another performance test where there is one competing tcp flow and the results show some significant unfairness so what we pay what we basically did is at satellite-related performance tests um shown in the table at the bottom right first the terrestrial scenario is very similar to the goodwood scenario except that we use a link rate of 20 megabits per seconds in the forward link and to make a bit per second in the return link as a baseline for most of our scenarios then we have the set scenario which has a rtt typical for geostationary satellites we have the sub-loss scenario which has an additional random uniform packet loss of one percent and then we have two real satellite operators astra and oilsart this required some modifications to the interoperator which we will see in the next slides and what we also did was the generation of time offset graphs which we will also see in the next slides next slide so very brief the architecture of the original quick indoor runner it consists of docker containers running on a single host machine ns3 is used as a link emulation we have 12 quick client implementations and 13 quick server implementations and for each combination we run 10 iterations and this setup is used for the emulated"
  },
  {
    "startTime": "01:00:02",
    "text": "scenarios terrestrial sut and satellites so on the next slide we see the modified architecture which integrates real satellite links in our case the client host in the server host is located on different machines um so the client was directly connected to the satellite modem whereas the server host is located in the in our university network and the main modification is then that client and servers are interfacing with real interfaces [Music] there's only a single vantage point so um it's not very representative but it's mainly used to check how well does the emulation setups compare to the real satellite links um yeah next slide okay so let's let's have a look at the results on the left side we see the satellite scenario with a emulated forward link of of 20 megabits per seconds um the first thing to note is that in in legend the scaling goes up to 10 megabits per seconds so it never exceeds 10 megabits per seconds although the link capacity is 20 megabits per seconds the timeout for for one experiment is such to two minutes so after two minutes we are bored the test with a which is denoted as a red t in in case of any other failures um we see a red cross the servers server implementations are shown in the columns and there you can see that mainly io quick k-wick and ancient x is performing rather poor whereas the client implementations are shown in the in the rows where k-wick move fast to some extent nico and ng-tcp two are"
  },
  {
    "startTime": "01:02:00",
    "text": "not performing that good um in general we see very mixed results on the right side we have the sub scenario but with an additional packet loss and you can clearly see that the performance decreases um we have a lot of uh we have a lot more timeouts also the timeout was set to six minutes in this scenario and actually there are only a few combinations which perform okay okay in next slide then we have the real satellite links on the left side we have asper which again uses the baseline of 20 megabits per seconds in the forward link results are again mixed some implementations work quite well some don't i won't go into details here maybe on the on on the right side we see oilsat we said which has a link capacity which is twice as much as the other scenarios um from looking at the legend you see that the absolute good put wells are slightly higher but compared to the link capacity the higher the good puts are still not very very satisfying okay next slide then we have a summary of the results uh considering all implementations um we use this metric the link utilization defined as good put divided by the link.rate and here you can clearly see that the satellite scenarios are way beyond the terrestrial link um in term of absolute numbers oilsat has the the highest values but again compared to the link capacity and it's it's not really"
  },
  {
    "startTime": "01:04:01",
    "text": "beneficial the the hiring capacity is not really beneficial okay so next slide um now we try to understand why the implementations before before performed differently um we try to look into which congestion control algorithms are the implementations using um so this table is not guaranteed to be 100 correct we try to look at the code and the command line parameters and the documentation um then we created a correlation plot which has the link utilization of the sub scenario on the x-axis and the link utilization of the satellite scenario on the y-axis you can clearly see that the implementations which are using renewal or nerium do not perform well in the satellite scenario [Music] regarding cubic there are a few most implementations perform well in the satellite scenario with a few exceptions and bvr performs equally well in the sat and the satellite scenario what we did not look at yet uh is the is the impact of flow control which of course is also important especially regarding because when the client has limited flow control then the server cannot do much about it okay next slide to further understand the behavior of the implementations we try to automatically create time offset diagrams this was not possible for every combination due to various reasons but i've picked four exemplary time offset diagrams which we can look at so the first one on the next slide so we have 10 iterations and the"
  },
  {
    "startTime": "01:06:00",
    "text": "iteration with median and finish time is is highlighted so here we have k waker server and ms quick as client you can see that this slow start takes very long roughly 10 seconds and this is a problem for many combinations and implementations we then see kind of a bursty behavior and we see that the sending rate of a few iterations significantly significantly differs from the other runs um yeah next slide um for the next um example we are still we are still in the sub scenario without artificial loss but we see a many retransmissions marked as orange points what's interesting here is that at five seconds the offset numbers offset numbers are not sent continuously so there are gaps in sending the offset numbers and these gaps are then transmitted at six seconds so the two parallel lines at five and six seconds are not retransmissions at six seconds so we do not have really an idea what's going on in such implementations on the next slide we have an example of the sub loss scenario and this is actually an example which works very well we have pico quick both as client and server we have a very quick ramp up we have the retransmissions due to the packet loss and we have speculative retransmissions at the end which is probably very helpful for such high latency links okay um next slide we have uh another example which does not work that well ms quicker server and x quick as client so you can see the that the sending rate goes up and down a lot and the overall time is"
  },
  {
    "startTime": "01:08:00",
    "text": "yeah it takes really long for for for transmission of a 10 megabyte file okay so the summary we have modified the quick intro runner we included emulated satellite links and real satellite operators and we also automatically generate generated time offset diagrams the key message is probably that quick over geostationary satellite links is still performs very poor it's worse with packet loss the performance depends on both client and server of course some open implementations might be proof of concept implementations they are probably not optimized for satellite links um but we also saw a big great variation some implementations do okay others do not that good for us it was very hard to debug each and every implementation or combination in detail simply because there are quite a lot of implementations um but his next steps we try to get do some more detailed analysis uh maybe consider the influence of the flow control at some first further test scenarios and long-term measurements um because we we think that the interpreter is really helpful for us to to to test the performance of multiple of a broad range of quick implementations yeah in case of any discussions um i'd like to welcome you uh to invite you to the itunes mailing list thanks yeah thanks a lot i'm very interesting so you're you're keep trying to debug or look into implementations finding out what the problems are are you also talking to the implementers directly yeah we have received some feedback already which is very very good um but like really understanding the code of every implementation is just yeah it takes a lot of time yeah"
  },
  {
    "startTime": "01:10:00",
    "text": "okay we have the cu hamas hello hamas uh university of iowa so i do understand that a very interesting talk first of all so uh i understand that the performance is very poor for quick and geostation satellites but how do so can this study be kind of like transferred over to lower earth orbit so something like starlink and what would that look like like would quick still perform very poor because now you're connected to a satellite for let's say six minutes at max and then the satellites keep on changing so how would this study transfer over to lower earth orbits yeah satellites thanks for the question we actually have some performance measurements with starlink already there on the website um starlink in general performs quite well it does not have the problem of the high latency um so yeah it seems to work better but we are also interested in making quick work over geostationary satellite links but on this there's still problems with with every five minutes as that satellite changes so you have that additional that hand hand off time so whenever that comes in would that affect uh these uh measurements um we don't know yet okay all right try to look into this but lorenzo coletti i'm just just really out of curiosity if there's if you've taken any sort of suggestions to the quick implementers like about what to do like one one like obvious and probably very stupid thing to do is if the latency is more than 500 milliseconds just be a bit more aggressive because like that doesn't happen on wired links but i don't know if that's like just really stupid or only stupid so that's just really out of curiosity like what you know have you thought about what to do you've got the data right so it would be good you know i'm sure you're sharing the data with them but also like if you said hey like if you could experiment for example like see hey this works this doesn't work just out of curiosity again maybe you've"
  },
  {
    "startTime": "01:12:00",
    "text": "done nothing so the if statement in the code like i am on a satellite link actually pickle creek does just does do this um there are other approaches which have been discussed in the quick working group yesterday it helps it helps pick a quick is one of the better uh performing implementations over satellite links so um there are other approaches like the zero rtt pdp approach which was discussed in the quick working group yesterday and of course you have to do parameter tuning and then the question is how how does a good contestion control are going to look like for such kind of links so these are all very basic questions but it already helps if the implementations simply add a satellite test case to their benchmark scenarios that would really help a lot because a lot of papers and and so on they simply don't consider this kind of link which is a bit sad and i hope to motivate the quick implementers to to have a look at this kind of links very interesting right i actually have a my own high latency link it's not a satellite link but it's uh and uh i had to write my own performance enhancing proxies for tcp and you know it's yeah so thank you very interesting thanks yeah thank you i think uh yeah that's at the end of the queue um and we move over to matthias at this point so yeah hello everybody and this is a talk about quick and um"
  },
  {
    "startTime": "01:14:02",
    "text": "ddos scanning oops just take it out yeah hello so this is a talk about quick and the measurement study that we conducted last year to better understand whether quick is used to conduct um ddos attacks and this joint work with marcin rafael and thomas and presented was presented last year at imc so next slide please um so in a nutshell the main takeaway of this message um is we are asked the question is a quick use for ddos and the answer is yes and we measured this based on data from a network telescope so now i want to present you a little bit more in details next slide please so just as a brief recap quick is based on udp and has to pick a udp property so usually it does not have states and it's based on udp course to prevent ossification attacks on the other end it has also some properties inherited from tcp because it needs to actually implement states it is connection oriented and this leads to some vulnerability next slide so the main one of the main design decision or options for quick was to actually delay to reduce delays on the one hand and i had to make it robust against a typical deny of service attacks and the typical quick handshake looks like this client sends an initial message to initiate the handshake and the server responds with two with the initial message with the server hello and the tls handshake and after this first roundup time a client can already send data to the quick server and the server will reply next slide please um and yeah so the point here is that the server replies to a source even though"
  },
  {
    "startTime": "01:16:00",
    "text": "the source is not verified which is very important so next slide and this behavior makes a quick server vulnerable to two types of attacks the first attack is a reflective amplification attack but this idea here the idea is that attacker sends an initial message to a server but use a spoofed source address and the server will reply um to the spoofed to the spoof source address and instead of sending one a packet it will send two packets so the server actually reflects the initial message and amplifies the initial message the question here is whether this is a likely attack next slide and the answer is it's rather unlikely it may happen but it is unlikely because quick by design allows this client only a server only to reply uh three times more volume compared to what the initial client was sending we heard this already in the previous talk first and second and so it is limited amplification volume is limited by a factor of three and and second there are many many more udp-based uh protocols available that allow for much higher amplification factors such as dns or ndp so this is a possible attack but by design is limited and rather unlikely next slide the second type of attack that the attacker might conduct is a resource extortion attack and the idea here is that the attacker sends an initial message again using sports source ip addresses to allocate states at the server side and the server will reply as usual within the first round to type this initial message and that is handshake message um and this is sent back to the source to the support uh source ip address and um ideally this source address does not i mean is offline so it will not reply at all with so reset or something like this which means that the server allocates states for a decent amount of"
  },
  {
    "startTime": "01:18:01",
    "text": "time and if the attacker has a distributed botnet for example and floods the server the the local queue at the server will fill up and the server will not be able to reply anymore or even to a benign request so and in addition to this to allocating state it also introduced computational resources um because of the cryptographic glsen shake next slide and um these replies from the server to the support source ip address can be observed and a typical measurement infrastructure to observe spoofed packets are network telescopes and these tele telescopes actually will receive packets that are spoofed with ip doses from the network telescope ip prefix and in our measurement study we leveraged this telescope such a telescope next slide what we actually did is that we analyzed data from the ucsd slash kaiba telescope which is a slash noise prefix or as a large address space which allows us to capture more than two percent of the actual ipv4 address space so that's only focusing on ipv4 as we heard in the previous talk um measuring um malicious traffic in ipv6 space is a different story and much more complicated um and we did this for a whole month into an april uh 2021 and this telescope receives a lot of malicious traffic it's not only quick scans or a quick back scatter but also tcp scans tcp back together and so on so next slide what we did is we need to distinguish um this quick traffic and how did we do this we um did first and uh port-based"
  },
  {
    "startTime": "01:20:00",
    "text": "classification so we filtered for all udp for 443 traffic and um this is a very common method to distinguish quick traffic from other traffic but we also applied some kind of the packet inspection to exclude for its positives and for this um we use the bioshark detectors and we also did some manual verification so we were very sure that the traffic that we identified were actually quick driving and uh based on this this denver telescope and our um let's say to identify the quick traffic we detected 92 million quick packets and then we distinguish this or split this traffic into types um request and response so requests are packets that are sent to the udp 443 and responses are so the destination port udp 443 and responses are packets that included the source port udp 443 and requests are packets that are more or less scans now i mean as we heard researchers do scans for example um to find quick servers and the responses are backscatter traffic so traffic that was sent from a quick server back to the spoofed ip address so response okay next slide um when you then but what you see here is a high level view on the traffic that we captured and on the x-axis you see the time and on the y-axis the number of packets and um we saw two heavy scanners um which were from tom we heard about it and the other one was and then um some other traffic remains so we excluded uh the scan traffic because that is not of interest at all um yeah and in 2022 uh we also saw some scans from census so um the commercial scanners were a little bit late compared to the actual research scandal so that was sanitizing excluding all of the"
  },
  {
    "startTime": "01:22:01",
    "text": "scanners because this traffic is benign but we are interested in the malicious part next slide please um um what you see here is now the traffic that we captured after sanitizing um distinguish between response and requests and um for the request packets you also see a little zoom a little inlet here and that's the blue curve and um this shows more or less additional pattern um which most likely are the scanners but not the heavy research scanners maybe malicious cannot looking for um quick servers but with a much lower rate compared to the heaviest gamers and more importantly it is a orange curve which shows the response packets and here you see a very very erratic pattern so you have a quick increase in terms of packets and then a brief a quick decrease which is a typical pattern for denial of service attacks next slide so then we um checked who actually is attacked who receives or from which autonomous systems do we receive the responses which means which where where the server located that receives the spoofed request and um that you see in the second column responses and each line um shows you the type of autonomous systems where the responses comes from and the vast majority of responses that we receive in our network telescope are actually located in quantum provider networks so next slide um now the question is uh whether these um responses um to the sport uh source address um from the source uh address um are actually denial of service attacks or"
  },
  {
    "startTime": "01:24:02",
    "text": "something else and this is a general challenge um identify um i mean you see a lot of traffic is it a attack or not and what we did here is that we applied comments research from a prior verb so first we group our packets in sessions and each session is split by an idle timeout of five minutes and then we applied this common threshold which means that we identify a session as an attack if the session lasts more than longer than 60 seconds and if we see more than 25 packets and a maximum packet per second rate of 0.5 so and next slide applying this we found actually more than 2 900 attacks in our measurement set of these one months and surprise surprise next slide the majority of victims is actually a google servers and the other weight is relates to facebook so most of the attacks go to one of these famous content delivery or service for web services um next slide and now you can ask i mean this is identifying the attack is somehow based on empirical data whether our thresholds are valid or not because they are from prior work some years ago and what we did is that we um changed these thresholds based on of different weights so um any that you see here on the x-axis and on the y-axis as the blue proof is the number of attacks that we identify based on the weight if you have a weight that is smaller than one we have a more relaxed sweatshirts and if"
  },
  {
    "startTime": "01:26:01",
    "text": "you have a weight that is larger than one we have a more stricter thresher and what you see even if we put a much stricter 10 times stricter in our service version we find still a significant i mean a decent number of attacks and the second is shown by the orange curve that the um that's a share related to a content delivery networks um is i mean that's a at expressions more or less independent offices next slide then we looked a little bit more in detail into the victims and uh to better understand whether a quick uh denial of service attacks relates or correlates with other attacks and what actually can happen is that um quick only that the attacker only quick uh attacks a quick service that's uh that that first another service is attacked and then the quick service or that an attack of different servers occur in parallel so um this is you see here illustrated at current attacks um that triangle shows the start of the attack and the green attack as a green triangle shows the stop of that attack and the first column shows you a concurrent attack which means that the tcp typically tcps in flutter attack for example occurs in parallel to a quick attack and after that um the quick attack continues which means that you have a sequential attack uh next slide and um now we do a little statistic of all uh all events and found that half of the attacks are actually concurrent attacks which means that we have a quick attack and a parallel gcp icmp zoom flat of icp tcp zoom flat and roughly 40 of the attacks that we found are"
  },
  {
    "startTime": "01:28:00",
    "text": "sequential attacks which means after the tcp is in flight the quick occurs and only 9 of the attacks that we found were quick only attacks so which basically means an attacker leverage all of the protocols that are available even new protocols such as quick to do research exhaustion ethics it's a victim next slide now the question is can we protect against this and they submit one mechanism which is a quick retry mechanism that allows these type of resources uh exhaustion attacks before a client is authenticated and the idea here is um to follow an approach similar from tcp cookies the before the server the quick server establish the state it sends a cookie um a secret to the client and the client needs to reply with this and if this is correct after that as a as the server starts as a as a typical clicking check and if the answer is not correct the server just ignores it and no state is established at all and now i'll show you a brief um uh emulation or i mean a testbed evaluation uh whether this quick retime is a mechanism helps to prevent resource exhaustion attacks and what you see here is i mean what we have to do we use this engine x and initiated actually these quick flats um to uh towards the server and um but based on different attack rates and different configurations and what you see here is in this configuration if you have a packet per second rate of 100 packets per second and then you already will be able to to make the quick service unavailable then next slide you can argue that you can use more resources more cpu resources yes you can do but still i mean"
  },
  {
    "startTime": "01:30:02",
    "text": "if you increase the packet rate you can will be stay still able to um turn the service online offline and the quick server will not work so next slide but if you enable as a quick retry message you will be able actually um to prevent the server from being exhausted and the servers will be still still available um but on the uh which the one that's uh what no tears that's a quick res tries to measure and adds an additional round trip side that time that is a little bit the disadvantage but it prevents this type of ethics so next slide um yeah very important i mean this evaluation that they presented here is not about nginx that is a design issue by a quick and what we found in our data is that none of the server servers you actually currently use the retire options most likely because of uh because the retro message adds additional enterprise time and you actually want to reduce on trip titan delays next slide um so we this the data that i showed you was from 2021 we uh also analyzed more recent data from this year and we found that these quick initial floods actually doubled so i took will increase and we also analyzed um google and of facebook's uh often at servers or servers that are not located in the google or facebook ads and if you consider this you even find more text it's currently not only google and cloud facebook that are attacked in our data set we find also messages from cloudflare but luckily i mean some of the services servers that are"
  },
  {
    "startTime": "01:32:01",
    "text": "attacked use retry packets even though that's a very very small minority next slide so to conclude um uh quick is vulnerable against initial floods um that uh try to um do a resource actuation attack on the on servers and we actually find this type of attacks deployed in the real internet with an increasing trend you can prevent this and mitigate this by using retries currently many many servers do not use retry but if you want to prevent this you have you need to enable it or find another way okay next slide final slide if you're interested in more details there's a full paper linked here including a publicly available pdf and all of the software and artifacts are also available thanks thank you and we start with the queue alex thank you very interesting information um do i understand correctly that you said your telescope would catch about two percent of the potential backscatter of it of an attack yes it was because of nine yeah so that means that the attacks would probably be like roughly 50 times the size that you observe and compared to other types of u.s attacks those appear really really tiny yeah i mean we are dealing in the dns industry with tens of millions of packets per second um and my question is you said you're gonna you saw more attacks in terms of number of lots that were coming in in 2022 do you also see that the number of packets that are used in those decks the packets per second are actually increasing because that would that would indicate to me that this is coming out of kindergarten and finally getting a real tool because quite frankly i don't think that 100 packets per second would actually be noticeable on google's side at all yeah i mean um i don't know the"
  },
  {
    "startTime": "01:34:02",
    "text": "it's it's i think that that was my sorry sorry that that made me think is that maybe something like a gamma ray beat flip in a client that goes oh certainly on the server so is it actually an intended attack and what you said about the multi-protocol back scanner that tells me a little bit more it's probably actual a try by someone um but not really a professional attack yeah i mean we are not arguing that the services that receives this data i actually got to get offline or something like that what we are saying saying is that we see i mean patents that hints towards this attack whether they are successful or not is a different question that is not what we are analyzing here um but it's a good point um i don't know the pbs uh for the uh updated data is something that we should yeah uh i don't just don't know just recently that's a good that's a good point but i mean um this is i mean this talk is meant to be also be cautious i mean people are trying to explore this type of attack and even though it's currently maybe a low data rate it might increase in the future and you should be prepared for this right but you're not seeing that the packet numbers or packet rates increased as i said i don't know i mean i don't know it now i mean we did update the data recently so um yeah i have to check this sorry thank you excellent research thank you all right um really good research so one of the things that we saw in our ipv6 scanners was that some scanners controlled let's say 48 and they would send two packets from each of the ip addresses and then increment the address and then keep on sending two packets every time so would your uh calculation of attackers miss that so if someone's using some sort of a distributed architecture to send scans maybe let's say if it doesn't send 5000 packets it"
  },
  {
    "startTime": "01:36:01",
    "text": "just sends 10 packets from each ip address and then keeps on changing and would your uh what your spending yeah if you have a distributed uh type yeah yeah i mean it yeah can we catch up no that's not an issue okay so uh i forgot forgot like exactly how you uh said that if these are the conditions are met then we considered it as a scanner so i mean we um if you can go back um yeah anyhow i mean we group it by uh by source ip address um yeah yeah the next one i think that's fine yeah yes um by uh by source ip address um and um we i mean we did not do it here but you can do you can also consider um requests or replies from uh for multiple sources in parallel which and then you can track and emulate some of the state increase for example yeah yeah that makes sense thank you okay thanks a lot um so jerome uh quickly drove to university and now has better connectivity so we will give it another try um yeah so can everyone hear me now we can hear you right now oh great thank you and first of all uh my apologies for any inconvenience sorry i just moved to another place for a better connection so i hope it sounds better now um right so this is jerome from cwru and uh today i'm going to present our results oh i'm hearing like echoes you can just"
  },
  {
    "startTime": "01:38:01",
    "text": "meet the room locally on your site and we take all the questions at the end then okay okay so um yeah so today i'm going to present our results in measuring the support for dns over tcp in the internet um right so let me see oh yeah so here's uh the topics we're going to cover today so we're going to look at the dns over tcp support on two sides of the dns infrastructure the recursive resolver side and authoritative dns server side so we focus on the support in those two sides because they are where we can directly associate the tcp support behaviors to the actors that are that are responsible for them and at the end of the talk i'll briefly discuss the race condition between the recursive resolvers and authoritative dns servers in dns over tcp so let's move on to the tcp's fallback support by recursive resolvers so let's just use resolver which is the shorthand for recursive resolver or recursor so the general approach is that we want to measure as many resolvers as possible and we want to force those resolvers to talk to our ads through tcp so what we did is our ads send truncated udp responses to the resolvers and those udp responses have no resource records at all so therefore if the resolvers want to complete the resolutions successfully so they are expected to fall back to tcp protocol we have four data sets on those resolvers the first one and open ipv for our scan with unique queries to our own domains the second one uh we use the bouncing message we use"
  },
  {
    "startTime": "01:40:01",
    "text": "the bouncing message in smtp protocol so our result also our scanners and emails to non-existing recipients at the domains from the majestic top 1 million list from our own domain and those mail servers they won't be able to successfully deliver those emails and so they are expected to send bounce messages for the delivery failures back to our mail servers so they have to query the uh all they have to query the mx records in our in our domain and our ads are gonna force the tcp fallback in those mx resolutions the third data set we use the famous write ls platform so we let the rap alice probes to send unique chorus to our own domains through their resolvers and the last data set we have the 18s logs from a major cdn those logs are highly aggregated so we only use those logs to assess the real world activity of the resolvers from different categories uh right so here's a challenge so when we are talking about tcp fallback the tcp fallback is a two-step process and it is hard to associate the dns over udp queries and dns over tcp queries and because of the collaborative resolution it is even harder so our destination for tcp fallback capable is that either a resolver itself is capable of fallback to tcp or this resolver has appear that falls back to tcp for it so so here i i expect an animation here but sorry it's pdf version so there's no animation but uh just to illustrate what is a canonical scenario in tcp fallback you know tcp fallback so if we've seen one udp query from"
  },
  {
    "startTime": "01:42:01",
    "text": "resolver and one tcp query and another tcp query from this resolver for unique query then we can say that this resolver is tcp fallback capable and this is a canonical scenario of the tcp fallback and the figure here shows a more complicated non-canonical tcp fallback scenario so we can see that you can see that we have two udp queries from resolver a and resolver b and a tcp query from resolver c so in this case we do not have enough information to say that the tcp query from resolver c is a consequence of a or b's udp udp queries then resolver a and resolver b are indeterminate in terms of the tcp fallback capabilities in this transaction and uh in fact in fact the con the non-canonical scenarios they are very common actually in our datasets on the resolvers only 46.8 percent of all the resolutions are canonical and even for the canonical scenarios 18.9 percent of those scenarios have two queries coming from different ip addresses so udp query from one ip address and tcp query coming from another ip address and for the non-canonical scenarios they sometimes it can be very complicated to match the udp korean tcp queries so here i just listed two uh real world examples here so the first one you you can see that so our scanners and just one single unique udp query to um to the resolver and in our 18s it ends up with five udp queries and four tcp core is"
  },
  {
    "startTime": "01:44:02",
    "text": "and another in another example the same it's another unique uh udp query quoted by our scanner and you can see that there are like around four udp queries and three tcp core is coming from three with hours so you can see that like the udp the udp and tcp tcp queries there fallback relationships are not quite very obvious in those two examples so we have developed an algorithm that tries to group the queries into clusters by their potential fallback relationships and uh we assume that like the maximum gap between a udb query and its real tcp fallback tcp fallback tcp queries is two seconds so uh you might be wondering so what what is a cluster so why do you split all those three different queries into four clusters so cluster is a group it's a group of queries and a cluster ends with uh dns over tcp query and uh in addition to that two consecutive tcp queries in a cluster are preceded by at least one udp query so here we have 13 udp and tcp dns over udp and dns over tcp core is here and we split them into four clusters in the first in the first cluster you can see that um we have udp query number one and three and tcp queries two and four um the tcp query number two is preceded by udp query number one and tcp query number four is preceded by udp query number three and also we can we can say that like the ud the tcp query number four is also preceded by udp equal number one but if we if we assume that the tcp query number four is the fallback for utp query number one then we're gonna left the"
  },
  {
    "startTime": "01:46:00",
    "text": "tcp query number two unmatched so which is unlikely so in this case we say that so we just assume that the udp code number one is fall back by tcp query number two and udp core number three is fought back by tcp number three number four so in this case both number one and number three are tcp fallback capable and in cluster number two this tcp core is not paired by any udp query so we just leave it at this and in cluster number three it's obvious that like the tcp number seven is the fallback of bdb query number six so quite obvious one and in cluster number four it's a little bit complicated because um you know there are several kind of potential pairs and like you know we we cannot find a way to to perfectly um associate the udp queries and tcp queries because you know there are three udp queries but only two tcp for example so in this case we just say that all those three udp quarters are indeterminate we just don't know which udp query is uh is tcp fallback capable and which one is not but finally in udp query number 13 it is not followed by any tcp query so we can confidently say that this udp quarter this guy is tcp fallback incapable um right so just like what we we've seen in the previous two slides so some dns transactions they just don't allow ambiguous influence of the tcp fallback capability like in cluster number four uh so in this case what we do is that we have two estimations we have the optimistic estimation and we have the pessimistic estimation and the only difference between those two estimations is how we process the indeterminate"
  },
  {
    "startTime": "01:48:02",
    "text": "queries so in optimistic estimation we just consider the indeterminate as tcp fallback capable so in so in this so you see in this example in cluster number four we just assume that like udp code number eight nine eleven are tcp fallback capable and under pessimistic estimation we just consider them as tcp fallback incapable and throughout all of our data sets we have studied around 130 16 000 resolvers and around 95 to 97 percent of them are tcp fallback capable so we use a range here to represent the number of tcp fallback capable and capable resolvers according to the optimistic pessimistic estimations and the tcp fallback capable resolvers they contribute to around 96 to 99 of the cdn traffic from other resolvers we've studied so the takeaway here from the resolvers measurement is that there is a non-negligible number of tcp fallback incapable resolvers and they are about like just equally active as the capable resolvers uh so next we're going to move on to the tcp support by the authoritative dns servers so in those measurements we're going to act as so we have the control over the transport so the general approach is just we try to send tcp queries to the adns serving certain domains from a testing machine on campus so only one vantage point and uh we still have three data sets here the first data set includes the domains from the queries handled by the resolution service operated by the major cdn and uh we test all the 18s lists for each domain and secondary set we use the majestic top 1000 rule domain"
  },
  {
    "startTime": "01:50:00",
    "text": "websites and we call those popular websites and still we test all the 18 as listed for each domain last data set uh we have a list of cdn accelerated domains and we just pick one domain per cdn um and we test all the 18 as list every domain uh here's the results for the adns server side so first of all the domains from the from the query is handled by the resolution service operated by the major cdn uh more than five percent of the domains they fail to resolve tcp queries through some 89 servers and still and like even for the uh popular websites still like around a little bit more than three percent of the domains they fail to resolve the tcp course through some 18s and even for the cdns 11 out of 47 cdns we've studied they have deployed 89 servers that do not support dns or tcp and uh so for the rest of the adns that do support dns over tcp we're gonna we're gonna move on and see the uh the race condition that are related to those to support dns over tcp so uh so first of all rfc recommends reusing his tablet tcp connections and actually resolvers they do reuse connections in another email scan we we've successfully induced around 13.5 percent of the resolvers that are that serves the uh mail servers to reuse the tcp connections so resolvers do reuse connections and here's the race um if the server tries to close the connection after sending a response and you know the server is trying to close out this connection after responding to this dns query but the client you know the theme or the reset segment is still on the flight and client doesn't know that like this this connection is being closed out and"
  },
  {
    "startTime": "01:52:01",
    "text": "the client and the client is trying to reuse this connection for further queries we can see that this code is going to be left unresponsive or we're going to be left unresponded so that's a race condition and actually around 33 of the popular websites and for cdn providers they deploy 89 servers that just like close the connections immediately after responding to the first uh dns over tcp query so they are vulnerable to this uh race condition uh so uh next we are just uh trying to propose some uh optional update to the dns over tcp and we would like to put them here to discuss with the community so um there has already been a fantastic ddns edns tcp keeper live extension and this extension allows the client and server to dynamically negotiate the timeout of the current tcp connection so maybe you can try to work more on this uh ens extension so here are our proposed optional update here first a resolver must now reuse the tcp connection unless and explicitly dns tcp cable live negotiation has been complete has been completed the second the second resolver has to be loyal to the negotiated keep alive duration the third one maybe we can just let the adns servers to retain the tcp connections for uh for another two maximum segment lifetime beyond the negotiated keep alive duration so that all the um outstanding all the tc dns over tcp courses in the flight can be correctly acknowledged and processed and uh"
  },
  {
    "startTime": "01:54:00",
    "text": "there might be another potential optimization we haven't studied on that yet but it's just uh maybe we need more discussion on that so a resolver so maybe we were thinking like so right now edna's tcp keep alive doesn't allow to be negotiated in udp but maybe we can try to let the resolvers and adms servers to to like to to negotiate the uh tcp cheaper live in udp so that um the resolver and ads are going to know that okay so the the remote endpoint do support or doesn't support that support doesn't support the tcp reusing and maybe the client can try to uh shorten the um can try to shorten the previously negotiated keep alive duration here and also because uh dns over tls explicitly borrows the connection management policy from dns over tcp so we hope that like these updates can also so we hope that like dns over over tls can also benefit from these uh updates here um right so here's the conclusion for today's talk uh first of all on the recursive resolver side a small but non-negligible number of resolvers they do not support the non-server do not support tcp fallback and actually they are very active so they are under the risk of not being able to um to to to to receive the answer for for uh very large to receive very large dns messages the second of all on the on the uh dns server side still a non-negligible number of top websites and cn providers they use authoritative dns servers that do not support e-mails or tcp and for the adns servers that do support dns over tcp many of them are vulnerable to the race condition that's it"
  },
  {
    "startTime": "01:56:01",
    "text": "thank you very much thank you very interesting and glad that we that we finally managed to get here in here um so did you did you try to reach out to the cdn providers and figure out if they're planning to support this or why they are not supporting this oh yeah yeah yeah we just we we are we we find some like some bugs in in some in some uh implementations and we we reach out to them okay yeah definitely interesting to see these deployment numbers so now we have somebody in the queue yeah hello so um the question i have is the numbers you presented you said some servers were not reachable did that mean that the resolution still could complete without tcp or was it just kind of one of these or was all reservers not responding with tcp i mean because some servers not responding from a user perspective is not the thing yeah yeah right so some adms means like so right so if so maybe like they have uh a 280 ns one supports tcp another one doesn't support tcp so that means some were like both of them none of them supports tcp so that's another like sum 18s did you actually also calculate that number because that would be much more interesting for for real users uh yes uh i think yeah we have the numbers we have the numbers uh are you talking about this a little bit more than five percent domains or you are talking about the numbers on 18s but now the number of domains from my understanding is these numbers just mean you have x x domains that or x server that don't respond but you don't have uh the amount of numbers the number that actually would not resolve that's a different outcome"
  },
  {
    "startTime": "01:58:02",
    "text": "so now resolve um without tcp i mean if you have a domain say example.com that has two name servers and one response with tcp and one doesn't then that would be still okay even if it would be in your list of domains that where some are not responding right maybe you have to come closer to the mic um yeah can you say that again please yeah i i'm trying to follow so maybe just we take it off offline yeah okay yeah we have peter in the queue hi yeah i have a question on the interpretation of um large samples of of resolvers or in fact it's two questions so i think um your slide said you checked about a hundred thousand or so resolvers and when we do um reserve studies we always wonder um how um like how much does that mean because if i find some some random ip address that that responds to dns queries um it may be resolver that is not actually used or it may be and um yeah so the first question i have is how to assess the significance or relevance of any given resolver and that might lead to a bias in this 95 to 97 percent result if you would filter out the ones that aren't really real world resolvers and just leftover orphaned deployments or whatever and the second question i have is similar [Music] people sometimes argue that especially in corporate environments tcp support may be blocked by middle boxes or whatever and that that would be a problem for dean essec deployments and other things with"
  },
  {
    "startTime": "02:00:01",
    "text": "large payloads in dns and um maybe i didn't get it in your talk but if you could point out one or two insights on that aspect and whether that is true or maybe not true about the tcp support lacking in copper environments that would be nice thanks and very nice research in general thank you so yeah so first of all let me try to uh read try to uh repeat your question first so so the your first question is uh so you're saying like maybe some of the tcp fallback capable resolvers are not used or some tcp feedback incapable resolvers are not used in real world applications so how do we know that right um right so we we have the uh we we have the uh we have the 18s log from a major cdn so uh so we so we know that like so whether this adns is being used or not being used and to what extent it is being used and for this number here you can you can see that like so we say that like around 95 to 97 percent of the resolvers are tcp by capable and they contribute to around 96 to 99 percent of the cdn traffic from the dollar study so that's uh that's our aggregated results here so basically um that leaves like around uh so so we we cannot say that like so basically uh which resolver is being used and which resolver is not being used but we can say that like so overall the tcp fallback capable resolvers are being used as this and you know the tcp overall the tcp fallback incapable results are being used are still being used because they still contribute to around like one percent of the"
  },
  {
    "startTime": "02:02:01",
    "text": "cdn traffic yeah i think we have to take the second part of the question to the list we are kind of at time um it sounds like interesting question about um blocking of dns over tcp which might need further study and maybe just take it yeah yeah thanks again for being here and taking the effort to drive in the middle of the night to university that's great to have you um thank you everybody else also who joined in the middle of the night this wasn't a convenient time for everybody but i think we had a great session i'm so happy that we ended up in a two-hour slot even so we only requested one hour we got we had because we got some nice talks here and i'm already looking forward to the next session dave yeah thanks maria and thanks to the contributors too you guys have a good week okay thank you everybody and also thanking alex for taking notes that's it bye bye the meeting notes itself are kind of already so this is not like the final official version but it's all here so if you want to check on that you can even edit it effectively"
  }
]
