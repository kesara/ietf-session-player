[
  {
    "startTime": "00:02:10",
    "text": "[Music] things [Music] like say almost wasn\u0027t you came in like a minute ago yeah yeah no better terms are coming to the thanks Alexa we also have a presentation well we have meet you as well so they come you push one your knittin you push second and reject them and then you just appear on the world\u0027s news yes it should appear on the thing and it depends whether they come yeah sometimes it doesn\u0027t you you try to twice and you ask them to chew every joint button oh yes afternoon is it good afternoon this is CFR D session at AI at ITF 99 so I hope in the right room we have note-taker we have person rich relaying from jabber to the mic we have blue sheets coming going around "
  },
  {
    "startTime": "00:05:13",
    "text": "so let\u0027s get going we have a quite a busy agenda this time there were lots and lots of requests in the very last moment so that sounds all very exciting sorry just very quickly we\u0027ll do a gender bashing if Ronnie changes for people asking to move stuff around going once going twice ok then we\u0027ll just get on with things right I assume you all seen the note well and which applies to ATF and equally applies to RTF okay we\u0027ve done that you see two chairs in front of the room so Kenny sitting next to me and me Alec C so I hope you all know who we are just very quickly about status of various documents this is since so because we haven\u0027t officially met in Chicago we got to the dwarves is published which is good we have a couple of documents in IRS G review they either being actually reviewed or there are some comments so they they need revisions to documents we have three or four very active documents in the working group I think are going to and you see em as IV nearly ready for our last call we already had some other reviews for the GCMs IV document wreaking document editors have been quite quite active also the war there was an official meeting in Chicago so there was some revisions and presentations going to be later on today yes I think that\u0027s that\u0027s the main thing [Applause] "
  },
  {
    "startTime": "00:08:16",
    "text": "the other thing the chairs would like to say is we started crypto review panel last September and we worked it slow getting up to speed and getting the panel reviewing documents but we had lots of very good reviews recently GCMs I will I got three or four reviews you know I think even Kenny and I was surprised how much interest was in this so III think the things are working well so we\u0027re looking forward to using crypto panel more in the future and at this point let\u0027s start with presentations okay so first up we have stanislav is going to update us on rekeying stands laugh I think we allocated ten minutes in the agenda for your talk and please try to stay this is for all speakers please try to stay inside the pink box that\u0027s where the video will find you and I\u0027ll be doing just ask for next slide each time okay thank you my talk is dedicated to the current status of work in document for symmetric is the termination of draft is zero 5 - ORS are on the slide and let me say some few words about the motivation so process and we need some limits to be mighty to be taken into account first of all these are just general internal properties estimations for few staffers if they have some potential for abilities and the most important of all side channel the newest methods of work theatres so for example there was paper recently about tempest attacks on AAS we is attack taken only 50 seconds to get an a ski from opens ourself implementation so taking into account these problems we have to limit the key usage the key lifetime and when our limits are reached we have to change keys so of course we can just do a new handshake key establishment but it can be quite "
  },
  {
    "startTime": "00:11:17",
    "text": "expensive so in a lot of stations we prefer to update the keys so to do 13 a good example here is Arrakeen procedure interest 1.3 as a key update procedure and I think that this is a good example how when and and in which stations 13 must be done place next slide so the main objective for the document is to prepare a document with a mean of choices for the developers of protocols that just contain a lot of skill and efficient procedures that solve 13 tasks in most relevant cases of course this document shouldn\u0027t be redundant and the document should contain general recommendations and chase principles for as a 13 mechanisms one of another one or another and there is some specific parameters we consider both external and internal key methods parallel serial hash based or cycle based reasonable and without metal keys for the most the most for the best majority of them the security increase is about critical so the key lifetime can be increased approximately quadratically in most relevant models late next slide so the work started after soul meeting there was a proposal from the separate G chairs to create such document there was a talk on Ricky in film so meeting we had discussion and the end work started as a zero zero draft and zero 1 draft appeared before the Chicago meeting and in Chicago we had one our side meeting honor King war we had some important decisions about the document please next slide please next I think so first of all about the durations of the scope and aims of the King we decided to focus on his following four features four reasons to user King first of all again it\u0027s additional side channel resistance secondly in some cases in some types of her King helps us to gain BFS security regarding segments of encryption process then if you use ciphers with short block sizes for example for lightweight cryptography beam is used just to extend the key lifetime because just the community of properties limit us to strict limitations and then most complex issue "
  },
  {
    "startTime": "00:14:17",
    "text": "here is an additional security against possible attacks on the youth theaters we edit a remark to the draft and also the service on the slide that this must not be used as a method to prolong life or broken ciphers it\u0027s just a security security margin a safety margin against possible future attacks so it\u0027s not it mustn\u0027t be used to prevent life or something already vulnerable some words about what quantum issues are added no post quantum issues can be solved better in because if we talk about doors algorithms we can just talk about attack on two or four blocks are sufficient for us to mount attack so were keen couldn\u0027t help and some wars about reasons that remain for even for ciphers these large block sizes worried about a channel resistance FS and again the safety margin race next slide about the commendation guidelines the most complex question here was about limit between as external and internal working and now we base this auto-draft on the following concept that external regime is chosen on a protocol level while and it don\u0027t leak in internal routine mechanism is chosen linked to block size and what say for mode operation so the most important thing here is the mode operation and when we try to understand which frequency to use Ouiser king it\u0027s dependent on a block size then we edit a text about advantages and disadvantages from soul slides provided simple cases working examples and some toy examples for protocols we rearranged the document so that as a recommendation part in the maturation part is before as a mechanism ourselves and this there was decision not to consider any related questions for skip soidiers because all models are quite different this next item and all the mechanisms themselves there was a number of minor iterations about constants in the internal wiki in for CTR mode the most important direction was about the CCM mode there was proposal to consider also SSA mode Weiser King and I\u0027ll return to this question a bit later please next slide so after the Chicago median we had four more versions the current version is 0 5 "
  },
  {
    "startTime": "00:17:18",
    "text": "we had two major revisions after a list of durations by less costly by shaker on admissible ASCII and the current version is 0 5 of course will be happy to have any reviews alterations for this version is next slide we hope that structure the principles and made recommendations in this version are quite close to being ready but we still have a lot of important question to be solved first of all as I mentioned before internal reinforce the same now we have proposal for mechanisms for internal routine for cesium it\u0027s added to the draft but there\u0027s a problem with it that now this is the only mechanism that doesn\u0027t have security proof for all other mechanisms we either have food security proofs or they can be obtained by some flight notification source and it improves because principles are the same that in textbooks but position because of authenticity problems can\u0027t get security proofs using since we have now so it must be done from from a scratch if so we need help here and if someone would like to help it will degrade and for now we think that if we unable to obtain all security proofs for the same with looking we need to exclude the mechanism because we don\u0027t want something without guilty proof explicit or implicit to be in the document then we are thinking about ATM a section about eateries the key hierarchy in terms of needs to speak 800 108 of course we will need to attest to actors and also I would ask for its directions and concerns for the current version first of all opinions about whether all the concerns from the Chicago meeting have been properly addressed and any new comments first of all about the recommendation as and use cases for protocols for instance for from the IT field I think it will be very important to have as much refined from relations about recommendations from the developers of protocols as we can next quite so thank you very much questions stations thank you very much Tanis lab so we have 10 minutes allocated for questions and discussion of this draft so please approach the mic and state your name and usual way and then we\u0027ll get into your questions and comments hi you have your few slides back had "
  },
  {
    "startTime": "00:20:22",
    "text": "this distinction between 64 bit ciphers and broken ciphers so concretely is Triple DES broken or just 64 bit I don\u0027t think in the scope of the draft but in my opinion document is about ciphers that don\u0027t have any even any theoretical problems so I don\u0027t think that this must be in the scope of the new protocols and I don\u0027t think that anything must be strengthened with akin if we have some theoretical properties so in my opinion see this must be out of the scope but it just only opinion we don\u0027t have to add some such thoughts in the document itself so are there any ciphers in use that are not broken and 32-bit or 64-bit if and my opinion is that if real security for now for the node X is the same as a purely security then it\u0027s everything is okay with the cipher so we have some for example 32-bit cipher of course we have some a priori limitations for its usage of course but if we don\u0027t have any attacks then that can lower as a theoretical security then we can understand that the cipher is still unwell doable and it can be considered so in my opinion we have to distinguish the ciphers with a priori low security or broken cycles bingo franca Akamai I\u0027ve actually been jokingly subtitling this draft as make three days great again it\u0027s so so so I mean actually don\u0027t actually going out of your way to apply this mechanism so that you can use three days seems like a questionable life decision but nonetheless I think it is a almost a canonical example of what this mechanism can do it can take it can take it can take a cipher where the best attacks are generic attacks but the generic bounds are not great and make them more comfortable yes it agree so if we have only generate attacks then it\u0027s a good way to prolong the life of a key we use this mechanism thank you and thank you for reading it once again Daniel maybe and we could ask for a quick show of hands from the from the audience no putting you guys on the spot who has read the draft oh that\u0027s fantastic great are there any yeah she\u0027s got you get your hands down first are there any volunteers to do a thorough review of "
  },
  {
    "startTime": "00:23:23",
    "text": "the draft on behalf of CFR gee now that it\u0027s reaching a fairly mature state any can you put your hand high and then we can take a photograph ah that would be really helpful thank you so much anybody else of course we can make use of the CFR G review panel but we don\u0027t want to make that the exclusive mechanism by which CFR G traps get reviewed we want to continue to involve the wider see if our G membership in that kind of process okay we also always on the lookout for new you know panel new future panel members so you know that\u0027s a great way of demonstrating your capabilities okay if there\u0027s nothing else on on this one then we\u0027ll move on thank you very much Dan stuff thank you okay so I presented this draft at sag at the last ITF we were asked to bring it here it\u0027s substantially updated so what I\u0027m gonna do is I\u0027m gonna go over for people who didn\u0027t see my sag presentation I\u0027m gonna go over what exactly a verifiable function random function is and why we should standardize them I\u0027m going to talk about the applications and since that meeting I have more information about what the applications of this primitive is people are using it in ways that I didn\u0027t know before and then talk about what\u0027s new in this draft okay so to get an idea of what a verifiable random function is we all know what a hash function is it doesn\u0027t have a key if you take an input it gives you an output and to verify that the hash was done correctly you just recompute the hash yourself okay so there\u0027s a verification mechanism for this hash function next slide a PRF has a symmetric key the holder of the key can compute the hash but if you don\u0027t hold the key you cannot verify that the hash was computed correctly right so that means that there\u0027s no one-to-one relationship between the input and the output with a pseudo-random function with a vrf it is a keyed hash function next slide it\u0027s keyed hash function but this time we have an asymmetric key so the way to think about this is sort of like the public key version of of a keyed hash function and what happens is in order to compute the hash you use the secret key so that means only the hash sure can compute the hash but anyone can verify that the hash was computed correctly so that gives you we\u0027re gonna see that that gives you a one-to-one relationship between inputs and outputs if you know the public key that\u0027s what we want from Aviara cool okay great okay so how does this work so I want to go over the API of how a vrf works so what happens is the verifier holds the public key the hash sure holds the secret key he sends his input over to the hasher the hash sure computes the value called the proof so the way that\u0027s computed is by taking the secret key in the input and computing some sum value called the proof okay so only the hash sure can do "
  },
  {
    "startTime": "00:26:23",
    "text": "this now the verifier needs to verify that this hash is correct so it\u0027s going to run the verify function using the public key the input in the proof if that comes out valid then he can get the actual vrf hash output which just is derived directly from the proof so you can think of it as just taking the proof in hashing it in some way and that gives you the vrf hash output okay so the way you verify is with the proof but the actual hash output is computed using this proof to hash function on the proof itself notice that the proof the hash function is not keyed okay so the keying is for the proving function the public key is for the verify function okay so um what are these things useful for so the way we came to this is four through n SEC five which is solving a zone enumeration problem in DNS SEC it\u0027s also used for key transparency in the conex project and all sorts of derivatives of these projects in both cases vrf is used to prevent dictionary attacks okay we\u0027re gonna see exactly why this works since then it\u0027s I also found that it\u0027s being used by cryptocurrency called Al Gore and in this use case it\u0027s a little bit different what they\u0027re doing is to do randomized selection that can be verified and I\u0027m not going to talk about this application in this presentation but I\u0027m happy to take questions about that after we started talking about vieira have some various venues that we started to find various implementations slightly different implementations of the same underlying idea okay so it comes from the Sean penderson Sean penderson proves for that we that are used on elliptic curves very very similar ideas but a lot of them were flawed when I say flawed I mean seriously flawed in the sense that you could break uniqueness and uniqueness is the key property of URF so I\u0027m going to show you in the next slide what uniqueness is but I believe that a standard would be very helpful because of various different implementations of this some of which actually had this bug that was fatal for a vrf okay so now let me show you the the uniqueness property which is one of the key properties of Revere F so what uniqueness says is that there\u0027s a one to one input one to one relationship between the input and the hash as long as you know the public key okay so if you think about sha-256 if I hash X I\u0027m always going to get Y it\u0027s not going to come out to a different value every time I do the hashing a vrf will give you the same property that every input maps to a unique output given the public key okay and that that\u0027s why you can use a vrf the same you might use a hash function you know that there\u0027s a one-to-one relationship between the input and the output more formally what this says is even an adversary that knows the secret key cannot find two two outputs that map to the same input okay so it can\u0027t be two outputs that map to the same input we know that there\u0027s a one-to-one relationship between the input and the output even if you know the secret key and that\u0027s the same as with sha-256 right we have this type of relationship with that similarly we want to have collision resistance even in the face of an adversary that knows the secret key in this case of the public key is fixed even an adversary that knows the secret key can\u0027t find two inputs that go to the same output so these two properties give us a nice one-to-one relationship that "
  },
  {
    "startTime": "00:29:23",
    "text": "allows us to use hash functions and hash based data structures basically you can substitute your hash function with a vrf okay but why would you want to use a vrf into this last property which is pseudo randomness and what it says is if I take if someone gives me a vrf hash output I have no idea what input it corresponds to okay so if I just get the hash value so notice what\u0027s happening here the hasher is computing the proof then computing the hash by taking proof to hash of the proof he gives me the vrf hash output i as the adversary have no idea what input this hash corresponds to you I cannot do a dictionary attack where I test all the inputs in my dictionary hash them and see if they match this hash value that was given to me I cannot do this because I don\u0027t have the secret key okay so that\u0027s why vrf is actually useful in a lot of these use cases is because it prevents dictionary attacks okay so if this isn\u0027t clear please ask me questions in the in the question answer session but this is the real important thing so it acts like a hash function that stops dictionary attacks and this is just the formalization of what I just said it was so these two use cases which are where we started with this are about preventing dictionary attacks I just want to briefly show you kind of what the idea is here forgetting about V ahrefs for a second and I\u0027m sorry there\u0027s public key and secret key on this slide they shouldn\u0027t be there in the normal case where we have a hash based data structure and we have the root of the data structure the querier will send a query to the hasher he\u0027ll give him some sort of information from the data structure so this case I\u0027m drawing the commercial tree of getting the Merkle path anyway it doesn\u0027t matter you get some information he this information about about the data structure and he can verify that his input is in the data structure but if he does this a bunch of times he starts to learn a lot about the data structure right so the second input he gets another branch and you can imagine it keeps doing this and you can get the whole Merkel tree and so on then he can start to do things like dictionary attacks to try to learn what exactly is present or absent from this data structure okay so this is where this is where VXR come in if instead of using a regular hash function we use vrf hashes then now when I want to get the answer to a query I have to ask the hash sure to answer the query for me I\u0027ll get still the the part of the data structure that I\u0027m querying about but I\u0027ll also get this proof that only the hash sure can compute okay and then I\u0027ll use that proof to verify first of all that the hash are answered me correctly so I\u0027m going to take the prove the the proof the input and the public key and run the verify function then I\u0027m going to get the hash value and check that the hash is in the data structure so it\u0027s just like what you would normally do except you first need to do the verify function and rejected verify fields okay so this property ensures that the hash sure cannot lie to you about the output and the vrf pseudorandomness property ensures that because you can\u0027t compute hashes on your own you can\u0027t start reversing hashes and trying to enumerate everything in this data structure and that\u0027s what we\u0027re using it for an N SEC 5 that\u0027s what\u0027s being used in con X and key transparency and all these other applications ok so what\u0027s new in this draft there\u0027s two vrf specified in this draft "
  },
  {
    "startTime": "00:32:23",
    "text": "the more efficient one is the elliptic curve vrf we\u0027ve done sort of so when we were designing this we wanted to be fast we wanted proofs to be very short because the proofs are the things that are on the wire so as you can see here the thing that\u0027s traveling around on the wire all the time is the proof so we really don\u0027t want that to be excessively long for various applications this is important in order to optimize all this stuff we did very careful security proofs you can see on the bottom of the slide I have a link to a paper that we have on the crypto eprint archive you can see all the security proofs for all of these brf\u0027s that are specified in this draft so everything we specify comes with the security proof that you can find there in terms of the elliptic curve vrf it so the way we wrote it is we wrote the general vrf then we have cipher suites so depending on what curve you want to use what hash function you want to use those are specified in the cipher suites it for this particular draft we were very careful to deal with curves with cofactor greater than 1 so 8255 1 9 curve has a cofactor greater than 1 we were careful to actually make sure that we did everything correctly with the cofactor our security proofs were actually also updated to include the cofactor in the security proofs so that\u0027s the one major change that we did here the other thing that we did is that we added a key validation function so if you saw my presentation that sag what we were talking about we were saying that if the public key was given to you by a trusted entity then the vrf was secure ok so if you trust the guy who gave you the vrf public key then the vrf was secure with this draft we found a way to get around that requirement so now you can validate that the public Hugh was given to you for the vrf is actually a good public key for a vrf and why would you care about this right because the adversary is the one who potentially is choosing the public key in some applications so we want to have a way of verifying that the public keys secure ok so I\u0027m gonna wrap up really quickly with the elliptic curve vrf and just to show you what it is it\u0027s very very high level okay so our public our secret key is X our public key is G to the X what do we do to hash an input we take the input we hash it to a point on the curve that\u0027s H then we raise H to X which is the secret key ok so that\u0027s R and then if you look at the book the corner of my screen the screen over here you can see that the actual hash output is just the hash of this gamma so we take H to the power of X and compute the hash of that that\u0027s the V RF output ok now I need to tell you what the vrf proof is what the vrf proof is is this value gamma sorry I said alpha but I meant gamma this value gamma is part of the proof and then there\u0027s these two values C and s what are these things there are zero knowledge proof that gamma and the public he have the same discrete log so they\u0027re raised to the same exponent ok so we do we attach the G or knowledge proof that this is the case and that proves to you that gamma is a valid hash of the input right because gamma is H to the X so you verify that what our gamma is is H raise to the power of "
  },
  {
    "startTime": "00:35:24",
    "text": "X you know H you don\u0027t know X but you use this year knowledge proof to verify that ok so that\u0027s this is this by the way this is the pattern that you see in all the different implementations that we found in different places and the details aren\u0027t exactly how you specify the cofactor and so on which we\u0027ve done in this draft ok so it with that also okay Thank You Sean we have time for questions comments please come to the mic state your name my name is Miss Louise bus and sure I get it rides at least instruction involves public key cryptography so it\u0027s performance is much vs. and performance of classical PRF functions that are very often used for in places of where performances require didn\u0027t for example for computing mic so these construction wouldn\u0027t it this yeah absolutely you would not want to replace prfs with a VRS absolutely not that\u0027s not what they\u0027re for so there are very specific cases where you want to use brf\u0027s it is not just like now this is the better thing and we\u0027re not going to use PRS anymore absolutely not no they\u0027re totally different types of primitives thank you yeah Brian 40p film first I just wanted to support this you know general effort I think you know VR I can you know brf\u0027s are indeed extremely useful and actually another application case I\u0027m familiar with some very nice pass a distributed password protection protocols and algorithms by Gregory Nevin and some some others are used VRS that I know of so that\u0027s I think yet another interesting application area you can add to the list um just a a question on the the the trusted versus untrusted public key and the verification function you added is that something that would also we solved by just using say a decaf encoding or or is it a separate I haven\u0027t had a chance to read that part of the draft yet I unfortunately don\u0027t know what a decaf encoding is but we can talk about that after I tell you that for the EC vrf they looked a curvy RF the way that we do it I think what we have to do is just check that the public key if we raise it to the cofactor is or is not a point in infinity I have to write something really simple it\u0027s not complicated like we we take it the string we convert it to note the kerf point we check that\u0027s a valid elliptic curve point we make sure that you know that\u0027s all it is and it turns out that that works yeah yeah so really like the engineering task was not specifying the function but proving that this function is sufficient sure yeah well you at night nice work right just a clarification Brian I don\u0027t think you could use decaf because you need to know the discrete log of the element that you generate and I don\u0027t think decaf gives you that like this thing okay I I "
  },
  {
    "startTime": "00:38:25",
    "text": "understood decaf was mostly a mechanism to make sure that you don\u0027t get burned by the cofactor in doing you know arithmetic so so that you canonicalize all points that have that are you know the you know the same modular the cofactor but I\u0027m not an expert on decaf so it you know you could be exactly right so yeah this is Jeffrey askin approximately how many bytes are the proof and how faster the are the operations okay so I don\u0027t know the numbers for the operations of the how fast it is for some reason before this meeting but in terms of the proof so that\u0027s an elliptic curve point gamma that\u0027s one elliptic curve point so if you do point compression which is what\u0027s in the draft that\u0027s 256 bits if you use eg 255 or 9c is a half of a sha-256 output so it\u0027s 128 bits this getting this down to 128 bits like we cut it in half we our security proof showed that this was okay so that was one big innovation from the security proofs and then s is the output of a hash function again so I sorry this is an exponent value so it\u0027s again 256 bits so it\u0027s 256 plus 128 plus 256 okay any more questions or comments so let me put a question to to you as a group what are your feelings about adopting this as a si FRG draft going forward so we could do a hum we like to do hums in IOT fo Nixon I believe we are so I\u0027m gonna ask two questions one should we adopt this I sort of hi w this I say we\u0027d like to adopt this and you hum and then we should not adopt this and then you hum okay so let\u0027s start with please hum if you think we should adopt this draft at the CFR G doc document and for the opposite if you think we should not adopt this as IC FRG document please hum now or forever hold your hum okay I think we\u0027re I think we\u0027ve got a pretty clear consensus in the room that we should adopt it we have to go to the list formally and ask on the list what people think because not everybody can come to to the meetings and we\u0027ll do that in the next few days and kind of sense they are - okay okay thank you very much chef that\u0027s great thank you [Applause] [Music] "
  },
  {
    "startTime": "00:41:25",
    "text": "okay next up we have brian ford from epfl he\u0027s going to be talking about a collective Edwards curve digital signature algorithms thank you pray so this is based on something we\u0027ve been working on for for quite a while now in very various forms the new development is basically we finally wrote the first draft of a of an actual attempt at at an actual spec for for a simple collective signing algorithm and I\u0027m going to mostly just go over I won\u0027t get deep into the detail technical details for that you know read the draft if you haven\u0027t already but I want to mainly mainly kind of motivate it and talk about applications why why this is interesting because because at this stage we\u0027re just trying to trying to see if there\u0027s support for for working on standardizing of this now this is one of the many types of cryptographic algorithms that\u0027s motivated by splitting trust avoiding single points of failure x\u0027 by by building constructions that split trust more widely so that so that no single compromised node or participant can compromise the whole system and collective signing or aggregate signing or multi signatures you have a lot of different variations and several different names this is the draft it is one of a kind of constellations of possible ways to design collective or multi signatures but the basic idea is where you have multiple independent parties who want to collaborate to validate and sign some message or statement of some kind and often this is a threshold group of some kind T of n it doesn\u0027t have to be it can be there can be more complex predicates like you know t1 of n1 you know in one group and you know another threshold of another group also has to sign things like that too but the this is useful for many purposes one is just for transparency you know ensuring that you know many witnesses have seen something and also to eliminate single points of failure and a nice property of the of snore signatures of the type that RFC 880 32 just standardized is they support this kind of multi signatures very very easily and efficiently again without getting too much into the technical details this is this is just kind of a an illustration of the basic way this works any any snore snore signature is formally a challenge response Sigma protocol basically operates in three stages the signer creates a snore commit which is basically a picks a random "
  },
  {
    "startTime": "00:44:26",
    "text": "value little v computes G to the G to the B which to produce the public commit big V uses a hash function to turn that into a challenge and then computes a response which is mathematically related to the private key which is a little K in this case and the classic snore signature is is the challenge response now RFC 3032 uses the slightly different RS signature format which is which is equivalent but you know kind of formally the difference doesn\u0027t really matter it\u0027s just a representation thing so the the fundamental modification to support multi signatures is quite simple all you do you know at least in the basic basic form you have multiple signers now you can have any number each signer has to create their own have have their own Shanor commit so each signer picks a you know we wanted little v1 bit little v2 computes a corresponding public committed we big v1 big v to somebody let\u0027s call it leader but it can can be anyone there there\u0027s no no one has to be trusted especially trusted here someone basically combines all the commits together computes a computes a challenge I should mention everybody needs to verify the challenge there were there\u0027s a bug in the first draft it\u0027s it\u0027s being fixed that that\u0027s important but so everyone can can compute or recompute and verify the challenge and then everybody computes a response corresponding to their part of the of the of the commit and then outcomes basically when you combine them together in the right way outcomes basically a single standard snore sing that signature and this can be in either challenge response form or RS form the draft is written with RS form to be consistent with RFC a t32 and so it kind of just works and so basically you can then verify one of these collective signatures against the combined two public public key of all the of all the signers all at once with basically just the cost of a signal a single signature verification so this is you know nowhere near new there\u0027s tons of background theoretical background of you know this and vary in different variants of this scheme that won\u0027t go through these just going through some applications so so actually a your two ago I can\u0027t remember how long ago I presented this this paper where we were experimenting with making this really scale and applying it foot as a as a transparency mechanism basically the idea is you can take kind of any critic security-critical Authority service maybe the you know DNS SEC root zone or a CA or you know any "
  },
  {
    "startTime": "00:47:28",
    "text": "protocol you you like if you want to ensure that if if it does something wrong a lot of people will know you know whatever you know the complete collection of any everything the authority sign so the authority can\u0027t be coerced into secretly signing some backdoored you know some evil something or other without anybody knowing well you you attach this group of witnesses so that you know the witnesses have to sign to and make sure everything that gets signed gets logged you know for better better or worse right so without getting into the details again basically we demonstrated that with the right protocols you can you can make this extremely scalable up to like you know the thousands or tens of thousands of participants can can work together to collectively sign messages in a matter of a few seconds we don\u0027t you know I don\u0027t think that any immediate application is going to really need that scalability but it\u0027s nice to know we can get it when we when we went to verification cost in terms of CPU time goes down by orders of magnitude depending on the you know a number of verifiers as you would expect and also of course you get significant signature size savings and now one one caveat the reason that that the collective version is not absolutely constant time as you might expect based on there\u0027s a simple way I explained it is that you know realistic collective signing scheme you\u0027ve got to have a way to tolerate participants that are failed or not online at the time you want to sign and so you have to be able to adjust down you know the group of signers to some set of signers that still satisfies a threshold and so in the design proposed in the draft there\u0027s basically we basically have a standard IDI 25 519 signature plus a bit mask of the actual participants and the bit mask is is part of the signature in that it\u0027s fully checkable you know any attempt to to change the bit mask will be will invalidate the signature but but so we basically need one bit of per participant and so so it\u0027s still linear you know linear space but but way way smaller um so and although it\u0027s not really the emphasis of the draft we can make this and and this is you know this is optional we can make this extremely scalable with with pre structured communication but that\u0027s probably not necessary most you know small-scale applications so we\u0027ll get into that other use cases so so cryptocurrency the cryptocurrency community has been expressed quite a bit of interest in this for compressing the "
  },
  {
    "startTime": "00:50:29",
    "text": "size of transactions by aggregating you know a bunch of transaction signatures into into smaller space I again I won\u0027t get into the details some of our own blockchain work has has used collective signatures to build faster and more efficient BFT based block chains basically using collective signing as a as a primitive - as a scalability primitive within the inner loop of the DFT and just extremely recently actually uh our latest work in this space just coming out in the USENIX security next month extends this to use these collective signatures coming out of a blockchain to create basically a cryptic cryptographically verifiable and traversable blockchain set that\u0027s kind of time travel above any two parties have two reference points anywhere on on a you know a time on the blockchain they can prove to each other that you know any transaction they care about is indeed there and you know with respect of you other reference point either forward or backward in time so for details see the paper but just just you know kind of laying out some of the applications we think are interesting and I\u0027m sure there are others I would welcome suggestions you know kind of if you know of other applications that I missed please let us know anyway so again I won\u0027t get into a lot of detail on the draft itself this is only intended to be a starting point it\u0027s you know definitely not perfect we don\u0027t claim it\u0027s the ultimate or you know perfect formulation or representation or things we tried to put some effort to to align it well with Rd RFC a t32 that\u0027s probably not perfect either but so it you know goes over the basic algorithms and then then a simple non tree strong kind of simple as possible for Phase protocol for a leader and some some co-signers to work together to sign things and then a briefly covers the more scalable tree-structured protocol for situations in which it\u0027s useful and then message formats currently protobuf Facebook we\u0027re not you know we\u0027re not we\u0027re not wedded to any particular format now there are plenty of questions so you know if the group decides that this is a worthwhile thing you know direction to try to standardize there are some of course some issues and this is to be discussed and decided this is certainly an incomplete list but some of the questions that we already know about one is there\u0027s a trade-off between getting strict absolutely strict and non malleability of the signatures which would certainly be desirable for various reasons you know not malleable signatures have burned you know various people in the past and so yeah that we "
  },
  {
    "startTime": "00:53:30",
    "text": "would like non malleability on the other hand we can\u0027t get strict non-vet non malleability as well as protecting against a certain type of internal do s attack so so if somebody goes offline at the wrong time during the signing process do you have to restart from scratch or can you continue if you want to be able to continue you need some limited degree of malleability so a trade-off potentially to be discussed another important topic is the way to defend against well-known related key attacks so if I just use the the simple algorithm I mentioned and you know take any collection of public keys then I\u0027m hosed it\u0027s not secure because somebody an adversary can compute a malicious public key related to other to you know the honest nodes public keys to cancel them out that\u0027s not good the standard way to do this is just to make sure that everybody who has a public key in one of these groups has proved knowledge of the corresponding private key standard is kind of standard cryptographic press practice anyway if you\u0027re using PGP keys to feed this thing you\u0027re all automatically covered but you know it\u0027s an important caveat another alternative way of doing this that has some advantages and some disadvantages is to modify the actual signal signing and verification algorithm to hash the different public keys separately to eliminate that that requirement it set up a stage but it has some some important downsides too so and finally the basic algorithm assumes that verifiers know the whole list of public keys that they\u0027re verifying the signature signature against if we care about you know if we expect that list to get very big and we care about the amount of state that the verifier has to know in its root of trust then we can you know we have ways of changing that again at some cost in complexity and stuff so and they\u0027re probably going to be more issues so that\u0027s so I wanted to leave it at that so at the moment the main priority here is to get feedback on you know kind of is this interesting and important and worth worth working to standardize and you know kind of with the understanding that you know all of these issues you know potential questions or would be to be decided in that case okay Thank You Bryan let\u0027s open the line for questions and comments see a race to the mic it\u0027s not a race no Steven so yeah I think yes discuss stuff is interesting and useful what wouldn\u0027t be once I clear to me as hell would see f/4g know it\u0027s finished unless there\u0027s somebody actually saying here\u0027s exactly what I need in an idea spec or something like that well we are we are using this I don\u0027t "
  },
  {
    "startTime": "00:56:32",
    "text": "know you know for a number of things I mentioned but you know of course I would love to hear other people you know who who would like to or are you know interested in using it and and I you know I\u0027m not sure how so yeah that\u0027s a good question I just I don\u0027t know the answer mm-hmm for hon Becca yeah I\u0027m very interested in this I mean I\u0027m not managed to get my head round than the crypto yet what I would like and why could you really use is other moment for encryption I can split a DP hand key into two I can put a private key in a trusted platform module inside the phone that is unique to the device never leaves it and then have a second encryption going on at the application level so that I don\u0027t need to trust what the manufacturer put into the device when it was made now you said that this is a drop-in replacement for the ad 255 1:9 signatures is it another I didn\u0027t say it\u0027s a drop in replacement yeah it\u0027s kind of not it\u0027s I said I said it\u0027s designed to be as closely aligned as possible in terms of formats or stuff so it\u0027s not a not a big change from it but it\u0027s a different thing you know in it I\u0027m going to have to have a different verify I\u0027m not just going to be able to I mean with encryption I can make it completely transparent and you\u0027re not even aware that have especially that that\u0027s a subtlety so that whether you need a different verifiers a subtlety that maybe should be added to the issued thing it could be designed so that the ED 2519 verifier core you know kind of the core 64 byte you know ed 25 519 signature is exactly compatible with the classic ed 25 519 that would have some downs certain downsides you know we because it would prevent us from messing with that with what goes into the hash and stuff we wouldn\u0027t get to do that would exclude the possibility of adding the hash of the bit mask in there too first trick malleability and you know certain things like that but it\u0027s that is an option that that could be could be considered and I\u0027m very open to that yeah I mean if you can do that and I can do it as a die I will take any of the other compromises because at the moment the only way that I can create a signature like that basically as soon as the public as soon as the secret that was used to create it is known well you know you blow up the whole system so I\u0027d be very interested if you could okay yeah that\u0027s that\u0027s I haven\u0027t hadn\u0027t thought of that use case but yeah that\u0027s very worth exploring Thanks this Richard Barnes right um it seems like we have a combination here in this "
  },
  {
    "startTime": "00:59:33",
    "text": "draft of some cryptographic instructions and some protocol considerations and a lot of these issues here are kind of at the intersection of those things and it might have kind of some disjoints pools of expertise in terms of people we need to review this I mean do you think that there is I thought that\u0027s what this room is for isn\u0027t it like we have both here and those yeah well what I\u0027m wondering though is how conjoined those two issues are like whether it makes sense to kind of define the cryptographic instruction that could be realized in various protocols as and kind of so recall separately by my understanding that the cryptographic part is basically already done and defined and formalized and proven in the collection of papers I cited on an earlier slide we really haven\u0027t you know added anything to the to that there\u0027s I do the the only subtlety is you know the bit mask and how that interacts with the kind of a standard cryptographic construction there there\u0027s you know it it would be worth you know looking at that a little bit more closely than we have but you know I\u0027m pretty sure that it doesn\u0027t fundamentally change anything on the on the crypto side so this is mostly you know intended to be about you know kind of just defining a standard usable format in a concrete rather than abstract form you know kind of that I can actually build a signer and verify for it and and you know kind of a protocol and actual functional protocol for producing these things that\u0027s the intent anyway time Brian Ross Owsley um so I\u0027m very interested in something that can be validated in the same way that Philip just talked about but I thought it maybe I misunderstood that you said the verifier needed all of the public keys for the pool of potential signers so that doesn\u0027t look like a public key on the verifier side yes so so fundamentally yeah so fundamentally that\u0027s not a requirement so it depends on so remember there was the the caveat about related key attacks you have to be careful of right so if you and the need for the list of public keys is partly related to the need for you know if you\u0027re trusting the trusting this group of public keys at some point you or someone you trust needs to verify that each public listed public he also comes with a self signature or a proof that the owner of that key really knows the the private key corresponding to that and so at that point you know somebody you or somebody you trust needs the "
  },
  {
    "startTime": "01:02:33",
    "text": "whole list two tickets but you don\u0027t necessarily need that list at verification time right so in other schemes there\u0027s a dealer right who gives like a mere scheme yeah or or you know the Boyd split thing and if several have dealers and then of course you got to make sure the dealer forgets yeah here you\u0027re saying there\u0027s a combiner or there\u0027s needs to be somebody that when each of them makes up their key pair make sure that none of them are related or says pick again so no and so so this scheme does not require the public private key pairs to be generated in any special way unlike Shamir secret sharing they don\u0027t need to be generated for this they can be JEP solutely bog-standard at 25 519 or ed 448 public/private key pairs that are you all the time first standard you know you know our FCAT 32 signatures and you know once you have formed a list of these public keys and verified that you know kind of a self signature of any kind for each one of them that can be generated in a totally standard way then all you do to for the actual verification of a collective signature is you basically just add together on the elliptic curve just a little it the curve point addition the public keys that actually participated in this particular collective signature according to the bitmask in the signature now the bid mask is only is only there so that you can tolerate failure so you know if you want five of ten nodes and three of the groups aren\u0027t currently available you know somebody can still form a signature without him and just transparently declare okay seven nodes were there but those three were missing so the verifier has to you know take the full list of public keys and then subtract out the three that weren\u0027t there and then after forming the adjusted correctly adjusted you know aggregate public key then it\u0027s a completely standard and 2519 signature verification again so that\u0027s so in other words you just first adjust the public key the aggregate public key correctly and then it\u0027s completely bog-standard ed ed 25 519 or ed 448 so the verifier needs to do know all the public use to know what subtraction to do in in this in the the current formulation in the in the draft so yes that\u0027s that\u0027s one of the options to be discussed though that can you know there are variations of that so yeah the second to the last bullet in my mind is trading off some trusted set up where the pool has to all be there and cooperate to do something yeah but then everything else from sign of the signatures that are produced and "
  },
  {
    "startTime": "01:05:33",
    "text": "the public key that needs to be certified look just like ones that don\u0027t require a collaboration would be a big big again so so so for that so for that purpose um that combination of functionality you probably really want Shamir signatures which is very useful as well and I\u0027d be very open and supportive to standardizing that too if people like Shammi are better than better than this so there are some phrases I guess I\u0027m saying I can see protocol requirements for that but yeah I totally agree yes and I\u0027m not sure where so so just to you know a couple of the relevant trade-offs so you know kind of this Shamir sharing has a lot more complicated set up but but then you always get the same signature for it for any given set of signers Shamir has so if you want privacy of the signers Shamir is better if you want transparency and accountability then this is better because with Shamir you never know which particular threshold actually signed and so if an evil Thresh a malicious colluding threshold signed something bad you know they can all deny responsibility I didn\u0027t I didn\u0027t sign that you know and and so it\u0027s there\u0027s there\u0027s kind of a somewhat subtle important trade-off there okay I think we have time just for one more question if it\u0027s a quick one and a quick reply please train thank you thank you yeah Vasily nikolaevka for a very short question about the performance so the participants have to establish a secure connection with a leader and how does it affect them performance of the whole signature and does it still remain a more more faster and faster than existing CMS will signature the CMS EMS Multi signature um so so the you know the the the simple way to to actually run the protocol if you know we\u0027re not if we\u0027re scaling to like tens of signer tens or even hundreds of signers and not thousands is you can just have the co-signers connect directly to the leader with whatever in whatever way you want that that doesn\u0027t even you know you probably want those connections to be secured for do s attack resistance and stuff but if you know if they aren\u0027t secure it\u0027s not going to produce bad signatures it\u0027ll just you know at worst make the protocol fail to produce the signature you wanted for scalability you know kind of once you get into the hundreds certainly thousands range you start to not want all of the co-signers to have to connect to the leader directly and then the you know kind of the tree structures or other topologies become useful but that\u0027s you know I I don\u0027t expect that\u0027s "
  },
  {
    "startTime": "01:08:33",
    "text": "going to be the first use case anyway you know so I but you know so our experiments tried to do you know kind of try to explore that space you know how hard how hard you can you push it and it looks like you can push it pretty far fairly efficiently but you know I think I don\u0027t think you necessarily need to go to the full level of complexity and a lot of applications is that thank you okay we\u0027re quite over time though so thank you Brian just a very quick question to the room like who is interested in reviewing this draft and can I have a show of hands please one two three okay about six hands or something maybe I haven\u0027t seen somebody okay all right thank you thank you so Kenny is going to present on behalf of poor Hoffman who is having conflicted session at the moment so I\u0027m gonna I go to channel Paul Hoffman to the best of my ability that\u0027s quite a tall order actually so Paul has a draft Hoffman c2 PQ which has been around for a while and the motivation for his talk is to try and reanimate that draft and get people interested in it so why should see frg care as he says on these slides there\u0027s a lot of good discussion going on about what algorithms we should be using post-mortem thinking about that but there\u0027s not enough discussion going on around how we deploy these algorithms what is the likely timeline for the arrival of large scale quantum computers what the costs of deploying those algorithms will be when we need to switch over and we know that changing algorithms especially saying algorithms is expensive its error-prone actually it takes a lot of time as well we have lots of experience of that in ATF so coming to his draft it\u0027s not manifestly not about post-mortem algorithms themselves it\u0027s about how we handle the timing issues around there at the transition to post quantum algorithms the draft is intended to address a number of different audiences so execs c-level people I suppose we want to understand the transition and what needs to happen when we get there security experts who want more information about quantum computers their capabilities how they will scale what the cost will be how fast they can break ease and cryptographers and physicists who want something readable that they can point people to so a standing document that people can be directed to when they have questions about why should be why should we be worried about quantum computers Paul has admitted that the draft is quite incomplete it needs work there\u0027s a lot of sections that are missing material and references and it\u0027s really very much an early draft there is not enough information there about the wide range "
  },
  {
    "startTime": "01:11:33",
    "text": "of guesses that people have about when this this might happen whether it will happen and indeed it may be too early to give useful guesses for that kind of thing but at least we could document that fact and be honest about the uncertainty surrounding the advent of large-scale quantum computer so Paul is proposing a particular way forward you would like CFR G to adopt this as a work item he would then bug people in the only way that you know that\u0027s how Paul will operate to fill in the holes and to suggest new parts I think I for one would be interested to help him if I can to do that you would like to get it discussed outside of CFR G at post quantum cryptography events and general crypto meetings to get feedback on it and in to finish this document within a year or so but recognize that it\u0027s only a snapshot of our understanding at that particular moment in time so we would have to return to it some years later if we have better research better information about how large-scale quantum computing is developing and that\u0027s all of poll slides so I\u0027m very happy to take questions or comments and I\u0027ll do my best to answer Phil Han Baker yeah one of the things that would help me would be to at the moment apart from Lamport signatures there isn\u0027t a lot that I can go on so from my point of view looking at protocols what I\u0027m interested in is the size of the pieces of data and rough estimates of how much of the computing power and whether it\u0027s going to be practical to do something that is per Squanto in public key or whether we have to back out into doing Kerberos on a large scale which is certainly possible but might be challenging so in response that I would refer you to the first bullet point on this slide it\u0027s not about the algorithms it\u0027s about it\u0027s about the transition and the time scale for the transition and how we should do that yeah so the may not meet your requirements that\u0027s that\u0027s polls in intended scope but I think that speaking in a personal capacity I think that\u0027s a fair comment that\u0027s a big challenge for IETF as a whole hi David Murray Oh Cisco so I think it would be not a good idea if this draught discouraged people from using 256-bit keys and so I understand that I believe I understand Paul\u0027s goal is to make people think about the costs of a transition and not rush into one and not make decisions too early and I totally "
  },
  {
    "startTime": "01:14:33",
    "text": "respect that and agree with that but at the same time you know I have a number of customers many customers who use aes-256 because they can as a configuration option they say oh this is postpone Emre and they take that box I don\u0027t think we should be discouraging people with that mindset I think the current draft documents that exact fight the the major threat is to public key algorithms rather than symmetric yoselin versus Grover\u0027s algorithm doubling the key lengths etc I think that\u0027s in there but maybe it could be emphasized more strongly hi Steven fellows oh I know real opinion about mothers-to-be industry or somewhere else but I second what Phil was asking I think you know a guy ETF people could do with input about the sizes and performance as known today of the possible options I guess the sizes is probably more important in terms of you know future protocol changes because you can maybe assume that some performance decisis could be probably more precisely specified or range is given now I\u0027m not sure I think we can certainly do that for hash based signatures we indeed have two drafts working their way through CFR G at different stages where there are very concrete parameters I think for things like key exchange in public key encryption it\u0027s still very much a matter of development that\u0027s up in the air it\u0027s happening right now I think the NIST competition for which submissions are due in November this year November 30th is gonna flush out a lot of concrete proposals with parameters and then as a group we might be in a position to get a sense so the kind of ballpark of the kinds of numbers we may be looking at without making concrete decisions about those numbers so we should be able to get at least order of magnitude right than this yes I think I am you know and I don\u0027t think that information needs to be in an RFC right necessarily and notice need to be finished in a year but if it was some information was available somewhere people would to know what kind of things to start to think about thank you hi I\u0027m Don Piper I guess to me this sort of falls in the realm of fear uncertainty and doubt there\u0027s a lot of guessing here and now there\u0027s a lot of hard science under this and I\u0027m not sure what the point is to bless a draft that kind of guesses and doesn\u0027t suggest any sort of real concrete way forward okay thank you I\u0027ll let I\u0027ll let Paul respond to them yes don\u0027t cheat okay so I guess the question for the chair is about adoption putting you in the hospital Allison yeah can we have a show of hands of how many people are interested in working in this on this document in the CFR G if you\u0027re in favor of this of working on this document and helping out reviewing it please show your hands "
  },
  {
    "startTime": "01:17:35",
    "text": "okay with one seven eight hands now that\u0027s not bad um if people think it\u0027s a waste of time and we shouldn\u0027t be doing it here show friends I know you\u0027re on the job remember two people in Jabbar Andreas and John I\u0027ve got I\u0027ve already said they would help work with on it - okay okay so I think there is some moderate support and nobody wants to admit that they hated the idea of it so or not okay hi I\u0027m David McCoy I\u0027m going to be talking about some joint work that I did with Scott Kluwer and Michael Kershaw and this is the latest iteration of the hash SIG\u0027s draft so what\u0027s new well we did a security tweak and that\u0027s the major difference between version 7 and version 6 and this tweak is motivated by an analysis that Scott dead which is posted up on ePrint and I\u0027ll talk about that that\u0027s the the most interesting part of my talk today we also published I shouldn\u0027t say we panos Campanas one of my colleagues and Scott Fluer published a comparison that\u0027s also on ePrint archive which compares this draft this specification with X MSS and Scott also posted on github a full-featured C implementation and I won\u0027t be talking about that much later but Scott did a good job of considering the different optimization strategies needed especially for a signer which is it\u0027s a little non-trivial so it\u0027s nice to see a complete implementation of something so as far as security goes Scott built on Jonathan Katz\u0027s previous analysis right which was called analysis of a proposed hash based signature standard and Scott says further analysis so again this is in the random Oracle model and it assumes 128 bit security level it demonstrates 128 bit security level for the sha-256 based hash based signature scheme and the important difference is going to be described by the following right so shot 256 is a merkle-damgard style hash function right and there are "
  },
  {
    "startTime": "01:20:35",
    "text": "different security assumptions that you can make about a hash function in this model so just as a refresher the merkle-damgard hash breaks up the message X is the vector at the top it breaks it up into blocks feeds these as inputs into the compression function denoted F here and there\u0027s a chaining that goes on of the compression function and then there\u0027s the padding that happens at the end then you might truncate the output so the different security assumptions you could make are these you could assume the entire hash function as a random Oracle this is what Layton and McCauley did in the 90s and what Jonathan Katz\u0027s analysis of the the more complex and more complete scheme recently did right you assume the entire hash function as a random Oracle now this is good in a lot of ways but it also you know we know that hat there are ways that hash functions are not random Oracle\u0027s hash functions are not random Oracle\u0027s because of things like leg length extensions attacks now the the previous graph was not vulnerable to a length extension attack but we did analyze that I should say Scott is the one who analyzed it in this other model assuming that the compression function is a random Oracle right so this assumption is much closer to actual crypt analysis if you look at crypt analysis of reduced round shot 256 this is much closer to that assumption and in analyzing version six of the draft arguably there was an attack that would shave off a few bits of security so we tweaked the order of the elements and it\u0027s not a big tweak but it actually impacts the security analysis in this assumption this model where we assume that the compression function is a random Oracle so a small change and and this is the the motivation so totally independent from what Scott and potterson I did Edward Eden of Waterloo and I Sora who\u0027s a that\u0027s a post quantum security start up on anyone from asara here no yes not so Edward analyzed the security of version six in the post quantum I\u0027m sorry the quantum random Oracle model right which is something that is slightly different in the comparison publication the most interesting thing is probably the performance of the the two different "
  },
  {
    "startTime": "01:23:35",
    "text": "schemes and essentially the LMS scheme the Layton mccauley scheme is over three times faster and these performance figures come from the published X MSS code being compared to the published implementation that Scott did of the LMS based scheme and we actually he he got something like you know four to five times faster nearly for you know the different operations vary in the different parameter sizes in theory you would expect something like a three times faster so I think Scott spent more time optimizing his coat and his implementation strategy but at any rate it\u0027s a very appealing performance number so for next steps I\u0027d like to invite your input on draft seven and we especially encourage you to look at the security analysis and you please also do look at the the comparison document if you\u0027re interested in you know understanding the differences either in security or performance and if I could you know I think a number of people have given us private feedback on the draft so I encourage everybody to you know copy the CFR G list because we would like to ask for this document which is an official RG document to go into a last call process so it can proceed forward and and then you never have to hear me talk about hash-based signatures again you might have to hear Scott talk about it again so you know our point of view is that diversity of mechanisms is a good thing and you know we think X MSS is good work that you know deserves to be an RFC we also think this work is interesting we\u0027ve heard feedback from some others who have implemented it we know that there\u0027s at least one other implementation and performance is attractive and the security analysis has given us and some other people some confidence and maybe lastly the the this approach that we took in the LMS draft went back to the 1995 patent by Leighton and Macaulay which we felt was a good approach technology wise and we did you know changed some of the ordering of the elements and so forth and that we\u0027ve moved to a selection of functions to make a post quantum secure but our our goal was to minimize the amount of innovation that we were doing so that implementers could claim the benefit of implementing an expired patent or a technique where most of the description comes from an expired patent so we think this is interesting and we welcome your feedback so thanks for attention I\u0027ll "
  },
  {
    "startTime": "01:26:35",
    "text": "happy to take any questions thanks David you have time for questions comments from the floor this is Jeffrey Gaskin as mentioned in a couple of the comments on the West on the previous paper it would be nice to have like sizes of signatures and public keys and private keys documented in the in the specification of the algorithms so it\u0027s it\u0027s implicit in there but it\u0027d be nice to to see it explicitly stated that\u0027s a great comment thank you I have a quick question could you could you go back to the slide with the performance fears can you explain what\u0027s happening in the second half of the table where you seem to have like a if I read it properly a 1000 fold improvement for public key generation or is that my misunderstanding I\u0027m sorry say again the second half of the table PK gen 4x MSS is something like 3,000 seconds and you have four point six four did I misunderstand some so the the last column is the ratio of the performance of the two schemes so 33 40 divided by 720 should be four point six four oh I see sorry I couldn\u0027t read your table thank you yeah sorry about that my van is totally my mistake I think I was trying to go to I\u0027m Scott fleur has come to the mutual mic remotely oh no he\u0027s gone because I think you answer the question I get a little Pacman here okay guys can see the pigmented I think that\u0027s a great feature thanks Steve well yeah so I think in addition to size information again it might have to be in an RFC anywhere but it would also be useful to kind know the different trade-offs and implementations that people have faced so whether that would be just a presentation at CF or G at some point maybe that might be enough but there seem to be a lot of them at least from having read the xn and XM SS stuff there seems to be a lot of different possible optimizations you could choose to do I assume the same is true what LMS yes so the the the published code is pretty decent in terms of that and that would you know perhaps Scott could describe his implementation in the future I think that\u0027s that\u0027s a very long topic and if it\u0027s of interest of people then absolutely yeah that would make sense to document some of that better yeah yeah and I think it would be but I guess it would become of interest in people starts as seriously before using these things I would think and also some test vectors will be good as well yeah they\u0027re in the appendix right Thank You divots so your your request for last call is noted by teachers "
  },
  {
    "startTime": "01:29:40",
    "text": "okay we move on we now have a presentation of kangaroo 12:00 unfortunately the the author Benoit vqe couldn\u0027t be here but we\u0027ve got Quinn from NIST who\u0027s very kindly agreed to present it you want to take the thank you can um I\u0027m queen dangit Ness this is not my talk they\u0027re not my slides I would just ask to give this talk for the author he\u0027s gonna come here so the kajak team they designed the ketchup limitation like about nine years ago and we at NIST we standardize the Mushaf three including the chase function arm the shakes they do have very high security margin and the ketchup team right now they they did they designed difference hashing function was reduced like half number of rounds to make it faster and I believe it still wear right they believe still very secure and that\u0027s function called the SOF the zof variable output function shake 128 and in this function not just the hash function but the segmented hash function a parallel of version arm which looks very well today would with the modern machines where you have multiple cores and multiple processors in in the machines arm and the parallelism is really good it goes with the size of a message and they chose only one segmentation size so which is good for for for a hash function which works everywhere um you don\u0027t have to choose different parameters and then end up with different functions so this is very nice function and they got security proof in 2008 and have further extension and other up there proof for their security in different modes and references are listed right there and the nice thing about security analysis for a sponge function are with the permutation is a 4k check you just increase or reduce around to do the trade-off between security and performance and has been no tricks at all since it was designed like almost 10 years ago and the purpose doing that was up to to make the the analysis easier and to and and so that "
  },
  {
    "startTime": "01:32:41",
    "text": "people couldn\u0027t have standard security better and also you didn\u0027t have to do any tweets which changed everything which happens to a lot of different ciphers you know when we do the tricks and then your raphelson change but this reference interchange accept only the around constant so this really excellent design would hopped out to get a lot of independent analysis and to make people feel more comfortable and more confidence about his security instead of different order desires even including AES in the past if we change something that would become completely different cycle so the current attacks current security status right now is that for five rounds people have found collisions on five rounds and for the six rounds with the capacity C 160 which is a lot smaller than standard one 256 people will foul collision on six rounds and the complexity was around 2 to the power 50 and the best attack right now as we know off was done like the couple years ago at Europe on the paper form from Euro Crypt and he got to return even at a time 2 to the 128th even though it\u0027s a claim security level however the attack work was really impressive and to get through a browse and indict India the next output off of the function so it was really good analysis paper excellent and with the performance number arm for the sorts messages and long messages for long as I said you have excellent perform another way around one point something and even go down to zero point seven psycho per bytes but bit of a bias so it\u0027s very impressive on modern machines and again they shake 28 the the function is then can be used as a normal function is even is parallels function but it is a normal house function you can replace any couldn\u0027t have function with this function and used to you will gain a lot of performance so the so as you all know the decay check was was a public design you had a public design personnel it has a lot of analysis and an security proof in the originals back and it was the result from the open international competition that we ran a few years back "
  },
  {
    "startTime": "01:35:41",
    "text": "and they\u0027ve been very actively analyzed by the crypto community and until now it\u0027s been very actively analyzed by different fluorophores around award and it has been proven very secure and thing is to speed up without wasting any crypto analysis basically right now reducing around and any analysis you had in the past they will remain valid that means because you did not change the permutation it\u0027s just different different constants for the from rounds that was that was it and everything I was the same and scalable too if your machine has more cores more more processors and if asking the hash will be a lot faster and it\u0027s not limited to anything and with that I would be happy to try to answer your questions thank you very much please come to the mic if you have questions or comments and state your name Kyle Rose so I noticed that the that Kangra 12 takes three inputs it takes a message a length and a customization string which sounds suspiciously like a label notice that we\u0027ve had arguments in the past about having labels big to the carrot of takes the just a normal message and and as every you said it has function then determine the lens and there will be it that\u0027s a normal hash function sorry I\u0027m looking at the at the draft not at not at the slides I don\u0027t know if there\u0027s a new draft plan if there is a neat customization a spring I believe it is optional I\u0027m not author so but I believe it is optional um so basically you take a message and you hash it out but the shake is an X or F function so you need to specify the lands for your application we want to use um ya know was the it was the customization string that I was pointing out I think I think it it is ocular I\u0027m not the author but I think is optional but thank you for that question I think I think the author would be shade out that information maybe I could ask a question for me absolutely doesn\u0027t have any plans to do anything in order to standardize this right now it\u0027s not a nest function but we don\u0027t know why buzz and you don\u0027t know about a future but the thing is in the past "
  },
  {
    "startTime": "01:38:41",
    "text": "good crypto coming out from the ITF and we we adopted at them for example HTML construction and also yeah we we did that and and we and we are in the in the consideration of adopting the new two curves you know we we are the assembly produce over here so we we are in the consideration of doing that so my personal expectation is that there\u0027s very good chance the curves will will probably be be endorsed or allowed by nurse but that\u0027s my personal expectation the only thing is you can say now is in the consideration of adopting those curves so so it it bears on how things are now thank you and the thing is because the kangaroo drove the function now the security motion is still compatible to this CUDA motion offshore - or even a slightly better so internal security is solid in thumb performance it two times faster then then the occurred in the shake 128 for short messages but for long messages it\u0027s a lot faster and one size fits all thank you for your attention [Music] okay so we\u0027re gonna move to the last presentation of the day is Andrew Alan here ah fantastic let\u0027s see if we can get the technology to work for us so he needs to step on up I think I did change I did try and contact him by email to see whether he was actually yeah but whether he could actually hear and participate so Dan if you can hear us time Brian is your turn to do what you need to do in vitro and take the mic I\u0027ll get your slides up we can anybody hear me Oh send my oh I can hear you dine if you can hear us can you hear me now we\u0027re there in Canada where I assume you are if you ah can you hear me "
  },
  {
    "startTime": "01:41:41",
    "text": "did you come through there first hand do you have my video antastic yes we can hear you hi done fantastic we can hear you too so we have your slides up I think I\u0027m in the wrong hurry I\u0027ll just switch is my mode there okay can you still hear me oh yeah can you hear us no I can hear you fine okay yep yes we can hear you can you hear us yes I can hear you you can hear us okay okay we have a little bit of a lag between the video and the audio but I think we can handle that so done I have your slides up ECC mod 8 to 91 plus 5 if you will try and keep things in sync so I just wanted to first say that we\u0027re intending to comply with the IETF IPR policy so next slide and this work was I basically wanted to find the simplest ECC parameters that were also remained secure and fast so next slide and so there is basically these these features and they have have the benefits so that me the main feature is that it only requires six symbols up for the Galois field size its prime and on that that\u0027s good because it avoids um test keys backdoor and other security concerns and it\u0027s not too big or not too small and some well-known stuff so let\u0027s go to the next slide so this slide explains why I the advantage of the plus five in the permit in the prime here it\u0027s because the Fermat inversion step which is often used in ECC is very simple and here\u0027s the code and ends quite fast so "
  },
  {
    "startTime": "01:44:42",
    "text": "next slide this slide is obviously too much - to present all the information but basically it\u0027s saying there\u0027s many other possible fields and some of them are better in some aspects but all of them are worse in in at least some minor aspects so none of them is better in all respects than this this field so next slide okay so this slide addresses the the question that the symbol count of six symbols it seems a little bit arbitrary but nevertheless that that heuristic still is is somewhat reasonable um so the two points to highlight here is when I use this heuristic for the field I only came up with two secure and fast field sizes that\u0027s in the bullet under just the term lucky and then I only realized as I was preparing this talk that um the one that I prefer of those two shares the same digits as the birthdate birth year of ECC so I thought that was um a happy coincidence so next slide so for this proposal I\u0027m I\u0027m proposing to use a very special curve because basically it matches that you the the proposal to use a special field it\u0027s very compact and it has many of the desirable features that we want and it\u0027s um it\u0027s listed here and you can and read it off offline at your convenience so next slide "
  },
  {
    "startTime": "01:47:54",
    "text": "so this is just one of those usual pictures that you see I don\u0027t know if you guys can see the diagonal lines in the grid here perhaps not on a little bit faint but it\u0027s not really necessary for the theme of the talk so we\u0027ll just go to the next slide right away okay oh I got disconnected can you still hear me okay I\u0027m I\u0027m back on I see the slides so what\u0027s this special curve has to create this special curve it has to forego many security features but each of each of those features requires sacrificing some of the simplicity in the curve and so there there\u0027s always a fear that oh it may be vulnerable to an attack or maybe maybe otherwise so let\u0027s go to the next slide that sort of so we can be fraid of all these risks or we can just go ahead and just try to eat to dive in and try to use this special curve and now next slide so this slide is reminding us that Miller invented ECC in 1985 co-invented it with Co blitz and he use very similar equations to to the ones that I\u0027m proposing to use and he raised the issue he asked whether it was prudent to use such a special equations and so indeed yes he was quite correct about that some of the special equations were attacked the famous MOV attack but in other cases the cases that wood that I\u0027m proposing others but no published attacks since then and in fact such equations have been used in Bitcoin with a great success I suppose on next slide "
  },
  {
    "startTime": "01:51:10",
    "text": "and in this slide I just want to wish happy birthday to ECC and that\u0027s the end of this talk so there\u0027s another talk if you want to take questions on this talk we could do it that way I\u0027m happy to do that first or the other way around let\u0027s let\u0027s open the mic or for questions from the floor on this specific proposal for this curve there any comments or questions from the floor okay I don\u0027t see any so what I\u0027m going to do down is switch to the other presentation and then I will give you a signal to go when when I\u0027ve caught up great okay okay that\u0027s no displays if you want to start so let\u0027s go ready for slide then we actually have a question on the last talks which you just pulled off one second I\u0027m gonna I think I can enable Scott go ahead Scott okay I just had one very quick curve dan could you yeah I heard the question okay thank you this is wonderful isn\u0027t it this table that\u0027s great yep and I think you got the answer so let\u0027s let\u0027s move on to the second presentation now if you\u0027re ready so if you helman mode so let\u0027s go to the Napa next I\u0027ll add the actual first slide okay so this is a slide reviewing Gordon\u0027s attack and the current countermeasures and the main point is that um the risk of a backdoor in in diffie-hellman is is actually quite real fortunately we do have good countermeasures but this proposal will "
  },
  {
    "startTime": "01:54:10",
    "text": "do something a little bit better so let\u0027s go to the next slide okay um so there\u0027s two main main benefits of using this prime for diffie-hellman is one that it has a more compact description so that\u0027s one perhaps informal but quantifiable way of measuring the the possibility of it there being a trapdoor and the second security benefit is something quite unrelated it has to do with um den boars proof relating the difficulty of the diffie-hellman problem compared to the discrete logarithm problem and if you use this Prime then you what happens is you basically nearly optimize that proof so in other words it we have something that\u0027s nearly ideal in terms of diffie-hellman security at least in the proved ability of that so next slide oh and and this slide is just these are highly speculative um heuristics that are nice but they shouldn\u0027t figure too much into a very serious analysis so let\u0027s go to the next slide I think that will be the end yep so that\u0027s the end yeah I\u0027m done so you\u0027re done okay thank you very much so again we\u0027ll open the floor for questions or comments on this if there are any okay thank you okay so a question from me you mentioned the relevant RFC 791 nine "
  },
  {
    "startTime": "01:57:15",
    "text": "could you give us some feeling of why this is better than what we already have in our Z 791 they I guess Daniel is here but too polite to ask that question yes so the RFC chooses a prime with a special compact form based on the idea going back to Gordon himself and the only improvement here well the first improvement is that by being able to represent this Prime in fewer symbols that there\u0027s less room possibility for a trapdoor admittedly that\u0027s an informal argument the second improvement is that is that um the den bore proof so if you took the that RFC 79 19 which I haven\u0027t done yet and you try to reconstruct the den bore proof as as far as I know and the expected result would be that the proof would break down okay thank you anything else from the floor before we close the mic on this one okay it seems not dan thank you very much for your two presentations and for our superior technology appreciate your in a delay so um I think we\u0027re at the end of our formal agenda now so if there are any other any other business that anybody would like to raise bring to our attention tell us about it please please come grab the mic then Irish please come grab the mic and and tell us what\u0027s on your mind are there things that CFR g should be doing that we\u0027re not doing are the things that we are doing that we shouldn\u0027t be doing I could go on okay finishing on time okay in that case thank you for everyone for their inputs thanks everyone for the great presentations and forgetting slice us on time it\u0027s been really helpful and we\u0027ll see you in Singapore we hope thanks a lot have a great time at their social this evening [Applause] "
  },
  {
    "startTime": "02:00:17",
    "text": "[Music] you "
  }
]