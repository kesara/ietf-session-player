[
  {
    "startTime": "00:00:04",
    "text": "chair slots okay oops sorry it's itf 112. you can catch the chair who copies and pastes the slides from last meeting um that's me not joke uh i'm chris the other guy is uh joe or yop if you prefer uh this is itf 112 we're somewhere in the eu time zone um i think we're supposed to be in madrid here's the note well everyone knows where to find this you should read this eye chart but it's probably not i chart on your computer so you can read it all of our resources are available also at austria wherever it says 111 should say 112. boy i'm gonna hear about this later um all the slides should be updated on the meeting materials page fortunately medico has a little quick link for that which is super nice thanks to the medico folks uh blue sheets we don't have to do any more that's taken care of in the medico thing we do need a minute sticker so if someone wants to volunteer to take minutes that would be terrific i thought i didn't make the display any better did it all right i can't tell if it makes any better or not anyway um so until somebody speaks up and says i'll do the minutes we're going to wait the jabra scribe is the jabber room which automatically gets caught up for us or is the in in midiko chat i mean oh ah i think mr holliman is telling me ah joel will do the minutes terrific he's a great minute sticker mike holliman"
  },
  {
    "startTime": "00:02:00",
    "text": "perhaps has a link that will tell us how to use the share slides for next time um agenda bashing charter update oh we're no charter update this time sorry here's our agenda we have some tlb stuff for bmp from paolo an update to his document uh five or ten minutes worth of chat time and then edward uh has an on-hose load balancing for telemetry conversation to have some research work i believe he and thomas uh did in the in the hackathon i think i could be mistaken about the hackathon part please feel free to correct me edward when you get up i think at this point we're not finished it's time for paulo so unless there's any questions we want to get out of the way first give people a couple minutes while i find the slide deck or paulo you can do the slide deck yourself if you prefer just do the screen cherry thing and we'll oh i i prefer if you can share the deck that i sent just because i have troubles sharing my screen with me deco yeah no problem and uh screaming video so that you can look at me as well okay let me give you a second to go get the right set of slides going sure share screen and hopefully i i did the copy paste better than you guys maybe i mean it's honestly gonna be easy to do that better than me for sure all right let me uh also i suck at webbing my son tells me all the time all right"
  },
  {
    "startTime": "00:04:00",
    "text": "okay so yeah and so um i can start um yeah five minutes about um tld's for route monitoring and peer down messages um it's just a very quick update um so if you can go to the next slide already uh very short recap what the draft is doing is that like not every uh bmp message does support the tlds and so essentially and so namely the root monitoring and the peer down so we wanted to add that support and in order to do that we are bumping the version to version number four next slide please still as a recap so as you will see it's a two slice of recap and one slide of content so um we have a generic and indexed tlds so generic tlds is just the tlv of all all your life uh doesn't need to have any explanation whereas the indexes the tlv uh is the tld but you have also that one index field after the length okay um so why the indexed tlp is needed in the end it is needed for uh root monitoring messages right so we took a kind of a scenic route on saying what do we do if we have a tlv routine route monitoring message whether we want to address the whole uh you know all the nlris that we find in the in the bgp message encoded you know in the uh root monitor message or whether we want to address just one specific or uh you know an lri right so the index comes from that need so next slide and essentially so what we did and that's the only change that it's in revision number six is that like is if the index"
  },
  {
    "startTime": "00:06:02",
    "text": "is zero then it means that uh the tlv does apply to the whole uh bgp message or otherwise if it's a positive number then it applies to a specific nlri so very simple in concept um i would say this is the only change that we did and i think that was the only outstanding you know piece of work that really needed to be done um i don't know if anybody has some feedback i mean i anticipated this change already uh in the previous uh itf meeting so we had a couple of options and this kind of looked uh the most natural uh for us authors and also i received some feedback and uh it seemed you know kind of consensus to go down this way um i don't know if there is any feedback you know from anybody in the audience otherwise if not i would say for the chairs like this seems to be a kind of don't work and maybe we can last call it or something like that and move forward thank you thank you i think it um if you are uh asking for work group last call you should definitely make a comment on the mailing list so people can speak up as a standard chairs always say this thing fantastic we'll do that after this okay uh other than that thanks for the presentation and thank you thank you if no one else has put their hand up to say anything so i think definitely put your message forward please make a work real ass call for this now now now fantastic we'll do that thank you thank you very much uh edward do you want to uh run your own"
  },
  {
    "startTime": "00:08:00",
    "text": "slides or do you want me to or somebody else to run them for you uh i think i'll i'll try it on my own we'll see how it goes um okay unfortunately the the test room uh from me deco was down so let's see how this goes all right all right audio is perfect yeah let's see screen requested granted yes yes and okay it looks to me like you all should be seeing it it's just a lame title slide is that can anyone see it looks great yes okay awesome in that case let me get started so uh the context of so this is uh compared to the previous stuff there's a bit more academic work the context was a was a master's thesis in this case um thomas was so kind as to supervise the whole thing and pierre also on this call was also providing tons of help so thanks to them upfront so uh let me get started then so um the the context for this whole thing is the notion of device telemetry right and what we want is to to to understand the networks we want to combine different perspectives from from basically combined information from various angles so specifically we might want to um extract information about whether a certain flow belongs to like a specific vpn by combining a control plane and for avoiding plane information and the way we obtain that information in the first place is via like network telemetry protocols and"
  },
  {
    "startTime": "00:10:01",
    "text": "these can be various things right it can be it can be um bmp for well for for bgp type information right ipfix for for flow information or like yang push for for device level stuff so to collect that information at all we need to um basically make sure that it gets routed to the to the right place from across the network so ideally we just define a single um globally uh globally unique uh end point and just rather anything there luckily that also allows us to use anycast for the original for the originalization path part we can just ecmp balance across there and um finally once once the traffic actually actually hits our machine vm whatever this is where things become interesting and we actually need to balance among different processes and the challenge here is primarily from a administrative and like local performance perspective right because getting it to work somehow is easy enough it's more like in terms of making it work efficiently so um in the context of this work you have to imagine we have like a network with routers pushing information to like some sort of a host the the main interesting part here is that to combine the information to combine the different perspectives on that target host we need to make sure that all these different perspectives from the same device arrive at the at the same end point right because otherwise we just don't have the data streams to combine them early we could eventually i mean we could of course like globally tag everything and then throw everything in like some data lake type setup and then just do matching there but that is uh first of all it is it is expensive it is late in"
  },
  {
    "startTime": "00:12:01",
    "text": "the process and um uh prone to failure at that level so for this for this case we can just imagine the setup where we have these routers and they just push bmp and ipfix information the collector that that we used here was pmacts and fxd thanks to polo for for creating these so and later on you can imagine this is then used use them for some sort of like olap querying where you want to like find out information about your network but that's out of scope here so if you look at the if as a baseline we can just simply look at something really simple where we combine that data by introducing a layer of indirection right we have we have our we have like n telemetry daemons that are collecting all that information but to make sure that the right streams are ending at the right demons we put some proxies in front of them that where all they do is uh basically forward the traffic and in this case we can just use something like h a proxy for the tcp part and nfd itself can actually act as a udp proxima we can use that very pfix right like a relatively simple setup the main uh challenges that are the main challenges in this type of setup is the um is the [Music] the mapping right because we need to in fact make sure that stuff ends up at the at the right target the the file in the the little box in the middle in this case um the challenges here are relatively straightforward i think it's the reliability of having additional components that may fail at any time right they they suffer for update cycles they suffer for process lifetime cycles all of that stuff there is overhead right you have proxies that you don't strictly need in that sense you have a latency impact which"
  },
  {
    "startTime": "00:14:00",
    "text": "you probably don't care about so much but in in general just limits to what we can do here and the main administrative challenge is the is the configuration overhead because we need to con it's basically these files right we need to configure two different proxy with different configuration syntaxes to support like all of these devices sending across multiple telemetry protocols my example only two but can be any of them right to then find eventually any um to correctly balance it across the different backends and this is what we want to solve so what what can we do so there is a socket option just as a refresher there's a socket option called as a reuse part and that allows us to basically statelessly load balance among a bunch of different processes that's super convenient but unfortunately the problem with that approach is that by default it takes or in its standard configuration it takes the hash of the whole flow which means that different telemetry protocols will most likely end up at different demons telemetry collection daemons so you're breaking the link which is the whole which is the whole purpose of doing this right you want to correlate these two these two different data streams so luckily this is something we can influence using bpf specifically there is a program called um um it i think it's just called the reuse port that we can attach to help us influence the selection as a refresher bpf right sd is the subsystem that basically allows us to attach logic to custom predefined hooks and one of these hooks is specifically the socket selection in the so reuse port mechanism so using bpf we can basically just patch"
  },
  {
    "startTime": "00:16:02",
    "text": "the collectors minimally to register themselves on some sort of lookup table where they basically say i am collector number one out of n and i will receive one end of the traffic this is the communication there and then we can simply using bpf choose our own port selection mechanism in which case we can simply again hash the flow except that in this case we the only thing we need to look at is the ap source address under the assumption being that we only have a single a single um that all the telemetry is coming from a single interface that has like only a single ip address and the second step is that we no longer balance across all the listening demons but across the intended number which also means that we don't need to dynamically um reallocate or so try to solve the problem of reallocating uh to establish tcp connections across multiple daemons when some of them are going from some sort of like update cycle like imagine some sort of like you have 10 of them listening right and then you have like a rolling update where you restart one after the other with some new binary version and then you basically have chaos going back and forth but if you just say that we always intend to have 10 of these and we simply respond with a tcp reset or just simply drop the udp datagram if something is doesn't happen to be listening in that moment we have basically solved that problem so we just can package up this logic into a little program and attach it to the to the reuse port group and in practice we will just have the first daemon that starts up do this and we can use the existence of the vpf program as the as they basically indicator that it exists at all um using this approach we basically now have stateless balancing right we don't need any we need to don't we don't need to deal with config files because they don't exist in that sense well i mean we still need to"
  },
  {
    "startTime": "00:18:00",
    "text": "of course know how many we intend to have but that is basically it it's based on device identity only again the link here being something like that the ip address corresponds like a single ap address corresponds to a single device if that wasn't the case we could also we could also do more complicated stuff you can just parse the skb from within that program that is possible it's stable across restarts in the sense that we are not uh throwing connections back and forth you prevent a scenario of cascading failure so imagine like a packet of death scenario that exploits the this collector if you if you simply load balance across everything that is listening at this point basically every time this connection is trying to be it tries to get itself re-established or the pdp packet gets resent in the ap fix case you would basically like do a round robin of like killing these demons this way you only have like a single victim that basically stays down and doing it this way basically requires no configuration right it's just this this one piece of information about how many you're going to have so we have this so to summarize what we have what we've seen here right so we say that network telemetry aggregation meaning combining different streams we have additional requirements on load balancing that we wouldn't usually have by simply needing having the need to combine the streams from different uh across different transport protocols and transport critical ports down to the same process having to balance it this way so we can solve this problem relatively straightforwardly using ebpf without using additional daemons on the system side this is uh this is a pretty nice property right because that means no no"
  },
  {
    "startTime": "00:20:01",
    "text": "vulnerabilities at that level no upgrade cycles uh no i misread the configuration or in incompatible configuration updates um with vpf uh um compile once run everywhere functionality that loop bpf provides you can basically it's it's portable enough in that sense um we can balance stuff across an arbitrary number of end points because scaling up and down is relatively easy in this type of setup this basically allows us to do the proper networking by by always having this data correctly correlated instead of split across multiple data collectors and then having to aggregate it at a later stage and that's much easier to maintain and as a bonus actually if you try to benchmark it by doing like a ba by running a bmp load generator in terms of connections we're behaving much better i think we're saving around like 20 uh cpu time by doing it this way and the ramp up is actually feasible like if you run the load generator against like h a proxy you you i basically have to slow it down before things just crash whereas in the kernel level balancing case that is not a problem at all and that's basically already it um the slot ended up being much larger than anticipated so i guess that means we have plenty of time for questions this time and if you want to read the same stuff in the form of like theses with like huge appendix and 70 pages blah blah blah feel free to visit the link so that's it from for for the scheduled items of my talk i'm happy to answer any questions at this point"
  },
  {
    "startTime": "00:22:02",
    "text": "all right ideally if anybody has questions they'll speak up yep other edwards thank you so much for this presentation this is edward's first ietf so i think you did a fantastic job of uh sharing your insights with our community yeah this is a great presentation all right well given that there doesn't seem to be anything on the chat nor is anyone raising hands interrupting that case thanks everyone uh was also presenting and a special thanks again to uh thomas pierre and phone all right i think uh that's the end of our presentations for the meeting and unless anybody else has other things to talk about going once going twice anything you want to cover you i want to go back to uh upgrading rpk validators okay all right i think this was it thank you everybody and we'll see you at ietf ietf113 bye bye thanks"
  }
]
