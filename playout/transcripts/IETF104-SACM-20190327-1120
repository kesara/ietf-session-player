[
  {
    "startTime": "00:00:04",
    "text": "I need a volunteer for Jabbar describe describe this come Jabbar you can delegate there you go and we have a winner got it anybody else wants "
  },
  {
    "startTime": "00:03:13",
    "text": "to you know back up Stevenage jabber scribe is welcome okay good morning it is still morning okay good morning this is the sack I\u0027m working group this is the note well which you have seen which you agreed to when you signed up for the IHF in which you will have been reminded of a number of times since if you have any questions about that be sure to see Chris next time this is our agenda for the day is there any agenda bashing nope okay we have scribes and we have a minute taker so we are good to go yeah Steven you are on deck I only have one other slut all right this should be very short and very easy so I recently just pushed a new version of the Rollie software descriptor draft I think technically I actually pushed two versions like in quick succession the first one was just updating the draft because it expired and the second one was a very small number of editorial fixes there\u0027s no major content updates and then there were one or two like to do placeholder texts that I filled in as well next slide please at this point I pretty strongly believe that the document is ready for a working group last call and to use that to drive the final stages of review for the document so I\u0027d like to ask for that ask the group and the chairs on what they think about that is there anybody opposed to the working group doing a last call on this I know the chairs are strongly in favor of it okay oh yes so it would be nice if we had a couple people that will commit to reviewing this any hands for folks that are willing to review it\u0027s not super long short draft Thank You Jess one person can we get at least two people committing to a review [Laughter] "
  },
  {
    "startTime": "00:06:13",
    "text": "I was about to call out Mike Rosa over there looking passive you saved him this will teach Lou not to sit on Frankie made eye contact how\u0027s the mistake all right so we have a couple of volunteers and I think that\u0027s bended cool thank you for doing that Thanks I might post another version just because I think there was a metadata error on the Oh for and I it\u0027s like literally the smallest possible change well could you make that small as possible like today how about I could do it right now that\u0027d be fabulous okay all right I\u0027m gonna start with talking about the hackathon efforts and then I\u0027ll go into some more architecture related things as well so a couple of weeks ago when setting up the sort of project for the hackathon looking at the architecture draft and kind of looking at some of the xmpp components you know that were illustrated in the draft there I wanted to try and get a couple of those xmpp based clients together an Orchestrator an endpoint and a repository sort of examples kind of stubbing a few things out but allowing some of the traffic to go between them and transfer some information orchestrate some collection activities perhaps retrieve and and push some information into a quote-unquote repository I guess and do that so to do that it was going to start using the extensibility of XMPP and create some extensions to perform those operations between the components so again kind of want to query a repository to grab some content or at least pointers to content things like that and then instruct the endpoint to perform an assessment all I had was an Assessor and not just a decoupled collector slash evaluator so just gonna had to use the tools that I had at on hand so at the at the hackathon a little bit before the hackathon I got had some communications with Hank and [Applause] so he brought a colleague karl-heinz to the hackathon and brought an implementation of a map client and "
  },
  {
    "startTime": "00:09:13",
    "text": "server basically a pub/sub broker that can basically you can the map client can publish information to the map server which then you know sort of in that was salt sort of see boring coded information links metadata in there and and store that into an object based sort of graph graph data model so what we did at the beginning first we kind of you know had our introductions and went over what each of our pieces of software did and talked a little bit about how we could you know integrate the integrate the two components and see how they how they could work together so we decided on a couple of workflows that we wanted to that we wanted to try the first was basically in the stuff that I brought in in the XMPP clients that were there to implement that as one of the map clients as well so we could publish information to the map server and store it in the graph which we were able to do and then the more complicated example which we did on the second day was actually to kind of do the reverse of that we we took the the map client and and made that into its own little XMPP client and then we sort of orchestrated a full-on publish/subscribe workflow where an Orchestrator would publish guidance to collect a collector would would do the we sort of stubbed out a little bit of collection activities it would publish those results back where the the map clients was subscribed then to that topic grab that information and pushed it into that that graph data model so we were able to see that and retrieve all that information and get a dump of all of that and again it was all nicely Seabourn coated so it was pretty pretty tiny bits of information that were really fast yeah so I guess I just sort of covered those these two slides in in the last one there so again sort of started with making my XMPP client able to talk to the map client and then made it more more difficult to add the XMPP adapt or two to the map client so the publish/subscribe workflow would work so what we what we learned first off in a lot of the introductions that the map server and that and that graph data model is it was really actually great for the repository information so we could really store any of that collection information any of that data along with metadata along with links between all that stuff and it and it looked really promising to move forward "
  },
  {
    "startTime": "00:12:13",
    "text": "and also it kind of helped to inform that the the current architecture document really a lot of the diagrams and things like that are focused more on the transport aspect so a lot of those examples with XMPP than it does on the actual integrations and interoperability between components so we kind of had that as a nice take away from from the hackathon and then what\u0027s next well I\u0027m going to consult my notes a little bit yeah so we want to keep and kind of keep doing this and keep going with the progress that we made on the hackathons you know the idea of having interim hackathons and kind of working in a distributed way was was talked a little bit about as well and then really kind of built on those lessons learned to kind of focus on really defining the interactions between the components and using that to help refine the draft and things like that we also kind of had had a realization you know sort of at the beginning of the hackathon that you know the preparations and planning for for all of the hackathon efforts could have been better instead of just sort of showing up at the beginning of the hackathon and you know kind of talking about the you know what we might be able to do with the different pieces to kind of maybe hold a little couple of calls or meetings or whatever beforehand to have a little bit better planning for what we\u0027re gonna do going forward I\u0027m happy to take the lead on doing that as I\u0027ve been kind of coordinating at the hackathons in the past kind of missed turns for reviving the information model but kinda want to you know kind of re take a look at some of the information elements there and start to really get that down to a minimum sort of level of information and data elements that to to help facilitate the interactions the next steps is really to sort of start really creating those data models and really defining those interactions between the components and just wanted to give a big shout to to Carleton rights for his contributions to that component so Hank if you pass that on it\u0027s very much appreciated yeah I think that\u0027s pretty much it for the for the hackathon things the questions on what we do for that suite yep all right big letters second work oh it\u0027s just sort of scrolling as better all right still just scrolling local "
  },
  {
    "startTime": "00:15:17",
    "text": "scroll a little bit oh yeah yeah that\u0027s better all right all right architecture draft it was about to expire so we made some updates and published the oh one version minimally cleaned up a few spelling errors there was an overview diagram that we added a little bit of extra notations to to kind of illustrate that external data feeds can contribute to policies within an enterprise and we sort of drew the the line there to illustrate the boundary between the enterprise and those external data feeds we simplified a couple of one of the diagrams to just kind of take a little bit of the bulk out of the diagram so it was kind of mentioning all of the sort of XMPP connectors and different flavors of collection capabilities we just sort of narrowed that down and called it one collector component within a dabbu you know with the XMPP adapter in it put a lot of descriptive information about some of the XMPP extension protocols that we think were useful in in you know in this sort of data flow just a part you know kind of building on some of the things that were in the mile working group with the with the publish/subscribe we found in previous hackathons that sort of helped inform this before that other extensions would be useful you know to kind of leverage larger policy definition structures you know some of the you know oval definition files or you know XML based checklists or anything like that are kind of large data sets which have a hard time going through it the standard published subscribes you can have to work around it a little bit to to move that that bulk of information and other potential things that would be helpful in the in the workflow for onboarding new endpoint components and and things of that nature so we added a bunch of descriptive information to those to that section and then section three three added a very very large diagram that actually put in a great amount of ASCII art to show the endpoint posture collection protocol in in the actual model as one of those collectors so it\u0027s like a full-on full-page diagram looks pretty awesome again kind of intertwining that with the information that we gleaned from the hackathon showed again that we were fairly focused on the actual transport and kind of hard lining it towards XMPP "
  },
  {
    "startTime": "00:18:20",
    "text": "but found that we were very it was very beneficial to have the sort of map client in that graph data model to use as a repository and just to allow for you know interoperability between the different components and different ways to implement those components so it helped us you know with the idea that again we kind of really need to focus on the interactions instead of really how it\u0027s how it\u0027s being built so I think this is yeah so this is the last one last slide about proposed direction again we\u0027ve got a very XMPP centric view of the in the draft so we\u0027re looking for I guess input direction on if we should sort of leave those components as they are in the draft or if we should maybe move those anything specific to XMPP or specific to you know specific implementations maybe to an appendix or even you know another another draft or anything like that and then attempt to refocus really on the interactions as I\u0027ve mentioned a number of times and then start really looking towards the you know an extensible registry I guess of those information elements you know we can again want to start with something basic that we can you know start building those interactions out and then continue working towards those that future hackathons even if that\u0027s just one component to another component going forward I\u0027m happy to you know entertain any sort of suggestions and we want some feedback on the draft keep keep moving the keep moving it forward that\u0027s I guess what we need and you know any amount of constructive criticism on the list we\u0027re happy to integrate in going forward that\u0027s it has anyone read the latest version how long it was it fell a few weeks separate February 2nd was the answer has anybody read the latest version I got like two but one of those is the author we really need more reviews and more people reading the draft that would be huge can some people commit to actually reading it it\u0027s actually still not very long oh can I ask a quick question sure a Francia how uh what taste graphs data mode are so maybe Hank can help answer that a little bit better with the the data model that\u0027s used in the in the map map server oh sorry I just had to run to "
  },
  {
    "startTime": "00:21:20",
    "text": "equity not to tell them to do stuff and then I ran back here because I forgot my headphone and so this is the architecture yeah and the question is sorry I just what\u0027s the Massa gravity what\u0027s the map over in graph day tomorrow yeah okay insight inside the the the graph there is no prescription how to straw the data you can do it whatever you want on a piece of paper whatever so we only define data motion and the data model for data motion basically is a nested message protocol that does not require anything but a secure channel such as TLS and there there will be messages and the messages are either client created or server created so that\u0027s the first distinction there\u0027s always an application Association and the data model insight is basically always a container containing two identifiers or the two nodes and one link on metadata item so and all of these three items can have a multitude of attributes associated to them when being published to the graph they also have two times of lifetime one of them is infinite so even if the application Association dies the data will still remain in the graph or you just give an expiration date with it so it will be removed automatically from the drop over from the graph if you do infinite lifetimes all the time you need something like a garbage collector at some point otherwise we explode yeah yeah I guess it\u0027s a good way to define like really the the nodes and the relationships between them using just using metadata so then it\u0027s a little bit it\u0027s a interesting way to than query based on those relationships thank you I think yet it\u0027s also my question and I think I assume that there are some benefit uses model for what what kind of information you need to collect right so you use this kind of emission data it can bring out you a lot of benefit to show their relation so I need to review the document yeah thank you and and I think any you know the contribution comments about you know certain information that\u0027s helpful to you know create some of those relationships where we\u0027ll take so this is saying again the data model is very contact agnostic so our first example was minions and bananas and that they like them so they are metaphors I thought I was like so it is that you can really create whatever you want to star in there and I think that\u0027s very important to highlight also it is the place or to get a bigger picture of you for everything that\u0027s happening or supposed to happen and in the state the processes I in so it\u0027s originally produced for syndication state so which devices of authenticated "
  },
  {
    "startTime": "00:24:20",
    "text": "and which devices to go into the network why and and there\u0027s a very specific purpose and we we just crushed that and and did all they\u0027re not photo to put all the nice pieces put in receiver and CDR and then works quite well so yeah people ideas criticism will take it off so um Adam asked a little while ago if there was anybody committing to review and we didn\u0027t really get adequate response so you\u0027re committed to River who was the other person that was committed to review this one I will move up multiple distinct again I think after seeing the output of the hackathon I think we have a better understanding how AB spected operation we work for the architecture and give them the time of the next four to eight weeks you might even be able to give you a contributions for the relative reviewing it because now we will really understand what we are doing actually have other coding yeah okay thank you does anybody I guess the question does anybody have an opinion yay or nay about moving some of the XMPP specific things around or can they be left where they are move to an appendix taken out completely guess we can move that on the unlist as well about what to do with the XMPP specific components they should be put in the appendix left where they are take no ask that on the list I think that was it thanks Bill huh Hank you want to do terminology I don\u0027t know if there\u0027s updates I think again the terminology draft has not progressed as was highlighted at the last ITF when we will come at a point "
  },
  {
    "startTime": "00:27:23",
    "text": "where we were able to expect the XMPP grid out of the architecture we come back to revisiting some of the concept I was reading the complete terminology two days ago and it\u0027s still quite applicable surprisingly what we do not have yet considered and which was always out sourced into the terminology document where does not belong is the target endpoint characteristic profile so what is that that\u0027s a very long term first of all it\u0027s complicated and it doesn\u0027t mean the whole description is in the terminology so what does it do if you encounter a discernible endpoint target endpoint you want to assess in your network and you don\u0027t know anything about it you learn about it with different specific collectors for example a profiler or a I don\u0027t know intrusion detection system or something like that and then you get superficial information but that\u0027s the best but you can get so you aggregate these information somewhere and you also have to label this information that you can find it again so that you can we recognize this device that you actually do not really know again when it reappears so this is of course a best-effort scenario if people are smart enough and this is an attack they will change the identifying information that means about the target endpoint so it cannot be that there won\u0027t be a consistent profile about it but the concept exists it somehow is now I don\u0027t know uploaded to the terminology I hope there will be textual considerations that will enable to prove that in back into it for example the architecture or some solution but otherwise at some point we will gap on this concept and the continuous monitoring of things we do not know will not be a thing so that\u0027s that\u0027s just my my consideration here so I hope we will pick up on this again so and let me reiterate the terminology draft will proceed alongside the progress of the architecture that\u0027s the only way we can do it at the moment otherwise there\u0027s no otherwise we would be the leading document to define how the architecture works I don\u0027t think that\u0027s the way it should be and the other thing is that there are still this one profile characterization profile things stored in there that should not be in there should go into a yeah well higher level draft I think and that\u0027s my report and next time ever request him anology time if there is something to report "
  },
  {
    "startTime": "00:30:23",
    "text": "you could just say there\u0027s nothing to report on the way here the one thought came up [Laughter] oh wait wait oh no you\u0027re good so this is a atf sacrum KO suite so we haven\u0027t made any changes since the last posted data tracker version but we\u0027re in the process of making them so a number of changes were made this week on the draft so we\u0027ve based on some of Chris\u0027s feedback we worked to reduce some of the representational complexity of the the media type one of the challenges we\u0027re facing is it\u0027s a little under specified from from an Isis with tag perspective I the ISIS wit tag basically just points to the w3c specification for media type we were attempting to try to correct that in the prior versions of coast web but we just didn\u0027t have a lot to go on so instead I think what we and what we\u0027re gonna end up doing is just treating it as a text field and then again pointing to the w3c specification on the use of media type so what we\u0027ll have parody essentially with the ISO spec we\u0027ve added we\u0027ve enhanced the ability to include signature schemes and cozies so that now you can have multiple signatures you can do co-signing and a few other capabilities so um that\u0027s in the editors copy right now we\u0027ve we were looking through the normative language of the draft and and noticed that there wasn\u0027t really any sort of top-level normative language for the model so we added a must statement that basically requires the minimal set of required attributes to make sure that you know since it\u0027s the standards track document that there\u0027s some top-level requirement to drive everything within the CD VL we reordered the attribute names and sort of refactored how the integer labels are being declared we believe that the new version is much more readable than the previous version and and and so we\u0027re currently in the process of actually merging that into into the in line i\u0027m "
  },
  {
    "startTime": "00:33:26",
    "text": "CBL that\u0027s in the documents so so we have to finish populating those CD VL changes in to enter the internet draft and there\u0027s a few other small bits of work that that remain one thing that we talked about I think last time that still remains is the creation of a Miami attribute registry to support us with extension so we want to take the the current attributes that we have the 50 or so put them in a registry and then have a registration process to allow updates to the model to be you know edit over over time we\u0027ll just make enhancements to the coast whit model I\u0027m easier to do I\u0027m in a much more controlled way it will allow for some expert review of those changes as well there\u0027s a few references that that Chris could probably use a minor bit of editorial changes to to sort of frame the references a little bit more they\u0027re just kind of disembodied at the moment right now so we need to do that and then there any clear F units fi that work in a in their address so I\u0027m the plan is to person or not a no.9 version by the end of the week and if we do so we\u0027d like to recommend that that the documents ready for working group less call as as far as as we feel so would would it be possible to forward so you\u0027re gonna post by the end of this week yeah yeah the remaining work is really not that much work a couple hours work so we feel like it\u0027s what we have is pretty much ready to we\u0027ll be pretty much ready to go all right how many people have read this document so far okay okay is there anybody objecting to going to working group last call so I I will remind folks that when we get to working group last call we need to show that we need people to actually state publicly that they\u0027ve read and reviewed this document to show some consensus for it and we can\u0027t show consensus if it\u0027s like the two authors and one other person but that\u0027s not that\u0027s not a very big consensus so so thank you Thanks yeah you\u0027re complexed Dave freely I have a quick question maybe Hank you know which is - I mean if you put a yang model in a document right there\u0027s you know automated review and it comes through in the data tracker well is "
  },
  {
    "startTime": "00:36:28",
    "text": "there anything even a little bit close to that like a CD yet CD DL in a document sorry any is there any validation that it\u0027s there like a CD VL doctors kind of sending in front of you it\u0027s it\u0027s all manual no there\u0027s a tool chain to create and validate everything you create with it I meant that it\u0027s not in kind of the data tracking you know infrastructure that it\u0027ll automatically pull out cvbl the way it will no hole out yang no no CDA is just an RFC editor cluster I think before you can have a true chain that put something out and says hey this is a normative sum that you should do it right it has to be at least a proposed on that and it is not one yet it will be in a few days fill in last call right all right as iesg no it\u0027s an RFC editor it\u0027s in cluster yeah the iesg right up there does require that the shepherd makes sure that any schemas validate and yeah those kinds of things so guess it would be covered under under that aspect okay and if you need help with the tooling to do that it could certainly help you it was an Emmy I don\u0027t know if I was going to do it I would like it to be done I\u0027d like to have some assurance that the you know if we\u0027re publishing a you know I\u0027ve read the schema but that doesn\u0027t necessarily mean that office world that\u0027s you know compiler of CD DLC so yes okay I have at least one now I live to other individuals that continuously read this and the disc arson Borman in close hotkey so I think we have at least two people that have an idea that this is a consistency DD and also the tool tells you so so yeah but there is not a no no no such thing as a CD DL doctor yet and I think it would be overkill the fact alone that you need yang doctors means something so I hope we do not need CDA doctors because that would means something else yeah I don\u0027t go off on a tangent too much but what it\u0027s worth we\u0027ve been putting xml schema and RFC\u0027s for ages and there\u0027s still no automated XML schema system and data tracker so I don\u0027t think it\u0027s ever gonna be anything like that for CDL either he probably yeah I wasn\u0027t necessarily hoping that it got to data tracker I was just hoping for you some amount of verification that what\u0027s in there is valid all right thanks "
  },
  {
    "startTime": "00:39:42",
    "text": "damn tall people all right my pasta collection so I\u0027ll tell you about some changes we\u0027ve had since the O for draft and go through a couple of next steps the changes from the overdraft are based on comments received on list and on the virtual interim so what do we do we clarified the scope of the ECP the original scope was sort of broad discussing not only how you collect endpoint posture data but also how you know how you continually assess that that posture information how you\u0027d share it with other tools things on the network there aren\u0027t standards for a lot of those things though so is making the drafts in there kind of incomplete so reduce the scope to focus really just on the how you get the posture information from the endpoint how to communicate that information to a centralized server we still maintain the other components that we had originally discussed in the before draft but it\u0027s now all very clearly identified as future work you know here are things you could do with this collected information but we don\u0027t really define what you ought to do we received feedback that the draft was long we agreed we try to improve the readability of it mostly because videos for transitioning this to a best practices document the readers really care about the technical details not all just goes like great ideas about what you ought to do with the data so we tried to change the drop to make it read more like a BCP so to do that we consolidate a lot of the introduction information remove a lot of the rationale text to an appendix we explain the structure of the draft that goes from you know sort of the architectural to implementation to best practices to extensions it moved all that into the into the intro so readers knew what they were looking at a lot of our examples particularly pertain to yeah we moved appendix and we move supported announcement of these cases to the appendix yeah we did a lot of like fixing knits addressing Oh a just comas new in the privacy information may not be owned by the enterprise when we originally wrote this it was sort of with the intention that the enterprise owns the data but that\u0027s not always true so we updated that removed a lot of duplicative text and yeah fixed some of the diagrams we all dated the API to an architectural component but I did see your comments that that was confusing we will work to make that seem better and we\u0027ll adjust your other comments as well I think I feel like a lot of them seem to the substitute one centered around it understanding whether or not the posture is pushed from the endpoint or queried it\u0027s both right with yeah that wasn\u0027t super clear so we will we will fix up "
  },
  {
    "startTime": "00:42:44",
    "text": "next steps Jessica\u0027s losing her will to live on this document so presumably Danny has to but Danny\u0027s so calm I don\u0027t know I may be he\u0027s fine but like I need this to be over so Adam bill had said they will help with some architectural alignment we integrate Chris\u0027s feedback well published no five and then like I don\u0027t know you know someone reviews it or we just accept that it\u0027s dead and Jessica can stop this now I don\u0027t know tired really tired thank you for asking that is the best plea for review I have ever heard like a few people cannot review the document after that like we\u0027re all done so volunteers please thank you let\u0027s see it\u0027s alright people who have agreed to yeah it\u0027s okay that\u0027s fine it was you Steven it was days even thing it\u0027s people who cared about Jess thank you thank you yes you can okay yeah I\u0027ll try to we\u0027ll try to get a read I can\u0027t promise this week but I\u0027ll try to get one now in the next couple of weeks that good all right Adam volunteered on actually so having actually read the document yeah so it actually has a bunch of interesting comments with hey Bill it has a bunch of interesting comments with regard to actually architecture in there right so it talks about how to integrate kind of what we\u0027re working on on sockem from an XMPP kind of point of view although that\u0027s the minor part it really talks about TCG but it also then talks about net cough and yang I would point out that talks "
  },
  {
    "startTime": "00:45:44",
    "text": "about Nia that\u0027s fine fair enough it does your you are correct and it talks about how to maybe not with separate comment from my review maybe not coherent we talks about the back and between if you had net Kampf running push versus the TCR the NIA components running right it doesn\u0027t it says they should just go into the same data store but doesn\u0027t really you know no profound thought past that missus so yeah that\u0027s why we sort of move the repetitive future because there isn\u0027t really a standard on how to necessarily do that absolutely should ought to do that but absolutely fair comment but so to the point of kind of review and other folks reading the document it actually states some very interesting things about what ii would look like in future parts of sockem which you may or may not agree with but you should absolutely read in order to kind of get their their vision of how these things go together and so some amount of me being at the mic as a contributor here cristinaw CEO is does it has anybody looked that far into the document and have any other thoughts because i find that part particularly interesting thanks yeah we accept negative comments to if it\u0027s not someone else\u0027s vision yeah hi there Eric Voigt just as an extra in focus you\u0027ve mentioned MIT conf and push right now the is G it\u0027s not just net compton yang push at the is G it\u0027s also HTTP push and we\u0027ve been talking about having a concise see more type of push as well so if you\u0027re looking for other pushing mechanisms don\u0027t think it\u0027s limited to just the net comp transport um I I think that would be valuable if we could point out there other options it would be wonderful if someone who knew about them could yes thank you capture that though just Eric volunteers this is Stephen banger at the mic I have linked the most filler neutral comment about the name of the document as ECP but the title of it is EPC P yeah yeah yeah yeah it\u0027s a great afternoon only made by the government oh yeah so can I change the name like the URL of the document at this point does anyone care like you know I mean it\u0027s obviously not a big deal but I mean I can tell you that from just trying to find the pull up the slides in the document everything I was sitting there for a couple minutes like is this the same document is this the same thing what is this ECP when the "
  },
  {
    "startTime": "00:48:45",
    "text": "title is EPC P stuff like that so a man with answers is behind you Venkanna yeah you can still change it you would have to resubmit with a new name as working group zero zero of whatever appropriate form and you can mark it as replacing the previous one so that\u0027ll be like obvious somehow and I do that okay Dave volts where I just wanted to point out the obvious if it gets published as an RFC then the URL doesn\u0027t matter anymore sure I mean but if we\u0027re resubmitting it then we could like yeah not upset Steven but yeah hey second thank you they weren\u0027t ordered okay I guess we actually should have agenda bashed this but we didn\u0027t so I\u0027m bashing late I guess yeah we talked about it so this is something I put together as a contributor so it talks a little bit about the information model and Karen promised not to bludgeon me for doing this but that was only actually because she said she was too tired I know that\u0027s why you\u0027re a little bit further that way all right so just some thoughts on the kind of the previous work we did on information model right so it was very ambitious there\u0027s four hundred and seventeen elements in the - ten which was the last revision of that document it felt like you know kind of further refining that was a bit of an effort probably because there was actually 417 data elements or information elements in there I personally certainly felt like it was really hard to understand what was important in that because we defined everything it felt like I thought became super hard to make any kind of trade-offs to think about data models you know what would you actually do for an actual implementation because I really personally felt like there\u0027s a lot of if you\u0027re gonna make progress have to boil the ocean there so you know kind of the talk that you things that came up with the hackathon what if we "
  },
  {
    "startTime": "00:51:45",
    "text": "approached it from a can we do the Minimum Viable set of things in order to make some of this stuff work into you just kind of you know 15 minutes jotting down some thoughts of what\u0027s the minimal minimum viable set you know there\u0027s some super obvious ones right IP address host names we absolutely know we need that we need to be able to handle kind of date time so the sample time and event time some real basic things of kind of Coast wits with firmware revisions you know to the point of kind of saying hey you know if I you leverage what\u0027s in the CPC ee and what end point posture collection profile I\u0027m just gonna actually say the name because it\u0027s actually easier than remembering the acronym you know we would we would need to know what those kind of core what are those main points of what are in the kind of yang profiles that we really find as being the most important and then we need a handful of basics on what do people really use in practical terms for identifying an endpoint right so what are their what it\u0027s what\u0027s the naming anchor that\u0027s kind of currently used right so I think that\u0027s the kind of really basic minimal set and I\u0027m hoping the hackathon you know kind of bill and Carl Heine\u0027s you know as they glue some of these disparate pieces together I think there\u0027s huge huge knowledge that\u0027s gained from that and understanding like no we we only actually figured out how to exchange as kind of key fields as these three fields because that went from what we have as a back-end to what you have is a back-end and it\u0027s not clear to me that we need that much more you know so that leads me to the kind of thought of what are the database keys that people use in kind of current systems for this and do we need to standardize more than that to start to start building something that would allow us to exchange data this is Hank as a rodent enthusiast speaking there might be information events that can tell you about the trustworthiness of the identifying attributes and those would be super cool and and maybe that is something that is not the viable minimum but again viable to pursue ok so just as a note all right so thank you for being the you know my plant in the audience for the next slide Steve is Steven I\u0027m channeling at a "
  },
  {
    "startTime": "00:54:48",
    "text": "Montville / Jabbar so is another way to say this is to look at the workflows were interested in tackling and extracting the minimum set of things we want to see from those if so I completely agree we are making good progress on exchange mechanisms now we just need to do what you said yes so I would say I\u0027m 90% in agreement with Adams risa memorization only in that I don\u0027t know that I didn\u0027t want to capture every detail of information we want to exchange and if we have to exchange information that is maybe more complete in one of those information exchanges between systems that I just need enough to be able to know what to ask for from one side to get to the other and an ability to express how I\u0027m sending the rest of the data so that both sides can agree that they can decode the larger part okay and Adam just follows up with to me trustworthiness can come after we have the minimum viable things so so that goes this slide so you know being a lazy engineer which I\u0027m totally in that bucket you have to respond to Adam which who can not see I\u0027m waving it right now if you start with the minimum of item and then at trustworthiness realize you have to refactor the whole thing though this was the experience from tool time is reiterating this in different domains so maybe just keeping that at the back of your head by assessing the minimum viable will save a lot of time okay Nancy Kim wins it but also speaking as one of the chairs of the so-called rodent group I agree that okay so if we start speaking about the trustworthiness on the attestations then we\u0027re now building some hospital domaine because the level of trust and the attestations is what the rats group is doing now speaking as an individual I agree that we can take on that part after we define this so my approach to being a I\u0027m with you my approach to being a lazy engineer is do that minimum viable part and then make extensions work and so but this has implications to how you do data models and what kind of data types you would exchange right because I think what this implies are you know this approach implies that there\u0027s an ability to exchange meta information about types right so it means I really want to be able to express another data type as part of "
  },
  {
    "startTime": "00:57:48",
    "text": "however the data exchanges work and give you the meta information about what we\u0027re going to exchange so the minimum viable would potentially be the absolutely minimum mandatory to implement and then also you must implement methods to understand these extension capabilities right so as we did you know trustworthiness any other forms of attestation as we decide that you know we should tackle you know satellite you know health and welfare and somebody needs to figure out you know what it means to be within the VNL and belt versus without the availability some vendor or somebody else can do that right it doesn\u0027t need to happen in the working group to make that work but we need to have enough of these meta elements right so kind of this and I don\u0027t know what this set is this is just me noodling right so named basic data type byte length those things that are necessary right we need to know is this a standard element or is this a vendor specified element we eventually move this into a an a registry so as a bunch of vendors say yeah hey I changed that same piece of information we can eventually agree on what that is and promote it but so you know just kind of historical knowledge of you know what we\u0027ve added in the past here somewhere here yeah I have structures slash composite in there I\u0027m more nervous about that as a thing but you know I think that there\u0027s some set of these things if we could define them it would allow us to do that and we can keep that set that we need to focus on here and now really short all right so thoughts is this is this a workable way forward for the folks in the room can you see how this would or would not work for you you know could you build a repository that\u0027s smart enough to deal with this such that if I you know effectively you know how painful would it be potentially to have a repository that might have significant hunks of blobs in it as a possibility right so if it\u0027s a data type you don\u0027t understand you don\u0027t necessarily throw it away but maybe you just kind of store it in you know an unexploded you know you don\u0027t really know what\u0027s in there but it\u0027s there ideally it wouldn\u0027t be a new data format I\u0027m not really looking to like reinvent a data model from scratch so if somebody knows something that does this where I don\u0027t have to have a predefined schema and a bunch of other things you know I\u0027d love to know what that is this I think gives a well-defined way to make the set of translators from the different data formats and the different encodings that people need that enables the kind of interoperability we\u0027ve talked about so thoughts "
  },
  {
    "startTime": "01:00:54",
    "text": "this is Hank again question for ya can it be dumped enough to store it yes you were talking about blobs did they opaque right exactly so basically you don\u0027t have to be smart you just have to just have to be very careful that and yes there is a actually the the software we brought has a feature that that alidium eliminates metadata in identifiers checking just stores it into the map and if you are doing something crazy there like like like like gigabytes of data if we brett break so so maybe you can have some minor filters with the size restrictions or a possibly can be smart and record or types it has in there and especially if it does not know the type you can still use it so this is this is a developer mode we call it because if you won\u0027t have an in production probably that\u0027s not the best idea ever but if your start experimenting when using it and using your own data model extending it and sometimes just just the MIT data and the graph really doesn\u0027t know what the semantics are you can still allow it to the basically categorize it as and this is a typing your thing is an identifier or as a metadata and then it\u0027s in there so we have this today and I think that\u0027s that\u0027s a way to go forward with the development and with vendor specific extensions and enable lazy engineers to just start on and just throw something in there but I don\u0027t think it\u0027s a head-fake as a standard way to use it so maybe there are different opinions about this when people like to see a draft on this this fleshed out a bit more of you know what a you know a minimal information model and what that would look like oh the question let me replace what I get us understood the second needed information water yes does it has to be the same as we had no maybe giving more structured information like how to nest things and how to yeah name them and how to extend it but you have full and having a minimal set of then as basically working as an example null so as a viable minimum that would be a good middle ground for an information model and I would say yes that is the answer yes that\u0027s longest yes I\u0027ve ever heard Adam Montville voice is his support for a draft on Jabbar how "
  },
  {
    "startTime": "01:03:55",
    "text": "about the does anybody not well does anybody not want to see something like this I mean obviously take this to the list but you\u0027re looking for the rooms response to well I guess I would ask I mean if you if somebody wrote this draft I mean we seem to be struggling to get reviewers are there people out there that are actually willing to read this draft should it be published oh wow just a little bit more enthusiasm and Adam says Fator says yes okay um oh yeah that was worthwhile I don\u0027t get bludgeons later well I write I mean I think part of this is we\u0027re seeing the positive impacts of doing the hackathon yes and I think that that\u0027s a good thing I just you know we we\u0027ve spun a lot of cycles on this topic already and I don\u0027t want to write like another 60 pages to like put it on a shelf that that would be awful I think a year or two I referred to this as the pit of despair but that thing\u0027s kind of appropriate actually alright so this is the part of the agenda where we talk about next steps I think we\u0027ve identified two documents that will be close to going to working group last call by the in the very short future as soon as we get a couple updates we really need reviews to those we have identified further work for the architecture draft and a new information model draft so I am we could do a quick oh we did so let\u0027s let\u0027s just do a quick exercise and assigning some dates to some of this stuff let me find my and uh all right so the Roley we should be working group last call the end of the week now right oh you did publish the update okay excellent so so assuming we get reasonable response calendar we could say so "
  },
  {
    "startTime": "01:07:04",
    "text": "April for working group last call Dave altameyer um may be proposed like three weeks because that\u0027ll give people time to decompress after after the idea yeah I usually if I do working your blast calls close to an ITF I make them at least three weeks and sometimes I\u0027ve had to extend them so so yeah so at least three weeks for that one so if we go to working group last call April then we should look at submitting to the iesg before the next IETF meeting right it should be no reason why we can\u0027t do that okay coast wid was also ready for working your blast call Dave again so if we can publish an updated draft by the end of the week then the same timeline but why okay so so it\u0027s reasonable to think we could working group last call April submit iesg by the next IETF okay huh the draft formerly known as ECP I I had a quick comment the previous two drafts really quick the Rolly software descriptor draft references coast wid so it would actually be really good if they go through together okay cuz then we can resolve that linkage as easily as also for the document formerly known as CCP um I can have another drop-down maybe in three weeks four weeks trying to build on a buffer so I don\u0027t be annoying but longer is better right okay so we we have a you know like first of May for a new draft on that so Jessica do you think that\u0027s likely to be the last revision or so um I would strongly encourage people than to read that draft now if they have any last comments to send them to Jessica she has warned us she\u0027s not going to keep reviewing this revising this document so alright so it\u0027s a CP architecture yeah this is built yeah another revision around that same time "
  },
  {
    "startTime": "01:10:05",
    "text": "line three or four weeks you know in in conjunction with some of the work um some of those information model elements that Chris took them out that would that\u0027ll help us move that move that forward together as well okay all right I\u0027ll I\u0027ll write a informational draft but I would before kind of really putting it out there I\u0027d like to actually send it to to Hank and Adam and you Bill and some others just to kind of say you know so the folks who kind of more hackathon frankly participation to say that like did I get this really really wrong and then just kind of natural look does that sound reasonable to okay so that\u0027s everything okay so based on that it seems to me that it makes sense to have a virtual interim meeting mid-may so just looking just I mean we will do a poll and all of that later but just to get some ideas like if we do it like the week of May 6th do we think the work in advance will be done any known major conflicts for that week or the week of May 20th that\u0027s the other one I was looking at nope make 20s better okay all right well we\u0027ll do a quick poll on the mailing list to see when people can participate but that was the rough timeframe and nothing from the jab room all right with that is there any other any other business open mic questions comments with that we adjourn the Sacko meeting and give you back a little extra time in your day "
  }
]