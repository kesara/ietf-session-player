[
  {
    "startTime": "00:00:11",
    "text": "okay well during the five minutes when we\u0027re waiting for the connector past the blue sheets and maybe I can ask you to come forward more because this room is very hard to hear in and I want to make sure that although we will try to be really close to the mics that you can actually hear us and see our slides so feel free to come further down we do have a riveting program so I know you\u0027re not going to want to escape out the back door the forces of research have Victo are victorious today it\u0027s actually hard to have the mic close enough the way all the other things are cluttered on here but I will try so if you can\u0027t hear me signal quickly because I I might not actually be able to keep the discipline to use the mic as close as it needs to be this the sound here is difficult okay so welcome to the IETF 99s IRT F open we were told last night this is really IETF 99 and that we lost track of one or we went one ahead so there\u0027s a showing of the note well that\u0027s been adapted for the IRT F because we actually do follow by consensus we follow the IETF s-- IPR policies unless you\u0027ve seen in all the other meeting so so it\u0027s not theirs nothing surprising here in addition I wanted to add since we have the ability to add to process more readily than the ITF does that we follow the anti-harassment policies and this is just a pointer to them since there wasn\u0027t actually a mention of that last night in the plenary either that we have them and that there\u0027s an Ombuds I actually happen to also be one of the Ombuds team so as you have a concern about something in our Gees there\u0027s two other people on that team so you can certainly consult Linda Klee forth or Pete if you need if you have a concern and we\u0027re we\u0027re especially interested in just having everybody have a voice and speak up it turns out we have one so thank you so much I apologize for your run somebody kindly ran ran for another connector okay and next is that there is going to be a "
  },
  {
    "startTime": "00:03:15",
    "text": "good bit of photography during the set this session because of the prizes and if you have a concern to not be photographed we have a policy that you can inform the photographer please don\u0027t include me in a photograph so that\u0027s one of the things that I wanted to also mention here okay so that\u0027s all the policy and I\u0027m going to give a little bit more extended update because with the long plat panel and so on I volunteered not to give an update during the plenary last night in case you\u0027re wondering it\u0027s not that we don\u0027t exist or that we were dissolute it\u0027s that we actually tried to give some time back and there we can have a little open mic session if you have some questions about the IRT F as well before we start the prizes and then at the end one of the discussion topics that the current idea is the idea of having some sort of lightning talk session that would be like the kinds of lightning talks or or these some numbers of other names that that take place at research meetings and it would be actually IETF and I RTF lightning talks mixed up together possibly in an evening if we don\u0027t get bits and bytes back so be thinking about that you can always comment on that in the ITF discuss list but we are interested in your opinion on that topic I\u0027ll introduce the a and RP speakers when we get to them but we have very good speakers this is the last of the 2017 awardees this time okay so in overview as you as you know because you\u0027re here the IRT every focuses on the kinds of tasks that are not standards engineering basically but that seem important to the IETF and to the Internet community to be tackled by people we tend to work on Applied Research we wouldn\u0027t we\u0027re not so likely to have a theoretical program here although I suppose we could we\u0027re organized into para a parallel to the working groups called research groups as you know and there is an internet research steering group which is all of the research group chairs and some at-large members and I\u0027ll introduce them but that\u0027s the basic picture of the IRT F and it\u0027s been around as long as the IETF has been around with different organizational relationships our current relationship is a close tie to the IAB actually so I thought people might be curious about some of the differences because many of you go to our G\u0027s but may not have really thought about the way in our G is organized differently the most important thing is that where we have freedom to be very creative and how the work is done so the goal is to "
  },
  {
    "startTime": "00:06:15",
    "text": "have extremely impactful work potentially work that will eventually solve a hard problem that we\u0027d like a standard for or solve a hard problem that standards folks need help with but there\u0027s no requirement to have a strict process so we can the groups can meet they happen to meet at ITF quite a lot but they can have meetings co-located with other other organizations they can have the lengths of meetings they need we have had in the past had closed research groups we have none now I would have to think really hard before we would charter a closed research group but soon and since all the research groups are open the one caveat about meeting anywhere wherever is that there help to the same process as the ITF of announcing where those meetings will be with enough time for people to participate if they want to similarly the output of of working groups is RFC\u0027s but the group where the working groups does not a research group does not have to be RFC\u0027s it could just be code it could be a hackathon it could be a series of publications in a journal it could be an agenda that\u0027s used for for another body you know for a scholarly body it could be a project of other sorts and all of that is cool so if you come to a research group meeting and they look like they\u0027re doing kind of IETF process they don\u0027t have to be and feel free to suggest if you have a good idea for how to pursue their mission well some other methods where we\u0027re very cool with experimenting being creative we want to get a great mix of people and a great mix of activity and we don\u0027t need to be bound by publishing RFC\u0027s um and then definitively research groups do not produce standards track documents so that is one of the agreements that we have that we are parallel body that does not produce standards some groups aim to to do their work well enough to solve some hard problems and then transition to creating a working group most recently disruption tolerant networking has done that and this is this is certainly one of the modes that we like but we would not be publishing standards track through the publishing informational experimental or open source or whatever up to the time that there\u0027s a transition to IETF and then the other thing is that some groups do perform roles there are of service to the standards track so that one of the key examples of that is that the cipher G produces those crypto reviews that are sometimes extremely important and normatively required for documents in the ITF and that is done with with very close ad sponsorship so that is another model and if anyone has a "
  },
  {
    "startTime": "00:09:17",
    "text": "question as we\u0027re going along I\u0027m also happy to answer questions okay so it would be nice for people to get as a to get very involved we\u0027re very open we have a discussed list that\u0027s quiet and an announced list the analysis is very quiet you can subscribe to it and just get information about new groups and reviews of new charters and and the prizes the research mailing lists and all the wiki links can be found on IRT F dot org and we are restructuring that page you\u0027ll see some changes my email officially I RTF chair IR s G is available I didn\u0027t get enough time to put in the picture where the cats are not just looking at the TV they\u0027re actually typing because that\u0027s actually what we\u0027re looking for but you can start out by reading the way they are those are my cats they\u0027re they\u0027re at-large members of the IRS G in a spirit yeah oh so many out this was where the picture of the Taiping cats will be later so how do you how do research groups originate you may wonder about that too and some of them have been around a long time so their origins are lost in time perhaps I\u0027m going to start requiring research group chairs to know who all where the chair is going back to the beginning because we some groups have really different feelings and structures now but but it would be good for them to actually have a sense of their past as well as their future um but the main thing about research groups is that they originate more more freely than the ITF groups do we are very interested in making sure we don\u0027t block something new that could be important that perhaps we don\u0027t appreciate well not by we I mean me and the IRS G but it also you so so it turns out that you can propose a group and with some tweaking to make sure that it has a sensible charter and that the that it has a mission and it has some you know vision of what it should be doing it can run for three meetings in a status of proposed research group before being considered for for a more long-term gig and we have two of those right now so the and the way that we do the evaluation is they get to three meetings and then the chair has a kind of review with the with with the chairs of those groups to talk about how things have gone but also we will start to have some more some more requests for a review by the community and by the IRS J there so here\u0027s the celebri of research groups and I\u0027m happy "
  },
  {
    "startTime": "00:12:17",
    "text": "to tell you that last meeting everybody met this meeting everybody but the decentralized internet group met and they met informally so I they\u0027re actually not yet counting as having had one of their meetings but we won\u0027t we don\u0027t do that for too long and you can see what they are they have a wide range of topics I\u0027m actually quite interested in soliciting a privacy research group and if you have an interest in that you should talk to me we still have human several of these groups still have to meet ahead of us and I don\u0027t have the schedule right in my head but if any of the chairs would like to pitch their working group at the mic for the rest of the meeting I beat you could you certainly can do that h h RPC every would you like to come up and say word their meeting Friday morning who else is still to me they apologize for not memorizing okay so pan RG maybe you\u0027d like to mention pan proposed research group maybe you\u0027d like to mention your your plans as well is that one more still to me um okay this is totally embarrassing but at the HR PC I\u0027ll be giving an analysis of treatment of law policy and politics in the history of the RFC\u0027s with a special emphasis on the first decade when there was a great deal of discussion about these issues and that and you want to say your name does Sandra Bremen yes so that should be a really great session an hour you want to say a little more about HIPC I\u0027m Avril Dori I\u0027m one of the co-chairs of the group that Sandra\u0027s going to speak at so she already said the good stuff but you know another thing though about the group is we just got one RFC out and it\u0027s going to be time to sort of look at okay we\u0027ve got some other things that are in the works but how do we continue so it\u0027s a great time to get involved in terms of how we continue okay good good Matt yeah Matt Ford just had a question about the dinner gee I was kind of looking forward to going to their meeting on Monday morning it seems like it\u0027s been quite an active group while it\u0027s been meeting informally before can you say anything about why that that meeting was canceled because I I think I heard something about the presenters were remote but that doesn\u0027t seem like a strong reason to cancel a meeting so Melinda do you want to say something about that we have one of the chairs here Linda co-chair with Kuchar yeah we actually did did have a problem with attendance by people who were doing active work we\u0027re still serving in start-up mode and we just were having problems getting enough active participation so I so so yeah we are going to be we\u0027re talking about doing an interim in conjunction with nd SS and of course we\u0027ll be meeting in London but we thought it was a better idea to not meet rather than have a bad meeting okay and um we have had very "
  },
  {
    "startTime": "00:15:23",
    "text": "good meetings of the group so far and I will put out a report with my observations about the groups on the discuss list that\u0027s something that I need to gather by their cadence for Brian will say something about the pan that proposed research group as well path aware networking we\u0027re gonna have our second meeting of the path or networking repro post research group right after lunch actually I think we might be in here I\u0027m not sure so the 30 second elevator pitch is we\u0027re looking at it\u0027s it\u0027s kind of a it\u0027s kind of a fuzzy sort of architecture sort of thing it\u0027s like what what could we do in a world in which the end points have a more active participation in this election that has that their traffic takes and this came out of an observation about a lot of sort of disconnected work that\u0027s happening in this space in the IETF sort of things like IP views excitement routing things like peas multipath transport protocols work on these how do all of these fit together and what can we do with this stuff so we\u0027re trying to figure out we\u0027re drawing a box around what it is we want to do and we\u0027re still trying to figure out about interest in the room in this community is this correct is entirety of research group is there are there other things that we should be doing in this space we\u0027re doing a partial review of sort of the whole story early in the group because we had an unfortunate conflict with the ideas both in Prague or first-time Syrians like okay for those of you who didn\u0027t come to the first meeting here\u0027s what we\u0027re trying to do and a few presentations we\u0027re sort of diving deep on what we see is research into mechanisms for realizing paths awareness in the internet so some of these are things that people know about work that\u0027s been done in the IETF already such as Alto so getting path properties down to down to endpoints and some of this is sort of blue sky stuff so please come by that\u0027ll be the other first afternoon slot Thanks thank you yes do go by to attend so I want to also say something since we have a good number of research groups I do participate in these scheduling rounds for the meeting it\u0027s difficult to completely deconflict anything at the ITF these days and we want to work on that one thing that I hope to do is move this slot into a lunchtime so that we can make sure that since we always have some a lot of interest in the in the prize when their presentations we actually have some ability to not conflict with people for that and in general I think we\u0027re just at the same mercy of of time like everybody else but the the groups cover you know span from there I don\u0027t "
  },
  {
    "startTime": "00:18:25",
    "text": "wanna call other things out too much but but there\u0027s lots going on we also have recently talked about having documents which are in two different groups because network coding and IC and RG have interesting interaction between between their work where they can support each other so we may have parallel work going on that is is really co-sponsored by two groups and we don\u0027t need to have it owned by one group we actually have the freedom to do that as well so if you have any questions about other things I\u0027m happy to entertain them okay you could see the membership so the chairs are our members of the IRS G if you go if you write to the IRS G mailing list you\u0027ll get all these people and then we have some at-large members as well who especially helped to tie us back to for example the transport area Spencer is one of our at large because transport and an IRC have a lot of relationship okay so people we\u0027re going to have the 2a and our P meetings and I thought I\u0027d make sure that we know as a group presentations what those are essentially it\u0027s a best paper prize for all possible published papers and applied networking and I like to say security too because we have an interest in security applied security topics and you\u0027ll see we have had a number lots so they are four previously published papers we\u0027re not very strict about the deadline but it should be the last couple of years and somebody numb or either nominates their own paper and themselves or a paper and the speaker and the speaker is specifically nominated in order to come and give a presentation we select six awardees for the year at the beginning before the year but we announced them two at a time and the prize winners receive some money which has gone up for next year and also a trip to speak here and we actually have the resources to offer if they request it to come back for a follow-up if they\u0027ve made lots of good ties and would like to follow up and spend more time at the ITF and I RTF and the origin story is that Loras had the stroke of genius and created this and it\u0027s a very good way for us to connect I believe that\u0027s right era and it was Lars and it is um it\u0027s a very good way for us to connect between the larger research community and the IR TF and bring people that are not always here to the to talk with us the Internet Society funds it primarily but there are some sponsors as well and you might be a sponsor if you think you could it\u0027s not a large amount but we certainly would love that you can talk to AIESEC about that um and thank you to Comcast for being a current sponsor as well as thank you to Isaac the process starts with a yearly call "
  },
  {
    "startTime": "00:21:25",
    "text": "for papers I hope you all saw it because I tried we tried to get it everywhere and if we didn\u0027t we need to do better and it completed on November 5th we actually got our largest number ever almost 60 submissions and very good ones to boot which in my estimation I\u0027m not the only reviewer obviously we have a peer reviewing committee that is from academics and from industry and you can check on the link there if you want to see more about the call for papers the original call who\u0027s on the program committee things like that and then before the end of the year we will be actually before the middle of December we will be selecting all six and then start to announce the ones for 2018 so that is how that actually happens that\u0027s where they come from that\u0027s where Roland and Paul came from from last year\u0027s group so there\u0027s also the confusion because we have to A\u0026R things and people sometimes say well did you mean the W or the P what is the difference between those so it seemed reasonable to deconflict that for you today they both have an annual call a and RP calls for nominees a and our W calls for papers and our W gives a prize for an already published paper I sorry piece yeah I\u0027m doing it myself wrong and ANR W gathers new papers new submissions air workshop submissions the prize there\u0027s two presentations at each ITF the workshop does presentations at a workshop co-located with the summer ITF we\u0027ve chosen the program chairs for the a and RW there\u0027s a steering committee for that including Lars and and Colin Perkins and myself and Sharon Goldberg who many of you know and they\u0027ve Chavez who you may not know are the the co-chairs of the PC and there will be more information soon about the rest of that okay so hopefully you\u0027re no longer confused and you might want to follow us at at in Retta fo which is our twitter handle and we also have a facebook page and you will notice that roland is in this picture because he has been an awardee before but it was a great sort of triumphant picture to attract people\u0027s attention and I tribute to this picture the large increase in the submissions so thank you to the people in that picture and thank you too Olaf for taking such a good picture okay so if there are any other questions we\u0027re in a quick open mic for IRT ephra or RG questions hi Aaron Falk so some of you may recognize me as the host of the Pecha Kucha which is actually tonight and I but I\u0027m here because there\u0027s an "
  },
  {
    "startTime": "00:24:27",
    "text": "idea under discussion to take the the lightning talk idea that has Pecha Kucha has been kind of a fun thing and trying to actually make it available for people to present new ideas so more of a serious lightening talk session so I\u0027m sort of at the center of a small group that\u0027s doing some brainstorming on this so just to be clear we\u0027re talking about doing a lightning talk session at the IETF sometime during the week that\u0027s not going to conflict with working groups or research groups and if you have ideas as or opinions as to whether you think that\u0027s a good idea or not or if it\u0027s something that you\u0027ve got some ideas on people who might be interested in participating part of the goal of this is to do things like advertise barb offs advertise interesting research that\u0027s going on but you know this is sort of like a zero to two slide slot so it\u0027s intended to be very fast the kind of thing that they\u0027re doing in dispatch but to a broader more general audience so I would love to see researchers come and throw up a couple of slides on stuff that they think that they\u0027re doing and so I think this is kind of a natural fit to the a and our PA and our W discussion and hopefully some stuff that will come out of this will lead to new research groups thank you yeah and so um Aaron and Aliyah are two of the key players there and the IR s G will take an interest in this as well so so definitely we\u0027re going we\u0027re supporting this idea we think it\u0027s a good one okay so I\u0027m going to introduce our first an RP Prize winner although not I\u0027ll introduce you briefly because you introduce yourself and your slides as well but Paul Emmerich is from the Technische universität münchen in Germany and he\u0027s going to present moongeun which is a really interesting high speed packet generator and I will hand you the dongle now let\u0027s see you need just the this one and you\u0027ll have to stand in the pink box okay great let\u0027s please welcome pop okay yeah thank you for the introduction and yeah I\u0027m here to talk about my package generic emulsion and I will just start this rough introduction Who I am where I come from and then I will go over a few aspects of Mugen I won\u0027t bore you with any details "
  },
  {
    "startTime": "00:27:28",
    "text": "about the implementation or performance evaluation or how we built it I will instead just show a few key points and show an example about how you can use it because in the end this was about applied networking research which often somehow includes a packet generator and what I really want to show how to use it and how it\u0027s different from what you might be used to when you are used to different package innovators beat\u0027em hardware or software so me I\u0027m a PhD student at Technical University of Munich and it\u0027s at the German name on the side before because when we wrote that paper we had the company policy that university-wide policy that we were not allowed to translate the name to English which leads to these awkward moments people trying to translate it to just pronounce that name and we luckily changed that I think two years ago and since then I\u0027m allowed to say Technical University of Munich which is so much easier to pronounce for others and while I\u0027ve been there since 2014 and my start as a PhD student and I\u0027m hopefully finishing soon ish I have to like write a thesis and so on and this yeah I plan to finish next year sometime I said that last year as well but yeah and my thesis will be about testing different network devices where our network devices can refer to a typical classical two black box you send in packets out that does something you get packets back but doesn\u0027t divide seeing how fast it is this is kind of boring compared to a complex software system that can investigate a lot of different effects in depth and where the performance is not always clear where the bottlenecks are and so on and to do this i well I kind of started this work on moongeun or the idea when I did my master\u0027s thesis where I had a deeper look at performance of open V switch and virtual switches and cloud environments and I just realized there wasn\u0027t a really good software package generator that you could just use and that did all the things I wanted to and in the end I always ended up modifying some code of some software package generator to get it to do the things I wanted to was kind of annoying and so I\u0027ve addressed this idea of building a Pecha generator to start to really get even to really be able to do what I wanted to do I had to build this first and now it seems to have consumed almost everything about the teasers and it\u0027s mostly about the package generator now so where do I work at what was the context that this work is being done in this is the networking group at the thematics faculty Technical University of Munich we are a relatively large group of about 20 people plus some external guys and we do a broad range of network research topics that ranges from everything from your usual traffic measurement and analysis where we look "
  },
  {
    "startTime": "00:30:29",
    "text": "at look at traffic you look at we have a mirror port our internet uplink where we look for animalist here we do internet byte scans we do everything we have our own autonomous system for just for research stuff and for doing internet scans then we do of course all your hotbar things from software-defined networking and yeah Internet of Things and the usual we do a lot of security and privacy research as well and peer-to-peer networks and of course the performance analysis and modeling part where I\u0027m at this is this is really the subgroup that working in and what are we doing there it\u0027s well the the main research question or the main question that we have is that packet processing becomes more and more complex networks become more and more complex it\u0027s no longer just a few simple boxes that switch all out your packets there are more software components India there\u0027s the important passwords from software-defined network to network function virtualization and even even when this is done in hardware it often a software component to it and it\u0027s often even done in software nowadays instead of in hardware just just last year we had a project in the 5g area where we worked was a big company who were interested in in doing some performance research of software components in the 5g back and where a lot of stuff is being virtualized their virtualized network functions doesn\u0027t need to be chained together it\u0027s quite unclear how the how it impacts the performance if you have different things competing for the same resources if you have different configurations that then run in software that can compete for hardware sources from bandwidth to cache to memory to whatever and so the research questions are from the assembler thing what are the important performance metrics sure performance metrics you can just go to let\u0027s say of C 2544 defines measure these things on your box but it doesn\u0027t really work well for a for a software device compared to a hardware wise yeah it\u0027s 20 years old and was designed for hardware devices there of course new standard things and the benchmarking methodology working group is also meeting today that will be an interesting session yeah then how do you measure things in a realistic scenario what is even a realistic scenario do you just send some packets what packets do you send it\u0027s particularly interesting when you benchmark stuff like firewalls where you might want to simulate some complex denial of service attack or anything it can quickly get very very complicated and another big topic that we are working on is how do you make a measurement reproducible if you if you run this one thing once with your home built package generator on and you are testing your own homebrew solution this really and how do you ensure that someone else can reproduce "
  },
  {
    "startTime": "00:33:30",
    "text": "ourselves or even the you yourself can reproduce your sites in one year because you might not have that server that hardware any longer or not that specific software version which might not want any longer because you upgraded a system how do you keep that stuff manageable and of course how can you predict performances modules how do you can can if you are planning a network and you want to know how much harder how much softer to buy what hardware to buy how can you do that how can you kind of get a model for the for the behavior for the performance and kind of predict what you need to buy what you need to plan instead of just adjusting after the fact so and this is what unblocking lists we are lucky to have this big rack of test service which has a lot of 10g port and the some 43 ports at stripe diverse hardware from low end power saving 4 core CPUs to big Numa nodes with 40 cores and so on from small portable service that we can direct somewhere to show off in a demo to these big boxes and they also as the NHIN Sdn router and one one key thing here that really makes work work easier is that it\u0027s fully automated test workflow that means if I want to run a network experiment on and benchmark something I really write a script that defines everything that starts starts from I want to use this this and that server I want to configure the switch that way or I know that they are directly connected and then there\u0027s a management note that allocates the service exclusively for me so I\u0027m sure that only I\u0027m using it and only my tests is currently running on it and then the test script completely boots the server from scratch this means there\u0027s no fixed installed operating system that might break and Mike might challenge our producer bility because you don\u0027t get the same system again and that we do a pixie boot of a network and just deploy the notes as we need them and we can get the same operating system again and again can just reproduce the exact same software setup again and again and then collect some data and aggregate the data and already has some basic stuff puts them in a Drupada notebook and so we can start running analysis of the data on top of that this is really nice especially with the knife Buddha have a big collection of operating systems and kernel versions so if I want to run a test how to see how the front kernel versions evolved and then I want to maybe afterwards I think oh this might be another good metric then I can just boot the whole thing again and run the test and the whole thing again instead of having to cope with downgrading an operating system or anything so let\u0027s get to the the main part of the talk really is lost just a longer introduction is this about packet generators so this is a big packet generator but you might have seen or anything there are a few problems with these big harder boxes first of all they "
  },
  {
    "startTime": "00:36:31",
    "text": "are big second they are quite expensive and as I\u0027ve heard some the nice guy from into explains it like this so the problem is them shipping around extra boxes doesn\u0027t scale this is I think true because often if you have multiple labs and you don\u0027t have a package generator for each of them then or you might want some hardware features that are not available and the end people often go back to this fancy commodity hardware here a few normal network cards they are quite cheap comparatively they are readily available you can just plug them in your server and use them but then you run into a lot of problems because of course there\u0027s a reason why these big Hardware generators are so expensive and they are very reliable and precise at what they do whereas if you use a software package generator it\u0027s typically it might be slow it might be imprecise it might be unreliable it might just give a different society every time you run it and this is the problem that I\u0027m really trying to solve here I\u0027m trying to combine the advantages of both software and package on our software and hardware packet generators that is software typically cheap and flexible and hardware typically very precise and accurate and so the five main design goals that I had first of all it had to be fast obviously so and this is building on top of the DPD cave framework which was nowadays a Linux Foundation project which was just a set or a collection of drivers optimized to Ibis current by past drivers - for network cards and some utility stuff for whatever you typically need when building a network app and then on top of this I built an nice API to build packet generators and everything I built about as was always explicitly this multicart motive setting in mind every we if you will show a few examples of how our typical moonshine script looks like it\u0027s always its core it\u0027s always an explicit multi-threading and explicit multi-core because that is really the only way to scale to higher speeds sure a single 10g link you can fill that up those minimum size packets meaning about 50 million packets per second those are single CPU call it\u0027s not too hard as long as you\u0027re not doing too complicated things but as soon as you go beyond that you need to be able to run multiple multiple threads at the same time luckily modern network cards make this really easy because they offer this multi queue interface and they are natively doing doing a multi-layered approach in hardware so the software on top of it really fits quite nicely with the hardware then I wanted to be flexible because from my experience when I used packet generators I always in the end I always ended up modifying modifying the code at some place and doing doing something because I wanted this view at protocol and then it didn\u0027t support this real protocol or just just one simple example I had this requirement I wanted to generate "
  },
  {
    "startTime": "00:39:31",
    "text": "different flows only at layer 2 level meaning I wanted to modify just the MAC addresses and the package owner we were using at the time didn\u0027t support it so the source code and changed it and patched it and so on and the change denied it was okay I had to modify the source code of for package generators now so this doesn\u0027t see all these configuration languages they didn\u0027t seem to scale really so what I did was I for Monday I give full control of the main application to the user meaning that if you use moon gender idea is really that you write the code for the main transmit loop yourself meaning every single packet you sent out goes through your code and gets executed in real time for that packet and for that we are using the scripting language Lua which is has a very very nice just-in-time compiler and that allows us to really run custom script code for each and every single packet because it really come it really integrates very well with lower other things and you can get direct access to the packet memory without pesky things like bound checks or anything but just be careful when you write your test then another another thing that was traditionally very very challenging for software package generators is time stamping meaning that you want to maybe do maybe people often only evaluate their a few if you read some academic papers about that great new whatever out or whatever switch they often only give you ok this is the number and throughput that man uses this million packets per second or even though us disband versus typically the software devices I was tricked by packets per second and not by banthas but you rarely see latency because it was just so hard to measure as a software package generator and especially in academia people really have these big expensive hardware boxes so I really wanted to change that and of course turns out time stamping doing it precisely in software is a challenging problem but if you read the data sheets of your typical commodity mix carefully you can find some tricks how you can convince the heart that two times ten packets basically the the the hardware often has support for the PTP time synchronization protocol which needs to support hardware time stamping in order to work properly and if you then craft packets and trick the hardware a little bit actually drive a little bit then you can get the hardware to time stamp almost arbitrarily packets and this gets really nice result and really nice insights into software or hardware boxes that just weren\u0027t possible with software time stamping another thing that is quite quite the interesting thing and was more more important than I initially thought was doing rate control meaning controlling the traffic that assented that is to get between packets like if you want to send one million packets per second there I can send a thousand packets sleep for some time for like a millisecond sent another thousand packets sleep for one more millisecond and then you get at the "
  },
  {
    "startTime": "00:42:32",
    "text": "million packets per second but I can also sleep between each packet for one microsecond or I can do a fancy Poisson process or any more complex pattern and this turned out to be hugely important than investigating software systems and an aspect that\u0027s unfortunately often ignored and then of course I wanted to make it open source because what\u0027s the point if only I\u0027m using it I wanted to make it really easy to use and freely available you can check it out on github and what I\u0027m now going to show you is just just only only a few well measurements and and a few sites basically and I don\u0027t want to bore you with the details you can go to the paper citations down there if you want the NGO implemented details but I really want to show some some well examples of how to use it how the user is and why a few things are important and this is the static patterns this is really a point that I really like because it was just so much more important than I initially thought and it\u0027s so often just ignore what people just send a burst of packets and say oh the average is fine let\u0027s call it a day okay so this is kind of kind of big confusing graph but it\u0027s actually really easy this is a really simple test set up what I did here was I took two servers connected them as two 10g links and one mooned run on one of them and different package generators but that sort of on paper and open research on the other one and just forward packages open twist which no fancy packet modification nothing no fancy open V switch configuration just sent take packets from one part and send him out on the other using the normal open V switch kernel module I think okay there\u0027s a really boring test but if you dig down into even such a simple software software forward in case it really shows you what kind of complexity is hidden behind the seemingly simple example so this graph shows the x-axis is overload meaning I\u0027m increasing load in this case was restricted to one flow the forwarding device was restricted to one CPU core because if you go motocross Numa then this opens and you can so the simplest possible thing and I configured moongeun to use different different sizes the default I just generated constant bit rate traffic meaning a constant gap between the packets and this is the baseline of this measurement meaning a hundred percent and the measured thing here is the latency relative to that case so what you would expect if you run your packet generators a few times on the same case you would expect the device under test to show the same latency response because why would it be different and especially you would expect to get the the same latency result if using different packet generators if you don\u0027t change your device and a test but what we did in the past we had different package owner doesn\u0027t get completely different results for the latency of this the same device and a test to be investigated estrada and this graph is what i\u0027m varying here was the different graphs in that diagram "
  },
  {
    "startTime": "00:45:32",
    "text": "here is just the burst size meaning the baseline one packet sleep for some time one packet and so on and then four packet sixteen packets are the two packets and so on and see how the latency relatively to the base case changes and as you can see even with something as a burst size of four or 16 you can quickly get a relative latency that differs by a hundred percent or so so you just get a completely different asides just by changing how the packets are spaced on the wire without even going into anything from the content of the packets just just changing the this one thing and the the problem is why I\u0027m showing this is that people often send bursts as the default case because it turns out package after package generators are only really fast in if you have a naive implementation if you send out bursts because all these frameworks are always optimized to do the last packet processing or batching or vector or whatever you want to call it all of it is optimized for this and so the typical default burst sizes are between 16 and 256 for software package generators as you can see here this is a really bad idea if you do latency or if you want latency measurements it doesn\u0027t matter so much for the maximum achievable support that was around 2 million packets for all these configurations and now it\u0027s it was really tricky to get different package inheritance we have another paper on this where we compared a lot of different software package generators on how reliable you can generate what we configure them to do at first it turned out to be very hard to even get them to send didn\u0027t try to send CBR traffic some claimed to send CBR traffic but then have some optimizing oh sorry thank you and can people hear well enough ok this quiz will have sound from the other room so yeah ok I just talked about Oh or closer to the microphone so it was really annoying you can get these package owner like us to do what we wanted then and even then there were some optimization module sometimes in there sometimes had a kernel component and that then fetch them together well to make it faster but it was not very helpful and then they lied to you and even even when we managed to configure it it they still send out bursts because it turns out it\u0027s only hard to send an individual packet to a network cut it\u0027s just not what they are designed to do and in mundo and this is better but there\u0027s not a detail I\u0027m going to bore you about a 12 page paper about it what that one word you are just caught this incredibly boring to get through but I guess it\u0027s important so thank you for that ok why is this even different well one "
  },
  {
    "startTime": "00:48:34",
    "text": "reason is the CBR traffic is not a good case so typically it\u0027s not a realistic case the Internet traffic is not CBR traffic but like people people passed the CBR traffic because if you look at for example the old RC 25:44 it calls for a CBR traffic by default and people just follow that 90 I mean the RC even says you can test other traffic pattern afterwards but their default the CBR and people are just like oh well let\u0027s just do CBR and it\u0027s good enough and so this graph shows the same measurement SD SD was had before it\u0027s basically the baseline with collecting the latency here for the CBR case and it looks really weird at first it\u0027s okay it seems to be increasing and there\u0027s one we had spiked the Spikers completely reproducible across different systems different things and right before overloads it drops completely latency latency gets better this is also completely reproducible consider more measurement points in there because when I first saw this I was like well as can\u0027t be - I need to measure maybe destroy study different but it just reliably drops there I also have a paper about that but it\u0027s also another deep dive into the details of how the Linux kernel and the driver works basically there are two things that are trying to well one is trying to prevent the system from locking up and from interrupted terms that\u0027s the Linux poly mode Nappi which just switches to some fancy poly mode and just poison at work out and disable interrupts and then guess the interrupting rate which is typically found in all these drivers that tries to save power and then you and goes into power steering that\u0027s a whole other can of worms to open so and basically you\u0027re going to say here I was having kind of okay kind of works also hard to measure because by default the UNIX can you find in most distributions doesn\u0027t report the CPU time consumed per interrupter can be at 100% CPU load but edge drop reports you are at 10% because it doesn\u0027t account enter up time properly unless you sit there acute time accounting selector compiling economy or directly with the CPU performance counters and this is completely different piped off so now this kind of looks weird and they\u0027re the reason why it looks weird is really that the that these algorithms that try to estimate rate and so on they don\u0027t play well with CBR traffic they kind of get confused and there are a few state machines if you get them unready it keeps switching between two states all the time because they get surgery confused by the CBR traffic then I don\u0027t know exactly what happens but you get these motor sites so let\u0027s use APIs or process instead and there just looks much smoother and the site is much more what you would expect and the only thing I\u0027m changing between these two measurements is again their time between packets in this case from CBR to Prasad and much more reasonable result and if you look at real world traffic it\u0027s of course you all know that really old sitcom paper about how you shouldn\u0027t model your Internet traffic proposal process but that\u0027s only really true for larger timescales if you\u0027re running your "
  },
  {
    "startTime": "00:51:35",
    "text": "test of a few minutes or whatever then you can use a Poisson process to reasonably approximate what real traffic looks like and then you get these nice nice smooth suicides and more in more realistic scenario which is also what this is about so what does the latency measurement of look like if you want to if you know installed Mundra and you want to drill down into an - really one measurement you get these nice histograms which are just some way to represent how the latency is distributed and typically for many cases you want to see the F instead but a histogram is well easier to see visually so what you can see here these are just measurements of a few systems the first one is a software for water running directly on the machine you can clearly see there\u0027s some interrupt sorting going on so you get this uniform distribution kind of neat and the second one there\u0027s a virtual machine and the virtual switch and everything involved then we get this long tail distribution and I actually cut it off yet it\u0027s actually long tail and there are some of those cases there\u0027s also an interesting thing to measure they are just just if you work until 99.99% I love some latency measurement and then you see some horrible results there if you if you are benchmarking a virtual machine or anything this can also be a big problem and it\u0027s also another thing where you can get probably a whole PhD on how this happens why it happens how to measure it and a hacker box this is just something to show how precise this really is because if you note the x-axis it\u0027s and microseconds and only goes up to 3.5 microseconds typically saying the position of the Mundra and hardware time stamping approach it\u0027s typically plus minus and plus minus subscribe nanoseconds that is quite good for most things that typically typically agencies of the hardware box its new range of a microsecond or maybe 500 nanoseconds to a few microseconds a software box around ten microseconds if it\u0027s good and a hundred efforts during some parts having stuff and you can see here is a nice bimodal distribution and this is who\u0027s just an example where if you want to break it down to one way or your latency or anything it doesn\u0027t really work because what\u0027s the average of this what\u0027s the meaning of the average of this is really nothing so really meaning of the median of this it has two clearly distinct paths in the hardware in this case it was it was an output part that was used by multiple input parts and they are just two cases either it goes to the cut through pass if there\u0027s no packet being queued from the other port or there\u0027s a packet being Cureton so it gets queued for a short time then goes to the other one also have a full paper about that stuff and then I want to show you how to use it and there\u0027s just the only boring architectures like basically Mundra and sits on top of DP d k + TB DK is it\u0027s on top of the network card and the network card offers multiple queues and at the top you have your users what we call a user script that is your user controlled you a script you write the whole script "
  },
  {
    "startTime": "00:54:35",
    "text": "and yes there\u0027s a lot of boilerplate and the idea is kind of you copy/paste one of the example scripts modified to suit your needs documentation might suck at some parts but it\u0027s just your typical open source project if you look at the example scripts and if you look at your basic assembly script that generates different UDP flows and to report sir sites you should quickly get an idea of how to modify it for your needs or how to act multiplayer things and the way we do multi-threading is respond completely independent virtual virtual machines virtual machines in the sense of a language implementation virtual machine for the just-in-time compiler and that is really they are really completely independent and there are nice api\u0027s that allow you to talk between these these independent sets but their main ideas are shared nothing approach because in the end what you want to generate as multiple flows and they are often independent from each other or you can break them down into a few independent chunks and that makes it really really high performance and we have to look at the examples to get an idea of what I mean by this so and I don\u0027t know so a quick except I don\u0027t know how much time I\u0027ve left there\u0027s no clock how much time I got left continue for another ten minutes okay great this seems perfect so okay I\u0027m gonna show this example and in this case and this example is based on our the excellent example this is also something we wanted to test the exam and then the package you know that is almost VX and I don\u0027t know that\u0027s protocol and well but for you and so there the first thing that you can do is you can dynamically general design a a complex take of headers and really these are just ahead us moon drain is still a low-level package generator not a traffic generator meaning there\u0027s no protocol logic behind that you\u0027re just sending out packets and the protocol logic is like absolute minimum like we implement up and NACP and whatever you you expect for basic functionality and there\u0027s like the hashing algorithm to get the source port of the expand and so on and to check some check some stuff and check some of loading but it\u0027s not the traffic generator can\u0027t build a tcp stream from it you just can build packets but it\u0027s meant to benchmark some devices on there on the lowest level and what you can do here is you can stack together arbitrary had us and like in this case it\u0027s me extra and running over ipv4 and the VX plan there\u0027s a VLAN tag is enough remembers another ipv4 header and UDP oh by the way everything we have is also a p v6 capable because the guy who wrote the body the protocols tech stuff he really likes ipv6 and so all the examples also do ipv6 yeah and once you have that thing it gets just-in-time compiled and the next thing you do is you create a memory pool with a packet archetype meaning this is just some some "
  },
  {
    "startTime": "00:57:37",
    "text": "basic packet this is quite memory pool this is just Maps if you know duplicate then you will see how this maps to the PDK stuff and then you fill each of your packets with the archetype this means you - AVX dance Texas initially an empty packet you tell it okay I wanted to be a the exam tech packet and then you can call this magic filler method which is very slow but you only do this once and this has nice nice setters basically for all the headers you have in your stack with the names that you defined before and you can do stuff like lookup and MAC address here and you can basically set settings and then you can tell it to calculate the checksums and the scheduler checksum is again a magic message that goes through the whole stack and knows that okay there is this checksum and to calculate that or adjust either example just does it for one thing but there\u0027s also another method that does it for all the things in your header because I think I\u0027m using or flirting here um okay now this is basically the initialization I defined my stack I defined what I wanted to look like and then there are some boilerplate code like commands and argument handling and and whatever and and then the next thing is you write the actual transmit loop meaning you write the the core of the of your transmit loop of your package generate a test script and this is this is so completely different from other package generators because in the end it\u0027s it\u0027s not really a package generator pose it\u0027s a it\u0027s a framework for writing package generators and you write your own one based on one of the examples and a lot of boilerplate code so what you do here you allocate some buffer array in this case it\u0027s default back size and yes this example sends out bursts unless we configure that\u0027s right after I can\u0027t go into the details about how the right control works and then you at the actual main loop that just checks is the process they\u0027re learning meaning that someone press control sees and sick term or did another task stop it and we tell it okay nice we want some packets on our memory pool and we just get the packets with the packet archetype that we previously filled we iterate over these packets and again cast them to the to detect that post period compared this castle operation is a completely free operation there\u0027s no circus behind you it\u0027s just the equivalent of a see cast and it doesn\u0027t do anything besides tell the compiler okay I want to use these offsets for my packets and then I can just access these packets as you write offsets that gets compiled down to a few assembly instructions and I can do fancy things like in this case I want to randomize my destination port I can just call it mastered random or whatever here whatever arbitrarily complex processing you need for your thing and you can of course access other fields here and then you set some offroading flex and you sent a queue to tell you to send out the bots and this is the whole main loop and this is also runs typically on an independent thing and you could start multiple threads running this and if you look at the examples that is really what they are "
  },
  {
    "startTime": "01:00:37",
    "text": "doing now let\u0027s say you don\u0027t want to write a script because there\u0027s this recent development that we are working on is to really provide an actual packet generator for our packet generator meaning a set of predefined scripts that have extensive command-line arguments and a sense of config files because it turns out not everyone likes likes writing script but everyone likes writing scripts in the newer a scripting language there are I\u0027ve talked to two people from some companies they\u0027re like basically oh but our at least engineers they are not programmers they can just click on the X jeogori and click the start button we can\u0027t use this okay so let\u0027s make it somewhat easier and in this case at a config file to it there\u0027s some new work the work-in-progress might be completely buggy but completely buggy but might contain bugs and so and in this config file we just define defined flows to give the flow a name and we tell that the packet type a few predefined packet types otherwise you can use the magic magic protocol it\u0027s taxing again and then this is basically the same syntax just with a few syntactic sugar things you need to tell it if it\u0027s a MAC address you need to tell it if it\u0027s nappy address and then you can define things like ranges or random Rangers which then get basically well not compiled to code but they basically it\u0027s efficient in the end it works for us a lot of anonymous functions and magic in this case this is your typical simplest example please don\u0027t copy paste and run it because that ipv6 others as one of my first service and then once you have key he\u0027s pretty of these flows you can purify flosser can define your own one then you can under moonshine simple interface that in start and the name of your flow on which device to send on which device you want to receive if you want to just see counters for how much stuff is getting back and then you can set a few parameters like the packet rate like if you want to use per so for control if you want to use whatever how many shots you want to use bypassing the same device multiple times you can combine different flows you can set time limits for individual flows and so on and you get then per flow packet counters and yeah what you also can do here which is it\u0027s often quite annoying to be back something like this because in the end you want to know what it\u0027s actually sending out and then you might end up using TCP dump on your destination device or dumping message here\u0027s like I simply debugging interface that can show you okay giving this config file in this configuration the packets that I\u0027m going to send out would look like this here\u0027s an example of five packets and these are the fields that are being randomized or modified and this is still work in progress in particular I know that I just recently broke the Poisson process for the traffic generation so yeah I\u0027m working on this we got it fixed by next week somehow work on outright a duplicate no idea okay and one last thing I want "
  },
  {
    "startTime": "01:03:37",
    "text": "to show us how others using using moongeun just so that you might get an idea what you have other successfully done with it and how this might be useful for for you or how this might work and a few things I want to pick out across the high profile applications and in particular I want to point out the the DNS DDoS resilience tests and was just recently presented 1274 and they contributed some DNS logic to moongeun is of course not a full genus implementation but some boilerplate code to get DNS queries and to randomize DNS queries in an efficient and relatively simple way I said I couldn\u0027t share the actual code for the for the DDoS attacks for legal reasons which apparently something about hacker tools and France or so but they contributed in escort and it should be relatively simple to build some in SD dos testing device based on top of that so this is interesting because it uses the complex protocol six and then the last thing I want to point out this is really interesting because they actually used Mundra how it was intended to be used most people just use my standard example script which sends out randomized to DP packets and say okay it gets me the number of flows I type in there and it gets me latency that\u0027s good enough and they maybe change one line in the code but these guys and we open a few project they really build a nice test harness around moon drain but they actually use much support multiple different package innovators munis one of them was one of the first and I really like this because they actually use their API the way it\u0027s meant to be and they built this complex test harness and this is a nice thing to look at if you want to automate more complex test setup also Big Shot off to the first pair piscis before software switch because they contributed the hardware time stamping code for the engineer 40 gigabit mix that was kind of annoying work and I haven\u0027t done it I was just about to get into it and they sent me a polar has all great thank you and yeah so I hope you found this kind of tutorial style lesson useful I hope maybe to find a few new users for some things that has a broad range of applications as you can see here and hope you enjoyed it and I believe there are a few minutes for questions left and if anyone really scans a QR code ever you can scan it here I can just type moon run into Google and you will find it on github hi Dan Bogdanovich actually very nice work and I\u0027m pleasantly surprised in package generation the package iteration so far has been an issue and domain of the expensive guys and seeing an implementation using VP DK you know solves many of the problems especially up to a 10 gig space so thank you for the work thank you "
  },
  {
    "startTime": "01:06:37",
    "text": "so I\u0027ll ask a question will it make sense to come to one of our hackathons and generate test traffic on the fly for various people doing testing of their that\u0027s actually yeah yeah II understand I\u0027m the second ITF and I was not really aware what\u0027s going on at these hackathons London yeah okay good good another question yes thank you hi Paul great talk as always al Morton you might have to get closer closer like yeah how\u0027s that you mentioned the comparisons between packet generators and the sensitivities of the devices that are being tested like OVS and VPP and so forth we\u0027ve seen some of that in in OPN fe testing anyway what would you say to a specification that calibrated the generators in other words like policing kind of like policing the police would would I mean I\u0027m beginning to think because of the sensitivities we\u0027ve we\u0027ve seen and you\u0027ve seen that that might be a valuable spec to pursue what\u0027s your thoughts that\u0027s an interesting idea as I\u0027ve said I have and I believe I have a few graphs here and like this just for me in packets per second and a few software package a notice and how they even even wind and then configure to use poor software doesn\u0027t try to hit this target which is 12 50 nanoseconds and but some significant diverse and package on duplicates not not visible here but they\u0027re huge outliers and they\u0027re because the printer statistics in the same settings are really bad idea I\u0027m just such a specification might look like it\u0027s unfortunately one half user so it doesn\u0027t include Cisco TX package generator which would have been a really interesting addition and this what looks like those are different generator at the firmware control method so what we did was used an FPGA for that - just really precisely measure how to how how they interpret actually behaves and that for us it was quite challenging so any thoughts on how you would specify that again I mean some metric you would look for how you can break it down to a single number I mean what the is the mean squared error but it\u0027s I believe not a very good metric because yeah well I mean if you have a target and it\u0027s as stark as comparing those two slides they "
  },
  {
    "startTime": "01:09:40",
    "text": "may be just looking at the distributions says all but the but I think the bottom line is we need to learn from really everyone\u0027s work that CBR streams aren\u0027t really realistic yes we should we should have different specifications yes in the benchmarking that we\u0027re going to do and then we need to ask the question across different generators how accurately are they producing the streams that we desire so we have a template and it ends up being the thing that we compare against so it\u0027s actually easier to generate because these are just scale up to process new processor process and CBR try to motivate thatit\u0027s can\u0027t do it can\u0027t do it reasonably so I hope to see you this afternoon Thanks yes thank you very much and it sounds like a very good hallway further conversation in the hallway we have our next speaker momentarily we\u0027re just switch shifting so thank you Paula and I\u0027ll see you at the next IDF here\u0027s your machine back okay so the yeah ok I mean introduce you so mm-hmm okay so our second speaker is no is rollin round race fake day from u-20 Fanta and surf net he\u0027ll say it better but and he has been as a speaker here before and thank you for being here and I think you\u0027ll enjoy this talk - Thank You Alison so it\u0027s actually pronounced lowland fungi swag day but I\u0027m not going to torture people and force them to do that so my talk today is going to be about the use of elliptic curve cryptography in DNS SEC which is what the paper that was that gautier NRP was actually about and I\u0027m going to go into a little bit more detail about some follow up work that we did after writing that paper and also going to go into some detail about the doctrine of these cryptographic algorithms in the in a sec and since Paul did a really nice introduction of himself I decided to add one slide about myself because I made that step and actually wrote the thesis and if and defended it on June 28 this year and this is a thank you this is me surrounded by my committee you may recognize some of the people in that committee some of them are here in the room so no pressure no pressure but I\u0027m gonna go into the meat of the presentation now and I\u0027m not going to repeat all of the earlier research we did there are some pointers on the "
  },
  {
    "startTime": "01:12:40",
    "text": "slices slicer actually up on the material site for this this meeting and at the end there is a set of references to all the papers I\u0027m referring to so if you want to look those up you can look us up offline so in earlier research we looked at issues with DNS X specifically and there are two things that we focused on which were technical issues and first was that if you start deploying the in a sec you may encounter packet fragmentation and this can cause issues and in earlier work we actually saw that up to 10% of resolvers on the internet have issues receiving fragmented responses which causes delays or in a worst case scenario actually causes them to be unable to resolve certain domain names that are domestic signed the other issue of course is that because packets are a lot larger the NSA can easily be abused for denial of service attacks and it in the past few years it has actually been abused for that purpose and there have been reports about that in the media now all of these issues are linked to the fact that DNS egg makes your response size a lot bigger because it includes signatures and keys in the DNS packets and in an earlier paper we actually argued that the root cause of all of these problems might be that RSA was chosen as default cipher for DNS SEC because if you think about it if you use RSA 1024 a bit or 2048 bit and every signature adds hundreds of bytes to your packet and and that quickly inflates packet sizes so this made us wonder it can\u0027t be do anything better can we use for instance ECC so elliptic curve cryptography tow achieves the same goals as RSA so it\u0027s a public key cryptography you can do signatures with it but the nice thing about ECC is that the both the keys and the signatures are much smaller than they are for RSA while they offer greater cryptographic strength and to give you an example a typical case is used for elliptic curve cryptography is using a tour in 56 bit group that gives you a 512 bit key and signature in Indi in a second that\u0027s four times smaller than for instance RSA 2048 so that\u0027s attractive because it makes your package much smaller so why wouldn\u0027t we switch to ECC immediately for the in a sec well to quote our essays RFC 605 which is the RFC that actually standardizes the use of the elliptic curve digital signature algorithm in DNS SEC validating RSA signatures is significantly faster than validating ECDSA signatures about five times faster in some implementations it\u0027s kind of a weird way of saying ECDSA is slower and this potentially means that if we switch all of our signing for DNS SEC over to ECC that we\u0027re pushing problems to the edge of the network right because validators are the machines that do all this nasty work for you of doing a full dns "
  },
  {
    "startTime": "01:15:41",
    "text": "resolution and they\u0027ll have to validate these signatures and we might be increasing their load significantly if we start using these cryptographic algorithms and actually this is five times faster which you should actually read as five times slower is a bit optimistic so we did some benchmarking and it\u0027s it\u0027s way worse than this so the goal of the study in this paper was if we switch DNS sick from using RSA to using ECC how does that impact validating DNS resolvers right so rather than recommend to everyone well these easy say ECC signatures are much smaller it\u0027s really nice to switch to them let\u0027s work out if we\u0027re not introducing a new problem by giving this recommendation and that was the purpose of this study so how do we go about doing this we decided to do a measurement study and some modeling for this and our method I\u0027m gonna describe our methodology in the next couple of slides and we started out from the premise that we want that we had this intuition that if we knew the number of outgoing queries from resolver so that\u0027s not incoming crash or clients that\u0027s the queries the resolver sends to authoritative name servers on the internet that we might be able to predict the number of signatures validations that it has to perform given that load right and that was our premise and I\u0027m gonna get I\u0027m gonna talk you to four factors that influence the number of signature validations that a resolver will have to perform now the first factor is that for every query that a dns resolver sends to an authoritative nameserver on the internet it will not always get a response right so the number of responses that come back are somehow a factor in this the second thing is that because dns a is far from universally deployed not every response that you get back from an authoritative nameserver oaken contains signatures right so you won\u0027t have to validate signatures for every response that you get back the third factor is the number of signatures in the response that contain signatures right because while you might expect a response to contain a single signature for the record that you requested actually we observed that on average every response that contains signatures has somewhere in the order of between 2.4 and 2.5 signatures per response that has signatures that\u0027s because there are signatures in the additional section there might be extra records in there that require signatures so this intuition doesn\u0027t hold there are more signatures in a response than just for the record that you requested finally your resolver may not have to validate each in every signature in that response either because it\u0027s already validated one and it\u0027s cached it or because it\u0027s in the authority or additional section and it decides not to validate those right it doesn\u0027t have to validate those and another reason for "
  },
  {
    "startTime": "01:18:41",
    "text": "why it might not validate signatures is because it doesn\u0027t have a full chain of trust all the way up to the root so it might not be able to validate that particular signature and it\u0027s not going to bother to spend the CPU cycles on it right and to remind you again we do not measure the number of queries from clients because that very strongly between resolvers we wanted to build a model that we can apply to any resolver regardless of its client population and if I\u0027m not gonna go into detail in the presentation but in the paper you will see that the model is actually a little bit less accurate for resolvers that have a small client population but those are the resolver that we really don\u0027t have to worry about as much because they will be validating far fewer signatures because they\u0027re processing far fewer queries right so how did we measure this this picture shows you our measurement setup and on the left hand side you see clients which is typically you with your laptop unless you\u0027re an idiot like me to run servers over on their laptop and then what we did was we captured traffic that was going to production DNS resolvers and then we forward this traffic to an instrumented DNS resolver so we\u0027re sending a copy of the exact query traffic that goes to a production resolver to one that we instrument and then we measure certain factors on that so to make sure that our instrument the resolver is actually performing in a normal way we measure the number of queries from clients in the number of responses sent back to clients which you see on the left hand side marked as QC and RC and compare that to the production resolver right just to make sure that we\u0027re observing the same thing right because if we instrument resolver when we break it and it and it doesn\u0027t respond to clients as it should that would be a mess right and there\u0027s some some stuff in the paper about the ethics of this because of course we\u0027re sending people\u0027s traffic to an instrument to resolve Reni we don\u0027t want to violate their privacy so he took some measures for that on the instrument instrumental resolver we measure on the outgoing link towards the internet so where the resolver talks to a third date of name service the number of queries it sends all of these factors that I talked about in the previous set of slides and the numbers of signatures that it validates and for that we actually had to alter the code of the resolver right because this is not something that most resolver implementations typically keep statistics for so in this case we modify bind and unbound to record this number of validated signatures if we then look at all of these four factors that I described in the methodology slide and you plot those as a scatter plot and then you see from top left to bottom right all of the four factors so a shows you the number of queries on the x-axis number of responses that come back on the y-axis B shows you the number of responses on X versus number of responses containing signatures on Y C shows you number of response a number of signatures per response and D is a number of signatures that are actually validated or fractional signatures "
  },
  {
    "startTime": "01:21:42",
    "text": "that\u0027s actually getting validated now if you look at these graphs then your intuition might be that you could model this with a linear model although especially graph B has a lot of noise in it but as it turns out graph B is not the one that we want to worry about which is the number of responses that contain signatures because we\u0027re actually going to use that later on in the model so there the accuracy is not an issue for the other ones we tried if we can approximate this with a linear model and it turns out that we can so I\u0027m not going to talk you through all of this but we came the details are in the paper but we created a simple set of linear equations that you can then combine to make a model for a validating resolver and it has four important parameters so the first one is the average number of responses per query and this is something that you need to measure on an operating resolver to actually populate the model so these are parameters that you would need to measure the fraction of responses with signatures and the average number of signatures per response and a fraction of signatures that is validated Oh is the mic broken again okay I\u0027ll try to eat the mic as somebody suggested yesterday okay so of course if you make such a model you need to validate if it works so we validate or we said as we set out for criteria that we use to validate this model and the first one is that we wanted to make sure that the model works for a different DNS resolver implementation so we tested it for unbound and bind and actually I had a student do some tests with all readiness as well which at the time they were implementing validation in the resolver unfortunately wasn\u0027t stable enough and the same is true for not resolver which was then still under development but in principle you could apply to those two and we get some tests with it now the second thing we wanted to validate is whether the model has stable properties over time and actually like I said only alpha s which is the number of the diffraction of responding responses that contain signatures can vary significantly over time because we vary this parameter to do to make predictions right so we don\u0027t really care if that\u0027s stable over time but the other factor should be more or less stable over time so we measured over a five month period and didn\u0027t see major changes in in these two in the rest of the parameters of the model we also wanted to make sure that this model works for different client populations so we use traffic from four sources to test the model we used so I work for certain international research in education network in the Netherlands so we used traffic to three of our production resolvers but we also use traffic to one of the resolvers for my University and in addition to this because we realize that we may not have typical resolvers because we have certain client populations we also did "
  },
  {
    "startTime": "01:24:43",
    "text": "some worst case estimations that are in the paper where we take sort of worst case estimates for all of the parameters of the model and finally we checked if the model is actually a good predictor of imparity observed data so what we did was we populated the model made predictions of what we thought the number of signatures that we need to validate what\u0027s going to be and then compare that to what we saw in actual practice and again the details forever in the paper but we did some statistical goodness of fit tests for that now the next thing that we have to do is now that we are we have a model for predicting the number of signature validations that are required we of course need to know how ECC performs right and although there are some benchmarks publicly available that we used in in in an earlier paper we wanted to make sure that we had up-to-date benchmarks so we took five implementations of elliptic curve cryptography and benchmark those on them on a modern CPU we took three versions of OpenSSL one that we consider a legacy so you\u0027ll see that in older distributions one that we considered long term support because it\u0027s in in all of the long term support versions of for instance the major Linux distributions and one that we considered at the time to be new and optimized and it\u0027s actually moving into long term support now and that had optimized implementations of ECDSA P 256 so that is the algorithm that we expect will be used the most in DNS ech at this point in time and actually there\u0027s now OpenSSL 1.1 or 0 which was not performed that differently from 102 so this is actually still a good set of benchmarks to use today then we looked at the newer elliptic curve algorithms 82 four five one nine and eighty four four eight which have only recently been standardized for use in DNS SEC and there again we took optimized implementation of these two algorithms because the the reference implementation of those don\u0027t perform very well so we took optimized versions there and then what we did was we did a hundred tests of running the algorithm for ten seconds and then measuring how many signature validations it would perform in that period and the details of the benchmarks rendered paper to give you some idea so easy do you say P 256 is an order of magnitude slower than RSA 1024 and why is that a good comparison because we\u0027re you could argue that we\u0027re comparing apples and oranges right because ECD is AP two five six cryptographically is much stronger than RSA 1024 but I would argue that you need to make this comparison because most of the signatures that you see in the NSTIC today are signatures with zone signing keys that are 1024 bits where did that make sense in terms of security or not you can debate but that\u0027s the case and "
  },
  {
    "startTime": "01:27:43",
    "text": "then if you take ECDSA P 384 which is greater perfectly even stronger you compare that to something like RSA 2048 you see again that there is an order of magnitude performance difference so this is way more than the five times that is quoted in the RFC even 8255 for nine which is way faster than the ECDSA algorithms in terms of implementation that is still almost up to an order of magnitude slower than RSA 1024 and only four if you compare to RSA 2048 do we get into the same sort of order of magnitude as is quoted in the RFC right so some key benchmarks because you\u0027re going to see these in the graphs that I\u0027ll be showing you a little bit further along in the presentation you see five implementations on the left-hand side the specific elliptic curves that we did the test for and then the performance in the number in terms of the number of signatures validations that you can perform with that algorithm per second on the CPU sides at the bottom right this is not the top-end CPU but it would be a common CPU that you would encounter in server architecture so this is typically something that people have in their data center right we have data centers full of these stuff unless you\u0027re really rich in your Google you have these and why did we pick these particular benchmarks because you\u0027re going to see them in the graphs that I\u0027ll be showing you in the in the next couple of slides so we picked ECDSA p3 therefore because this was a worst case scenario right this is the the slowest of all of the algorithms that we benchmarked and it is the strongest broadly supported cipher and what do I mean by that you could say that 84 4 4 8 at the bottom of the slide is stronger in terms of cryptography but it\u0027s not widely available in implementations so few people are going to use it so that\u0027s why we took P 384 as sort of a benchmark to compare against then P 256 on openness l1a1 we took that for as long term support version of the most likely use sulfur the one or two implementation of P 256 as you can see that is almost three times as fast because of the optimizations in there and this is to show you what an optimized version of ECC can do and then finally newer algorithms which outperform the older ones by yd250 509 is is again another more than a half times faster than the most optimized version of B 2 5 6 although I\u0027ve actually seen a paper recently that claims that they can make P 2 5 6 almost as fast as 82 for 509 right so let\u0027s go back to our original question which was what is the impact on validating the US resolves right because that\u0027s why we started this so we use our model to evaluate or to estimate future "
  },
  {
    "startTime": "01:30:45",
    "text": "performance right and we took a look at two scenarios so the first scenario is what if we take currently in a sick deployment and we switch all of those domains over to ECC overnight so everything that\u0027s now signed with RCA or DSA or whatever you use will switch that to ECC is that an issue well based on the measurements that we did on our resolvers we argued that you need about 150 signature validations per second for a busy result where this was a result for processing around 20,000 queries from clients per second so this was a busy resolver and that\u0027s not a problem and even if we take the model and we put in the worst case numbers we still don\u0027t go above the worst case scenario which is using p 384 so that\u0027s not an issue but what if everybody deployed the NSTIC and if they used ECC right so right now the NSTIC deployment is roughly three three and a half percent of all domains on the internet what if a hundred percent of those were to deploy it in a sec and they all use ECC to sign what does that do to your resolve so the second scenario that we evaluated is a popular domains first growth to a hundred percent domestic deployment and everyone uses ECC now what do I mean by popular by popular I mean that the domains for we for which the resolver sense to most queries to the internet if that switch is to ECC first right and so here I plot the query popularity for queries that the resolver sends to authoritative nameservers and and this shows you this is a classic so you can see the the axes are low cloak and this is a classic internet distribution it\u0027s commonly referred to as ship for long-tailed or Plato or 80/20 doesn\u0027t really matter but basically what it means is that the most popular query and is by far responsible for the highest number of queries sent out to the Internet so let\u0027s look at some results so these are the really nice graphs that my student cooked up he\u0027s a MATLAB wizard and what you can see in this graph on this axis and we can debate what you call that axis but I would say it\u0027s the y axis you see the intersect deployment and as you may remember I said that we will be varying the average number of responses with signatures to simulate the intersect deployment right so what we did was calculate based on the distribution for popularity we modeled what if we go from left to right in that distribution how many queries with how many responses with signatures would that result in and that\u0027s what we model on that axis so from 0 to 100% in a sec "
  },
  {
    "startTime": "01:33:45",
    "text": "deployment on the x axis you see the number of outgoing queries and actually the maximum number of outgoing queries we observe observed on the busiest resolver was some 1,800 queries per second and then you see a grey playing in in the graph which is the maximum number of signatures that we can validate for ECDSA p 384 on a single CPU core the takeaway from this is that there is ample room for growth in the NSTIC deployment and outgoing query load right so we can go up to 100% DNS SEC deployment using ECD DSA P 384 and the number of outgoing queries from that resolver could still double and Unbound would still be able to validate those signatures on a single CPU core right and so this is a conservative estimate if you run a really busy resolver you\u0027re working at an ISP you typically don\u0027t have a single core assigned to that resolver so this is something that is easily within the realm of the possible so that\u0027s a great result we can we can take the worst case algorithm and unmount we\u0027ll still be able to cope with it so what does this picture look like for bind um I apologize because the title of the slide has somehow gone but this is bind and for bind we observe something a little bit different there might be a potential problem for P 384 because as it turns out bind validates up to two and a half times the number of signatures for the same query load that unbound us and okay there are there\u0027s been suggestions for reasons why that might be because blank might be trying to chase if it gets negative responses it might be trying to chase more chase up other authoritative nameservers it really tries very hard to get the response and it might be validating more signatures because of that but we actually didn\u0027t investigate in detail why this was the case we just we just took this as a given but what you can then see is if we look at the the the situation for the ECAC ECDSA p25 six long-term support version which is actually the one that we expect people to use if they sign then you see that even though buying has ample room for growth or the green line that intersects the red slope intersects it way beyond the number of queries that we would need to be able to validate signatures for on our busiest resolver today and the same is true if if for instance you want to go for something really strongly you don\u0027t even go for p 384-bit you go for the newer curve 84 for 8 that outperforms even the long term support version of P 256 so to take away from this is in most cases binary code but there are slight words for P 384 now answer the original paper we did some additional benchmarks because you could argue that we did our benchmark on into "
  },
  {
    "startTime": "01:36:45",
    "text": "x86 some of the optimizations that have been implemented are only available for x86 architectures so what about other architectures what if I have a home router and I want to do dns accreditation on that how is it going to cope so we did a stupor chicken actually had two students working on this where we did benchmarks on arm CPUs and on MIPS CPUs because we would think that these are representative of what you would find in the typical home router now the key I\u0027m not going to go through all the details of those benchmarks but the the key takeaways there is that the performance is low but it is more than sufficient for home scenarios and that interestingly ECDSA may sometimes be faster than EDSA because there are already some optimized versions of ECDSA available for for instance arm CPUs that outperform the stock EDSA implementations that are available for that we did an N equals 1 home router experiment so this is take with heap of salt right this is not representative but it\u0027s an interesting experiment to run and one of my students really wanted to do this and and I mean he got extra bonus points for the fact that he got informed consent from his roommates before he did the experiment right so and I didn\u0027t even have to enclose that because he it turned out he had an ethics course going on at the same time so and there are two takeaways for this shows you 24 others of course to the resolver that they run in their student dorm which he equipped with the unbound and then instrumented and they\u0027re measured and the there are two takeaways from this and the first one is a little bit serious in the sense that with them concurrent user so he did this during a party so there were lots of people in the house and with them concurrent users to query loud peaked at around 60 queries per second from clients right now he also measured cache performance on on his resolver and the cache performance was atrocious right because there are too few users to make good use of the cache so actually only about 10 20 % of his queries could be received responses from the cache so I said well let\u0027s go for the worst case the couch doesn\u0027t do anything and the resolver has to validate everything and then you see that if you if you would run this on a mich CPU and we take some of the benchmarks that they perform that the resolver would actually start to struggle because these home devices typically have only a single CPU core and that CPU core would be swamped with signature validations at point load so there\u0027s maybe some work there but this is actually only if we have a hundred percent domestic deployment which today we don\u0027t have so I believe we can safely assume that by the time if ever we reach one hundred percent the NSTIC deployment these devices will be fast enough and there will be optimized "
  },
  {
    "startTime": "01:39:45",
    "text": "implementations of elliptic curve algorithms Oh in a second takeaway from this is that stood apart e\u0027s are not what they used to be because it observe the observant viewer will see that the party stopped at 1:00 a.m. oh there\u0027s a question is this a is this a representative student party where people are browsing the web instead of getting drunk yes these are Millennials so they\u0027re checking Facebook the whole time posting selfies I mean yeah yeah yeah my my XE my student was we told him not to look at the queries and of course he did and and then he thought I don\u0027t want to notice about my room Hanson and he gave up on it so I want to end with some insight into adoption right because I\u0027ve written a couple of papers on on use of ECC in in DNS SEC but until 2015 there was virtually no adoption of ECDSA signing schemes that are standardized in RFC six 605 and RFC six 605 dates back to 2012 so and actually the first implementation in environment unbound is a little bit older than that right because about the records one of the authors of the RFC is also one of the developers that implements Unbound so there was supporting unbound actually before the RFC came up so in late 2015 CloudFlare actually was the first DNS operator to adopt ECDSA signing at skill right they rolled out their DNS ik as a service thing and they said we\u0027re going to Oliver Guth Monson said we\u0027re going to be signing with ECDSA so we want to know how is it option developed since then these graphs refer that continental org and what they show you is the period from the first of March 2015 until somewhere last week before I got on a plane and what you can see against the legend is little bit hard to read but I\u0027m gonna be pointing out some things such as the fact that the majority of signed domains income net and org still use RSA sha-1 that should change sha-1 is not considered secure anymore for signature algorithms this is the point in time where CloudFlare announces its Universal DNS accusing ECDSA P 2 O 5 6 and as you as I hope you can see there is an uptake of ECDSA that starts from that point onward right the the pink area at the at the top of the graph starts growing slowly but actually ECDSA adoption is now driven completely by other operators that are adopting is our mas and and so of I think the beginning of this month ECDSA is the second signing algorithm after RSA sha-1 please people change "
  },
  {
    "startTime": "01:42:45",
    "text": "that replacing our say sha-256 as second popular signing algorithm so that\u0027s actually a good news this is getting adopted pretty quickly in combat and org if we look at them to the 2t of these that have the largest number of sign domains which is Tata now under 2 C and the picture is a little bit different and as you can see that adoption of ECDSA Peter 5:6 is still quite low it\u0027s only a fraction of the total number of sign domains but one takeaway here is that especially the C is doing really well right no RSA sha-1 it\u0027s all our sha-256 so kudos that it actually makes way more sense and they\u0027re actually going to switch to signature algorithm for their TLD to our sha-256 as well and at the end of this month in the Danelle also for a lesser session one then in common an org but also only a little bit of ECDSA so a take away is the early large-scale adopters of DNS SEC take longer to get a significant share of SEC signed domains it\u0027s not surprising but it also means that replacing signature algorithms will take time also because replacing signature algorithm is actually difficult if we look at the Alexa top 1 million completely different graph also quite interesting here you can see again that there\u0027s quite a bit of adoption of ACDs AP 2 5 6 22% of the electorate of 1 million signed domains in about 1.7 percent of them are signed use ECDSA and 61 percent of those use cloud fair so there\u0027s actually also quite a significant number that are not using cloud thread for lists are using another operator that\u0027s interesting right so I get to my conclusions the first main takeaway from the paper and from this talk is that ECC is sufficiently performant for use in DNA SEC right we can easily validate the number of signatures that we would need to validate if everybody deployed the intersect with ECC on a single CPU core and still have room to spare so my recommendation is going to be that operator should switch to ECDSA for signing if you haven\u0027t deployed the in a sec yet don\u0027t bother deploying RSA please go for ECC algorithms straight away because it gives you all of the benefits of ACC which is smaller than a speck eights which means no fragmentation we show that in an earlier paper fragmentation is fully gone and much less amplification and if you combine that with some of the newer ways to tackle amplification attacks then switching the combination of switching to ECC and deploying those countermeasures will actually make your domain unattractive to abuse in amplification attacks finally resolver operators may want to look at deploying newer optimized crypto libraries to have some CPU to spare you can do this even with long term support "
  },
  {
    "startTime": "01:45:46",
    "text": "versions of the software but if you want to save on CPU cycles you might want to deploy newer libraries and finally as I showed you in the last couple of slides adoption is slowly taking off and with that I get to the end I would like to thank my students who who helped me with this so it\u0027s Kasper Hagerman who just started his PhD in at the albergue in Denmark Bruce and JJ who helped with the ARMA mips benchmarks and the data for the adoption was supplied by the open Intel project have a look at the URL if you\u0027re interested and the references to the papers as I said are included in a PDF of the full slide deck thank you for your time so any questions I\u0027m freedom from NIST so we we have been working on PGP sec implementations which also uses ECDSA P 256 and listed together with a SBIR contract the company\u0027s name is a antara technique so we have as together we have developed a high-performance implementation of PGP SEC and in that we we did I mean it was mainly our SBIR contractor anchor a technique and his name is mammoth it earlier so excellent work you should look at that we we presented that paper at Nanak 69 in February this year it will say BGP SEC a high-performance PGP SEC implementation but it has a lot of measurement details about ECDSA performance and we use DAC we compared it with the OpenSSL 1.1 dot 0 and significant I mean several several I mean significant multiplication factor improvement of this high-performance implementation compared to open SSL 1.1 dot 0 so I\u0027ll be happy to give you the pointer to that paper ok thanks but that was actually the paper I was referring to when I said that there might be an even faster implementation of p2 five six but thank you and it\u0027s great work thank you thank you for that and thanks to least for sponsoring that West heard Icarus I thinks this is a really good measurement work and I enjoyed seeing the results quite a bit you know the this sad thing so two things first off the sad thing is that you know you are providing guidance to signers but it\u0027s but it\u0027s really the resolvers don\u0027t have a choice right so so if there is like a sudden uptake of DNS SEC usage and you know that could eventually blow through the roof and if everybody used to looked at curve you could still imagine that you know although you you stayed at the end that everything\u0027s okay right now based on the current growth of DNS SEC you could you "
  },
  {
    "startTime": "01:48:47",
    "text": "can imagine the case where it actually might be a problem down in the future you know based on the fact that they\u0027re they\u0027re still significantly slower and that actually brings me to my next point which is it would actually be interesting to look at you know what DNS SEC adoption rate might combined with you know the CPU increase in speed and and have this early warning system of we\u0027re headed for danger because all of a sudden the resolvers are not gonna get it you know if all of a sudden some major you know GoDaddy or something you know started signing everything I don\u0027t know I wouldn\u0027t I would HIGHLY encourage go date to do that and stop judging people money for for using Venus ik so there\u0027s actually some discussion about that in the paper and in the paper we argue that even if you go for a worst-case scenario so we\u0027re we we take away all of the measurements and we just model we assume that the model is is accurate enough to put in worst case parameters you it\u0027s arguable whether or everybody has the CPUs that can deal with this right but that that\u0027s an assumption and that\u0027s an assumption that we cannot not prove or disprove but even if you put in worst case data then with the way the DNS currently looks I am confident enough to say that every resolvers would be able to handle the signature validations however if suddenly the new gTLD has become wildly popular and we see a fragmentation in the namespace then this picture might change because that might blow up the number of cache misses which might blow up the number of signature validations and then we are in unknown territory but it doesn\u0027t seem like the new g2u these are that wildly popular so I\u0027m not too worried about that yet but there are and these are actually discussed in the paper so there are some situations where our assertion that we can deal with these signature validations where they might not hold and this is one of those situations the other one which is also discussed in the paper is about what did it do not how can I just reopen that oh yeah okay there we go one of the other things that we discussed in the paper is a denial of service attack where you try to cause cpu starvation by forcing signature validations and this is actually something that was that Polaris man who was then still at Comcast brought up and we actually verified this so that\u0027s also in the paper and what we did was we forced resolvers to do lots of ECDSA signature validations by making them verify signatures for cloud layers sort of black life thing that gives you a fresh signature for every NX domain "
  },
  {
    "startTime": "01:51:48",
    "text": "that can kill bind easily unbound survives so there is an issue there but we also sketch an idea of how you could solve this in resolve or implementations by doing some form of rate limiting and and I know I\u0027ve discussed this with router from an outlet labs who thought it might be feasible to do that I don\u0027t know whether he has had time to implement it yet does that answer your question okay dear defendant congratulations with your work I still want to reflect on some of your results well you made some analysis of the you made some analysis of the number of signatures validations done by unbound and and and and bind can you also continue this kind of work or reflect on this work and say something about what is the ideal if I could build a platonic or ideal resolver what are the number of signatures I have to fill a date to come up with an answer okay it\u0027s not the measurements actually but it\u0027s more than theoretical approach and that can give maybe some guidance to this operative oh yeah learned opponent thank you for your kind words an interesting question this idea of designing a platonic resolver I like that I really like the term so I\u0027m gonna have to steal that from you and use it somewhere that\u0027s actually interesting because as the paper states there\u0027s a huge difference between bind and unbound but there are many factors that play a role in that right a platonic reserve or resolver what would you need to specify requirements what that does does that give you does that work the hardest to get you an answer does that find some middle ground between spending time finding something and then deciding it can be found those are questions that would need to be answered but I think that\u0027s interesting research and so we have a PhD student in our lab who\u0027s working on resolver things and this is yet another idea to put in his head we can make explicit decisions ya know and I think I agree so there\u0027s actually in my opinion far too little research on what is optimal resolver and for instance we can talk about caching strategies we can talk about time spent finding responses I think that yes I agree with you there is interesting research to be done there thank you thank you very much bilham tulip internet loves so very nice work in very interesting definitely "
  },
  {
    "startTime": "01:54:48",
    "text": "quality work and it\u0027s very important to find the right measurement metric so I think it\u0027s very nice that you identified those factors but however there are some factors which are under exposed in now a current scientific world and you know there\u0027s this single factor which is of paramount importance and I think you know what I\u0027m talking about and you know it has driven many scientists to do madness it\u0027s it\u0027s a difficult beast to capture I recognized it so I don\u0027t expect that you did attempt but have you looked into the jelly Maori Factory and so that\u0027s that\u0027s a really good question and and and yes it did nearly drive me insane but I have a backup slide on that so we actually we actually measured the legality you can see in the graph here we measured the Galle more free factor and I\u0027ll talk you through what you see in the in the graph here so what we did was we measured the callee more free factor in micro more freeze for queries per second leading to ECC validation well I think that William already said this everybody knows this factor so we you see the initial ramp down right which is what you would expect to see typically in a galley more free distribution and then some normal noise but then there is this weird peak an unexpected rise around 425 queries per second which we can\u0027t explain right so and and that has puzzled us for for for the past year and a half and if you know what causes that I I would I would like to I would like to know right so and we see this for unbound and for bind and and there\u0027s also the suspicious absence of noise beyond 450 queries per second so help us so quantum factors actually that influence the micro murphy\u0027s a second sure it\u0027s actually a different measurement unit every time right sir it could be electrical variation from satellite deborah\u0027s or the static from nylon and we\u0027re but I suspect that this will be the micro mischeif assignments okay but anyway I\u0027m very impressed impressed that you managed to capture this so it\u0027s "
  },
  {
    "startTime": "01:57:49",
    "text": "definitely it surprised me that that someone would win twice the network research applied into a research price but now I understand violence because you know thank you i concerned that the audience is wondering if they\u0027ve gone out of their minds about this so we should explain that Willam has been challenged and now he\u0027s taken rolling into a challenge to to use the word galimov free at three mic lines and this is a very successful version of that as well as a you know a deep enlightenment of an important scientific factor it\u0027s good and micro factor and and and i think i am our our play to surprise you so let\u0027s go back to the serious questions in this case unless matter once you ask me a question about the galley most refactor map not with in on that labs can we go back to your ECC adoption slide yeah which one keep going that one no yeah maybe it was the one you said yeah why not so anyway that\u0027ll do ash sorry yeah I\u0027m trying to understand this because it looks like there was a lot of growth around this time last year and then it\u0027s been actually been pretty flat in terms of ECC adoption since then do you have any reflections on yes I do so let me see if the pointer on this works what you\u0027re referring to is is here where there is especially not org it\u0027s very steep this is very actually in look at that in detail what happened there and this is one Operator in particular who who was using RSA sha-256 before and they decided on their own and to switch to ac/dc ECDSA p25 six for all the right reasons because they wanted to reduce their packet sizes they wanted to increase the security of their signature algorithms I think I I\u0027m trying to struggle onto him remember the name I was domain names show up it\u0027s a Norwegian company and I can mention their name because I had some email communication with them and they they did this it\u0027s actually not been flat it\u0027s kind of hard to see in the in the graphs here if you if you look at the slice you can zoom in a little so there is still some adoption and and it\u0027s still increasing slowly but the the large peak was when this one organization decided to switch and then later on there\u0027s a smaller bump that you see if you zoom a little bit into the in on the graph there\u0027s another small bump which is another algorithm rollover and actually I brought these people because I monitored their Okrand rollover they did it completely correctly right so those of you familiar with the NSX will know that this is difficult to give a "
  },
  {
    "startTime": "02:00:50",
    "text": "little bit of idea to the people that are less familiar an algorithm rollover requires you to take those specific steps which is to introduce new signatures first before you introduce new key because the NSA actually has a provision that if something is signed with a certain algorithm so if key for a certain algorithm exists signatures for that algorithm must also exist because otherwise you\u0027d be able to perform downgrade attacks and most resolver implementations take that quite strictly so if you don\u0027t follow those steps in an algorithm all over your domain goes into goes bogus and resolvers will actually refuse to validate it so they did this correctly and I think that was the first example at scale of switching to a different algorithm that was done successfully because we\u0027ve seen algorithm rollovers for two days and CCT of these in the past and almost all of them had some hitch where they for instance introduced the key at the wrong time or made another mistake introduced at the yes at the wrong time so this really is something that needs attention and to finish off with that there\u0027s sort of as I said the Swedish registry is going to switch from using our session 1 to our sha-256 4.2 C and we\u0027ll actually be measuring that I started this other project called root canary which we presented at the Mapuche meeting earlier this week and this the suite actually came to us and said can you measure our algorithm below for because we\u0027re kind of scared that something might go wrong so we\u0027re actually gonna measure that and see how that works in practice so we get a much more detailed measurement and we have for this particular instance where we only have a granularity of one day intervals well thank you very much this is great work and congratulations on winning this prize the second time I should say that blow lands now been invited to join the selection committee for the AARP so no good deed goes unpunished thank you any other questions ok thank you for your time thanks everybody I think we should end on this high note and so you know please feel free to cluster up here and ask any questions or get involved further and oh I wanted to show you the cat who was working so let me quickly do that and then well and on that note this does anyone else have any questions of a general nature before we go do you when you\u0027re trying to do things you can\u0027t alright okay so go out and write papers "
  },
  {
    "startTime": "02:03:55",
    "text": "the way this cat owl does and and send them here and participate in IRT F thank you so we\u0027ll give everybody back some time I think we have to do a presentation of certificates which we don\u0027t have to do in front of everybody all up do we have certificates to present to oh we oh sorry yes so we we were going to actually if you want to stay and work in here you can see the the honor the presentation of the prize certificates and the photographing of same but thanks very much for for being with us we\u0027re really very good session "
  },
  {
    "startTime": "02:07:05",
    "text": "okay "
  }
]