[
  {
    "startTime": "00:00:47",
    "text": "it's almost it's 1 o'clock Pacific, so we should be ready to start. Just wanna make sure that folks that are remote, they can hear me. Just making sure that the logistics are working fine. Hi, Sanjay. Hello? Hi, Do you hear me? Yes. I I hear you. Yes. And I'm I'm likewise, I I hope everybody is able to hear me. Yes? Yes. Hey, Sanjay. Sorry. I'm having some audio difficulty. that they have, you know. hear you, Kevin. Okay. Great. Yep. Very good. So I I think why don't we get started? It's 1 o'clock. We have busy schedule here. let's see here. Does everybody see the full screen of the slide? Alright. Let's move to the next slide. Next page. Okay. Page number 2 makes it easy. Okay. So"
  },
  {
    "startTime": "00:02:00",
    "text": "Not well. And you can see the policies that are outlined here. Number 1, as a reminder, by participating in the IETF, you agree to follow IETF processes and policies, And if you're aware that any IETF contribution covered by patents or patent applications that are owned or controlled by you or your comp or your sponsor, you must disclose that fact. or not participate in the discussion. Also, as a participant in or attendee, to any of the IETF activity, you acknowledge that written audio, video, photographic, records of meeting may be made public. personal information that you provide to IETF will be handled in accordance with the IETF privacy statement. and then you can see the rest of the details there and the BCPs that you can follow through. So please note well. and note really well. These are some of the additional policies that of point to making sure that we all are at our best behavior and back to each other. So please follow the they expected professional standards and demonstrate your appropriate workplace behavior. Alright. With that said, I'm gonna move on to the next slide. These are some of the tips for remote participants and for in person. One important thing is that everybody that is in the room, make sure that you please click on the the sign on sheet, which I know I'm not showing right now. And in in fact, there should be a clip If you can pass on the clip clipboard, Okay. Very good. So make sure that everybody, you know, sign in and also remote. Please make sure that When you use your online tool, you're automatically signed in, so you're good. Okay."
  },
  {
    "startTime": "00:04:03",
    "text": "Next slide, this is just the references. So I'm just skip over these quickly. and Where is my slide? not showing me here. So the working group mile zone. So these are the the active working group documents, and these are some of the milestones that have been adjusted So we are and, obviously, These can be adjusted in the sense that if if the working group draft is ready to move forward with, then that can move forward they are not tight necessarily to these dates here. If you're moving forward, fastly, that faster, then that's fine. the first one we have right now is that and they submit the CDNI extensions to the HTTPS Academy Star in August, Did somebody had a question? That that was me, Sanjay. I was just gonna mention with respect to Acme Star. I think we had the working group last call I I believe we're ready to submit it to the IASG. I have I updated the status to waiting for the Shepherd write up I have the shepherd write up done. I'm just waiting for Frederick to get back from vacation and confirm that there's no IPR per the note well. and then we will get that submitted on time in August Excellent. And then there are a couple of other documents that we have that are moving fast. 1 is the The the the next one is the CDN extend CDNI sessions for capacity capability, advertisement document. Right now, we're showing that to be submitted to the IESG by December of 2023. obviously, that can accelerate sooner. And then the next one we have is the https TLS tlsub certification."
  },
  {
    "startTime": "00:06:01",
    "text": "subset delegation document that is targeted for February of 2024. And likewise, the RFC 8007 bis is also scheduled for February 2024. And then that is based on the current working group drafts. and then reach order or dissolve and make that decision in November 2024. Any questions here? Okay. If not, I'm gonna move to the next slide. And as you can see, it's pretty trammed. we have today, we have 3 documents that are working group documents currently, we're gonna go through those three firsts, And then we've we have about 4 documents that are submitted into the working group. So we're gonna discuss those and potentially see if they can be adopted into the working group. And then we have also allocated time for open mic. there are 2 major topics there. 1 is the logging extension that Ben Would like some time on it. about 15 minutes, and then we also have request from Alan who wants to talk about the request routing interface specifications. So we will cover those two topics. So without a further ado, I'm gonna go back to The first item that we have on the working group documents the CDNI capacity advertisement extension and gonna give it to Ben to run with it, and then we'll come back to near on the c d CDN Outriggers, the RFC 8007 biz, And then thirdly, we'll have a a presentation from Christophe on delegated credentials, and then we move on to the other drafts. So In the chat, Ben is apparently having trouble connecting. I don't know if we have someone from me who might be able to help him out. we may need to we need to switch up the order if Ben is unable to"
  },
  {
    "startTime": "00:08:00",
    "text": "joined immediately. Okay. Let's see. I don't see Chris Kristoff is there. Kristoff, do you do you want to go or Neil near? any any one of you who's willing to I I can go now. you can go, okay, Nir. Alright. Very good. Doing well. Yes. Yep. Okay. Hi, all. Let me see if I can bring up your document near Okay. Okay. Let's start. a this isn't the not the first time we talk about the CD and I trigger's interface. And second edition, It out, Goal in this meeting is reaching work Copless Call. We will have a quick reminder about I have c807, the 1st edition, and the 2nd edition we already presented. we will then go over 3 revisions and new to newer visions, made for the for the for the for the draft. their their vision additive Each one brings another an additional value. Well, I we then have few minutes for discussion, and, hopefully, select 1 of the versions, as for working group less call. And Let's proceed. So"
  },
  {
    "startTime": "00:10:00",
    "text": "I see 8807 Defined control interface and triggers and triggers which publicly allows an Obsidian an Upstream Sedan to manage a content or metadata between the town's incident. and it defines the exact protocol, creating 3 gears, canceling 3 gears, and the objects that and monitor triggers and all object I used all around. and expressed a And, practically, in the second a decent, what will we change the clear your object's capture, adding much flexibility to it. and extendability. and avoiding the new and and they need to a defining you to your object every time a we need another way to select content to run or anything like that. We also better dealt with the Aero propagation, when in situation of cascaded CDNs, and in in in it is important to note that we Did not do any Changing the protocol itself. Okay. the to this protocol, the generation, the cancellation, the API, a part of the objects The entire flow looks the same. and it was very important to have to preserve this flow and just a and create the new objects and use them. next So this slide, this kind of part the the change we did in the video a structure, take the object structure,"
  },
  {
    "startTime": "00:12:01",
    "text": "The 3 door object was built of a type. which said Packaging what to do, It's, for example, preposition. it then it then had a list of a set of properties. a to select room to run on the targets of the trigger, and the CDN test which is useful It's a list of CDN that he is useful. Look protection, detection, detection, what we did what we did here is we knew renamed. the type to accent, we took all the properties and they put in them in the list of trigger specs, a generic trigger specs. We added the disputed extensions which give further a execution instruction, but it's practically out. to 280 Randi Twilio, for example, And Do it in certain hour. I will also and we kept the serial Okay. against So as I said, the trigger type was was renamed to trigger action. Next, And this is all just reminders for the what we presented so far. We added the generic with with the spec. that's basically a specify the target to execute the trigger ons, on it as 8, practically 2 the the speakers, 2 main items which is first one is day. how to run asset out how to select their content For example, by URL, by registers or something like that. and the second part is to take a subject which which specify whether we run on a metadata or content."
  },
  {
    "startTime": "00:14:01",
    "text": "It's important to know that those 2, The we added a a a set of capabilities and they the FCI packages suppose they specify it will just scope. For example, we support a quick position of made of content, using URLs. So this we added this to we're adding this to the X Two supported by the FCX. Next. Okay. And the extensions are there. How good for the instruction for the the execution and type policy, location policy, next And let's thing we did there as we changed the trigger object also needed to change the l object and the status object. because they maintain an internal technology container within that. So That's I think that's it for Okay. That that's it for version for it to recap. next. So what we did in version 5 and the 6, me mainly started with the an overall core instance check and alignment of the entire document. It was mainly the document was we started with the But reaching out at I have c 807 and made a lot of changes operate in augmented the draft that OEM Sanjay I worked on initially. So a we Verify, it's all current current. we eliminated the DVDs and loose ends there that we have"
  },
  {
    "startTime": "00:16:00",
    "text": "We added new registers analysis for the 2 suspects, to the subject, to the extensions, In this contents, context important to know that day, day, day, We didn't add a a trigger trigger action in the registry because we already had the type of registry, which is We just renamed x a tags to action. So we will keep on using the all the registering a defined RCA tools, sir. Next, please. Okay. So what year are we for the Genexpect object, as I said, we we added relevant registers, AI analysis, and also added new error code. a they all cause a allowing the access in the dumpster sedan. to a specify it too. indicate. it couldn't in pass, the trigger to to an answer mirror spec. financing it. Send me a subject of issue with the actual passing of the spec. And yeah. So we added those 2 error codes. there's all there are also registered. They are also appearing and the o h s 3? Yeah. The entire of the all call all list. Yes. was it? Go ahead, Raju. Yeah. Finty Connex. I just wanted to confirm. in the new error code that you've registered, You are allowing the downstream CDN to indicate an unsupported specification"
  },
  {
    "startTime": "00:18:01",
    "text": "does that necessarily also cover cases where a certain extension is requested but, you know, that extension either could not be supported or or is supported but could not be successfully executed. An example for this would be say an upstream has requested if if preposition with a time extension, saying that, okay, I want it to be done within this time period. Yeah. And now she manages to complete trigger partially. So I download some of the assets within that time but I can't complete the entire set of assets. within the time window specified. So do we have some examples around how you would indicate an error like that. Yeah. We also defined the e extension error code. it it is not new to for this task. It already it was already defined in built in OFO. so I didn't mention it here. I just this is part of the Korean c fixes that we and figure out those error codes are missing. but the extent extension in all calls is already there. Okay. So, yeah, so My question was in cases where it's not really an error, but it's it's kind of running out of bounds of the constraints specified in the extension. So it it's not like it's are time that isn't supported, But I -- It's more like a warning. -- but I couldn't complete it within that time. So how would I indicate an error like that? I don't have an exact I I we I don't remember actually. I and I'll need to get back to you. That way, we'll talk we'll talk about it. I I remember the discussion. I don't remember the conclusion."
  },
  {
    "startTime": "00:20:05",
    "text": "So I need to get back to you in that. Yeah. So so maybe we might want to maybe add an example around these kind of use cases in the of text so that we have some guidance around that. Okay. Okay. Let's proceed because we don't have enough much time that day, Okay. Next slide, please. Okay. So the extension object we just day. edit the registry for the to the extension, the the location policy and dashboards and such. Next, and an an additional change in this draft, is adding a Status tuition a reason to the stats object, it didn't appear previously. If they It's human readable a field It's optional one. And and we figure out it it is needed for a day to day extensions. Okay? So example, if you have an extension saying, a this this should run in his a a specific time, it triggers to join a specific specific time. so we can give an indication that the a trigger standing due to the term policy. Next page. Okay. I think this one is actually the most important change we did in the in the in the in the revision in the revision 6. And till now we discussed version 2 of the API. This was the time we used the And when we discuss this, I see it is this draft, the second edition,"
  },
  {
    "startTime": "00:22:02",
    "text": "And realized we are not actually changing the API. We are we are just change it. changing objects. And So We are the the high level object the the man the meme that that are sent within the and GTP request. we actually gave the version to this object. We And this we are we now have and we originally have the CI trigger command object. We're now having the v 2 for it. and we have the CI table status concept. You know, we had it and now we have every 2 for these objects as well. internal properties within those there's object that's not does not have the Victor Suffolk because Once you are in this object, it's the it's imply that we are using the think, o v 2. structured, And it's we also wanted to note here that a We they object versions are now can now be published the the supporting or supported object versions can now be a published via the FCI. We have a capability object for that. Next, please. Now As I said, we we took the CI to the command and and added the video and now create it as set to the command v Two and then set to the status v Two. And it's important to note that those objects are paired. Okay. If one is you and is creating the trigger, using the shared reader command Vithu, the status services that they will be created is a v 2 status. and it's it's tensor in"
  },
  {
    "startTime": "00:24:01",
    "text": "in our the way I see receipt it will stand for further version as well. if someone will define a victory of the of the l trigger victory and the the command command with 3, it also have would also have a status 33, And, also, actually, we have they, backward compatibility. of RC80s 807 that we have the command without the v 2 and the that creates the status. speak out. Maybe 2. So the command if the command object and the till the status object are you know, version impaired. a On the other end, in the country, the clear collection object. which is used to let's give for for absence again to get a list of to the pathways of all his triggers, This one is not it's it's not this object is not version. So once a an absentee then request a list of a 3 is list of triggers it will get the list of passes that some main point of a v 2 trigger and some main point of to a v 1 ticket. Depending, how the app's incident created taken So, therefore, the we didn't marked as a version They say I take a collection object it's we actually did not change this object. Next, please. Okay. If we have questions so far, it's"
  },
  {
    "startTime": "00:26:01",
    "text": "a good time to ask. Otherwise, I would like to proceed to 1000 7. I have questions. So you're saying that that, actually, v 1 and v 2 can be mixed and matched how we manage and managing backwork and visibility. understand you're saying that the API is you're saying? They are not mixed. The the version once you when you use an object of video, create it's fair to create a start of your V Two. but we are not forcing the access to them to use only one version. of the of the API. of the objects, for backward compatibility. So same UCDN and BCDN may actually exchange the 1 and retrieve b2b2triggers? kind of I think that may create kind of semantic issues there. So This is my first question or comment. And second is How we actually managing network compatibility. and then v 1 and v 2. Again, API kind of it's only half half of, you know, a solution that, okay, API is compatible between objects cannot be parsed. It isn't, in fact, the new version of protocol. No. It's so the The dumpster sedan announce via the SEI, which version it support. The absence at the end chooses. how to create the triggers. And the downstairs and then works a accordingly. So if the absence today users, they I said it. Okay. So I wasn't missing your point here. Yeah. But okay. I understand. I I look at this as 2 separate APIs and"
  },
  {
    "startTime": "00:28:03",
    "text": "the absence today and we'll use whatever whatever it wants. Right. I kind of do we want to really allow kind of what you've seen in the CDM? exchange 2 versions of triggers. Horssein. Is is it really helpful, or is it more of a complication that I think it's more of a complication, but I don't want to force, the moving. I I don't know what the status of uses usage, usage, of this API, I assume it's not been used it currently a lot or nothing like that, but they I don't want to assume that. So I want to keep on supporting the old version. And, actually, the downstream may announce its support only version 2 or only version 9 or behavior. And by announcing its support via the FCI that supports only version 2. it can protects it it protects itself. Okay. So DCDN can potentially avoid cementic applications by just choosing v 2, for example, not not for you Yes. Support if you want it, they don't have to. I kind of but we they want to look at basically at what if you do allow that, we want Andrew to what happens if you send preposition v 2 triggers, but then you send the invalidation v 1 triggers. is something that will work. Do we have do we need to clarify things? They they are independent. They are credit to independent figures. why should they effect on another. if you're storing if you're storing again, if you kind of I you have a way for DCDN just to say we too only, and that's it. I resolve that."
  },
  {
    "startTime": "00:30:02",
    "text": "No. No. No. But but but I'll give you those value in knowing that. Just This is I I preposition the content using v one object, for example. And then I want to invalidate the same content using v 2 object. heavy to trigger. It can be done. It's the the list of we can specify the same content. It's it's the same they the action limitation of contact specification, it's always the same. We just change the structure, it appears back. So they they We have the same flexibility. Yeah. I'm saying that we too potentially can be used with extension policies to propagate more information about the content that can be used later birch. And if you -- Yeah. -- kind of allowing but kind of v 1 v one that will break not work with something the proposition with V2. think that that is -- Yeah. -- something that I did Okay. So you would suggest the that once a v Two is I actually not not sure it comes up only the what happens when we define a new extension, for example? it may cause the same issues that you define that you mentioned here. Okay. And I'm I'm doing and then this extension should support kind of all all operations. Right? So it's something that would be consistent. But if I'm kind of very kind of think about it. It should be 2. It should be 11. Right? Does that like, you you're allowing within same connection argue it's not a connection. I understand. But kind of within same interaction, you're allowing to different semantics. And, again, I'm just saying,"
  },
  {
    "startTime": "00:32:00",
    "text": "rather than trying to resolve that or or actually delegated to DCDM. Hey. DCDM, you figure out it can afford to deal with that. they would be kinda because I don't think there's a lot of value in keeping those too. Like, either you have if if you have v 1 customers that you already integrated with, work with them on z1. with other UCDS work on the 2. But we're saying, you know, working with the thing with the UCDM in both levels, I don't it as any value even those who have implemented that. And I agree. I agree. I agree. We can say say that we can make a comment on that. to the RC that with at least to a comment or something. Yeah. Yep. Okay. Okay. Next, Okay. So version 7 In version 7, we talked to the split the error Okay. We splitted the trigger and cancel commands object And please So so far, the as defined last year, 8:0:7, they CI trigger common object to add add a 3 year, cancel, and sedan plus properties. So when you wanted to create a trigger, use the trigger property, and then you wanted to cancel for it. hvgere use the cancer property with a list of with asbestos, and it just didn't make sense At least to me, to use the same objects with instead of having separate objects that can grow go and if a values of of the neuron. So what we did, Dale, next what we suggest in version a 7 Next, please."
  },
  {
    "startTime": "00:34:00",
    "text": "Okay. It's to create a trigger object and a cans and a trigger opposite object and cancellation object. So the we define the CI trigger command trigger v 2. It's and it's a v 2 because it will create a status v 2 object. which is the a trigger property and the CDN plus probability that is used for loop detection or loop avoidance. Next, And we define the cancel object, which is which is not version because it can cancel trigger's phone. different versions, and it holds a list of trigger patterns and is used for cancelling triggers. We didn't include the city and pass it here, and as we We believe that loop detection here is not really relevant. You want to cancel the trigger that was already created. and loop detection was already tested for heat. and Okay. And and what do you and if there is a little piece a little issue what actually it means and would we do? You want to cancel it? We go. So We didn't include the city and pass it. So this is a version 7. and the In version in version 8, a we took the cancel object, and instead of posting it. on the collection city absent city in collection part. Next. We use We post it on the actual trigger pass. it is captured whether to be canceled. It"
  },
  {
    "startTime": "00:36:00",
    "text": "Next, please. So we took I trigger For example, if we have this path with the 1 to 3 and then a If you feel that this is the pass of the trigger, we send and post the constellation object pond on on on this path And cancel the trigger, We lose here the ability to do But, Google cancellation. tickets cancel many triggers once, once, a From my standpoint of view, I don't sure it's really needed bumpers, bumpers, bumpers, bumpers, bumpers, bumpers, bumpers, bumpers, bumpers, bumpers, well cancellation. It's just moving the complexity from the options sit in to the dumpster sit in. of managing the list and I think it complicates the API. So what would you apply one trigger was canceled, and that it couldn't really cancel the other one. thing like that. So I think it the most straightforward to Keep to to remove this bulk aggregate cancellation ability. So And that's it. So if taking more time to learn finish. Probably, shouldn't Hello. Question doing today. the session itself. So that's it. I I would like to get to point of view, maybe later offline and like, they whether we go with They vote in 8. with the no API or not, and the milestone we wanted to reach is what a blessed call. So I hope you can I have a question here. So so there there is suggestion that this graph will go to the working group last call."
  },
  {
    "startTime": "00:38:03",
    "text": "Yeah. One of those, but I I would they recommend version 8, but day. Assuming the the changes we did are acceptable I kind of our think -- I think I may have -- -- for you Actually, in the in the cash management project in this VTA, we've been working with v 2 extensions, and there is kind of quite a bit of changes that we would like to add. So, certainly, if this is not closed, we want to introduce that to think will be ready to submit that sometime kinda before the next IETF, but there's certainly more things. We're adding policies policy extensions. There are some changes there. So so so that's really good basis for kind of the meet needs to be to work that you've done is is outstanding and really needs our needs. But we actually started using this and we would like those changes incorporated So our proposal is for planning purposes to discuss that and see how the changes are. incorporated, and they in know, before we close this off, Yeah. I I don't think we're ready for last call just yet. I think we need more folks to actually review the current versions of the drafts, there are some threads on the mailing list that We'd like to see a little bit more discussion on I I need to respond to near as well. I think we are excited that you are actually implementing this and have ideas that is helpful, that is always the best type of feedback. So We definitely are interested in incorporating that. But, yeah, I think folks should go and read the drafts. I think it was the work that's been done has been great, as you mentioned, in in near and Sanjay have been doing some you know, some major overhaul here, which which for the better. So I think we should just go and and take this offline and and take this to the list."
  },
  {
    "startTime": "00:40:01",
    "text": "and consider what we have thus far. Sounds good. Yeah. I I think that that's a good good suggestion. So keeping I think we've run a little bit over here now. And I so let's let's continue the discussion on the mailing list for this one. Now I see that Ben is online. I'm glad Ben that you're able to connect. Are you ready for your section. Should I bring up your slides? Ed, I'm on the I'm on my phone, so this might be a little awkward. hopefully, this will work out. unfortunately, none of my computers were able to join me to echo for whatever reason. Mhmm. But I think you you're coming on fine fine. Okay. and I have your slides up then. So Okay. Perfect. Thank you. Alright. I guess we're starting with capacity. So this is the first few slides here are gonna be familiar to anyone who's attended the past few ITFcdni sessions. The capacity extensions, are essentially a way to communicate both to advertise limits from the downstream to the upstream and to also provide telemetry so that the up stream can make appropriate routing decisions, traffic delegation decisions, based off the level of perceived traffic on the downstream side. and go to the next slide. this is just a brief example of what the the advertisement looks like. don't think we need to go through this line by line. just so you can get an idea There are a number of dimensions on which you can advertise limits, things like network egress,"
  },
  {
    "startTime": "00:42:00",
    "text": "request rates, and storage object counts and and utilization. We also have a soft and a hard limit the soft limit is advisory. the hard is the limit after which you should absolutely no longer be delegating traffic. not shown here in the example. is the current property to advertise a current value for telemetry that does not require going to the external telemetry source. Next slide, please. So we had quite a few changes from the previous revision. In large part, thanks to Kevin, because he did a a a very thorough review finding basically everything that was wrong with that draft. obviously, not everything because the I think, Kevin, you just I saw an email from you. I haven't gone through it yet, though. with some more fixes, but I think we're we're getting pretty close with this draft. to a final state. So the biggest remaining question that I have for this is whether we need and Ayanna Registry for the metric types and the and the limit types. And to me, the registry is only really necessary. if these things are gonna be utilized, outside of this draft. by other things, And I'm not sure if that's the case. I think these values are kind of internal to the capacity dentions, And the only The only time I would see them being Add it to is if we had another revision to capacity or we had some draft that superseded disc capacity extensions, draft. So if that's the case, do we actually need a registry for these types? So that that's an open question. I guess, to"
  },
  {
    "startTime": "00:44:03",
    "text": "to Sanjay and Kevin or whoever is an expert on that topic. Yeah. I I sent a I sent some comments earlier. mostly knits. And this question about the Ayana registries, I think that is the big one. if if if we don't think there's a need to have others be able to extend that, n and and to find their own for interoperability purposes, then we probably don't need one. I'm I'm I'm look to you guys as the implementers whether you think that's necessary. If you don't that's fine. We we just need to be clear on that. Right. I I could I could see a potential use registry, for telemetry types, because it's possible we'll have a future draft addressing telemetry in a more general way not targeted specifically at the capacity extensions, So perhaps that that could you know, be useful, for the limit types, though, I don't see those being used any case, outside of this draft. Okay. Yeah. I think it's really if you were to write a new draft and specify a new type? Do you want to have to re specify everything that was, you know, That's now that was in the previous version. Yeah. Yeah. That's fair. I don't know. I I guess my my expectation would be if we do something more with this in the future, it would be just a new revision that supersedes. whatever is approved here. which would be a full complete draft I I don't I don't really see you know, a separate draft that's just gonna add, like, you know, 1 or 2 new limit types. but but Maybe I'm not know, maybe my vision isn't broad enough to accommodate that. I would I'd open it up to the the rest of the folks"
  },
  {
    "startTime": "00:46:02",
    "text": "maybe send an email to the list just so that we can close off on this issue. Otherwise, I think the draft pretty close. Okay. Well yeah. Okay. So we'll have a brief discussion on that then. I'll address your add additional comments that you sent So maybe we'll have one more revision, and then I think do a last call, I would hope. at you know, if we can do that between IETF via the mailing list, I think I think it's reasonable if we can can get a new version out and folks can review it and and agree, then yeah. I'm I'm okay with that, Sanjay? Sanjay? Yeah. I I agree also. I think the the graph looks pretty good, I think. overall, good structure and completeness of So the draft is very, very much ready. There's some knits I also sent you some knits yesterday, Ben. on top of Kevin. So but Okay. I'll I'll yeah. I'll look for those. Yeah. Yeah. I think you you'll be able to, you know, work through those pretty easily and quickly. So so the I think the only question is that I'll I'll take a look at the draft again from the point of view, and, you know, if we give my feedback on the mailing list, but Outside of, then you clearing up, you know, these nets looks like, you know, between now and then the before the IT, we should be able to move forward. certainly with this one. Okay, sounds like a plan. Yeah. Thank you, Ben. -- working group who has any thoughts or comments, read the draft, and send them to the list. Thank you. Alright. So I think next off is Christophe, if I can bring out this document. Okay. So the next prompt, yeah, it's about delegated credentials on next slide."
  },
  {
    "startTime": "00:48:04",
    "text": "So the draft specifies how One in mind, one FCI object and which allows to use HTTP as dedication with dedicated credential in CDNI. So the actual actually, the delegated credentials they just put in our RC RFC numbers. So this is something that I need to update in draft. And and the graph defense 2 object So the FCI dedicated credentials MD in my dedicated credentials. So next one. So he see 2 examples for those 2 objects. of the FCI dedicated credentials is mostly there to amounts the maximum number of delegated credential that the downstream CDN supports. And and the MI dedicated credential just an array of dedicated credentials where the each allocated credential is encoded in, well, in base 64. or not. And, yeah, that that's that's it, basically, quite simple. next line. So there are so Kevin sent out a lot of comments, and So there was a lot of typos reformulation and polishing of text in the new version. There was some clarifications regarding mast And Shop. should So to typically, one was about keeping trap track of the certificates of the dedicated credential in the expiration times and the refreshing of of those dedicated credentials. So Those are should because there might be reasons where the upstream CDNs don't want"
  },
  {
    "startTime": "00:50:01",
    "text": "keep track of those. So typically, if you have just a single shot deployment, or when you're at the end of you are deprovisioning your downstream to the end, Another new thing is now that the encoding of those of some properties which changed a bit And also, I extended this security consideration section and explicitly stating that we shouldn't or passing private keys, typically, is dangerous and should be avoided. And that's mostly it. So I saw that there were a few more comments you sent out Kevin and Sanjay just today or yesterday. So I will have a look at them. And and then, yeah, and then the next step I don't know when if it's a working with blast call when we can target this. for discussion. I think we have someone we have folks in the queue, Rajiv. Please go ahead. Hey, Christophe. Rajiv here from pickonets. Let me confirm if you've had any chance to give out any thought to the possibility of using the new proposed secrets interface to be used as a channel to carry the keys for the certificates going through this particular interface. or through these objects. It's could be that's something that I Yeah. I I It could be a possibility, but I I think we need to discuss this. Yeah. Yeah. Did. Could be. I don't know. If you could if you could use this mechanism, and actually define that in respect somewhere that"
  },
  {
    "startTime": "00:52:04",
    "text": "while it is recommended not to transfer keys through this mechanism, a potential alternate maybe to use RFC number, So and so whatever gets assigned to the protected secrets interface. as a channel for transferring keys. I mean I mean, maybe we should currently, as it is written in this graph, in kind of out of scope how you get those private keys. So it could be outbound channels, or inbound, but we say that you should shouldn't do that. Or you have to be very cautious on that. So maybe if one day, we have an RF see your aspect on the on the protected secrets. Then we can maybe open this up again, but my approach would be that currently we kind of put it out of scope of this draft. And say that that it's some other mechanism which does it So is is is there scope in an RFC to say, hey. this is out of scope of the draft itself, but this is a potentially recommended method of doing it. while not specifying it as a requirement. No. Because then you have a reference to a document that doesn't exist yet, and that will hold up location, I think Okay. It is correct that it is out of scope, and we don't need to There's nothing here that prevents us from using it in the future. Right? But we don't have to say that now. Talking. And maybe after protecting secrets is actually published as an RFC, we can always circle back and publish an update to this specification calling that out. or it can reference this specification if it wants to make a mention as a use case. That is also possible but we don't wanna make a forward reference here. Okay. Got it."
  },
  {
    "startTime": "00:54:03",
    "text": "My comments were yeah. I I sent some more knits to the list. They're mostly knits. I think the the changes since last time are really great. We've done think we've done a lot with respect to the the security specifications around those secrets and and what the what the what the what the metadata is and what it isn't. I think we're pretty close. I would suggest that we request a sector prereview just to make sure they're okay with it. I don't see any reason why they wouldn't. It's a short draft. We can ask But otherwise, I think I think we're pretty close. Sanjay. Yeah. So I think the again, this document also has shaped up well. I I just feel that, you know, I I want to do one more review of the draft outside of the knits now just to make sure that not overlooking anything. And I agree, Kevin, that having a security review would be helpful so that as the document goes up the chain later on, it should be easier Right. We don't want we don't want it to get caught up there if if they are going to still have issue with the the the private keeping in there. But Yeah. Okay. Okay. Thank you, Christoph. Yep. Thanks. Thank you. Mhmm. Bye. Alright. So We're running few minutes late, but I think we should be able to catch up I believe, Glenn, you would be next if I can find the right slide here, metadata overview. That's the one Yep. And, hopefully, Kevin did the trick of uploading the the one that you sent, Yeah. It was just a minor formatting that but, you know, these things get get under under under people's skin. Yeah. So here we go."
  },
  {
    "startTime": "00:56:01",
    "text": "Just you go ahead next slide. Yeah. Just kind of a reminder for those who haven't, you know, been plugged into all of this within the SVTA where are driving what we call our configuration interface work which really picks up on the metadata interface of CDNI of both on the model adding lots of objects to the model, and then also creating APIs, beyond the metadata interface API that was originally specified. that's kind of just, you know, where we're coming from with SVTA. Next slide. Oh, it's not the new slide, but that's okay. It's just formatting. So This is the SVTA configuration interface. It's a 7 part document. We put out a a early version a year ago, this is a version 2 that all of the documents to see listed here should be ratified by the SVTA unpublished probably in another by IETf118. I think in September is what we're looking So we're moving these through the SVTA in pieces. this part 2, which is basically a list of metadata new metadata objects were organized by category, We've broken this up into pieces. You know, this original, we had submitted this couple years ago now, and Kevin was, like, you know, break it up into pieces. So these first three in red, the 3 we're gonna talk about today, We submitted those originally as draft for IETF 117, and we would like to come out of this meeting with a milestone of having these adopted as working group trends. this moving ahead, all of these we wanna move through and we have A couple others targeted for IETF 118. There'll be 2 more And some of these actually do make some forward references to these, but they're really only for the examples. They're not dependent on them in any way. Just the examples when combined with these new standards, the examples become more"
  },
  {
    "startTime": "00:58:01",
    "text": "clear. This is not an exhaustive list. of all the metadata objects that SVTA is adding. You already saw some come out and some other work that was presented. and logging will have some as well. whenever we decided whenever there was no other document that was a logical home, for defining metadata objects. We decided to put them here because they are in the same theme generally as the metadata objects were originally introduced in RFC 8006. Next, Yeah. And this just puts the SVTA configuration interface kind of in context of how the flow works between upstream CDNs and downstream CDNs, you know, where the Upstream CDN will query the downstream CDM for for capabilities. That's FCI, and we will be proposing an extended an FCI metadata and extended object that allows for some fine grain advertisements of capabilities, And then upstream CDN can either push metadata into the downstream CDN with what we're calling our simple API extensions on top of the RFC 806 metadata interface. And then we're also looking at some future work on an orchestration HP API with much richer capabilities and integration the TerraForm. We'll have to see how much of that we decide to move into the IET but that's the big picture of where SVTA is. going on this stuff. We can move on to the cache metadata presentation, unless there's any discussion on that. Yeah. No. I I'm I'm good. Kevin, anything you have? the goal there was just to set the stage that there are lots more socks coming. No. I'm I'm good as well. Thank you, Glenn. looking forward to it. Great. Okay. So this"
  },
  {
    "startTime": "01:00:00",
    "text": "is the version 1 update to the cache control metadata that was presented at at 117. Yeah. We've been scaped by this. So we we did yeah, we'd originally go 116. Here's 117. These are really just go beyond the relatively simple cash policy was that was defined in 8006 adding the ability to to do negative responses, set cash policies both internal to the downstream CDN in external meaning for the downstream client, the user agent typically. Yeah. So go ahead and we'll go to the next slide here. Yeah. So Kevin gave version 00 a real a row review. and we've addressed everything that Kevin brought up, this is just sort of the highlights of the bigger things that Kevin had brought up. trying to think if I wanna highlight anything particular on Yeah. There was some confusion about whether values were string or integers or enumerations, we clarified all of that. we added some examples that are as well, cleaned up a lot of language, eliminated the incorrect statement that this that this was extending in 6 it really is just new objects, So that's about that. Go ahead. I think what we wanna do here is yeah. because this is what a cash policy looks like, basically. Just setting internal and kernel values that either can be in this example, the 5 means to 5 seconds, as external as an enumerated values. Each of these internal and external properties can be one of a handful of enumerated values or a string that's interpreted as a integer number a second. What I'd like to do though, at this point, Sanjay, could you bring up the draft, actually? the previous slide? No. Actually, the draft. itself. Oh, the draft. I I"
  },
  {
    "startTime": "01:02:01",
    "text": "I think I'd put a link to it in the chat. because there's an example I wanna show. I just I didn't get a to put it into these slides, but it it illustrates a point that I think Kevin had brought up in his original review. Is that possible to bring up the I'm making you do an audible here. Or should I share my screen with it, or what's the best way Yeah. Maybe you can share your screen if you I'll do that next. We're asked to share. Yep. Yes. Oh, wait. You can get the pick which hotel I wanna share. Cool. Oh, actually, I picked the wrong Okay. What are you seeing? I see the -- Oh, that's okay. I think they're one tab, but this will be this will do, so that's fine. hold on a second. Yeah. Can you see this example here? Glenn, you could just paste the URL into this tab and load it. that'll bring the draft up in this tab that you're already sharing. Well, this is fine. Can you guys see this? Yeah. Yeah. It's hard to read, but I see the screen some Stay better. Yeah. A little bit better. Yeah. So the idea here and this is something Kevin had brought up that, you know, imagine there we go. we get a response from an origin and upstream CDN, and we wanna see if it's in in my example here, if it's a 200, response, response,"
  },
  {
    "startTime": "01:04:01",
    "text": "then set one type of cash policy. In this example, you know, it takes a successful 200 is good. So let's go ahead and cache for 300 seconds. both internally in our CDN, and externally. you know, downstream So that may be a typical, you know, 5 minute cash on a good response. However, If we get a 503 or a 504 from the origin, we may wanna just set a 5 second cash policy, and force that regardless of any cash policy that may have come from the origin. So this is an example of combining this new MI Cash policy object with processing stages, which will be introduced in the next IETF meeting. It's a separate one of those those parts I had mentioned. and that will I think address Kevin Kevin's concern about how you can set multiple different policies based different status responses. that was that for Hopefully, that worked out. Any questions or discussion on that? And, you know, we feel that that this draft there's actually one minor clarification we added in the SVTA version since we upload this. But we think this is essentially ready for you know, to be adopted by working through working through Let's go to the next one. is the edge control. Okay. And presenting this on behalf of Fonzo. He's the primary author on this, but couldn't make it. I'm the secondary author. So let's go ahead again. This will just be very quick. Next slide. Yeah. So these are a set of objects, that really define processing on the edge for cached responses the bulk of it really is around cores, policies, There's also directives about whether"
  },
  {
    "startTime": "01:06:00",
    "text": "the downstream CDN should be allowed. to compress, like, gzip compression of response that may have come from an upstream CDN uncompressed. And there's some client limited client connection control metadata that talks about time outs Next slide. So that's pretty much what I just talked about here. Next slide, go ahead. Yeah. So very minor changes from version 00. I don't believe this one ever got the serious detailed meticulous going over from Kevin that that some of the others had a could probably use either to give it that kind of pass, Kevin. But we did take the same a lot of the feedback we got on other documents applied to this. and we applied that feedback as well. We had had another again here, am I traffic type? you know, it might be a value like live streaming or vod streaming or what have you. can be interpreted as a downstream CDN, you know, that wishes is appropriate for them. We we eliminated that. That's gonna be showing up in a different documented at a different time. This was just not a great place to put that. But other than that, really very few changes from 00. Next slide. Yeah. And, really, like like I said, the bulk of the cross origin policy is is direct is for downstream CDN, on how to either generate synthetic responses you know, in the core sort of dialogues that need to go on. or to synthetically generate course response headers on behalf of the upstream CDN. And that's the bulk of it. Everything else we really talked about last time, so there's no reason rehash it here. You can just kinda go through in next. Yeah. We've we've we've discussed all this before. We handle both simple and advanced course cases, this, we talk about before. That was, you know, allowing compression, which I had mentioned next. And then client access control really just an ability to set time out values and keep alive values on client connections. for downstream."
  },
  {
    "startTime": "01:08:02",
    "text": "And I think that's that's it. So Ben will now present the 3rd in this configuration interface set, which is the protecting private features metadata. But as I said, we'd like to come out of this with you know, all three of these adopted by the working group, and then we can start, you know, moving some more drafts and That's it for me. Thank you, Adrian. So even though I finally got in, the computer, my connection seems to be really bad, and the slides aren't really loading for me. So I'm gonna reference a local copy of my slides and just trust that whatever you're seeing is what I'm seeing. So I'm gonna I'm gonna move to slide number 2. objectives. Perfect. Okay. hopefully, I have the the same version as what is what you're saying, but even if it's not, shouldn't should be okay. Yeah. We've we've got 3 bullets on the objectives. Yep. So the the the whole idea with this the secret metadata is to allow an exchange of values that should be hidden, but we we don't want to include them you know, directly in the the advertisement or the configuration metadata. So it's a bidirectional mechanism to allow the upstream to tell the downstream or the downstream to tell the upstream. hey. Here's a, you know, access token or here's a encryption key that you can use for whatever purpose. So To do that, we have defined a number of new objects. If we can go to slide 3, please. Okay. These are the MI secret store"
  },
  {
    "startTime": "01:10:00",
    "text": "slide. Okay. Yeah. So there are there were two ways that we decided to to do this. One is we wanted to be able to reference existing you know, enterprise style, external secret stores, that organizations use today. The one we've we've included explicit support for explicit support for so far is how she court Vault There are a number of solutions. I expect will incorporate the majority of those into here with explicit configuration objects. But that's an example of an external store where in the configuration data, you're just referencing a store endpoint somewhere else and saying, you know, here's the key or here's the path to that to that value as stored elsewhere. The other mechanism inside of the advertisement, or configuration document. To do that, we're using CMS messages, which is an RFC from 2 decades ago, originally devised for encrypted email. but it's a fairly generic format. that's well specified. So seemed like a good idea to reuse that here rather than defining our own thing. So the secret store just defines the metadata around how those secrets are accessed. you know, the endpoint or the format that the secret is In addition to CMS, there's also clear text for testing purposes. You know, if you're in the lab, you don't need something to actually be encrypted. but you still wanna utilize the secret objects in the same way as you would in a production environment. Slide 4, please. The the -- MI secret value."
  },
  {
    "startTime": "01:12:01",
    "text": "Sorry? Yeah. Go go ahead. I was just reading the title there. Okay. And my secret value the object that's actually embedded inside of other FCI or MI objects, in place of a, you know, a plain text string. Now it's an object in dead that references the the secret itself along with a reference to the store has the configuration for how the secret should be decoded. We have examples here, both of a external HashiCorp vault secret which specifies a path. Remember that endpoint is specified in the store configuration. and an example of an embedded secret so that, you know, that big blob you see there is the CMS Message, base 64 encoded. Slide 5, please. So slide 5 refers to the secret certificate object. So this is utilized only in the case of the embedded store type where we need to communicate the certificate that's used to encrypt the values. So if I'm the upstream and you're the downstream, and I want you to send secrets to me, then first, I give you my certificate and then you reference that certificate from your store configuration to say, this is a search that I'm using encrypt these embedded messages. It's fairly simple, straightforward workflow. and all of these workflows are detailed explicitly in the draft. Slide 6, please. So rather than redefining identical objects on both sides, we define them once as MI objects, And then for utilizing those same objects on the FCI side, there's a capability that wraps the MI object. it's only defined once. on slide 7."
  },
  {
    "startTime": "01:14:00",
    "text": "Very minor changes from the previous revision. we had marked it as updating 8006and808 That was incorrect. So that's fixed now. There are some minor text fixes. And the actual edition is a time out property on secret value Previously, there was no way to know how often you should go back to the store to retrieve a new version of the secret in question. Some back ends like Hashi Court Vault may have a expiration date explicitly associated with a secret But that's not necessarily the case, and we wanted to provide away to specify that in line with the with with the object. So that's it. Any questions? I'll just add also that all three of these docs, including this all got a SPPA tech rider pass since the first time as well. So Yep. That's important. Alright. Well, I I would like to if I could ask for consideration for adoption for this document. because it's utilized in quite a number of other drafts. that are in progress right now. It's kind of a you know, it's kind of a a prerequisite to a number of other things that we wanna do. And I guess the only the only impediment to for adoption is that there hasn't really been a whole lot of commentary from the group on it. So I'm not sure if that's just because it's you know, it doesn't have any problems or people just haven't had the time give it a thorough review yet. mean, if it's the former, then I definitely would like to ask for adoption. Yeah. I think"
  },
  {
    "startTime": "01:16:00",
    "text": "So I haven't had a chance to to read the updated draft. In principle, I think all of these are useful things, I don't see any reason why we wouldn't want to add these metadata objects. to to CDNI, I guess, have the same question about, you know, have people read the drafts. And I'm wondering if we can take a poll the old days, we went but guess, Mideco has a show of hands tool I don't know. Maybe we can try this. I'm going off script Sanjay. know that. And, Kevin, I would like to start to cut in I would like to also, you know, propose that those first two that I presented adopted as well. We think those are ready. Understood. Understood. I guess I just like to to get an idea of how many folks actually have red. If we could do kind of a show of hands, and And, obviously, if you've read it and you support it, that's great. we will confirm on the list for sure. But Let me see if I can make the show of hands to work. Did that work? Yes. It worked. Yeah. So please vote. If you've read this important from metadata, Please raise your hand. I should raise my hand. 1, 2, 3, 45, Okay. That's good. I'm gonna do another one. How do I end. And then I'm gonna do a second one who has red the edge control."
  },
  {
    "startTime": "01:18:09",
    "text": "7 people responding. Everyone's really reluctant to click the button. Not as many have read. Hello? And by the way, I I I know that there are other folks in the SVTA that are unfortunately not here in the call that have also read it. So we would want them to chime in. I know I've I've seen a couple of emails Johan. For example, he, I believe, he has sent in the meeting list but he's not here in the call. Understood. This is obviously, this is just for folks who are in the room today, but it's a it's at least a, you know, a rough idea. Yeah. Okay. And a lot more on the secrets. I'd have to say, absolutely, nothing is more interesting for someone to begin to then a secret. That's why it gets much I vote. Agreed. Agreed. Okay. Alright. That's good. I think how many in the room, twenty people in the room, eight people roundabout voted, So that's that's not bad. I think we can take it to the list Sanjay, do you think that's that's sufficient for having read. I do encourage people please go out and read them. And if you have comments, please do post it to the list. I will go and and read the updated versions myself and post some comments to the list."
  },
  {
    "startTime": "01:20:03",
    "text": "Likewise, I will also be reading this the 3 latest updated document and putting my comments on. Okay. I Sanjay and I will discuss afterwards how we wanna we wanna proceed with these, but I I don't see any major red flags at the moment. our request, which is that if we could set perhaps a date or a deadline for a commentary before consideration for adoption. and maybe that will help encourage people to get a move on with sharing their commentary rather than putting it off That's a great idea. We'll we'll send a broadcast out to the list after the meeting. Agree. who's up next? Alright. So I think we have Alan, we have you next here on the named footprint. I'm going to bring up the document. All yours, Alan. Say say Next slide. Naturally. Right. So so Again, this is a a kind of a recap and bit of change of what I presented the previous session, Again, kind of with several use cases, we see the need for more advanced footprint capabilities, meaning that Examples include when you have, like, disaggregated distinct networks on common DCDN Management. So you need kind of manage, basically, different sub CDNs on the 1 umbrella. use cases when you have agent core,"
  },
  {
    "startTime": "01:22:02",
    "text": "cash hires. So maybe distribute it given home last mile of catchwares, have different flow function capabilities. We need to be addressed differently. geographical requirements, GDPR or similar legislation. And to address that, we saw that we need to have kind of to move beyond, basically, In the FCI today, which is footprint capability that interfaces, really capabilities qualified And so sometimes are not actually stand alone objects that can be managed and addressed, and that's CI. So we need to do this within FCI, to allow kind of stable reference to what footprints are versus have different capabilities, state their own footprints, and monconsistent manner. and allow also cross center across interface usage so we can actually use There's not just within FCI, but also within configuration interface. and other interfaces. So that's kind of That's the main, I think, driver, and that's why we kind of hope kind of effort. It was make they're called main footprints, so we can have them as absolute objects that can be referenced. But additionally, these use cases required some more advanced capabilities of how we define footprints So we more complex logic. there. and also support for footprint that are changing very dynamic sort of on the network basis kinda when you have kind of parts of the network changing kind of from in on on that basis. So that's where we are. Next slide. That's why. So the changes that we're proposing is that kind of"
  },
  {
    "startTime": "01:24:01",
    "text": "again, FCI is already the natural home, so want to extend FCI to add more kind of API methods to allow advertising or footprints separately from capabilities, and that that such advertisement would be done jointly so we can actually get a stem scrape and get all footprint, but also potentially query individual footprints. because maybe not you as you see, they are not interested in all of footprints. They only want one or one one that that that done different timing. that your case should be supported. clearly clear requirement for building a hierarchy there. namespace. So we can actually send provide different breakdown depending on types of traffic or kind of host names and other other kind of So not all traffic necessarily. is broken down in the same way and managed self can actually have different overlays, and support for client side caching. So you can actually cash those footprints on the UCDN side, and kind of renew that and to provide support for caching of the particularly important for dynamically changing footprints. So that kind of as a result, the overall CD and I operation mode is changing so that will include kind of retrieval all footprints separately from capabilities. there's a new thing and also to add a refreshment. we're also looking to add 2 new footprint types to the registry One is main footprint, which is a footprint that is kind of defined elsewhere, so referring the footprint as opposed to defining that in line to refer to the to the advertising, And, also, we want to adopt the metadata expression with Amel to be able to"
  },
  {
    "startTime": "01:26:03",
    "text": "express complex footprint definitions in the size and flexible manner. Lastly, changes in include is the ability to for some sources of footprints to be able to define a source. So when we say ASM, for example, by whose definition is that? When we say g geography country and so on, when you're looking at actually translating that back to IP address on that, you have different sources, and I think it's important for for the 2 parties to align on on that definition. So you see the endoscopy and talking, and the CDN says US, and CDN should have the same definition, again, 99% of the cases, if the measure was the same, but those cracks then and discrepancy that may exist are may lead to traffic, the the location issues and so on. So it's important to allow kind of specifying a source of definition there. for anything but IP address, IP address, scrutally self defining. next slide. So those are the the the new verbs we're proposing in in the API, So so under FCI providing new new API methods, and allowing similarly to hostname, Hosppasses, supporting hierarchy of main space and and footprint so you can actually get individual footprint as well. Next slide. that's an example of named footprint I just want to point out that the footprint value here is actually a reference to a URL where that footprint is defined. So within let's say, in this case, configuration, an example of my private feature list. So say, well, keep that that"
  },
  {
    "startTime": "01:28:01",
    "text": "feature is available in this footprint And that's there is a definition of where you go and can find at the definition of what footprint is. Next slide. So your key kind of the one of the important things is that we define explicit hierarchy in their advertising because y, because there is no way really to do implicit. Like, you for example, can do with routing tables and IP addresses. you can actually do some implicit logic. kinda what that comes first. In footprints, we're combining different footprint types. So There's none necessarily clear which one is the kind of which one is a parent, which one is a child. So so this proposed structure will define ER key exclusively. and the root footprint will include All footprints below that. And, potentially, there's kind of menu levels over European would be possible. So and importantly, I think that All children footprints are within the parent footprint, but it's not necessarily some of all open. So I can have some, let's say, IP ranges that are in good footprints, but they're not in any child. So it can be matched only in the top level 4th Importantly, There is unambiguous matching within each name namespace. So any given IP address will match to 1 to one kind of footprint in the tree. Next, Kevin is in the queue, so let's give Kevin the time here. Kevin, you have a question on this slide or or actually on the prior slide. I I guess I had questions on the prior slide. So 8. it seems to me the this I haven't read the draft I admit, and I apologize."
  },
  {
    "startTime": "01:30:00",
    "text": "but we we seem to be mixing a a number of things. Could you go back a few slides Proposed changes. Okay. This seems to me like a whole new interface FCI today, as defined within CD and I, we define the objects, and we do we specify Alto as the trans report. Right? this seems to be specifying a new interface with you know, by adding, and and a new synchronous interface, which which is somewhat outside the scope of our chart we need to talk about that. But but that that's the concern I have right now is that This is not just adding a new footprint. this is actually changing the entire way that you retrieve information about footprints. as well as the structure of footprints and what they mean semantically versus we never really conceived of dynamic well, we've conceived of dynamically changing footprint, but we never considered them within scope. So we we probably need to have a a discussion at a higher level with respect to where this fits in our charter, but that's my comment, and and we can keep going. Could you explain the kind of where that kind of conflict with the charter? I'm not sure I got follow that. Follow that. Well, our our charter says that we will provide a we will create an interface for asynchronous operations to exchange routing information. we we can go and look at the requirements. We can extend it. I'm I'm just not sure how this fits with building something new, and is this going to replace what we have with Alto? Is it going to complement what we have with Alto? is it comes something completely different if it's something completely different, then it's a new interface that's not in our charter. or or we have to bend the the interpretation of the charter a bit. But, again, I have the full draft, but but we need to think about that."
  },
  {
    "startTime": "01:32:02",
    "text": "Yeah. But okay. Yeah. It needs to be clarified. I'm not clear to me why ALTA is involved here, but yes. So again, I don't think this necessarily is in the assessity to have that. I think it's to have it on the common FCI. It's just that the way that FCI FCI right now actually expresses footprint but does it in a poor way. and we're just looking to improve that kinda So that'd be consistent and used that'd be and used consistently. So that's the reason I think that kind of discussions kind of, again, coming to this draft. The consensus was that it's probably best home to be in FCI. But kind of doesn't have to be. rights or so. I think that's that's certainly a point to discuss. Then, yeah, I'm not I'm not arguing one way or the other since I haven't read the details, but I'm just posing that we may, as chairs, need to to address adjusting our charter. Okay? Okay? So you. We talked about Cherokee. So the the those are the the examples of the new footprint type that will allow as I said, more concise and flexible expression of footprints. so we can actually have complex buoyant buoyant expressions that can do things like all networks in Korea, but exclude the subnets, kind of So and or if there's kind of be kind of complex expression there. and we can use existing mail functions like IP match with the way it exists. So the only thing that is needed for on the mail side is to extend the variables. So right now, the valve supports"
  },
  {
    "startTime": "01:34:03",
    "text": "request and response, variables, so endpoint. basically a set of attribute the can be associated with the endpoint would be out of here. and with that. So as one thing to mention that this is potentially supersedes the union footprint type that is in in one of the drafts pending graphs, that with that, I think, when kind of we don't need that necessarily to have that because it sort of does bullying and and more. or Boolean Ore. really. So that that's another thing that it would impact. kind of if adopted. because it's sort of there is there was this effort to define complex footprint but it doesn't support kind of complex boolean bullying expressions. and these are needed for practical use cases when you you're looking about they're looking into defining geographies, and I'm actually real word scenarios. next So so namespaces. Right? I talked about that. So so briefly, the notion is that DCDM may may have different geographical structure depending on the type of traffic it carries, And The variety of ways it can be done at configuration level, So rather than tie that kind of very So tightly, where's it, where's it, where's it, where's it, kind of types of traffic think that the idea is that this interface would expose name spaces, and those names namespaces can be associated with configuration interface objects. So, like,"
  },
  {
    "startTime": "01:36:01",
    "text": "could be service ident identifier can be individual host host index or that and so on. So you kind of you may decide as DCDN, for example, to say that oh, That's actual practical example. So I'm I'm handling I have a core and edge sub CDLs within my network, and I'm handling VOD traffic kind of from from the edge, but live traffic election is is all always carry it from or once in a year. So depending on this, I may advertise different footprints depending on what kind of traffic that So that's one scenario, but they could be other other back multiple scenarios where you want to have different footprint, advertising, depending also on the track of traffic. So so, really, namespace mechanism, supposed to be blood, and kind of support various scenario that next Hold on a second. I lost my Control here. Okay. There it is. So we are the end of it. Okay. Thank you, Alan. And I I agree with the cabin that a, I have to read this draft as well closely in in make sure that the the changes that we are proposing how they are fitting in in within the current charter or this is something you know, outside of the chart return that we have today. But thank you."
  },
  {
    "startTime": "01:38:00",
    "text": "I see near on the queue here. Yes. Hello. Hey. us a manual question regarding the structures the footprint, the the name for the So days, presented back. as I've seen the draft date, there's a footprint that and a footprint value just like we had in, actually, 806, but the and that day. If I'll go correctly in 8 in half, 806 the value is list of values, And then the call is a single one. So is it on purpose? think that which it should be for for which type of app. So for for any type or So any type, it's always it's always at least with the additive semantic. So if, for example, country code, so the type is country code, and the value is US, US, list US and kinda check. attend that Think the list list is appropriate. So I think that if if that was the way the original original also that should be kept this way. Yes. Okay. still inside. I I suggest we would take that in the list near yeah, I think that would be the better place to have the discussion with a little bit running late here. I I know we've got open mic with couple of topics that we wanna cover. but would greatly appreciate if if you can put your comments on the meeting list near"
  },
  {
    "startTime": "01:40:02",
    "text": "Okay. Alright. So We've got couple of topics on the open mic here. And the first one is CDNI logging extensions. Ben, I have the slides up. Right. If you just go to slide number 2, please. Sure. So I just wanted to Reason I this is on the agenda is we did not finish this draft in time for 117. We expect it to have it ready prior 118, and it's a fairly hefty draft. So I wanted to just take a few minutes to go through exactly what it is we put together and that you'll be seeing shortly. I'm sorry. Is is the it's slide 2 showing now for motivation. Yes. It's it's not okay. It's just not popping up on my end. Alright. So the reason we're going through this effort is that 7937, Does not really meet the requirements of a large number of CDN participants as it stands today, and this stems from some extensive discussion, that has happened amongst members of the SVTA. in their efforts to implement the CD and I standards and the open caching variation of that in the field. So a group was put together, and through extensive surveys of the participants, and many numerous discussions"
  },
  {
    "startTime": "01:42:02",
    "text": "We developed a set of requirements for what we need to cover log So quarter of that is a defined set of logging fields that match what operators actually care about. So that you know, that that was a huge effort in itself. And then around that, how do you combine those fields into various log record formats? How do you collate those log records, into log files, or how do you stream the records for streaming applications. How do you collect log files? into larger collections or archives. And then how do you manage all of the metadata around these log files and and handling them. How do you How do you name them? How do you transmit them? etcetera, And then as a adjunct to all of that, Once you have the log files, is there information that is normally part of logs or part of raw log data. that should only be visible to certain participants or should be obfuscated or redacted entirely. to comply with the privacy regulations of various jurisdictions. So that's that's why we're going through this effort. Slide 3, please. Okay. So For log records themselves, we wanted to have very clear definitions for every field. that we care about. We wanted to standardize things. And we wanted to collect those fields into a a number of different sets targeting certain use cases. those being billing normal operations and troubleshooting. So the draft as it stands today,"
  },
  {
    "startTime": "01:44:01",
    "text": "specifies different collections of fields for different business purposes. We also allow the inclusion of http headers, you know, optionally supported, into those log records, but the draft does not currently implement completely custom fields So that's work. We expect to tackle in the future, But that's a that on its own is probably equal to the entire rest of the scope of work. So that was kind of that was deferred favor of putting something together that would be kind of a, you know, least common denominator and meet the needs of as many people as possible with the the least amount of of upfront work. And then the you you know, and then we also have a configurate around how to configure that stuff. Slide 4, please. So log records, once you've defined and configured your log record, Those have to be collected into files. which we call containers. So 7937 defines a single file format based on ELF. But that's not in line with what operators want to use and what they use today. So a number of other file formats are specified. including JSON, Protobuff, and CSV. There's also a facility for a container of containers. that being tarballs, so that you can collect, you know, log logs over some period of time or other criteria into a single file. Slide 5, please. Log of Transports. So 7937 defines an Adam Index"
  },
  {
    "startTime": "01:46:02",
    "text": "that has references to where you can find the all of the log files in question, it doesn't it doesn't specify how we get the Adam index. There's no transport specified for that. And endpoint discovery is left as an exercise that's out of scope of the the RFC. We felt that this was insufficient and that there should be explicit definitions, for how to communicate the endpoints for transmitting and receiving logs. So to that end, there are objects defined for s 3 SFDP and Kafka. Kafka being a streaming transport, s s 3 and SFTP for files. In addition to adding a discovery mechanism for the 7937 endpoint you know, so we maintain backwards compatibility with 7937 with a small addition. These these transports, last three n f SFTP anyway, work both in a push push and a pull direction. So the downstream can provide an endpoint where logs can be fetched. or the upstream can specify an endpoint that logs should be pushed to. Slide 6, please. And this was the considerations for privacy that I was speaking of earlier, So there are a number of transformation, configuration objects, that can be used to both advertise transformations of log fields that have been performed by the downstream or optionally allowing the upstream to configure transforms that should be applied to log fields. And this is a number of various mechanisms including truncation,"
  },
  {
    "startTime": "01:48:01",
    "text": "search and replace, Encryption and hashing. And this is this is actually one of the things that utilizes that secret draft from earlier for transmission of things like encryption keys. Those transformations can also be combined into pipelines so you can have multiple transforms applied and that's on a field by field level. Slide 7, please. So where are we today? we have a completed Google Doc draft from all of the collaborative editing. that's in the process of being converted into a final IETF draft document. it's pretty large, so it may or may not display I don't know. I think, you know, seventy pages I think probably is too large. for a single draft, and it it it can be split along pretty clean lines into different areas of functionality. So that's something we'll consider And then there's a number of things that we don't have in scope at all right now. as you can see listed on the slide. and we will tackle those in a future draft that'll live alongside the current draft. So that that's a summary of where we're at on this. Are there any questions? get on the queue. no hat on, So I I think from what you have presented and, you know, this this seems like interesting work, and it it sounds like something that really builds on what has been done before, but is not sufficient."
  },
  {
    "startTime": "01:50:00",
    "text": "So I think the the topics that you plan to cover in the draft seems reasonable to me. And I think I look forward for you to submit the draft because it seems to be hitting all the the key elements for how you wanna log. Thanks. Thanks. Yeah. Okay. I I would also point out that we've we've made an effort as far as we can to remain backwards compatible. So we're not deprecating 7937. This is in addition too. You know, that's why we're calling it extensions. here. Yeah. My question was gonna be along those lines. I I see adding more transports makes sense. I mean, we had but we had 7 years ago, there's new stuff -- Exactly. Yep. -- formats. Some of the transformations, we'll we'll have to get by sector obviously, if we're doing encryption or we're doing privacy related stuff, that's fine. We'll have to deal with that. My question was going to be with respect to the existing IONA reg trees. For the new fields and stuff that you're adding, does it all still fit within structure, and we'll just be registering new fields and new formats and new file types, or or do you have some new know you mentioned tarballs and things. Yes. We can add that on on top. Yeah. That that that's a that's a good question. I don't think what is in this draft fits with the definitions of fields as such as they're defined in 7937, so it would be something new. Okay. This is the the the draft contains completely new definitions. Some of the definitions overlap with the fields as defined in 7937. Some of them are even named in similar ways. but they are defined very differently. So -- Okay. -- I think that that's a really good question. I think that's something we should explore once we have the draft in place to to facilitate discussion. Right. Obviously, if we can reuse stuff or if we can at least massage it in a way that makes it more usable, that would be preferable to just you know,"
  },
  {
    "startTime": "01:52:00",
    "text": "kind of kind of building something new, but we can take a look once the draft is ready. Thanks. Chris, play. I support this work. attempted to I'm getting some very faint audio, but I can't distinguish it, that mic doesn't seem to be working. Is it on? Maybe, Chris, you can use this mic up here. Use this mic up here Outstanding in the front of the room. feel silly. I support this work the I've attempted to try to get 79 37 to work for us. and it mostly doesn't. and the thing the places where it falls short are the places where this new extensions pick up the Slack So I I really do think that this is a natural evolution and it takes into account the the intervening 7 years of implementation experience. coming back to the group, and trying to produce something better. So I am looking very I'm looking very forward to reading the draft. reading the submitted draft and progressing in. Alright. Thank you. All right. Thank you, Ben. I don't see any other questions. So thank you. Alright. Thanks. Alright. So last topic of the day, Alan, you get you get the last word, Right. Right. And if I can bring up your slides, Request routing interface."
  },
  {
    "startTime": "01:54:02",
    "text": "Yeah. So this is work that so I'm presenting this, but the work actually was done together with and and just recapping, we kind of first introduce this in the open mic session, actually late last year. didn't actually submit the draft, so there's a plan to do that. since then, we kind of worked on wording of this. and the CTA, so I think we'll be ready to do a draft before before our next IETF and prod. The problem statement is that existing 7975 RFC, specifies support for recursive request routing that is uses kind of custom JSON based messaging, which has several drawbacks think there is complexity of implementation. So several UCDNs face difficulty actually mentioning. that. There is we support for request transmission. So you CDN doesn't transmit and kinda quickly sets the stage that we're talking about. Right? So request routing recursive request routing suggests that UCDM receiving the request from the end user and kind of a consulting with these CNN about how to handle that, and then returning the request response directly to the user. in a way that and the user then proceeds to to work with DCDN. as opposed to kind of the iterative approach where user is when they actually, the UCDN differs, the the the request to to DCD and"
  },
  {
    "startTime": "01:56:00",
    "text": "and this is the end then proceeds to respond to that. So are we looking for better ways to support that? than that was down in 77975. Again, I mentioned the complexity of implementation. ability to act on request only. Actually, not providing the benefit of of latency reduction. So we want to introduce a new mechanism for doing that. that will provide a way to reduce latency. so that could be can we minimize the amount number of times that there is round time trip request going back and forth. providing you CDN with the desire for capability to control delivery. So manifest control for a service side at social is 111 driver there, and that will be something that will be supported only for HTTP forms of redirection, though. So, actually, providing that mechanism of request routing for HTTP it should be request validation Next slide, So, Alan, we may not have time to go over all the slides of any key things that you wanna hit through in the next couple of minutes. I just wanna say before that. time ends. the key features, right, is that again, we wanna move from JC some basic coding to actually moving request and responses as is in the data plane. which will resolve of the challenge of implementation complexity. and support to modes of operations. So that be done on request and on response. and kind of so the the request mode or response mode, and using the common open captioning faces for for this."
  },
  {
    "startTime": "01:58:01",
    "text": "just just, like, other interfaces in in this And yeah. So the I think the next step for us would be really to submit the draft. And that has been actually on hold for a while. So I think we we we should be ready to team. produce that fairly quickly. So And looking forward to the discussion on the list Man says he's done. Very good. Very good. Thank you, Alan. Kevin, before we end, I'm gonna pass it on to you. 4 Thoughts, comments, Yes. We will follow-up on the list with respect to the request for adoption, We have some follow-up to do on our existing working group drafts, we will get the ACME draft submitted to the ISG, Other than that, everyone, obviously, as we always say, please go and read all drafts, please put your comments on the list And, otherwise, we will see you at the next IETF, Sanjay. Any last comments? Yes. So the next IETF is in Prague. and incidentally, the SVTA is meeting in Prague a couple of days before the IETF. So hopefully, we will get to see some of you in the next IDF session. And what we will do is as chairs, we'll make sure that we request for session for the early part of the IETF sessions So maybe Monday or Tuesday. That way you can extend your SPTA trip and and participate in the IETF in person. That's all. With that, thank you everyone for coming. Safe travels home. and we will see you at IETfone 18. Thank you"
  },
  {
    "startTime": "02:00:00",
    "text": "Thank you. 1st Hey, Ali. Long time to see. I didn't realize that you were saying that I saw you, but I didn't know that you are seeing me. How are you doing? Good. Yourself? I think well as well. Excellent. It's been it's been so many years. I know. I wish I could have been there, but Yes. hopefully hopefully sometime in the near future. Alright. Well, you look more or less the same. Looks like So do you. That's good. Alright. Safe travels. Sanjay, I'll talk to you later. See you, Ali. See you, Chris. No. Hello. Once we go a good strategy So"
  }
]
