[
  {
    "startTime": "00:00:26",
    "text": "I think it's an hour's of time welcome to this session. My name is David Drew from Huawei Technologies actually from the network technology lab. In fact, my colleague bing liu should give this talk, but I'm unfortunately he cannot make himself here So I will give this talk on behalf of him actually. So this is a topic actually we are working on in our laboratory It's called AI Cluster Network, the infrastructure to support closer better? OK, sorry So it's about AI Cluster Network kind of infrastructure to support the AI training Oh, thank you Something went wrong, sorry Connections are down So I guess I need to just see"
  },
  {
    "startTime": "00:02:17",
    "text": "Sorry, the network is down so I just need to figure out Thank you very much I've done with her Thank you see Yeah Thank you"
  },
  {
    "startTime": "00:04:08",
    "text": "Thank you no Thank you Thank you where they disable the alarm and then disable everything Okay, great Now it works. Yeah, thank you Thank you Seems after they clear the alarm, they clear the network as well. But anyway, um let's continue It's about the AI class network, right? So where do we start? So basically the AI modeling people know that's developed very fast, since 2010 do we start? So basically the AI modeling, people know that's developed very fast, right? Since 2010, the computation power, the required for the AI training actually increased according to us, 4.2 types every year. Meanwhile, the"
  },
  {
    "startTime": "00:06:02",
    "text": "computing power increased for a single GPU process unit only increase 1.35 times. So there's a big gap and the gap actually increased exponentially That's the reason we need to build the actually large-scale high-performance network to support actually the high-efficient AI training So to do that, if we look at the efficiency of the computing power all, the effective computing power, it's determined by three factors, the power of a single chip, obviously and the scale of the cluster and the last of the least, the computation efficiency As a networking guys, we don't do the first part. We only care about the scale of the cluster and the computation efficiency It's easy to understand the class scale actually determined by the network, right? Network can help to connect hundreds of all thousands of the GPU or NPU together to let them work more efficiently. While the computation efficiency, how should we understand that? In fact, if you look at the training, it's basically composed of computation time and communication time. So if we can shrink the communication time, so that's actually in return improved efficiency of the computing So we will talk about those two things in the next slides So the first of all is the scale of the cluster, right? How big the scale can be People can always say the big the better, but the problem now is we need to find the balance between the high performance and the low cost"
  },
  {
    "startTime": "00:08:02",
    "text": "Later in the slides, you will see that, okay, definitely you can kind of a power up more and more processes together but the cost also comes very high This is one thing, topology actually is one of the things. The second thing, of course, once you define topology, the routing system running on top of that is another thing that you need to solve, right? The AI communication actually is the most communication in applications that we have seen so far. I give one example for instance, one GPU may sense 10 terabytes of the data in one integration during training. So, that's why we observed in the past years that the every increasing the boundaries coming out from single GPU chip, right? And this all almost come to the capacity of the switch chip So we need more and more switch chip, the network, to support the interconnection between different GPU According to Professor, Thorsten Hofler from ZTH, in a crawl topology the networking costs, including the SWIFT Hofler from ZTH, in a crawl topology, the networking costs, including the switch chip, including the optical module, fibers, etc., accounts up to incredibly 70% of the overall system for some small cluster maybe still do doable, but it doesn't really scale So the intention actually is to really look for kind of balance Well, on the other hand, actually the three three Taurus or Taurus topology offers kind of a direct connection between different GPUs without the needs of the switch chip It reduced cost is rather cheap"
  },
  {
    "startTime": "00:10:02",
    "text": "And it also offers uniformed bannories in different dimensions. But then the problem there is does not really meet the requirements of AI training because AI training actually come with specific traffic pattern. It's periodic or nearly periodic and it's bursty. So the Taurus topology is not so easy to handle that So actually the research community come with the variants of pharmaceuticals like Hemming, Mesh, KN, you can name more, right? But we believe that there's more to do, more to explore in this field If you look at the edge, training, more or less converge to five parisms, data paradigm sequence parism, pancer Pipeline, and Expert Parism Each different parism come with different traffic volume. So on the right side, of the screen, you see a table We take some typical settings of different parisms on top of a big model with 2 terabytes actually, parameters. And from the simulation you see that the pipeline parallel and data parism generates the smallest data volume, tropical volume comes to like 11 to 40 gigabytes meanwhile the tensor parism and the sequence pattern generates extremely large volume, which is almost 42 to 2 times, uh, 200 times more than the DP and and PP And on top of that, the tensor perils and the sequence parisome will further increase in proportion to the models"
  },
  {
    "startTime": "00:12:02",
    "text": "size. So you see that there's clear hierarchical of the traffic volume generated by different patterns and we need to design some there's clear hierarchy of the traffic volume generated by different persons, and we need to design something to match that, right? So one of the typical design, probably you today already find is more a kind of a high hierarchical AI cluster network system Typically, it involves three different levels chip level, rack level, and the cluster level. The chip and the rack level actually will we call it actually is high bandwidth domain It offers enormous bandwidth it actually connected actually connected, and in a low level even direct topology without evolved too much the switch chipsets. So all in all, actually, we want to actually reduce the data movements reduce the data latency, reduce number pops, eventually reduce actually the cost If we actually, um proper, we did some assimilation in-house, with proper model partitioning, 99% of the traffic can be terminated or can be consumed in the trip and the rack level, which left only 1% going out of the rack and then go through the optical links what we call it in the cluster level, to the other class level, yeah, to from one cluster to another cluster so on the top level, basically, um, we use one layer of the switch direct topology, like Dragonfly, Dragonfly, Plus You probably heard about this. We also have drafts in IETF. And we will we also use the OCS to in enable the topology engineering to support different training tasks running on top of the same"
  },
  {
    "startTime": "00:14:02",
    "text": "infrastructure So we have the top topology. On top of that, we also need routing system You may, you may say that, okay, the routing system IETF already by BGP or IGP routing system. You may say that, okay, the routing system, IETF already developed BGP or IGP, a lot of routing system, but I would argue that here inside the AI training, inside the data center, we need high-performance routing system So as I said, we have a direct topology, directly link from GPUs one after another, right? 3D Taurus, like things. So for that, actually, every GPU, every NPU acts as a small mini switch. And of course, we want to make most of the resources to the computation rather than for switching So we need to design a very efficient and simple forwarding around routing system for each change set to avoid consume too much of the precious GPU resources So high performance routing and look at table is needed here And on top of that, because of a direct topology there was a lot of non-shotted paths, unequal cost of pass right? And we need to make massive use of that in order to support, actually, reduce the flow completion time and support the training. How do we do that? It's also a challenge And once there something happens, we probably heard during this week I heard from AIDC signing meeting. I heard from HP1 signing meeting, and also some talks from RGG People worry about the failure of the training, right? They're evolving with hyperset percentage of the fails, so one something fails"
  },
  {
    "startTime": "00:16:02",
    "text": "how do we recover? We need to find a faster recovery to actually avoid the further delay of the over training job. Last upon least, actually, we need to guarantee that the whole over network is deadlocked But if we look at the existing routing systems, the longest prefix matching the host-based routing dimension of order of routing, or minimum non-minimal routing are typically running on the existing lines topology block topology, Taurus, and Dragonfly, etc cannot meet all of those requirements So there, I believe the IETF community has expertise to load at, look into this moment and try to find kind of better solutions for, to meet all all those requirements right to find kind of better solutions for to to meet all all those requirements. Then we come to the second part of the equation computing efficiency We have two ways to to solve that. The first is actually very obvious one is reduce the communication time too much more optimal. So here actually we adopt the network computing and the memory code design methodology to explore three areas One is collective communication Number two, load balancing and the transport protocol The second way to solve this is more integrated network memory and computing computing design. I mean, more integrated in terms of on top of the five parism I just introduced in previous slides. We want to do a little bit more. Can we actually?"
  },
  {
    "startTime": "00:18:02",
    "text": "find more optimized the parallel? mechanism? And can we do the proactive data movements? Why so? Because everyone heard about the memory war, right? The AI training needs a lot of memory to store the data and memory has a certain limits. So in that case, they have to really push the data from the tier one to tier two to even further memories. And then the idea basically is you can push towards out but when you need it can we actually pre- tier two, to even further memories. And then the idea, basically, is you can push towards out. But when you need it, can we actually prefetch the data just before your calculation? So that's once you, during your calculation your data is always on the high or closed proxity And on top of that, there was also the in-depth fusion of the network computation and memory So the first thing is collective communication algorithm. This is the most common things in AI training, right? Give an example of all all-reduce. Actually, it's the most common collective operation happens in the AI training So if we look at the duration of all all-or-reduce, it's determined by three factors the number one is step of the communication all we look at the duration of all or reduce, it's determined by three factors. The number one is step of the communication, or number of rounds, right? You need to run that's collective communication. Number two is the latency and number three actually is a throughput. So basically the volume of the data divide by the bandwidth. To optimize, that, we need to minimize the steps of the communication and minimize the communication traffic Meanwhile, maximize the bandwidth So if you look at the day, the, uh, why they used the all-reduced methods, is for instance, ring, or another one is half-doubling. Ring is"
  },
  {
    "startTime": "00:20:02",
    "text": "a very straightforward one so if you have an node to run the ring, then each node send the data to the next one for one round. And then you need to actually circulate the data to all the nodes in such a way that you need N minus one steps to finish this kind of or reduce calculation which obviously as well number of nodes increases the later actually increases as well. It's not ideal That's why people come out with the doubling, well the doubling mechanism which kind of solved that problem because the number of the run of the communication reduced to log-end But meanwhile, if the number of nodes is not the power of two, then it has a problem of the unutilization of the boundaries So there, in our lab, actually, we come out with some new algorithms on based on the classical P2P short algorithms Basically, we prove that VR will be called a based on the classical P2P short algorithm. Basically, we prove that VR will be called NB algorithm. It can really produce the optimal scale configurations and the settings At least we prove that in the factory or claw architecture, we can get the optimal result We haven't verified in the Taurus or other topology yet. We are working on that. Things I'm not expert there, so I will not give you too much detail. But in case you are interested in this kind of algorithm, just drop me a message and I will connect to you to the experts from our The second thing, is a lot of balancing. While yesterday, um"
  },
  {
    "startTime": "00:22:02",
    "text": "I remember well in the EIDC meeting Peter from a video talked a lot about the load balancing He said the AI networking is pretty much the load balancing People spend a lot of effort to optimise Why that? Because for instance, in the claw topology we use the hash algorithm to make the road balancing That's quite common. But in a the claw topology, we use the hash algorithm to make a road balancing. That's quite common. But in AI settings, the trap pattern changes. The most of the traffic come from a small number of elephant flows. So if you see pattern changes. The most of the traffic come from a small number of elephant flows. So if you still use the hash algorithm, passion, then what we call hash polarization happens and which will be reduce the effective boundaries from such to 50% according to our simulation So we need to find a way. And if we have the direct topology, like a tool or others, as I mentioned before, there's enormous unequal cost pauses. And how do we make massive use? of that? It's also the challenge actually posed into load balancing So we adopt actually the network computing code design methodology. So from one side, once we have the AI model, we can derive a lot of AI traffic characteristics because you have the model you know how do we split model and from that you can get a lot of information of potential AI traffic there. On the other hand, we know the topology, right? Well, we not only know the physical topology, but also the virtual ones, if we virtualize the network. So when you put two things together we develop some tools that, what we call netmind"
  },
  {
    "startTime": "00:24:02",
    "text": "So to create an optimal configuration, not only for the load and balance configuration, but also the modeling participation and the node assignments So we made some kind of simulation that via that tool we can reach the multipassal throughput 98% of the bandwidth and increase in training performance by 20% Transport protocols this is definitely the IETF thing, right? We need to actually improve the transmission efficiency. People talk about people actually research on that a lot, both from academic parts and also industrial parts. For the past years, almost for each CICOM conference I do see one or two papers talking about transport related stuff, either CC or other mechanisms Next week, we'll be in Cini I guess there will be similar things topics will be discussed again From industrial, probably you heard about the Outer Ethernet Consortium They have been working on new transport code UET, ultra-isternet transport, right? The specification most probably will be ready in the Q this year, and there the also spend our efforts to find a best transportation mechanism to support AI training So largely it's boils down to two things One is whether it should be lossless or lossy. Traditionally in the data center is lossless and we use if it in bands to support losses transmission. It works well lossless or lossy. Traditionally in the data center is lossless. We use equity bands to support losses transmission. It works so well. But the problem, we see that the buffer size actually is in proportion to the bandwidth and distance"
  },
  {
    "startTime": "00:26:02",
    "text": "especially when the overall system increased you need high bandwidth, maybe you also long distance. In that case, the buff side will be the nightmare. It will cost extremely high and there was a lot of problems coming with that. On another path, people talk about loss transmission, right? But then there you have to handle the reordering, retransmission in a proper way without doing too much Because there, for instance, UET is go for the lossy solution direction But there, it's actually impacted by two things. One is the slowness of the feedback signal. You always need to wait for the ARTT, or at least one RTT for the feedback And secondly, the feedback you receive might not be so accurate because a lot of things credit base or Nathan based on certain estimation. So whether go for the losses or loss is an open question. People need to figure out. Another thing is a throughput and the FCT, right? So we know that, for instance, TCP has a slow stop problem There's enormous solution to try to solve that Here, I think we have a lot of research have been done in the past but then for the AI, in the context, of AI, we need to see that once the model is decided once the paradigm is determined we have the pre-knowledge We have a pre-knowledge of AI traffic characteristics. With that knowledge, probably we can design some better way to control the sending window in such to avoid the congestion wall, make maximum use of existing resources"
  },
  {
    "startTime": "00:28:02",
    "text": "resources I will not go too deep into the network of computation memory integration because it's less IETF relevance Roughly it means in three directions. One of the things is, can we create a new paradigm? A new, the parentism on top of what I mentioned the five data tensor, etc., etc., right? But by taking into account about the memory, uh, top of, what I mentioned, the five data tensor, et cetera, et cetera, right? But by taking into account about the memory size, thinking into account into a account of the network condition so we did some preliminary research there as well And the second part is what I mentioned before the proactive data movement, right? What do we call the data profession? We know that when you need the data, probably you have to fetch from the far distance The current practice basically is if you really need to fetch, from far distance, you just recalculate them right? But recalculating them consume extra resources and might not be the ideal So for us, basically, we need to look at the modeling look at the from systematic view which data should be moved out and which data should be prefetched before the computer Last of on the least, the in-depth fusion of the network computation and memory computation and the memory. As illustrated on top, that we kind of dissembled each step, each big computation step, each big communication step into a smaller part. I can give one example, for instance, for the communication, you want to send it bunch of data from A to B. Then you disemble it's okay, you prepare that in the sockets. You send to the NIC and from NIC you send out, send out, you send out maybe into three hops. For each hop, you"
  },
  {
    "startTime": "00:30:02",
    "text": "just disassemble everything, and then you start to look at okay should i do this in this sequential? Can I do this in a better order, right? Can I combine the that element from the previous step? If we combine that together, maybe we can have better efficiency So there are a boundary colleague from Beijing actually working on that topic But anyway, as I said, those are the less relevant to the IETF, so I will skip that So the next two slides actually is link with the signed meeting we are going to do in this afternoon from 5 to 630 It's about across DC supporting for the AI training. Why Cross D.C.? SS also yesterday get verified without our thought A single DC has its own limit Number one limitation is power Yesterday, I heard that nowadays the GPU is a GPU, one GPU consume one kilowat And then a typical data center can support like 15 megawatts and then you can calculate how many GPU you can support in one side. It's very simple calculation. And on top of that, the complexity cost space cooling, they are all boils down to say a kind of a single cluster cannot scale unlimited so the size will be limited And on top of that, we are that how many data center will be build only for AI training. In the future, probably it will be cloud computing AI training host in one place. So in that case, you need to support multiple services. Resource will be fragmented will be virtualized. And in"
  },
  {
    "startTime": "00:32:02",
    "text": "that case, how do you actually group the kind of rest of resources together? to support the very efficient training? is another question to be answered Last of all the least, due to the GDPR, we all know that, yeah, the data cannot move freely from one side to another. Sometimes data has to stay locally. So in that case, the single DC actually has certain limits Well, then we say, okay, can we build multiple? across multiple DCs? We definitely can. And then it comes with an has certain limits. Well, then we say, okay, can we build multiple across multiple DCs? We definitely can. And then it comes with new problems. That's across different diseases. Then you have different resources They are not distributed evenly And the different DC may be equipped with different heterogeneous devices Even in the same DC, now, if you look at the new generation come only after half a year after the previous generation, you can imagine in one data center you will have different kind of GPUs with different capabilities then how do we actually people talk about the virtualization right you need to virtualize them and you need to anticipate their capabilities and then make the new generation with work with old generation and there were also kind of a multi-power or in the mentions so it comes with different challenges. The long-tison transportation federal learning topology of finding communication, etc., etc I will not dive into detail. If you are interested in it, as I said this afternoon from 5 to 6.30 in the room of the Prince of Wales Oxford, we will organize this sign meeting meeting. So you're more than welcome to join us to discuss about that and we believe that is quite relevant"
  },
  {
    "startTime": "00:34:02",
    "text": "to the IETF because we pinpoint like CCWG TAWG, auto, etc are quite relevant to that So then the CCWG, TAWG, auto, etc, are quite relevant to that. So then comes to my last slides, try to summarize So basically the AI training poses significant challenges to the computing infrastructure and the networking to address the challenges, we need a holistic view or holistic approach, innovation So the things we need to take care to basically is topology routing system collective communication load balancing, transport and the in-depth integration are the network computing and memory. Except the number six, all is five, I believe, related to IETF So IETF plays an important role in above fields. And we hope that we can collaborate we can leverage our intelligence in the IETF community collaborate together on the future developments of the AI cluster network Okay, that's about it. Thank you very much. I don't know if you have questions Yeah please Hi, Jean-Raswa Gerald, in some of the slides, you've been discussing about this tight integration between computers and the network topology itself and how they would be communicating to each other. Do you have any concerns about what would be the consequences from the collapsing?"
  },
  {
    "startTime": "00:36:02",
    "text": "of that stack? You mean the concerns about the message flowing or what kind of concern? So we because it's like what kind of concern? So we- Because essentially the way the stacks are typically designed is so that layers don't really know what happening on the other ones. And what you are what you seem to be proposing is that the um lower layers have an understanding about the type of information that is being transmitted and possibly even the content that is being transmitted which is sort of until antagonistic to what we've been designing in DCPA for the past decade. So I'm a bit concerned about what would be the environments in which you would see that applied and what could be the side consequence you can imagine? Yeah, this is actually a very good question So currently, the counter practice basically does not have this problem because they typical buy equipment from one vendor, right? But in the future, if you start to buy equipment from different vendors, indeed this becomes a issue. That's why I believe we need IETF to standardize the interface, standardize the information model, passing by from passing through different layers. So that we know what the information you receive from bottom or from the from top means, right? So this is definitely importance well meanwhile at this stage we are doing research, we assume this information can be interpreted correctly, but in the future, I agree with you. That is needed to be standardized Okay, thank you This is Paul Sung from SK in Korea Yeah, very nice talk. So I have a question You mentioned the inter-data center you know, some AI sporting"
  },
  {
    "startTime": "00:38:02",
    "text": "The question is, okay, energy consumption and other cooling system inter, this is, I think, a reasonable approach. The problem is, some computation point of view, some the task should be some decentralized or merging and also synchronized. So do you have some idea in how way, how to synchronize? the result so how to communicate? Very good question. So one of the initial ideas is coming from the five parism as in one of the slides I indicate that the data parism and the pipeline parism they generate the least data volume right um so we should put those parism actually across different data centers why because the these the data center DCI, Benowice is much smaller than the bandwidth inside DC. So looking at the hierarchical feature, of different parism, and we need to actually design our overall system to really match them So one of the practice we have been doing is we just split the data into different data center. So the data parism are performed, pipeline paradigm paradigm can also be performed, but definitely is not a good idea if you want to to apply CIP sequence parism across different diseases Okay. So the other things, you mentioned the privacy data, certain data, should be some not shared because of especially the medical data or something like that So do you consider some other, you know, the pedroids? learning or split learning, something like that? Yeah"
  },
  {
    "startTime": "00:40:02",
    "text": "we can see about the federal and basically if the data can be shared you probably can share some features you can share some features you can share the pederoid learning or split learning, something like that? Yeah, we consider about the federal learning. Basically, if the data cannot be shared, you probably can share some features. You can share something has been working on and not really a role raw data. So due to the limitation, yeah. Okay, I see, okay thank you. You're welcome Okay so if there are no more questions, I guess you're all finished lunch, so in case you have more questions later on, feel free to join me a message, email, whatever, so that's we can discuss more in-depth. Yeah, thank you Thank you question have a question I do. Do you make any other law or regulations like that applies to AI? applied to GDPR that has been or two weeks? Yeah two Yeah, well, outside to me"
  }
]
