[
  {
    "startTime": "00:00:15",
    "text": "Good morning, folks I'm going to get started on time because we've got a fully packed agenda and hopefully whoever's coming in, it'll just miss some of the boilerplate in the end intro. I'm dave plonka I'm a boat ben maddison, Wisconsin. So I'm two hours east of you guys Good morning. It's one of the few times when you're remote and the time is more convenient than when you're the local Myriad, would you like to introduce yourself? Yeah, good morning, everybody. My name is Mayor Kulovan. So, Maria and I are chairs of the Measurement Analysis for Protocols Research Group If you're joining us again, or if this is the first time here at MapRG a hearty welcome to you. I'm going to spin through a couple slides quickly and we're going to get set We've got eight speakers on the agenda today and hopefully enough time for your questions and comments The MapRG is an IRTF research group and we use the same intellectual property rate disclosure rules as the IETF. You should have seen those already, but if you can to see what you're bound to by participating, please take a look at the links here in the agenda and follow those links Note that this is being recorded both audio and video and there's stipulations there for if you have to do not phonograph, Lanyard, but you participate at the mic now and appear that you're giving up, you're agreeing to, I have a video you We have a privacy and code of conduct in the eye appear that you're giving up, you're agreeing to, I have a video you. We have a privacy and code of conduct in the IETF and the IRTF. If you have any problems with people behavior or whatever, you can contact the chair or you can contact the OnBuds team, which is linked there and they will be an ambassador on your behalf to help remedy the situation These are taken very seriously by the organization"
  },
  {
    "startTime": "00:02:01",
    "text": "so please absolutely do that if you have any concerns or questions Contact us or them So the goals of the IRT effort to focus on longer research projects in the IETF, so the rules of the way we can conducted are a little bit different. This is not a standards development organization The rules are a little bit looser. In fact, for instance, MapRG, this is the eighth year we've been meeting. We started in 2016, so it's quite a long-running effort to link up researchers that measure internet protocols and their operation with the people that developed them You can find our group's charter at that link There's a mailing list, of course, if you need to announce things in an ongoing fashion or want to see what's going on with MapRG and receive future calls subscribe to that. The slides are up at that link. If you're already seeing me talk, you're already in Meadeo. You certainly sign into Me.O if you're in the room as well, because we use that as a way to keep attendant for the meetings and get it a properly sized room and in a right timed event for the next time we meet For in-person participants, there's some tips here about how you use media go light and how you can join the queue at the microphone We are on time today, so I'll apologize in advance if we have to cut you off. It'll need a little more towards giving the presenter the time than people with questions and answers, but I think we can fit it all in if people stay on time Remote participants, just watch where your audience and video is on if you don't mean it to be Maria prepared this nice list of recent publications from TMA Give it a look. These are things that presumably she handpicked is particularly interesting to IETF participants Thanks for preparing that, Maria, and also thanks for putting together almost the entire agenda this time, or probably the entire agenda. I really appreciate it So here's our agenda for today. I'm not going to go over it item by item. We'll introduce the people as they come along but it's tight and let's all work together to keep"
  },
  {
    "startTime": "00:04:01",
    "text": "this on schedule. So the first talk is a short one, and that's going to be Joris So let's switch to those slides and get Joris up there Thank you Okay, okay morning everybody My name is Joris, Herbots. I'm a PhD student over from hassled University. And I'm here to do a quick hat to talk about a paper we presented yesterday at AANRW in case you already saw it. On H.S.D 3 successful prioritization scheme in the wild Next slide please Dave, you are moving the slides, right? Okay there we go. So HTTP, we have multiple versions. In a nutshell, the biggest difference is there is the way resources are being scheduled The newest versions have multiple resources on the same connection. And if we're talking about multiple resources, we should be talking about priorities because that's a way to signal the relationship between these resources. Next slide, please We have the extensible prioritization scheme, which is the latest addition in this field, which is a way to signal these priorities on HGP3 and 2 It's the replacement for its predecessor in HCP2, which was way too complex The idea of the extensible prioritization scheme is to be way simple However, when we tried it in reality, we saw a lot of chaotic behavior, and it's way more complex than expected. Next slide, please The way EPS works is with two basic parameters. We have an urgency parameter, which is basically the priority with the lowest number being the highest priority in practice and then an incremental flag, which is an indication whether or not these officers can share bandwidth. For example, if you have an"
  },
  {
    "startTime": "00:06:01",
    "text": "image, if you share bandwidth with all the images, if you have 10% of an image, you can show it. You can do the same with, for example, JavaScript files. The way we signal these EPSs, parameters is using H.D.P priority headers and the binary frames that are associated with it. Next slide, please We tested both clients, so browsers and servers, and what we know noticed that the browser site is a lot of heterogeneity So the main browsers, Chrome, Safari, and Firefox can actually be divided up into three distinct categories with different approaches when they are lot of heterogeneity. So the main browsers, Chrome, Safari, and Firefox can actually be divided up into three distinct categories with different approaches when they apply their heuristics, so their priorities to research even very different incremental behavior with for example chrome applying incremental behavior as we expect but then Safari applying in on every resource which makes no sense, and Firefox actually never applying it which isn't bad, but not good either. Next slide, please One of the elements that's also important, if we talk about extensible privatization scheme is fetch priority. That's the only way we currently have as developed of web pages to indicate whether or not we want to deviate from the standard priority being applied to a resource. And next slide, please The thing we noticed in practice is that browsers also have chaotic behavior here. Chrome, thanks to ignore 50% of these directives given Firefox doesn't even have support for it and Safari is quite deterministic in that if you up the price Firefox doesn't even have support for it and Safari is quite deterministic in that if you up the priority, it actually ups the priority in practice and the same for lowering the priority. Next slide, please So similarly for servers, we did experiments where we checked the support for popular servers We tested 12 servers based on popularity going from CDN deployments to off-the-shelf software such as Anginix and Caddy There's a whole spectrum of results. You can find those back in the paper. Next slide, please But let's focus on the partial support once because those are obviously the most interesting ones to focus on"
  },
  {
    "startTime": "00:08:01",
    "text": "a good example of chaotic behavior we see here is for example with the default scheduling behavior and the interplay within the incremental parameter. We have these two basic parameters like I said not having support for incremental parameter means being stuck on your default scheduling behavior. And we see that happening for Akamai and Google. Akamai, this is quite fine because they have sequence scheduling by default. But on Google, this is even worse because they have incremental scheduling. And if you look at Chrome, which is used or actively requesting sequential loads if you then use Chrome to access Google's own server, they are only scheduled in an incremental fashion meaning that we have diametrically opposed approaches to web performance optimization, which once again may no sense. Next slide, please. So the key point, or the thing we saw is that we have a lot of heterogeneity. That's the whole point of the paper we have unpredictable behavior at the client's side at the browser side so at browsers and at the server site. And the tools we currently have to streamline this a little bit are quite ineffective it's even worse in that if you want to optimize for one browser, you could potentially worsen the experience in others, making it a worst case experience pretty much This is important because priorities impact web performance and we care a lot about this as users, but also as website maintainers. Next slide, please This brings us to the recommendations we bring forward. We have a couple more in the paper, but the two key recommendations are we need full support for EPS by major deployments. We are currently in a chicken and egg program state where servers are not having full support because clients are not using them and clients are saying, well, we are not doing it because servers are not implementing it. We need to break this cycle. So we have inconsistent behavior at this point without basic features. We need those basic features and the whole extensibility part that's in the title of the extensible prioritization scheme is basically out of the window if we don't even have basic support similarly for browsers the clients we need better manual control"
  },
  {
    "startTime": "00:10:01",
    "text": "through developer APIs. If we want to work around these browser heuristics or even try to look what works best in practice. For example, allow us to change the incremental behavior. We have no way of changing that, for example, now through the Fetch priority API Not only that, but it would also enable more complex web applications such as video streaming, which one certainly might want to make use of this priority API Next slide, please So that's in a nutshell the findings. Feel free to scan this You can also find the paper back on the AANRW overview website that was published yesterday. The talk of yesterday is also online I saw this morning, so feel free to check those. If you want to talk about this, please find me during the breaks, whatever. Thank you very much Exactly. So this was just a short heads-up. So you see his face. You can watch the talk online, read the paper, talk to him But we could also take one or two quick questions if anybody is very curious about it But it's fine. Sorry, go back So just please talk to him Okay, then we can move on. Perfect Okay, I hope you can see me Can you hear me? Yes, we can see and hear you Okay, I cannot see myself. That's distracting Okay I guess the slides just go on when I present or do we also? when Pascal, when he came up, he's not in the list anymore and I don't see the button to pay control to him. Where does that go now? I still see him at the last list he's at the top now Oh, okay. Perfect Okay, so. Okay, so you should have control of your slides. Yeah now it works. Perfect. Okay, my name is Pascal Hen, and I will talk a little bit about looking classes and PGP So most of us know the internet as some source of news or, for example, videos stream"
  },
  {
    "startTime": "00:12:00",
    "text": "or as messengers for communicating with friends or colleagues. But wen lin this group, we know the internet more like a network of networks autonomous systems bgp and so on so the internet is, as we know, fairly complex system, and the routing in the internet is based on policies and also implemented by BGP By the way, I can hear myself a little bit That's kind of distracting distracting Yeah, I don't know how to fix that, sorry. Okay, let me just do the smart move Okay, so routing is based on policies and implemented by VGP Now what we do is we observe VGP configuration from different autonomous systems through so-called looking classes. Now before we can go into this, let's talk a little bit about the background because I don't know if everybody's familiar with BGP. Now, in the internet, we have so-called autonomous systems, which are networked on their own. And those have border routers which are used to communicate with the auto-autonomous systems on the internet those connections also host bgpcc sessions. BGP, it's with the auto-autonomous systems on the internet. Those connections also host BGP sessions. BGP itself, as said, I would introduce it. We have prefix in autonomous systems and those prefixes should be distributed through the whole internet. Now A, in this example, has the prefix and uses the BGP sessions to introduce this prefix to its neighbors Those neighbors, they collect or get the prefix knowledge keep it inside, and also distribute it further to the other neighbors over the BGP sessions that they have themselves This is continued until the whole internet, in this example, this topology here, knows or has knowledge about this prefix This is just a simple example, by the way, usually policies will not share everything with everyone"
  },
  {
    "startTime": "00:14:01",
    "text": "Now the whole internet or the whole topology knows mostly prefix. In this internet, or in this autonomous systems and this topology, some of the autonomous systems host so-called looking classes And those looking classes are basically query web interfaces, as you can see here where you can select a type of query for example, give it an argument, select a router, and so on. Submit your query and you get a certain output. And this output just has in this example BGP routes inside Now, those looking classes in general are rather a fairly sparse resource, not all of the autonomous system implement those looking classes or host them So what we do in this paper or in this work, we collect their outputs for research and for operators. Operators might use them for routing purposes and researchers for validation purposes, as an example Now, how do we do this in general? Those looking classes are set are web interfaces, so they have a URL attached to them. And we collected a list of URLs that point to those looking classes Initially, we had roughly 3,800 of those URLs, but not all of them allow for scraping or allow for automation purposes So we had to kind of thin them out. We did that by looking for example, at the robots TXT, which might as allow scrapers and we removed all of the URLs that had robots TXT, that this allows scraping well from this list. We did the same thing for example, for just forbidden like forbidden automation in their policies or also if there were any captures or similar things present on its websites. In the end, and this is where the sparsity comes in, we only had roughly 155 looking glass cereals, and some of them even stopped working since that website. In the end, and this is where the sparsity comes in, we only had roughly 155 looking glass URLs, and some of them even stopped working since done. Now, how do we collect?"
  },
  {
    "startTime": "00:16:01",
    "text": "the outputs? Is the next question? Well, we have to list of URLs already, and we have a data set, which we know we will produce, right? We will also have a scraper script. Now, the first thing that we did was we collected manually configurations about the looking class URLs or about the websites that are hosted there How does this look like? Well there's again on the bottom right this uh interface and on the top left is an example configuration. So you can see there's a type of query, and those are HTML elements. So we can kind of have X paths that point to those elements and we can just interact with them using some backend, which is play right in our case On the configuration side up there, you can see that we met one of those query types to one x paths and so on. Now what we just did is we went through all of the different looking classes. There are multiple schemes for the and even met those X paths to the different elements that we need to interact with There's also a certain output field that is mostly in a new browser top that is opened when the query is submitted. There we also just collected the container information on the of the HTML element and saved it into the configuration as well With those configurations that we now have, we can run these scrapers built And this scraper basically queries after a same scheme for all of the different looking classes in our list First of all, we have to select the type of query that we want to have. In all, case, this is the BGP route. So the backend just clicks basically on this point We also collect information about BGP summary store, but the main focus is on BGP routes. The second step is to enter a prefix, which in our case are the RIS routing buttons Those are 38 prefixes. All of them have a periodic manner or periodic behavior"
  },
  {
    "startTime": "00:18:00",
    "text": "and they are basically announced new to the internet in a four-hour interval, which we can use Then the next step in our execution is to iterate through all of the different voters that are available to us on this looking class Sorry yeah, we iterate through all of the routers on this looking class and then we just submit the query for each prefix, for each router for each looking class, which is kind of a lot of queries as well This generates this output, and this output is also given back to the scraper As said, we collect one of those snapshots every four hours The output, then, the set comes back to the scraper. This is good The raw data is also saved to the data set, so researchers or whoever wants to access, this data set can do with it whatever they want Another step that we do, though, is to pass this output information as well as you can see here again this is the output and those are multiple BGP routes. As an example, this is one BGP route, and it has multiple different fields in it. So we wrote some regular expressions for each of the different BGP attributes or information that we get from this For example, this field portrays how long a BGP route is on already established and then we just passed the out for all of the different outputs that they are There are also different output formats and we incorporated regular X expressions for all of them. All of those outputs are then saved in a CSV file format into the data set as well. So we have lots of information about all of the different BGP routes that we saw about all of the different queries in the data set on four hour times well, like on a four hour basis This is not all what we did. We also included some meta information from the looking classes themselves, which might, for example, be S"
  },
  {
    "startTime": "00:20:01",
    "text": "as you can see here, which might be the daughter name, which can have geolocation information, or it might also be a different autonomous system number from the AS that hosts a looking class Yeah, the router might be in another autonomous system All of this information together and builds the dataset, which is also distributed We can send it to you Now that the procedure is clear, let's see some statistics about the data set so far First, we started the collection on the 26th of January this year, which is coincidentally roughly half a year since the start And we have even data for since the first of August, 233. This is when I started developing these scripts now we collect a total of 36 million VGP routes which is even more since I collected those numbers And the average BGP routes that we get per iteration is roughly 25,000 The maximum that we seen so far was 35,000 blocks in one iteration, and the number of looking classes is 167 in the dataset so far. This is more than the list, as I said before, and that's because of the historical data that we have as well. Some looking classes just stop working at some point We also have information about 170 ASs in there, which is because of the route, that might be present in other ASS as well And we have information about roughly 500 seconds in there. Now what can we do with this data set as well? So we use this for a study that looks into AS diversity so diverse policies on BGP If you look at this autonomous system, for example, which has multiple water routers, there might be VGP configurations in those water routers, for example, for Creific 0, we have a local reference of 100, for prefix 1 a local preference of 90. Now, researchers nowadays assume that ASS are"
  },
  {
    "startTime": "00:22:01",
    "text": "not really diverse or rather it's hard to not have this assumption in their work And this is basically saying all of the routers have the same configuration This is a severe limitation though nowadays in research and this is also well known it's also acknowledged by most researchers we can look into this with the data set itself So let's say for example for this prefix, one on the lower left corner we have a local preference of 100. This is just reflected in data set so we can just search for this. And that's exactly what we did for different BGP attributes as well. So we did a this for the S-P paths, for local preferences, and for BGP communities, where we had information for 148 ASS for the ASP attribute you know, data set. Out of this, one of these 55, so roughly 37% were diverse which is not even too Chevy, it's fairly high number For the local preferences, we had 142 ASS in total that have information about a local preferences in the dataset And for the ASBuffs, this is where those were diverse as well and for the local preference value itself was roughly 40% which is also a fairly high number For BGP communities, we expected kind of a lot more because communities can be changed between the different peers that there are We had 80 ADAS and totaling dataset and only 36, so roughly 41% were diverse in this case And with that, we already come to the end of the presentation. So we have talked about some background information, some scraper itself, so how we collect the data set, some statistics, and also about some preliminary results that we also use in our studies and with that thank you for your attention And I'm just going to put them in. Okay"
  },
  {
    "startTime": "00:24:01",
    "text": "I should hear you. Perfect. Thank you so much for the presentation and also thank you for doing the work, creating this data set That's really great. We would have time for questions West Herdica, USCIS this looks fantastic. How large is the collection? data volume, if I may ask? You mean with filesize? Like how many? bytes on your disk did it take to store it all? At the moment it's roughly two gigabytes for the past one. That's not too bad Yeah, we compressed quite a lot Okay, if there are no further questions, we actually move on, but please go and use the data set and bring more results to Mapagi Yeah, maybe if you go one slide further, there's also my mail address never mind So for people that are interested, they can also contact me. Yeah, of course perfect. Yeah, maybe you're coming to an IETF meeting in future oh yeah i want to be there in Dublin so hopefully. Okay, perfect Then let's move on. All I was trying to get there, but I was not Check a lot of slides. Don't worry about it yeah um so we got tommy pauly up next. Tommy, let me share your slides here here You will hand it over to you Okay and then we don't have Tommy as the All right, I did the past slide control, so you should have it now Go right ahead. Hello, everyone I'm tommy pauly from Apple. Sorry, I left my badge in my room. I registered, I promise Okay, so I'll just quickly be sharing"
  },
  {
    "startTime": "00:26:00",
    "text": "some information that we've gathered and mainly this is kind of like a pitch for the importance of doing protocol testing in various simulated conditions and how doing some of these simulated measurements on different network conditions can help with the design of certain protocols deployments and then hopefully also for the rest of the IETF for like as we are designing our protocols themselves before we ship them making sure that they're going to work in all different types of networks so just for background there's a cool tool that we have had for many years on iOS and macOS that if you have done development there, you may be familiar with, which is called network Link Conditioner. And you can do a quick search on the search engine of your choice and find some information i think steward over here has done some WWDC talks that taught about how to use this tool So I wanted to highlight some of the ways that it can be used to improve some protocol to deployments. So very practically, about a year ago we had a particular setup where we're trying to be using some new HTTP3-based forward proxies So if you follow kind of the work on mask, et cetera these are mask style proxies although in these cases sometimes what we're getting to is just normal TCP upstream hosts, but connecting to a proxy over quick And the intent for this project was to be using these proxies to accelerate connection setup time to help improve the time that apps take to launch when they need to reach out to a bunch of different resources and otherwise we'll be doing many, many different connections So we have a particular app we were looking at here that previously would be opening up many different"
  },
  {
    "startTime": "00:28:01",
    "text": "HTTP2 connections and fetching various resources and there was a lot of handshake times going on there. And if you are on a slow network, that could really add up and end up slowing down the App Lodge. So we had the theory that, yeah let's put this over kind of one tunnel over quick, and that should improve things So here's a rough dive theory that, yeah, let's put this over kind of one tunnel over quick and that should improve things. So here's a rough diagram of the setup. So we have a client application and these orange lines on top and bottom represent kind of the fallback direct case that they could go. It's more than actual more than two targets actually, but you know, this is just kind of an example to show the different cases. And in the new protocol setup that we were trying to take we would have a connection using hdb3 a single connection to a proxy which would then have separate streams inside of it that would do the end to end HTTP 2 connections And so we wanted to see, you know, what are all the ways we can eat? out the performance of setup to that proxy and by doing tricks like you know, having really aggressive DNS caching or having zero round trip? establishment using session tickets, etc And then being able to reuse this pipe for all the connections So some of the benefits that we are expecting to get, we would have cached DNS answers to say, DNS round trips all the time. We would be able to do zero RTT to that relay, so we would essentially eliminate all of the TCP handshake times that we would otherwise have but then also, you know, quick, on this proxy, we were able to do a lot of tuning on it and we wanted to say, you know, based on the placement of this particular relay server, what can we do about, you know, adopting BBR or other tunings for congestion? control and other things to make sure that we're going to have a good experience across all the different networks? and so a key part of this"
  },
  {
    "startTime": "00:30:00",
    "text": "setup and making sure we were going to deploy something that was useful is to make sure that this is going to perform well in all different types of networks because the point of this is, you know the app launch when you're on a very fast network with low latency, it was always always It was not a problem, but it was the outlier cases. It was the P90 cases and the were on a very fast network with low latency, it was always fine. It was not a problem, but it was the outlier cases. It was the P90 cases and above that could get pretty slow And so we needed to figure out what are the right profiles to evaluate Just some of the tips I want to share here that, you know, we learned when you're looking at something like this network link conditioner I think it's easy and there are even some examples built into the OS of like, oh, really, really high loss oftentimes that's not very realistic it's good to try to understand what are the realistic ranges of loss that you would see You know, it's going to be, you know, probably 3% or below, even in the worst cases Delay and how much you're cranking up delay for both upstream and downstream potentially trying to imitate what may happen if you queues are building in the network, that's going to be one of the most impactful variables for what you're choosing here And we found that it was really important to test with, you know, at least three different profiles for how networks look, because as I'll show you just testing in some forms of impairment even if you're saying, oh, this is a bad network you will be able to hide problems that will arise on other types of impaired networks So here are three examples profiles that we used These are just, you know, roughly the tuning here. Now, they were trying to imitate some sample measurements and, you know, here are not really quite writer representative, but"
  },
  {
    "startTime": "00:32:00",
    "text": "we essentially had one, like, here's a you know, a cable-like network that may be something that someone sees, that's not a great connectivity, here's a much slower DSL network, but something with too much delay. And then we had a sample of like, oh, you know, this is representative at least some experience we've seen with certain cellular networks in India to say, like, okay, what are some different outliers? Now, of course, these are not going to be represented of all the actual networks we understand there's a lot of diversity, but the point is to get abroad range of delays and bandwidth properties and lost properties here So as an example, so this is a CDF where we show the kind of overall app launch time for the two different paths of using this proxy and not using the proxy And so this is the overall amount of time It takes to do kind of all of the networking connections and little file downloads that you need to do to be able to like display a UI And this is kind of been an earlier test that we had done with more naive implementation not a lot of congestion control tuning without zero round trip, establishment turned on And on the DSL Lake profile, it actually looked pretty good It was about the same at the very fast kind of lucky examples. And then from around P-7, they started to diverge and the proxied case did better. And it's like, great we're we're getting an advantage from reusing this connection from how having quick there So great. But then when you look at the exact same implementation running on that in DSL profile, it had the exact opposite behavior and we were actually making things worse So that's why it's very important to be able to test on these"
  },
  {
    "startTime": "00:34:00",
    "text": "different cases. Then later on, we used these and kept repeat the experiments to do different tunings for how the congestion control work, how aggressively we would be able to have zero round-trip reestablishment and we were able to get to the point where we were consistently winning across all different profiles So then how does this translate into the real world? So this doesn't have, you know, those exact same three profile breakdowns, but we do have the relayed and direct versions that we can collect from the real world live on data for both like US Wi-Fi networks and in DSL networks as just kind of two data points that somewhat map to those simulated environments and we do see kind of a consistent win If you look at some of the P90 numbers, when we are doing those reliance numbers, like in some of the worst cases were like twice as fast and that matches up to the same numbers we were seeing in the simulated environments So it's definitely a very effective tool So the main takeaways that I just want to kind of share and start a conversation about you know, first, I think it's important that as both in doing deployments, but as we're talking about new ways to put protocols together or new ways to design protocols that we during the process of doing that design, we're testing with simulated conditions to see you know, are the protocols we're designing going to work well in all the different networks? that people are going to be living on around the world? and then also potentially as a request to this group and measurement community it would be really interesting to have more collaborative efforts to share what are some of the settings and profiles that could be feeding into these types of simulated tests to make sure we have good representative examples of"
  },
  {
    "startTime": "00:36:00",
    "text": "real-world conditions that people are seeing all around the world to make sure that as people at, I don't know, hackathons or whatever, we could figure out ways to say, test out your protocol in these different environments and make sure that you're not missing something that will impact users later on. That's it Yeah, thanks, Tommy. So, yeah, I guess it's actually important for this community to look at the protocols we do develop or the IETF develops and make sure they work everywhere in the world Yeah, Matt How much work did you do to compare the simulations to real measurements? So I had not done all of the work on kind of like how these numbers were initially chosen. I think they're was kind of like that initial input of like, let's get a lot of measurement from certain areas a couple years ago to build the profiles. And then it was more that later after the fact we just compared the numbers we were getting broadly from the field and said yeah like these look like they're rough the same so we had so you didn't depend on the similar actually matching reality you were just testing corner cases for us. Exactly. We're trying. I think in that that's the point, like you don't need to have every single network represented. You just need to explore the space to make sure that you don't have a blind spot. Yes, I strong encourage that attitude attitude Lorenzo Collidy, not super related perhaps, but one of the main issues that we encounter when we do these sort of common communication channels like proxies or VPNs is that when you're using UDP, um, session timeouts are really, really, really, really, really short on some networks and what that means is, if you put all traffic on these proxied connections, including things like hanging, really, really short on some networks. And what that means is if you put all traffic on these proxied connections, including things like hang and gets and notification channels, right so um what that means is we connect"
  },
  {
    "startTime": "00:38:01",
    "text": "these proxied connections including things like hang and gets and notification channels right so what that means is we can never get rid of TCP oh yeah right so it would be nice to know if, if first of all, if these simulated environments could simulate that, but also it would be nice to know if we have some sort of data or some common data around how common, how low those timeats are on various parts of the internet and to see if we could sort of actually change that. Because you know, just keeping TCP around forever. And like, not being able to do this for important communications means that you can't just use this for everything, and you need to know as an app, oh, no, this is a long-lived connection, so I must send it out of it being able to do this for for important communications means that you can't just use this for everything. And you need to know as an app, oh, no, this is a long-lived connection. So I must send it out of the proxy, right? So you, you, and then, you know, if the proxy is a security functionality, then you basically say, well, I can put the purse channel outside the proxy, because it needs to use TCP and security-wise, that's fine because it's already encrypted. You end up making these sort of judgments that are sort of not great. So I'm just saying maybe it would be nice to sort of either at least collect data or maybe simulate these these timeouts as well. Yeah, it's a good point Yeah, totally agreed without everything he said that's two stuart cheshire apple thank you for presenting this, Tommy I'll quickly add a bit of background that I think might be interesting to people Going back more than 10 years now, when we launched the original Apple TV, the big slab one all the software was developed with engineers with the thing on their desk plugged into wired Ethernet. And because of the secrecy around Apple products nobody took them home. And when it was ready to ship they took them home and connected to Wi-Fi and it didn't work at all I mean I don't mean the performance was bad like it it couldn't load videos it just didn't work And, um, uh, and that product was late shipping, and Steve Jobs was not happy that he announced it and then we found it didn't work So dummy net existed as a command line"
  },
  {
    "startTime": "00:40:01",
    "text": "option, but really none of the engineers knew about it and that was the driver. That specific event was what drove us to put the GUI into Xcode to really try to make this more accessible to application programmers who are not necessary experts. So thank you for raising the awareness here because that scenarios models by network links conditioner may be fairly simplistic. It's not like running all the traffic through an NS3 simulation but if all you do is test plugged into wide gigabit ethernet or you just test in your office where you've got great 5G coverage you're in for a world of unhappy customers when you ship it. So it's so important to do this Hi, it's Chris Seal First thing, really good to see this work, thank you Just a little thing which I'm sure you know already, but you call, you just refer to sell technology, but I would say that the characteristics of 3G versus 4G and less so between 4G and 5G are dramatically different. So if we're going to model it can we can separate them out into their technologies? And again, like this is a label I put there that is highly reductive and inaccurate Sure, sure. I just thank you. You didn't want to call anything out more specifically Hello, Kofi Wei on Google So I have a question about how the pocket baths is simulated here. So do we roll it right? on every pocket or do we consider temporal locality? on the pocket happening? Didn't quite follow. So I don't remember the details for the dummy net implementation But I think that's a great thing to discuss and get into it for the dummy net implementation. But I think that's a great thing to discuss and get into the details. Thank you. Thank you"
  },
  {
    "startTime": "00:42:01",
    "text": "All right. Okay, thanks, Tommy and we move on Yeah, thanks Tommy. Jamo is up next. I'm going to share their slides I'm passing control right uh Can you hear me? Yes, and great that you're here We were supposed to have last time. Oh, yeah, sorry No worries. We had enough other talks last time, so great to have you this time here Thank you. I guess I'll get started Hello, everyone. My name is Jamal Leo I'm from the University of California, Santa Barbara This work is done in collaboration with Vizet, which is one of the leaving geo-satellite production Our talk is a about our paper publishing PAM Watching Stars and Pixels, The Interplay of Traffic Shaping, and YouTube streaming QE over geo-satellite networks Are you still logged in, Jamo? Yes I'm logged in, but I cannot really go to the next slide Yeah, I have to pass it to you I couldn't, I didn't see you there. Okay, there I go. All right you should have it now, thanks. Okay, yeah, it works now Yeah, so just a bit of background of you know, why geostationary satellites, right? So and it's actually a very critical part of delivering incident access to the entire world. As we know, there's a lot of very challenging terrains, very challenging environments in this world where laying fibers to those places just become very impractical expensive, and if not possible So one of the most attractive thing about geostationary satellite is that you could actually provide internet coverage for the entire planet with just a few satellites So as you can see on the top right figure, from Weissad, you know, we essentially provide the coverage for the entire planet, just a few And as you can imagine, these satellites are actually very far from Earth because the coverage of each satellite"
  },
  {
    "startTime": "00:44:01",
    "text": "is pretty big. As a result, your data must go through a much longer distance than your traditional fiber network And inevitably, this leads to a pretty long propagation delay in a range of 600 milliseconds in comparison to your everyday, you know, optics of maybe 10 milliseconds, 100 milliseconds delay. And as you know, like these long delays inevitably leads to a lot of problems at the network level For example, like, you know, the congestion window growth could be slowed down because of the super long feedback loop So there is a lot of additional components that we introduce, such as proxies, to really mitigate the impacts of these. On top of the this makes the system really complex and really different from what we see every day. On top of that, for every one set, satellite, they provide a relatively fixed amount of resource which is shared amongst many users And similar to a lot of other wild that, for every one satellite, they provide a relatively fixed amount of resource, which is shared amongst many users. And similar to a lot of other wireless ISPs or mobile ISPs, users usually have high priority data usage every month. And once you go over the data usage, then essentially your network becomes slower. So nowadays, when you watch a video, there's different resolutions, a user might innocently watch a very high resolution, even though maybe he doesn't know that uses a lot of data and then he would just quickly depletes his data and that makes the customer really happy about it And one of the practices or the effort trying to do that, trying to prevent this happening is using shaping which is also very common amongst many mobile and SSRIPs So these are opt-in shapings by users The idea of the traditional wisdom is that if we shape our network to a rate that is well above what we want them to watch, for example, for"
  },
  {
    "startTime": "00:46:00",
    "text": "ADP videos, then chances are the users will be happy, you know, watch that resolution and don't use a lot of data And there's a lot of work actually on understanding the quality of experience of QE versus the bandwidth profile or shaping. But those work are usually dying in the settings of a terrestrial network, so we want to focus on the dual geo-satellite instead. And we choose YouTube because YouTube gives us the most market share in terms of not most market share, but a very big market share in terms of video watching So all in now, our research objectives then becomes to analyze the YouTube QE of geo-satellite networks and in the production by biosets geosatellite networks. So the are three things we have to do. The first thing is collect the YouTube and as well as the quality of service data from the network. And then we analyze the collected QE, and we're trying to see this collected QE is actually below or above our expectation. And if there's any, you know, below expectation QE then we're trying to really correlate the QS data to figure out why this is the case why is our QE subpar Right. So before we jump into the results, I just want to give you a little overview of the network As you can see, it's a very complex part, but I'll just talk about what's truly relevant. So essentially, we have a laptop at UCSB that's connected to the production via set network. Essentially, as I mentioned, the geocatellite network uses TCB proxies that breaks the connection apart into three different parts, namely CY, C2, and C3. C1 is the link between the YouTube server and the TCP proxy server. So this one is mostly fiber and it's pretty low latency, as you can imagine and the traffic shaper is also here. And between the"
  },
  {
    "startTime": "00:48:01",
    "text": "proxy server and the user proxy, here's the satellite link And in this link, you usually have 600 milliseconds And lastly, it's between the proxy between the user modem and the user So as you know, these are T-Swin proxies. But Google or YouTube actually uses quick as their default mechanism at transport layer So in that case, these TCP proxies essentially don't work and they just simply forward the traffic, right? And I also want to mention that the shaping rate, we investigate is 900 pPS because that's the very similar rate are used by other mobile and sensors ISPs. And there's a reference from the wiki paper. And also, i want to mention bBR3 is used at a time when we did experiment for the congestion control control Yeah, so let's quickly talk about the data set that we collected and the methodologies so essentially we stream 13 different videos from 16 different categories. So this gives us a total of 28 distinct videos. We stream each video five times with Quake and five times with TCP and then we collect the YouTube stats for nerd as our QE metric. So example, is the top right um basically the focus is the resolution as well as the report rebuffering events that happen during the streams And also we collect the HTTP performance log for each video chunks and audio chunks So an example is the top, sorry bottom right. Essentially, these are the if you go to developer tools and you go to next network tab, and this is what you see Yeah, so let's set our expectation first for our, for our performance, right? Since we shape at Nigel KBPS, we want to understand or look at what is the"
  },
  {
    "startTime": "00:50:01",
    "text": "video playback, what is the video bit rate of the videos we've selected So here, this is given by the graph on the left We can see that at 360p, 900 kppp is well above the video bit rate. And even at 480p, 900 kp is above most of the majority of the videos we selected. So this kind of leads us to the expectation that we should be able to watch or stream these videos at maybe a bit more than 360p with minimal amount of rebuttering events if the adaptive bitrate algorithm works pretty well But it's that the case, right? So we found out that um actually that's not the case so 30 percent of both TCP and quick sessions suffer at least one rebuffering event And the average resolution is only 404 pixels for TCP and 360 pixels for Quick As you can see on the figure on the right, Quick has a lot of sessions that has really low quality, and this is not very good. So even though we can see from here that TCP performs a bit better, than Quick, but our understanding is that both TCP and Quick actually fall below our expectations and we are trying to figure out why this is the case and how we can improve on it So, yeah so we firstly have to model the video transmission mechanism So similar to previous work, we also observe the the video and audio chunks are transmitted in sequence, like the figure on the left. And in this figure, we essentially break the period down into three different parts, right? The first the first part is the time to first bite essentially is between when the request is initiated"
  },
  {
    "startTime": "00:52:01",
    "text": "to when the request is when the first byte of the data is received. And then you have the transmission period where, you know, where the transmission take place, the data comes in But because of the sequence, so it kind of forms a sort of a pipeline we can also define the period between the end of the transmission from the previous trial to the start of the transmission of the next chunk. So this is what we call the idle period or idle time So during this time, you know, there's literally no data being transmitted and essentially just waiting for the data to come in In today's network, the idle time is usually pretty small, like in the range of 10 milliseconds because, you know, use fiber and stuff, that's very quick. But in our case, we cannot really have negligible idle time because we use a relatively long propagation delay link of 600 milliseconds So let's start with the easy part for the analysis, which is during the transmission period So in this case, we're just trying to understand when we do transmissions, are we transmitting at the maximum? capacity of the link, right? So what we found is on the figure on the right, what we found is that TCP is actually able to saturate a link at around 900 KBPS, which is shown by the black vertical line here Whereas Quick actually varies well below the 99 Nigel KBPS. That means Quick is not able to saturate the link during transmission, and the median throughput is only 630 KBPS. So this kind of suggests that maybe the BBRV3 has maybe some problem with this long-identity scenario It's something that with investigation And next, we want to understand, okay, how does the idle period raise impact our transmission so in a long latency scenarios you really want to minimize"
  },
  {
    "startTime": "00:54:00",
    "text": "the amount of time to spend idling or waiting and maximize the amount of time you spend actually transmitting So when we compute the eye time, here's the PDF, and we found that the idle time actually clustered around 600 milliseconds, which is the propagation delay This kind of suggests that the adaptive bitrate algorithm is not able to really take the propagation delay into account And, you know, so it didn't really take that into consideration. And then inevitably has a much bigger impact for smaller video chunks or smaller audio chunks because you spend you spend most of time waiting instead of doing transmissions. So you can see that on the right here. You can see the indeed the chunk size is correlated to the achieved throughput, and essentially you achieve a platform as you increase the chunk size and what makes the problem worse is that YouTube tends to request smaller chunks when the bandwidth is low and this obviously forms a vicious cycle because you'd have lower bandwidth smaller chunks, and leads to even lower bandwidth And this is not really not ideal in that scenario So just to wrap things up, actually we, from our analysis, we found that you know, performing traffic shaping to obtain a difference video bit rate is actually quite tricky in geo-satellite networks and we find that the chief throughput for TCP and Quake are only five minutes and 470 kilobis, respectively And this is very far from our target or desired 90 kbPS because of this, because of this findings, Viya said has discontinued the support for the low bandwidth daily saving options in the US business and residential markets. And lastly, as we kind of encourage content providers application"
  },
  {
    "startTime": "00:56:00",
    "text": "product designers, to really consider like a wide variety of network types and characteristics in the product to avoid such performance anomalies. Yeah that will be at the end of it Yeah. Thank you so much. We do already have questions Yeah. I have solved the problem Lorenzo Khalidi, thanks for presenting this. It's very interesting. I have a similar long fat network for reasons I won't go into here, and I've looked at some of the main streaming platforms. One thing that I saw that you didn't see here, I think it was Netflix was it doesn't just do this chunking It actually closes and reestablishes new T-Streases connections. And so each chunk actually suffers from slow start, right? And so it really takes a while. And you can literally see if things go well, it starts to kicks into this higher band right? And so it really takes a while. And you can literally see if things go well, it starts to kicks into this higher bandwidth mode. And when it tries to drive the network harder, it will actually go faster for a while. And then it'll go just like go a lot, it'll experience some loss. It'll go, it was like, oh this network is slower. Let me throttle down. So think definitely it's not just, I mean, YouTube probably doesn't really lot it'll it'll it'll experience some loss it'll go it was like oh this network is slower let me throttle down to think definitely it's not just uh i mean youtube probably doesn't re you probably reuses TCP connections because it's a streaming thing so it can't really close them, but even if you close them in some sense you get even worse now i don't know how to solve this, right? Obviously, I guess from a from a satellite perspective, you could like, say, oh, you know, we'll block quick and everyone like, oh, you're really evil. And then you like, uh, basically in, in implement, you know, TCP performance enhancing proxies and they're like, oh, these middle boxes are, but I, you're in top spot, right? So I don't know how to solve this, but it's definitely very interesting and I think presenting it may be a first step to get middle boxes are, but I, you're in top spot, right? So I don't know how to solve this, but it's definitely very interesting. And I think presenting it may be a first step to getting people to sort of understand how, yeah, that the quality of experience is not great. Thank you. I agree Thank you Hello, gorry fairhurst. Thank you ever so much for the talk. I mean, it's really good to see things analyzed at"
  },
  {
    "startTime": "00:58:01",
    "text": "different levels and try and unpick what's going on. That was really useful. There's stuff in TSV on a careful resume draft for quick which might help consider with restarting Quick or with high BDP paths so that might be something to look at I don't know if you looked at it. No, I personally have not looked at that, but I'll definitely like something to look at it. Love to follow up on that one. And also, maybe there's Compro buff this time. So it's so timely to see this talk Thank ever so much for being here Hello, this is Jay Chung I actually worked on this project with Gemma I just want to actually make a comment that like, you know we actually know that, you know, especially when the protocol either TCP or quick starts, the just little start is one of the one of the enemy to you know, improve the performance, you know the protocol, either TCP or quick starts, the slow start is one of the enemy to improve the performance for almost all networking improve the performance, you know, for all, almost all, you know, networking in a long RTT high BDP networks So what we are trying to do is, you know, we are trying to actually optimize the slow start performance And, you know, that's basically, you know the most bang for the buck. And so that's the research direction that we are actually in know that's basically you know the most bang for the above and so that's the research direction that we are actually you know heading into we have we have all the talk in CCWG on this, so if you're interested Thank you. Yeah, thank you. Can you join? the queue next time? benjamin schwartz, Meta So I heard a lot of comments about high bandwidth delay product being a source of the issue here but actually the solution to this was to increase the bandwidth delay product by, in this case, removing the bandwidth limiter, right? So actually this is system performs very well at high bandwidth delay product"
  },
  {
    "startTime": "01:00:01",
    "text": "And the thing that makes it perform badly is the traffic shaper. So I think the interesting question here is what is the traffic shaper? actually doing precisely because if all it's doing is simulating a lower bandwidth, network accurately, then maybe everything would work better. The reason I mentioned that is that as the proponents of the scone probe buff have been pointing out, the behavior of adaptive bitrate video on networks that use traffic policers to try to reduce video bandwidth is extra specially terrible and weird And so I would just suggest that you pay very, very careful attention to the precise behavior of your traffic shaping systems here Those details matter a lot and could explain the anomalies you're seeing Yeah, just a quick comment on that. So it's a shaper. It's not a police or in the sense that it doesn't drop the packets if it's, you know, above the rate So yeah, so maybe that's a yeah. So I mean, it's it's great that like trappers that are like non-policy shapers are generally, I think less prone to these kind of anomalies but even so, I think I would just want to be very careful about whether you're accurately reproducing the behavior of an ordinary network or not. Thanks Ben. Let's take Matt next and we'll close the case on this one. We're running in pretty close on time Yeah, I'm 10 years ago and we'll close the queue on this one. We're running in pretty close on time. So, go ahead. Ten years ago, I knew the people who designed some of these algorithms And there's always a lot of complexity about I'll say, anticipating content we'll put it that way. And one of the things that experiment that might be useful to do is we run these on a network where you can completely control all the parameters. So synthetic"
  },
  {
    "startTime": "01:02:00",
    "text": "data on a terrestrial network. And C see if there is delays where most where behavior changes because there might be put timers in the design of the serving that either should be adaptive or not or something like that and provide insight of that source to the implementers because YouTube would like YouTube to work everywhere and constructive feedback would actually might actually be useful All right, thanks so much John Moe. I'm going to switch over to Amanda now And Amanda, I just passed control to you so you should be able to switch your slides. I think we have you on the agenda for 15 minutes Yeah, does everyone hear me okay? Yeah we do. Great, cool, I'll just get started then. So my name is Amanda I'm a PhD student at Georgia Tech, and today I'm going to be presenting to you a first look at NAT-64 deployment in the wild. This was a collaboration between myself my two advisors at Georgia Tech, as well as Oliver Gosser, at the Max Planck Institute for Informatics. And this is also work that appeared at the passive and active Measurement Conference last March Okay, so before diving, into this work, I'm just going to give you some brief background to kind of build up our motivation So I'm guessing most of you have heard about IPV6 adoption And IPV6 adoption has increased a lot in the past 15 years And Google has reported that almost 45, of the traffic that it receives is IPV6 And this is great for a lot of reasons but it does present some problems. So when we looked at popular domain top lists, such as Cisco umbrella, we saw that about 70% of these domains are actually only available over IPV4 And so if any of these clients are not dual-stall"
  },
  {
    "startTime": "01:04:01",
    "text": "meaning they are IPV6 only, how do they actually domains are actually only available over IPV4. And so if any of these clients are not dual stack, meaning they are IPV6 only, how do they access all of this content over IP? Luckily, there is a solution and it has existed since the consensus of IPV6 and it's to use transition mechanisms to bridge this gap. And today I'm going to be talking about one of these transition mechanisms called NAT to explore I'm just going to briefly go through how exactly this works so you can understand each component An IPv6-only client will begin by trying to resolve a domain to a DNS64 resolver However, if this DNS-64 Resolver realizes that there is no quad A record, meaning this domain only has an A record over IPV4, it's not just going to return this to the client Instead, it's going to create a synthetic quad A record and give that to the client And the address on this synthetic record can be from the special use prefix that is commonly used for this purpose, as we'll see, or it can be from some other globally unique IPV6-based or some private space. But regardless, one important bit here is that the last 32 bits of the address are going to embed the 32 bits from the IPV4 address And once the IPV6 client has this record, it can use this address to then access the NAT-64 Gateway and the NAT-64 Gateway will then actually take care of the translation between V6 to V4 and essentially allow the client to access content from this domain And in this work, we sought to characterize and measure Nat664 deployments. And what are methods? is really centered around is requesting a quad A record firmers resolver, and it's for a domain that we know is only IPV4 That way, if we get a record back, we know that this is potentially using DNS64 In this work, we want to understand Nat64 from a few different perspectives"
  },
  {
    "startTime": "01:06:01",
    "text": "One is just deployment, so is Nat64 prevalent? on can we measure it? Next is if we can, how are they configured? So, for example, what kind of address space? are they using? And last is security. So if we find these Nats64 gateways, are they publicly accessible? Following that, we also sought to use a few different perspectives in our measurement methodology. The first is, as I said, we use DNS resolvers, and so we look at IPV6 resolvers from the IPV6 hit list and IPV4 resolvers from the census data set Next is we also want to understand the client perspective So we look on the Ripe Atlas platform and we determine whether or not a probe is using Nat64 Lastly, we want to understand the server perspective So if a client is accessing a server using NAT-64, what does that look like? And what can you learn? from it? So we actually set up a domain and we only create an A record for this domain and we use Raybatless probes to send various measures towards it Okay, let's just dive into the results Before actually talking about what we find, I want to explain these plots to you a little bit So first we categorize the responses from DNS experience the results. Before actually talking about what we find, I want to explain these plots to you a little bit. So first, we categorize the responses from DNS64 resolvers into what kind of addressing they're using. So either the public IPV6 space, the Nat 64, special use prefix that I showed on previous slides, or some other kind of privacy space. Next we measured or we tried to resolve a various domains to these resolvers And one thing I want to note here is at the time of these measures, these domains only had a record. So they were IPV4 only, but I know that since the time of the measurement, at least one of these now has a quad A record. But then we also looked at how consistent those results are over all of our domain measurements So how many resolvers are actually responding in the same"
  },
  {
    "startTime": "01:08:01",
    "text": "way for all these domains So I'm just going to dive in some of the maybe more obvious results that you can see from here. First is that using the NAT-64 special use prefix for addressing is the most common. So this kind of gives us a clue to almost common configurations for these deployments But I also want to point out something else that you may have noticed if you notice the y-axis of these plots. And it's that we're really not talking about a lot of resolvers here. So we measured almost 2 million IPv4 resolvers. We measured almost 300,000 IPV6 resolvers and in both cases, way less than 1% indicated that they're using DNS 64 And what we can conclude from this is that DNS64 is not deployed widely across public resolvers. And I want to be very careful with how to phrase this and emphasize public because with this vantage point for our measurements we don't have vantage into private resolvers But from what we did find, we can do just a little bit of analysis on what kinds of networks they're in. So you can see in both V4 and V6 China Telecom is in both lists And we also see a lot of different kinds of network service providers in these lists And so from this, what we can conclude is that public DNS64 resolvers are concentrated in mobile and network service providers as well as some Chinese networks I want to point out one last thing just about these results that you also may have noticed if you look at these plots closely and it's that there are some inconsistencies in the results that we're seeing depending on what domain we tried to resolve And what I'm trying to point out here is just one particular outlier for a domain And when we look deeper into these results, we found that"
  },
  {
    "startTime": "01:10:00",
    "text": "a lot of this spike is mainly due to resolvers in China education Network that respond to request for this domain but no other domains. And we can't actually pinpoint exactly what's happening here but this is something that's important to keep in mind I think, for future measurement work that involves results resolvers Let's dive into what we found when we looked at clients. So how many Ripe Atlas probes? are actually using Nat 64 for their connectivity? Really similarly to resolvers, we found that the answer is not many. We measured over 5,000 probes and only 0.7 percent actually indicated they were using Nat 64 Similarly to resolvers again, the address type distribution is similar with specific were using NAT-64. Similarly to resolvers again, the address type distribution is similar with the special use prefix being the most popular followed by public addresses We then wanted to see if these probes are concentrated in any particular network And the answer is not really, although we are talking about a very small amount of probes in general but we did find just under 25% are either in Deutsche Telecom or Melchuan, which are ISP And what we could conclude here is that clients are not largely using Nat664 However, I think we should all take this result with a gain of salt because the Ripe Atlas platform although it's a really great resource it is not guaranteed to be an accurate representation of clients on the internet at large And so whether or not this result actually represents, the internet in its entirety, we can't be sure about just from this measurement alone Okay we're going into the last result that I wanted to talk about here and it's on the security of these gateways. So are these gateways?"
  },
  {
    "startTime": "01:12:00",
    "text": "publicly accessible? But before diving into the results, I'll explain a little bit of our methodology So we, as you may remember, we're issuing requests to DNS64 resolvers, but if the synthetic quad A record that came back to us had public IPA addresses on them, meaning they're not from some private space, we actually did a follow-up measurement where we issued HTTP and HCTP requests to that NAT-64 gateway for the domain and the IP that was on that quad A record And then once we got those responses back, we compared them to control responses And what I mean by that is these are HTTP and HTTP responses we just made to that domain as if we were just trying to regularly access the content. And then we compared them and basically determined whether or not we were accessing the content as intended So what did we find? Well, uh, large, these gateways are not publicly accessible And so in HTTP, we found anywhere from one to 26% of our responses were quote, correct. And in HTTP, we found anywhere from 0.6 to 15.6 were correct Some of the responses that we got specifically in the certificates were certificates for anything from hosting provider domains park domains, or domains that had nothing to do with our measurements, such as Netflix And so from this, we can conclude that NatSix4 Gateway's on public IP are not largely accessible publicly which is actually a good thing for security because if one of these was open unintentionally this could be a problem All right, so let's just summarize what we covered in this talk. We saw the DNS 64 Resolvers are not deployed publicly at large, and we"
  },
  {
    "startTime": "01:14:01",
    "text": "also saw a lot of variants in our resolver measurements Next, we saw that clients on the ripe Atlas platform are largely not using Nat 64 although this may not necessarily be representative of the internet. Lastly, we saw that most NAT-6-4 gateways are not publicly accessible, and something that we did get to touch on in this talk is that when we actually looked at the embedding, so those last 32 bits, we did see that most of these were embedding the A records correctly I also just wanted to take some time I see that I do have a number of questions already, which is great. But clearly in this work, we're limited a lot by our vantage points for our measurement. And so if anyone has experience or your network is using Nat6.4, I would love to learn about your experiences with it. And we also generally welcome any input on our methodology and findings And you can also reach me at the email address on the screen. Thanks for listening Thanks. Yeah, we go ahead with questions Hi, jen linkova, someone who deployed V6 only network in the large enterprise So a few comments. First of all, DNS 64 is not strictly speaking required for V6 only clients using not 64 You can, if clients using C lot, you can provide them the NAD64 prefix, where other means such as information and router advertisement right so it should be completely decoupled things Secondly, you measured public resolvers, obviously for things to work correctly, you need completely different resolver to be DNS 64. For example, Google Public DNS. It's one thing and Google Public DNS is completely different whips, right? So I would be, I think it's actually broken configuration if public resolve were provides like DNS6 for answer in general"
  },
  {
    "startTime": "01:16:01",
    "text": "because it might cause some delays for the client right? So you need to be like i would say slightly more careful and also RIPATLA's probe would not probably represent much because I believe like Robert can correct me. They're mostly sitting at home, home net networks right while I would expect like dinah six like not six four devices being mostly deployed in other environments and my last call comment I'm not sure you said that you see clients using not 64 how do you know if the environments. And my last comment, I'm not sure you said that you see clients using Nat64. How do you know if they're using not 64? Because using DNS 64 and B being behind not six four, it's actually different things. You see strictly speaking can not distinguish, right? In my network, not for four and not 64 is the same device doing the same pool. So there's no way you can tell unless you know your client is V6 on Yeah, yeah. Thank you so much for all the comments. Just to answer your question about the Ripe Atlas probes the way that we determined whether or not it was used I guess, the NS64, using your correction we actually attempted to resolve what we requested a quad A record for some of those domains that we knew only had an A record, and then we were actually able to look at the responses to see what the response was Yes. So it means your client using DNS 64. It doesn't mean it is using not 64, actually, to reach the destination, strictly speaking right? It doesn't guarantee you. Yes definitely I'm stuart cheshire from Apple We've got a bunch of people in the queue, so I'll try to make this short My comments are very similar to Jens I think when studying this, be really careful to separate Nat 64 from DNS6 64 because Nat 64 is broadly a good idea and DNS-64 is broadly not When this was first created, I think more than 10 years ago, there was some notion that you put in a"
  },
  {
    "startTime": "01:18:01",
    "text": "Nat-64 gateway and then you put in a DNS hack that lies to the clients and that magic makes it all work I understand that seemed logical at the time but the reality is we only have a handful of clients we really care about, there's Mac and Windows and Linux and Android and iPhone and, you know, a few others, but compared to the millions of web websites in the world and thousands of service providers and thousands of DNS resolvers there aren't that many clients and the reality turned out that it's much easier and more secure to have the client do the synthesis locally Take a look at RFC.8 880, triple 80 that talks about that. Because DNS-exam breaks DNS security and securities in important. If the client synthesizes locally, it can actually validate the DNS answers and synthesize for itself. Using DNS4 is also a problem for the reason. Jen said that if you're on some network that uses NAT 64, but you choose to set quad as your resolver, that the whole tying of Nat 64 to DNS 64 assume both services are run by the same entity that knows what prefix it's using. So there's a whole list here clients can discover the prefix the synch prefix using router reversment options There's a bunch of ways the client can do this And my last comment that there's a home wireless networking technology called Thread that is a low-power wireless mesh for IPV-6 and it only supports IPV-6 so every thread device is a V6 only device and threadboard routers are all required to provide NAT 64 and not DNS 64 All thread nodes that want to access a V4 host are required to do their own local synthesis for exactly this reason so that DNSSEC works"
  },
  {
    "startTime": "01:20:00",
    "text": "So this is a good study What I'm saying is be really careful to separate Nax-4 and DNS-64 because they have quite different properties and quite different deployments Hi, benjamin schwartz I'm pretty sure that there are hundreds of millions of people in the world who's internet connectivity depends entirely on NAT-6-4 The fact that those people don't show up in this study is interesting. I think I would flip your conclusion and say, you know, the conclusion is that the ripe probe are missing some really important huge chunks of the network space and are really going to give us some missing results if we rely on them for anything that's essentially location related I think it's really interesting to think about how we could get to that information The first thing that comes to mind for me is thinking about how we could craft packets that won't pass correctly through Nat 64 since Nat Gatewayways are essentially ossifying, pretty restrictive We, it might be possible to, uh, essentially ossifying, pretty restrictive. It might be possible to come up with some IP packets that will work on end-to-end connections, but fail on Nat-64 and that would allow you to run this kind of tests, essentially over the web with arbitrary clients, you know, arbitrary web browser clients. Thanks, Ben Please keep it moving quick We're one minute over all the rest of the time for questions and answers today. And we've got three more talks coming up. So go ahead, Lorenzo Yeah, Lorenzo. I think the reason you won't find a lot of DNS64 in public resolvers is that the configuration of the dn64 must be tightly coupled to the configuration of the network You can only use DNS64 in the resolution"
  },
  {
    "startTime": "01:22:01",
    "text": "that has, that is in a network that is known to use Nat-64 because the network and the Resolver have to agree on the prefix to use And so that's why you probably won't see a lot of this in public resolvers. A second benjamin schwartz comment, I mean, I think you issue is you don't have a lot of ripe Atlas probes in, let's say, mobile network environments where that's where that's where not successful lot of this in public resolvers. A second Ben Schwartz's comment, I mean, I think the issue is you don't have a lot of Ripe Atlas probes in, let's say, mobile network environments where that's where Nat 64 is primarily deployed. So that doesn't mean your study is not valid, of course. It's just a means that, you know, you, you know, you found what you found by looking in a place that is by biased like all places. But I think the issue around like dns64 is that sort of it public resolve, as you wouldn't expect to find it. So that's something that we should keep in mind mind Robert Kirstecki Lorenzo stole my talk, but basically I would love to have Atlas probes in every network, but that's just, you know, not reasonable. It's not going to happen in particular we are not really present in mobile networks where as the previous commenter said, you might observe this behavior more often So I would caution extrapolating the results from the presence of our iPadlas to the whole internet The system was not designed to do that, and I don't think it can Probably you should have said that your working for RIPNCC on RIPADLES Adelaus. I work for Ripe NCC. I'm somewhat involved in Ripe Atlas All right, thanks, Robert. Thanks so much, Amanda And if you have other things, you can put him in the public chain or of course, contact Amanda directly All right, we got Philip right, we got Phil up next Thanks. Phil you. Bill you got you 15 minutes Great, thanks. I'm up front I'm Phil Roberts from the Global Cyber Alliance and I'm going to talk about our interest in expanding honey pots to attract IPV6 attacks"
  },
  {
    "startTime": "01:24:00",
    "text": "attacks And I'm not doing something correct with the advancing slides There we go. Just passed it to you. You should have it now Excellent. Right All right. So at GCAG GCA is a global cyber alliance. We're not not-for-profit, U.S. UK and Europe and our goal is to help deliver a secure and trustworthy internet. We've been operating a honey farm for, I think, six years certainly over four years Our honey farm is a collection of 200 open source sensors that collect data of a tax directed towards IoT devices When an attack comes in, we record everything that happens. We record every telomone and SSH attack. We record IP address ports, embedded U URLs, scripts, etc We've been using this open source technology, but we're only out our own honeypot technology that allows us to do a few different things, one of which is a tag attacks or detect attacks that come in over HTTP and HDP HDPS We've recorded about 200 million unique attackers over the history of the project Our daily attack they're recording the attacks detect attacks from 3,000 different ASNs, each day. And we've recorded several billion attacks so far We're doing two things with this. One is we're all about building collaboration and we're working to build collaboration to clean up on unwanted traffic originating from networks"
  },
  {
    "startTime": "01:26:01",
    "text": "So we partner with network operators, share data about attacks you see coming from their network and work with them to clean those up Second thing we do is just research and we partner with research institutions who want to look at our data. So if you're a researcher who's interesting, in this kind of data, please contact us. We've had a few publications publications already. There's a link to one there from the Max Planck Institute And there are others that are in the pipeline So if you're interested in doing work in this space, please contact us. We like to build those collaborators one of the things that we would like to do, though, is to extend this to detect a task over IPV6. Everything we do is V4 only and so we start investigating what it would take to do this. And, uh, we published a paper in the is a summary of that. And I'd like to thank Microsoft for supporting this research So how do our honeypots work? Well essentially they in imitate an IOT device, accessible on the public internet and I say an IOT device because it's really a Linux device and a lot of the attacks we see nowadays are attacks that are looking for a fairly high-powered Linux server We see things in the scripts that try to detect the capabilities of the device and deploy different things if it's a more high-powered device We require everybody to log in That's how we tell when it's an attack, but we accept almost any logger credentials. That's how we allow them to make their attack We record everything they do, but we don't want we don't allow attacks to be launched Most of what we detect is malware distribution And one of the interesting aspects is we don't really make any effort to be found. We rely on malware distributors scanning or using lists of previous"
  },
  {
    "startTime": "01:28:01",
    "text": "scanned devices. And our sensors are devices make any effort to be found. We rely on malware distributors scanning or using lists of previously scanned devices. And our sensors are distributed around the globe. I think we're in 60 different economies and all kinds of different networks So we're we want to go? We want to be looking at attacks happening over IPV6, IPv6 use as you all knows, turning upwards IP4 deployment is still pervasion At some point, IPV6 only network will emerge. And there are reports of some tidbits that is actually seen over IPB now. I don't think we're terribly well instrumented to detect a lot of that so we're thinking that maybe more instrumentation would see more. And it would certainly be good to be ahead of the curve in detecting about malicious activity directed towards IPV6 so we can know something about what it looks like and build preventative measures and we're again, we would like collaborate with others with similar interest and concerns So how Honey Potts attract attention in the V4 world. Just by example really. Honeypots want to be found and attacked. We get scanned lots every day We know that some of these are coming from security organizations You know that some of these are known botnets There are new ones that appear Typically, we're scanned over a hundred times daily sometimes by people we know and sometimes by most times by not. But it's easy as you know, to scan the entire V4 internet fairly efficiently So the model kind of breaks down when you move to the B4 world. As you all know, it's the V6 space just can't be exhaustively scanned Even just the interface ID space cannot be exhaustively scanned So it's very difficult to find devices just by doing, you know, the kind"
  },
  {
    "startTime": "01:30:01",
    "text": "of scanning that goes on in the V4 world today So this is a known problem research have been working on this. There are people who have a legitimate reasons for wanting to measure the V6 Internet and so they want about devices that are attached to it, and they are developing techniques to do so. But how do you get your device found? Well, I think the first thing to do is to ignore IETA very good recommendations about privacy and security If you follow those recommendations, their recommendations prevent scanning to prevent activity correlation, location tracking, exploits you know, if you follow those that advice, your device is not going to be easy to find as long as it's not a well-known public device at a well-known public address address So people have been researching and saying, well, how are we going to find these? things? How can these things be found? And a lot of the research is shown that it's often too easy to find devices in your public resources. Somebody will continue to server that has an entry in the DNS And if you look around the address of that device, that's provided, the public address, if you start around it, you'll find other devices that aren't public that have just been configured, you know, because that address is close to the one that we just configured And so these things have been documented People in doing this research have also observed that there are hosts numbered with just the prefix and the right most maybe 16 bits. So there's some ability to scan in that kind of space People also notice that certain things prevent detection and scanning aren't implemented in V6 or aren't enabled in V6 or aren't turned on in V6 that are turned on in V4 Of course, a honeypot would not want these protections but it is something that's been observed by recent by researchers"
  },
  {
    "startTime": "01:32:01",
    "text": "So mimicking these observes Phil we lost you in the room or like I hope we only lost Phil. Yeah, I lost them remote as well well Okay. Yeah let's wait a few seconds, but I think we can switch to the next topic Or Leslie, are you able to? Hi, Leslie, Diego Or Leslie, are you able to? Hi, leslie daigle. I can probably pick it up if he doesn't come back, but he's got a pretty craptastic home network. He should be back in a second We're really kind of hoping for some feedback so yeah, if anyone has a question or comment, you can do that now Are you able to talk to the slides? I'll talk to the slides and we'll hope that he does in fact come back. If Dave you have control of the slides again again I can share them again I think we were about, where were you? eight? Yes. Yeah, I have control of them now You want, you want this? No, I think, hang on on I think he was here. Okay, so yes, I think we're ready to go onto the next slide for now Yeah, and there are, we're not doing original research in this, Phil made use of some prior research, which is very worthwhile to go through. Next slide, please"
  },
  {
    "startTime": "01:34:00",
    "text": "Yeah, so here's the key find in this, Phil made use of some prior research, which is very worthwhile to go through. Next slide, please. Yeah, so here's the key findings. The there is a hit list of potential targets in IPV6 which is useful for establishing, to the extent you can, establishing a hundred farm in IPV6 world because it is where attackers might look Phil mentioned that the compute possibility of scanning IPV6 is, well, it's not really feasible these days, so not surprising we aren't seeing a whole lot of IPV6 scans There are some security companies that are attempting it go security companies. But then there are, it seems like there are more tests that are looking at probing networks to see if there are vulnerable hosts, which is the kind of attackers potentially the kind of attackers we were looking for So scanning is not really a viable methodology Okay, next slide please From our own research, we struggled a little bit because apparently we live in lovely remote places that don't have IPV6 internet available, so we were a little bit struggling for test sites But the thing that we noted was in spite of whatever advice there is in in i assignment and general assignment practices, these whatever size of assignment you had it was typically not changed over nine months of work so it's, there are clearly some, there's clearly some implications in terms of how, how IPV6 addresses get assigned in terms of whether you're going to be visible and subject to attack attack All right, I'm just going to go to the next slide, please"
  },
  {
    "startTime": "01:36:00",
    "text": "So building the honeypot is easy. Deploying is a little harder not only just because finding somewhere to deploy it with reliable V6 connectivity, but also the thing that we've in because finding somewhere to deploy it with reliable V6 connectivity, but also the thing that we've been leading up to, which is very hard to get found, which is a basic tenet of having a honey farm Yes, next slide, please Yeah some good advice in terms of um what to do in terms of deploying an IPV6 honeypot Honeypot Yeah, next slide, please And at this point, I would really love to hear from people comment or questions Suggestions for further work that we should do in terms of exploring the space of IPV6 honeypods Y'all wish IPV6 would go away So somebody that analyzes honeypot research is wes hardaker, USAASI. Thank you for even trying to do this because it's simply not easy to get out there. I certainly would love to compare you know, what you see difference-wise between it tax base and either one, because hopefully more sophisticated I will definitely be contacting you guys for at least data Okay, in the interest of time, thanks Lizzie for picking that up. I think picking for picking that up. I think I think we should switch to Wes's presentation or We're already past the end of the time Ben, do you have something? Let's see we got somebody in the queue who got in there Yeah, we got 25 minutes of presentation in 23 minutes, so let's switch to Wes I'll try. So I'm Wes Hurtigris. I just said, USC ISI, I'm, since this is DNA in the talk, I'm also a member of the ICAN board. I'm not speaking for the ICAN board Next slide, and when you get me control, I'll do that"
  },
  {
    "startTime": "01:38:01",
    "text": "too. I just gave you, I just gave it to you. Yep. So as a network operator, and there are a few of us that are both network operators and protocol engineers, we get plagued with graphics like this. This is a live graph of and we get all these random bumps and it's like well what are they And sometimes they're really important to figure out, especially when you're, you know, be under attack. Sometimes they're just regular monitoring spikes little tiny ones. You can figure those out pretty easily. But they're really long flat ones that are really low and narrow it's much harder to figure out So I studied, I went about thinking about this, well, how can we do more intangelo? analysis? How can we figure out what actually is in those bumps? And the way I finally decided to think of about it is what's normal versus what had been added to it by the bump right? And this is very similar to when you go see a doctor. You go see a doctor and you say, hey, I stubbed my toe and he says, take your shoes off. And you take off your right shoe and you show him at the toe and he's like, no, no, no, take off both shoes. And he does that because he does know what your toe normally looks like. He doesn't see you every year right? He wants to do bilateral differential comparison. So this is what doctors do. And I'm going to argue throughout this talk that this is what we should be doing in the network world as well. And so continuing my analogy, in a DDo attack, you know what's going on. Anybody watching? TCP dump output or just live traffic and wire shark or what it is, it's very obvious, right? Something has shifted drastically. You know that the green is now over overwhelming you in this simple pie chart example. And everybody would know what it was. The little tiny bumps jared mauch, much harder. So here are two graphs. There's actually a difference between them. Can you see the from where you are? I'm betting no So one way to figure out this right, is you know, you sort of take one, you decrease the alpha level if we're doing it graphically and you over them. And now if you look at how they overlap, you can sort of see maybe kind of where there's a difference"
  },
  {
    "startTime": "01:40:01",
    "text": "so if we zoom in you'll see that there's actually two spots There's one on the bottom, which is fairly obvious, but there's actually a little bit of difference at the top just where the percentages have shifted And this is contrived, but the important thing is, when you do this subtraction, when you take the normal out of the abnormal, you're left with just the abnormal. You take it out of the bump, your own but the important thing is when you do this subtraction, when you take the normal out of the abnormal, you're left with just the abnormal. You take it out of the bump, you're only left with the abnormal So it's a very specific space So one huge caveat is people kind of want to, you know, take this sort of idea and apply it to anything where it goes up And that doesn't work. It's got to be a change where there's a change in the type of traffic that you've been seen. It doesn't matter whether it's these are DNS traffic graphs, but it doesn't matter whether it's web traffic or anything else. It doesn't work for the diurnal patterns that you get of, well, everybody woke up and now there's just more webb traffic and it's really not that different, right? It matter where somebody out on the internet is suddenly shift something in a fairly major way, whether it's a configuration change or a traffic anomaly because of an attack or whatever. So a little bit of terminology. The way I ended up defining this in the tool I developed is, you know, there's a notion of left and a notion of right. You want to compare a left thing against a right thing where left is normally in my example is going to be normal traffic, and right is going to be the anomaly It can be anything. It can be two times slices. It can be two five it can be too anything. But left and right is just trafficless terminology So I created a tool called traffic taffy that helps do this analysis for you, and it actually does deep packet inspection. It, it's, you give it a baseline saying this is what my normal traffic looks like. This is take this slice as normal. Here's a slice where I don't understand what's in it. Show me what's changed And it does this by looking at each value and counting and numerating every value seen in every protocol field, and it has sort of levels, so you, you know, give it to see it the deep packet inspection"
  },
  {
    "startTime": "01:42:01",
    "text": "but the deeper you go, the better results you get. And so there's a couple of tools. There's dissect that just breaks things down for you in a single slice there's a compare tool that actually will do the analysis for you of trying to show you what has changed and then you can graph anything as well. And it's very easy to install It's in a Python module called Traffic Taffy It is sort of beta. It used to be alpha. It's sort of beta. So let's go back to a real-world example So this is another traffic of DNS graph. It'll do anything this example is going to be DNS based. So these were three, five, you know, time, increases, and it'll do easier things than this. At one anycast site, for those that don't know, I'm a root server operator that runs the USCISI route server And so I kind of wanted to see what's in this. Can we determine what? the right thing is without knowing you know what to actually go look for so again there's a left and a right. The tap feet can hit tool basically takes a couple of PCAP files and looks what's inside of them, tries to figure out what's normal, does a count, figures out where's the shift, where's the massive increase or decrease into the next swath and an output console information, which I have an example of it the bottom. And so this is sort of the output of what are in those two tools where it gives a colorized console output It's going to be a little bit hard to read, apologies for that, but here's the breakdown. So I did pick what this tool reported. It did this based on sudden increases or decreases. You can see that over DNS over UDP over IP It did this based on sudden increases or decreases. You can see that over DNS over UDP over IPV6, there was suddenly a whole bunch of name queries for number colon number, right? Anybody that has ever used Docker knows this is now, you know, leaked queries where somebody, put port mapping in the wrong field, right? And we got a ton of them so clearly it was a configuration mistake not like an attack. They were all looking for A records So that line came up. We returned more annex domains, because this is actually including our response traffic And the truncation bit in DNS increased"
  },
  {
    "startTime": "01:44:01",
    "text": "saying you need to come over back over TCP. You sent us way to many requests that we don't think you're a real UDP client and it all went to our our IPVs are IPV6 address to make it worse, right? This is actually not a our current one it went to our older you know uh no longer technically in production IPV6 address The important thing here is this screenshot is a full screenshot of what it shows me. I'm not clipping it out with other stuff. It actually figured out each of these things to show me. It's dropping anything else in the DNS or other packets that is unrelated to what it found I will warn you, it's really easy to chew all of your CPU. I did make it parallel processing, so it will take some large PCAP files and use everything available. I've run out of memory before that was the only time I made a big mistake So I am looking for feedback. On the website, there, which is traffic traffic, and it's read the docs.io or TrafficTafag.com, or TrafficTafag. GethubDi. I.O There's some longer video presentations. There's some that are half an hour long that give like four or five examples of how it can be used and what can be found, including some lower level bumps I've given the talk at DNSO work and other things as well. It's easy to install, and great thanks to the Comcast Innovation Fund, which actually sponsored this work I appreciate their help. Any thoughts? And again, if you have any traffic that you actually want me to help you analyze, I'm always looking for more things. I'm actually now bored when my graphs look flat because I actually want interest things now. Any thoughts? any questions? We are Laurent in the queue. Go ahead, Lauren Hello. Thanks. Looks very nice Just a question. Do you need any, I mean, engineer or subject matter expert to tag? Actually, what? looks like an anomaly? You have to give it a time slice where the anomaly is and a normal slice where the traffic was normal, and then it tries to do everything else for you. There's actually a couple of different algorithms. I don't have time to go into that Do you put any kind of rule?"
  },
  {
    "startTime": "01:46:01",
    "text": "or i mean what the tool needs to look to it's more like based on statistics. Correct. You can give it TCP filters if you want if you want to narrow it down. But no, it does full packet all the way, whatever it can find. It looks very interesting Thank you Yeah, thanks a lot. Hopefully we will get a lot of talks using this tool in future Yeah, that looks cool. Thanks, Wes. All right, so we have Jason up next. Let me get his slides up And Jason, I'm going to give the control T the control to you. You should have it now and it looks like you got 14 minutes Okay, cool. We'll see what we can do. I know everyone wants to get out on time, so I'll do my best. So my name is Jason Gertsen from Sandbox AQ. I'm going to be talking about a class collaboration that we did with DSEC looking at post quantum DNSSEC sec so um a little bit of context where we died in. This is actually a follow-up to some work that the DESC folks did where they evaluated local online DNSSEC with the Falcon algorithm but we want to look at how these PQC algorithms behave in the wild, right? We don't, if you can just run on your local machine and it works that's not a fair representation so we want to see if there's any funniness in the network. So we want to deploy some test zones and get as many measurements across the internet as possible to see things how things go and we wanted to evaluate across a couple of different scenarios, so we wanted to look at what would happen if you rolled KSK and ZSK as well as just a CSK And then we also wanted to look at what would happen if you if a name existed versus it didn't exist and then if it didn't exist between the two different versions and SEC, NSEC and NSEC 3. And then also we want to look at what happens when, um, a client does the request by UDP or TCP to the Resolver"
  },
  {
    "startTime": "01:48:01",
    "text": "and then if the DO bit was set So if you've been in the PQ space, specifically PQDNSX space, you've probably seen this table but if you haven't, that's fine. The big takeaway is at the very bottom, you can see the classical algorithms, and they all have very nice signature and public key sizes But then if you start looking at the PQC side of things, such as Falcon or Dilithium or sphinx, it kind of becomes a horror show they get quite a bit larger in some case two orders of magnitude or more larger So that could cause some issues in the DNSAC setting So we look at a bunch of these, including a staple hash base signature algorithm called XMSS, specifically we looked at Falcon 512, Dilithium 2, and Sphinx Plus 128S, and we used the amazing test network Ripe Atlas with about 10,000 probes, and we actually performed about four and a half queries. That's a typo And then we stored things such as the return code correctness so were we expecting this return code did we actually receive what makes sense Was the AD bits sets, and then also the response time? which we don't report in this because it's wasn't particularly interesting We did do some filters though so we decided that if you give the incorrect results for RSA shot 256, then you probably are giving the incorrect result for pretty much everything related to DNS sex, so it doesn't really inform us on PQC, so we filtered those out we also filtered out we due to some a limitation with the right bat list TCP implementation, we filtered out all private all of resolvers in private IP range and then we also filter out time and network errors So if you were to do a dig to one of our zones using"
  },
  {
    "startTime": "01:50:01",
    "text": "in this case, Google's Resolver, you'll get something like this, and the big takeaway is the AD bit is not sets and that's one one of our zones using in this case Google's Resolver, you'll get something like this, and the big takeaway is the AD bit is not set, and that's what you would want because Google doesn't know how to resolve in this case Dioltheum 2 so it just passes it forward to us So getting into some numbers here if you look at the classical algorithm they more or less behave 100%. You get no errors near or close to 100%. And then as you look at the PQC, I'll algorithms, and this is for BIN errors near or close to 100%. And then as you look at the PQC algorithms, and this is for bind nine, you can start seeing more and more failures for a little bit more context, which I probably should have done first So the top row is you using UDP to contact the resolver, the bottom row is using TCP, the left column the DO bit is set, and the right ball the Resolver, the bottom row is using TCP, the left column the DO bit is set, and the right column the DOBet is, or sorry, the right column the DO bit is set, the left column is not set The other thing that take away is when the DO bit is set we sometimes see higher rates of failure When we look at the power dns, settings, so this is using one key instead of two, things improve because we're transmitting less flights we see about a 15 percent improvement in general and then Falcon, which is the smallest PQ's using one key instead of two, things improve because we're transmitting less spites, we see about a 15% improvement in general, and then Falcon, which is the smallest PQ C signatures and keys, approaches less than 2% error rates. So that's quite probably quite promising if we look for non-existence, it's more or less kind of the same story as you start using signature schemes with larger signature and larger keys, you start seeing more and more failures. And we get more and more serve fails between 38 and 44% depending on what we're looking at. You'll notice there's a lot more algorithms on the side there that's because we're looking at"
  },
  {
    "startTime": "01:52:01",
    "text": "NSEC and NSEC 3. The NSEC 3 algorithm are the ones that are post-fixed with the 3 at the end. So if you look at this, the slides later, NSEC 3 is the one with the three, the trailing 3 If we look at Power DNS again, we actually use different NSEC 3 configuration here In Bind, we use the conventional one. In PowerDNS, we use narrow And in fact, we start seeing some errors for XN2 because the actual response size exceeded 64K So the message, the entire message wouldn't arrive So now correct response. This is kind of more of a high level look at what we've already talked about. So it looks at did we get the return code we were expecting as well as Sandy checked if there are A records in the response, for example And it's more or less the same sort of story You'll see that there's up to a 70% correctness, but it improves by about 10% once you start using TCP And when you have the DO bit sets, it falls apart when you're using UDP And then when we go to the single key, maybe we go to the single key scenario, we see a 15% success rate improvement, so that's nice Okay, so this is looking at the ADP bits. So for those that aren't familiar with DNSSEC, what will happen is the Resolver, if it's able to value it will then set the AD bit to mark that it's successfully validated the response and then pass it to the client. A good third of the resolvers would set the 80 bit for the class"
  },
  {
    "startTime": "01:54:00",
    "text": "algorithms, which is consistent with what we've seen and other studies have done in the past, but interestingly enough, 8.5% set the AD bit for Falcon which is quite interesting because there's no way for Falcon to be validated or unless they read our minds, I guess So that's, just kind of an interesting artifact Keep losing connection there we go and more, no difference for the CSK situation situation And this is more or less what I've said, and in the interest of time, I want to move on to one more two more things but more or less the really big takeaway is if you use large signatures, you see large failure And unfortunately, with PQC, it's kind of hard to get away or get around the large signatures situation so really quick, we did some benchmarking using the PDN algorithm testing tool just to see how things compare the top bars are your classical and your bottom are the PQC and hash base signature examples and you can see that Falcon and Dilithium in terms of their sign and key gen and verify operations are pretty performance xMSS and xMSMt mt keygen is quite depressing and Sphinxs Signing is quite depressing, so that will limit, even if we figured out the sizes, that would limit adoption for sure now um just to make sure that we're actually successfully verifying things we actually set up some public resolvers as well that you can dig. So in this example, we are querying a zone that sign verifying things, we actually set up some public resolvers as well that you can dig. So in this example, we are querying a zone that's signed with dilithium 2 and served with power DNS using a bind nine based resolver. And as you can see, in the green there, the ADP"
  },
  {
    "startTime": "01:56:00",
    "text": "bit gets sets, so we successfully validate validated And Peter spent a bunch of work, making a nice, web app for you to go and try this. This will let you select what kind of records you want It will let you set algorithm, the vendors, for both Resolver and the name server and if you want a non-existent name, as well as if you want to use Insect 3 And our next work, that we want to do a follow-up on is we want to see how Merkel trees affect things. We want to see we can use this as a comparison mechanism to handle these large peaks signatures. So the idea is your signatures will become authenticating paths in a Merkel tree. And then your ZSK DNS key just becomes the root hash of the Merkel tree that you've constructed And then the KSK is some secure algorithm we want to use, whether it's the lithium or Falcon or whatever Just kind of to give context of how, what kind of compression you can get, we had a dummy zone file that would generate 10,000 signatures using P25 you would get about, it'd be about two megabytes. If we look at dilythium, just pure dilythium, it's 38.8 megabytes so that's quite horrific but if you use a Merkel tree to compress it, you can get almost four times better to 10 megabytes And Falcon, it's a bit less of a win because the authenticating path was starting to approach the size of Falcon signature, but you still saw some wins there based on some previous work that we submitted to CTRS, we think we can actually shrink these signatures the Merkel tree, um, authenticating pass significantly but we're going to have more to talk about on that in the next coming months, hopefully, so thank you very much and happy to answer it passed significantly, but we're going to have more to talk about on that in the next coming months, hopefully. So thank you very much and happy to answer any questions you may have"
  },
  {
    "startTime": "01:58:01",
    "text": "Hi, benjamin schwartz. I think it's really cool to see a running demo of Post Quantum in DNSEC. That's cool For the public resolvers, what algorithm IDs were you used? Of course, they don't exist. Great question So we wanted to use IDs that looked normal instead of the private algorithm identity so we wouldn't get any preference or trade or weird treatment. So we use the ID 17 through 22. Okay, thanks So correct me? if I'm wrong. It seems to me that under the usual assumptions, the only thing that matters here is the size of these signatures. That is because the receiving resolvers can't do anything with them. So they're just opaque bites as far as everything else in the chain is concerned Yeah, more or less. The size is the primary concern and would be why we're seeing failures yeah so for, as a DNS standards developer, the thing that would be really helpful to me here is to have, forget about the algorithms, show me the curve of success rate versus signature size or key size right um because you know, that curve is the trade-off space that we're faced with. And that, and the shape of the curve is really interesting, and something that's super important to measure One thing worth mentioning and I can maybe show you afterwards but for Falcon, it stays under that magical MTA number for a lot of its messages of about 1,200 bytes or something like that, and then all the others exceed that number. And Falcons, is the one that gets the closest to having a non-zero error rates. When we were using TCP and Falcon, it would approach to 2% failure rates instead of with some of the signing algorithms that were quite large would approach like a 50%"
  },
  {
    "startTime": "02:00:00",
    "text": "or something. So it's quite substantial but that's a really good point thank you yeah thanks Ben. That seems like a good idea for a way to present the results And Jason, thanks so much for taking us the end of this nice session. I appreciate it Thanks to Miria for organizing the level share of the work And I think we can close out the session And that'll be, I hope you guys enjoy the rest of the week if you're there and even if you're not Yeah, also thanks to the note takers, Eric and thanks to Ed thanks Dave. See you next time. Bye, bye Thank you Thank you"
  }
]
