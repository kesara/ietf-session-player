[
  {
    "startTime": "00:01:11",
    "text": "Alright, everybody. This this will be the IRTF Open Meeting. We'll be starting in just a minute or 2."
  },
  {
    "startTime": "00:02:07",
    "text": "Alright. So Let's get started. Everybody. Welcome Welcome, this is the IRTF Open Meeting at ATF 118 in Prague. My name is Colin Perkins, from the University of Glasgow. I'm the IITF we're gonna a reasonably full agenda, so we'll we'll get started, quickly today. So to begin with, a reminder this is an IITF meeting, and by participating expecting you agree to follow the ATF, intellectual property rights disclosure rules, which also apply to the IRTF. Particular, if you're aware of that, the contributions you're making are covered by patents or patents application that are owned by you or your sponsors. Do you need to disclose that? And the IITF, prefers, liberal licensing terms and time you disclosures. And the RFC selected on the slide. Give the details of, of these policies. In addition, we make audio video recordings of these sessions available. The session is being streamed live. On YouTube and on the ITS website, and the recording will also go on to you left at the meeting you participate, and and you're you're not wearing, one of the red do not photograph lanyards, then you consent to appear in the recordings. And if you speak at the microphone, or if just just you are myself. For one of the the free speakers, then you are going to be recorded and you're gonna become like If you're participating online, please turn off your camera and my phone unless you're actively trying to speak, and, a reminder that you do turn on your camera or you speak into the online session, you will be recorded your starting we'll go"
  },
  {
    "startTime": "00:04:03",
    "text": "In addition, as a participant, you you acknowledge that any personal information you have provided will be handled. In accordance with the, ITS and IITF privacy policy. And you agreed to follow the code of conduct and the anti harassment procedures, and to work respectfully with the upper participants. Have any concerns about conduct, please contact, even myself or the Ombudsman's team, and we'll do what we can to to help address those concerns for you. In person participants, please do sign into the on-site tool in Miteshco, by scanning the QR code which is on the screen. This is how we figure out how big of rooms to by signing into me tech on-site, your your presence is noted, and we use that to make sure we have enough seats for the next So please do sign in. Remote participants, again, turn off your audio, video, unless you're to speak. A reminder that this is an IRTF session, an internet research task or session. And the IRTF is, a parallel organization to the IETF which focuses on longer term research issues related to the internet revenant Standards Development In the IRTF, we're here to do research. We're not here to develop standard And so the the focus of the the discussions tends to be, perhaps a little, longer longer time focused, a little more open. While we can publish informational and experimental document in the RFC series, and we we do publish information on the experimental documents in the RFC series. The primary output of the research groups is typically, understanding and research"
  },
  {
    "startTime": "00:06:04",
    "text": "A large number of uh-uh research groups, do not, Do do not publish RFCs at all. They just They have talks. They publish research papers. So again, the the focus is on research on our The IITF is organized as a number of research groups. There's, 15 current research groups of which 13 are meeting this week. The 2 highlighted in down blue on the slide, the the Gaia group which looks at global access to the internet, and the Network Management Research Group we'll be meeting tomorrow. The FS, the those, those highlighted in light blue have met earlier this week. So, unfortunately, you've missed them if you did not go but please do look out. The recordings are all available. One piece of research, good news. I'm very pleased to to announce that we've, recently reached out to the internet congestion control research group and I'm pleased that, Reese Ink Arts, Nvidia Girl, we'll be joining as soon as I group Reese's here this week, Vidi and Simona are remote So you see reset in there. They're in the room today. If you see them, please do say hello. We have published, a few RFCs, mostly from the graphic forum research group, which met in in the previous session. We've had our are ceased from that group on RSA Blind signatures. The SPAC 2, password authenticated key. To each algorithm, verifiable random functions and hashing through elliptic curves. And from the, Pathway Networking group, there was a a document the vocabulary of has path properties."
  },
  {
    "startTime": "00:08:02",
    "text": "And all of these have been published since the last, ATF meeting, earlier this summer. 1 of the, One of the things we have been doing in in the the IRS g, the the steering group for the IOTF is discussing, a draft which provides an IRTF specific code have come to come to The IETF has had a code of conduct for many years, RFC 7154. But it's very much focused on the effective conduct of the standards process. And if you read it, most of the discussion, is about how to effectively develop standards in a, consensus based organization and have to work respectfully together to develop standard there's not been an IITF specific code of conduct. Until, man. Draft, listed on the slide, draft in IITF Code of Conduct. Is an attempt at providing that. It's focused on a research organization and so the emphasis is somewhat different to the the IETF code of conduct which focuses on standards. It's something that it's it's an early draft. It's a dash 00 draft. It's something we've been in the IRS G for a couple of months now. Would encourage you to please read this draft Please read it. If you have opinions on the draft. If you have feedback on the draft, please, send it to the IRS g, using the IRS g at the northwest we're very much seeking your input to make sure that this says the right things to make sure it's it's an appropriate code of conduct for the organization the goal is to try and get this published by the next meeting or at least ready to publish by the next meeting. So please do read it. Please do send feedback, and we will be you'll be updating it quite aggressively over the coming months."
  },
  {
    "startTime": "00:10:09",
    "text": "In addition to the research group, the IRTF also runs the applied networking research This is something we run-in cooperation with the internet society, with support from, Comcast And NBC Universe. The plain networking research prize is here to recognize the best recent results in applied networking it's here to recognize interesting new ideas from the research community might be of relevance to the internet standards community. And it's here to recognize upcoming people that are likely to have an impact on internet standards. Them to that channel. I'm very pleased to announce that we'll be making free, ARP awards today. First, we'll go to Shiva Cut Carla. For his work on, verifying the correctness of named server implementations 2nd, we'll go to the Dennis, Troutway. Says work on contents addressable peer to peer storage. The IPFS protocol. The third goes to, Ramakrishnan Sandora Raman. For his work on identifying and locating in network censorship devices. Those of you who, were in the summer meeting in San Francisco, will note Shiva's talk was originally scheduled for them. Unfortunately due to illness, we had to schedule it, but, I'm gonna have a a remote talk today But I think all all three of these are gonna be fantastic talks. I'm very much looking forward to if you want to read the papers, there are links to them on the website listed. And, the slides are also on the website there and in the dates tracker. So we'll be getting on to those in a minute. In addition, the ANRP relies on your nominations. We've had a number of absolutely fantastic talks over the book. The number of years this,"
  },
  {
    "startTime": "00:12:04",
    "text": "this award has been running. I think it's been a great success. In order to make it a success, we need your nomination So please do, Please do nominate if you go to the the website listed on the slide. The nomination deadline is the end of next week. Both both, self nominations, 3rd party nominations with surf. Permission that are accepted If you know any good work, please do contact the offers, or if they grad students please do contact their adviser and encourage them to nominate we we we do appreciate your nomination. In addition to the, the prizes, we also run the applied networking research work up, the NLW for 2024, we'll collocate with, the IETF 120 meeting in Vancouver next next July. The office for the the organizer for that will be Simone Ferlin, from Red Hat Ignacio Castro from Queen Mary University of London. I can't see him right now. I suspect he's in the room, Ignacio, right off the back there, is the is, here. So if you have any questions about the NLW please do grab him in the hallway. And expect the call for papers in early 2024. I'll also note that, we're very pleased to offer a number of travel grants due to very generous support from Mac and I, Comcast, and Netflix. We've got figured exactly how many four or five people here, at this meeting, for for these travel prints. We will be opening the trip the call for travel grant for the the the the Risbon meeting shortly. So if if you are interested in receiving one of these travel grants. Please do look out for the announcements."
  },
  {
    "startTime": "00:14:01",
    "text": "At the URL given, shortly after this meeting. And again, thank you to the the sponsors, for providing the trial grants. And if you are interested in, expanding the travel grant from provision, please do contact me, or contact, Stephanie Buchanan from the Secretariat and we will work to take your money and put it the juice. And that is all I have to say. The remainder of the agenda, We will start with, a remote talk from, Shiva Kakala who'll be talking about, automatically finding RFC compliance bugs in name service. Following that, we'll move on to Dennis's talk on APFS and then, Ram's talk, locating it in its vague censorship devices. Okay. So let me share the slide Alright, Shida. I'll just pass you control and then do the quick construction. Find her in the list. Okay. So the the first of the flag networking research price talks today. Andre pleased is from Shiva Kolkala. She does, senior research, Microsoft research. He's interested in, researching all aspects of design and implementation of high performance network automation tools, with insights from verification, testing, anomaly detection, algorithms, and autonomous series series It's got a PhD from UCLA, I think from 2020 2 was that, if I remember correctly. And his research there was focused on using far more methods to improve robustance of the DNS."
  },
  {
    "startTime": "00:16:01",
    "text": "He receives his ANIP award today. For the paper, you see on the slide, scale automatically definitely finding RFC compliance bugs in DNS name servers. Was originally published in the use Usenix NSDI 20 22 Conference Sheila, over to you. You for the warm introduction, Colin. And good day, everyone. Today, I'll be presenting a method we have to automatically check Whether implementations outline the specifications outlined in the analysis. This research was part of my doctoral studies that easierly with mad weather's purposes. Todd Milstein and George work is in collaboration with Ryan, make it from Microsoft. The domain name system or DNS is critical, but often overlooked component of the internet's infrastructure. Can think of it as an indispensable adhesive that keep the various elements of the internet consistently interacted. Essentially, DNS is a mechanism that translates the names of websites services into the corresponding IP addresses. For instance, if you want to visit the ITS website, It is DNS that seamlessly translates I ETF dotorg into an IP address that allows your computer to connect their service. The DNS landscape is broad with multiple implementations we have a variety of open source implementations, and each cloud service features its unique DNS deep in the software needs to be absolutely correct. When doing this does not work, we did get wrong response. The software is vulnerable to being attacked by malicious parties, this can have dire real world consequences. You lose connectivity worldwide So DNS has a very unfortunate property of what we call having a large blast radius. Isn't a hypothetical regularly, security adviser is alert users to create the vulnerabilities urgent users to upgrade to secure versions."
  },
  {
    "startTime": "00:18:02",
    "text": "Female file occasions often put in the news are testament to DNS issues. You can take the decent, slack dump time down type, for instance, caused by newest glitching Amazon DOW Fifty Three's DNS handling golf buy car today in a sec. Had bites That's the data software must be virtually flawless. Before we, Before we delegate our project objectives, I'll quickly walk through the process of getting a resolution using Microsoft website as an example. Imagine a user that wants to visit Microsoft.com. They started pretty They start by sending the query to the local resolver, which is often provided by their SP. The result was first contacts a root main server. There are several of them. Each chain server uses zone file to guide its responses to queries. But illustration purposes, let's say our query reaches the 2nd root name server. This server does not want the IP address for microsoft.com. But it knows how to find dot com servers, and it informs that to the resolver. And this step by step process continues until the query arrives with the main server that knows the specific IP address for microsoft.com. The IP is then delayed back the resolver finally, to the user. And if it finally enables the user to access the design web server, All this may appear like a simple process but dearest is significantly way more complex than people realize. At its code, DNS is an integrated string rewriting system and and complexity comes from 2 main sources. First, there is no mechanism. In the example that I gave, we follow a single part of a solution But in reality, a query might traverse newest different names over us. Secondly,"
  },
  {
    "startTime": "00:20:02",
    "text": "context is computed by the mirror of record types. That are partially re succeeding, completely re succeeding, wild card records that cover an, unaddressed queries around 60 other types, of the code speed across approximately 30 RXs. To encapsulate the nature of data succinctly, and the remainder of Jeff Houston's comparison to chess Simple in the rules that govern it, Bye. Performly complex in the vein's execution. So our project aim is to ensure the data to it unless you're standing, so I pay out of this. To achieve this, we have the perimeter to automatically generate test cases that encompass the behaviors dictated by the RCS. The cuts of a challenge lies in the fact that a proper test case, India, this is not merely a just input query. But it also includes a dangerous needs of a configuration file and gets it in the zone file. Before we delve into the intricacies of our methodology, will first outline what to the DNS test. Why crafting effect to test is a complex task? They will start this with an impactful test or tool created. Which let me crash and bind and extensively address DNS software. I would like to showcase a test case that was entirely auto generated by our tools. Every test is composed of 2 fundamental components. The first is the zone file holds a suit of, resource records, defined the rules for handling quizzes. These records come in diverse forms and can be interdependent. Zone flights are not just itself because the structured with strict, synthetic, and semantic, rules to be well formed. The second comment which we all know is the query itself. Characterized by domain name and the record type. Now when the, fine server process it query, using this specific zone file,"
  },
  {
    "startTime": "00:22:00",
    "text": "leads to a crash What appears to be straightforward test case here is actually complex with both dependencies. All of which must align perfectly to trigger the server failure. First, there should be a DNA record in the zone file. 2nd, that DNA because to rewrite a to the parent domain. The DNA records purpose is to create any query, any dot.com to simply end with com. Next, the crash worker, the query itself must also be specifically for a premium recall. The final and More complex criteria is a structure of the query name. It must include the substring food.attack at least twice and the pretty much, end with We discovered that This precise recognition requires an assertion failure in the bind server causing it to crash. Bind identify this vulnerability as a highly exploitable game of service, this to get simplicity and execution. Moving forward, We will explore scenarios. Demonstrating current attack. Good level is this failed assertion remote session check to remotely exploit the, bind server. the first scenario, In DNS hosting services using binds authority to needs of implementation or available to this An attacker can exploit this vulnerability by uploading the problematic zone file 32 server to the hosting service platform. Typically a hosting service will replicate this zone 5 across multiple servers to ensure redundancy. And then when the crafted query we discussed is requested, It causes this effort in senses to crash. Right? Given that such instances often handled zone files with many other customers is indeed crash can disrupt service for all those relying on those particular needs. This role. Therefore, provides"
  },
  {
    "startTime": "00:24:00",
    "text": "Adeccus is a straightforward and remote method to launch a dealer service attack with significant repercussions for all customers hosting effective service. We will explore a second and more alarming scenario where any public buying based resolver can be compromised. I think it's always like, 1.1.1.8.8.8.8.8 fetch and cached data to respond to users, quickly. In this type of attack, the attacker has control or authority to make server that host is on file, cutting the game record. And then here is how the attack unfolds. The attacker first makes it credit for the DNAN record to the public by dissolver. The result where there is a main server and retrieves the record. And then it caches the team record from the attackers authority to name server. Efficiently respond to any subsequent queries. It becomes a denial record to the attacker. Then they said that as, especially after credit that affects the cash to record. And this causes the result when to crash. That. If you consider, by its prevalence, which estimates around, like, More than half of all data results are using buying. This vulnerability exposes significant that could potentially orchestrate distributed dealer service attack with relay to you is targeting a multitude of ISPs and public is always used by the general population. Upon identifying the renewable energy and these two scenarios, We engaged a responsibility disclosure process with the by development team. Due to the critical nature of that, the developers asked us for confidentiality parts could be prepared and distributed. Subsequently, bind issue a CV with high severity classification and urged our users to update their software."
  },
  {
    "startTime": "00:26:04",
    "text": "This one will give us a specific all supported questions by impact major distributions, and vectors including data, we want to include bugs under that. I believe in this example, I can shoot that DNS testing requires It's a great generation of both zone files, and also queries to effectively reach until the server's resolution logic. Given the complexity of the zone 5 and the query, the necessity for automated test generation becomes evident. Before I dive into our approach of automated test generation, I've explained why conventional automated testing tools fall short in addressing this unique challenge. Turning to automated testing techniques. We have first testing, which inundates a program with a random inputs. And it has been successful for many court bases. However, they fall short for DNS testing as they cannot navigate the intricacies structure of zoom files limiting their ability to test the course where you look at logic. And also 1st in class coverage guarantees. The figure on the, slide shows expected tested parts implementation in red using fuzzy. And on the other side of the standard automated testing, we have symbolic execution which traces program execution paths to generate comprehensive test. While it is directly covers but it directly offers coverage guarantees practical limitations like art explosion indicates a scalability and effectiveness especially with complex data structures and logic for India. Both techniques Face agenda, the necessary zone files, and therefore, misalaxy violations, highlighting the needs for a more sophisticated approach to DNS testing."
  },
  {
    "startTime": "00:28:06",
    "text": "We have developed a new automated testing method called Scale a small scope constraint driven automated logical execution but Dean's names about Alexey compliance. Scales back to rise in its capacity to coach and zone files and matching credits. This targeted approach is designed to extensively cover player RXA behaviors and is applicable to black box genius implementations. So what was the key insight that enabled all of this? Well, unlike typical software, network protocols benefit from detailed specifications. Although they're the segment natural language. So we leverage our private world group. We had as a DNS former model, deliver directly from the agencies to state the destination process in scale. We have transformed the formal semantics of DNS into an executable model. Through symbolic execution of this model, we were able to generate comprehensive tests each comprising correctly structured zone file and evaluated these tests are crafted to prove distinct RFC behaviors. This metaphorical approach enables us thoroughly explore the logical space of Rfc different behaviors entering extensive coverage of the RFC behaviors. We have applied the scale framework to create further a specialized tool for the automated testing of DNS implementation. For that exhaust to the generates test cases, which are composed of zone files and query based guide by a logical model derived from the DNS analysis. Given these tests, what we do to what we do that is run every needs of implement we have against the desk compared their outputs."
  },
  {
    "startTime": "00:30:04",
    "text": "Disprevinces in response to signal potential deviations from Alex's occasions which FedEx I request for further investigation. This approach accounts for the ambigities and gray areas within DNS RFC. Where difficult answers are not always established. By adopting differential testing, technic frequently used in computer verification. We cross as a behavior of various implementations to pinpoint inconsistencies. I'm not going to dive, tell people it is best fix of direct test generation module. We started the former model we have from our prior roughly, you can think of this format as a very competition tree the team is goes through in order to answer the query. I'm showing a highly abstracted version of the formal semantics validating this, validation tree. Subsequent DNS logic on a slide here, So for example, if you first look at for an exact match, at the top of the jury and then go left. Alright. Depending on if this exists. With the recently as our guide, systematically explore every possible path to the, trip. Each file corresponds with the set, unique set of conditions that query and zone file must satisfy. Up. You saw this set of constraints using a constraint server to find inputs that meet each of these constraints. So this is basically symbolic execution of the model. If performed manually, this process would be both labor intensive unpruned the error. Thankfully, the process is streamlined by Zen, a $2 per line they get at Microsoft. Zenn enables us to clarify our formal model in an suitable format which in turn automated symbolic execution process. And that is a is a set of, test"
  },
  {
    "startTime": "00:32:01",
    "text": "which are repaid of Zonfeld and queries. Want one for each unit as in the model. Up next I'm then going to one of the key challenges we faced in developing this tool. The main hurdle was to ensure that the zone file Junika Bayzend were valid. We encountered issues where individual records. Like, each a resource record was valid the zone as a whole was not valid. Due to multiple constraints during the day of Alexis. Finance sense, There can be only one email record in a, zone file, but it given domain name. The DNS utterances define many such constraints to limit ambiguous on user zone 5. Namely, performance and logic execution will produce many zone file that are not well formed. And these implementations typically preprocess to digit eform zone files, then test the Internet execution part of the query lookup logic. Our scale approach admits an active solution to this problem. We have found ways to gain a zone validity conditions as predicates is it. Whenever that's symbolic execution strings for the pipe to the execution path, we co join these predicates before then passed off to an automated constraint solver. An the result is, valid zone for inputs. It is critical to be able to generate well from zone files for testing, Burks can also work in the implementations handling of platform zone files. Since we have impacted validity conditions for zone files, We leverage Zend to systematically generate zone file that violate one of these, vulnerability conditions. The use of a small scope property in testing is a strategy"
  },
  {
    "startTime": "00:34:01",
    "text": "choice that enables more manageable and efficient test generation. By capturing the complexity of the test, such as linking the length of domain names and the number of records in a zone to a maximum of 4 the testing process remains rigorous. Yet feasible. This method of testing gains and limitations has produced around 2600 tests, there are comprehensive within the setbacks. Confirming that significant coverage can be achieved with a relatively small test. This approach validates the small scope hypothesis demonstrating that a large spectrum of behaviors within gains and limitations can be effectively approved with LinkedIn. Carefully constructed scenarios. We found that 8000 tests showed some differences among implementation. And This is a lot of test cases to go through manually to figure out what went wrong. We realize that there can be orders of magnitude fewer root causes that faith is So as a final step, we provide a simple effective technique, help users with bug multiplication. We create a hybrid fingerprint for each test which comments the permission from the test pass in the zen model with the results of professional testing. And then group test where you can get printer user inspection. So if you look at the example thing I've been here, Alcon is from the zen program band and the remaining is from the differential test. This idea is generally used in intrusion detector systems but not for staging books. Our Sendability technique reduce the failed cases to a manageable 75 groups for public examination. Our evaluation into it 18 less names of implementations, ranging from the well known buying, to the Kubernetes favorite Each implementation had at least one detector bug vading severity levels."
  },
  {
    "startTime": "00:36:04",
    "text": "Interestingly, the maturity of the data software did not predict the number of bugs. For instance,byte had 4 issues including a severe vulnerability. Let's mention it, Thomas, like, a default address unit and multiple issues but our focus wasn't critical bugs. The majority of the the identified buds across the first five top of mutations have been dissolved. Using Ferret, I integrated this 12 k test into Amazon solved 53 DNS CACD pipeline repeating but similar has a testing process there. Actually with a specific, specific remains confidential that a final example, I'll give you one example, zone 5, that crash code DNS It is, it is a combination that is frequently used in Kubernetes. The query matches the white card which redirects the query to full dot example. The really can query will match, the white card record again and so on has a code to Lou and consume resources until eventually the server clashes with the following message. And are that good? Potentially upload zone files that export a one line with you like this one, confirming the service crashing the provided services. Subtlers, subtlers, I presented this work at a DNS specific conference called DNS or and it has done this with the DNS community. To summarize game, the main technical challenge is to jointly generate a structured federation, zone files as well as the input queries to check RFC behavior complaints. We live as a small scope property of DNIS the with symbolic execution in mind, simply execute it to generate high registers that got all odds in the model. Using these tests for 30 bugs including 3 critical security vulnerabilities. Thank you, and I'm happy to take questions."
  },
  {
    "startTime": "00:38:02",
    "text": "Okay. Thank you. Alright. We've got time for, 1 or 2 questions. What's thank you. Wes Hardaker from ISI. I'm on the ICANN board and also on the internet board. And I think from all of my positions, I'd like to say thank you for working so diligently to know, help protect the DNS ecosystem. I think your your contributions are absolutely fantastic. From a question point of view, it's clear you have You have figured out that all of your test cases completely mapped to your model, so your coverage is very good. My remaining question is, do you have any confidence that your model actually covers everything, you know, really well. You didn't describe how you actually built your model from the RFC Superwell. That is it's a hard labor. So we did it manually back in 2020. So I manually went through DNS RFC, read them, and then we built a magnetic model. And then we use the model for the disk generation process here. Okay. Thank you. Jim. Jim Deets, DNS guy. Think this is great work. So congratulations to you and your colleagues for this stuff. I'm glad to see that there's a much more systematic way of trying to do DNS testing. So fair play to you for all that. I've got a couple of questions or comments, though. The first one is that you found a lot of bugs in DNS software, which is good. But they don't seem to be from the information presented here as about issues about OTFC compliance. It looks to be more like implementation bugs or problems with the soft way rather than something that's faulty within the RFCs. That's a result of causing these failures. And the second point I was going to make I've got it in my head just now, so I'll leave your time to cancel that one for now."
  },
  {
    "startTime": "00:40:02",
    "text": "Oh, no. The the reason that bugs existed is because we expected according to RNC that if there is a delay because this is how the implementations should respond. And that our model generated test case showing that if there is a team and this is expected months. But when you actually run those test case on the implementations, then buying dot readiness respond with different way compared to others. So that's how it's an Rfc companies back in a sense. If there is a dynamic code, this is how you should transport, but you could actually should have responded to that pretty and that's how we give any Okay. That's right. Thanks. And I now remember the second point that I wanted to make which is related a little bit to what we just asked a moment or 2 ago. Lot of the problem with the DNS RFC is just now particularly the early ones is there's somewhat vague about things, and I think it's kind of difficult to use those makes some kind of formal method for a testing approach and we'll find this sort of thing time and time again. For example, was being developed for realized there were a number of gaps in how RSC 1035 could be implemented, and we saw clean some of their monthly to run. So I wonder what sort of problems you've had trying to develop your models based on, particularly, the historical data really through that, I've seen are maybe not a good choice. It's to be based on That's definitely a good question. And the way that some of where, one way we try to mitigate that issue is using differential testing. So especially if you can go to glue decors, there is no right answer when should they glue the cards be returned and when not? We were not sure how to x directly model that one. So we model in one way and then let the monitored and test cases. And then we took all the test cases and compared the response across all the implementations we had. The majority of doing one way and there is 1 or 2 limitations that are deviating that we are we just inform the DNS implementations like NSG, our DNS saying that opine and not are doing this way."
  },
  {
    "startTime": "00:42:00",
    "text": "Is this what you want to do, or is that the other way? So that's how we try to mitigate those ambiguities and RFSs. That's great. I remember some of the early developers of other open source software thing is We don't understand how to employ what's in the RICs, but we'll just do what binder And so thank you very much. You've done great work, and I wanted to see more of this stuff. Thank you. Thank you. Thank you for calling Yes. Thank you. We are unfortunately out of time for this talk because, we we have sort of a new agenda, but thank you again to, Shiva, this is a a fantastic Thank you next step. His goodness. You want me to share the slides while you're bringing me on and true. Truck. Yeah. Okay. So the the second talk today is second talk today is from, Dennis. Dennis is, a PhD candidate at the University of Nottingham in Germany. And a researcher at post collapse. Is research focuses on peer to peer networks, information centric networking, and decentralization the paper who'll be talking about today, design and evaluation of IPFS, a storage layer for the decentralized web, was originally put in the sitcom 2022 Conference. Thank you so much. And thank you,"
  },
  {
    "startTime": "00:44:01",
    "text": "giving me the opportunity to, present our paper, already said, it's called the design and evaluation of IFS storage layer for the Cholson? Okay. And I'm here on behalf of all these awesome authors here that, co authored people the So, what's in today? I would give a brief overview of what IPFS is, just to get everyone on the same page And then I would basically walk you through the paper. So we structured it in two parts. First of all, we gave a design overview of how IPFS is structured, and, you know, well designed. And then we did an measurement campaign measurement study and evaluated this is designed. And at end, yeah, the we'll get some point as where to go. From here because this will only be overview, Alright. So what is IPFS? IPF stands for the interplanetary file system, and the IPFS stack is very abstract for you. A suite of specifications and tools that share 2 key characteristics which are content addressing using CIDs and CID here stands for content identifiers. Transport agnosticity and, basic so IPS is basically the, space of resources that can be interacted with using CIDs over Arbitrary transports. And judging from some hallway discussions I had the last couple of days, I should have put this bottom footnote there a little larger So IPFS is not a blockchain and it's not using a blockchain and doesn't need a blockchain. So just to get this clear, because of this very abstract definition, there are actually plenty of implementations of this IPFS specification. Everyone who has used IPFS in the past probably has interacted with he probably has used kubo mainly, which was formerly called Go IPFS, but there are also other implementations, one, notable one is Helia, which is the JavaScript implementation. And IPFS is operational since 2015. They're around 3 1000 unique notes identified by IP addresses in the network each week."
  },
  {
    "startTime": "00:46:01",
    "text": "We see 3,000,000 users per day, using IPFS. And, 120,000,000 requests per day, but this can consider this the lower bound. Because this is just from our vantage point and decentralized network, it's hard to get by with numbers, like to get comprehensive numbers. Alright. So how is IPFS designed? So what are the core principles the core, like Primary thing is well content addressing. And simplistically, IPFIS uses the hash of the content stored in the system as its content ID higher. And you can see one on, one of these identifies there. The center of the slide. But in practice, it's much much more sophisticated than just the hash the content. So this CID that you can see there actually consists of quite some metadata. And so it starts with the multi base, which is just the the prefix that defines how the rest of the CID is encoded. And then the CID itself consists of a version prefix. So right now, we have version 0, which is just implicit in version 1. Multicordic, which describes how the data itself is encoded. Then a multi hash, which, again, is some self describing hash that consists of the hash function, like, the metadata of which hash function was used long the hash digest is is And finally, the actual hash digest. So all this metadata is encoded in this in the CID here. And with content addressing, there comes some well known advantages, but also challenges. So this probably not new. So I just I don't claim this is comprehensive, but, content addressing decoupled to the content from their house. So anyone can serve content to to you and you don't need to trust the server, the content that you've received is actually the one that you requested because you can just hash the content and match it against the CAD that you requested. You get data integrity out of the box because of this hashing scheme data depth application is also something, that often is is an advantage in this case."
  },
  {
    "startTime": "00:48:02",
    "text": "And, the last point that I have here is alleviated backbone addiction. So one nice example is all of us were to download a YouTube video, for example, which is, I don't know, half a meckin size. We would put quite some pressure on the link to YouTube servers. Wouldn't it be nicer if you just download it it once in this room and could share it among each other. I'm not saying IPFS is doing this, but in theory, content addressing would make it, I think, a little it could could be the, yeah, the vehicle here. But, yeah, so the challenge here in especially in this example is as well the the discoverability. So how do I know that someone else actually hosting this content. Also, access control, which is usually solved by, some elaborate encryption schemes of the data that is stored with IPFS. But talking about discoverability, there's probably the core thing on core interesting thing next to the, content addressing itself. IPFS uses a cardemia based. It DHT. So DHT is a distributed hash table, for everyone who doesn't know what the DHT is. It's just a distributed key value store. Simplistically where the key space is just shared or distributed among all the nodes in the network. And, everyone is just responsible for a small small part of the key space. And so this enables the system to be open and permissionless. And the 2 types of records that are interesting, for now, and that are stored in this distributed hash table are the provider records. Which map CIDs, so content identifiers to peer identifiers. So every node in the or every peer in the network when they joined the network generated public private key pair and the PRD again simplistically just the hash of the public key. And then the peer records map the PRDs to the actual network addresses. Which, can then be used to connect to that peer and request. The actual content. So 2 step a 2 step process. But, yeah, to make it a bit more,"
  },
  {
    "startTime": "00:50:00",
    "text": "concrete. Let's imagine I want to share a file, like, from my last summer vacation to the friend. So the content life cycle would look as follows. So I would add this file to my local IPFS installation assume this is Kuba for now. Kuba will give me, the content identifier what what who will then do is it will look in its routing table, which a concept of this chlamydia DHT and try to, identify the closest peer in this DHT Network, this content identifier. And closeness here is not in geographical terms, but in this key space, that this cutting that EHT is using. And so we arrive at a single Well, we we arrive at the note that this, like, responsible for this particular key space, and we store this provider record, which met the CID to the PRN entity, peer ID, then I will pass off band, the CID to my friend, And what they will do first is actually contact their immediately connect peers and opportunistically ask them hey, do you have the content? Because this peer to peer network, who is actually connected to sometimes a dozen, sometimes a few hundred peers at a time. And so we just opt to opportunistically ask them, hey, do you have the content? Let's assume this is like the the reply is negative So what will happen now? The, my friend will do the same DHT walk, how we call it, to find the same part of the key space that this response for the CID. Will find the brighter record request it and download this provider record. And, we'll actually do the same step for the peer ID for the peer record, which is then, contained in the provider record. And finally, is able to connect to me, and download the data. So this is how the content flag cycle works. And there are 2 important things to point out and take away. The first thing is no data is uploaded anywhere. So Well, besides the provider I got. So this is a common misconception. So this voted from my of occasion actually stays on my machine."
  },
  {
    "startTime": "00:52:02",
    "text": "I'm not uploading it to the IPFS network. So it stays with me. At the same time, if I went down and someone else in the network would host the content, my friend could just download it from them and wouldn't even need to trust that they will just that that that they will receive the correct photo because of this content content addressing scheme. So there's this is like a trustless setup on this case, which is quite Alright. And so after we've covered in the paper, the design of IPFS, we thought of So you cannot improve something if you, if you don't measure it as what you want to improve. So we did an enlarged evaluation, and measurement campaign, and we employed 3 measurement methodologies that complement each other, and we try to cover as much of the operational spectrum as possible. we were doing network crawls. So we were crawling the So network. We were, running have deployed network probes, which are just controlled notes in the network in different geographical to measure the performance of retrievals and publications. And finally, we were looking at infrastructure loss, which not in this presentation because there's a something that I left out so far is there's a bridge between like, the location of the addressed world, which is HTTP, and this IPFS content address world, because the kubo instance has, like, or exposes it HTTP gateway where you can just pass the CRD as a path component. And then the background, Google will go ahead and resolve the CID DFS network and just serve it over HTTP lose some, some nice properties there, but, this is just a bridge. And protocol apps is operating these, these gateways, and we had access to these logs. And, yeah, we were able to analyze them. But, yeah, For context, then we did this measurement campaign in this time frame, so we have on the X X the time in 2021 and"
  },
  {
    "startTime": "00:54:01",
    "text": "leading into 2022. And on the x, sorry, on on the y axis, the number of peers or peer IDs that we have found in the network based on our network crawls. And the blue line is the total number of PRDs that we found in the network the green line is the ones that were actually reachable. And the black lines are the ones, that we couldn't reach, couldn't contact because various reasons. And for all these 3 measurement methodologies, we, have a shaded area here. So, for example, this lutiated area is a detailed analysis of our crowds, which we did in our paper, then gateway data is is the gateway data is from early 2022. And then a little later, we've, performed this CHD performance measurement. And the takeaway from this slide is basically, the network is a moving target. So when, as you can already see in this time frame, the network has grown in size by 50% by the end, of the of this measurement campaign. And, so keep keep this in mind when we see these numbers. They would point it out as well. So starting with network crawls, we so in our paper, we did for this whole time frame, full network row every 30 minutes. Which, added up to around 9a half 1000 crawls, over this period of time. And, in this fluctuated area, which was October 2021, we identified around 406 4000 unique IP addresses. From over 150 countries in over 2000 700 ASSs. And this AS distributions also in this chart in the the center here with the autonomous system rank based on the KIDAR ranking system. On the x axis and the number of IP addresses that we found in this AS. And what we've seen is, quite some centralization here because the top 5 ASs actually hosted more than 50% of IP addresses. So this is just one takeaway from here. But I also want to point out to some fellow researchers, which gave it who gave it like, they gave a talk at the beginning of the week. The cloud strikes back, which"
  },
  {
    "startTime": "00:56:02",
    "text": "look more, in more detail into the cloud dependence of the IP address networks. What we can also do with, our crawl measurements is measure the peer churn. So this means how how long do when Pierre joins the network, how long do they stay in the network before they churn. So go out of the network, go offline again, which influences several network YD permit is like record. Replication, for example, because it's the peer churn is high. We want to replicate these records. Provider records as in probably more peers. To keep them alive in the network or routing table refresh rates, which again, it's like a concept from this cardemia DHT. And, so here we have on the X axis, the uptime and hours, and then how long these on the Y axis, how long, which percentage of peers stayed that long in the network. And in this example, we can see that first, Pearson from Germany, state in the network for 70% of peers stayed for 4 hours or less online from Germany, for example, This is from 2021. On the right hand side, they have the same graph, but not split by countries, but overall for, I think it's last week or the week before. What we can see here is, probably a little too small, but the takeaway here would be it hasn't really changed over the last 2 years. At the same time, what these graphs graphs don't show is the number of very stable peers. So this baseline of very very stable peers that, has been in the network. They don't show up in these graphs because they just don't churn. So, the right hand side, which is from as I said, 1 or 2 weeks ago, we have, like, a is a baseline of sale, a piece of 85 to 90%. So this is the amount of PS that are actually super stable. While in 2021, this was only around 55 to 60%. So this is something that has changed. Then as I said, we've deployed some network probes to measure the DG key performance, and, So we we deployed"
  },
  {
    "startTime": "00:58:01",
    "text": "7 different AWS region, a Coupa instance, and in turn instructed a single instance to write a provider record or yeah, to to publish content in the DHT and then instruct all the other notes to actually to to retrieve this data. So in this case, AF South 1 has published some data and then we instructed all the other nodes to retrieve the data. Then next round, eucentric publishes a piece of content and we instruct all the other ones, to retrieve the data. And in this process, we collected latency measurements and so on and so forth. And in total, we published around 3000 CIDs. And 14,000 CIDs were retrieved, than afterward. And this is all random data, so no one else in the in the network should actually know of that data. So no caching also going is going on. Looking up data in the in the network actually consists of as I said, looking up the actual network addresses with this provider and peer records, and then finally connecting to the pier and downloading it. The first part of identifying these records as in the middle this the actual download or the content exchange is on the right, And the total duration that the users would experience is on the very left. And here we have the, duration in seconds or the latency, on the x axis and then CDF, again, on the on the y axis across these different regions. And so what we found back then is that 80% of requests from In this case, the EU resolve in under 500 milliseconds, and I'm pointing the central central part out because this is this is the, I would call it the cost of decentralization because this is something that, in the location based address world. Just don't have to do because we know the address right away. But maybe DNS but, Yeah. So this is just the overhead of decentralization, how I would call it. Alright. Ah, sorry. And I don't have as much experience, but from what I've heard, this is actually, pretty fast for unstructured and permission to CHT Networks. But I'm happy to"
  },
  {
    "startTime": "01:00:02",
    "text": "to hear something else from from from the audience on that. On the flip side, publishing data is a orders of magnitudes worse. So, application latency is is orders of magnitude worse, especially 2 years ago. This is on the left hand side. We again have the duration and seconds on the x axis and the CDF on the y axis. We can see that it took around it it can take up to 2 minutes to actually publish data into into the network, which is especially for delay intolerant or delayed sensitive applications, probably nothing Yeah. It's it's just Yeah. It's it's not not good, and it actually has couldn't be used in these cases. This has improved. So on the right hand side, this is also from, last week. Some data. There, the median publication time is around 6 or 7 seconds. And again, this is due to this stable baseline of of nodes that support the network, today as opposed to 2 years ago. And with that, there's much more, so a cliffhanger. In our paper. So we have I only covered content content life cycle in in my presentation now, but there's much more to the design of IPFS, which is in our paper, I said, the public gateway usage docs are also interesting to look at. Looked at cloud provider dependence with this caveat this point or 2, these fellow researchers. Looked at geographical distribution as well and specifically compared HTTPS performance to IPF switch real performance and called it Big West Stretch that we defined there. And with that, where to go from here, we've published all our data in, on well, on IPFS, you can download it. And I think although I mentioned that these IPFS moving target, there are some time insensitive analysis you you can do. So if you're interested, use our datasets. Some call outs. So we are not operating in a vacuum, and we are some other fellow researchers who look are not affiliated with progress as I am, who are who is one of the main contributors to IPFS,"
  },
  {
    "startTime": "01:02:02",
    "text": "So I want to point out their their work as well, which is shift on, yeah, so they have done great work there. And finally, I'm part of ProLab Within Product Lab and we figured Again, APFS is a moving target. We should probably continuously measure what is going on in the network and all the graphs that are more up to date that you've seen in these slides are from our website. So check that out. And, we publish weekly reports on the IP s. Network on stats stats.itfs.network And, well, there's a myriad of this is a there's plenty of future work that we could do. For example, content availability is something, that could be looked into, severe network conditions something, something, something, I think no one has yet looked into And, yeah, content routing latency is still not satisfactory, I, at least, from quite a few delay sensitive applications. And all the measurements tool tools that we've built, I actually built on top of Lipitorp, a generic networking and they can also be applied to different P2P networks with it, which are not IPFS. So broaden focus and making a comparative a comparative study. Is also something. And all the tools are also open source linked from our website. And, yeah, with that. Thank you Okay. Thank you. Okay. So we have a couple of minutes, questions, if anyone has any questions for us. John Levine. Yeah. Thank you. This is interesting. I was wondering if you're able to characterize sort of places people are storing the data? You know, are they in hosting providers or a random you know, random cable modems or what. You mean, so where the data actually start? Yeah. Whether it's in the cloud or for example, in, on home computers? Well, it's on the cloud. It's sort of, what bits of the cloud. Right. Okay."
  },
  {
    "startTime": "01:04:03",
    "text": "In this particular study, we haven't looked at the content itself. And this very question was one of the leftover questions from the study and Again, one of the other researchers looked into exactly that, I can come come back next year. Come back next year when when they might be here. So so it's not it's not our paper that that I'm, I can get it It's just hang on. This is the IMC picker. Yeah. This is the IMC at the bottom right, they looked at exactly this question, because this was one of the I said, leftover ones, from, well, from from our paper, and I'm not saying because, but This was just not not covered in all paper. Okay. Thanks. Yeah. Okay. Thank you. Any other questions? I I guess one's from me. I mean, that, you know, that this is a a content address system. It's Does does that have any implications for privacy and being able to trace your who's looking at what content? And is is there any mechanisms to sort of help because that's yes. So there are some some, yes, some big implications, and let me go back a few slides here. Especially this first the first step from example that I have with my when my friend is asking, their immediate neighbors. So they're basically give giving away what they are interested in. So they're brought, so as I said, Kuber's usually connected to a few dozen to a few 100 nodes, And the first step that they do is broadcast to all of these notes. Hey. I'm interested in this CID. And is is is this has definitely privacy implications. Similarly, these provider records also have the CID and plain tech and there's some efforts right now to, enable redirect privacy and their concept of Sam employing private set intersection with this first in in this first step, for example, and, provider records to be thought of"
  },
  {
    "startTime": "01:06:00",
    "text": "doubly Hashing the CRDs and just request, the yeah, the hash of the of the hash, basically, Yes. Okay. But, yeah, it's but they are implication certainty. Yeah. Yeah. It's important. Let's let's work underway. Mhmm. Okay. Cool. Cool. Cool. Children. Hi, tutor Osborne, DNS Research. Pre duration. I'm new and not too confident with, my wording here. So just on the same content question, So it's peer to peer, and it goes through different peers. Does that mean that the content that is distributed peers are responsible for that content. So if it is something kind of Let's say that it's, illegal content. Does that mean everyone is responsible for the illegal content. Well, as I also said here, so the view content will stay on your machine still If you're participating in this DHT, you would probably help out with this illegal content by hosting these provider records, which which would point to the person who's hosting the malicious content for, for example. So this would be yeah, this would be the the the problematic part here, but to answer your question, you so no one else would your content unless you instruct your your your your local installation to to do that. Yeah. Okay. Uh-huh. A a a u. Sorry. Thank you. A u, Missus, Future Services of Europe. You for your presentation. Brief question. How do you, just or explain, improve the performances that you showed maybe in this the next slide. Later on you mean, the last one, I think, the one that you show the graphs, Yeah. Like, a huge improvement in performances. How do you explain that? Right. To be honest with you, I guess. So so when when you so in in in my slides earlier, I showed that I'm storing this provider. I could with just single peer,"
  },
  {
    "startTime": "01:08:03",
    "text": "But in reality, I'm storing it with 20 peers to combat this peer churn that I mentioned. So, your So I'm I'm trying to identify the 20 closest peers to a certain CID. And if if the share of peers that are undilable so I so so my reason here was, 2 years ago, peers. There was a significant fraction of the peers that are actually not reachable, and now this fraction is intently lower. So when you try to publish something into the DH or write something into the DHT, you will run into these undialable peers super often. And then you try to contact this particular peer, and then you will time out. Then you should contact the next one, and he will time out. This will just happen very frequently the share of undilable peers in the network is high, And on the right hand side, so which is just as I said, 1 or 2 weeks ago, the share of unreachable peers is in the order of 10 10 to 15. May maybe maybe only 10%. This pillar, happen much less frequent. If you won't, you will time out less frequently, And this is the reason why, the Yeah. Why this publication time is much lower? Thank you. Quick follow-up. If we change I I would say the peer network can use, other, resources that are farther away. How do how would that, impact this performance What what do you mean with further way? Because most of the the resources of how whether in Europe, I guess, figure test Right. So so when when I mean closer, just to clarify, everyone, when I mean closer, I'm not meaning, like, geographically close, but then it's key space. Oh, okay. Okay. Okay. Let me do that. Okay. Okay. Thank you. Okay. Thank you. One last question. Rochelle from the foundation. Could you go back to the slide where you were discussing about the 20 different implementations And in the the very first slide? Okay. It take take some time. Sorry."
  },
  {
    "startTime": "01:10:00",
    "text": "It's alright? I was there. Okay. Yeah. So a lot was that logo over there. That's 5 cone. Right? Right. Okay. So Could you tell us a little bit of the difference of their implementation since they are block blockchain based? Right. As opposed to Right. Yeah. So the and that that that's totally right. And, this is just with this very abstract implement, specification here. Like, with this abstract definition of what IPFS is, this will definitely also fall under this under this umbrella of being IPFS. Though I mainly well, I only covered like what the common sense IPFS is, which is the Google network that they will interact with. But, yeah, file con is also using IPFS because it's using CIDs in this sense. And, actually plenty of peer to peer networks in the web 3 decentralized web ecosystem, using CIDs. Yeah. And so the these are usually also then IPFS in that sense. But, yeah, as I said, so this this definition at the beginning is very, very broad. And, only see adhes with the the defining characteristic here. Yeah. Okay. Yeah. Alright. Thank you. We're a little short on time, so we have to I left there, but thank you again. K. Okay. Alright. So the final speaker today is"
  },
  {
    "startTime": "01:12:00",
    "text": "Ramakrishnan, Sundar Raman, Ram is a a PhD candidate at the University of Michigan, his research interests, are in network security and privacy. And this in his research, he uses empirical methods to detect, analyze, and prevent large scale security threats to the security and privacy of incident uses. He's particularly interested in the study of internet censorship. REIT's work on the sense of planet Observatory, global censorship measurement platform. His talk today, is, on measurement methods for locating and, examining censorship devices it was originally published in the proceedings of ACM Konext 2022. And, I understand he's on the academic job markets. So if you like the work, yeah, you know what to do. Ram if it's here. Is in the projecting. Okay. Trushing it again. I'm not sure why that's not like I'm trying to make Let me just share it from here instead. Yeah. Just say next slide. Okay. Sorry. I guess I had some animations loaded that I thought would work, but"
  },
  {
    "startTime": "01:14:00",
    "text": "you know, obviously, immediate. So it won't, but, yeah, thank you calling for that inter and and, you know, I wanna say first off, thank you for the in our pre committee, the ARTF, and all the sponsors giving me this award, this is, joint work with one eye Jonathan from Princeton Jacob from Citizen Lab and Royer from Michigan. And in this work, we were trying to, study the or examine the technology that performs in terms of shape. Thanks, ladies. Right. So in the past decade, we've seen you know, censorship and surveillance of unprecedented scale, such as the shortly of Twitter in Russia and the blocking of So social media websites in our end that's currently active now. And these censorship events are made possible by today, a very sophisticated, deep packet inspection technology that is available on network devices. These devices, can inspect and filter large amounts of internet traffic. And the increasing commoditization of this technology has made it available to most governments in ISB worldwide. So research from the amazing internet Freedom community has shown that studying network devices and understanding their use can lead to positive changes for internet freedom. So for instance, research from our collaborators to citizen lab show that the, Canadian filtering vendor NetSuite their devices that are being used by many ISPs in different countries, for blocking access to lgbtq And upon advocacy based on civil labs findings, see if we actually remove the option to blog websites based on this category. We can clearly see that this has led to some positive changes. But even even, despite the success, these studies have been few and far apart in the past. Exactly, please. And this is because most censorship studies have understand understandably focused on what content is blocked from which locations. So there are censorship measurement platforms like Uni and sensor planet, which we operate in a lab,"
  },
  {
    "startTime": "01:16:03",
    "text": "that could tell you right now whether a client in Prague is able to have access to Next late fees. But what we are interested in is learning more about the technical implementation of censorship itself. So like where it's located, who's manufacturing these devices. House and behaving. Previous studies on the subject are focused explicitly on specific censorship is very well known. Such as the green firewall. Next slide, please. And this focus on specific, systems is because of a set of challenges that are associated with studying network devices more broadly. So this includes the fact that understandably there is very little transparency in the world of censorship, both from the devices, the vendors of these devices as well as the actors driver. There are also a large variety of censorship mechanisms that make monitoring really hard. And finally, you know, research such as that performed in citizen lab involves a lot of forensic will work, which is really hard to scale. Obviously. So what the community needs right now is a set of general purpose, robust, and reusable methods to study these censorship devices. Next So this is exactly what we set out to do. So we built general purpose tools for studying, specifically 3 aspects of censorship devices. Where they're located. Who manufactures these devices and, what are their rules and triggers? Next slide? Awesome. So let's start with, how we can identify censorship devices. So we built for this purpose, we built a general purpose censorship place. Now, Trace is an age old technique that helps you know, locate, network path between a client and a server by sending packets, it's incrementing PTL values. And each router on the path will, you know, decrement the CTL value by 1. And when it expires, the corresponding router, we will send back ICMP time exceeded message, through that message, we'll know the IP address of router So now now we can map the entire path between the client and the search."
  },
  {
    "startTime": "01:18:02",
    "text": "What we are specifically interested in in this case is an application late phase. So can also carry data on the payload. Because these SCTP and TLS, packets are usually what is the target Next day. So let's consider the same client server environment. But, now with the censorship device at R3, that blocked censored.com.com. Next slide. if you do the same trace right now, as long as we don't So sensor.com, the payload, we'll see that, that, you know, the censorship device does not act, and we'll continue to get an ICMP time exceeded message from R3. Exactly. But when we do, since sensor.com, when the detail is high enough to reach the censorship device, we expect to see some indication of censorship. But how does his censorship actually manifest? Next slide. Here, unless the child's the problem is that there, you know, there could be a variety of responses depending on how the device is deployed and how it behaves. And accounting for all of these behaviors forms Traysort mechanics. It's like So for instance, you know, there are different types of censorship methods So some devices injected TCP reset to close connections while others silently drop packets forcing connections to time out. Next slide. Sensitive devices could be deployed in different ways. So impact devices can process packets at line rate and modify traffic while on cloud devices only receive a copy of the traffic. They can only inject future packets into the connection hoping that they'll win the race against There are also problems with, you know, certain devices try to hide what they're doing actively. And, some of our measurements could pass through the device while others do. And, you know, in in in our censorship research, we will explicit mechanisms to deal with each of these challenges. And I'll go through a few examples in today's talk. Say? So for example, to detect in Patrice, injection"
  },
  {
    "startTime": "01:20:02",
    "text": "we look for, a TCP reset at TTL3 and no other accompanying package. Because this shows that the censorship device is in part because it can prevent the the, the actual packet from going up to the router processing. Insight. In contrast, if if the censorship device is deployed on path, then in addition to seeing the TCP reset packet, we'll also see and ICMP time exceeded packets from Adi because, you know, the packet goes through pass the censorship device to the, to the route of processing actually. To look at a different mechanism, the censorship device shopping packets, then we'll see no response starting from TTL3 all the way up to any maximum detail that we use. And finally, you know, to to account for path variance, we actually perform multiple tracer. To build a probabilistic estimate of the, path between the client and the server, we look inside that to see where the censorship has actually happened Perfect. So, you know, we built our censorship, research, which we call Centres, And then, we performed, measurements in four countries. Specifically for the study. So asir, Belarus, Belarus, Kazakhstan and Russia. And in each of these countries, we've performed both the GDP and the stress roads. And we perform 2 types of measurements. The first is in country measurements that we obtained access to vantage points within these countries and then sent measurements to destinations outside the country, specifically to North America. And we also performed remote measurements to increase scale because it's really hard to obtain accessible vantage points So we also perform measurements where we send packets from our vantage point of the university Michigan to multiple public organizational servers within these countries. And this help has decreased our scale by a lot. Looks like So for each of these cases, we performed, 2 two traysorts. One with a possibly censored keyword, let's say, censored.com in the payload. And another control threshold with the benign keyword, example.com, and the payload."
  },
  {
    "startTime": "01:22:00",
    "text": "It's like And by comparing these two trace routes, we are able to know, identify the exact location of where censorship has happened. Next slide. Okay. So let's look at a few results from our trade show. Okay. So this figure shows the in country trade shows that we performed from our vantage point in Kazakhstan. Here, the red arrows show the location of censorship And we can see here that the censorship actually happens in the ISP that's immediately upstream to our client. And a s 9198 is, in fact, a classic telecom, the state owned ISPSCASC So this figure shows the remote thresholds that we conducted in Afghanistan in in Nassar Weijan. So the root note on the left is our measurement machine at the University of Michigan. And all of the endpoints of the right are all endpoints in a self paced And here we can see that the, censorship happens on the first top into the country. So here, it's likely at the internet exchange point. You say? This figure shows our remote measurements in Belarus to turn points in Belarus. And here, the we saw that the censorship occurs quite close to the user in a residentialized. So we're seeing a wide variety of behavior here that sensorship can be implemented Next slide. This, this, this video shows the remote measurements that we performed to, endpoints in Kazakhstan. So there are two interesting things here. One is that most censorship happens again quite close to the users, but in an upstream ISP, But we also saw some cases of censorship where it happens way before the endpoint. Next late. And when we actually zoom into some of these cases, we can see that censorship actually happens in the Russian even though our probes were listen to endpoints in Kazakhstan. So we saw a bunch of cases like this where censorship policies of ISPs in one country affected traffic going to or from another country. And this is significant implications on how censorship has been reported so far because, you know, usually censorship is reported"
  },
  {
    "startTime": "01:24:00",
    "text": "are based on the network or the location of the host or the endpoint. But we show that censorship would actually happen somewhere completely different in Next slide. Our, measurements also had, other implications for censorship women in general. So we saw that much of the censorship actually happened at the endpoint itself, which indicates local policies and not ISB blocking and might warrant a slightly different level of scrutiny. We also saw that some devices, actively try to hide what they're doing. Or or, you know, they're harder to detect because they do things like copying, IP header values. Exh. Okay. So, you know now that we identify the network location of censorshiphip the next step that we wanted to perform is identify who the who manufactures these devices. Excuse me. From the, information that we collected with the trace loads, you know, we we have some knowledge that can help us with this For instance, if the device is deployed in path, then frequently, we'll know the actual IP address of the hardware that is, you know, hosting the, firewall software. Next slide. And using the, the IP address, we can gather a bunch bunch of information. Like, we can perform you know, handshakes on application with protocols and color panels on any open ports. And even if there are no open ports, we can still perform, packet level fingerprinting techniques like nmap, and collect lower level Next late. Okay. So using using, these application led protocols, we collected a bunch of different banners then we investigated these banners both manually and also using popular fingerprint database, like rapid service recall. We also investigated, some of the block pages that these And from these block pages, we were able to see that most of the censorship in these 4 countries were enforced by the ISP consists using locally grown technology. To Next slide."
  },
  {
    "startTime": "01:26:04",
    "text": "But what I actually wanna focus on in today's sorry, the graph the graphic in this slide is in is in clear, but I actually wanna focus on in in today's talk is the 19 network devices that we found that are manufactured by commercial including manufacturers like Fortinet and Cisco. And We are seeing that, you know, these these companies usually develop security firewall that are designed to prevent attacks. But we were also able to see in some context that these devices can be used for a dual use that for content locking of legitimate network traffic. So, you know, the the same technology that is used to block network attacks and stuff like malware can also be easily repurposed and reconfigured leads to make network access. And we saw that, you know, network administrators were using this device in these countries to block everything from gaming to social media websites. And, you know, we didn't we didn't go our study doesn't go into the appropriateness blocking because that is subjective. But one thing that I really wanna emphasize here is that the lack of transparency and auditing can easily lead to the misuse of network firewall And I think there's a significant need for developing guidelines and standards, especially here at the if if for, you know, more auditing and more transparency in in in the censorship ecosystem, especially when it comes to, these commercial providers. Next slide. So, this is a good start, but wanted to go even one step further. We wanted to ask the question of whether you know, censorship devices in in different deployments across different countries behave the same way or not. Next slide. So, you know, we we went back to the question of how does how is censorship implemented? Right? A key insight is that even though censorship at a high level follows the same techniques, there's some amount of deep packet inspection of the traffic and then somewhere that the censorship system reacts But actually, the lower level implementation tends to vary quite a lot depending on the network ex,"
  },
  {
    "startTime": "01:28:00",
    "text": "these devices Right? So we wanted to see whether there are specific idiosyncrasies that we could And next slide. We can go on further. For this purpose, the, develop a censorship further that we call Sunfirs. That sends slightly different traffic to see how the censorship device behaves. And this This gives us more insight into the rules and triggers of the censorship device. We are effectively reverse engineering, then the book stacks here for instance, let's say that a normal HTTP get request for sensor.com's plot, Next slide. We see that even changing the request in slightly different like capitalizing the, post keyword in different ways. Manages to evade some devices. And, you know, we we we created a bunch of strategies like this. Next slide. So we use the, you know, HTTP and TLS to identify key parts of that could be passed differently by a middle box. I think I'd like to note here is that our fuzzer generates both, valid and invalid that is not RSC complaint. Request, but that is completely fine because we don't need the correct response from the server. We were just trying to see how the middle box behaves to different requests. Looks like. So these are just, a few of the, you know, the HTTP strategy that we developed, next slide. Run through a few, we, you know, we used a different HTTP methods other than get, like, post put Next late? We changed the path in different ways by adding parameters and other values It's like And we also removed or capitalized certain parts of the, HTTP method and other parts of the request to see how the middle box behaves. Excellent. this shows a subset of our results. You can find a full, set of our results in the paper. So But I'd like to point out some interesting things here. So we saw that, you know, changing the GP method at the patch. Evaded a lot of different devices in all four countries. But interestingly, we see that, you know,"
  },
  {
    "startTime": "01:30:03",
    "text": "changing the capitalization of the HTTP method. Specifically in Russia. And the same way, you know, change the capitalization of the HTTP word if it's some devices, specifically in a separate there are clearly some of these cases where, you know, some of these requests were able to evade certain devices in in a specific country. And, you know, we can use requests like this to then fingerprint these devices. Deflate. We can go on further. And, you know, this this shows, a subset of her TLS fussing results. Again, we can see that changing the Cypress or, the version, even the TLS version, actually successfully made some devices in Russia. And it this this can actually be useful for circumvention too. Exlect. Okay. So now we have collected a bunch of different features from know, from our trace rocks, from our active probing, and from our our fuzzy. Next slide. Naturally, the next thing that we did is put all of these together. And then, you know, try to see are the similarities between different devices in these countries and what are the differences? Next slide. So we took all of the features that we we've collected so far and then we've clustered these features using the DB's scanner And then we label these clusters using the vendor information or the ISP information when this is available from our panels of blockages. Excellent. So this shows, the results of our clustering. So the Y axis here shows you know, the number of devices in different countries And then the X axis, each each bar on the X axis is a different cluster. Next slide. There are 2 interesting things here. The first is that, you know, maybe maybe a very, in a very expected manner devices within the same country or ISP form really, really tight clusters. But but I I another interesting point here is that even within the same country that are different clusters within the among different ISB. There are different diff device deployment. Next slide. And we also saw that, you know, clusters from devices to the same vendor also found really tight clusters."
  },
  {
    "startTime": "01:32:04",
    "text": "And, you know, we we can identify cross country deployments of the same devices. This matter. So, that is a summary for study, and all of our code and data is completely open source. So you know, we hope that our tools and our reports democratize the technical in-depth investigation of, these censorship devices across the world. So if you wanna find more information about our work, I'd point you to paper in connection between 2 and also, reports on since the planet and the open technology fund. I we think that, work future work in this and this direction is solely needed to highlight policy gaps with respect to the spread of information controls globally. Next slide. Yeah. And we also have, know, very ambitious future directions for this work. I think, it has been very useful to already researchers, at at Central Planet, Atchelson Love, and other researchers 1. Right? So we're trying to, we're actively integrating these techniques into censorship light houses, like, planet and money. This will this will help us collect, you know, data in more countries including, you know, other than I would also love to work with the ITS community to, and encourage more transparency in the, in the censorship ecosystem. So one thing that can help monitoring a lot is enforcing standardized error messages and blocking mechanisms, and this can both help, you know, the researchers like us who are actively trying to monitor this but also inform end users of the reasons behind block. And I think this is super important. We can even go one step further and encourage some of these commercial division and just to publish their block list for auditing and this can help with you know, avoiding any false positives. And finally, I'd also like to, you know, encourage the community to adopt privacy preserving standards like 0 non attributable boxes, that can, you know, give more privacy to the users and encourage auditable blocking."
  },
  {
    "startTime": "01:34:01",
    "text": "And we can go on further. Okay. So I'd like to leave you with the slide with the key takeaway I'm happy to take any questions. Thank you so much. Thank you. We'll start Okay. with, Mallory since you're there, and then we'll go to the the queue online Thank you. Because I my my devices of brick right now. Thanks for your talk. Was really interesting. Background, like, slide 30 132. I thought to myself, I've seen this before. It's when, censorship or convention tools that I'm familiar with. That now have a lot of different techniques that they use are trying to figure out how the thing people are trying to reach is blocked. So they kind of get to this point where they could really leverage what you've done figure out what's being blocked. And then the tool or the VP PN or whatever is they're using, then more straightforwardly can decide how to get around the block. So I feel like your application, you've taken it in some obvious directions for additional measurement. And making that better. I'd also like to think about how, the work you done can make the circumvention piece. Easier. Absolutely. Yeah. I think that's a that's a great comment. So we actually saw that some of the requests that our facility was generating was actually able to you know, bypass the sensor and get the correct content from the from the server. So I think that's definite value for the circumvention community, and for tools like Geneva that that perform some of these 1st requests. Hi. In one of your slides, we have seen only IP before at the IP addresses. As you try to do with ipv6 to connectivity in this case was the measurements the same. a great question. So in this, study, we only considered ipv That's addresses. We didn't look at ipv6. Although,"
  },
  {
    "startTime": "01:36:00",
    "text": "That's, you know, something that's an easy extension because we can use the same hot limit value for conducting our trace outs, and for performing the other measurements. The reason why we used only ipv4 is that bunch of the data for a remote measurements and IP addresses. All came from platforms like sensor planet and Uni, which only have ipv4 data available. That's how we selected our endpoints. So that's why we restricted ourselves to IPV forth, but I I think there's a great future action, and I expect to see these This is, a generalized video lead towards ipv62. Okay. Thank you. Yep. Can't read it. I think am I next in the queue Chris Patton? If if if Okay. Hey, Rom. This is great work. Keep doing it. Thank you. If you could wave a magic wand, and change anything about the TLS charter or the HTTP tar charter or any working group in the IETF that's working on standards that are relevant to censorship. What would you What would you say? What would you tell those people to do? That's a really, really, really good question. So I think the most realistic thing that I really want to happen is, again, encourage more transparency. Right? So there's a lot of forgiveness about anything that's happening here. Censorship is obviously a very you know, like, opaque from both the people that deployed and from the people that provided the knowledge to deploy it. And you know, more transparency from either of these would would benefit a lot. And this includes things like you know, actually, providing error messages that say this is the reason for the blocking. This is who is performing the blocking. Right? And even, you know, providing our open sourcing blockless can help us understand why certain website is blocked. Maybe it shouldn't be blocked, and we can make more in a weekend advocate towards the removal of that blocking. Thank you. Yep. Hi, Mike. Will be UKNTSC. I think you can mostly just answer my question, but"
  },
  {
    "startTime": "01:38:01",
    "text": "Is there any reason why these firewalls that are dual use should be able to transfer, should be able to block something sort of hyper factor. They are in indeed blocking it. That doesn't seem like something a firewood should be should should be up to you for such security reasons anyway. Yeah. I mean, I've, that's a great question. I think we've seen that most of a a lot of this firewall technology, you know, they provide their own domain categorization. That's not just limited to classifying something as an attack or not. They also classify, you know, what type of website it is. And then they provide the network administrators the feature to be able to go and just click those categories to block. Right? So, yeah, if I if I am using 1 of the security firewalls right now, I can go and say, I don't want any users behind this device to be able to go to social networking website or something. And then the real danger from when these five walls are inappropriately used by you know, large networks like ISPs to block access to residential users who are not aware of this. Me. Gotcha. Very much for the search. It's really interesting. Thank you. Yeah. Hey, Ram. I was gonna ask, you made the comment Thanks. as to, you know, investment in things like your knowledge of boxes can help here. I wasn't quite sure I understood it, but I think conversation with Chris sort of clarified it for me, So I just wanted to say, for the talk. Even though I caught the end of good work. Thank you. Alright. Thank you. Thank you again to all all three speakers, actually. We we free absolutely fantastic talks today. A reminder, A reminder that the the ANRP only exists because of your nominated So if you know any good work, you want to see the talks next year, please do nominate Thank you, everybody. Hope to see you, some of you in Brisbane in, I guess, March it is. And, again, I look forward to your nominations, etcetera."
  }
]
