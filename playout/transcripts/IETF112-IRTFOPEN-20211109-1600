[
  {
    "startTime": "",
    "text": ""
  },
  {
    "startTime": "00:03:41",
    "text": "all right so i make it a couple of minutes after four o'clock and i see that all of uh award winners have joined and the number of people still joining seems to be slowing down so in that case i guess we should get started so my name is colin perkins from the the university of glasgow"
  },
  {
    "startTime": "00:04:00",
    "text": "i'm the irtf chair and and this is the irtf open meeting for itf 112. so i want to start with the uh the usual note well and reminder of the intellectual property rules for the irtf um the irtf follows uh the the same sorts of intellectual property disclosure rules as the ietf you know if if you're aware of uh contributions um if you're aware of patents or patent applications that cover the contributions that you make that are controlled by you or your sponsor uh you do need to disclose that uh or not participate in the discussion um uh perhaps uh a difference from the ietf disclosure rules is that uh the irtf expects you to to file these disclosures in in a timely manner in a period of um days or weeks rather than months uh and we expect the the most liberal uh licensing terms uh possible are made available for irtf stream documents and if you need more details about this process um the um that the rfc is listed on the slide uh rfc is 5743 uh 5378 8179 uh give details of this and the url iot irtf.org slash policies slash ipr also has links to these and more details on on the process um also uh as should perhaps not be a surprise uh the irtf makes recordings of online and in-person meetings and this includes audio video and photographs and so on uh this session is being streamed uh live uh i believe we're going out on on youtube um as well as being uh the the meet echo session and as well as"
  },
  {
    "startTime": "00:06:02",
    "text": "being recorded uh and the recording will be on youtube after the session as well um if you participate online um if you turn on your camera and microphone and make sense of the microphone then you are consider in the recordings and i'd also remind you that the chat logs uh in all of these meet echo sessions uh are recorded and they're available on the ietf site so what you put into the chat is also recorded um i'd also like to remind you that we we have a code of conduct in the irtf and in the ietf uh and you know as a participant uh as an attendee at the the irtf sessions uh you agree to um work uh respectfully with the other participants in the session um you agree to to behave in a professional respectful and polite manner and you know we we have the code of conduct we have the anterior harassment procedures which you see linked from the slide that go into a lot more detail about what we mean on that if you have any concerns about behavior of the uh various participants um the the ombuds team uh again linked from the slide um is available to to um investigate those and and to act as a point of contact if you have any any issues with the the participants in the session if you provide any personal data uh and in order to register you would have had to provide at least a little bit of personal data that's handled in accordance with the its privacy policy and again that's linked from the slide so what are the the goals of the irtf well the irtf is is there to provide a"
  },
  {
    "startTime": "00:08:03",
    "text": "forum for longer term research issues um while the the the parallel organization that the ietf focuses on shorter term issues relating to engineering and making standards the goal of the iitf is very much to conduct research we're not a standards development organization and while the irtf research groups can publish informational or experimental documents in the rfc series the primary goal of the research groups is not publishing rfcs it's not publishing documents uh it's to promote um collaborations it's to promote research it's to promote development of research ideas that relate to the the internet protocols applications um architectures and technologies we have a very nice rfc that spencer dawkins wrote a few years ago rfc 7418 which gives an irtf primer for ietf participants which talks about some of the the differences between the goals of the irtf and the ietf um so if you're not familiar with that uh i would very much encourage you to have a look at that the irtf is arranged as a set of research groups um we have uh five research groups which are highlighted in dark blue on this slide which are still to meet later this week uh the crypto forum research group uh develops cryptographic algorithms and looks for an understanding of various cryptographic algorithms and techniques uh many of those uh are used in itf standards uh the computation in the network group is looking at all aspects of in-network computation programmable networks uh and"
  },
  {
    "startTime": "00:10:00",
    "text": "the impact of programmable um forwarding hardware programmable control planes uh on the internet architecture uh the pathway networking group uh is looking at um the the interplay between the endpoints and path elements and how that communication should happen uh the privacy enhancements and assessments group uh which uh my apologies to the chairs i noticed this was listed as a still a proposed group rather than a full group in the plenary slides is looking at privacy enhancing technologies and the human rights protocol considerations group is is looking at the impact of standards on human rights and the way we can reflect uh various principles from human rights in into the um the types of protocols we we develop in the community we also have a number of groups which have either met uh or or um even met already or not meeting at this session uh including the the map rg which uh had a bunch of interesting measurement papers uh looking at the network architecture in in the previous session and the network management global access and congestion control groups which met yesterday the research groups in in the irtf um require um some understanding of of the various research questions and problems that that are interesting in a particular area and are looking into understanding that the relevance of those problems and questions to the internet they're built around a particular community which is trying to conduct relevant research and that has been interested in bringing that research to the irtf and they're built on an understanding that there is a benefit for conducting that work in the irtf"
  },
  {
    "startTime": "00:12:01",
    "text": "both for that community that brought the work in and for the irtf and the broader internet community the goal of these groups is very much to understand a problem right that they're not there to lend support to particular outcomes they're not there to produce documents in in the way that i i etf groups are the goal is understanding rather than products rather than rfcs in the irtf groups that said the groups can publish rfcs we have had one rfc published on the irtf stream since the last meeting which is rfc uh 9106 on on the argon 2 uh memory hard function for password password hashing uh which came out of the crypto forum group but as i said a minute ago that the main focus is on developing understanding uh and in um you know the vast majority of the research groups i think research papers uh and discussion and understanding is a much more common out output than the the rfcs so um that's what i wanted to say about the irtf um there's a couple of other activities uh been happening uh which i i want to to mention in in this introductory talk the the first is that the internet architecture board has a program running to develop a new model for the rfc editor function and this program was created after the departure of the previous rfc series editor i guess uh a year 18 months or so ago i forget the exact timing um in it back in the times when we all used to meet in person in wonderland hotel rooms so i guess it was a while ago"
  },
  {
    "startTime": "00:14:01",
    "text": "um and that that program has been um developing a new model for how the rfc editor function uh is is organized and arranged and arranged the model they're proposing has some fairly significant changes to the rfc editor function um it's proposing to to change the rfc editor role to that of a consulting editor which will be known as the rs series consulting editor um he'll be an expert that will provide guidance to the community to the uh rfc publishing streams and to the rfc publication center and it's proposing to shift the um the the the responsibility for oversight of of the evolution of the rfc series um from the internet architecture board to a new working group um which will be known as as the rfc uh fc series working group um which will be a a working group which is independent of the the i i etf and the irtf and the independent stream and we'll just act as an oversight for the the series um and once that group reaches uh consensus on proposed changes to the operation of the rfc series um there'll be the the the proposals will then be reviewed by an rfc series approval board which will comprise the uh the rfc series consulting editor um and representatives of the various streams um which are likely to be the irtf chair the ietf chair and the iab chair although they can delegate that if they need to but it's shifting the responsibility of of the management of the rfc series from the iab on to the community now this may seem like uh a fairly uh you know a bit of a process arcana but it's actually a fairly significant"
  },
  {
    "startTime": "00:16:01",
    "text": "change to the administration of the rfc series going forward uh and potentially has the ability to impact the way the irtf can publish documents since if the way the the rfc series changes the way the irtf music changes um the new model the the description of this process uh is described in an internet draft that the iab have published that this program have published uh draft iab rfc ed future rfc ed model if i got the uh the draft name right that's just completed last call in the iab program and is about to enter it a community wide last call for comments so i i would encourage you as participants in the irtf to review this document and provide feedback to the rfc editor future development program they have a meeting tomorrow uh in the 1430 utc slot uh they also have a mailing list by which you can provide comments uh and as i said please do review this document please do send feedback on whether you think what's being proposed it is or is not appropriate so as you see fit all right onto um perhaps happier happier notes than um the arcana of the itf and irtf process um the next thing on my list today's is to uh congratulate one of our research group chairs uh vansant and i i believe marie jose has something to say on this topic murray jose are you there go ahead you hear me you hear me okay so okay so"
  },
  {
    "startTime": "00:18:04",
    "text": "somebody has echo um i this morning actually it's a it's a new thing this morning at 11 a.m uh paris time uh weinstein um received uh his award is another chevalier of the audre du merit of scientific signed for scientific work of the french government and while the award was given to him for this contact tracing um system or or protocol that they put together for the french government it was actually highly mentioned in the um presentation from the french government official uh that veins was a major contributor to the ietf with 14 rfcs which i think is about i think there's two french people have tons of rfcs the other one being crystal utema of the iab and so that was actually mentioned as a major contribution also his work in the irtf which i am very much aware of but what was mentioned was also i think what we all came to appreciate of him the hard work the the community spirit the fact that he's always open to new ideas every team leader um very committed to the people he works with to also to his family and to the environment it was funny for those of you who know him that he had a shirt and tie and a jacket which is like first for me but i think it was highly um like it was highly deserved and i think uh we always forget that there are people like weinstein who are not like the most vocal and who are a little bit quiet but in the end do all the major work and i felt this was such a great recognition of all his achievements and and i'm absolutely proud of having been"
  },
  {
    "startTime": "00:20:07",
    "text": "yeah think thank you murray congratulations to to vancouver as you say this is an incredibly well deserved award uh panson has been uh involved in in the ietf and the irtf for very many years and has made some some really tremendous and really important contributions so i think that this is uh astoundingly good news all right continuing the good news um one of the uh more pleasurable uh parts of um being the irtf chair is is that we get toward the applied networking research prizes which we organize in in conjunction with the internet society uh and with support by by comcast and nbc university uh these places these prizes are awarded um for some of the best recent results in applied networking um for some interesting new research ideas which may potentially be relevant to the internet standards community to the internet research community and to recognize upcoming people that are likely to have an impact on internet standards and technologies in the future and in normal times we would bring these people to to an ietf an irtf meeting and i would be introducing them into you in person uh and um we we could uh chat with them in the breaks and bring them to the to the related sessions um of course in in these times that this is not necessarily possible but uh i i i hope you i hope they will be uh uh around for the week and then similar breaks and and you can chat with them and uh we can try to get some interaction going but uh it's my pleasure to to announce that the the winners uh of of the applied networking research prizes uh"
  },
  {
    "startTime": "00:22:01",
    "text": "which are being presented at this ietf meeting there's this irtf open meeting uh uh thomas vertigon uh for his work on uh extensibility of bgp implementations and other routing protocols uh they have to uh axa cash f for her work uh looking at third party service dependencies on the internet and they go to kevin bock for his work on internet censorship and the the evasion of various censorship techniques so congratulations very much to the these three speakers uh we will see the uh the talks in in a few minutes um if you do see see these people around and i see they're they're in the chat here but if you see them around at the rest of the itf irtf sessions or during the breaks uh please do talk to them please do congratulate them i think this is some really nice work we've got here and in addition to that the nominations for the 2022 applied networking research prize are now open um the the deadline for these nominations is the uh 19th of november so you have 10 days so if you know any um any interesting applied networking research um if you know any good papers in this area if you know any any people who you think will make interesting contributions to the community any work which you think is is especially valuable and isn't getting the recognition it deserves then please go go to the website there please make the nominations and we very much encourage both um third-party nominations where um with someone's permission you nominate someone for the price but also self-nominations uh if if we you know if so if you think your work is deserving of the price and isn't getting the recognition as it deserves then please do nominate yourself and if you're shy about that reach out to me uh and i will uh happily"
  },
  {
    "startTime": "00:24:00",
    "text": "uh discuss this and you know highlight whether i i i think you should be dominating the work and and be as encouraging as i can to to get as much work in as we can and for the community um if if you have um you know as i say if you're worth work which deserves the price uh please do nominate with that uh we'll move on to the the talks uh we have three talks uh we're running a little late but i think not all of the talks use their their full time uh so the first talk uh is by toma uh talking about uh xbgp um and in a certain amount of irony that the talk entitled when you can't wait for the ietf uh is presented in the last of the three uh award talks of the year so i guess there's something to be said the uh last of the three irtf meetings where the award talks are presented in the year um access talk on uh third party service dependencies we'll follow that uh in about 30 minutes and then finally we'll have kevin talking about censorship and server-side censorship evasion so uh with that uh i'm done uh we will move on to the first of the pre-recorded uh videos um which is by thomas uh vertigon my apologies if i'm mispronouncing my name thomas received his master's degree in computer science from van in belgium and started a phd looking at the extensibility of routing protocols under the supervision of olivier bonaventure he's currently working on techniques to improve the program programmability of bgp um which led to the prototype xbgp system that he'll be talking about in this talk and his research interests include distributed routing protocols"
  },
  {
    "startTime": "00:26:00",
    "text": "programmable networks and system architectures now he'll be talking about the paper xbgp when you can't wait for the itf and the vendors which is originally presented in the acm hotnet symposium in 2020 and i think thomas is in the chat so if there are any um questions for clarification any quick questions during the talk we can ask them in the chat and then he'll be available to answer questions live after the talk so thank you and meet echo if we can play the first of the videos hello everyone my name is tona and i will explain you xbgp a new way to add flexibility inside a routing protocol so why it is needed to bring programmability to a routing protocol inventing yourself as being a maintainer of an autonomous system the current best practice today is to have routers from different vendors but the problem with that is that you don't have a unified interface to configure your routers and the second problem is that if you would like to add a new functionality inside your network then all the routers must implement the functionality but all of the routers do not implement the same set of functionality and this is a problem and the other problem is that if you would like to add a new functionality which is not standardized or implemented to the routers then you need to pass to a standardization step by the ietf in average for bgp it took 3.5 euros on the slide you can see the standardization delay of all bdp features so we measure the time since it is released as a draft to the moment it is released as an lfc this is not an isolated case what are routing protocols suffer from the same delay but the least i have shown that this is also the case for other lfcs next you need to implement your feature through the operating system vendors and finally the"
  },
  {
    "startTime": "00:28:01",
    "text": "routers must be updated but if you are a small network such as bullet you cannot easily influence step one and step two because you are you don't have enough in fact to actually convince routers vendors and the itf from your new feature because of those problems we introduce xbgp that is designed to bring back innovation to the network operator we would like to open the bdp box so that the network operator can actually modify the protocol i will explain you how hvp works in practice we start from bgp which is considered as the black box you can configure bgp with multiple interface for example cli netconf or snmp to make some monitoring so those are the main interfaces to configure but you'll not have any way to modify the protocol you as being a network operator xbgp will open delete and will expose the protocol in the protocol memory inside the protocol you can add a new state you can modify existing states and also you can modify the transition between two states concerning the protocol memory you can access through the routing table you can also access to other internal desktop structure now the operator thanks to the interface of edgp can now add extension code to their router so they write the extension code once it will compile to a bytecode and this my code will be executed to all the routers which are usbd compatible now actually is mgp let's add it on the top of operating system vendors now the routers can be seen as a kernel where you can actually run some extension which is the modification to your pdp protocol now i will explain you how enzyme work inside bgp but before i will take an example of a new feature which is not standardized by the itf and not implemented inside any operating system so what we would like is to add the"
  },
  {
    "startTime": "00:30:01",
    "text": "geographical localization of the router when i see the vdp update to make some for example filtering or some analysis this information about the geographical localization will be added to the bgp update that will be spread to other routers of your network and so that other voters can use your functionality that you have implemented and then in order to not spoil the order neighbor you will remove your geographical localization from the mdp update if you would like to add this new feature inside the mdb protocol then you need to think about how bgp implementation works normally all wdp implementation follows the rfc 4271 and inside the rfc 4271 it explains the bdp workflow first you have bgp message that comes from your heels it will pass to the ajar bin then inside the import filter when it was the import filter it goes to the locality all the acceptable route inside the long calling will be taken to the bgp process to take only one baseboard that will be pushed to the forwarding table of the routers and then putting inside the export meter to take whether or not the route can be exported to other networks and if it is the case it will be stored in the rj board and finally the message will be generated if i take back my geographical localization extension you need to modify multiple steps of the bdp protocol and those are represented by the blue arrows with the bdp it is possible to modify the protocol thanks to the introduction of insertion point insertion points are a way to execute arbitrary code and so if i take back my geographical localization it is composed of multiple subpart and also part will be added to the right insertion point but that's not all because our extension must talk to the protocol member to"
  },
  {
    "startTime": "00:32:00",
    "text": "retrieve the geographical localization and this is known via the xmdp api and this is something that makes the interaction between the plugin and the protocol memory now i will explain you dxpgpl this is some way to communicate with the vdp implementation and all the extension code or plugin must use this api to retrieve or set some protocol memory why first because it ensures a designation of the plugin and the gp implementation and second it is a unified way to access to the memory remember that xbgp must support multiple pdp implementation name xvdp api contains multiple functions such as function to send and read bgp messages also the function to get or set some memory inside the protocol memory for example for the geographical localization you must access to the geographical localization of the router then you use the getter function you have also function to access through the routing table and underneath utility function to manipulate the mode memory or doing simple maps function so now we have the big mixture of xbgp and i will show you some new skills that you can do with ldgp but before you need to adapt some bdp implementation to make them xbgp compliant you have a favoriting and board in our case in the slide i put an array with all the lines of code that you need to modify your ad to make the bgp implementation xbgp compatible the first use case is a simple one this is the monitoring and what we would like to do is to monitor the length of the ice path actually it is difficult to achieve it with traditional interface but within gpa you have an interface that is easier why would you like to monitor the is pass for example you would like to monitor it to filter out large aspars or make analysis afterwards so now i will explain you a complete example of c code that represents the"
  },
  {
    "startTime": "00:34:02",
    "text": "retrieve the data from the host implementation this is known true get arc function so this is a function of the api then you will do your computation so here you will pass the response attribute and count the number of autonomous system contained inside the path and then you will use another function to actually log the number of as you computed this extension code will be added to the import filters the second use case is related to the use of bgp inside data center in data center you use bdp to make your internal routing all the best practice concerning the use of bgp in a data center is stated in lrc7938 inside this document there is a constraint you don't have any exact path so for example to go from level two to level zero you must go to level one and level zero but you cannot make something exactly between level two and level and one for example and to avoid this zigzag path you must have the same autonomous system number for all routers of the same level over here level 0 all routers have the as1 number but these constraints make a debugging difficult if you would like to check where the route are source from you cannot because all the paths have the same autonomous system for the same level and so you cannot decide if the packet goes from the left part of the data center or the right part but now with xvgp you can get rid of this constraint and put a different autonomous system number for the rotors on the same level to do so you must first design your xbgp extension and pass them a data center topology configuration so that you can actually check if there is any exact path to your data center this extension will also be put on the"
  },
  {
    "startTime": "00:36:01",
    "text": "insertion point related to the import filters the third use case is related to the road selection so in this slide we consider a stop network for the source network connected to several transit transit 1n102 and to reach the network destination bgp must advertise a route to the source the source will directly receive the green road via transit one and another one on transit 2. according to the local 22 policy it will either announce the blue or the yellow path maybe transit 2 choose the path for its security properties nurses prepare the path with a longer latencies and so the ranking of the source and the transit does not match xbtp can solve the problem to execute a plugin that will choose the route on the edge router of 202 according to the source ranking network and so transit to choose the root the source preference this mass election service will be added to the decision process insertion point the last use case is above detecting bgp zombies if you know a prefix p then your motor will announce that it can arrange in the prefix speed and the order router will do the same by advertising to the neighbor the the prefix speed and so all the motors can reach me but now consider prepping speed is no longer reachable so the rotors will send a result to the neighbor but for whatever reason due to a software bug or misconfiguration the router cannot process the result and so it fails to send a missile to the other label the upstream router will still have the key prefix in their forwarding table this is something we don't want because it may lead to some black owning or traffic and this is a big misogyny this is a route that is still in the theme of some"
  },
  {
    "startTime": "00:38:01",
    "text": "routers but no longer reachable anymore with it is bgp you can actually detect those bdp zombie routes you need to write a plugin that will analyze the routing table and flag all the roads that are older than the given threshold then for all those prefix it will ask to the upstream router to confirm if the route is still valid or not there are some mechanisms such as transforming staff that can actually reload all the entire routine table of the neighbor but since mgm is concerning a small fraction of prefix according some measurement inside the internet it is not really necessary to renew all the entire routing table so we designed a prototype that only asked for those uh means that we detected this extension code will be added to the background task instruction board which acts like a current job so each unit of time you will execute this plugin so now we've seen some use case that we can do within gdp once the code that is executed is untrusted and we can ask the following question could it break usb well we need to have an automatic way to verify some properties before injecting it to the routers with with vdp we make the following approach first you start from the extension source code that will annotate then it will be passed to the software verification tools and if the verification tool says ok the code can be trusted it will be compiled to the bytecode and then injected to the protocol now i will explain you some properties that we define that all the plugin most"
  },
  {
    "startTime": "00:40:01",
    "text": "satisfied before being injected to the routers the first one is the termination properties we do not want that an infinite loop for example breaks the protocol another properties is related to the memory isolation of the plugin all plugins must not modify memory that are not authorized to modify we also restrict some api function to some extension code for example for double detailing they do not have the right to modify the protocol finally we use cr a lot of verification tools to verify properties related to the bdp syntax as an example we will verify the syntax of the geographic coordinate extension so this geographic coordinate extension is defined on a graph and it must follow the format specified on this graph and those properties i've defined on the on the slide are those that must be followed by the plugin we write all the properties and we annotate the plugin and then this annotated plugin will be passed through the application to server that will say yes or no and if it say yes then we are the guarantee that all execution of the plugin will satisfy the constraint to conclude within pdp you can make hptp implementation totally expensive if you want more details you can read the paper related to this presentation we have also a website that collects all the source code of xbgp we designed a methodology for bgp but the methodology could be also applied to other routing protocol thank you okay thank you really interesting talk"
  },
  {
    "startTime": "00:42:00",
    "text": "thomas uh if you're there uh you can open up your video um some questions hi okay can can you all hear us yeah yes i can hear okay um well yeah as i say really interesting talk um do we have any questions for thomas uh i see elliot had something in the chat uh elliot jones maybe uh go ahead yeah thanks very much for um your talk it was very interesting um two points the first is a question um in terms of that last slide it was it was your desire for this mechanism to always have the transitive bit set to zero um which i think does allow for a certain amount of prototyping for instance right uh yeah yeah yes this is uh actually xbtp is designed for quick prototype prototyping uh we just added an interface to allow some network operators to implement quickly this functionality and then you will uh actually test in another network and finally it will be uh maybe uh if it is it works well in maybe a release on the prediction routing network but uh actually uh if you would like to have performance you have to uh to write it directly inside the implementation level code so yeah maybe when you introduce new code you will actually break some some rules on the bgp protocol but we designed some mechanism to guarantee the execution of the plugin locally but the execution in the global network is not something we've done and this is something that you can uh"
  },
  {
    "startTime": "00:44:01",
    "text": "you you you test on yourself so you you make some tests and if you are confident about that you will push on the network so there is no really um way to test the global uh safeties of the the implementation you you put on the on the router so i mean this is something we can actually do afterwards but something i think it is something quite complicated to uh to verify and satisfy the security property of the global network but with me we have also some just no care verification yeah thank you um the reason i ask the question obviously is um you know one of the reasons it takes three and a half years to go through the standardization process is uh because i think uh people are cons routing is a is is a difficult business to begin with and we we do see cases where routes disappear uh for sometimes inexplicable reasons um even with the current mature mechanisms and so uh the there's a certain amount of proof time and so one question i think i think one research question to ask is just how flexible should a routing paradigm be and what what are the guard rails that make it safe to experiment hi this is an interesting question because when xmgp you can do what you pretty much we can do um well the first thing is that uh you have to uh choose uh the while right tool to uh actually guarantee uh the security property of the underwater uh maybe well now we we write all the plugin in c code this is not something which is uh really secure uh but you know in another we can also use on other tools some some maybe some language that are safer like"
  },
  {
    "startTime": "00:46:02",
    "text": "rust and uh those two will will be um actually um provide a better security but yeah you know we don't have any yes maybe obviously there is a balance between us flexibility and and security and if you add flexibility inside the routing protocol then the security part will be less uh less mean in the part of the the the reasoning of the [Music] thing yeah yeah thank you thank you um so uh a sort of i had a follow-on to that to some extent uh i mean if i understand write this this adds a bunch of hooks throughout the the bgp implementation where you can insert different sets of functionality um how much of the the core protocol has to be implemented in the core and how much can be implemented in in these programmable how much of the core bg bgp logic is hard-coded and how much is extensible oh yeah actually um what you have to do to put xbgp inside an implementation is to first follow the original llc's of 4271 and uh we also need uh all the extension above multiprotocol uh for example in the in the paper we reproduce the root reflector functionalities and this is one of some some some new skills that prove that we can actually implement a lot of complexity but the strong basis of hbgp relies on the the original draft and multi-protocol extension the other thing you can you"
  },
  {
    "startTime": "00:48:02",
    "text": "can do you can actually put your code inside the extension code and then put it in the the implementation which is six php compatible but if for example if you want to um to modify some data structure that you just argued inside the implementation you cannot for example uh if you uh take the the functionality that enlarge the the buffer of uh of bgp message or 4k to a 74 kilobytes of memory to one message that's this is one example of a feature that you cannot uh do within ubdp so everything related to uh to the memory or internet that's data structure which is when you put it inside the the implementation oh okay that makes sense that makes sense uh does anyone else have questions if you put yourself in in the queue if you have uh elliot are you still in the queue or just left over from before okay so i just had uh what one last thing uh i mean you know obviously the focus of the the thing of the mechanism is extensibility of the protocols uh and you know you you make the point of when you can't wait for the ietf and i think it's well known that that the itf process takes time uh and you know obviously uh elliott mentioned some of the the reasons for that uh a minute ago um and um assuming we can't make the itf go any faster um other things we you think we should be doing differently in the way we structure the protocols that will help this sort of experimentation once the rfc has been published uh other are"
  },
  {
    "startTime": "00:50:00",
    "text": "there any general lessons that that can be learned from this or is this very specific to the details of how bgp was implemented actually when when we look at uh xbgb the the protocol was pretty well established so if the what we we try to do with single gp is to uh understand what are the properties of the uh the protocol and then uh we um establish a set of properties to actually um actually model a general um actually architecture of of the routing protocol and then when you have the general model of this routing protocol then you will look inside your implementation if it actually implements this kind of general model and if you implement this general model then you will be able to actually put xbgp inside the protocol but yeah if the the implementation is only exotic i would say then you know gp will be um more um it might be difficult to integrate inside the implementation yeah that that that that makes sense so i guess the the general conclusion is the modularity in the in the the way the specifications are written might make this process easier for developing modular apis and extensions yeah okay so that's maybe some something for the the standard groups to think about all right uh and this is where over time and i i don't see anyone else in in the queue so um thank you very much uh i think that's really interesting some really nice answers to the questions um if there are any any more questions to tomorrow please please put them in the chat uh and um maybe he'll be able to to be around in"
  },
  {
    "startTime": "00:52:02",
    "text": "if people want to catch up with him in in the cover space perhaps or of course drop him an email all right so um thank you thomas um we'll move on to the the next talk uh of the the session um which is by uh axa kashaf uh axa is a phd student at carnegie mellon university uh she's co-advised by eva sekker and uh you you've reached a gowell and i apologize i must have mangled those names um access focuses on um network security uh particularly uh distributed denial of service attacks uh and currently she's working on building reconnaissance techniques to to profile the capabilities of ddos defense systems uh looking at uh understanding and improving the resilience of the internet against ddos attacks uh the award paper today uh is uh analyzing third party service dependencies in modern web services um what have we learned from the the mirai din incident and it was originally presented at the acm internet measurement conference in 2020 so if we can play the next video please hi this is alexa i'm a phd student at cmu and i'm going to present my work on analyzing third-party dependencies in modern web services this is a collaborative work with my advisors varseker and yuraj agarwal the study is motivated by a ddos attack that happened in 2016 on a dns provider named time there are a lot of lessons to be learned from this attack particularly its root cause and as i show in this talk we can expect more dine-like attacks in the future"
  },
  {
    "startTime": "00:54:00",
    "text": "during this attack in range 16 a lot of popular websites such as github spotify and twitter were inaccessible to users for many hours here is an outage map of the attack in total 178 000 domains went offline and tens of millions of users were affected the list of websites that went down goes on it includes almost all the popular services we know so how were the attackers able to bring all of these down this was possible because all these websites such as netflix exclusively relied on dyne third-party dns provider that acted as their authoritative name server as a result when the authoritative name server was down a lot of websites were inaccessible to millions of users attacks like these raise many questions about the resilience of the internet how easy is it to bring down the internet particularly we need to understand how prevalent these kind of third-party dependencies are for example how many other websites are at risk of client-like attacks are there other hidden dependencies that we also need to know about there might be inter-service dependencies which indirectly affect the website and finally since stein attack had such a huge impact it is natural to think that websites would adopt as a result how has the world changed in using third party services as a consequence of denying attack for example did websites stop using dyne or did they start using multiple providers to answer these questions we analyze top 100 000 websites alexa websites alexa is an external service that ranks websites by popularity"
  },
  {
    "startTime": "00:56:00",
    "text": "alexa rank is calculated from a combination of daily visitors and page views on a website to see how the world changed in response to the diner tag we look at the dependencies of top 100 000 websites in 2016 and 2020 in this talk i'll first describe our measurement methodology to measure the third-party dependencies of websites and then explain our main findings then after the findings i will also present some recommendations in light of our findings and finally some limitations of our work to start with the measurement methodology we first need to identify the services which are crucial for a user's access to the website to do this we look at the lifecycle of a web request when a user makes a request to a website it first goes to the authoritative dns server which provides the ip resolution of the website after the ip resolution of the website user starts a tcp handshake followed by an ssl handshake if the website supports https if yes then the website provides a certificate which is verified for revocation by the user by contacting the ocsp server or crl distribution points set up by the certifying authority finally if the certificate is valid the user requests the content of the of the website page if the website is using a cdn then cdn will serve this content based on this we identify three services the first one is dns second one is certificate validation by certified authority and the third one is cdn note that these by no means are the only services that a web request encounters but we just identify these three services and define the scope of our work"
  },
  {
    "startTime": "00:58:00",
    "text": "given these services and websites we need to identify what exactly we want to measure so we we are interested in third-party dependency of a website for example here the website tech radar uses another entity let's encrypt as its certificate authority next there might be indirect dependencies between websites and service providers for example here let's encrypt uses cloudflare as its authoritative name server and hence techradar has an indirect dependency on cloudflare we are also interested in critical dependencies which exist if a service provider is integral for a given service for example here if cloudflare is down ocsp servers of let's encrypt will not be accessible hence let's encrypt has a critical dependency on cloudflare in case of dns in cdn we compute critical dependency by measuring if the website is redundantly provisioned which means that the website is using multiple dns or cdn providers and in case of certificate authorities we compute it by measuring if the website has enabled ocsp stapling here techradar does not support ocsp stapling and hence techradar has a critical dependency on let's encrypt as well as cloudflare if ocsp step link is enabled the user doesn't have to contact the ocsp server for certificate validation so let's encrypt will not be contacted here but if ocsp stapling is not is not enabled then techradar will have to users of techradar will have to contact the let's encrypt ocsc servers to validate the certificate now given this we first describe our measurement methodology for dns what we essentially want is to find the authoritative name server of each website this can easily be done by"
  },
  {
    "startTime": "01:00:00",
    "text": "looking at the name servers resource record of a website then we want to identify if this name service a server is private or a third party i would go into details for this in the next slide that how do we actually identify if it's a private or a third party and finally for a website and its name server we want to identify which of these name servers belong to the same entity for instance here azure dns and o365 filtering both belong to microsoft so since live is live.com is using these two which belong to the same entity it is actually not redundantly provisioned to identify third party name servers prior work looks at second level and top level domains of the name server and the website if they match then the name server is private otherwise third party for example here in case of google its name server also has the same second and top level domain google.com hence it gets correctly classified as private however this fails in case of youtube where the name server gets classified as third party when in fact it is not other approaches look at the start of authority dns records which contain administrative information about the dns zone if the start of authority record of the website and name server are the same then it's private otherwise third party this works in case of youtube but it fails for instance in case of twitter where it is using a third-party name server and its startup authority is also that third party for instance here twitter using twitter is using dyneck.net which actually belongs to dyn to cope with these issues we add a few more heuristics"
  },
  {
    "startTime": "01:02:00",
    "text": "so for all website and name server pairs we classify the name server as private if the second level domain and top level domain match for the website and the name server or if the name server is present in the subject alternate name list of that website sand list is present on the https certificate of a website which tells that this certificate will also work for the domains present in the send list hence it's a good way to know if a given domain is private or not if the start of authority do not match we classify it as third and finally if the name server serves more than 50 websites then we say that it's more likely third party in general we identify approximately 10 000 third-party dns providers in our data for measuring cdns we fetch the web page of the website that we want to measure the cdn of and then we extract all the resources in the web page and identify the resources which are internal for example for reddit.com this resource that comes from redditmedia.com is an internal resource we use tld matching startup authority and subject alternate named lists here as well to identify internal and external resources we do this because internal resources if they come from a cdn then it means that the website is actually using a cd for all the internal resources we fetch their cname records if the website is using the cdn then these cnns will point to the cdn for instance here the the cname actually points too fastly we input this cname to"
  },
  {
    "startTime": "01:04:01",
    "text": "to our cname to cdn map which we are which will output fastly here we manually build this map by taking help from available seniors on the internet on the cdn's websites and also in public suffix lists after we have identified the cdn being used by our website we also need to see if the cdn is private or a third party and to do that we again use tld startup authority and sans list to see if the cdn is private or third party in general we identify total 86 third party syrians in our data for certificate authority dependency we just fetch the certificate of a website and from that extract the links for the ocsp servers and the crl distribution points and we use the same techniques which are tld mentioning startup authority non-matching and sandless to identify if these are third-party certificate authorities or not in our data we identify total 59 third party certificate authorities now we present our main findings i will start by answering the question how prevalent third-party dependencies are and how concentrated different third-party providers are overall we find that eighty nine percent of the top hundred thousand websites use third party dns thirty two percent use a third party cdn and 76.8 use a third-party certificate authority moreover 85 percent critically depend on a third-party dns provider"
  },
  {
    "startTime": "01:06:01",
    "text": "28 percent critically depend on a cdn provider and 59.5 percent degree depend on a certificate authority for dns in cdn it means that they are using just a single dns or cdn provider and for c it means that they do not support ocsp stapling all in all we found that 89 of the top 100 000 websites critically depend on a dna dns cdn or ca provider in addition to this we also found that critical dependencies are higher for less popular websites for example we observe that 49 use a third-party dns provider in the top 100 websites and 89 in the top 1000 websites same is the case for critical dependencies for instance twenty seven point nine percent critically depend in the top hundred while eighty four point eight percent in the top hundred thousand this means that more popular websites care more about availability now since third-party dependencies are very prevalent it raises the question of how concentrated various providers are the greater the concentration greater will be the single points of failure this is a dependency graph of website to dns dependency where each edge shows the dependency of a website on a third-party dns provider the size of each node here is proportional to its in degree which in the case of dns provider is the number of websites using it we observe that the top most dns provider cloudflare alone serves 23 of the top 100 000 websites"
  },
  {
    "startTime": "01:08:01",
    "text": "in general we observe that only top three serve almost 40 percent of the websites when the total dns providers we observe are almost 10 000. this indicates dns space is highly highly concentrated we see similar trends in cdn where the top most cdn provider amazon cloudfront alone serves 26 percent of the websites that use cdn particularly we observe that only three cdns serve almost 60 of the websites that use cdn which are cloudfront akamai and cloudflare the situation for certificate validation is also similar digit alone serves 28 of the websites that support https particularly only three certificate authorities digi cert let's encrypt and section so 60 of the https supporting websites out of the total 59 certificate authorities we observed to summarize our main findings were that third party critical dependencies are highly prevalent and third-party services are very concentrated a direct implication of this is that 89 of the websites are vulnerable to dine-like incidents a single provider can affect as much as 25 percent of the top 100 000 websites now the question is if this is the complete picture or do we need to take indirect dependencies into account as well and this brings us to our second question which is are there any indirect dependencies between websites and their third party"
  },
  {
    "startTime": "01:10:00",
    "text": "providers to do this we first need to analyze inter-service dependencies between dns cdn and ca providers in case of ca to dns dependency we found that out of the 59 cas that we observed 48 of them use a third-party dns provider in case of ca to cdn dependency 36 percent used a third-party cdn provider moreover out of the 86 cdns that we observed we found that 36 percent use a third-party dns provider hence third-party dependencies are also prevalent among service providers in case of critical dependencies we found that 31 percent of certified authorities relied on a single third-party dns provider 36 percent relied on a third party cdn provider and 17 cdns relied on a single third-party dns provider these critical dependencies between service providers give rise to indirect dependency but why should we care about indirect dependencies i mean okay sure they exist but what's the harm turns out that as a result of indirect dependencies we see huge amplification in the impact of a given provider for example in case of ca to dns dependency cloudflare now critically serves 37 of the top 100 000 websites as compared to 24 before and in case of dns made easy the impact goes from one percent to 23 percent hence because of these in indirect dependencies we see"
  },
  {
    "startTime": "01:12:00",
    "text": "amplification of provider concentration to summarize critical dependencies are also widespread among service providers and these dependencies can amplify the concentration of service providers which means that the effect of single points of failures in the internet is amplified and a single provider can now impact 37 percent of the top 100 000 websites this is what we observed for data in 2020 now we see how the world changed after the dying incident by comparing 2016 dependency data with 2020 data which brings us to our third question which is how did the world change after the time incident did websites try to reduce their third-party dependency on service providers did the concentration among providers decrease we observed that critical dependency in dns increased with respect to cdn and ca we see no significant change in the critical dependency similarly in case of inter service dependencies we observe an 8 percent decrease in critical dependency in ca to dns dependency and a four percent decrease in cdn to dns dependency so overall in terms of provider to dns dependency we see a decreasing trend which is encouraging now let us look at the change in concentration of service providers according to this figure in 2016 2700 dns providers served 80 of the websites while in 2020 only 54 providers serve 80 of the websites this means that we are in general moving towards more concentration this is not an encouraging result"
  },
  {
    "startTime": "01:14:00",
    "text": "because it means that single points of failure actually got bigger and we see similar trends in ca providers while concentration decreased incidence to summarize we see no significant change in the third party dependency trends among websites we do observe a decline in third-party dependency among service providers we also observed that services became more concentrated in 2020 as compared to 2016. all in all we do not see an increasing trend in redundancy we did observe that websites using dyne were more redundantly provisioned in 2020 as compared to 2016 but in general there was no significant trend towards redundancy and because the concentration of providers increased in 2020 single points of failure actually got bigger now based on these findings we make some recommendations for websites they should consider redundancy while using a third party provider moreover they should try to understand their indirect dependency arising from the inter-service dependencies of the providers that they are using similarly for service providers they should make it easy for websites to to be redundantly provisioned as we observed that many dns providers do not support redundancy and also service providers should try for redundancy in their inter-service dependencies finally service providers should also try to be more transparent about attacks on their infrastructure to highlight some of the limitations of our work our measurements are from a single vantage point only and hence we may miss"
  },
  {
    "startTime": "01:16:00",
    "text": "region specific dependencies we analyze dependencies only on the landing page of a website hence we may miss dependencies that manifest deeper in the content hierarchy a direct consequence of this could be that we may miss certain cdns used for our website because the landing page didn't access any resource from a cpm finally we also did not look at the physical and networking infrastructure dependencies such as routing and hosting mainly because it's not the scope of this work to summarize the ddos attack on dine in 2016 raised some questions about the resilience of the internet in our paper we try to answer some of these questions particularly about the prevalence of third-party dependencies and their impact to do so when li stop hundred thousand popular websites we also look at their inter-service dependencies to get a holistic view and we identify the potential single points of failure in the internet we see that third party dependencies are highly prevalent indirect dependencies amplify provider concentration and we do not see a move away from third-party dependency after dyna incident the code and data used in this paper is extremely also available online thank you so much okay thank you axa for that really really interesting talk um please do come on up all right can you all hear us yes we can yes we can all right uh so yeah as i say thank you for the really interesting talk uh we had a bunch of really good questions in"
  },
  {
    "startTime": "01:18:01",
    "text": "in the chat already uh and uh i see a couple of people in in the queue uh where's uh i guess your first uh hi yeah thanks a lot for the work uh you know measuring [Music] i'm not able to understand i think uh my internet maybe is very bad uh oh can you hear me or anyone oh [Music] her reception is it's odd her transmission looks great her picture's great so give her a minute i guess you yeah yeah you might have to take her questions after the other presentation or something i think i can hear now oh good okay can you hear me yeah yeah okay great um so thanks a lot for the work uh so it's you know uh any studies we get on centralization and dependencies is fantastic because it's definitely a trend as you have uh proved in your numbers you made one statement that said uh so in your recommendations you said that you recommend the companies be more transparent um can is there any data to back that up or or why companies should be more transparent that has been something that people want but it's hard to prove that that's actually a good thing for them to do so actually i have a multiple sort of data points for this for instance uh currently let's say i'm doing research in the redos area and when i say that companies should be more transparent what i mean is that they"
  },
  {
    "startTime": "01:20:00",
    "text": "should be transparent about what attacks they're observing so that we can sort of determine the skill level of the attackers to some extent for instance here in this in this attack that happened we don't really know uh for for the dyna tag we don't really know if the uh if the target of this attack was particularly dine or if the target of this attack was some other website and all other websites were just collateral damage and these kind of questions can actually help in making informed decisions because if many websites just became collateral damage then people might might want to think about okay also having a private provider rather than just having third party providers and if a particular provider was specifically being attached then we can make an argument that okay maybe we should be redundantly provisioned and not in just this attack but there are many other attacks for example uh currently i'm working on uh building attackers who can do reconnaissance on on different service providers and these type of attacks have existed for so long i look at all these companies aka my verizon and their ddos reports and i see that they talk about okay these attacks have existed but we don't really know we don't know how to detect these type of attacks how the attackers were actually doing this attack and i think this is very important for for at least research in in ddos attacks to know that what the capabilities of existing attackers are that makes sense thank you okay thank you uh elliot uh i guess you're next thank you again colin axo this is a great paper i echo uh what what uh wes said it's a really helpful and an area that is uh i think"
  },
  {
    "startTime": "01:22:00",
    "text": "really challenging you know given our dependency on the internet today um two points first um the numbers you presented in terms of concentration uh very much uh are consistent with the numbers that anna maria mandalari at the imperial college uh showed when she was looking at iot cloud dependencies um so uh you can look up her work if you'd like i think she presented at the at imc as well at some point about this um the uh second point was was just that there's an activity going on inside the ietf which may benefit from hearing more about your work which is benoit clays presented something called sane please don't ask me to expand the acronym i couldn't possibly but it essentially is a dependency it's a service architecture it looks a it and it's based on a dependency graph and um there's a lot of work actually going on in the ops and m area that is focusing on this very important area of resiliency so again congratulations on a great paper and uh i look forward to seeing a lot more interesting stuff from you and your colleagues thank you thank you yeah thank you uh elliott if you could put a pointed tube in my stuff in the chat that might be helpful all right uh other than that uh i noticed we're running shots on time we've had a a bunch of really really interesting questions there uh please do read reach out to exo if you have more more questions uh and so as as everyone said this has been a really nice piece of work all right and with that uh we should move on to the the final talk today uh which is by kevin bock kevin is a phd candidate in the"
  },
  {
    "startTime": "01:24:02",
    "text": "department of computer science at the university of maryland uh advised by dave levin and his work focuses on enabling open communication improving network security and evading censorship and if we can have the final video please one hey everyone my name is kevin bach from the university of maryland before i get started i wanted to give a huge shout out to all the ietf organizers for making this event possible doing all this remote organization and i wanted to give a big thank you to all the collaborators on this project it's really taken a village to get this going now i'm going to be talking about censorship today and specifically a new form of censorship evasion we've been working on server side of asia before i get into that though i wanted to give you all a brief background of what nation state censorship looks like nowadays to motivate some of the approaches we've taken now there are many types of censorship that operate around the world today and today i'm going to be talking specifically about the automated in-network censorship that operates in the network by nation-states now some nation-states operate censorship as in-path sensors like this one with the sensor physically sitting inside the network path other sensors operate censorship like this instead of being in the path they're on the path so if the client makes a forbidden request you'd see this request moving to the network now the server will get the packet and the sensor will too if the sensor wants to sensor this connection though it can't drop it anymore it's no longer in the path so instead it performs deep packet inspection on this packet and if the pack contains something it disallows what it will do is it will inject its own packets into the connection specifically it will inject spoofed tcp reset or tear down packets these are normal packets are computers sent all the time these just exist to tell the"
  },
  {
    "startTime": "01:26:00",
    "text": "other side stop talking to me immediately it's going to send one of these packets to the client pretending to be the server and one of these packets to the server pretending to be the client now when these packets arrive the client thinks the server terminated the connection and the server thinks the client terminated the connection immediately both of these sides stop talking to each other and just like that censorship has been achieved now in order to pull off this attack of injecting tear down packets the sensor needs to have some information about the connection it needs to know the port numbers the sequence and acknowledgment numbers what this means is that the sensor requires per flow state and that means that these sensors have to be tracking the full state of every tcp connection coming into and out of their country and if you're trying to track every single connection into and out of a country at a country the scale of something like china you're going to necessarily have to start taking some shortcuts and we as evaders can take advantage of some of those shortcuts so let me give you an example from prior work of how researchers were able to take advantage of these ideas to evade censorship now once again our client is about to generate a forbidden request to this resource but this time we're going to inject a tcp reset packet of our own we're going to set it in such a way that the ttl or time to live this is a field in these packets that tell the network how long it should survive in the network and it decrements once per hop we're going to set the ttl high enough such that we reach the sensor but not so high that we reach the server so watch what happens when we send this packet now just like before the sensor will get a copy of the packet but the packet's not going to reach the server it's going to get dropped along the way now the server never saw this packet but the sensor has and the sensor says well it looks like the client just terminated this connection so i can stop tracking the connection now and it throws away the state it's been maintaining about the connection now for the rest of this flow the client and server are free to communicate the sensor has no state with which to censor us and the server is no idea we pulled"
  },
  {
    "startTime": "01:28:00",
    "text": "off this trick now this is just one example of these strategies that have been developed and discovered over the last 10 plus years and the one thing they all have in common is that evasion has always involved the client and that statement is true even beyond ways and mechanisms of evading censorship that do not rely on manipulating your packet stream consider tor or vpns or proxies all of these things require the client to install some software on their system or take some direct action to make them happen this has always posed a significant barrier to deployment though installing the software can pose significant risk to the user in the first place and beyond that it can help users who either don't have the technical know-how to set these tools up aren't comfortable doing so or even those users who don't even know they are experiencing censorship in the first place now ideally servers would be able to help instead of deploying software at the client instead we would deploy it at the server and if such a thing were possible then the server could subvert censorship on the user's behalf without clients needing to deploy anything at all and just think about the benefits of this this would immediately broaden reachability and accessibility for these resources without clients needing to do anything many clients connect to one server clients no longer need any technical expertise or to download anything every client immediately gets plausible deniability and it helps all those users who don't have technical expertise or don't know they're being censored in the first place so this sounds amazing the problem is it shouldn't work and it shouldn't be possible and to see where this is let's consider the waterfall diagram of packets that are exchanged leading up to some censored query clients going to send a sin to which the server responds with the synack the client completes it through a handshake and then the sensor keyword is sent from the server's perspective though there's very little it can do before the sensor keyword is sent in fact the server can't influence this connection"
  },
  {
    "startTime": "01:30:00",
    "text": "past the synack at all underscoring the difficulty in this space there's been no prior work on evading a sensor evading censorship from the server now i'm thrilled to tell you that server-side evasion is indeed possible and the rest of this talk i'm going to talk about how we discovered these server-side techniques and our results across multiple different countries and protocols and then i'm going to talk about the insights we glean from the sensors in these experiments before i jump in to discovering server-side evasion techniques i wanted to first give you a little bit of background on the tool we used and modified to discover these strategies and and get some insight into this idea this tool is geneva geneva is an open source originally client-side tool that my team built a few years ago for automatically discovering censorship of agent strategies this was originally published by my teen in 2019 geneva runs strictly one side of the connection we originally designed it for the client side and the way it works is it manipulates packets as they enter and leave the system now geneva is a genetic algorithm so in fact it learns how it should manipulate packets specifically the way it can manipulate packets is it can do that with just four actions that's duplicate you take one packet of two packets tamper you take a pack and you change it in some way fragment take a packet and break it in half or drop take a packet you drop it on the floor now i'll call out two things specifically here the first is that tamper is allowed to alter or corrupt any tcp header fields importantly we don't give it any semantic understanding of what these fields mean we give it syntax but not semantics what this means is that it can access the tcp flags fields it can change the flags fields but it has no knowledge that if i set the flags field to sim that means the start of a connection so syntax no semantics i'll also call that fragment here it does a bit of double duty the ip layer does fragmentation but but it can also"
  },
  {
    "startTime": "01:32:01",
    "text": "do segmentation of the tcp layer now let me show you what it looks like when geneva puts these actions together because genita actually composes these things into trees and these trees look something like this at the top we have some sort of match or trigger okay and then associated with that match we have an actions these are match action pairs and a strategy is comprised of some combination of these match action pairs so for this specific strategy the trigger is outbound tcp packets where the flags set to ack okay when that match happens when a packet matches that trigger it gets pulled into the tree and then the tree decides how that package should be modified the duplicate action makes two copies and you can see different things happen to each copy then we do an in-order traversal of the leaves and we send the packets that emerge so let me show you what this looks like on the wire once again our client has just finished the three-way handshake where they're about to finish the three-way handshake and they're about to make a request to this forbidden resource we're going to generate our act packet which matches the trigger geneva pulls it into the tree duplicates the packet the left child is done the right child the flags are changed to reset the ttl is changed to two and then in order to traverse the leaves and we send the packets and if you'll notice this tree exactly implements the censorship evasion strategy that i opened this talk with this is the ttl limited reset now for this work specifically we modified geneva so that it could run on the server side so we deployed it against real world sensors in our previous work and a lot of other previous work in the space we've tended to focus on http in this work we've broadened the protocols that we try and support censorship for so specifically we look for http https dns ftp and also smtp censorship now triggering censorship for these protocols largely involves making"
  },
  {
    "startTime": "01:34:00",
    "text": "requests or queries with forbidden keywords and domains smtp is a bit of a special case for smtp you can just send an email to this email address this is censored in china and for context this email address is an old mechanism by which how bridge nodes used to be distributed in china so the email is censored to show how this could be applied broadly in addition to the diversity of protocols we also tested against a diversity of sensors in both how these sensors operate and where they're located so we tested in china iran kazakhstan and india across all of these different protocols now i do want to note there's a small asterisk on the slide this refers to that although iran censors dns over udp at the time of our testing dns over tcp was not censored from any of our vantage points now recall that server-side evasion shouldn't be possible in any of these cases because the server can't really influence the connection past the synack but geneva was able to find ways to do it and i would love to show you what some of those things look like so this is one successful server-side evasion strategy that works in china now there's a lot going on here so let me break this down the client starts off just like normal by sending a sin packet okay now instead of the server responding with the synack doesn't respond with the cynic anymore instead it responds with two syn packets the first is a normal synth packet the second is a sim packet containing a payload now this amazingly is legal the first syn packet serves to trigger a tcp simultaneous open this is an archaic feature of tcp that's still supported by every major platform and this was originally written to handle the case of what happens if two computers send a syn packet to each other at the exact same time now when the client receives the send packet the client now sends a synack and it's actually this combination of the client sending a sin back the client sending a synac"
  },
  {
    "startTime": "01:36:01",
    "text": "immediately preceded by a syn packet containing a payload that causes the great firewall of china to desynchronize from the connection and for the rest of this flow the client and server can now communicate censorship free now this seems crazy but this actually works with varying degrees of success across all the protocols we tested and for context the baseline success rate if you do nothing at all for evading censorship is about two percent now this is one successful strategy but it's actually just one of many altogether we found 11 different strategies at the time we did this work we discovered eight in china one in iran and india and another three in kazakhstan now our paper goes into detail across all of these and unfortunately as much as i would love to i don't have time to go through them all now but i would love to show you just one or two more the next one i want to talk about takes place in kazakhstan this is the null pcp flags server side of asian strategy you'll notice it starts off just like all of them do the client sends us in but the server instead of responding with the synax instead of what it responds with is a packet with no flags at all now the client when it sees a packet with no flags it has no idea what to do with it so it just drops and ignores it but when the sensor sees a packet with no tcp flag set at all the sensor can't handle it and assumes there's something wrong with the connection and immediately stops tracking the connection and just like that the server has helped uncensored the client now one of the big takeaways from this is that it really has helped motivate why we should be using automated techniques to discover this this is likely a bug in this sensor that it may have been difficult for a human to discover now here's another example that i wanted to show you also in kazakhstan now in this one during the during the three-way handshake instead of the server sending one synack it actually sends two synax and it includes payloads on those synax specifically the payloads it includes"
  },
  {
    "startTime": "01:38:00",
    "text": "are two uncensored get requests get requests for something like example.com or something innocuous and of course the client is just going to ignore these payloads but when the sensor sees these well-formed http get requests on these synax the sensor confuses the connection direction it says whoa the server is sending this data i must have had this mixed up the server must actually be the client and it reverses the roles of client and server in the mind of the sensor as a consequence of this because it treats packets differently from the client server when they begin communicating again and when the client issues the real censored query they can now communicate scout free because the sensor is no longer examining the packets from the client the way they should be and this works with a 100 success rate in kazakhstan now at this point we've seen a lot of different types of bugs and sensors not handling esoteric features in tcp correctly correctly error cases or just confusing the logic altogether for more details for any of these i'll refer you to the paper but it will note that all of these do not require any client behavior whatsoever they may induce some behavior from the client but no software changes as needed and we tested this across a wide range of diverse clients and in fact we found only one case of a strategy that relied on platform independent behavior we were able to very quickly write that strategy to make it platform agnostic and all of these really teach us more about how sensors work next i'd like to talk a little bit more about the insights this work has helped provide us with the first thing i'm going to talk about is something called the resynchronization state this is a feature of the great firewall of china that allows it to be more tolerant to packet loss so specifically the way this works is let's say the sensor misses a packet the idea is simple it can just re-synchronize its state on a later packet in the connection"
  },
  {
    "startTime": "01:40:00",
    "text": "now the exact dynamics of the resynchronization state are constantly evolving over time and understanding how the resynchronization state operates allows us to build better censorship evasion tools so looking at the resynchronization state gives us some more insight as to why that first strategy i showed you actually works so once again here's that strategy now i'm going to focus right now on the second syn packet we sent from the server if you recall we sent this syn packet again we sent it from the server but we sent the sim packet including a random payload we notice here is that this payload actually triggers the great firewall's resynchronization state it tells the great firewall hey you may have missed something go re-synchronize on the next packet the next packet that it resynchronizes on is the synack packet from the client but because we force the tcp simultaneous open when you do a tcp simultaneous open the mechanism by which sequence numbers are incremented during the three-way handshake changes and the grade firewall is not properly incrementing its initial sequence number while it's resynchronizing what this effectively means is that this is an off by one bug in the great firewall of china the sensor is not properly incrementing its isn making it desynchronized from the rest of the connection because of the presence of this payload now strangely the success rate of this varies greatly by protocol and in fact we found that the great firewall itself has a different resynchronization state depending on the protocol itself which is strange this is just the first example of protocol variability we found in the great firewall and i'm going to talk about that a little bit more next now all of our server side strategies operate strictly during the three-way handshake okay which means they happen before there's any application layer specific data going on what we realized though when we started looking at the success rates of these strategies is that they didn't always work the same way with the same degree of success"
  },
  {
    "startTime": "01:42:02",
    "text": "this led us to raise the question why are different applications affected differently in china if the only actions a server takes are done before there is any application layer data exchanged what this has led us to what it led us to think about is a new model of how the great firewall of china works because in the past we've kind of tacitly assumed that the great firewall works with this kind of very same network model where it seemly separates uh application layer from transport layer protocols it looks something like this but what's going on here is that we're finding different tcp layer bugs for each protocol which means that each protocol censorship must have its own tcp stack and this strongly suggests that the great firewall is actually running multiple sensory middle boxes in parallel it's not just one set of machines so let's see what this looks like instead of just one monolithic sensor instead we think there are multiple sensory middle boxes and each of these are running in parallel now this begs the question how does the great firewall know which middle box to apply to a connection now you may look at this be like well it's obvious it's just got to be using the port number but it's actually not relying on port number the great firewall for all these protocols can sensor effectively on any port if you try and make an http request for some forbidden content on 80 or 8000 or a random port it will still censor you the same so it's not using port number we think is going on is that how does it know which middle box to apply we think it doesn't we think that every single little box gets a copy of every packet independently and then every middle box applies protocol fingerprinting every middle box checks hey is this something i can censor is this belong to me does this belong to me and then if it discovers hey this is something i should be taking action against then that specific middle box will take action"
  },
  {
    "startTime": "01:44:00",
    "text": "this raises the question where are these guys located are they nearby are they far away so we did was we used ttl limited probes we sent forbidden queries with different for different protocols with different ttl limits and we tried to see are these located in the same spot in the network or they located far away what we find is that largely they're co-located at the network level they don't seem to be located at different locations now next i want to shift gears a little bit and i want to talk about where we've taken these ideas since we wrote this paper now the first thing i want to talk about is how this idea of service activation and this idea of an automated approach to service side evasion has allowed us to be highly responsive to new forms of censorship back in february of 2020 iran launched a new form of censorship called a protocol filter the way this works was that it monitored all protocols entering and exiting the country on certain ports and it only allowed certain of those protocols to happen it performed protocol fingerprinting across all of these and protocols that didn't match were then subject to censorship which means if you tried to use a protocol they didn't approve of regardless if what you were doing is innocuous they would take that connection down in fact the only connections they did allow were those that were they were able to be filtered by their standard sendership system so within a small amount of time of this censorship system being deployed we threw geneva at this problem and we were able to discover four strategies to evade this filter uh four from the server side so this means that as soon as iran rolled this out we were able to defeat this thing and help roll out those evasion tactics to those in iran another example of new censorship events that occurred is last summer china began censoring the use of encrypted sni this is a new feature in newer versions of tls whereby the s i or server name indication fields inside tls is encrypted now the sni field is revealed in tls 1.2 where when a client connects"
  },
  {
    "startTime": "01:46:02",
    "text": "to a server the client announces in plain text the domain it's trying to get to sni is clearly a privacy leak and the tls developers are working on fixing that but sni is how sensors have been censoring https for quite some time es and i people have been looking forward to as a mechanism by which they can protect themselves from smi based censorship and unfortunately last summer china decided to ban the use of esi completely so if you try and encrypt your sni fields and any connections china will take action against that connection even if you're talking to something innocuous and has effectively stopped the rollout of esmi within the country now within 24 hours of china rolling out this system geneva had discovered six strategies to evade it six from the client and four from the server this is very exciting and it really allows us to be highly responsive and dynamic as new censorship events occur now the other exciting direction we've been able to explore since this work has been a real world deployment of this system we've been working with a number of anti-censorship groups to integrate our software and our findings into their systems and we're starting to see success with this we've been using geneva to help really in two regards the first is in bootstrapping initial connections so let's say you're something like a vpn and you really want to help users get connected initially and the weak the weak spot in a lot of these protocols is that initial connection is those api calls that initial reaching out now we can use what we can do is use geneva to help protect those initial bootstrapping and assist in bootstrapping clients from a weaker protocol to a more secure protocol the second thing we've been able to help with is in hardening is hardening existing evasion protocols evasion protocols that sensors may be working on harassing or taking action against use geneva on top of those and deploy these server-side strategies to protect those in use and make it harder for sensors to take action against these"
  },
  {
    "startTime": "01:48:00",
    "text": "and we're very excited about these new directions i want to conclude here to talk about new directions that this work has opened up for us and really how it's changed the way we conceptualize middle boxes because these middle boxes are really creating new possibilities in the network it's good because this has opened up the ability to do censorship evasion to more people even those who don't know they're being censored now this is made possible though by the ugly fact that middle boxes have bugs and bad assumptions that can be exploited and unfortunately this can also lead to some very bad directions in another recent piece of work we showed that middle boxes can be actually weaponized by attackers to launch attacks that were long thought and possible ultimately this leads us to conclude that to really make sense about what these new additions to the internet are doing we really need to keep investing and automated tools like geneva to understand what new things these middle boxes enable and how they change the landscape of the network and networking research now i'm going to wrap up here today i've talked about geneva and specifically how we've applied it to make server's light evasion possible if you want to learn more our website is available here at the slide it's geneva.cs.umd.edu thank you all very much all right so thank you uh kevin really nice talk um are you there yes hi so uh i see tremendous amounts of discussion in the chats already are there any questions which people haven't asked already in the chat see spencer maybe what else do we have stewards yes please don't join the queue if you have"
  },
  {
    "startTime": "01:50:01",
    "text": "questions all right where's go ahead well since you needed questions and then two more people popped up so it was unclear to me and maybe i missed it how did you determine ground truth for if you were doing server-side you know studies how do you decide whether something is successful or not and actually got transferred to and from the client properly [Music] can you hear me let me know if my audio is not great so in our case we have the we have the good fortune of being able to control both the client and the server so we did our experiments we actually had vantage points located in all of these censored countries and so we could ground truth was very easy to determine we could try and obtain some forbidden resource see the sensor take action and then try and do that again with the server side strategy running and see it succeed now when you're talking about scaling this up to cases in which you don't control the client or some of our more real-world deployments that we've been working on recently it starts getting a little trickier when you start needing to really look at telemetry and look very carefully at what the if the packets are getting exchanged if the connection is staying alive that sort of thing i will say most sensors when they take action against the connection it's fairly obvious like it they're usually not quite about it so generally the server can determine uh pretty accurately whether a sensor's taken action against something or not okay thank you uh shiva hey kevin thanks for the talk um can you hear me okay i just so i i think he might have touched upon this but when it comes to deploying this at scale were you i think you mentioned like tunnelbear someone you were working with british wondering were you thinking about the coming up the strategies in a centralized way and then distributing"
  },
  {
    "startTime": "01:52:01",
    "text": "those strategies to every deploy to every um server running geneva or were you thinking that every individual deployment server deployment would come up with its own strategies um yeah like what were you i guess thinking about like coming up with the strategies themselves from a deployment point of view for initial rollout the the methodology has been um will be kind of a strategy oracle and provide strategies um i didn't get to talk about it too much of the talk but geneva's broadly broken into two halves there's the component that runs a strategy on the network and a genetic algorithm that discovers the strategies and those are fairly self-contained so for now we've been we've had success with um having third parties deploy the strategy engine the thing that takes the strategy and runs it on the network um and we've been more strategy oracle providing strategies uh that being said there are more and more groups who are working to find strategies themselves and so as that becomes more decentralized that'll things will just continue to improve okay great area yeah and thanks a lot for this talk it was very interesting to learn um what the senders are doing there and i probably have to read the rest of the paper as well to understand what's happening there um we talked a little bit uh in the chat about this is a cat and mouse game right um can you just um tell a little bit about like how long because i think somebody said in the chat that some of these bugs have been fixed so like what's the timeline how long did it take what was the dynamics behind that just curious yes that's a really good question and that's something we're still studying of what does it look like when a nation state rolls out a bug fix um like how long does that take where does that go um but still something we're studying and i i don't think i have a great answer for you um the yeah i think i'll put a tbd stay tuned on that um it's something that's super interesting we're going to keep monitoring um and we do test these"
  },
  {
    "startTime": "01:54:00",
    "text": "things fairly often to see what's goes on but i don't think we have a great understanding yet of how those fixes get rolled out and part of that is a limitation of that can be quite difficult to do at scale for example a country like china may have may have many instances of middle boxes and i mean perhaps the fix get rolled out somewhere and it takes time to traverse or there's different sets of middle boxes doing things so it it really takes quite a concerted effort to measure these things at scale and get a good understanding of how the fixes move through the network if you will so a big tbd on that but it is a super interesting question okay thank you i will read your next paper but yeah it's already known that the system is very heavy so it's probably hard to measure great all right uh andrew i think is next uh siobhan are you supposed to still be in the queue yeah should i go ahead yep um uh yeah hi kevin thanks uh very interesting presentation um just just amplify a comment i made in the chat which you answered but i think um clearly i can see nation states have the resources to you know respond over time with countermeasures etc so it's an arms race and no side will win i think by definition um possibly china might because they got more resources than anybody else but notwithstanding that my concern is for the non-nation states affected by this so if you like for the typical end user versus the malicious content developer um clearly server-side evasions in the hands of malware has seriously bad implications for for most end users so if you like any tool can be used for good and bad i think this tool has tremendously negative connotations for"
  },
  {
    "startTime": "01:56:00",
    "text": "the vast majority of internet users um which doesn't negate the work but i think that needs very very careful consideration really true i mean the the true it's a true statement generally that anything like this can be used for good and bad and malicious i think dave said it best malicious is in the eye of the beholder so we've intentionally not taken this work and applied it towards um the firewalls like you're describing like this really has been a very concertedly focused effort on nation states to try and at least angle things towards the good side as as we can help direct it um i think it's difficult to quantify if the who's affected the majority here um i mean there are billions of people living under the centered regimes there's also billions of people who would like to be protected by firewalls so it's kind of a tough question to answer but it is something we've been thinking about and it was a serious consideration we were talking about should we release this tool should we make it open source um at the end of the day we we thought it best to put it out there and um yeah let the community use it i guess i i just uh i would observe that some of the earliest adopters of for example encrypted dns um were malware developers um so malicious content in the way that in a definition that most of us would recognize as malicious um [Music] but yeah anyway food for thought certainly thank you yeah it's it's a difficult problem i i'm tempted to say the the only winning move is not to play um [Music] just uh following on a little bit from that can you say i think you've touched on this to some extent already can you say something about how you did the tests and how you protected the uh the users who are running the tests in these"
  },
  {
    "startTime": "01:58:01",
    "text": "countries so in these cases we got some bad echo here is this on me are we good okay we're good um in these cases we did all these tests ourselves um so there were no end users directly involved and the no end users were harmed in the making of this paper if you will um and no end users were we tried very hard to not put any end users at risk i was from machines directly at our control and um that we monitored all that very carefully going forward as these things are deployed that's a whole separate question um and that's that's something that as people deploy these things they'll have to wrestle with those um but at least in in all the work we did and in those circumstances there were no users um no users involved all right great thank you very much uh we are basically out of time here um i think we've had three really interesting talks we've had a bunch of really interesting discussion um if you anyone here would like to follow up with the the rest of the office uh you know please do reach out to them uh i'm sure everyone involved will be happy to chat more um the uh you know people may be around for the rest of the meeting we have the gather space if people want to chat afterwards as well uh which is all linked from the agenda um yeah uh really nice talks as i say please reach reach out to the office and uh please uh submit your nominations for the price for 2022 uh the deadline is i believe the end of next week so thank you everybody uh thank you again and and that's all we have so thanks everyone thanks colin"
  }
]
