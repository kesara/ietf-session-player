[
  {
    "startTime": "00:00:04",
    "text": "foreign hello everybody we'll wait a few minutes uh for people to join"
  },
  {
    "startTime": "00:02:21",
    "text": "thank you it's not good all right hang on here okay I've just pasted a copy of the agenda into uh the notes file"
  },
  {
    "startTime": "00:05:03",
    "text": "foreign"
  },
  {
    "startTime": "00:06:17",
    "text": "hey David I don't remember is it difficult as a participant to upload the slides so that I don't do screen share I um I don't remember uh how should we handle this so it's uh you can upload as usual we're using the upload tool and I can approve it if I did yeah I'm just looking so then what I see here there is this uh but I see the chest view there is an upload slides button button here this is the meeting we are at oh okay sure the standard I guess that is uh I thought it was in the mid Echo tool but on the wiki it's usual data tracker I think you go and propose slides there and then uh Jana sure I uh uh approve them yep and thanks for doing this we still don't have a sanitized version of Tucson a shoe song's uh slides from the last meeting um need to need to go reminder you have uploaded it let me take a very quick look before moving it's just like uh"
  },
  {
    "startTime": "00:08:02",
    "text": "is this you still have the Chinese yeah that we are making progress on that we we've got the English translation now but I didn't get around to it for uh I'm moving it clicking on it it looks uh that is just the motivational section I don't bother too much with the title if you don't mind and just uh so it approved and now should I don't know how much how long it takes for meter code to that's fine we we have a topic number one first so should be there I'm in uh we see it in them and uh why do we see it in the middle I don't see it as the slides yet so but uh hopefully soon thank you materials no slides available refresh oh it's uh there is a oh at least the link is there now it should be okay all right now all right"
  },
  {
    "startTime": "00:10:09",
    "text": "okay go ahead Giannis I'm just going to go grab um get the um there it is we need to put uh need to put this up uh to get started so this is the note well slide this is an ietf meeting you're expected to be familiar with this because uh the provisions of this slide apply to all participants okay let's see that'll take care of that now alrighty here's the agenda this should look familiar I just basically take I just keep editing this file every week to uh hopefully get a time slot to write uh going forward and um insert uh the uh the draft that we're gonna do an item two um we have a couple more drafts queued up for the next two meetings um we have uh uh time slot draft and then uh Antoine's uh new draft uh are technically queued up for the next two meetings if people want more drafts in the queue uh just send email and maybe I'll maybe I'll start maintaining maintaining the queue in the wiki somewhere okay this is the agenda anybody want to bash it"
  },
  {
    "startTime": "00:12:05",
    "text": "all right consider the agenda bashed um see if I just call that Bingo okay that's what that was the right thing for me to Echo to do so I guess we are on the sort of uh General discussion uh portion uh I see a new version of uh the requirements draft was posted um do you and the authors believe that draft is stable now hello okay yes I guess we can hear you go ahead uh yes um uh one of the some of the authors uh proposed some new comments and we add the the um to the new version and but we just add one new requirement and for others we add them as a text in the existing requirements and for this version we think uh maybe it's um really better than than the previous one and also then the um zero zero version of the uh of the wgs draft so um maybe um well I think it is stable now but uh by the way yeah but I sent the updated version two weeks ago to the mail list and haven't got any comments um on it yeah I apologize so I hadn't found the time to do a full run um and I think with the slides today there are a couple of discussion slides on the operation requirements and I wanted to make sure if or how best they're captured in the requirements"
  },
  {
    "startTime": "00:14:01",
    "text": "document all right we'll have to see what happens when we get there we do need to see a requirements graph because um I think we've gotten to the point where we've got to at least be attempting to evaluate one or more of the existing TSN mechanisms uh um I view this as sort of test running with the requirements so we can shake out uh bugs or unanticipated problems with them um without them reflecting on a proposed on a proposed new mechanism let me quickly share I did run a diff on that this morning just for my own uh grins let me quickly share let's see if I do this right that one I think yeah there it is okay I popped this up simply to show here's uh three seven which is here this is this is the new requirement uh that Peng was uh was was referred was referring to yeah and you can also see the other major text Edition just above it on the screen so this is just for everybody's information and I guess we'll come back to this during uh the uh item two uh presentation okay all right meet Echo seem to survived closing that that worked oh all right um yeah so for example on that complex topology um I very much liked from the prior from one of the prior presentation uh the the explanation about the you know burst buildup in rings right so I mean that"
  },
  {
    "startTime": "00:16:00",
    "text": "might be a good reference to add to that section to uh to to to to give some um better understanding how to assess the requirement right because I think that's that's that's one of our next issues we can write requirements but how do we assess them and so those those aspects are then they're very helpful I think all right so I think we're looking for a volunt we should have had this discussion last time I think we're looking for a volunteer or two or three to take um the existing uh TSN mechanisms and and attempt to evaluate against the requirements we know that they're not going to meet most of them but having an initial attempt to do the evaluation will give us some uh confidence that the requirements are tractable before we start going through uh the new proposals um so looking for volunteers don't have to speak up now but I think this is I think this is this is is an important Next Step uh to uh to make progress I just have a little bit of backlog on my on the things I wanted to do for that net so uh in two weeks time or so I can't do it something in the next two weeks but afterwards I should be able to yeah and there's there are enough people interested in in cqf I mean we uh we have at least three three drafts that are that are proposing to uh in effect extend uh uh extend cqf with a uh cycle ID mechanism so between the authors of those drafts I"
  },
  {
    "startTime": "00:18:01",
    "text": "would hope there'd be one at least one author who'd be willing to take a um take an initial attempt at evaluating the original uh cqf mechanism uh against our requirements I think we're probably looking for is about about a sentence per section of the requirements draft saying whether it meets or does not meet and and why does not we don't need a whole a whole new draft on this I would hope about a sentence a sentence per requirement would get the job done certainly good point if if we can't get an answer from from from from the from the folks now I'll certainly take it up with them offline okay I need to take some notes Here uh would help if somebody else uh could get into hedge talk and start taking notes I'm not trying to run the meeting and do all do all the note do all the note taking myself"
  },
  {
    "startTime": "00:20:30",
    "text": "okay I'm caught up again um so we're still in uh in item one process oriented topics any other uh comments requests questions okay Turlock should you volunteered yourself as the uh designated victim to do the presentation on tcqf to I think the colleagues are online so they can jump in if I'm missing things okay sounds good why don't we do that because I think you said you had some slides in there that may affect the requirements so let me go ahead and go go ahead and go to item two do that presentation and then uh we can get return requirements discussion and talk about next steps uh as we go along all right now let's see if the theory works I click on meeting materials um and I hit refresh and I see slides well I don't see slides that's good um uh there's there's a refresh button in the materials window if you hit that uh Miracle goes back and looks okay"
  },
  {
    "startTime": "00:22:00",
    "text": "oh all right so we're like okay now um yeah I watched the right thing to do with me if I click on the slides I get a Chrome tab that I can share but I think we want to um do something different there yeah if uh next to the race and that is share preloaded slides but nothing appears there for me so I can just uh yeah if you approve me if David approves me then I think I can share and run the stats myself otherwise ah always have to ask uh back and forward all right I just I just hit I just I just hit the green check mark So Turtles let's see if this works yeah this doesn't look that way it says no slides available okay okay at least for me it's in the middle of the night so I have some excuses why the tools don't like me now let's see slides no if I click on the slides just ask to share slides all right let's go ahead and approve you again no it still says no slides available I don't know all right I have the slides in a Chrome tab so what I'm going to do is I'm going to go ahead and share that you're going to tell me advanced slides and hopefully hopefully we won't get we hopefully we we won't have problems with queuing and forwarding of that request yeah okay so if I click on share pre-loaded slides um ah okay so it's so all right so what we're going to have to do is we're going to do it the hard way um well I can I mean I can uh okay with"
  },
  {
    "startTime": "00:24:02",
    "text": "luck your title slide just showed up and now if I go and fit one page and get rid of a few other windows all right is this reasonably visible to people on meet Echo yeah there it is all right I think it's good yeah it's a somewhat frustrating that we still can't figure out our own tools so all right so here we are um it's like the queuing in forward I'll even make it a little bit bigger all right go for it all right so um and you can see on the bottom where the slides are um so this is um presentation for what we've been talking about several times in the working group hopefully with improved explanations and of course more time to ask questions and as you see there is now a lot of authors because we're in the process of merging uh two of the drafts presented already and we're going to get into that next slide so I was trying to figure out how to structure this uh this presentation and you know um as a completely opposite to you know how I've seen a lot of research uh work being done uh where you know a lot of theoretical stuff is being done and at the end some very insufficient explanation how this relates to reality and has actually test been tested in reality I wanted to try to um start with the motivational stuff being not only the history of what we came from but also what we have done over the years in support of it and then um the other points I think we will we'll see them when we get to them next slide okay next slide so we've been on this uh since about"
  },
  {
    "startTime": "00:26:03",
    "text": "to be looked into um and uh the first draft we presented was in 2018 until 2019 um with Christina and a couple of other co-authors um there was also around the time when we had the Bangkok interim together with TSN if folks remember that another draft in 2019 was also looking at mpls encapsulation um and then of course because we didn't have the uh agreement and time in that net uh to work on this type of work it kind of fell into disrepair um and was revived when we were getting closer and and finishing up on the charter items in that net so 2021 a new version of the draft queuing with multiple buffers was written um and uh then I started to work on with with new co-authors coming from the mpls side of that net to figure out how we can add to the draft not only the basic mechanism but also the other pieces that hopefully are sufficient for standardization um and uh from that respect it also focused uh then as the name implied on the mpls encapsulation because that was pretty much what we had a lot more complete in the debt net forwarding plane then an IP forwarding plane and with all the aspects of the IP forwarding plane being less well resolved it seemed like a faster starting point um so then um in 2022 when we uh discussed this in um in the dead net working group meeting David made the wonderful remark that uh we um don't have necessarily the need for dscp standardization which would allow us a lot easier to do dscp"
  },
  {
    "startTime": "00:28:00",
    "text": "encapsulation in detonate itself and I changed the draft to the IP and mpls encapsulation and then you do another co-authors also um uh added a draft that provided a lot more of the original text improved on the explanation of the mechanism that's what we're also going to review today in more detail and so we're just in the process of merging these two drafts and hopefully that should be also ready within a month or so definitely before before 117. okay next slide so that's that's kind of the the the draft work on the ietf side um you know within Huawei what what we call the the whole thing is deterministic IP so I think the name already implies that we're a lot more interested in IP and IPv6 forwarding planes which is kind of um I think easy to explain when you look at the landscape of service provider networks in China and then Asia where um mpls just never had that much um uh proliferation than the newer IPv6 technology um and so the the two main vehicles that we use to validate um the technology um was um first of all uh very um thorough large-scale high-speed um simulation with uh I think ns3 if I remember correctly so that was presented in ifip 2021 uh that was already done much earlier but it took a while to get uh through to the conferencing side so that is a very good mathematical review um I think for the model the picture on the right hand side shows a little bit um The Logical Network that that was tested for that but to me much more interesting we also built a high-speed um router implementation of the hardware so that was based on off the shelf 100"
  },
  {
    "startTime": "00:30:00",
    "text": "gigabit Ethernet routers um with an fpga to do the cyclic queuing just because that was not standard in that type of Hardware now of course the cycling queuing itself is very much standard on one gigabit and 10 gigabit switches that are built in the ethernet space against TSN requirements so those could much more easily support this technology than than the high speed switches [Music] um and that was also then validated with omnet plus simulations um on the larger topologies of a is 1239 which is the well-known Sprint topology uh with 315 routers and 1944 links so both Hardware simulation in detail and in large scale next slide uh one quick question on the side of just shown here let me go back on the lower right hand diagram um the red flows or B is obviously best effort what does the HP stand for in the green flash Precision sorry that was another wonderful marketing um term that was uh invented so we could have said that net or dip okay tkf yep right so the large-scale validation was uh done on the cine research Network across China so that's similar to you know European American Research networks dark fiber uh connecting in many of the cities and then also having processes in place on how research work can be uh deployed there so effectively you know in different parts of of the network these prototype routers were deployed and then measured and so the report so we had problems with these web pages so that's that's going to change and those reports were just in Chinese we have them now in English so stay tuned I'll send it to the mailing list once once we"
  },
  {
    "startTime": "00:32:02",
    "text": "have overhauled that next slide shows a little bit kind of of Eye Candy of um from unfortunately still um yeah sorry next slide from that so that's that's an example uh topology I think the longest links that that were used in this were um uh a couple of uh several hundred miles long so I think the the total length um for example just for the propagation latency um which of course is very important in in the mechanism was over a thousand kilometers um and then of course also going across multiple hubs doing the um injection of traffic measuring the burstiness next slide and then actually proving here um that uh the uh latency really sticks within a very uh narrow window so it's not only bounded but it also has a very low Jitter um and uh yeah once once the team is translated it makes a lot more sense yeah so so this was just you know for me it's always good eye candy to to uh to know that there was a lot of validation that this is not just a you know a research project for which maybe a best a simulation has been done but that this is a technology much further down the line so that we feel very comfortable for um saying this this can be standardized in the ITF next slide um quick question uh it looks like um the on the right I see IP being compared to dip what what is the heading on the two bold columns that seem to show the massive difference yeah so that that I think is the the total Jitter and so that's where on the on the IP that's best effort right and and that of course shows that that you have in the IP case uh across many hops"
  },
  {
    "startTime": "00:34:00",
    "text": "so the um the rows are the number of hops um and um that that shows then that that you're accumulating more more Jitter in the best uh effort traffic and of course you're not uh incuring more Jitter in the um deterministic uh in the cqf traffic thank you and I think we look forward to a version of this slide with uh with with suitable translation thank you yeah although sorry this is the Chinese class here sorry if you are on the wrong meeting no sorry kidding next slide okay so so our claims right so tcqf can easily be implemented at scale in existing type of wide area network routers tckf has been validated through simulation POG implementations in white area network deployments up to 2000 kilometer path um it provides very low latency very sorry very long Jitter which we call on time forwarding um and I think then we're going to talk about that later in the slides that's one of the core requirements for many core use cases um tcqf can support iitf network layer data planes validation was done with IPv6 headers draft currently supports IP IPv6 mpls and then Sr srv6 srmpls are equally supportable and of course even my favorite topic of beer from the multicast site is is also supportable the header extensions there are another interesting aspect um and the the the biggest Beauty would be that for immediate adoption you know standardization of of extension headers always being something that the ietf is slower with where six men and mpls working group would probably have a big say we can do it without New extension headers for a good subset of the what"
  },
  {
    "startTime": "00:36:02",
    "text": "what what the solution overall could offer with DHCP and traffic class next slide uh let me quick interject on a little bit of structural question before you dive into uh the the the details so my understanding from shushong's presentation last week is that um the basic um um extensions to the cqf uh uh forwarding uh queuing and forwarding are common between what you're doing here and what that draft was doing and the primary difference is this draft is proposing just uh communicating a single identifier tag or cycle label whatever term you want to use whereas the csqf draft last week is specific to something like Sr or mpls and is effectively communicating a stack of tags or labels uh to the same mechanism does that match your understanding yes that does and unfortunately it didn't have the time to watch the Youtube yet so I'm not quite sure how much in details she for example was trying to do a comparison in terms of complexities and or benefits downside between the two mechanisms I could easily see that we merged that all together into a single solution um but obviously there are you know Pro and cons of the two options okay let's let's see that's another discussion cool I was I was only trying to go after make sure we have a shared understanding of structure I don't recall much in the way of of comparison it was a rather different architecture in that uh it relies on a centralized controller to make the label Stacks work okay thank you now we go next slide yep okay so here is the uh wonderful um Joyride next slide"
  },
  {
    "startTime": "00:38:02",
    "text": "so right so we're starting with cycle queuing and forwarding from TSN 802.1 c q CH as it was originally called and then um merged into the overall 802.1 standard um but if you hear qch from TSN people that means cqf you can have never enough tlas to to name the same thing um so and it was evolved actually as a very simple profile from the TSN time aware Shapers which is 802.1 qbv um and so the interesting part is that you don't necessarily in TSN equipment see that they're supporting cqf you just need to look if they're supporting qbv and then pretty much um you know it's it's it's a controller plane mechanism over there to configure that so too as to do cqf um and so at the core of Tess and cqf are the pro hop forwarding of packets based on their arrival time on each switch right so um and this allows the you know being built against the arrival time allows these two mechanisms to operate without additional new headers to carry um you know queuing information and that was I think one of the big design goals um and so ultimately they were also designed against the requirements back then from TSN which means primarily land deployments intra-car up to maybe manufacturing floor and then speeds of 100 megabits and one gigabits um I think the maximum would be 10 gigabits in switches but I haven't actually been able to validate which 10 gigabit switch interfaces would support it in the TSN space but certainly be happy to hear about that from TSN people um with respect to their knowledge next slide"
  },
  {
    "startTime": "00:40:01",
    "text": "so in this slide um was already shown in ITF so a couple of repetitions here um so as you see you have a path here um from left to right through three routers um each router has uh two cycle buffers and you periodically let's say every you know 100 microseconds are switching between these buffers one buffer is receiving the packets the other buffer is sending the packets so it's kind of bucket passing if if you know that old scheme um and so that obviously means that you very simply are having um a well-defined perhap latency which is the cycle time and you are just having to fit all the traffic you want to forward into the cycle buffer which you can calculate based on the cycle time and then the link speed and so you're just periodically running this and that way you get deterministic bounded latency and of course only cycle time latency and on the right hand side um uh they're the the formulastic repetition of what I said um now the interesting part is that um the propagation latency from one hop to the other is introducing a dreaded problem which is called the dead time and so we'll have that on a later slide in more detail and that's the orange block so I think let's go to the next slide should be the explanation of the Dead time no oh no we're getting to that later sorry oh there is some okay we've got a PDF issue but hopefully we will survive um now the the interesting part is that even though cqf was built"
  },
  {
    "startTime": "00:42:00",
    "text": "um for in our opinions rather low speed networks nothing against cars um it actually becomes a lot more attractive fundamentally the faster the networks become right because the faster the network is the more bytes you can fit into a um buffer um for the same length of time and so the table below shows that obviously right so whenever you increase the speed by Factor 10 you get 10 times more bytes in there so um you don't need to increase the cycle time to um uh make make make more traffic work with this mechanism but instead you can build very high speed networks with very short latency incurred by the cycle time right on every hop you need to wait cycle time before you can forward the packet again so that latency adds up if you have many hops that adds up more but if you have 100 gigabit networks you now um got a much lower overall impact on that than in smaller networks as early slower networks gigabit Networks was there anything else no I think that's that's that's it next slide so now here is that that time issue and hopefully that's visible so this was directly taken from the uh draft um and so if you look here what you'll see in the picture is that you have um the buffer on cycle I minus one that is being sent and um on the upper part you see node a so it is sending this uh this buffer content just a sequence of all the packets filling up the cycle you know"
  },
  {
    "startTime": "00:44:00",
    "text": "let's say as good as possible but there is propagation latency there is processing delay so there are a couple of aspects why these bytes arrive at the receiving node B somewhat later and what we need to ensure is because any byte any packet being received can only be put in the right cycle buffer based on the arrival time we need to make sure that the last byte of the last packet still arrived at the time before um the cycle is being switched so if the total latency that is introduced by the propagation of the link by the processing um if that accumulates up to let's say here in the picture looks like 30 of the cycle time that means you cannot really support um the cycle to be full of data you can only fill it up to 63 66 33 percent of the cycle you need to keep empty so that you can send out the cycle in time to arrive at the next hop and so obviously the biggest issue that you have is that the speed of light is unfortunately still not changeable and so when a network link becomes larger than let's say two kilometers and for I think was the cycle time did we calculate should have written it on the slide something like 20 microsecond then the dead time becomes larger than the cycle time and effectively have zero throughput now you can argue that you never need um 100 of deterministic traffic in a network so that's why some um that time may be acceptable but it's certainly something that makes a design of a network especially when it becomes larger really really problematic and would be really good to avoid having to um deal with the debt time and eliminate"
  },
  {
    "startTime": "00:46:00",
    "text": "it um so we would rather like to have something that independent of how large the network is we can have 100 utilization next slide so now when we go to three cycle operations you see here um hopefully a little bit better visualization although I'm not quite sure how well I get that into the draft then um so Cycle One cycle two cycle three then cycle one again so now we're cycling through three cycles and uh yes we're looking at cycle one and it's being propagated to the receiver node B and as we see it does arrive somewhere in between cycle one and cycle two so what we obviously need to do now is to say okay packets from cycle one when they arrive they need to be put into cycle three because obviously during the time that Cycle One packets are arriving we are sending out first packets from cycle one and then packets from cycle two right so we need to kind of map anything from Cycle One into cycle three so when cycle three is being sent out we know Cycle One packets never arrive so far so good and we can't even do this without you know indicating the cycle identifier in the packet purely relying on the reception clock um because we assume or let's say if we assume that the propagation latency is exactly accurate as um it was in the TSN case because you know when a packet arrives exactly in this interval that we're showing here between cycle one and cycle two the controller would have had to figure out okay this is how high the propagation latency is um so in this period of time all the packets that are arriving will be from cycle one so this can be done this is"
  },
  {
    "startTime": "00:48:02",
    "text": "also was proposed to a TSN as the so-called multi-buffer cqf version um to the best of of my understanding that has not been adopted by TSN but would certainly love to hear um if that was adopted by TSN so next slide so now the reality unfortunately is that relying on the um accurate arrival time um is really impossible in large-scale networks and that's where we come to the variations of the arrival time that we have to deal with in realistic high-speed wide area networks right and the most important reasons for variations of the arrival time is the inaccuracy in synchronized clocks and that is uh being measured by a factor called the maximum time interval error and that pretty much means that if B thinks you know some particular time stamp is time T then the sender has thought it is time T minus mpie up to the point t plus mtie right so that is the window of difference between the timestamps in two synchronized clocks and so cqf at one gigabit per second already requires sub microsecond mtie so that the variations due to the mpie Do Not impact the performance so the mtie becomes a factor in the debt time you want the dead time maybe to be less than 10 percent of the cycle interval you have other processing latencies and so on in and so you try to keep the mtie below one percent of the cycle time and Ebola you're ending up already with the one gigabit link"
  },
  {
    "startTime": "00:50:01",
    "text": "um with um such a high accuracy now if we wanted to take the same mechanism to 100 gigabit maybe even reduce the cycle times then we might end up even with as much as 100 times better um uh clock synchronization so that wouldn't be possible um because the you know clock synchronization can only go to a certain degree every time you're making clock synchronization more accurate you're increasing the cost of the clock synchronization right so that's that's in our opinion the the most important factor so then we have differences in the link media propagation latency um so um one good example is the length of the link changes right so if you have any type of wires hanging from Poles typically those are on power lines but in the US I think almost everything is uh you know over the Earth copper or Fiber lines and then they actually do hang through a lot more you see it in summer so if you go into uh you know power networks they do a humongous lot of calculations and if they're seeing 30 to 30 percent length increasing between the hottest and coldest times so even through a 24 hour cycle so that's that's one example of actual um differences in link and media propagation latency I'm not sure actually I put all the all the factors that impact uh on that do create variations in the real right buckets right so um when we look into processing latencies right so we have various different um Advanced mechanisms forward error correction re-transmissions so for example in DSL we have eight millisecond Windows if a packet is lost that one is recalculated from FEC would be eight milliseconds later obviously it's a bad example that's way too much of a low speed link to be of interest but just you know the only FEC example that I really understood well I've have never"
  },
  {
    "startTime": "00:52:00",
    "text": "looked into the 100 gigabit FEC that I think they also have there as well um Wi-Fi radio links um there I don't think all of them are processing latencies right we also get interference Reflections which I'd rather say are linked media propagation latencies um then when we get into actual high-speed fabric based multi-line card forwarders and I I'm not sure if people remember from early days in in that net when we had these discussions on the basic forwarding plane right so so there are a lot of variations possible um in the processing and uh you can argue that oh we can you know shape these out all at the lower layers um and basically just take the worst case latency that we're going to accept and buffer up everything but this may actually introduce a lot of unnecessary complexity at these lower layers If instead we can solve this problem in the cqf solution so that's basically um in our opinion then also a overall system design simplification by having the shaping effectively happening through the cqf layer next slide sorry did I lose you or I don't see the slide moving I turned off my mic let me turn it back on sorry I was going to say is uh I would suggest that uh we use the term Jitter for where you have different processing processing latency so that we can the link has uh both latency and and Jitter properties that that need help with Clarity okay my apologies next slide yep so here is is an attempt to visualize the problem and explain why the variations the Jitter"
  },
  {
    "startTime": "00:54:00",
    "text": "um in processing in propagation do necessitate um putting the cycle identifier into the packet in some shape or fashion so we started with the previous picture right so we have cycle one now we're also showing cycle two now if we look at at the bottom here of the picture we have some cycle I and we have the variation and that's the orange part right so what effectively happens is when a packet arrives and we're looking at the arrival time to identify which cycle a packet comes from we have to add all the variation as kind of attributing to the possible um time window in which for the clock of the receiver packets are arriving and when you then paint two different Cycles in this case cycle one in cycle two you see that the arrival windows are overlapping right and so this over Apple over overlap of the possible arrival Windows of adjacent Cycles means that we cannot determine from the arrival time alone which cycle the packet was sent from right so um let's go to the next slide where we're kind of discussing this in more detail right um so we cannot simply guess that right um the sending cycle and put the package just into oh yeah just one of the cycle buffers will fit so that completely messes up the admission control the latency calculation and creates possible congestion packet drop right so we could use the dead time concept but if you go back to the prior slide you'll see that this quickly reduces the possible throughput Right with variations of 50 of the cycle time the throughput already goes down to zero in terms of the Dead time that that we effectively are getting eats up uh the whole cycle time right and with tagging we never need to reduce the throughput right but the higher the variations the higher the Jitter the more Cycles we simply need right so and and this is another part we"
  },
  {
    "startTime": "00:56:02",
    "text": "still need to put in more detail into the um drafts but I think it was easily visible from the prior things so if we have less than 50 cycle time um then three Cycles will suffice next slide so here in summary right so um we think that cqf has attractive features and potential for large-scale deterministic network deployments multi-buffer cqf is straightforward extension from TSN um easy to build PLC Hardware um cycle to cycle mapping configuration based on link propagation latency and the number of Cycles based on variation right so then simply we need to eliminate the reception time-based assignment of packets to a cycle buffer and so instead we are relying on a packet metadata where we put the cycle ID into the packet header we're calling we in in the case of my draft we called it tag because you know late in the 90s there was a technology that was called Tech switching some packet header field which was Rewritten in every hop and magically that actually was later called mpls so Steward basically suggested that we call this that then now a tech um tech-based um cqf right so in result we can support arbitrary links we can support lower clock sync accuracy right so we can go up to um mtie that is larger than the cycle time and simply calculate how many um you know Cycles we then need um together with the propagation latency and of course we can support uh higher node propagation latency variations as well in case we do have that and I think where especially then also safe if let's say the mobile World wants to come and"
  },
  {
    "startTime": "00:58:00",
    "text": "also adopt this and not only the wired links that we are I think currently mostly looking into okay next slide okay so here is kind of a couple of sections which hopefully we can run through fast so these These are repeating what um I was um I think presenting two years back just as as as I think reminders um why is low Jitter important and what impact does it have to uh the oval design complexity next slide so this this was some picture to to outline that we've always been thinking about two type of um application requirements but also mechanisms that we have in the network right the in time means that the packet can arrive as fast as as possible so if the network has no other competing traffic then there is no no queuing latency so TSN ATS is an example of that z-score I think is claiming to do the same still need to sit down on the details um and then of course if the network is loaded with the maximum amount of traffic then you have the maximum bounded latency and in the on time case uh you have just a very small red window of latency variation Jitter um and so tcqf cqf they're all on time mechanism um and the damper mechanisms as well um so those are the two things we're primarily looking into and um so why are we interested in the on-time mechanism next slide so this goes pretty much back to um how a lot of distributed applications are written that want to have deterministic networking and so manufacturing is one example but if we look into large-scale networks then it would equally let's say remote driving of a car Cloud PLC and so on um so what we have are control loops"
  },
  {
    "startTime": "01:00:01",
    "text": "where packets are being sent between let's say one Central entity often called the PLC in um you know industrial networks and then many so we're just showing two type of devices some of them are only returning data they're called sensors and then others are basically doing something they're called actors um and what you're doing from the central logic is you're sending out packets that are triggering a response and so the sensors and actors in uh in an on-time service in the industrial world is called synchronous Network or synchronous traffic um the arrival time of the packet and the immediate triggering of the response that is by itself the timing mechanism the sensors in the actress they do not need to have clock synchronization the Central device knows what the latency is what the the guaranteed synchronous latency is to each sender and each actor so it can calculate when to send the packets to trigger an action at a particular time or to get a feedback that is from a sensor that is valid at a particular time so you are not requiring clock synchronization through the network to all these devices so if we are now replacing this type of deployment from synchronous networks with the mechanisms like TSN ATS which is asynchronous in the network then we're going back to building more expensive sensors and actors that need to have clock synchronization so that they get these packets that arrive too early buffering it out so that at the point in time when you know the PLC wants to have the actual response they can start responding right um and so that means that even if I take out supposedly clock synchronization from the network with something like TSN"
  },
  {
    "startTime": "01:02:00",
    "text": "ATS I need to bring it back to the network just to be able to provide clock synchronization to the client devices and if I look at a lot of networks like for example mobile network access rings every node in that network is connected to Edge devices so I pretty much have clock synchronization back everywhere so that's why the benefits of TSN ATS and we had these these type of meetings with people from the industrial world or so aren't necessarily very um popular in these type of applications education environments next slide so I think I said this all next slide okay scalability next slide so this is now um kind of the the the outline of how networks with our current per flow per hop State based models like TSN ATS uh have to look like right so how inserv RFC 2212 TSN ATS uh would look like and you see on the bottom a multi-hop networks with a service provider notion of the provider Edge node connecting to senders and receivers and then intermediate hops even if they are you know serving for other flows as Edge devices still we need to look at the totality of flows that they're dealing with um and so for every TSN ATS or you know insert flow what we need to do is the controller plane needs to push down for each of these flows um a state um and you need to process this state right so there's both controller plane signaling plane um and then execution forwarding plane impact on the forwarding playing the biggest issue is the per flow shaper which was improved with TSN ATS with an"
  },
  {
    "startTime": "01:04:00",
    "text": "interleaved regulator but it is still a per flow Hardware processing that in large scale wide area networks would need to scale to a large number and I do have from a completely different technology a lot of experience so we did exactly deploy this in wide area networks with a technology called multicast not for queuing but for replication and customers really hate it and still hate the need of having to look on every intermediate router into the behavior of every individual multicast flow TV stream and which is why you know in the ITF for five years we've been working on getting rid of that per hop per multicast 3 state so very same you know thing different technology but obviously same you know requirements desires from Network operators next slide right so the scalability issue that I also showed this slide right so it's an n-square issue right of course we're coming in and saying well you know we have aggregation when we get into the service provider Network we can aggregate all the flows that are behind um our sender Edge device that go to the same egress Edge device to the same egress PE but if we have a hundred PE devices then we just have 100 times 99 divided by two so we have you know 100 square in the order of that many flows and that's where we need to have Shapers for that's what we have interleaved Regulators for um the signaling whenever these are changing right um so and if you look at it in tcqf on each of the intervening routers we would just need three to five cycle cues um on P1 on P2 right so um totally independent of the number of edge devices however many queues we need"
  },
  {
    "startTime": "01:06:01",
    "text": "to have in the hardware and need to Define mapping tables for it doesn't depend on the number of edge devices next slide foreign ly really brings us to then the desirable that net Qs option right we of course still have the controller plane we're still doing all the wonderful calculation for every aggregated and aggregated flow that we want to have through the network and we still need to talk from the controller plane to the sender and then also the Ingress PE to make sure that you know the senders aren't sending too much traffic um but then on every hop on all the P nodes when we only have our tcqf cycle cues we really have what I would call and I'm happy to hear if if David disagrees uh what I would call a diffserv only per class Qs right so um with a tcqf maybe being one div surf class of course needing more than one marking for it um the cycle ID but that's basically is exactly the same scale discussion which in the 90s kind of 30 years 25 years ago brought us to go away from insurf to diff Surf and I think that's exactly the same direction which I think hopefully we're going to get through tcqf or one of the other mechanisms here um for large-scale Networks with the bounded latency and hopefully the bound it tightly bounded Jitter next slide David says we can split hairs on exactly what words used to call the concept later diffserv has this has this structural concept already uh with the notion of uh multiple multiple drop uh precedences in a uh in in a group and this would be applying the concept into in a different context"
  },
  {
    "startTime": "01:08:00",
    "text": "that's a good comparison okay so summary Metro and larger Network operators do not want per flow perhaps State on P routers it's an operational nightmare it is why segment routing in mpls in IPv6 and beer in multicast were created by the ietf right it's about control plane performance reliability scale challenges updates to each P routers and the dead net Solutions need to support all those stateless forwarding traffic steering options right so we need to have something that works well with these mechanisms and not only ones that require perhaps during a setup like RSV pte was doing before the whole service provider World moved to segment routing small Interrupters could you remind everyone what you mean when you say p router uh so that's provider core router right so p and p e p is the provider Edge router so it's the first Hub into a service provider Network and the P router is then a router uh internally in the service provider Network um and then the packet ultimately arrives at the egress PE router where it is passed on to the customer again thank you so that was in the pictures on the bottom and uh yeah the slides are then readable yeah um right so and I think I'm repeating that a little bit as a summary right so I was referring to that we did go through the same discussion uh in the 90s in Surf RAC 2212 exactly because of the same reasons um but I think the problems today um with more Hardware uh faster Hardware becoming even more important um and also from the multicast experience it wasn't only kind of looking at thousands of multicast States it's also the problem which if we expand"
  },
  {
    "startTime": "01:10:03",
    "text": "the the debtnet use cases or even some of the use cases we have in the dead net work already one of the important core things to see is that um traditionally if you if customers are changing traffic that doesn't impact the behavior of the service provider routers forwarding and control plane right so only when the topology of the network changes does routing change right so now it's it's certainly not 100 true if if over longer periods of time the whole traffic patterns of customers are changing then you do change your traffic engineering accordingly but that's a very convoluted complex well-planned and long process you cannot have a few customers very quickly creating big changes in the control and forwarding plane of the router in normal unicast you can do that exactly with multicast because you create a lot of flows and these flows create State on every P router and so you have you know total new type of attack vectors and that was uh what what another aspect of what was killing normal multi cars and service providers they wanted not to have these attack vectors and the same way if we consider that we don't want to have long provisioned that net Services where you say oh I'm sending effects to service provider and a week later I'm going to get you know my that net a service class but you're starting an actual application like an instance of a remote driving and you want that traffic to immediately have um death net services with guaranteed latency and low Jitter then you're also talking about what service providers may call Auto provisioning automatically the application signaled and that holds signaling and that admission of resources should only go to the"
  },
  {
    "startTime": "01:12:02",
    "text": "controller plane and at best to an Ingress router on the service provider for you know a service provider managed CE but it shouldn't impact any of the state in the core routers and only these stateless mechanisms like what we can get with tcqf will allow for that next slide okay so that gets us to the wonderful um short section on packet encapsulations of the cycle next slide okay just a quick reminder that um we're we're separating a selection of the queuing and uh scheduling uh mechanisms in the nodes from uh dealing with the data that has to be communicated to make them work and the pack encapsulation is about how to communicate the data with that go ahead yep next slide so of course um you know I I think it's fine when we you know architecturally want to separate that and by the way there is this on the last Slide the question of what should we put into which draft I think the what I have a hard time in separating out just architecturally is that different encapsulation options of course have different Pro and cons and different mechanisms have different Pro and cons with respect to different encapsulations so one of the biggest benefits for short-term adoption um of any mechanism is that if we can fit it into existing packet header attacks that we can use that's you know I think a deployment benefit and because we only need a very few um different values to carry um the cycle ID which three or four um would be perfectly fine for a lot of initial deployments it can fit for example in the mpls traffic class field where you know we arguably could have"
  },
  {
    "startTime": "01:14:01",
    "text": "four values in dscp we can have have up to 16 private dscp so that will give us I think all the you know cycle IDs we would ever want um so that's an interaction I think it would be good to be able to somehow um highlight that um as as possible value proposition compared to other mechanisms even even mechanisms I would prefer longer term but which would require a lot more header work um but even in tcqf obviously um when we have the discussion um you know how do we do extension headers um then obviously the question is are we going to have extension headers differently for each queuing mechanism and then also for the other uh death net aspects I for example would be you know a fan of having a single um that net extension header at least for IP right so the deterministic IP header or something like that um and so just to that extent um that one of the two drafts that we've written next slide please um is proposing um as a starting okay let me quick make one comment on this slide there is a there is Believe It or Not a severe understatement on this slide right in the about the middle of the slide towards the right it says TC is quite limited uh that's being very polite there's a lot of stuff that tries to go into TC if anything that you have a bigger problem with TC than you have with the SCP right right I'd certainly love to to find some you know hallway time to to be educated on that because you're probably because I might suggest you talk with you I might suggest you talk with Stewart Bryant who uh is very familiar with everything that uh that that wants to land in DC and what happens out there well actually I think that was the the one piece is where I think as Stewart uh could could remember a lot of drafts but I think the the problem was a little bit"
  },
  {
    "startTime": "01:16:01",
    "text": "uh the difference between drafts and that what was deployed in networks and so the the deployment uh memories that we have was that there was really not that much adoption of uh different TC um values in the networks so and that may be wrong so because that might have come a lot later so that's that's why certainly more input yeah I talked with Stuart a lot about that when we started the draft um Okay so um and uh when it comes so so the proposal of course uh with IP uh you know uh being well under represented compared to mpls in the forwarding plane natively there would certainly become a a discussion with the six men working group so um the the draft specifies uh some uh possible options so with the destination option header and the uh what was the other option header called I'm always the hopper hop header um so obviously we know there is a lot of ongoing work already in six months so that's the rightfully something which I think we should start talking about six men how they would like to see you know um this being discussed with them and so the header here very simple right so typical overhead of an IPv6 extension header cycle ID and then also proposal for starting extensions um I I think we may want to think about adding um the the fields that we wanted to have for pre-op to that right so um there was uh so we'll leave it in here right now um in in our drafts and then see how that goes in the discussion uh next slide okay quick quick uh note of featuring this slide uh there's a nasty air cream Collision on Doh and ietf you probably ought to be ought not to be using that acronym here uh which which doh do we have DNS over http oh"
  },
  {
    "startTime": "01:18:01",
    "text": "yeah I was I was already yeah just just just just just just a warning particularly if we um for discretion that's outside of this working group um we had that that acronym has a collision problem that's a good question if uh what what six men says about that right because they were a lot earlier than uh uh the DNS people next slide you should be looking at specification outline yep right and so I think that's a general yeah next slide yeah so I'm and I'm not going to show uh any any any of the slides uh uh with with the details of the specification we very much overhauled uh by by integrating in the next version um uh the second draft the the overview motivation background so this from cqf to tcaf to understand those details um then uh we already got the section how it fits the detnet architecture which is also trying to bring in that type of uh you know aggregated behavior um kind of like more more like that net and then of course the specification of the forwarding plane um with a per hop configuration model where basically for every incoming interface there is a cycle mapping of the received cycle to the outgoing cycle based on population by the controller um and then the uh per hop packet processing specifications so how to uh basically send the packet to the output queue cycle ID termination textual pseudocode and I'm coming from the multicast side so we've kind of developed a preference there with pseudocode well after here if people like that um the Ingress node Behavior where for each individual flow we need to determine um you know how to put the packets into"
  },
  {
    "startTime": "01:20:00",
    "text": "um the uh cycle buffers obviously um we're trying to keep this at a minimum I think that's that's not necessarily in our opinion even part of the basic specification but for the systematic stuff I think it's good you could equally use something like um you know the tests mechanism on the first Hub router from from TSN for that um and then um section five per encapsulation specification so the details of what is different in the different forwarding planes mostly it is about I think encapsulation but I also thought that the kind of where stuff is carried what's the semantic in mpls between the service and the transport label um so I think that's that's not necessarily only encapsulation um but yeah next slide yeah and then the considerations consideration chapter right so high speed optimizations uh how how implementation can be made easier the uh computation of the cycle mapping at the controller um by taking latencies and then different clock offsets into account um I'm not sure what other you know issues for the controller plane are are important for us to specify um I I think most most of the calculation is really very simple um support for tcqf across links with different bandwidth um I think that that's an interesting aspect that may not be obvious that it's very simple for everybody because it's also just a controller plane issue and then I think uh General controller plane considerations there was a feedback in terms of do we need a centralized controller can it work decentralized yeah we can also use the same decentralized mechanism that we've"
  },
  {
    "startTime": "01:22:00",
    "text": "for the controller plane that we've done in the past for um decentralized cspf calculation um in with RSV pte but of course it will run in the same issues when we get into the high utilization space so I think there are some basic things and I'm not sure kind of how we're actually going at some point in time to to talk about that with teas um yeah so I think in general it would maybe maybe one of the subjects for for for for the team overall is uh you know how how do we progress with these drafts in terms of so that they're mutually you know more common and easier to absorb for for for the other draft owners so certainly opinions are very much welcome what we should do here for the structure the content of the of the draft and I think that's it the next slide nope that's it okay thank you very much all right any comments or questions uh shafu go ahead and let me just flip back here if I need to go to a particular slide please tell me what slide I ought to go to shafu if you're speaking we can't hear you"
  },
  {
    "startTime": "01:24:02",
    "text": "all right xiaofu unfortunately is losing his battle with with with the mute with the mute again that's unfortunate uh any uh hopefully he can overcome it we can have a discussion any other questions or comments I think they're so Kieran had one question about this cycle sizes uh whether they can be changed or configurable and um I'm trying to remember if if I if I wrote something there I think there is a requirement for one particular um cycle time to be supported I think that's very much for us to specify in the spec uh what requirements we want to have against interoperability right so um that's that's that's I think a fairly wide open how how broad or how limited I think we definitely need some um mandatory to implement set of parameters I I did specify a must for three Cycles probably you know should be a must for site four cycle operation and then some mandatory um cycle times that that we would agree on I think that's the minimum and anything more that the the iitf working group feels appropriate should go in the draft and I think it's probably worth noting at this point that all of the nodes involved have to use the same cycle time thank you um so my best understanding is that um that so that's that's that I I wrote in the in the controller spec um that that's not necessarily true but you need to be able to do the mapping right so if you have different speeds if on certain links you don't have the full um capacity of the link that you're"
  },
  {
    "startTime": "01:26:00",
    "text": "offering right you obviously can have different cycle times I'm trying to remember if I had a more succinct summarization of of the conditions so that you can have different cycle times let me uh take that as a note here to to review myself from that section because it's been a long time that I worked on that section yeah I mean at a minimum um I think all of those have to agree on what the duration of a single cycle is otherwise I think you're you're you have an opportunity or confusion uh Karen go ahead yeah so Tallis does that mean if I have one gig link over a 10 gbps Network everything will reduce to 1 gbps Network no no no certainly not obviously the the aggregate of traffic that you can pass end to end across a one gigabit link will not magically be more than one gigabit um but obviously you know on the next 10 gigabit Hub um the same cycle can already Multiplex packets coming in from other path right so that can again uh you know be be a lot more than than one gigabit okay thanks let me see where there are other don't simply also I'll jump I'll drop in a question so it was in listening to you it occurs with by comparison to past sessions I heard the word multicast a lot more this time do you um do you believe the scalability draft adequately covers uh covers multicast in addition"
  },
  {
    "startTime": "01:28:01",
    "text": "to uh to unicast this is this is the requirements draft yeah yeah I don't I probably not everybody forgets about it and and I'm now joining those ranks so all right I think yeah okay I was I was only using multicast as as as a comparison of actual deployed experience with per customer per application uh traffic State and networks right and and that we made miserable experiences with that but I think your yours is a completely different point and um yeah I'll have to go back to the requirements draft and then talk it and see the multicast part as well yeah I suggest you find the Cycles to uh look at look at the requirements draft and ensure that it does an adequate job of cover of uh recovering multicast please especially because it's it's the funny part is that in um in TSN you don't hear it being talked about a lot but then it actually is supported in all the places and when I talk with people in the car industry and so on they're actually also using it so um certainly is is not to be ignored foreign any opinions that people have about you know structure of the document or um that that type of thing that may not be applicable only to cqf it certainly love to hear what people think about um you know what how"
  },
  {
    "startTime": "01:30:00",
    "text": "um we can improve the draft okay any other comments or questions on what trail is presented or anything in uh uh anything that's in scope of this meeting okay so I think we're done for now um would very much like to see someone intent to take on uh evaluation of the uh cqf mechanism against the current state of the requirements draft this is this is not so much about uh is cqf unmodified useful but rather shaking out um whether we have a good set of requirements in the requirements draft uh that is useful for evaluation of these mechanisms you honest anything you want to see before we wrap up here he must be tied up with Hydro police stuff this week all right I any any closing words from anyone Kieran just for a clarification so when you talk about uh evaluating with DSN it is"
  },
  {
    "startTime": "01:32:02",
    "text": "only for the cyclic related stuff not for the asynchronous part of the queuing I think any of the anyone who's willing to to take a shot at evaluating any of the TSN mechanisms uh against the scale requirements draft would be greatly appreciated I'm just looking for let's take a take one of the TSM mechanisms which is not sort of Up For Debate uh debate whatever on on whether we adopted here and use it to to do an initial test on usability of requirements draft so if you'd like to do uh to do a different mechanism that's fine I've just simply been suggesting cqf because this is the second session in a row that we've been looking at extensions to cqf another one please go for it uh my understanding is request uh requirements draft is slightly broader right it is uh addressing large-scale issues and not only specific on uh specific to the queuing aspects so will it clutter the document if we try to bring in that perspective um um I'm not sure I understand the question uh the requirements draft ultimately is gonna going to going to wind up writing us some guidance on the data and the encapsulation but there's quite a bit in the requirements draft that can be used to evaluate uh the uh queuing that the queuing and scheduling here and so the the ask is for just some freely written text that can be reviewed in whatever shape or form discussing how well a mechanism pick your choice cqf is popular now and because we talked for"
  },
  {
    "startTime": "01:34:01",
    "text": "two weeks about it right so um how such a mechanism um existing from TSN does or does not meet the requirements and I think the the the the answers to the requirements of the review cannot only include meat does not meet and why but also okay this requirement does not you know uh account again and the basic mechanism itself but is encapsulation specific for example or something like that right yeah so only it's not sort of the requirements in the document would map to queuing and scheduling and uh when we present a requirement we do not say that this belongs to uh this can be satisfied through asynchronous queuing mechanisms or synchronous queuing mechanisms so um or should we put potential solution how those requirements could have been fulfilled in TSN and then the question is to take request is to take TSN to take a TS take one more TSM mechanisms as they exist and you brief run through the requirements draft resulting in a sentence per section requirements draft of whether it meets partially major does not meet the requirement and why um speculations on extensions for TSN are not are not helpful the goal is to take something that's known known and understood and use that to uh to to test uh the use of the requirements draft is that making sense yeah it does I think we have to take one example run through it and see if it satisfies what we are trying to achieve from the requirements documents and we do only"
  },
  {
    "startTime": "01:36:00",
    "text": "want to improve our requirements draft we don't want to improve TSN mechanisms for TSN right right the goal is right his choice the goal is to have to improve our requirements for apps that we use it to evaluate the new proposals we have some confidence um uh in in its use okay any other comments or questions on anything well thank you so much for running this again all right I do believe we're done thank you everybody for taking the time time to attend and participate"
  },
  {
    "startTime": "01:42:06",
    "text": "foreign okay"
  }
]
