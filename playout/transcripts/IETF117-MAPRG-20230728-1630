[
  {
    "startTime": "00:02:28",
    "text": "Good morning, everybody. welcome to Mebrigy. Oh, I have to mute here. Last day of this IETF meeting, yeah, you made it until here. Hopefully, you will join the session. My name is Mia Kulevend. and I'm very excited to have my coach here in person next to me and take longer. So that's the happy note. The sad note is we only have remote presentations today. just like to make you aware, but we have some really good presentation firms to look forward to the session, and we can also have discussions with remote participants course online Do you wanna take over? Yeah. Yeah. Good morning again. been 4 years since I've seen you in person. So and I'm happy to be on the same time zone as a meeting for a change. look look at the agenda for today after we cover the initial things. The the IRTS intellectual property policy is similar to that for the ICF, meaning you're meaning you should be disclosing IPR if if there is some involved with your work, details, the links are there. the session obviously is being recorded. both audio and visual, so be aware of that. And I think by this time in this day, you know that the colors of your lanyard indicate whether or not you want. close of photographs, we have a a privacy and code of conduct for the IRTF and the IETF So we typically"
  },
  {
    "startTime": "00:04:00",
    "text": "not have problems in this group. But if you have any That's it. It's a it's a joke. We typically do not have problems in any Can you get closer to the mic, please? The the but the the link the privacy encoder of conduct issues are here. Please take a look at those. If you there is an Ombudsman team you can contact a person that can help you through this if you have an issue. Yeah. I I just wanted to add to that. Like, even don't notice any problems, of course, there might be problems you don't know about. So in general, I think being very friendly and and being aware that not everybody is on the you know, knows all the background, knows everybody, and so on. Really helps us to improve our work. So please to them to them, to them, to them, to them, to them, to them, to them, to them. Thanks for your yeah. So the goals of the IRTF are different than the IETF We do not create standards here. We do publish some RFCs, but it's a research effort So not a standard development organization. so a little more relaxed in some ways and just know that they're a little different, and we have details linked in the slide too. links to the important things having to do with the meeting. It's important that you find the Meadeko link or scan the code from the the tablet that we're passing around here, And when we when we're doing the when we're running the session, when you wanna get in the queue to speak either remotely or locally, please use Meadeko so we know who you are and it'll manage queue order for us. So it's sufficient to do either the of the the scanning the code there and having a like, for instance, on your phone or using the full display in your computer. We have a mailing list that the link was there. If you wanna do follow on work with Map Archie, please go there. That's also where we do call for contributions, which is how we got the content for today. some tips about what the buttons mean in and meet that go here."
  },
  {
    "startTime": "00:06:06",
    "text": "and here's the agenda for today. So we're going we're we're doing the overview right now. We've got let's see. That's 6 presentations coming up, each of which are scheduled for 15 minutes. maximum for the speaker to talk, which will take us to an hour a half. So I'm happy to say for a change. In some years, we have less content than time. So there should be an average of 5 minutes for a question comments for each of them. So I'm looking forward to participating and giving some feedback to these speakers today. I'm gonna get right into it, and you can see the the topics here. We're gonna start off with Kevin Ramillion with Internet scale reverse trace route And, Kevin, are you ready to go? If you can request to share screen because you're doing it by share screen sharing, we can give you that. your control. Okay. Can you hear me? Yeah. I we can hear you. I'm just clicking grant screen, and it's not getting there. So I'm not hearing you very well. 5th of snive. think I did ask for yeah. Okay. Yeah. You did. It was it was me I'm a little rusty with this. I I I stopped sharing my slides, and now your screen's being shared. So it should come up. Okay. We can see your screen now so you can focus on just the application that you wanna share, There you go. Right. You can see yourself. Alright, Kevin. You you got up to 15 minutes You you can go. Yeah. Thanks. Okay. So okay. So Yeah. Until now, no one has been able to mirror reverse path scale. So all our analysis sees on behalf of the story. in this talk, I will present an Internet's called reverse stress route, the first system that allows you see the other half, the routes used by hosts across the Internet to reach him, to reach him, to reach him."
  },
  {
    "startTime": "00:08:02",
    "text": "And this work has been published at IMC in 2022. So why is it was it not possible to mirror reverse path at scale in today? Well, we had trace route. that could call me a reveal forward path. all we had reverse straight shot. but it had some limitations. It did not scale, was not open, had limitations that I'm going to detail later. what happens when you want to troubleshoot path? So let us suppose that you want to debug a bat. performance between your client and your server. So in this case, you control the server. To troubleshoot the problem, you can use stress route. from your server, the client, which gives you the forward path between your server and the client. In this case, FaceRock revealed that the forward path is not 0 graters, and that reason of the bad performance is not on the forward path. And then the troubleshooting becomes hard because we have no way to debug what is on the reverse path And if the problem is on the reverse trace bars is unlikely to give the relevant information. as we found in our paper that 47 percent of the path are are symmetric. pay us level and much more at RAVO level. And, of course, we do not care about when the client, who care about all your clients across the net in particular if you're a CDM with multiple sites. So an ethernet scale reverse stress route system would solve 2 key problems. know where things went wrong to know which operators to contact, and do it at Internet scale to be able to have information about the maximum number of clients. it can also serve in any scenario when you need reverse path. I don't need troubleshootings. So I told you that reverse phase rather exist but is limited. So here are some more details about these limitations. So we'll call it resting on 1.0 clock. So on accuracy, it sometimes return incorrect path without warnings. What could not address any measurement? On throughput, It could not mirror many reverse path per unique time, because each required many probes. And as we'll see later,"
  },
  {
    "startTime": "00:10:00",
    "text": "reverse freeze rods since packets to make a reverse path. So the fewer packets you send per reverse path, the more path you can mirror in parallel. you cannot just ramp up the propping rate at the packet sent diversaries from our record route packets that are most subjects to rate limiting. we want to minimize the number of them that we send in the network. Finally, on coverage, it could only merit reverse path to planetlab and mLab sources. and was not open to outside sources. This means that an operator in the previous example could not measure reverse through its own sources, meaning the usefulness of the system. in this work, our contributions were refTR 2.0 and inter a reverse phase route system capable of marrying 50,000,000 reverse path per day with higher accuracy from 40 k ASS. and allowing outside sources. This is way above what the original reverse trade structure was capable of doing. and this means that our new system can be an everyday part of the Internet target of researchers and operators, or just like forward trace routes out today. also provide in our paper a demonstration of how to use RFPR 2.0 to troublesh fully performing path with an example very similar to the one I presented. At the beginning, and the large scale study of Internet path So I've shown why your 2.11 rsi level capabilities. So let's have step now to a presentation of the key ideas to build reftr2.0.0. To be clear, the basic technique to merit reverse half is the same as REST GL 1.0. but we identify the key design question of a reverse traceroute and to overcome where it's just one 0.0 limitations. And RevGR 2.0 is a combination of insights found in audio work as well as new measurement techniques and studies that we'll lead in this paper. to understand the design question that I'm going to present. I have to introduce the basic technique. of refjr1.0"
  },
  {
    "startTime": "00:12:01",
    "text": "So recall that our problem is that we want to mirror the path of destination d back to our source as So in the example that I showed at the beginning, we are the operator of a server s, and we want to debug the performance of the client d. The first idea to find reverse hot is to build an Atlas of path to as from voltage points that we control, your V1, V2, and V3. So if later, we intersect the known trade route path because of destination based routing assume that the rest of the hour path is the same. This does not give it the full path from d, but it gives a baseline we can build up. The treasure on ATLAS gives us a good basis, but we'll typically not reveal ups close to the destination. To reveal them, reverse face value is the record route IP option. With the recurrent IP option, each router on the pad implements the option cards quick, records its interface. Up to 9 interfaces can be reached in the record run IP option editor. In our example, if we send a recovery packet from std rather on the forward path, f Oneuptof4 will record their interface. then when you respond to s, reverse hops r one up to all 4, will feel the rest of the RECOT route slots. This is great if you're near enough to use Reconcrown. But in average, a path is more than 8 hubs. So what if d is not reading 8 recurrent ops from s? If s is not close to the all the recon rod slots will be taken at the forward so we will uncover no reverse hops. overcome this limitation, we try to find the vantage point close to the end spoof as this. So here, V2 sends a recall route packet to the proved as s. when the packet when packet verizond as V Two is within 8 recover hubs from the, similarly to what we've seen before, There are slots available for routers l 1 r2 and r3, on the reverse path, it proves that IP address. and then we continue to send smoothly caught route until we intersect the trace route."
  },
  {
    "startTime": "00:14:01",
    "text": "Here, we send this proof recall right from v 1 to r3, revealing R4, R5, and R6. Finally, r 6 intersects the traceroute from 52, so we're done there in the reverse batch. So now that we've to mirror a reverse path. Let me give you some of the design questions related to these techniques. First, for the threshold ATLAS, when the number of sources and the endage points increased, we cannot issue from all vantage points to all sources, and our probing widget will be stretch even thinner. if we need to keep the Atlas up to date over time to avoid introducing errors. we had to smartly select the trace route that we issue leading to a first design question. which traced us to issue for the Atlas. 2nd, it is not easy to identify that. r6, in perspective to hop in the trace route from v 2. Because Trace when you call Roger, not necessarily reveal the same interfaces of the router. So we have a second design question. to identify intersections between the Chorus and the trace of Atlas. And third, How do we choose the best vantage points for any probe to mirror as many hubs with as few measurements as possible when we have to spoof I will only delve into details for the first because of time constraint, more details can be found in the paper and the other ones. So let me dig it now into that is another question that I've presented in the previous slide. So the question was which which which transverse to issue for the ATLAS? But the challenge in that problem is that we are using ripe as each of the trade struts. the platform that has a high number of damage points, but it raise limited. the more hops we can mirror with traced route, the fewer we have to mirror with the court route. So which are, as we have told before, a limiting factor of throughput The goals are to maximize the number of from trace rods to savory caught rock ropes. and we also want to minimize the number of stay all the trace runs in the Atlas to keep a high accuracy."
  },
  {
    "startTime": "00:16:02",
    "text": "So I'm going to show the insight that we found that a random of a 1000 traits for us per source, sufficient to get the most important hops. By important, I mean, whether it is intersected by many reverse tree struts and whether it is many hops away from the source as these two factors impact the number of your caught rock probes that the help would see. To demonstrate its insight, we run traceroute from all the 10 k writebackless probes to our sources. we use 5 k for the trades around Atlas, and get 5 other k as reverse trace routes back to our sources to see where they intersect the assets. On this graph, on the x axis, we vary the number trace rods per source in the Atlas, While on the y axis, you can see the min fraction of hops in the reverse wafer coming from the Teresa Rob. So the higher the better. can see that the random strategy is close to optimal. would define with an Oracle numbering in advance the reverse trayscros that will be issued. What we can see is that a 1000 trays rods gives us most of the benefit we would 5000, and it's close to optimal. So let me tell you some other results that are not shown on this graph. Only open 7% of reverse stress drives intersect or sales release route throughout if we keep trading in the Atlas for a day, we can say trade routes are valid for a day. And therefore, we do not have to reissue them during the day, saving a lot of tracer environments. Finally, the the Atlas converged to optimal in 5 days. where during one day, we run reverse trace rods compute. intersection with a trade route Atlas, and keeping the trade route Atlas, the trade routes team, the most important hubs replace the others with new random ones. now that we've seen how we select the trade routes for the Atlas, let's just look at the impact of throughput. So we see that 56% of the hot of a REITR2.0 measurements actually comes from the threshold ATLAS on average. stating saving recall route probes."
  },
  {
    "startTime": "00:18:01",
    "text": "In the advantage points will sense fewer than fewer recall route probes reverse bath except mirror more path in parallel. So I know that I've shown some ideas behind the design of Rev or 2.0, and that recall that the other ideas are available in the paper. us step into the evaluation. So these are the result that Haitan introduced at the beginning with some more details reversed your white button over. Though we see that BRAFT 2.0 has a way better throughput and a better accuracy. and graph share 1.0, and it allows outside sources. With this, we put an accuracy and the ability to merit to your own sources, referral 2.0 is ready to serve the needs of operators and researchers. And as a result that is due sale in the paper, it's how RevPAR 2.0 compares versus sort of their approaches. To merit reverse path, you could, for instance, run trace routes from ribathlets to your sources or perform forward trace route and assume symmetry. the paper, we show that REFTR 2.0 is an order of magnitude more complete than RIVE Atlas. as with ripe assets who are limited by the ASCs with a probe in them, represent about 4 KAases. We also show that ref TR 2.0 is also more accurate than assuming symmetric. we found that 47% of their path at isometric at AS level. So now that I've shown some results for register 2.0 is misstep to the use case on how one can use RevPAR 2.0 to troubleshoot poorly performing path. to show how a CDN can use RAVR 2.0 to troubleshoot Polycom forming path, we deployed RASTR 2.0 on the peering announcing an any cast prefix. after having announced any cast prefix from peering sites we mirrored the performance of clients that are represented by destinations representative of of Vicidian clients. So we mirror the performance of the different clients, and we observe that some ASE located in the south eastern US"
  },
  {
    "startTime": "00:20:00",
    "text": "Here issue, Lease webinarstipsasgigapop have bad performance because they are routed to the Brazil sites as it should go to the closed site. Without reverse race run is hard to say why. So to find the root code of this problem, we run reverse trace routes from all the clients finding that least within Nordics as Gigapub are some cogent clients, response to improve the performance is to restrict the Brazil announcement to not propagate to Cogent. And after that, we verify Lease Webb and North pump are routed to to Boston with a good performance improving RTT by 70 milliseconds and 99 milliseconds. So these examples show 3 things. With RESTTR 2.0, you can find which routing decisions made to some clients and know that sits at a sub ordinal site. And you can do it for your own sources if you're an operator. as the system is open. we see it later. In our example, peering played the role of the operator. Then accuracy allows you to trust the results. And the coverage of revenue of 2.0 allows you to merit reverse path for many clients. and the throughput allows you to include reverse path measurements in reactive traffic engineering system So to conclude the talk, we've what we've seen is this target that tracer cannot give reverse path, but graph charts 2.0 can. and you can do it at scale, overcoming rest your 1.0 is limitation. GraphTR 2.0 has already proved to be useful in other context. used it for another AMC paper, which looked at traffic engineering strategy during failover, And we we we were told that a security company was using our API and to uncover the providers of malicious ADSs. So, finally, what I would like to tell is that the REF TR 2.0 is an open system. It is currently deployed in mLab, And if you wanna run reverse trade routes, please ask for an API access to that email."
  },
  {
    "startTime": "00:22:00",
    "text": "And if you also want to run reverse phase right of your own source, it's possible to add your source in the system and same same process. You send an email to the an email there with the following information. a lot for your attention, and now I'm open to questions. Thanks Thanks a lot, Kevin. Folks, please use the the little hand icon in in Medecco to join the queue. I see Tim's in there. We've got up to 5 minutes and queries in there. So should I look at the should they look at the chat now or Yep. You you'll be able to hear them asking the questions via audio. Yeah. And I'm not sure if you're able to plan on your video, But if you would like to, would be nice Oh, sure. Yeah. Yeah. I I can do that. go ahead to We can see him turning. Yeah. That's serious. Hi, Tim Chan. So my understanding is the record route option is b 4 thing. This is so what are you doing for v Six reverse roots? So if you have a solution, I'm very happy to to hear it. Because, yeah, yeah, as you said, like, your corporate option is only billable forth. So we have no solution currently for v Six. I will not mention hope by hope. Sorry. Kick you off the video, so you turn your video on. back again. Back again. Back again. Back again. Back again. Oh, sorry. Alright. Corey's up next. Hi. Gary Fairhurst. that was a fun talk. I'm very fast, but I saw a lot of interesting Talk. Stop. Stop. The question I have is what was the packet that you used when you did the record route. because I do stuff in ECMP, and I see the transport payload makes a difference to the routes we see. So you look at different packet types? Is it just an IP packet with no transport or or what is it? It's i it's ICMP echo request. actually."
  },
  {
    "startTime": "00:24:01",
    "text": "it's it's just the ICMP traffic. Yeah. Okay. So Yeah. But but you're right. Like, the the with ECM. So what we observe is that when they're ECMP on the path, recall rights to g like per packet in general. Okay? I mean, that was a really great presentation, so it might be fun to look at that as well. Thanks. Thanks. Ben Schwartz is up next. Alright. Bench works. Not a So this is not the IETF. but we're very near to the IATF. And at Oh, is this microphone not? No. It's video. so he can see you. Oh, Okay. Hello? So we're we're very near to the IOTF here, and the IUTF is in the business of making changes to Internet protocols. it my understanding is that you shows not to make any changes to Internet protocols here. But if you could, Would you would you do this differently? So Yeah. So so, basically, this like, all this work looks like using some hacks to get the reverse path. Right? So I know there is a reverse ratio RFC drafts going on. I don't know the state of it. So my sense is that during the whole process while this RFC might be adopted someday, we should use that system, to get to reverse that and then shift to something that is less a key probably to whenever the the rabbit derived c gets it up. If he gets it up at someday, But there is something out there. Like, a new it's basically based on new ice. It's not from"
  },
  {
    "startTime": "00:26:00",
    "text": "our group that is based on the new ICMP type type Nope. or code, I could not remember. But message that would trigger a reverse stretch rather. So Okay. This is -- Thank you. Yeah. Thanks a lot, Kevin. Thanks for joining us. Oh, we've got Tim Chone in the queue. Let's let's take Tim's question or comment. No. Hi, Tim Chiang again. So Gregory said, there are a lot of very interesting things now. I think you said right near the start that something like 60% of the reverse reach you looked at. the roots were asymmetric. Was that correct? I'd sit is for the 7 person that are symmetric at AS level. And that's -- Alright. -- anymore. probably. back. Alright. Thank you. Alright. Thanks for joining us. from far away, Kevin. Thanks a lot. Yep. Take care. Okay. We have a js recent Gupta up next. I'm gonna share his slides. Jay as re can you say something? Hello. Jake, can you say something? Okay. We heard something just again. Yeah. Am I audible now? Yep. Yep. it. Yeah. It's yeah. That that's that's good. anyway, your your your slide's up and you're ready to go. Yeah. Thank you so much. Yeah. So good morning, everyone. So my today's talk is about web privacy by design. And this is part of a master's fee than by Houston at Technical University of Munich, and the work is funded by Volkswagen Foundation, and it was published last month at IP networking conference. So"
  },
  {
    "startTime": "00:28:01",
    "text": "So we begin our today's talk. So as we know that So, traditionally, unencrypted DNS over UDP is still used as a default. for every DNS resolution on the web. As a result of which, What happens is that the part parties, they are able to create user profiles because they are always able to see what kind of DNS traffic is being transferred. don't, these days, browsers offer users to in to use DNS over HTTPS but users have to specifically opt in to use it. And over that, the other problem is that both DOH and DOT, which are the encrypted DNS protocols that exist now have several problems like they are limited by head of the line blocking problem as well as the multiple round trips. So as a consequence of which, quick, was standardized. which eventually led to the development of DNS over quick. That is DoQ. which is the new player. And DOQ brings in the advantages of a faster handshake as well as it removes this multiple handshake problem as well. by reusing the same by by by using HTTP 3. And but, however, we realized that even then using quick, whether to your q and h three, improvements are pretty much uncovered. And this figure actually shows the typical web browsing scenario that exist in today's world. So the question here is that how to improve the web browsing experience with in build that privacy, the design. And that is what we explore in today's talk. So so the idea that we put forth in this work is to reuse the same underlying quick connection for the DNS resolution using DoQ as well as the web"
  },
  {
    "startTime": "00:30:00",
    "text": "content delivery using h 3 with 0 RTT. We basically coalesce the both the connections to so that it happens over the same underlying 0 RTP connection. And that is what we show in this figure as well. so that every fresh HP request to the web server is also served using the same connection. And we believe that this will not as well as faster, and it offer more optimization potential. So the major question question that comes from these proposal is that is the impact of using the same quick connection on that performance? And how does it impact for both fixed and mobile access network technologies? And these are the two questions that we explore in the in the in the next few slides that are coming up. So here, we present in this slide, we present our methodology, which is our measurement setups So in our measurement setup, what it does is that our measurement setup automates the DNS resolution and the web browsing while emulating network of a user at the age, the setup is encapsulated in network name spaces by nadaling separating clients and server into different network domains. and it primarily decouples the DNS resolution from the actual web page loading on the client side, whereas on the server side, we have the DNS server and the h three server. running under the same process as a design choice. And the Chromium is used the Chromium browser is used to measure the page load times of 3 different categories of web pages So we have a basic HTML page on another HTML with JavaScript I said, and the 3rd page is HTML. with JavaScript and CSS and other cookies. So we perform the experiments under different network conditions"
  },
  {
    "startTime": "00:32:00",
    "text": "which are emulated using the net team package So we have the fiber, the cable, the DSL, and 4g as well as the 4g medium, and we use the CCs measuring broadband data set, to represent the fixed broadband scenario, as well as our net dataset to represent the mobile wireless access technologies. So as part of the measurement, we have made certain changes to the to the open source tools, such as the core DNS and the Chromium, the details of which is available in the paper, And during the evaluation, the data points are either normalized by the scenarios delay or around time of the access technology is depending upon the need So Next, we move on to the evaluation. So our evaluation is broadly broken down into 3 stages. So we first evaluate how Quick interacts with the different application. Let protocols that is DoQ and H3. We then move on to evaluate the DNS overheads. And, finally, we move on to the performance evaluation of how our simulated setup behaves under different network conditions. So in the 1st set of experiments, we see that h three. So we see the handshake connect durations. And we see times roughly corresponds to the DoQ handshake times. where the difference between h30rtvandoner IP is less than one round trip. And certain this thing run step patterns are also visible, which refers to the different access tech logies that we use in our measurement. So what we realized from this of from this is that the performance of DoQ and three one RTT are majorly synchronized. And in general metric scale well with round trips. So in the next set of experiments, what we try to analyze is that how to how does quick scale while interacting with both to your q and h"
  },
  {
    "startTime": "00:34:01",
    "text": "under different network conditions. So here, we show the plots for the fiber scenario as well as the Koji scenario. And from the 5th scenario, we observe that the connect times have a long left tail in the higher percentage, and one RT has a relatively large tail from minimum until the 20th percentile. And Since as I already mentioned that these are The scenario's delay normalized by scenarios delay, therefore, we realized that the actual data for 0 RTP has less variations compared to that of minority and become And then we compare the graph for the final scenario with that of 4g scenario, we realized that the 4g handshake time scales better with our RTT while having less variation. So as a result, the takeaway from this slide is that the processing delay is large for lower RTT access technology, but in absolute terms, the processing delay is seen. for higher RTT access technology. It bays in much less relatively resulting in the observed differences being small between 30 RTT and 1 RTT. So next Next, we move on to what is the override of parison to view UDP. So here, we see that there are there we can again see the steps that are visible, which is because of different access technologies, as I already mentioned, But here, we see that there are no steps for DOUDP. However, for DoQ and DOH, they do not exhibit the expected number of round trips as it should be. So The DNS exchange duration of DoQ is is one round trip faster than that of DOH. So next, we again see this in comparison to different network conditions. So here, we observe that there are leftains for lower person cars visible for both GEOQ and"
  },
  {
    "startTime": "00:36:01",
    "text": "which eventually starts getting smaller for from Pfizer to 4g scenario, and the range of values for 4g is much smaller implying that there is less variation in the data. So, again, the takeaway is that the lower RTP access technologies exhibit longer letters, which eventually get smaller. with increasing delete. Now we move on to the actual evaluation where we evaluate the impact of our emulated setup. So here, we try to see that how our emulated DoQ up with h30RTT setup performsacross different access technologies and web pages with respect to competitors that is deal with respect to its competitors. That is the original DOQ as well as the DOH. So here, we see that along the x axis, the web pages are web pages are placed according to their increasing level of complexity, whereas the access technologies and the y axis are placed as increasing order of the delay. So from the we from the h map, we see that Duh has the highest relative increase across all web pages and access technologies. And the example page shows the highest relative increase overall. whereas the release of increase for the Wikipedia page is greater than that of the Instagram page with just one section where we see we it is being highlighted in the red box. that is the fiber scenario where we see that the Wikipedia page performs better. compared to the traditional DO UDP as well. And the other observation from this map is that performance of the access technologies, degrade an order of their respective round trip delays. However, there are quite a few exceptions to this. For example, we see for the example page in the docu setup, we see that the DSL performs worse compared to 4gor4gmedium And even for the Instagram page in BOH, we see that fiber"
  },
  {
    "startTime": "00:38:01",
    "text": "performs worse compared to DSL So this this hit map was actually showing us The median relative PLT increase. So on the next a slide, we actually go in more detail detail, and we show the entire PLT increase the entire with not just the medium values. So from here, we again have quite a similar observation. That example page has a short left tail. and Wikipedia has a longer left tail, and in has no left tail at all. And the difference in percentage points between the protocol combination is smallest for them gram, whereas it is largest for the Wikipedia. And difference between the protocols that is Tokyo and DOA scales well with the round trip as expected. and difference between h30rttandonertt does not scale with wrong prep as expected. So what we realize from this slide is that both dimensions have an effect on the relative increase over the baseline and increasing delay between the client and the server reduces potential time savings of 0 RTP by settings using the OQ instead of DOH increases. So in the end, we try to analyze the overall impact of DoQ. with h three zero RTT. That is our emulated setup against the baseline, which is the typical web browsing scenario of using DOU UDP. So here, we see that so here, we do the emulation Here, we have 15 data points with the rest one for each web page and access technology couple. And here, we see that DOQ with h30 RTP setup matches the best line for one couple as we had already shown in the hit map. which where the median relative increases around 7.3% And median of DoQ set setup is slightly higher at 10.8%"
  },
  {
    "startTime": "00:40:04",
    "text": "And DOH setup has an average relative increase of 14.7 percent, And the interesting fact from this slide is that even in the worst case scenario, DoQ with h30RTT that is our emulated setup is at 26% whereas DoQ is at 31% and doh@the3 3.7%. Therefore, what we understand from the entire set of results is that using h Three with one RTT, PLT for DOH is inflated by greater than 30% perfect plan and by over 50% for mobile compared to an the DOD. However, when we use our emulated Geoq with h30r80. It reduces the PLT by 1 third of a explain and behalf over mobile compared to the existing setup. So So to discuss the few limitations and the future scope of this, works. So we see that our presented we see that our emulated setup decouples the DNS resolution from the that browsing process. So in the future, we want to actually make changes to the Chromium, which will support for DoQ to be incorporated so that both of both of the DNS resolution in the web browsing is coupled together. And the measurement setup of also uses DoQ with h30RT that currently limited to 1 to web pages having a single DNS resolution. and it is also implemented with a single h three server right now which is uncommon, which is a pretty uncommon scenario if we consider a real life web page loading. And, therefore, we also want to tend our methodology in the future to incorporate web pages with more than 1 DNS resolution and also incorporate cross traffic network conditions. And, finally, as do h three is also"
  },
  {
    "startTime": "00:42:00",
    "text": "coming up, we would also like to incorporate that in our measurement setup. So overall, the takeaway is that DoQ with h 30 RTT performs best across all web page and technology combinations even in the worst case scenario as we had seen. that it performs better compared to the other competing ones. And, yeah, that we have already said that the okay with h Three 0 RPT reduces our page load time by 1 third of a fixed line and behalf of a mobile compared to the existing web browsing scenarios. So our code is available in the GitHub repository. yeah, the link of which is given. Thank you. Thanks, Jayshree. That is right on time. So got plenty of time for questions and comments, and we have three people in the queue. And up first is then towards Yeah. Hi. Again, maybe if you want to on your video. I really appreciate it. Sorry. Yeah. Yeah. Hi. Hi. Alright. So first first, I wanted to say, as a matter of terminology, I think you're using DOQ in a way that doesn't quite match with the way that that that term has been used at the IETF. Yeah. Yeah. Right. Yeah. Yeah. I had a little bit of difficulty understanding what protocols you're referring to in these different slides I think I understand correctly, most of the time when you use DoQ here, I think you're referring to something that we've referred to as DNS over http3. Or if you need a short form, go 3, like, you you had in in, I think, slide 12. Oh, Am I correct? this one, So you're asking whether we are using boh3? So at the bottom, you have dough dough 3 here. dough 3 support, in the last line, Yeah. Is that the is that the same as what you mean by"
  },
  {
    "startTime": "00:44:03",
    "text": "DRQ plus No. So what we are trying to say is that So we are try so what our measurement setup right now says is that we are trying to serve the DNS resolution as well as the h three web content from the same server by putting both of them together on the same process. right now as part the measurement so that so that when the when the web content is also being requested, it happens with the same underlying quick connection, and a new quick connection is not required. So, currently, this is the concept. Okay. I believe that in that case, I I still think that you're you're telling me that you're using dough 3. for the for the DNS query. You're using DNS over HTTPS using HTTP 3 over a quick connection also used for the web HTTP 3. Yeah. Okay. Yeah. That is IETF terminology and not considered DNS over quick. DOQ is a separate technology which in which HT the HTTP layer is omitted. So just as a it will it will I would I would encourage you to to just adjust the terminology that you're using to describe because it's confusing for for some of us to to see these terms used in in in different ways Yeah. Yeah. Sure. Sure. I think my fundamental question is why would it ever be the case that or that the DNS server is colocated with the web content. So, yeah, that is all of course, very good question. So the idea is that so we thought of this in a this way that currently, the content delivery works, for example, Cloudflare. They are also serving so they have this web server as well as the h three server. They are serving it from the same measure network. So it could be a possibility. So this is a proposal where we are where we are looking into the future and that it could be a possibility that they serve both of it the same server. Okay."
  },
  {
    "startTime": "00:46:00",
    "text": "and And if you get a response back that tells you an IP address, that is not the same as that server. So I I've I've been I have a web connection open to some server. I get a response back. But if if a DNS request was necessary, it's because I don't know the location of the the content. If it comes back with a new IP address, shortly, I have to Definitely. Definitely. Yeah. start over with connection. Right? a fresh Okay. So so there's a a lot of people who wanna talk here. I'll just mentioned that if you haven't run across the resolverless DNS topic, which has been discussed here and there at the IOTF over the years and and also in other context I would encourage you to look at that literature, which has explored this problem space a bit, I would also want to encourage you to take since you're in Chromium, I'd recommend looking a a little bit deeper at why the why the doe connection and the web connections are actually partitioned so that they cannot share the socket pool, And that is connected to things like client identifying authentication, which is important for the web content, but but undesirable. for the DNS requests. Sure. I'll definitely look into it. Thanks. thank thanks, Ben. You cleared up a lot of things there. Next up, we have Mike Mike Bishop, Akamai. I think I came up with a lot of the same questions as to when this would actually be the case. Yeah. I had interpreted this as a hybrid protocol that could do DNS on some streams in h 3 on others. So, yes, please clarify. exactly what the nature of the protocol is in Yeah. So as I already mentioned, as this figure also shows that we are trying to quellies the DNS resolver as they and"
  },
  {
    "startTime": "00:48:03",
    "text": "well as the h three server into some into one infrastructure on the server side so that we can serve both the DNS resolution as well as the h three by using the same underlying connection. Right. So it's DNS on some streams and h 3 on others? Yeah. Okay. and and and in this scenario, how did you come to connect to the server in the first place? Is it con is it your configured DNS server? Is it web server you already happened to be talking to, So here, we are just doing it inside our Linux net network namespaces So what we are doing is so so we have the SCORE DNS tool which is giving us the IP address and then we are sharing with the Chromium browser, and then the Chromium makes makes the request to the h three server. The only difference in the measurement setup is that we for the time being, we have kept the core DNS as well as the h three server are the same process on the server side, and that's why we are able to reuse the same connection Alright. So I understand for experiment setup, you're just feeding the IP address in. But -- Yeah. -- the situation you're trying to simulate in the real world, Yeah. So that's with the current so the as I already mentioned, that this is a futuristic thing that we are trying to look at. given that CDNs already have they they have the servers. They are they are they they had the server support the same agent network. So maybe this kind of a coalescing would be possible with some infrastructural changes at their end. Okay. Thank you. Thanks, Mike. Up next, we have Anthony Somerset. Cool. Cool. Thanks. Alright. Spencer Hawkins, are you still in the queue?"
  },
  {
    "startTime": "00:50:05",
    "text": "Spencer Dalkins, I'm I apologize. I thought I had a moment to to stall. So I wanted to thank you for working on IETF Technologies that are still new enough that we keep changing the names. I really like I really like where you're headed, and the we we we We are doing other protocols over quick that We have the the you know, like, media over quick and RDP over quick that gonna have the same kind of questions about and Good. I I think you have an opportunity to do a lot more research in the space that would be very valuable to us Thank you. you so much. Thank Thanks. And we'll wrap up the queue with Eric. right, Eric, Kaniere Apple. Yeah. I I would echo that. Thank you for for the talk and and and doing some of this research, I think it's super interesting. One of the things that you mentioned around having the server do the thing and and to Mike and and Ben question. Once upon a time, we we did have a document that talked about the concept of kind of designating a resolver for a given domain or zone. And so even if I wouldn't necessarily want to have a generic resolver that I could talk to that would also accept HTTP requests for content perhaps for someone else, Yeah. Saying I am serving this content from this domain, I'm also would like I I and I can prove that I'm authoritative there. I also would like to to provide"
  },
  {
    "startTime": "00:52:01",
    "text": "better DNS privacy potentially than than you'd get from from your local resolver on your local network. So some of that is is a concept and is a thing. I think some of the questions around how do you partition and and who knows what about whom are are things that we'd want to probably answer. Yeah. Sure. Sure. We would definitely look into it. Yeah. Yeah. Yeah. But but Thank you for bringing this here. This is really cool. Yeah. Thanks so much. Alright. Thanks. Thanks to Jay Ezra for bringing this measurement experiment to us. Yeah. Please check also the chat. There's more there. Yes. Thank you. So -- Yeah. -- I've been trying to get in touch with Ali who's up next. Ali, are you with us? Hello? Can you hear me? Yeah. Yeah. Let let let me get your slides up here. And you hear me? Yep. We can hear you. I cannot hear you. I don't know why. We we can hear you. We're getting your slide there. Okay. And now you should be ready to go. I just pass slide control to you so you should have controls on your meet echo screen where you can the slides yourself. Ali, can you tell us how to pronounce your name? Yeah. Can you hear us? Hello? Hello? Hello? Okay. Ali, can you hear us? Yes. Yes. Okay. You're we're you're ready to go. Yeah. And don't see Should I share my screen or what? This I've shared your slides already. You should see them through Meadeco. and you should have slide control on your screen if you don't take control back and you just tell me when then it's the slides, but you should be able to see your initial screen right now. I can see my slide, but I cannot see my notes."
  },
  {
    "startTime": "00:54:00",
    "text": "Okay. No. You probably have to put your notes in a separate window then. Okay. me Or if you'd rather share your screen, I can stop this and you can Let share it yourself. Can I share my screen? Yeah. So request assure oh, yeah. You do. Thank you. Okay. It's loading up. Mhmm. let me go to the presentation with No. Okay. Now you can see my screen. Yep. You're ready to go. Okay. Okay. Hello, and thanks to join my talk. In the following minutes, I'm going to because, like, my recent study published in 2 1000 23, which was done as a collaboration It's my colleague, Srirani As a total of trials, it's subconsciously And as well as the study focusing on the cookings in particular, the rule of multibanders and today's were Before regarding to the land, I have to reserve the paper cocross, airplanes reduce 2 type of marketers. technical cookies are small piece of information stored on a major browser by Press servers using HTTP had once up, in Belgium includes them in the subsequent request for our best particular domains. are 2 types of cookers based on the way First participant, which are those that are by the website that the user is currently using it and the 3rd party could use because then that was built by the the models. Other than the actual device in the Mhmm. Sorry to interrupt you Your audio quality isn't that great. I'm not sure if you can get closer to your mic or anything like that. Okay."
  },
  {
    "startTime": "00:56:08",
    "text": "I was with her. Of course. you hear me? Can Now it's better. Thank you. Okay. So yeah. And as an intrinsic part of the web technologies, cookies offer valuable advantages, like enhancing the intelligence and coherence of HTTP reports. For instance, they in a storing shopping can be used cost and optimizing other advertisement. However, the extent of their usefulness depends on how cookie ColaTunes, and actually do behind the phone. In particular, cookies can be employed to track users and gather information about their highly. The personal business such as browsing behavior and user. He's undoubtedly raised privacy concern. To address this issue, regulations, Nish plus sorry? Sorry to interrupt again. You audio is changing like, up to 18 multiple modes. It's sometimes good, and then we can't understand So -- Okay. Let me let me just change my value some sounds good right now. Whatever you're doing right now is good. it's good now. Yeah. We can hear you now. Okay. It it just sometimes kinda overloads, and we can't hear it. it's good right now. Okay. saw where I was. So I just this issue regulator put some laws and other things for the good potential privacy risk. postbank is 1 of the loose ends That's the problem. Android in this TV player. And first of all, since May 2018, it's a new generator and to empower individuals by granting them greater control and rise order profoundly. to comply with the DVR website located within you"
  },
  {
    "startTime": "00:58:00",
    "text": "will require to obtain user consent for their collection and personal service except when certificate or receiving necessary to provide their request service. Similarly, The CCP is a state law. in California that took effect in January 2000 many times in Azure as GDPR, this is still an issue. save all the funds here. California is In response to this regulation published around in some of the cookie banners on their website is currently a simple metric to compliance with the As a result of these changes, there were question routes are how many of website show the banger, Does the banner have an experience that stuff with your partner? Do they really respect the user choices afterwards 92. What type of difference is with this another call down to regard to their geographical location or browsing easily. Couple of reasons, I was trying to answer some of the questions. However, both of them doesn't consider different aspect that and cut the result. in addition, the measurement are conducted in a manual or scan the automated link, which makes it follow inconsible to if you just we talk to another time, the original configuration. or for cousins or channels. And I'm sorry if I make you. If I'll tries to tackle this problem, they want to use pre vaccine tool to do auto auto item, maybe a month. Claspect uses a list of 2000 260 possible words that have been manually handled, before simply search for them in in I assume it's got your mix my decreased accuracy and has to escape. Plus, it doesn't consider a minute. plug the start, I'm going to introduce the automated tool to join us through the landscape from the ground. But first, let's start with all the methodology we use for the product before collecting the data. Any seven points to trial,"
  },
  {
    "startTime": "01:00:03",
    "text": "it's it opens the domain from the target list, then it start to connect more cookies and then start to a a tool to detect the banner and then insert it a banner And during this during start, it keeps collecting the cookies call it into this working some amount of time. And this loop will be able to check for all your resolution and follow-up with them to stay this long. The whole process can be developed in and include separate parts. First part is related to data tools and the second part is possible to detect themselves in the panels on your That part is already being implemented. in the open source, privacy measurement, framework name, and the the right side, using selenium browser automation, we have multiple which can potentially track with the banners, panel fluke. and so into the open loop 2 parts, you have the design system to collect the data and completed this What the exact amount? balance sheet. First, it needs to detect the boundary in a given website. After detection, it should be able to interact with the balance and change on the option we want. For example, if you want to investigate the behavior of the website after opting out, the it will be able to click on the music button. Of course, if such a back. Now let's see how it's done in the quote. For answering to for So we got the banner, I think we need to answer And we need to know how is the structure update still working. Typically panels are a separate dinosaur of the body type, to which are included with the cookies related text. And the banner detection algorithm used to find field test the funeral sequentially versus the annual it is an odd because cooking related work. Like, cooking as as as that supposedly on these type of things. And second is the first one. can allow me the CFF fixed"
  },
  {
    "startTime": "01:02:00",
    "text": "position fixed properties. And given CSS speaks position property with quick heuristic because, as you know, doctor Dana are fixed, and we don't. And after finding these 2, you know, it travels from Is it so that's the path from the First note, you know, to end or to find out tomorrow. optimal node is the minimal size node in the path that encompass all the visible issue So that's it. Now we have the token amount, which is the root and the main representative of the banner. They run this detection process will jam with top 10, So a 10 10 hook of 1000. and check them sprint on manually and we saw it's worth it. mountainland Okay. now we have the panel to So we can talk a bit more as easy. To do this, we just need to find a button that leads their instruction model 1. For example, For example, the Outlook looks for word inside the panel that's multiple are related to except, right, except that we confirm on these kind of things. And same things happen for reject, except for reject, is checks for possible reject by clicking on the setting option of the mouse. This is because install some banners show reject option in the setting of the nuts. Because it is three words set for top and 13 languages. That's all the most important ones in toptop 10,000 limit. run the measurement funds 8 of manual fund, spring across 6 months now. plus them are in few countries and 1 California, so we can see the possible effect of GDPR and CCPA. for the toilet leads, we use 10,000 plus In total, we are able to detect banners in around 47% of the first time in new countries and around 13 inbound new countries."
  },
  {
    "startTime": "01:04:03",
    "text": "you definitely show that you you guys are taking on the behavior of the website, answering better. Although we're looking at the blue part of the line, we can see that Premier House of the work then all my different don't have any easy to our out option, which might be interpreted as not completely compliant with GDP. Next, let's choose the box plot for a number of cookies for different inside and most you. Yeah. You can see how inserting with Nanook and substantially insert cookie distribution. For example, After access, you can download another 1st party tuition fees by more than onetime and the number of third party in your face but 5.5 times on average. As for tracking the cookies, the average in 3 from 0 to 7, which show the stuff substantial Furthermore, we saw You see that the number of tracking tools is near 0 and then it is not accepted. which is the case affecting the PDP has to reduce and wants to check As per reject, you can see for both 1st party and third party cookies, the others increase That was my house. by one. This might be attributed to proteins the the the cookies because to save the state of the cookie band. And for tracking, we've seen no difference between cookie before and after Okay. Yep. is in comparison to the This block, you could easily ask for the extraction of website if you to the number of cookies is sent. it shows that websites that less people you countries, when compared with one new company. For example, we can still np around the section of the website, it's a fewer than elephant in EU. Well, I've seen the person 70 cookies for my new And regarding training and cookies, we see that 60% of websites is less than 1 while around 6. While this number is around 6 and 9 is"
  },
  {
    "startTime": "01:06:04",
    "text": "And the this figure shows that different group can collaborate number of cookies, my business, and website, it says this might be their agent, and they only track road, for 3 months. Generally, when we're here, For the majority of website, these are the same and should be same number of cookies, it's obvious And you guys are gonna remove that, the number of cookies. some websites that more cookies and web service programs, some state, or or roaming. We detect the similar number of panel on the website when visit is from desktop and Furthermore, we explore the prevalence of constant management platform or PMPs, on the web, same to our platform that upper cookie concerns I'm going as a service, that is what they can consume and ready to use. Yes. configurable banner instead of uploading their own cookie panel solution. In this regard, we can't see the distribution of topic me on this or the top line list. I've installed this Come 16,000,000 at the end around as 70 of the whole market share. Based on the figure, I don't know more than 10 personal birth of this PMP. And in terms of when you can see that top 50 websites won't see don't give a CMP something in the valley. if my because of the fact that the very tough website. prefer to my have a complete control over their clients data and all they want to handle. Before conclusion, I would like to talk a bit of my personal insight on around took his landing statement, particularly of how the user can find or collect It says that Daniel was just merely a straightway of confirming with loss, which are kind of fast upgrade the low price publisher to always find However, from an engineering perspective, they may not see the most"
  },
  {
    "startTime": "01:08:00",
    "text": "lactimol, use a family solution. typically, considered as annoying, which means too much effort to comprehend and inside properly, as you can see, there are many variation of this in the web Moreover, I have the permit the previous research top, many of channels in the dark as well as to Caller's model to click on the accept. Not to mention cookie oils that add to monthly ad subscription as an alternative to not accept. Please all and all make user decision completely biased and subjective which are based on time and mood and not the overrun It does good as their house prediction, I would say the cookie banners as they are now, have no future in the Internet repropria. company if big companies finally agree on, we end up kitchen do send solution, which are supported in the application layer. Currently, growth from progress in this area, for example, Google privacy time works and to replace 3rd party cookies with a more private be conscious, be conscious, approach, allowing user to manage their interest and grouping them and record based on the similar browsing patterns. And I'm personally, as a user in as a internal user, For me, the best solution The best possible solution is the one which I would be able to opt out by 1 simple kick, for the entire sales and experience. To summarize, in this letter, we're investigating that there. different path to another tool is, like, build up the like, the location of the user and their inspection with the panels, also introduced or 2 channels to run the measurement in a automotive. Right?"
  },
  {
    "startTime": "01:10:01",
    "text": "And for more details regarding biophilia analysis, please check out or paper, that you can find more about in terms of different factors on web cookies. Like, PCPA, landing versus inner pages, and also, consistency analysis. Finally, you can find the data and analysis script, which are Available in Valley 3 kit further through. Thank you. Thanks for joining us. Regrettably, the audio didn't did not work very well during your presentation. So we don't have anyone in the queue for quite or comments right now. If there is someone with a you're welcome to join right now. But otherwise, I think we're gonna go on, and we just couldn't do about getting the audio to be better, and we don't when we're enable, isolate why. So thank thanks for joining us. Folks Thanks, Lauren. You can contact him directly Yeah. by email or or use the chat. Thank you. Ranya, we have her queued up to share her slides I just did right now. And, Ronia, you should have local control of the slide deck once that's done. And you're welcome to turn on your video it's nice for the participants here to be able to see what you're talking, if you don't mind. There we go. Awesome. Alright. Is Opio working fine? Can you just say something again? Can you hear me alright? Yep. It sounds good. Alright. You're ready to go, Ron. You you got up to 15 minutes. Alright. Perfect. Thank you. Well, thank you for having me. And today, I will be present saying our paper on user awareness and behaviors concerning encrypted DNS settings and web browsers, and this work has been published at usynixsecurity 2023."
  },
  {
    "startTime": "01:12:03",
    "text": "So the encrypted DNS ecosystem is increasingly becoming a place where various Internet stakeholders try to buy for control through technical protocol design, and really these design choices affect market consolidation and power, corporate visibility into and control user data and user privacy on the Internet. And at the end of the day, humans are impacted the most by these decisions which is really why it's essential to study their awareness and knowledge of these settings. Recently, many major browsers and operating system vendors have deployed encrypted DNS functionality and particular configurations of the settings are often enabled by default. encrypted DNS settings have implications for performance and privacy. An example of this is that until recently, Firefox sent all the user's DNS queries to cloudflare through their default DNS setting. And, really, this creates privacy vulnerabilities since one resolver sits on heaps user data. And this recent centralization really just causes significant issues power dynamics, which is what we sought to better understand. So our study confirmed that most users are unaware of these new technologies and the subsequent implications. So some background is that the domain name says also known as DNS, maps human readable names to IP addresses, And historically, these DNS queries are conventionally unencrypted. which really just leaves queries and responses vulnerable to eavesdropping and manipulation. However, with encrypted DNS protocols such as DNS, over TLS, known as DOT, and DNS over https, known as DOH, as many know. have emerged to encrypt these queries that they're no longer vulnerable to third parties. So in our study, we studied encrypted DNS settings in Brave, Chrome, Edge, Firefox, Opera, and the Android mobile operating system."
  },
  {
    "startTime": "01:14:08",
    "text": "So say for instance, after hearing about what encrypted DNS is say that Sally wants to try enabling encrypted DNS after just learning more about it. This is what she would do in her Chrome interface. First, she'd navigate to the settings tab Then she click on privacy and security. Next, let's scroll down and click on the security tab. Then she's find the use secure DNS setting, which is how they encrypted DNS, and she could toggle back and forth to enable it or to disable it. Then say she wants to enter a custom resolver like she wants to use Google's resolver, stacy typesandgoogle.com as a custom resolver. Here, we see that she gets an error because it's incorrectly formatted. Then she enters it in the correct format, which is DNS.google/dnsquarry, and then it has full functionality. There is all for So as this demonstrates, The settings page is not easily accessible and very little explanation about the setting and resolvers are actually given, which significantly affects the users understanding of the resolvers and their interaction with it and the overall encrypted DNS page. So brave Chrome and Edge all operate similarly and that they refer to encrypted DNS as secure DNS. If a user makes no changes, the browser uses the user's default DNS resolver. and it falls back to sending unencrypted DNS queries if the encrypted DNS resolvers on available. On the other hand, Firefox and Opera refer to encrypted DNS as DNS over https. And Firefox's default behavior is to use encrypted DNS with cloudflare as a default resolver."
  },
  {
    "startTime": "01:16:00",
    "text": "and interestingly, Opera Disables encrypted DNS by default. but one encrypted DNS is enabled, Opera uses Cloudflare as their default resolver. And another interesting point about opera is that only Opera explicitly mentions that DOE uses 3rd party services. Mozilla operates a trusted recursive resolver program and that they ensure the DOE providers recommended by Firefox, best protect privacy by not over collecting and sharing data. also demonstrated here with the dropdowns that all the browsers provide. Some of the resolvers shown in these interfaces offer additional features such as walking malware or adult content. So the research questions that we explored are are users aware of encrypted DNS settings in browsers and devices, what encrypted DNS settings the users have enabled One shown encrypted DNS settings for different browsers, which settings users select, and one of the technical aspects of these systems are explained to users and they really just have a broader standing of how encrypted DNS works and what the DNS is How do their choices of these settings change? So our study consisted of a 2 part survey, and we recruited participants via prolific, and they were compensated for their time. In the initial screening survey, we ask 800 participants about their use of different popular web browsers, And based on their responses to these questions, we assign participants to sub groups determined by the browsers that they reported using at least once a week. After assigning participants to subgroup according to browser, We invited up to 50 participants from each group to participate in the 2nd longer survey which was our main survey. So our main survey had 184 participants, and the structure of the main survey was as follows. So first, we asked participants if they had heard of the DNS"
  },
  {
    "startTime": "01:18:02",
    "text": "prior to the study and if they knew what DNS did, really just trying to gauge their understanding of prior to the survey. Next, we randomly assign participants to anonymize versions of actual DNS settings that can be found in browsers. But rather than simply showing them a screenshot of their assigned interface or multiple choice questions, that kind of resembled an interface. we embedded in our interactive versions of the browsers directly into the surveys that participants could really interact with them, and it would simulate what their experience in a browser would be like. Next, we ask participants questions about the rationale for choosing different options. and really just wanted to understand why they chose the settings that they did. and we showed participants an interactive version of the private DNS setting on Android as well, and once again ask them about the settings that they would choose. here and represents a number of participants that were assigned to each interface. Next, we describe the DNS, and we check participants understanding through multiple choice question about the basic functionality of DNS, and really just explained in more detail how the specific settings work. And given this additional information, we ask their thoughts on the setting. After that, we showed participants interactive in encrypted DNS settings screen that they saw earlier in the survey and ask them once again to decide what options they would choose and why. And the reason that we did this was that it allowed us to really understand how respondents changed their settings after gaining a high level understanding of what encrypted DNS does and the benefits that it provides. Finally, we ask participants to report on the current settings in a browser that they regularly use. So we really just ask about their experience with navigating these settings and enabling or disabling them. the browser that we assign participants to look at dependent on their responses to the screening survey."
  },
  {
    "startTime": "01:20:00",
    "text": "So if for some reason a participant did not have access to the browser subgroup they were assigned to from the screening survey, we reassigned the participant to another browser that they did have access to. once again, and over here, relates the number of participants assigned to each browser. The first question that we look at in our results is are users aware of encrypted DNS settings. Here we find that 73% of all purchase pence reported having heard of DNS prior to the survey. Off the participants are supported having heard of DNS 59.9% had heard of encrypted DNS. And one participant who claims to know the purpose of DNS were asked to describe it in their own words, Fewer than half mentioned the DNS as being responsible for translating domain names to IP addresses. refine this interesting because participants had various misconceptions of its purpose, including that it identifies computers on a network or connects devices to the Internet. which we all know. It's not really what the DNS assumption is. Next, we find that regarding stakeholders in the encrypted DNS ecosystem, Most participants had heard of Google and Comcast, but were less familiar with Cloudflare next DNS and quadrime. So what encrypted DNS settings do users have enabled? We find that 79.4% of participants had the default settings enrolled in their browsers, which typically meant that participants' browsers were opportunistically their DNS queries. We also found that no participants reported correctly configuring a custom DNS resolver in their browser. Participants entered queries such as mcaffeeorgoogle.comor1.1 point 1, and they weren't correctly formatted as needed their queries to actually be encrypted."
  },
  {
    "startTime": "01:22:05",
    "text": "So when shown encrypted DNS settings for different browsers, which settings to users select, One shown the anonymized browser interfaces for encrypted DNS, 71.7% of participants continued to use a default settings shown to them. And this finding really just illustrates the importance of how browsers and mobile service providers configure their defaults. One participants did enable encrypted DNS, but did not go with the default setting, they were much more likely to choose a resolver that was it in the setting rather than just specifying a custom resolver and as earlier explained, we saw the issues with the customer solvers. So in choosing a resolver that was listed in the settings page, Cloudflare was by far the most popular also found a strong connection between the name of the setting and perceived impact We And as discussed earlier, the two names of settings and browsers are secure DNS and DNS over https. With secure DNS, we found that participants associate your DNS with safety and security. So secure was an they thought that the setting had something to do with security. here one participant said, the wording makes it sound like enabling DNS would make my browser more here. Here. Another said, I don't know a lot about it, but it seems like an extra step of protection. really just associating the setting with security, and that had a late a major factor into how they either enabled or disabled encrypted DNS. Next, for the DNS over https, setting, we found that interfaces that label the setting with the technical name DNS over https caused additional confusion in some participants, because instead of interpreting the name as meaning DNS using the HTTP TPS protocol, they interpret interpreted DOH as meaning using DNS instead of HTTPS. And of the participants who saw the Firefox or Opera interfaces that label the setting in this way,"
  },
  {
    "startTime": "01:24:04",
    "text": "10.6 percent mentioned an incorrect interpretation of the study name, as part of their reason for choosing the setting option that they did, which we found was quite significant. Here, one participant said I have no earthly idea what DNS is while at least have a passing familiarity with https. And another said, from the little I know, I believe that https is more secure than DNS, really just highlighting the same confusion. really, the impact of all of this is that it emphasizes the issues of technical jargon and the necessity of explaining concepts to users. Right? is the audio working fine. Alright. Thank you. Apologies. Okay. Next, I found that when technical, again, aspects of these systems are explained to users how do their choices of settings change? So after being provided a description what the DNS is used for and what encrypted DNS does 37.7 percent of participants chose to modify their settings in some way. and 30 participants reverse their choice with an equal number choosing to disable the setting had previously enabled and enabling the setting that they had previously disabled. which was quite a significant number and really just showed how participants change their interaction after receiving greater knowledge of what encrypted DNS is and how the whole process functions as a whole. So based off of our findings, we make the following recommendations. First, we recommend to provide a basic primer on DNS function and the privacy risk really just to explain DNS function, privacy risks and the trade offs associated with each setting. Next our results indicated that while participants understood the setting after it was"
  },
  {
    "startTime": "01:26:03",
    "text": "drive to them, they still struggle to understand the differences between the recursive resolvers. So we recommend providing policy privacy policies for these resolvers really to just lead to more informed choices and to help users understand the differences between the recursive resolvers. Be thoughtful of the technical protocol terminology. Again, this is really highlighted with the TNS over https name confusion that we found and participants didn't understand So it's setting correctly, and that really impacted the ways that they interacted with it. and finally, provide users with the necessary format to select a custom resolver adding instructions, guidelines, and warnings for more clarity. So in summary, We study user understanding and awareness of encrypted DNS settings in 4 browsers in the Android operating system. and we suggest practical recommendations for designers and policymakers based off of our results. We indicate that work is needed in several areas including to improve user awareness about the privacy implications of DNS, to provide users with information to better understand the implications how encrypted DNS is configured and to design setting interfaces make these options intuitive for users to customize. Thank you for your time, and I'm open to any questions. thanks a lot, Ranya. We're running a little short on time for the meeting, so we got three people in the queue. and we'll take Ted Hardy first. Ted Hardy speaking. Thank you very much for the for the work and for bringing it here. Very interesting indeed. two quick questions. One in, if I understand your methodology correctly, you did random assignments to to the online browsers that enable the interactivity. May I ask why you did that rather than by picking the the people who had self selected particular browsers because"
  },
  {
    "startTime": "01:28:04",
    "text": "there is at least an impressionistic sort of anecdotal association between people who have already selected non default browsers and security consciousness and it would have been Perhaps a different question had you done it that way. And I kind of wonder why you had made that choice. Sure. Definitely. So I completely understand that. What we did in our methodology was when we showed anonymized browser interfaces, That's when we randomly assigned users to them, and that was really so that we could an equal amount of people that were looking at each of the interfaces. rather than we found that many people by ball use, they Chrome the most, and they use We found that brave and opera, 90 participants weren't as familiar with those interfaces. So really by just randomizing participant assignment to these browsers, we could have a greater and more equal amount of people that were looking at each one. Kind of just to spread it out evenly, and the reason that it was anonymized was so that participants did not know at all, which browser they were actually looking at, whether it was a browser that they were familiar with, or it was a browser that they had never looked at. for. Thank you very much for that explanation. The second question in your discussion of future work or or potential adjustments for this, were there particular pieces of data that you felt sharing with the users gave them the best chance of deciding whether to change the settings or not, or we're using a script so that every time you were presenting it in exactly the same way, We were using a script to show like, every Every participant receives the same explanation, I think that would be a very in other parts of the survey to kind of show different explanations and see how participants' choices of settings changes based off of that. But what we did find, we asked participants what they would like to or what they would"
  },
  {
    "startTime": "01:30:01",
    "text": "like to know more about encrypted DNS when making their choices We found that a lot of participants wanted just more explanations that there wasn't as much technical jargon and that it was easier to understand They didn't have a computer science or networking background, and they wanted pros and cons listed for each resolver setting and just the encrypted DNS setting as a whole. Thank you very much. Thank you. Ben Schwartz, Thanks for thanks for doing this this very deep analysis, I really think this is fantastic, and I hope the browser vendors are paying close attention. I do believe that the chromium interface does provide a link to the privacy policy. So all of the resolvers in the drop down sub probably some of the other browsers do as well. So I think that's a great recommendation, but it is partly already being followed. As a if you're interested in future work in this area, I would encourage you to look n in different markets where the role and purpose of encrypted DNS may feel a little different For example, I suspect that if you ran a survey like this in Indonesia, you would get very different. kinds of answers, then you get from one run-in the US. Thank you. Definitely. Thank you. I think that would be an appropriate next step. West Hertica, USCISI, and the ICANN board speaking for neither of them, but myself, Excellent work. Thank you very much. I I we need more studies like this to figure out know, how well user understands, you know, security settings in particular. One of the my wondering well, I was listening to you is in the education section, did you talk to them about the differences in potentially giving their data to one large company that, you know, is a is a DNS provider versus giving it to their ISP, which may be a different large company, you know, and and and providing them different services. I I could even see this leading to different choices depending whether it was a mobile device they're wandering around with or whether"
  },
  {
    "startTime": "01:32:00",
    "text": "were at home and maybe trusted their ISP or Definitely. We actually did in our survey, and something interesting was we asked Yeah. participants, like, if they would rather their data get into a smaller company or to their ISP, and we if you would are more interested in this. I do recommend reading our paper. We have a sec where we analyze how users decide whether they would what where they wanna show their data and whether they want a larger company or a smaller company to see it. So that is further in our picture. Alright. Thanks so much for joining us around here. That was interesting. We're gonna have Rob Beverly up next. I'm gonna share his slides right now. We're running we had about 28 minutes left. So the final two presenters If you can, we're gonna have 15 minutes for each of you with the questions with any incumbents within those 15 minutes. Rob, you should have control now. your slot your first slide's up. Alright. Great. Thanks, Dave. Can everyone hear me? So hi, everyone. My name is Rob Beverly. Thank you for having me. This is work done primarily with one of my former students, Eric Rye, who's now at University of Maryland. And I'm gonna cheat a little bit here and tell you about two pieces of work One paper that appeared at IMC and 1 that appeared at security and privacy, because they both kind of rely on the same trick. And so I'm subtitling this, what we did with with millions of EUI 64ipv6cpeaddresses. So let me tell you what what that means. So this comes from sort of a long trajectory of work we've been engaged in with doing active ipv6 measurement atinternetscale. some of this, you know, dating back several years even work that I did with Dave and and others. And what we've been doing is uncovering some of the behavior of ipv6 deployed in the wild"
  },
  {
    "startTime": "01:34:04",
    "text": "So things like how ipv6 addressing is done how ipv6 allocations are done and even some interesting things like prefix rotation that I will tell you about in a moment here. The end result of this and sort of the punch line that folks may or may not be surprised by, at this point, is that we discover, you know, order of 100 of 1000000 of of CPE customer premise equipment that that use this legacy form of ipv6addressing. I'll tell you all about that. And then in this talk I really wanna talk about, like, the the sort of implications And, again, the two implications here are prefix tracking and street level geolocation via ipv6. And then I'm also gonna say a few words about some remediation efforts because we've made some some cool progress in that area. So just so everyone's sort of on the same page and this is a bit of a simplification for those of you who are x efforts in b 6. But the mental model, I think, useful to have is to remember that at least for residential deployments as opposed to the typical I IPV 4 residential deployment in v Six there's a a subnet between the provider's router and the the CPE, the home router in the in the house, And then there's a separate distinct subnet that's behind that CPE router on the other side. This is something we've been calling the periphery for a while. One of the interesting measurement implications here is that We don't necessarily require a responsive target within the customer subnet to discover something about the CPE router, we can send a probe into the customer subnet, which will elicit a response from the CPE router itself in many cases."
  },
  {
    "startTime": "01:36:04",
    "text": "This allows us to do things like binding the customer's subnet to a particular CPE device that's discovered. And so we've been able to do some cool stuff with this. down in the the lower left here are some visualizations of how different providers are allocating subnet to customers. So here's examples of, like, slash 56. This is just a a graphical way of looking at the 7th and 8th byte of the prefix that's allocated and you can see very distinct patterns for distinct providers. So that's all background. One of the things that I'm sure, again, folks in this room are very cognizant of is that RFCs have, you know, a long avoid lifetime. One of the questions you might ask is when we do these IPV 6 measurement studies at scale, you know, what are we finding in terms of the responsive addresses from the CPE And the answer is there's all kinds of things you might expect static, DHCPV 6, of course, privacy extensions, and there's lots of RFCs for that. And in addition, there's this legacy mode of addressing. Again, a bit of simple application, and and there's a bunch of additional RSCs for more but the lower 64 bits can be formed, deterministic basically from the MAC address of the actual interface, and there's RFCs on this. This has been well known for a very long time, and there's even more modern RFCs that talk about the privacy implications of of this. And so, you know, you might ask well, you know, these this has been known for for, you know, more than a decade or and so what's the status of things And the status of things, the the punch line here is that, yes, all modern operating systems use ipv6 privacy extensions, but there's, you know, again, as I"
  },
  {
    "startTime": "01:38:03",
    "text": "said at the beginning, order millions of CPE, home router type things that are still using UI 64. So there's two pieces of work that we've written to exploit assistant identifiers. So the first one that I wanted to tell you about that we discovered is is ipv6 prefix rotation. This is something that surprised us initially, the basic idea here is we know, for instance, that the host might be implementing privacy extensions and the lower order 64 bits of the host, may be changing all the time. In addition, however, providers are actually rotating within a particular pool of addresses the assigned subnets, both the point to point subnet and the host subnet. So these things can change over time. They can change on the on time scales of you know, an hour. They can change on timescales of a day. it just depends. And so one of the things that we did in this IMC piece of work from a year or 2 ago, was to try to get a handle on whether we could attack this by exploiting these EUI 64 addresses. And, again, the the high order bit here is that in these prefix rotating providers of which there's you know, hundreds of these across the world. We we find about 9,000,000 CPE devices using EUIC 64 addresses. So what does that allow us to do. So this allows us to do to actually reduce our search space so that we can try to actually track individual nodes that have both their upper bits, their prefix spending as well as their lower bits spinning. So the idea here and this is sort of a a time lapse of some of our imagined adversary, and we"
  },
  {
    "startTime": "01:40:03",
    "text": "we play the role of the adversary here. We're we're sending probes into the into the network, and we we may have misses. But when we have a a hit for instance, we're gonna see this we're gonna not necessarily see the end host we're gonna see this CPE address. Right? And as this goes, on over time, the thing that you're going to note Sorry. Let me just move this forward a few slides. The thing you'll note here is that, yes, while the prefix are spinning, and it looks like the addresses are quite random for the end hosts. indeed for the CPE because they're using EUIC 64 for the lower order bits, we can, again, do this correlation extra guys. This allows us to essentially track some of these devices. So in this particular piece of of work, we showed how we could, at Internet scale, do things like exploiting these to track users cross both prefix and their their host changes. We did courtesy being this envisioned adversary, and we show that this extends across order of of 100 of providers. So the next piece of of work that's directly related to this is the most recent piece of work. This is the ipvcu work. this appeared at at IEEE security and privacy quite recently. And the idea here again was to to take these EUI 64, but now in essence to do a correlation, a large scale data fusion with Wi Fi geolocation databases. And punchline here is that there's millions of residential homes that are exposing their exact physical street level location via ipv6. So let me tell you how this works"
  },
  {
    "startTime": "01:42:00",
    "text": "and give you a bit of intuition. So I've already demonstrated hope that one at remotely without any privileged access can obtain just by sending especially spend it sending a bunch of probes into the network and and reducing your search space You can obtain both the WAN side MAC address of the the CPE And it turns out that many of these home routers also have Wi Fi. They're an all in one device. And so what we do then is to say, okay. Can we do a prediction that predicts the offset between the WAN MAC address and the wireless MAC address. Right? Because often these are system on a chip and they come from the same So that's the intuition here. If that's possible that we can predict the is offset. And in this example, you'll see, for instance, that the the the WAN MAC address and the the B SIDS for the Wi Fi all come from the same OUI, If we can predict that offset, sometimes it's as simple as, you know, plus 1-1, Then we could, in theory, query war driving databases to get the the exact geolocation. And this is what we did I'll just point out in the interest of time, you know, please read the paper for the details. This is nontrivial because we have missing data points and and it's the case that it's not so simple as being just The MAC address is, you know, plus 1 or -1 from the WAN address So here's an example where and this we we've actually purchased devices some of these all in one home routers, we purchased them to get ground truth test our algorithms. And it's a scope based algorithm, and there's a bunch of details in the paper, but here's an example where naively. you know, this this MAC address of this WAN address of this one all in one router on the right here is one off from the MAC address of this center all in one"
  },
  {
    "startTime": "01:44:01",
    "text": "home router. But, however, these are 2 different device And in this in this piece of ground truth, this is a real example. The true offset between the same device as WAN and the same devices WIfi was either +5 plus 6, again, depending on whether it's the 2.4 or the 5 gigahertz hurts that we actually discover in the Wi Fi draft driving database. There's a ton of limitations here. Right? Some CPE, of course, don't use EUI 64. Sometimes they don't respond to our probes. Of course, you know the besids. that are in the war driving databases may not be there. so on and so forth. But what were we able to to actually do So by doing this large scale data fusion, and combining WAN MAC addresses that are discovered via V Six and via this sort of, you know, abstraction layer violation. where these layer 2 things are percolating up into layer 3, by come doing this data fusion with with B SIDS, and with our our offset algorithm, we geolocated 12000000 unique devices out of 60,000,000 a 147 countries and over a 1000 unique OUIs. there's again, many more details in the paper. And you might say well. and this is, you know, very very much directed at sort of an IETF audience well, you know, let's just not use EUI 64, and that's totally true. But one of the things we also show in this paper is geolocation by association. And so even the non EUI 64 devices or vulnerable to geolocation because the other EU I 64 devices are hooked up to the same ISP router, which allows us just because of the physical medium constraints, to do geolocation of non EUI 64 devices, and and there's a bunch of details about that. in the paper."
  },
  {
    "startTime": "01:46:01",
    "text": "So the cool part about this, we did We did engage National Scale United States Residential Service Provider. both for validation of our technique then they subsequently did remediation. So they actually changed the behavior of their entire network to prevent this. We also talk to multiple vendors. of CPE that we're doing this. We had mixed results some vendors actually told us we were wrong that that they weren't doing EUI 64 when we could absolutely tell them they were, and they just refused to believe us. Fritz OS is a one example of a provider that did actually change their software stack to to remediate this. So with with that, I think I've I've blown through the the presentation, and I I definitely invite you to to check out the papers for for more details, and I am happy to take any questions Yeah. Alright. I appreciate it, Rob. Bob Hindon's up first, and then we have Ruie in there and might have time for one bar depending on how long these guys take. Hi. Bob Hindon. So you left out something important. So the iTap has deprecated the use of EUI 64. It's not just not recommended, it says we've done a lot of we realized this was a problem years ago. It's been deprecated. and most I think the vast majority of end systems phones max, PCs, use, the random interface IDs. They don't do this anymore. So you didn't make that very clear. It's not like there's a big project here for the IETF left to do. I certainly agree that the CPE, you know, the ISP provided devices need to fix this that was not aware that it was this bad But but work you're doing I think is very good. But for the IETF. I mean, the"
  },
  {
    "startTime": "01:48:02",
    "text": "we've we understand this problem, and I think we fixed it. some years ago now. So that shouldn't be an issue going forward. I wanna say maybe people are in the room or watching the talk. You know? It's not that the IETF has to do some standards work but we can still help, I think. Yeah. Thanks thanks for the comment. I I maybe I didn't make that clear enough I mean, this has absolutely been known as a problem for a very long time. I think the the one point I might make, however, is you know, these types of devices They don't really get updated frequently. Like, what the last when was the last time that, you know, my parents updated the firmware on their home route? Right? So there's a really long tail here. And I just you know, I think the point of this work was to, you know, sort of illuminate and part of my point here was to illuminate the extent the problem. I mean, again, we're talking 100100 of millions of these things are in the wild, but thank you. Hi, Rupaul. This is a great presentation and I as wondering how you we're able to gauge the accuracy of your results especially when you're talking about street level Great great question. There's so there's a ton of sort of academic level details and rigor. in the paper. The short answer is we crowdsource first with friends and family for a very small dataset But then what we did is we engaged with this large US national secure service provider, and we actually got ground truth from them. and did our our validation there. So you can you can check out the paper. Obviously, we make mistakes. But, you know, the accuracy is is quite high for for that case."
  },
  {
    "startTime": "01:50:03",
    "text": "Hi, Tim Chiang. So, yeah, what Bob said, it's not actually deprecated. I rrc8064 says, Those should not use EOI Sixty fours and should use, I think, 7217, the per prefix, ident Jeff. identify is generated per prefix. that would also prevent the issue you saw with stability across when the home network is re numbered because they get a different identified because it's a different befit. So It's it's it's the no. It doesn't say must not. That's part of the issue. The other is vendors just don't follow that. It's a lot as well. Noted. Thank you. Alright. Thanks a lot, Rob. A lot of interest in it. Follow-up with Rob outside of the meeting if you can. We have LAE queued up, and I'm gonna share the slides grant your slides right now. And as soon as you see that, first slide up you're ready to go. We've got a little over 10 minutes. Can't hear you like you. Right? me know? Not getting your not getting your audio. How about now? Yep. There you go. Okay. Great. I had to press unmute, it turns out. Okay. So hello. Thank you for having me. Thank you for sticking around. for the last spot. My name is Leah Olson. I'm a presenting Measurement Lab. I'm the lead data scientist for the project. we are a fiscally sponsored project of code for Science And Society. If you're not familiar already, measurement labs. Mission is to measure the Internet, save the data and make it universally accessible and useful. This is some that we've been trying to do since 2008, and we consider this a group effort. alongside all of you in the room as well as many others, and I'll talk more a bit a bit more about that. Really, my goal today is to just open up the conversation around how Map RG specifically, but as well as the IETF and mLab could be more complementary and work together."
  },
  {
    "startTime": "01:52:00",
    "text": "collaborate more. I think there's a lot of opportunities for scaling each other's work. enlove was created and in an effort to support Internet measurement at scale. And so I think it would be interesting to think about how to scale some of even the ideas that we heard today. So just talking through those collaboration points, or to do that, I'll set the stage by talking through just what mLab was created to do and how we're doing that today and then, again, talk more bow, bow, bow, bow, bow, what we could see perhaps in the future. So why was n live created? As I mentioned, we were created in 2008 to address sort of 4 primary challenges that we're seeing as barriers Internet research at the time. Please pretend that says 1, 2, 3, so it seems like I can count. But the first one, either way, is thinking about or at the time, we were thinking about how to measure the Internet at scale. So I think this is is still often the case, but definitely at the time, there were many instances of, you know, someone writing an experiment or or a tool for an experiment or collecting interesting data or coming up with a new metric that people wanted to standardize, but you know, it was written for a paper and then never, you know, went anywhere after that. No one was able to collect that data anymore until 5 years later someone discovered it, and it went through sort of the same cycle. So there was this recognition that Internet measurements at scale meant that someone there had to be a sort of infrastructure, and organization to keep that measurement going beyond the life cycle of an academic paper. There's also the challenge of just sharing datasets large datasets at scale as well. So this, again, in 2008, course, this has become a lot easier with as datasets have gotten bigger, and it's become more common to share them openly. But at the time, it was a challenge. And then there was also"
  },
  {
    "startTime": "01:54:02",
    "text": "consideration of what it meant or or how we could measure the Internet or the the so called real Internet. So Of course, there's a lot of interesting work done with test beds and sort of localized and can role measurement in in a lab environment, but there is this desire and notion of measuring where real traffic was being sent and being able to collect metrics. in that. environment. And then the third one, which is and again, sorry. It's a for. But the third one was thinking about, you know, all of this research was being done sort of in an effort to improve the Internet as a whole, but it turns out that needs a lot more than just the Internet network researchers to promote those ideas, and so there is this observation that bridging the gap between industry and academic research was in need, but also bridging the gap between into industry and policy makers and having there'd be a better more shared understanding about what the needs were for the Internet and how policy could play a role in that, how research could play a role in that the hope was that if there was open data, then the open data could facilitate a conversation such that we were all looking at the same numbers and looking at the same sort of understanding of how the Internet was evolving. So those were the the lofty goals that measurement lab was founded to create. And so now I'll just talk a bit through about what we're doing to address those those challenges, what we've, I think, done a good job at and what we still are actively seeking solutions for collaborations with. So the first one is that we now run a platform and an infrastructure I'll talk more about that, and we also collect a ton of data and publish it for open access. And then through that publishing or through that collection and publication, we're able to then support"
  },
  {
    "startTime": "01:56:01",
    "text": "researchers, policymakers, and members of industry by using it and helping it inform their work. So the platform is the sort of nuts and bolts of it all. You know, we so Since 2008, we've grown a platform of 750 sites around the world that each have servers host our measurement, services or experiments. These are placed in interconnection points and as of as of last year in cloudnetworks as well, asterisk there is right now, it's only Google Cloud, but we are actively seeking partners to be able to branch out into other cloud networks if that's thing that you are interested in or know people who would be interested in, I'd I'd love to talk. But in any case, right now, we are in 750 plus locations globally. And on those servers, we host what we call measurement services. which are essentially the server side of experiments. such that when a client runs our clients run the test when an application runs the client side of the test, then they test against an mLab server, and that's chosen based on geographical proximity. So if I'm in New York, I can test or sever server in New York and so on. And so right now, we host 4 measurements services. And every time anyone runs a test, the results of that test are archived publicly, and then pushed into Google's BigQuery tools such that they can be used easily. And as mentioned before, one of those tests is reverse trace route. We have a test called dash and then also WEEH, and NDT, which stands for Network Diagnostic tool. I'll talk more about NDT because it has our greatest data volume. So since 2008, we've been able collect over 6,000,000,000 rows of NDT NDT data. NDT stands for network diagnostics as they said, and it measures"
  },
  {
    "startTime": "01:58:01",
    "text": "the bulk transport capacity is defined in RFC 3148. of a single stream TCP connection. it is commonly considered a speed test. This is something that we have thought a lot about in terms of the ambiguity of the term speed. But nevertheless, it is often associated with the notion of speed and the measurements of throughput. and we have been able to amass this amount of data because the Google search or largely due to the Google search integration of our open source clients. So when people Google Health It's my Internet. they're able to run an mlabNDT test. And so through this, we've been able to a really large amount of data, which admittedly is can be quite noisy, but there's so much of it that I think there's a lot of interesting questions that can be asked and a lot of research to be done in terms of its potential use cases. Also worth noting that while Google the majority of results are collected through Google Search's integration. There's also a variety of smaller scale collections. So for example, Airbnb collects or provides NDT as a way for their host to tell their guests about the quality of their inter Internet connection when they're booking a stay and want to be able to work from home. or from Airbnb. There's also use cases within local communities trying to under stand their broadband availability and digital inclusion. So don't I definitely don't have time to talk through all of the current efforts at the SEC And NTIA to improve broadband availability in the US, but suffice to say data and mapping are a huge part of that. And so often we'll see people integrate NDT organizations integrate NDT such that their communities can test their Internet and get a more localized representation of how the Internet performance is evolving in their areas. So many tests come from these integrations as well."
  },
  {
    "startTime": "02:00:03",
    "text": "consumer reports integrated NDT into a study about broadband pricing as well as performance in the US. I recommend reading that report if you haven't. And I also wanted to mention that every measurement that is run on the mLab platform as what we call sidecar services run, which includes the PCAP data, the TCP info, and run a trace route from the server back to the client. So this means that for every one of those 6,000,000,000 tasks in the BigQuery database these sidecar service tests have been run as well. So there's there's a lot of data they're waiting to be analyzed. Also wanted to mention that we have also started hosting 3rd party datasets, meaning datasets that we have not generated with the AML platform, but that have been first in there. case of Cloudflare generated with their own infrastructure, and then we publish it for public use. And so this is all these past few examples are all examples of us providing data in the public interest such that when we're looking at how the Internet is evolving and how the Internet has changed over we can have as many data points as possible. So in my last minutes that I'm already over, I just wanted to talk through some ideas for how we could be, hopefully, of use this group and definitely the reverse. I think the first is just again, like, there's a lot of data there. It would be great to hear your experience using it and the questions that you're asking. So just using mLab data in the research that's being done in this in this group could be really interesting, I think. Again, 6,000,000,000 There's also the opportunity to propose new data for us to collect. So this could look like 2 different things. You can propose new measurement services. Again, those experiments that run on those 750 sites around the world and growing. and be producing data that will then be published publicly. And then also you can recently, as of"
  },
  {
    "startTime": "02:02:02",
    "text": "Recently, we can also start thinking about what datasets already exist but maybe have not been published or don't have the infrastructure to be published. that we would also be interested in providing for public interest And then I think the last one oh, is just giving us feedback. There's a lot of expertise in this room that we would be we'd want to be informed by, and I think you just using the data and or thinking about ways that we could improve the platform, different research that's being done that we could benefit from utilizing within the structure of structure of our measurements we would from all of that. So mostly, this is just a pitch for you all to be aware that mLab exists. in for there to be an open dialogue on how we can bet support, map RG, and potentially other parts of the IETF. But because we were, as I, you know, talked through, really found to support this kind of work at scale. I think there's a natural hopefully natural synergy in terms of helping scale your work. So I'm happy to answer questions, although I think I might have ran out of time. but my email address is just back here in the first slide, just laihi@measurementlab.net. you for having me. Alright. Thanks, Lai. We've got two people on the queue, Ali, and then Francois, let's hit them quickly, and we'll wrap Thanks much. very This is Alira Zaki from Nokia. Great presentation, There is environmental impact in SAS inability related work starting at the IETF And, for example, one of the topics is carbon intensity across the network and being able to do carbon aware routing, for example, have you done any related work, or is there"
  },
  {
    "startTime": "02:04:02",
    "text": "any measurements being done or metrics available already. Thank you. No. The end the short answer is no and the and the other slightly longer answers that that's absolutely something we'd be interested in. I think there's because we've collected so much NDT data, there's been a lot of emphasis on performance from that perspective, but I think there's a lot more that our platform can do, and we would be really interested in and seeing what it would be to measure that. Let's see. Thank you very much. Thank you. Good morning, Francois Nguyen. Can you show the map again? Yes. There we go. There we go. So it looks like Russia, China, don't seem to have many labs, That's correct. If those are partnership that you would have access to. We'd love to hear about it. Those have mostly been just due to our own and abilities to find partners, And where do you store data? Is are there any issues with GDPR or anything like this? to stress traces, for example. Yeah. You can look at our so when you take a test, users are asked to read our privacy policy and accept that their measurements will be published online. But we are under the this is not probably the right terminology, but they're covered by research and then under GDPR. or there's more information on our privacy policy. Thank Alright. you. Thanks so much. Thanks, Lehi, for joining us. Thanks to everyone. Yeah. I would I would also like to at least maybe can you share more information about how to use the data and what what kind of data do we have on the mailing list so we can continue the discussion there. Absolutely. Thank you so much. Perfect. and safe travels to you that are traveling. see you in Prague. and watch the mailing list for the the invitation to contribute to that Yeah. Thank you, everybody, for contributing, Angori, for taking notes."
  }
]
