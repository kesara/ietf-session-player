[
  {
    "startTime": "00:00:16",
    "text": "note well. is is The OPUS RST is still the largest RST. in terms of bites. I I I think it was at the time. But bites. sound,"
  },
  {
    "startTime": "00:02:00",
    "text": "seen that earlier this week. Yeah. He's doing the just rotate the device. Yeah. Yeah. How did you get that to go away from yours? made the window bigger. Oh, making the window bigger fixed it for him previously. Full screen up, maybe? Maybe? Yeah. I I don't know. at that point control alt minus and try to reduce your font size. Yes. That's it. Okay. Alright. also excuse the harbor. Yeah. I can't make That's good one. Okay. That's bad. is time. Alright. I think. Alright. I think we can begin Welcome to the first meeting of the ML codec working group,"
  },
  {
    "startTime": "00:04:04",
    "text": "If you're not expecting to be in the ML codec working group, you're in the wrong place. Standard IETF Notewell applies. and people should be aware. Make sure that you log in to the data tracker to be entered in for the flu sheet functionality, height It looks like everyone here probably is based on accounts. Do we have an notetaker for the meeting today. I believe we do. Right. Okay. someone willing to act as a Zulip relay to relay comments into the room from Zulip. Okay. That'll work. Okay. So the agenda as currently set we'll be discussing. Jean Marc will be presenting Opus extension mechanisms. hopefully, we'll be able to do consensus call to adopt that as a working group document, and then and then we'll Jan will discuss the speech coding enhancements draft. And then Jean Marc will discuss the deep redundancy draft. Is there any agenda bashing? Yep. Okay? Alright."
  },
  {
    "startTime": "00:06:06",
    "text": "Go ahead. Jira? Okay. So I'm presenting a proposal for an extension mechanism for OPUS that would be used to implement some of the changes where proposing, including deep redundancy. Name of the draft is draftvalenopus extension, currently at 1. Next slide, please. So the goals we have here for this extension mechanism, obviously, we wanna support all of the goals we set ourselves for the working group. we want to have that while maintaining full compatibility forwards and words with the original specification, we want to make it possible to do all that signaling this extension signaling in band. like the rest of the way the rest of the OPUS Works. And there's some trade off to do here between, you know, future proofing to be able to extend it while at the same time making sure it's as efficient as possible. We're This is a codec. We're trying to compress as much as possible. So the proposed solution we have is to transmit all of these extensions within opuspatting, the What that implies is there's no changes to any of the other bits of OPUS, everything goes into the padding. and it also ensures that the extensions will be ignored by the older encoder that don't implement the extensions, but it's not gonna break them. Next slide. And by the way, if you have any comments, you can you know, go through the mic. No need to wait for the end."
  },
  {
    "startTime": "00:08:02",
    "text": "So quickly, what just an explanation as to why the compatibility would be preserved in this case. Reading from Rfc6716, the this RC. essentially, it says that on the encoder side, the additional padding bytes must be 0. that's what all implementations we know about June. And the on the decoder side says the decoder must accept any value for the padding bytes. And so based on this, as long as we ensure that when we use this new version of OPUS All padding is still interpreted as 0s, the I mean sorry. If the padding is all zeros, it gets still interpreted as padding, then we should have compatibility both ways. Next slide. So this is the proposed format. for the extension What we are proposing is to have essentially a concatenation of extensions is more than one extension in the the padding bytes. no table of contents or any of that, just a bunch of extensions one after the other. Every extension would start with an 8 bitheader this header would contain a 7 bit unique ID for the extension. and a one bit flag that we call the l flag here. And that l flag along with the ID would control whether there is an optional length an optional length byte or sequence of bytes, and those would signal how many bytes of payload follow for the extension. Next slide. Oh, is there an comment, comment,"
  },
  {
    "startTime": "00:10:02",
    "text": "I'm sorry. It's a pain in the ass to get the this tool to work. smaller firms. So, hopefully, we don't have to use a tool. One comment on the length extension, a lot of the other IETF work is kind of using variants tub to support you know, variable length lengths. So it's a good idea. Reducees one more bit. So if you think you're gonna have roughly a 128 extensions. You're approach works better. But if you think you may only have a few, like 64 or less, a potential of of even even more than aligning on the variance may be a you know, more consistent with a lot of the other drafts Okay. I would needs to read more about that. It's just it's just two bits of two bits of size. you know, 2 bits of size, saying whether it's one bite 2 bytes or 4 bytes or 8 bytes. probably never gonna have the 8 by variant, so that doesn't make sense. But Oh, you mean for the extension ID itself? you have a you have a extension ID that's 7 bits and then an optional extension. Right? An an optional more than 7 bits. Right? Oh, no. No. The the length is for the payload of the extension. the ID says which extension, for example, for Dread, we would have, like, it's extension ID 32. Oh. Oh. And then I'll I'll go with -- -- gets load length, not the ideal length. No. No. The payload length. Okay. The ID length is fixed to 7. Okay. At least in this proposal. Okay. You're confident that that that fixed length will will will encapsulate all possible extensions if we really run into problems, then we can have an tension that says, you know, here's another one. But right now, we're at 3. So -- Okay. And it's been 10 years. So I think a 128 should be good for few centuries. But, you know,"
  },
  {
    "startTime": "00:12:02",
    "text": "can -- Unless, like, people suddenly had, like, tons of ideas and you know, we need to use more, but for now, it seems reasonable. But, you know -- But you're you're pointing out that it bit bit bit our hands aren't tied in the future if more extensions are needed because there are ways to address that. So if we really needed some more extensions then we would have an option to define you know, extension ID a 120 that says More. something else. Mhmm. Yeah. So where it we're not completely locking ourselves out. Okay. No. That's fine. That works. Yep. Next slide. I'll try it. So to make sure that you know, we can add future extensions and that future decoders can just Skip the extension they don't know. we essentially have to predefine the behavior of the length and coding for all IDs even if we've not assigned them. So this is the way we propose to have work. ID 0 essentially remains padding because we need all zeros to keep the padding definition. And in that case, l equals 0 will need will mean that The rest of the path of the Original OPUS betting is still OPUS betting. So essentially, you have a sequence of all zeros in that case. We map l equalsoneforid0, to mean that this single byte was padding, and there's no lamp that follows, no payload. So in other words, you can use a bite bite equal to 1 to do as much padding as you like. So this padding is kind of a special case. Otherwise, IDs 1 to 31, are"
  },
  {
    "startTime": "00:14:01",
    "text": "reserved for what would would would call short extensions In that case, l equals 0 mean that there is app there is no length or and no payload. So, essentially, it's just signaling like, a flag in that case, and l equals 1 would mean there's a single byte of payload that follows. So for extensions that only need 8 bits. That's for small extensions, for larger extensions, ID is 32 to 127. then l equals 0 would mean the rest of the padding is used for that extension. There's no extension that follows that. And L equals 1 In that case, it's the only case where we would actually signal a length And the signaling would essentially be the number of bytes. And if you use 255, then the number 4 plus the bite that follows and you can have as many of these 255 codes as you like. k. k. Now in terms of assigning these IDs, So as I mentioned earlier, ID 0 would be the original padding. ID 1 would be a special frame separator. because OPUS makes it possible to put several several frames in a packet. You can have a packet up to a 120 milliseconds. meaning you could have 6 20 millisecond frames in that bigger packet. So if you want extensions that are associated with one frame or another, you need to wait to you need a You need a separator between those. So ID 1 would be the separator. You would For example, code 1 extension. You put in the separator, another extension and so on. So this is for ID 1. 22119"
  },
  {
    "startTime": "00:16:02",
    "text": "would be assigned through Standards Action policy. In other words, at least our current thought is not to have, like, some kind of first come, 1st served because this is you know, OPUS is not really transport protocol. We need to be careful when extending it. If you create extensions, you wanna really check that it doesn't interfere with other extensions because it's all related to you know, the same code base, the same decoder. So at this point, the thought is we probably want We probably want, you know, a working group or some kind of IETF action where we really checked that This is done right. And then we would reserve 88 IDs for just, like, experiment purposes so that, for example, when we due to this dread implementation we can start experimenting with it deploying it in limited circumstances without getting problems once it's finalized. Yeah. Yeah. Harold. 22 Yep. Go ahead, Farrell. If you want to follow the tradition from other places, reserve 1274. you run out of bytes. We can do that as well. Yeah. We would just need to change the existing code base that uses 120 7 for the experimental dread stuff, but It's not deployed, so we're good. Next slide. So in terms of SDP, the idea would have would be to have these generic parameters, extensions and s prop extensions. respectively for the receive side and the sender side."
  },
  {
    "startTime": "00:18:00",
    "text": "and just have a list of supported extensions for the sender and the receiver. So like just a comma separated list of numbers, And then each extension could define its own specific parameters using extndash whenever the name is required for that particular extension. most nice serving tools not working still. the these are negotiated parameters or this just declarative since there's no interoperability problem just declare everything you can send and receive. there's no negotiation down to anything Yeah. It would be basically declarative. It would be treated exactly like the original OPUS parameters for, you know, bandwidth and like, Right now in the SDP for OPUS, you can say, you know, sampling rate 16 kilohertz. means, you know, I'd rather receive 16, but if someone sends me 48 I'm still gonna be able to decode it. This is how it's currently defined in the office payload format. It's kind of You know, for example, like, to take a practical example with dread it would be something like saying, you know, I support dread. I have a jitter buffer that can go to half a second don't bother sending me longer than that. But if you do, I'll still be able to handle it So there won't be any normative language that based on offer answer semantics, you end up doing this. It'll just be up to implementations to decide what they want to send or or or what they wanna enable receiving because no actual rules, and there's no interoperability problem."
  },
  {
    "startTime": "00:20:01",
    "text": "Yeah. I mean, all all of this is for, like, optimization purposes. You know? Maybe I'm sending the same packets to a bunch of people. and you know, some of them wanna send the same packet. Some of them can support 48, some 16. I might as well send 48 to all of them. Like, essentially, the intent is to do exactly the same thing we did for all the other parameters in in the RTP payload format. Okay. I can't remember the exact language, but I remember that exact discussion being brought up and I can't remember the exact details, but what it came down to was essentially treating those as you know, I I can decode it like like like the way the decoder works, the decoder can always decode anything you send it. Mhmm. So it's just about not being wasteful. like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like I'm going to play this out on a single speaker if you send me stereo, it's gonna be wasteful, but I'm gonna have a maybe useful to mention in in the draft that that these extensions are purely declarative, but the individual extensions themselves when they register their extension they may have, you know, requirements about incompatibility with other extensions or something like that. or rule more rules than this generic draft will would allow Yeah. I think that would be kind of an interesting one if an extension showed up that If you enabled it, it would cause you to break something, like, that would be like, different from any of the other opus behaviors. I'm not sure how to pronounce it. -- some extensions that are mutually exclusive. that are competing, and you would never turn them on both at the same time. So they're would be some rules, and those extensions say, I am incompatible with this other extension. ever turn this both on at the same time. I will defeat this other extension. We'll defeat each other a few. if you do it. So it's fine for the generic draft to say that these are just declarative, but there may be further rules by the extension, you know, specs themselves."
  },
  {
    "startTime": "00:22:02",
    "text": "Yeah. I'm not I would be open to, like, proposals on language for that. Definitely one It's from the next slide, but one of the open questions maybe related slightly to that was whether we wanted to define I think it was from this UDP options draft that define these unsafe extensions, telling decoders, you know, if you don't know about this extensions. You must ignore all extensions. So I don't know if it's useful in that case or not. open to comments search. I'll come in. Yeah. Yeah. Telephonics. I just I Wanted to Observe, I know the design of the parameter name for the extensions were designed such that could forward parameters of that mind about them, but I on further reflection, I think that's not true. at least if you're at, like, an SFU mean, forwarding them, like, just 1 to 1, obviously, you can do, but Forwarding like, if you're an SFU trying to aggregate them, you have to know their semantics. Like, you know, yes, the bread parameter have to do the max across all of them or something like that. nor any there's no way to do generic aggregation of these. parameters as far as I know. Yeah. The original intent was not so much about the forwarding, but just like imposing some kind of order in it. So, yes, I get structure so they don't Okay. But, yeah, I realized that it would not be, like, kind of a perfect solution 1st first first. Yes, Omaya. yet to to most point, I think, the confusing part with As far as I remember with OPUS is that it basically says, I can do this And as you said, like, the receiver always needs to be able to handle it. I'm wondering if that actually with the extensions will work, or will we get to a point where"
  },
  {
    "startTime": "00:24:01",
    "text": "the center basically says, I can do extension 55, the receiver is basically like, no. Don't do that. rather than was saying something similar. It's, like, Well, for extensions, the decoder is always free to ignore the extension. They did something which is we should spell out, but none of the extension should mandate something. But in part because of that of, you know, compatibility with the existing decoder. Like, the existing decoder will see -- And then you you'll be -- Here's padding. Let me throw that away. So -- So then that would result in the and center potentially, like, activating the extensions and kind of wasting the on the wire because the receiver couldn't handle it, basically. Yeah. The the the point of this extensions list is just to not waste resources. But but that but that's what you're what like, you The sender would say, like, I support like these 5 extensions. Yep. And I think in in OPOS right now, that basically means that he's also going to activate all five these because the receiver doesn't have actually have a way to tell them, like, hey. out of these 5, please turn only on these 2 because the other 3, I don't understand. Well, the the way this would typically work is, you know, For example, I would say I support just this one extension And You tell me you support a bunch of extensions. But from seeing that I only support that one, you're probably not gonna bother with the other ones. But So the the standard side, I had to -- -- for the for the for the for the receiving side. Yes. But what I was saying, like, basically, if the the offerer busically says, here's, like, as I said, like, 5 different extensions And the right now, I think in in OPOS, basically, the the the Sever doesn't have a way to I think that's where most confusion was coming from that normally an an offer answer. you have like, the receiver has a way to say, like, like,"
  },
  {
    "startTime": "00:26:00",
    "text": "Don't do that. Like, I, like, I don't understand this or, like, don't turn this on. Right? And, like, I think in opus with how things are specified right now, it it's always basically like, The sender will basically tell it like I have these 5 extensions I'm going to send you them anyway because you have no way of turning them off you will feel free to ignore them. don't I'm sorry. for this. That that is not at least that is not the intended semantics the intended cementic is more like, You're telling me you're able to support that. I'm telling you this is sorry. You're able to send that. I'm telling you I'm able to receive that. So, normally, if things work correctly, like, 6 you will take the intersection of both Unless you somehow have to send me this because you're sending the same packet to someone else? Mm-mm. But, otherwise, you will say, okay. This -- So you're saying, like, only -- Under stand that, so I'm not gonna send it. But on the other hand, like, if I support this extension and you tell me your you want to send it to me, then I'm gonna be prepared to and you're you're -- -- coded. -- and you're going to mirror it back, basically, in your list extensions kind of like I'm willing to do this extension. Or, like, you know, for example, if I say I support dread, dread, on on the receive side, and you tell me you don't. on the sender side, I'm not gonna bother initialize the dread decoders. because I am going to assume you're not gonna send it to Okay. It's it's like Right? Oh, sorry. Yeah. I forgot about the last slide. Yeah. Yeah. So some of the things that, you know, wasn't clear which way to go. You know, I took some guesses in the in the draft as to these answers, but you know, and kind of willing to go the other way. One open question was"
  },
  {
    "startTime": "00:28:02",
    "text": "know, should we allow or how should we handle repeated IDs for the same frame in a packet. Like, coding extension number 35, twice for the same frame in the same packet, Does it have a meeting, or should we disallow it Or does the order should we keep the order, or does the order make any sense. I don't hand a strong opinion either way. that was one that So the general ordering of extensions if I code extension 35 before extension 50. or if I code 50 before 35, should it matter or not? You know, in other words, like, should it a if there's, like, a middle box that's gonna deal with extensions, does it need to guarantee it preserve the order, or can it do whatever it pleases. Sorry. John phonics. Yeah. I think for the repeat ID, My inclination would be to say, In principle, it's allowed, but only if a specific extension allows it. They just have been the the, you know if you for generic ones, because it can happen any number of times. But for any specific one, if it should ex it should have to explicitly say, you can send me multiple times, and this is what it means, And if if an extension doesn't say that, it means, no. Don't do that. this is the case where perhaps the the draft should lists some cases what are this. extensions need to define certain semantics. They might consider, like, a a template of what what the exceptions need to talk about. For the ordering I mean, my worry -- Sorry. Just to make sure I understand. So the proposal would be for the extensions draft to say, by default, you should never have the same ID unless the draft defining that ID specifically says that it's allowed for Yeah. But that means that for an extension you don't know, it could happen multiple times and just deal with So Yeah. Okay. So the SFU would need to say, okay. I"
  },
  {
    "startTime": "00:30:03",
    "text": "Yeah. It's allowed. Okay. That that that is the main question. For the ordering, I mean, that's hard to know until we start getting into, like, Specifically, my worry is things like extensions on extensions, like You know, this is a tweak I mean, this is a tweak on this part of the waveform, and this is a further tweak that part of the waveform or something. a tweak on a tweak. I don't know. I'd say probably leave it alone, but I don't know. I'd have to the interesting thing is quoted as a few partially strip extensions like if it knows that receiver 1 is take understands 123400 only under understands 3, put a strip the other ones. and I answer it. I don't know. Okay. Alright. Going to close the queue in a minute. We have one more comment here. And then if anyone else Francois Nguyen. My first I'm here. maybe it doesn't make any sense. But is it possible that the extension is treated as a toggle. if it doesn't have any any values and it modifies the way the next extension is being treated. And then after that, you want to have the same extension again to say, okay. Now it's going back to disabled, and further extension would be treated differently. Oh, to have defining an extension to have, like, a a change your behavior for the next extensions. that's My general so the Text currently doesn't say anything about that. My general idea is it would be terrible thing to do because RTP because you toggle something, and then it packet got lost, and it would I think this was within a packet. not within not within the list of extension of Oh, within this In some sense, the the separator extension does something similar to that. Yeah. Yeah. Alright."
  },
  {
    "startTime": "00:32:05",
    "text": "the separator changes sort of the state of the extensions parsing. Because it says, you know, from now on, every All the extensions you're gonna see are gonna be associated with frame 1 And then you see it again, it's like, oh, from now on, it's gonna be frame 2 and so on. Yep. Okay. So the third one I already mentioned, the unsafe thing. Yes. Can we get account of who's read the draft? in this case. Okay. 20 to Yeah. Is it telling you to rotate your laptop? Oh, interesting. But it is it's currently work working for thirteen people. refreshing Alright. Okay. It looks like a decent number of people have rubbed the draft considering the counter the participants in the room Do we think that this is This draft is a useful starting point for"
  },
  {
    "startTime": "00:34:00",
    "text": "working group activity, Let me let me dismiss Alright. I'm fighting this tool here. a starting point. Yes. That's alright. Yeah. I think we saw from discussion that there's you know, opportunities to improve things in it, but that's that's not the question. So So let's change those spots. It's 10. Right? Yeah. Yeah. was 30 Yes. Yep. It it's 10. Okay. I think we had 2 more Yes. Oh, that's true. For the effective tools though, if don't use them. Once they're in place, you stop existing, it seems."
  },
  {
    "startTime": "00:36:00",
    "text": "I'm sorry for your lack of existence, Tim. I raised my head. No. But, normally, there. Okay. I I I I'm and we can, you know, this this sounds good to me that I think we can adopt this as a working group. draft. but if there's more bashing can occur, you know, on the list, There's always. Okay. So now we're up for the the next set of slides. Yes. Yeah. And for the notes, that was 12 raised hands to to adopt. Hold on a second while I finds lines. Yes. Hello. I want to presented. Closer? Yeah. Like this? Okay. Great. I want to Yeah. Yeah. Yeah. Yeah. Okay. Okay. Okay. Okay. Yes. Yes. Okay. I got this. Yes. I want to talk about speech coding and enhancement for the Opus codec. So that relates to the 2nd goal of the MLCotic Charter, charter to improve the quality of speech coding atlowerbitrates there is a corresponding graph, which has almost the same name, I should have put this on the title slide, but Yeah. You will find it, I think. the next slide. So the specific methods. that I'm proposing concerns the decoder. So it's basically to improve the decoding capability"
  },
  {
    "startTime": "00:38:00",
    "text": "to make the quality of on coded speech better. And in the beginning, I wanted to give a brief overview over the office decoder, there are some pump peculiarities, Regarding this, so it's a switching codec, it has basically 3 different modes. So on the left side, you see the county coder that is the MDCT based on general audio coding mode. On the right side, there is the silk decoder That's the dedicated speech coding mode, and then there's also a thing called hybrid mode mode. where the silky decoder actually coats the low band and KELT codes the high band. And then there is this mode handling block down here, which has to either handle transitions or combine the 2 signals in hybrid mode. and the logical place for adding an enhancement model model would be, of course, directly after the silky coder, so it would take the distorted speech as input, and it would output a better version of that. It's Can I have the next one? Okay. So But, Jed, I think they're -- One quick question on the previous slide, please. So so this precludes any non voice optimizations, Yes. This is specifically about each more monophonic signals that might be coated with silk. So it's just about improving silk at the moment. Yes. Okay. Alright. No. Yes. So There are 3 general challenges. The first one is that you can have very heterogeneous inputs regarding language, language, Quality. It could be clean speech, noisy speech, But also, A lot of the encoder settings are not known to the decoder. So"
  },
  {
    "startTime": "00:40:01",
    "text": "There is a huge variability, and whatever enhancement method is used has to be able to deal with all of that. second or second thing is You seem so to say that the speech color is embedded into a larger structure. So whenever you change something in the processing chain, you have to make sure that it preserves, so to say, the functionality of the decoder, the seamless mode switching, 1, and We don't want to destroy hybrid mode. And the last one is an interoperability issue, issue, So from the specification, the encoder actually has a lot of freedom to and quote, the valid ocas bitstream, And there are only recommendations of how to do this. There's a reference implementation. And if you're now at an enhancement module to the decoder, then the optimal settings of the encoder will very likely change And this means that if you tune now the encoder for a decoder with enhancements included, that would very likely degrade the quality when decoding was a legacy decoder, And this is also something to consider. So what is the minimal requirement of quality requirement when you decode with a legacy focused decoder, And that one next time? So so this is now one proposed method. That's very recent. it's it's it's basically I mean, Doing this kind of enhancement has a long history. It's Not surprising that people have been in the same situation, this each codec is done? How can we make it better?"
  },
  {
    "startTime": "00:42:01",
    "text": "And, also, this linear adaptive coding enhancer is actually just a supercharged version of these classical methods, which means so on the right hand side, You see the signal path And here, you just have what's traditionally called long term and short term filtering. but but The difference is that on the left hand side, you have a rather powerful neuron. network, which provides filter coefficients to the right hand side. Mhmm. And for DNNs, it's at the very low end of the complexity scale. So it's 100 megaflops. It's built still very significant for codec. It's also fairly small with 300k parameters, parameters, And if you're interested, so the paper is just made it to the archive. There are also some demosamples to look at and there is Python code for training the model It's not yet fully integrated into the OPUS codec about you. it's so to say stand alone, you can on his Yes. Gentlemenix, are those complexity and model size talking about? Is that for Encode side, decode side, or both? That's for decoder side. So, actually, the it's it's a blind method. So the encoder is not involved. It just runs in the decoder. and Yes. that's There's the encoder is not doing anything? The encoder is not doing anything. It's just encoding, so it's it's a blind, speech enhancement method. I mean, it's it's it's basically, like many of the classical speech enhancement method which would just do comb filtering and remove some inter harmonic noise. and do some format enhancement. So it's In in this version, it's it's blind, we should think about later maybe to use the extension mechanism and to also transmit information for guidance, but the first method on on the first"
  },
  {
    "startTime": "00:44:01",
    "text": "approach is to do it completely blindly. Yes. Okay. So then -- So you have some more questions. Yes. this this Fostling again. So the decoder is more complex than encoder. In that case that would cc the case. Yes. That's okay. That depends very much on what you are running this. on I think if you're running this on a smartphone or if you're running this on a laptop, that would be completely fine. If you want to run it on I don't know. An ear plug that's probably not fine. But then it's probably I mean, it's unlikely that you would want I mean, it it would be optional in a certain sense. You would not force people to use And, I mean, even if you had an ear plug, you would probably receive the audio first on a different device that could do the enhancement and then forward this by reincoding is Huddl Alastair. If I understand you correctly, this is a the code site only. So why should anyone care? So why should anyone care about it in a standardization context? That's what I basically said before. So to say, I mean, There are these 3 things you have. I mean, I think you need requirements for adding an enhancement method because it's inside the codex. So it's not something you do at the very end when you collect your output and then you do something, but you are really changing the processing path and on So, I mean, the specification should at least put together requirements so that when you put something in there. And if you call it OPUS, that it works. so you can break it by adding adding adding believe you mentioned If you enhance the signal, Yes."
  },
  {
    "startTime": "00:46:02",
    "text": "you are not reproducing the bit pattern on the input Oh, you you are creating something that s different output from the reference, That seems to be a bit strange. Yes. Jean Marc Feney, answering to Harold. You know, essentially, right now, the spec says, these are the test vectors to be compliant. You need to decode exactly that. and and And so if you you cannot enhance and be compatible with the existing specs, so we are updating the spec to say this is the kind of enhancement you're allowed to do. You need to be within these parameters. For example, at this bitrate or this condition, you should not change the signal by more than this much, otherwise, it might cause compatibility issues. So we need to clearly define these parameters And eventually, the intent is to also add some some side information to do better enhancements. So it's gonna be kind of tied together But otherwise, you need to at least clarify if you're doing it blindly to what extent you should modified the sigma or not. I'm I'm all in favor of announcements. No. Don't take me wrong. at We had this with video for h 261 or something that in on that order. and the results were that The Standards committees ran away screaming in horror and and settled on on their settle test vectors that had to produce an exact match for the output. So I'm only in favor of doing this. but I don't understand and why it needs to be done in a needs to have a standard. Mhmm."
  },
  {
    "startTime": "00:48:00",
    "text": "Maybe I mean, we will get to this a little bit. I I brought you back a slide here too because I think you also raise some points with the encoder behavior may ultimately be changed by some of this, which might perhaps interoperability impacts. that may maybe a consideration. It's Okay. Then I think we can go to the next slide. So I mean, this is now one attempt at pressing the challenges that I mentioned before. So regarding the heterogeneous input. What this model does is it's trained on the multilingual dataset. And there is random switching of bitrate and encoding parameters involved. And so far, in the tests that performed well, but it definitely needs more from testing in the future? Regarding Decatur Integrity, on the strategy for This model was actually to operate with 0 delays, so you don't have to adjust any delay in the signal pass. And it's also trained to be approximately face preserving So the expectation here the expectation here is that you can just just just plug it in and all the switching will be fine. And hybrid mode will still work. but it's not tested yet. So I think it will happen integrated implementation later this year. and then we will see whether this really turns out this way and regarding interoperability. So think when you listen to the demo samples, you will understand is a little bit better, so Here, we changed the encoder to do white band encoding down to 6 kilobits per second. Usually, starting from 9 kilobits down, you would do narrowband. and This means basically that if you use a legacy decoder,"
  },
  {
    "startTime": "00:50:01",
    "text": "It sounds worse, but it's still intelligible. And the question is, would this be acceptable Yes. Okay. And So we'll also have some some listening test results, and another example. So In black, you see baseline that's just focus at 6 kilobits oh, yeah. Opus white band at 6 9:12 kilobits per second, and the blue, line is, so to say, the quality, if you add Lace, There is also one interesting point in green. This is a LPC net for synthesis. method that has been published a few years back. And it's so to say interesting to see that you can actually have quite better quality still? However, the complexity is much higher as 3 gigaflops. and it has a 25 millisecond delay, But so to say improving further improving this late model is ongoing research and research and if you go to the repo, there's already a new model which I think already closes the gap, and it's in the middle between, so it would be something like 5 to 6 times the complexity of lace, but one fifth of the complexity of the reason this is matters. press it. Okay. And And the final question is, how to standardize if one standardizes this. then a very easy, way would be to just standardized and enhancement method. However, that could be very quickly outdated. So if you fix a model, research is going very fast at the moment that would be very likely inefficient Also, depending on the application, you might have different"
  },
  {
    "startTime": "00:52:00",
    "text": "complexity, quality, trade offs. So for some platforms, maybe you can just the 40s 100 megaflops but for a different that's something you don't care whether it's a gigaflow. So the proposal would be to just specify requirements So what what are the conditions that you plug in an enhancement method into the Opus decoder so that it still conforms to the standard. and So I think these three points that I mentioned before makes sense. So the first question is, what are the quality requirements for the enhancement model itself. when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's when it's run on on or sorted outputs, What are the requirements to ensure that it's doesn't break the decoder. And the third one is what are the requirements. So for encoder tuning so that you can still operated with Legacy Opus Decatur. Yes. I think that's another slide, but, yes, that's Okay. Yeah. I'm not losing any. I think it's unfortunate only had 7 minutes, and we got to that spot because I think that is by far the biggest issue in this working group. and that could have eaten the entire hour of discussion about how to achieve the right result. So I expect a lot of this discussion or side discussion during this week. try to resolve that point because I I I personally don't see a good clear direction for for how to standardize this and it'd be good to start really digging into the details of the pros and cons of all all the possible options. Yes. I mean, that would be much appreciated. The main idea is, of course, to get feedback when see how other -- And by by your your suggestion to to focus on requirements and and and and, you know,"
  },
  {
    "startTime": "00:54:00",
    "text": "quality, parameter specifications. You you don't mean to imply that there'd be a random closed decoder model that that nobody knows about as long as it meets the requirements, it's they can use that extension. We still expect people to register or somehow you know, publish their extensions and how to how to encode them not just that that they met this quality bar, then they're opaquely using ID 7 to mean My magic my magic enhancer that nobody else can use, but it meets the quality bar. It'll be open somewhere. Yeah. Oh, Jean Marc Vanay quickly. The idea is the quality bar would be only for the no side information case. So you can always receive something And the intent is definitely to publish the models, but we want to make possible for people to have, like, better things Like, we don't wanna freeze a model, and you can never change it. But know, compatibility is the very first criterion. So if you actually signal bits, then we're gonna to literally define what these bits are. Like, we don't want exactly the case here. you're describing. that that is the top priority to avoid that. So at 6 kilobits per second, the the most call is better, but it's still low. And at at12 about the same. Yes. I mean, You you mean regarding the -- On this child, the the blue one, which is lace, at 12 kilobits, it's about the same Moscow. It the improvement for this model is not Huge hits. 9 or 12, but The next generation model is much this Okay. Yes. Click yeah. The"
  },
  {
    "startTime": "00:56:04",
    "text": "Yeah. That's so it's it's it's very compressed. I mean, between baseline and -- Yes. -- delays, you look the same except that There's 25 music on delay. Yes. I mean, it it it's it's yeah. But it's so close But it it's it's Statistics is yes. I mean, it's statistically significant. That's wanting, You can also listen to it. It's it's audible if you listen with headphones. but but the difference is not huge. That's clear. However, As I said so to say, this leis is really an experiment, how low can you go in complexity it it it operates at the very low scale. I misread. Yes. You make, so to say, the complexity requirement a little bit looser and you get more improvement Yeah. Yeah. This Okay. Thank you. Thank you. You got 3 minutes. How fast can you talk? That's that's no. They can't. I'm just gonna keep pressing forward every time -- Yep. So presenting deep redundancy very quickly, That's the, you know, general goal. Make Opus robust lots of packet loss, we can get up to 50 past frames into one packet with about 32 kilobits second, which means we can get up to one second worth of redundancy. This is generally how it's done. We have the in blue, the main Opus encoder and decoder. We add on the encoder side, some feature extraction, We compress those features very heavily with machine learning based model. And on the decoder side, when we've lost previous packets, then"
  },
  {
    "startTime": "00:58:00",
    "text": "we can actually decode that redundancy using feature decoder, and then we have these features. We send them to a neural vocoder, and we recover the audio. Next. The way this is done, it looks a bit confusing, but there's a good reason for that. We have this forward backwards scheme where know, we don't We have 50 x redundancy. We don't want the encoder to encode every single frames fifty times because that would be insanely complex. So we have an encoder that just run forward and every 20 milliseconds, it produces so called latent vectors that represent 40 milliseconds. So there's 60% overlap. It also creates these initial states that I'll explain a bit later. No. Sorry. Go back. Just essentially. And then on the encoder side, what happens is You take because there's 50% overlap, you take all the odd ones or all the even ones, you pack them, this would be here. 1, one packet that would be sent. It would have all the latent vectors, and it would have one initial state And the way the decoder works when it once to recover that information. It starts from the initial state. and it decodes backwards in time. And the idea for decoding backwards in time is the most recent pack are typically the most useful. It's gonna be very rare that you won the frame from one second ago, but the frame from 20 milliseconds ago, it's gonna be quite common. So decoder goes backwards in time. Next, So So in in the audio stream, if you miss what second. When you get the data back, you have to start replaying it. at a faster speed or something. Yes. The the way you would practically use it"
  },
  {
    "startTime": "01:00:03",
    "text": "is indeed, you get the data back, you decode the entire thing, and then you play with a one second delay and eventually your Jetta buffer you play faster and it eventually catches up. Essentially, that when you have dread, then losing packets becomes you you end up with the same scenario as if your router had just piled up a full second of packet and released them all at the same time. So -- Okay. -- you know, it's not great. You still get a hole and things play back for faster? but at least you actually understand what the person said instead of leaving a big hole. So these I presented the dispatch meeting. I think I'm you were there. Essentially, bottom line is if you combine the original OPUS LBRR with Dred you can actually get with packet loss no longer than one second. you actually get very close to the clean quality. Disregarding this whole Jitter buffer thing, assuming that Jitter buffer was already at one second. Next slide. So there's a proposed format. I don't think there's much time to go over it. I suggest you read the draft. We define a bunch of feels there, I'd be very happy to get feedback on whether these are useful, if there's anything we missed Essentially, you know, and discuss the latent features, the initial state, and the rest is stuff like, you know, what kind of quantizer, what kind of bit rate you wanna use and these sorts. Thanks. Next slide. In terms of the normative aspect So there's, again, this balancing act of ensuring everything is interoperable, but leaving as much flexibility as possible because, you know, machine learning is improving very fast. and"
  },
  {
    "startTime": "01:02:01",
    "text": "the idea right now would be to have a normative spec for the decoder weights, the part that takes the bits and gives you these acoustic features And we would need to somehow define precisely what these acoustic feature means, But from there on, we would leave the encoder completely free and also the vocoder that turns the acoustic features into audio we want you know, implementers to be able to improve those over time because we have something right now with LPC Net. We're in the process of replacing that with something much better, and I expect a few years that are gonna be something even better, higher quality, lower complexity. So we don't want to restrict ourselves, but we still need to keep this decoder the same so that everything is interoperable. Next slide. So running code, we have All of dread is actually currently running in OPUS. if you get the OPUS code in the OPUS NG branch, You can test it out. it's usable, It's got a few rough edges, improvements or suggestion bug reports welcome. Complexity is about 5% to 10%. We're working on reducing that by maybe half we think it's possible, something like that. Dominic Monnex. I also heard, I guess, a rumor that this also looks like increase to library size by, like, 10 megabytes or something like that. So right now, the current implementation will increase by about 17 megabytes. We've not It's still not optimized for size. We're in the process of trying to reduce that to probably 2 to 4 megabytes It's gonna be a challenge to try and go really below that, but I think 2 to 4 is kind of what"
  },
  {
    "startTime": "01:04:00",
    "text": "we think is achievable with similar or better quality. and we already have a mechanism to have these weights completely separate from the library. you can ship the library and have the weights sent you know, only when needed or in a separate channel or something like that. Colin, just a slight bit of feedback on that. I mean, That's yeah. I'm sure you'll get it way down or whatever. But even, like, 17 megs was, like, that's less of a big deal to me than in the 10%. Like, that's that's in the workable category as far as I'm concerned if we're talking an arm or something like that. So I I guess, like, sure. Let's keep working on this or whatever. But doubt didn't explode my head. Thanks. Yeah. Of course. on Yeah. So we're still open to feedback about a bunch of questions, including, what is the maximum duration that we think is reasonable? Right now, we've been centering around one second really arbitrarily Like, is Like, are there use cases for, like, 2 seconds, 5 seconds, 15 minutes, I don't know. So, you know, get getting an idea of what what is the upper bound of what's potentially useful is probably good. And same thing for, you know, the lowest and highest useful bit rates there. you know, right now, we have this one use case that seems reasonable to us, but we've not tried it too much in the wild, so definitely feedback is welcome there. Right? Thank you. You have more comments on that? It's not? No. I had a federal comment on this. that We're not"
  },
  {
    "startTime": "01:06:00",
    "text": "-- general concept on the whole thing. My I have one worry and that maybe it seems to be in somewhere in security considerations or something. As far as I know, almost all ML models and be have adversarial input. and that we have to worry about adversarial input because I could certainly imagine I mean, this is just sort of off the top of my head, but I could certainly imagine if you, you know, you could send a signal which People with the new OPUS decoder here is gas, and people with the old OPUS decoder here is no. That's there's probably interesting things you could do with that. So That's interesting. I mean, the the only way that could happen if you could control the loss, then, indeed, you would be able So Yeah. Yeah. Oh, with with the other one, the whole idea of the spec is to ensure that this is not possible. to to turn a u a no into a yes. with dread, Technically, like you could and you could have redundancy for yes transmit no. And then if the packets get the packet loss gets aligned, you could turn one into the other, It's kind of an interesting one. I don't know how to fix that one. This has also been a problem for multiparty chat protocols that run into this equivalency issue. I mean, assuming your model came from a Like like, here's I mean, I mean, The model won't do it. Right. The the model won't do it by itself. Right. Yeah. And then The streams are all authenticated. I I'm not really seeing an attack yet. I'm I'm sure he's being dumb. I'm not following it. a server could also just replace the stream entirely for some clients. Yeah. Yeah. that's why I'm I mean, you the only attack case that uses here is one where the base stream with OPUS today? Could it the yes could be changed to a no. Is that I think the the only case I can think of is I encode my like, on my side, I put in a yes in the dread and simultaneously encode no in the main Opus packet. And then somehow I cause packet loss on your side so you get dread and not on"
  },
  {
    "startTime": "01:08:01",
    "text": "mean, it's kind of far fetched, but theoretically possible. Yeah. But there's probably bigger problems than that. It's a good thing to discuss. the the general sort of you know, but the this would have to be obviously in a conference, but I sort of think, like, you know, the The underling here is yes, and the auditor here is no. and and ask whether they should do something. So Yeah. And I think that's more probably a problem for well, I know that the the you're the specification says for the half of that cat happened, but I We we can ensure when we train the model that CAN'T HAPPEN. ARE YOU SURE? the the the adversarial no adversarial inputs that could do I mean, you can that's also something to discuss. But -- Yeah. -- we could have, like, some part of this decoder thing where you get the model and then you compare You see how much change it did. And if it's too much, it's like, Yeah. So I was happy something after the ML model that sort of validate. So that that that makes sense. you could this is all definitely stuff that we want feedback on and that we we wanna make sure this is not possible. Okay? Thank you, everyone. Yes. to say yes very quickly. Yes. So he was talking about the coworker."
  }
]
