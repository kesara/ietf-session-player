[
  {
    "startTime": "00:00:14",
    "text": "having a thing where people know it's been discussed on the slides uh is the right middle class uh [Music] okay this will start very soon please take a seat here evie [Music] okay let's get started here welcome to the media with quick buff i'm magnus westland my coach here here is alan frindell so this is today's agenda we'll go over a few administrative things lead up in and then go into use case overviews uh looking at a couple of different use cases a bit of this a little bit of the potential solution space for future work but that's setting the stage for discussing what's possible not solutions then we'll have the discussion and wrap up so that's the plan for today so the note well"
  },
  {
    "startTime": "00:02:01",
    "text": "this is to remind you that the it applies and you there's intellectual property there's behavior codes etc so i have no takers thanks to jonathan lennox and stefan wenger if you want to help out in the hedge dock please do remind it to everyone in the room if you use the me take a light or meet echo to join the queue if you're using the full reminder remember to mute etc so so why are we here we are here to discuss if there are media delivery challenges that we have common enough understanding of so do we understand what it is about what's the problems are has issues or challenges that we have encountered so say in in trying to do things and not theoretical only um they are sufficient interest to solve they would benefit from standardization work protocol definition we believe that we can solve and is appropriate to solve here in itf we also want to see who's interested in contributing to working in space so this is a non-working group forming boss there are a couple of potential outcomes after this buff one is that there's one or more problems there's the sufficient support to be addressed interesting and then we go into some type of scoping"
  },
  {
    "startTime": "00:04:00",
    "text": "of the work figure out if there's an existing working group or you need to write a charter for it and then do the discussion around that such that will happen in that case on the mockbuff unless directed elsewhere and then either resharper or have a working group for above depending on how good the support agreement are around such a proposal the other alternatives is that we actually need more description discussion before number one can happen that's something that okay we are interested in doing something but we are not really agreeing what it is or we could actually conclude there are no problem to solve at this moment so so um that's kind of the scope setting uh and um let's go into the use case overview now good games i must go ahead james excellent good morning people uh so in in this bit of a presentation i'm going to be covering a couple of different aspects of the use cases uh little disclaimer before i continue this is views of mostly myself and also of spencer and not really of a non-existent working group um we're also going to be talking about uh why quick and describing sort of a high level overview of its of some of these use cases and their applicability in in sort of the tv"
  },
  {
    "startTime": "00:06:00",
    "text": "domain and i'm not going to be talking about requirements or solutions too much here so how did we get here we got here because we've had an awful lot of discussions about this over a very long time some of the first bits of documents and discussions were happening 2017 and perhaps even earlier um we've also had the mock mailing list around for a while we've had a few side meetings uh there's also been with with asides from uh the use cases draft that my self and spencer have been working on there's also been many other uh drafts that have been published out there that are you that have got some sort of an attempt to solve something that may fit into the use cases uh that that we're going to be talking through today and i'm not going to go through these but there's there's a few here uh that just shows sort of how long this timeline is progressing we've sort of had these these documents around for a couple of years now and this isn't a complete list um there's uh rips which is one other and there's probably some more out there that we've completely missed um so perhaps the big question we're going to ask is well we have all these other protocols that can ship media why would we put all this effort into using quick well quick although it is quite general purpose it it gives a lot of useful functionality that have have very useful applicability in in the media domain um this is just a list of these most notably uh the fact that a lot of deployments are going to be carrying it in order to to do http 3 and and other protocols as well and so we can take advantage of these uh and make and sort of make use of them as best as we can"
  },
  {
    "startTime": "00:08:01",
    "text": "about the use cases so um spencer and i've spent an awful lot of time since the last side meeting better refining the ontology of of these use cases and this is not complete uh but it is what we think enough to explain the scope and and the the area that we're working in and i've put latency on the other axis not because it is the most important thing but more just to give some structure to this um there's certainly many other uh requirements both functional and known that we could do it this is just one of them that clearly is very important for people and just sort of working your way from top to bottom we've got sort of what we call interactive media so that could be uh things from like uh cloud-based gaming remote desktoping uh operations both of those tend to be more of a one-to-one operation uh and then we have video conferencing which is uh either a one-to-one or a many-to-many um the live media use cases uh which i'll be talking a bit more in depth here are but are not the only important ones sort of we sort of split them into three sort of groups uh live media ingest which is where media is coming out of being sent on to somewhere so an example would be an output from a camera uh into a transcoder or some other into the broadcast chain syndication is a little bit more complex and this is sort of where it is being fanned out into distribution networks so for example when it comes out of uh out of the broadcast chain in in television and needs to be sent on to uh cable tv providers or uh over satellite or terrestrial networks and for live media streaming this is playback"
  },
  {
    "startTime": "00:10:00",
    "text": "this is this is p people consuming and watching live media and and and lastly i'm not going to talk about this too much is more of the on-demand media use cases where uh media is stored and and also made available for playback uh but for for all intents and purposes i'm not going to be dwelling too much on that one so uh to set a bit of context here uh this is a very simplified example of a television broadcast chain end to end uh and i've grayed out the bits that are perhaps not so pertinent for this discussion but i think it's kind of interesting to talk about sort of the end to end or the glass to glass call it what you will um and we start over on the left hand side with uh sources cameras either inside of studio or remote being fed into a central point which is typically a master control room uh and then managed and sent out into the distribution side of things uh such as for transcoding and eventually delivery for the viewer in the ip domain there is also if we zoom in for a little bit this this lovely green box here of live streaming and this is uh representative of uh use cases where we're talking about user-generated content there in these three sort of ingest arrows as it were the blue the purple and the green they all are effectively ingest but they do have some key differences in their requirements and how they work uh the two most notable differences between them are around sort of throughput and resiliency requirements and in the case of in some cases bi-directionality uh for outside broadcasts for example uh"
  },
  {
    "startTime": "00:12:02",
    "text": "it may be required that you send audio and video both ways uh say a person a weather person in bad weather having a interview with a news presenter in the studio they need that bi-directionality there when we talk about syndication this is again that fan out onto delivery networks and there isn't the the key distinction here is that there isn't any further transcoding or splitting up of the video there may be a transmuxing or repackaging up there's no further transformations to the video no further uh editorialization uh and then lastly we got to the streaming which should be fairly clear here which is is just people fetching the media and playing it out that's pretty much about it are the use cases clear do people have questions or do you have other use cases that you don't think fit uh into this space jake go ahead hello is that working all right um do you uh so when you're merging multiple media streams does that go under the uh like which section would that go into or is that just a sort of independent question because you have to synchronize them right so there's there's many points where you could have to merge what context are you talking about specifically well"
  },
  {
    "startTime": "00:14:00",
    "text": "um i mean i i guess there's there's several that i think of as as related uh those being um things like uh multi-language channels uh for the audio streams uh merging the video and the audio um you know the the sort of uh uh if if you have any sort of user driven uh switching of views um that is important to have synchronized on the on the viewer side um if you've got and so i guess things like overlays also might be relevant here although i'm not sure maybe that is a different section um so but these are the kinds of things i was i was considering so they are an important concern but i don't think that that's particularly orthogonal because you'll have situations where at any point in this in in the chain where there may be multiple forms of media that need to be pushed through so the cameras out there that can do audio and video and then there's others that are just video only because they're used in a studio where it's assumed that audio is supplied separately um and they're kind of like lanes in the highway as it were and sometimes they merge off and sometimes they uh new lanes emerge depending on where they are in the chain and what what kind of what kind of things happening so part of the sort of solution space that this uh this work would need to address is how to synchronize those properly like rtp style with timestamps or whatever yes that may necessarily be a an important requirement to solve okay just wanted to see if i understood where that fit thank you"
  },
  {
    "startTime": "00:16:02",
    "text": "so pete resnick just left a comment in the chat uh i i'm happy to answer this uh he says maybe the caffeine wore off but is there something about the use cases that are different in streaming that is currently going on i think the answer is no there's nothing different here we're just trying to be very clear about what is what what is out there what is the existing state of affairs in terms of where the applicability for this work might go no uh mozinati in in uh two more uh slide decks there'll be more descriptions about the use cases and i think it'll be clear what the real differences are between just today's traditional streaming and some of these uh new use cases that require maybe uh more advanced uh types of streaming lower latency types of streaming and maybe even more hybrid switching between extremely low latency streaming almost like video conferencing type latencies and traditional you know distribution delivery and for the comment about uh jake's coming about um i would take it as almost like control protocols or or uh um you know i didn't think that the the group was going to be looking at things like uh signaling and control protocols because you think about this you know at the bottom where there's media distribution and media transport then there'll also be you know feedback and control protocols and i didn't think that the people were in this group were going to be looking at the control protocols for things like user selection and things like that thanks spencer spencer dawkins uh i just wanted to add the draft that james and i worked on has a section trying to explain at a high level what the difference between interactive and"
  },
  {
    "startTime": "00:18:00",
    "text": "live and not live i don't remember what we called it uh media uh you know basically how we were fitting things into those different sections and i should also mention that the slides have that james is presenting have links to the different sections in that draft so if you're trying to figure out things uh if you click you know if you don't if you have a copy of the slides and click on them uh that that would uh take you to the right place in the draft thanks okay let's go to the next part presentation now so ying uh i will put up the slides if you don't want to drown uh you can go ahead when you see the slides do i control the slide or not can now you should be able okay thanks um hi everyone my name is ying i'm a software engineer working on youtube's live streaming infrastructure so today i want to talk about the challenges we see in terms of live media ninja protocols based on our experience so first to borrow james uh diagram on the live media broadcast chain here um i just want to highlight"
  },
  {
    "startTime": "00:20:02",
    "text": "the focus of the talk is on the live ingestion part of the chain and uh and also in particular um i'm talking about ingestion from clients to live stream live streaming platforms um so what i think is nice in this diagram is it shows the variety of different clients we need to support there are clients around the tv broadcast facilities where they use high-end hardware encoders or it can be ingestion from consumer level devices game consoles software such as obs or ingestion from browsers mobile phones so the diversity of the encoders ecosystem is actually poses some unique challenges in in terms of live ingestion so first i want to talk about some of the high level requirements we see are needed in live ingestion protocols so this is not necessarily exhaustive list it's just the requirements we consider that are important and aware that there are challenges um so first we we want to able to support high visual quality live streams um for example 4k hdr but not limiting to that we can see in the future there probably will be demand for even higher resolution higher frame rate content and this is mostly driven by broadcast level premium live events like sports gaming tournaments concerts but we also see increasing demand from general users especially in terms of gaming live streams um so this leads to the next requirement about coda agility in order to support high visual quality content we want to use newer video codecs such as hevc bpm"
  },
  {
    "startTime": "00:22:01",
    "text": "vp9 even av1 that can have better compression rate so that we can stream at high quality at a lower bit rate and also some of the new features like hdr also require requires new codec support so it is very important for protocol to be able to update it easily whenever there is new codec coming and next requirement is about being able to support low and ultra low latency and ultra low latency here we mean less than one second this is especially important for sports live streams and also game live streams where creators want to interact with viewers via chats here i want to highlight the need for the ability to do latency and quality trade-offs i think basically everyone likes wants low latency but um you theoretically can always drive down latency by lowering quality lowering a bit rate resolution or dropping frames but this may not necessarily be like always the trade-off we want uh for example uh if it's a live concert then maybe we can afford a little bit more uh latency in order to maintain high visual quality the next requirement we see that's important is ease of adoption among encoders for a new for an ingestion protocol as i show in a previous diagram because of there's a wide variety of encoders so how to make sure all of them can adopt a certain protocol is important and i think one of the keys for ease of adoption is ease of implementation and lastly we think it's important is the ease of large scale employment because we need to support many many concurrent live streams at the same time so being able to do load balancing correctly and"
  },
  {
    "startTime": "00:24:01",
    "text": "easily is important and also we need to do regular service update and we want to do that without interrupting the live streams so next i want to talk about the challenges we see in some of the current live stream live ingestion protocols in respect to the requirements i just talked about so first of all rtmp is one of the most commonly used live ingestion protocols and the main challenge we have with it is it doesn't have an official way to add new codecs the protocol is is not a little bit stagnant it's not updated the last update was like 2012. furthermore it uses a 4-bit idiom for codex so only 16 values for the codec you can see we will easily run out of values for new codecs and also in terms of latency because it is directly on top of tcp so you can easily switch to use quick for it so it has the head of line blocking issue um so a few years ago we also added support for using webrtc to do live stream ingestion we added primarily to be able to support a live stream from browsers in order to lower the barrier for new creators our first party mobile clients also use webrtc for ingestion the main challenge we have with webrtc is it's it's quality because webrtc is created for video conferencing so it adapts the visual quality down very quickly in order to maintain conversational latency so we have a challenge with using it for premium content so in order to support premium content"
  },
  {
    "startTime": "00:26:00",
    "text": "and also to support new codecs in order to do hdr we also added a live stream we also added support for ingestion using hrs and dash but these protocol also has its own issue where they have higher latency because they are segment based we understand that now there is a newer version of it that support low latency but we don't see a lot of demand like adoption from the encoders and furthermore because hrs and dash they're really created for delivery so some of its constructs like the playlist and manifest those are not really needed for ingestion but we still need to support it but they do add overhead in in live streaming um lastly the srt protocol is a relatively new protocol i think it's cr created mostly to solve some of the codec and latency issue we do see that it has a very good adoption among tv broadcast providers it is originally created for the point-to-point contribution use case the challenge we see is we're using it for large-scale deployment because it's because it is the udp based protocol so load balancing is not a given for um for off-the-shelf load balancer for udp and also its current life its current congestion control algorithm in the live mode may have some challenge for large-scale use so to summarize um we don't see currently we don't see a perfect solution for ingesting high-end content at ultra-low latency and at"
  },
  {
    "startTime": "00:28:02",
    "text": "large scale would you see that quick can provide a unique solution for some of the challenges like deployment and latency so we're very happy to see that there are movements in this area there are proposed um quick based solutions um and some of them are explicit specifically focusing on live media ingest some are and some are like the design is initially for delivery but they can also be used for ingest but in general yeah we uh we're really looking forward to collaborate uh through the ietf community to find solutions that can address these challenges that's all from me thanks okay bernard uh yes so i'd like to talk a little bit about the trade-off between the latency and the quality so when you implemented the webrtc ingestion you actually natively you actually change the congestion control algorithm to favor the quality over the latency i'm wondering if the same problem exists with quick particularly the bbr congestion control in its probe rtt phase and whether you foresee a need to change the congestion control algorithm to again favor the quality uh over the latency um [Music] that is a that is a good question um i think we really need some more uh experience with testing it i think like using the bbr and quick"
  },
  {
    "startTime": "00:30:00",
    "text": "i can't give a definite answer now but yeah now that that is a very good question i think we need more testing with that luke to answer bernard's question a little bit um i've implemented bbr for warp um and yeah we see the issues with uh probe rtt state i think it comes down to when you're doing frame based delivery you care way more about keeping queue sizes small and that's just never really been a concern for tcp and therefore quick based congestion control algorithms so i definitely see that at least it's going to be algorithms that should be suited towards live video and there's other issues like um my video is mostly application limited uh so a lot of the time you're not fully utilizing the congestion window and the algorithms aren't designed for that so there's just a huge swath of space for optimizing congestion control for you know any quick solution we do uh thanks spencer i want to thank ying and luke for bringing this presentation and the next presentations forward and i want to thank you both for uh answers to bernard's question i think that it's fair to say that the last time i asked about such things the possibility we just don't know that much in the congestion control iccrg sense of the word we don't we just don't know that much"
  },
  {
    "startTime": "00:32:01",
    "text": "about the interaction of various uh congestion control mechanisms that might be used from the same endpoint and how they would uh they would play with each other the you know the big issue for like scream the media control or media or congestion control things was that they would not self congest you know self-congest that they would that they would not uh that they could that they could share bandwidth between themselves and other in uh connections that we're doing uh scream and so the question about having two arbitrary congestion mechanisms i think that that's still really early question but i could be wrong [Music] thanks harold i love this john just one thing i don't quite understand you mentioned that webrtc was problematic because it scaled down quality when congestion occurred now in the live ingestion use case you have a definite latency budget right yeah so that means that when you hit congestion you have to either reduce quality or increase latency right right yeah but it's just that kind of fundamental to right to the information theory of space so the question is really whether you can get if you have to drop latency drop per quality because of increa because"
  },
  {
    "startTime": "00:34:00",
    "text": "latency otherwise will get increased too much whether you can get it fast back up fast enough i think and that is a common problem with every solution right yeah uh yeah i think that what i want to convey is the latency budget though is different for live stream versus video conferencing but the webrtc library that the commonly used library is beautiful video conferencing so it's really there are control knobs on it uh oh then um we would like to learn about that before we go on to next is close to queue currently let's see if we don't run out of time and so let's see where we are when we have gone through this cube you are using srt to do the fcme interest i wonder what's wrong with srp you mentioned it's difficult to do nice scale deployment uh yeah it's uh uh in terms of load balancing because of the the udp protocol itself right it's not connection based so um load balancing is definitely not a given i'm not saying you can't do it it's just more more complicated and um sorry hong i didn't hear if there was another part of your question or if you're done song udp and these connection id is"
  },
  {
    "startTime": "00:36:01",
    "text": "undertuned kind of encrypted so you still need to do more things to use balancing on click right um that's true yeah i think the basically i think for quick though um [Music] because it's connection based and also load balancing is much well documented i think that there are a lot of documentation on how to do load balancing for quick thanks uh stefan so i have a few things one is i want to support harold in saying that i don't think we have a fundamental problem with the webrtc protocol suite when it comes to the trade-off between latency and [Music] quality i think we may have a problem with the current webrtc implementations right we may not even have a problem with implementations themselves but a problem with the way how the implementations are configured so uh there's indeed a lot of knobs that can be played with so i i think it in that regard uh the slides may be a little bit misleading because as harold said and i agree with that the problem is a fundamental one which should occur in the same way with all the other protocols right so that's point number one point number two is uh i said that a thousand times i say it again congestion control is overrated here in this organization um the networks nowadays are sufficiently elastic that you know you can't push out more for a while"
  },
  {
    "startTime": "00:38:01",
    "text": "without any everything melting down certainly for and for a used case where you basically have only one stream like that ingestion right what happens in practice is people just ignore it and things work just fine so and that's something which would probably require a lot of or at least some software work both on the current webrtc uh senders and also in the quick stack potentially so my third thing is and that's independent of this draft here i think an independent of this light deck i think it's time that we put together a draft that defines our view on the terminology for those various latencies right when i hear ultra ultra low latency and i hear sub sections here something my brain starts spinning i come from the video coding field the ultra high latency there is meant like single digit milliseconds right basically you you start shipping packets before you have encoded a whole frame on the other hand some of the tv guys think ultra high ultra low latency is when you don't send a whole crop so a group of pictures right so someone should write a draft maybe i write that myself one day yeah and and just just just put that terminology problem aside once and for good thank you i'll just jump in and say that uh with respect to that last point uh that this draft that james and spencer written does have a classification of different things and tries to demarcate ultra low latencies lower than one second because i think there's an agreement that one second is not really then we need an ultra ultra low latency or super low latency or come maybe the easiest way would be just forget about the terminology and just"
  },
  {
    "startTime": "00:40:01",
    "text": "use categories like below a millisecond up to 10 milliseconds up to 100 milliseconds up to one second up to five seconds and up to an hour yeah in fact that i think the draft calls it like ull 250 and ull 100 and things like that so um thanks uh cullen so i want to just come back to the uh the congestion control issue because you know like it's itf there's nothing more important than congestion control [Laughter] [Music] the um i don't want to give the impression that people have not played deeply with the congestion control and quick we unders like there's a ton of different people including us have done a lot of experiments with us i've played with it i've implemented alternative implementations we've been working closely with christian on the quick implementation some you know some of these types of things what i would say though that's relevant for this buff here is that uh i i think there are things you could do to the quick congestion controller that would improve it for media no matter what type of media what what version of low latency you you think you're dealing with right uh and we know some of those and they seem like they can easily fit in with the quick framework i think that's something that's completely separate from this work can be taken to the quick working group of how to have something that's done for that but what this working group i think needs to focus on is given whatever you do getting quick which it is what it is today and you get other things is you know how do we build the type of uh rate adaptation and how do we interact with that or you know how do we adapt what we're trying to accomplish as an application layer on top of what quick basically gives us and it's very clear that you can get a fair amount of you can get a long ways on quick as it stands today with no changes and you could probably get slightly better if you made some changes to quick and those can be proposed separately over to the quick working group uh so that would that would be my view but i so i don't think we're talking a lot about the congestion control issues and how we might want to change bbr and all of those those issues here but i do think that people have thought about"
  },
  {
    "startTime": "00:42:00",
    "text": "that a lot and done a ton of experiments with that's it thanks colin okay victor's got the last word okay uh can you kermit yes okay so two things the first thing is about congestion control quickly the person who spent a lot of time working congestion control for life and non-live media delivery uh at youtube it does matter a lot uh to user experience uh and uh in non-trivial way uh the second is more to comment about webrtc uh and i'm having a little bit of deja vu here because when web transport was first proposed uh people assured me that things can be solved the web transport laws can be solved with webrtc data channels you just need to implement sdp and then implement ice uh things that run on top of it and then implement a ctp and then there are also dragons uh and all of that is true because webrtc is like network-wise during complete in the sense it can do anything but that doesn't mean it is the best tool for the jobs that is to say if we went through that there are people who tried to use it for delivering high quality live media and their experience was that it is not ready in the state it is and the answer the question would be do we really want to turn webrtc into something that can do this or should we start building with a more with a different starting point like quick and i think at least for me personally quick seems to be more promising because quick"
  },
  {
    "startTime": "00:44:00",
    "text": "dash over http 3 is a really mature widely deployed solution thank you ying and luke let's go on with luke um hello um so i've got some non-professional slides just to mix it up um but yeah i'm luke uh i work in twitch for a while now and uh um amazon um so i wanna talk about distribution mostly focusing on the requirements and use cases and you know maybe going a little bit into the flame war that is this protocol protocol can do x y z yeah distribution um the the main difference with distribution is uh versus ingest is uh this fan app mechanism uh it's no longer a one-to-one protocol you have to make sure that whatever distribution protocol is designed to be replicated and passed off multiple hops this includes passing off the cdns this includes just going around the world and fanning out to as many users as possible so one broadcaster multiple viewers this has some ramifications um the big one is you just can't have feedback you can't have the viewer send something to the encoder say hey lower your bitrate it just doesn't work because you have too many viewers trying to watch the same content um and uh so it does limit the space a little bit your solutions um uh you can't so that's why there's different protocols typically the other thing that's uh well i kind of mentioned here is congestion because we can't tell the encoder hey lower your bitrate we need to drop something we need to somehow lower the bit rate this is typically done using abr the idea is that you create different renditions and you can"
  },
  {
    "startTime": "00:46:00",
    "text": "i i've got boundaries you can switch between them it's very jerky uh it's very slow and dramatic you kind of have to plan in advance so if i'm running into congestion right now cues are building up my buffer's depleting and in two seconds i'll be able to like jump down five megabits per second all of a sudden um we don't really see frame dropping too much in distribution um but that's also an option that would be another way to reduce the bitrate uh and typically instead you uh these particles will buffer uh hls you you've seen it like in dash if you've ever just your buffer runs out because this abr wasn't fast enough for congestion control uh you pause and you wait till the buffer fills up um the other thing is kind of unique for distribution is latency so this i imagine what we talked about a lot uh i don't like talking numbers i think i see the chat people still arguing over like is 25 or 100 or right threshold it really just depends on the broadcast like what the pres and even the viewer uh what the preference are so um if i'm doing a presentation like this actually the latency doesn't matter that much it could be i put i call interactive latency like people in chat can say stuff but they don't need to be able to talk back to me um obviously if you have a conversation you want real time you want to be um you want to be able to talk to another person but somebody is also watching from home has no intention of interacting with chat and they can watch it lossless maybe they don't want to drop any data at all they want to see every pixel um like you know they're watching a soccer game so uh like for the soccer example you can have one one viewer might care about they need real time they need that their coach or something they need to see the the game asap you have another side where people are just um maybe they're"
  },
  {
    "startTime": "00:48:00",
    "text": "betting on the game maybe they're chatting with their friends they want lower latency there they won't be interactive and then the further spectrum you have even the same people watching the same game but they want to see all the action they don't want to miss anything uh so yes the broadcast matters but i think the big thing with distribution is the viewer matters and the viewers network especially somebody in the us on fiber can watch at a lower latency than somebody in brazil on a cellular network um just and the the way you tweak the buffer size is very important for distribution and hence the latency um and the other thing that doesn't get talked about too much is compatibility so um we have a lot of viewers uh and we need to make sure it works on every device uh in a broadcast scenario you can get away a lot of the times with saying like i have a studio or i have a dedicated software i can run on the person's computer to create this stream uh but for the distribution use case we just don't have that luxury all the time like it just needs to work on their tv it needs to work in their browser uh and it needs to uh uh it needs to work on their phone and again their network it needs to work wherever they are in the world uh you could be watching somebody streaming from the us you could be watching from korea uh again on us on a mobile device on a on a desktop and this has a lot of ramifications for the protocol um and uh of course that's what we run into a lot honestly the biggest thing with distribution i think these are limited uh to only a handful of protocols based on compatibility more than anything else so speaking of the protocols uh so twitch is an hls stack um specifically we're using what we call lhls it's a chunk transfer um variant that we developed um also if you want more details i sent out at google doc about this before uh"
  },
  {
    "startTime": "00:50:00",
    "text": "the talk to the mailing list uh but the main issues with all the different flavors of hls is uh it's got headphone blocking the idea is that all the data must arrive like you must download every segment and every segment depends on every previous segment which means every frame depends on every previous frame uh which means you need a large buffer to handle any variations in the network there are obviously uh ways to reduce latency like the fact that you have these different flavors of hls but you're still fundamentally limited by the fact that you just can't drop data and if there is a network issue you you rely on avr to switch uh to avoid it and to recover and that's just too slow um we also have some issues with client-side abr uh that's uh just worth mentioning because when you deliver download frame by frame it just breaks a lot of abr algorithms that traditionally use for hls um but it's not a fundament it's it's something that there's a lot of different solutions and we've done a lot of different things including just like almost literally running a speed test while we're uh viewing the stream uh just to try and figure out uh if we can switch up so um i was tasked with trying to reduce latency on twitch uh and we ran a big project to try and use webrtc uh again the document has more details but this is a hybrid approach where we took our existing ingest stack and just tried to put webrtc for distribution um the biggest issue with that was just we didn't have any control over the quality we wanted to have support these use cases where we don't need real time in fact we have like a in just stacked with a ton of latency and then we will try to use distribution at the very end and webrtc's forcing us to make it real time um we have to a lot of people bringing up"
  },
  {
    "startTime": "00:52:00",
    "text": "like well that's not a problem with the uh the wire format and that's true like webrtc as a wire protocol you can do a lot of stuff there's a lot of extensions you could do but again for web support we need the browser to work and uh it's not a great way of of conveying this information too um the implementation uh because we need browser support it means we have to petition google to change things we have to petition ietf to add stuff to the uh uh the protocol we wanted something that we had more control over um again using web transport and the idea is that we could send packets or we can we can uh do our own protocol and you see this a lot actually you see a lot of people using webrtc data channels to this extent and we did too i implemented basically rtp over webrtc data channels a crazy amount of complexity just to get udp working in the browser um and it just wasn't a good experience and uh yeah i've implemented every webrtc protocol at this point i've optimized them uh i ended up throwing away like a year and a half of work because we ended up abandoning this webrtc edge project um again this is some of our specific things i think anduin webrtc has fewer issues but still quality is an issue uh so uh our solution was to make something called warp um i released a draft of it a very simple draft trying to focus some more on the uh prioritization scheme uh the idea is that we can have a latency gambit we don't really do real time very well for multitude of reasons but it it just works with our existing stack it's cmaf so we can fall back to hls the server apr so we can fix some of the issues with uh latency hls um and that's really what we're settling on"
  },
  {
    "startTime": "00:54:00",
    "text": "um it uses web transport and each uh each segment is sent as a quick stream it's actually really simple but that's kind of what we settled on [Music] and um i don't want to spend my time like you know preaching warp that's probably a future off or a future meeting um but uh any questions about distribution no go ahead mozinati um hey look i asked you this earlier on the list and i think you mentioned that you were you had thought about it before and you're gonna still be you know continue to think about it in the future wonder if you've had some more thoughts on it now um but it seems like conceptually warp is would be very similar to having an h3 capable server and just grabbing the segments as you know separate resources over it and they would automatically come over with separate quick streams like warp does and then the prioritization the mechanisms aren't there yet but there's proposals to do something very similar to that prioritization do you think that there's uh do you think that there's more value in trying to pursue that route or do you think that there's something that would be fundamentally harder uh to make the right changes that you think you're going to need eventually in an h3 stack uh versus what you're what you're doing separately fundamentally we want to be able to download segments in parallel and prefer the newest data like if there's congestion um and you can do that like you mentioned with h3 uh you can i think lucas was working like a priority header would be a great standard way of saying this request should come before this one um the reason we're doing web transport really right now for distribution is just because twitch runs we run our own cdn like we don't need um uh to use http we're refined pushing"
  },
  {
    "startTime": "00:56:02",
    "text": "last mile but i think for standardization especially uh a solution that uses h3 or even h2 and looks more like dash uh would probably be a low barrier to entry um there are some other concerns mostly around server side abr but it's all solvable thanks lucas hello lucas pardue clyde play i hear my name mentioned and i'm summoned um no thanks luke luke and i have talked a bit about this in the past uh the hp extendable priorities draft entered or 48 like a couple of weeks ago or a few weeks ago now maybe um and this is just really about signaling and and some guidance on how servers predominantly servers in this case can use those signals um in ways to prioritize or schedule the sending of multiple resources that are concurrently active um that's just guidance effectively servers can ignore it completely or do whatever they like and but it's optimized around the web use case so it's kind of like a fiso general recommendation that could be ignored um and speaking to luke like months back it seemed more like this use cases is really like a a lifo um and that's completely possible you know you just you might need a slightly different signal um at the at the each independent priority message or you might just want a signal for the entire session that this is a different kind of application than web like these things are possible um all i'd say is it is optimized around a client and a server model rather than a you know maybe the video which is slightly different collaborate a little bit one of the issues with the using hp3 for warp is uh"
  },
  {
    "startTime": "00:58:00",
    "text": "if you talk to a server that doesn't support prioritization like it just ignores that header you're going to get a worse experience than just hls so like you this is something that lucas and i talked a lot about is you need to make sure the server supports prioritization if that was the approach we took hey look uh thanks for the presentation and uh thanks thanks for going us through uh various things that you tried out and i had a clarification question uh the more i kind of read about low latency dash and talking to the experts it does feel like the some of the things that you require kind of can be solved with the low latency dash does switch or your team ever try to experiment with that and figure out something is not working and why it's not working or is it something not done so far our own low latency dash solution um it's a custom hls thing instead of dash but same concept as far as i can tell okay i think there's quite a bit difference in how templation templatization and other things but yeah it would be good to know if that's explode at some point thanks tell us we advertise segments in the playlist hls playlist before they exist and then the player will start downloading them the chunk transfer it'll be delivered frame by frame um so not the exact same as lillian's dash but pretty close okay sergio hi hey one question from your presentation is that regardless would you be interested in media uh media real quick or specifically about media platforms because they are slightly smart but i think that we need to first start thinking if the goal is to have medieval quick or if we really are requiring weight transport"
  },
  {
    "startTime": "01:00:02",
    "text": "for them for them specifically for building blues because i think that this has some extra challenges that are not present if just using quick um quick is mostly required for to better eliminate headline blocking but there's actually some other benefits with web transport like h2 fallback would probably actually work thinking about it um it wouldn't be ideal you'd still have some head of line blocking but it would be nice uh but really we just need browser support and like you can't get quick native in the browser without web transport so that's why we're focusing on it yeah because we have the problem of the with a specific link with traffic how to plug then congestion control of them other of the quick implementation in the browser with the with the user specs so i think that uh speaking about media equipped without a specific statement that this is going to be used with web transport it may lead to some issues when we try to actually implement them to elaborate the um we don't have that issue because the sender is a server we control we have a custom quick implementation uh but 100 if you're doing ingest over web transport um via the browser uh you do not control the prioritization of streams and you do not control congest control um and that would be an issue so the queue is closed but there are a couple of um comments to be relayed from the chat kirill uh says that refreshing manifest for ll-dash is not fun and lucas needing per request signals to express priority introduces an immediate latency cost whereas my understanding of warps needs is that a declaration that the session is best served lifo will avoid that"
  },
  {
    "startTime": "01:02:01",
    "text": "entirely um and with that i think we're ready to move on to colin's presentation great thank you can you guys join me fine yes okay so next slide please um you should have control at the bottom oh yeah thanks um the i want to talk i'm going to talk a little bit more about the various sort of solutions that have been talked about and try and abstract those out to the the type of thing that i the direction that we could choose for a working group would form to go but to clarify it up i want to talk a little bit more about a couple use cases here and i hadn't seen it into the mob spot the other day this uh report from dash if but it has a great set of use cases i highly recommend people reading and and of course i think it's great several use cases i've put beforehand where we're in there um but it is a good summary i'd encourage people to read the links at the bottom now so this case i want to talk about right here is is one that that often comes up of you know you have a soccer game or some people uh a sport like that and you're watching and there's a bunch of different things that people want to do that aren't really very easy to do with the protocols we have today and that's one of the things that in general when i'm looking at building some new complicated thing that's difficult to deploy i want to make sure that i have some features that are not just better versions of what is already deployed today but are actually solve problems that people want to solve that you can't do with the stuff that is widely deployed today uh otherwise you know you end up with a very difficult deploy path so uh this you know the soccer game uh example when we start looking at this there's um a desire to be able to watch a camera over the goal post when you're right in the in the same stadium watching the game it's very common now for people to be streaming those types of things there's the issue somebody brought up later of the sort of timing and latency of everybody receiving this"
  },
  {
    "startTime": "01:04:02",
    "text": "uh so that we have a low late that the end viewers are getting a fairly low latency version of the game so you don't hear all your neighbors cheer before some people don't care about the problem you know some some do right and this one variant of this that gets particularly into that thing is when there's betting going on um for so for any anything that has regulated betting on it is highly likely to have very strict latency needs and you don't want somebody who's just sending a you know a a webrtc video of the game from their phone uh to somebody else remotely to be getting a substantially uh lower delay than whatever the betting delay is so you know that's that's another one that comes up the other one that we sort of see is that you have a large large number of um people viewing it well beyond where webrtc is typically scaled to today i mean most the large-scale webrtc systems support single conferences up to around a thousand people obviously you could do more than that with webrtc less but it tends to be in that sort of framework is what's cost effective uh the but if you have uh you know 100 000 people viewing this and you want to be able to bring in any one of them as a participant as an interactive person on the call maybe it's any you know esports and you want to bring in one of the fans or spectators as part of the commentary that's going on live in it now the moment where you bring those people in is you don't want to lose five seconds of latency that that person doesn't hear when that switch happens because that's exactly the context they need to comment when they come in so you have to keep that means driving the latency of everything down so i think there's a bunch of edge cases like this where we switch back and forth um let me talk about a slightly different version of this uh this is large company meetings right now if you have a a meeting even like this one right with only i see we got 129 people remote right now uh if we allowed all those people just be active speakers at the same time it wouldn't really work that's why we request the"
  },
  {
    "startTime": "01:06:01",
    "text": "queue and go do these things so it's usually a signaling mechanism that allows people to move back and forth between interactive and non-interactive mechanism now we can usually do this with webrtc today uh for small meetings but it doesn't scale very well as it sort of gets large so that's that's certainly uh a use case that's coming not from the streaming direction but is coming from i'll call it the interactive media direction and what we see is more and more use cases where coming from both of these groups that are converging to the same thing where they want to act a little bit more you know the streaming people want to be a little bit more like the interactive people and interactive people want to scale like the ability of the streaming people the sk the design that we have for scaling for streaming today is amazing the other thing that we sort of talk about a lot is the way that streaming uses uh relays and distribution points on the the distributions is very clear we're all familiar with those types of things but there are also a lot of things that can be gained by similar models on the ingress and what we're the the main place where you have gains here is if you had a relay or a cache node some type of thing that's on the other side of a low of a a network like a wi-fi i mean in many cases we see the actual problems that you're hitting with the loss of packets various congestion control issues and stuff happening very much at the wi-fi network or the the very last edge and if you can insert a relay there you end up with a very low latency between the end client that's reproducing the video and this relay and you can do re-transmission based you know you can run quick over a stream and get your data really quickly reliably across that link and then put it up so basically we all understand the sort of issues of uh how rtt and congestion control relate with each other and re-transmissions but if you can reduce your rtt dramatically by inserting relays at key points uh you can change this and this really comes to some of the things that are"
  },
  {
    "startTime": "01:08:00",
    "text": "changing in the network today i mean the what's happening with 5g and the ability to have uh compute very near the edge what's happening with very flexible access points to push stuff down in them allows us to have an ingest side cdn effectively as well that dramatically improves quality so a bunch of this is all changing so with those sort of use cases in mind i want to jump over to the uh looking at sort of the design solution space and i'm not arguing for any of the particular drafts one way or another here or anything i don't really care about that i'm sort of pointing out like like a bunch of different people looked at this different ways and we're all coming up with very similar things okay we're we all have some way of publishing and pushing the the media and over quick that's why this is called over quick and they um you know they they tend to put dependents uh video frames like a got sequence in uh one stream uh they tend to have some way of trying to think about how to allow multiples of those happens maybe how to prioritize them those types of issues the the most important thing for us to focus on initially is a little bit what this pub line looks like and whether it's rushes you know just bringing on ingress uh warp uh you know does it can do both what directions on this uh we we need to have that uh video together we need to uh think about how the congestion control relates to it so i do not think that this work should as i said earlier i don't think this work should take on any of the congestion control i think that should happen in quick quick provides you what it provides you and we can maybe improve that for media but it also means to provide us like right estimates because we need those and we need to be able to adapt to them quickly and we need to be discussing the strategies that we're using for the latency i mean i luke got me on this term of latency strategy and i i think that's a it's a very good way of thinking about that's what we need to do and this is one of the things too that comes up constantly is well couldn't the ingress and the you know can what's coming in and what's going on be completely unrelated to each other and"
  },
  {
    "startTime": "01:10:01",
    "text": "the answer is basically no they can't they are highly related to each other they don't have to be the same protocol we prove that and obviously we'll gateway to different protocols but whatever your strategy is for dealing with latency on one side if your strategy for dealing with the latency on the other side is totally mismatched with that you have problems every time somebody says oh no we don't they can be completely separate they inevitably have a set of assumptions and models about what the output is that they're trying to get their input to match up with and those those outputs could be different and you know jake brought up some of those issues earlier on some of these things so i do think that we have to deeply consider both both sides of this together and they have to be designed to work together uh to meet the goals that we want or won't happen i'm not heard any compelling reason at all why they can't actually be the same protocol for both how video and audio media moves you know on the ingress side to the egress side the um let's see if i want to make sure i'm not missing too many things i want to say about okay so uh jumping into a little bit more the other side of this here the latency part of this is a little bit of a red herring in some ways we talk about what constantly people are trying to figure out a definition for you know what what different latencies are but the reality is is that we can get a certain latency over quick and we can do it pretty reliably today um the various implementations measure this and tested it um our stuff is currently sitting on top of pico quick and we've uh it's we're open sourcing all the implementations of it people want to play with it but i'm not arguing for that one i'm just saying that we've everybody that was working in this space has implemented enough things that we sort of figured out what we can do over quick now we can get the sort of latencies that we expect for interactive uh webrtc type things uh over quick over this these types of protocols that all"
  },
  {
    "startTime": "01:12:01",
    "text": "of these things are designing okay so i don't think we should get too wrapped up on exactly what that is i think that uh that to improve beyond that you have to change quick or use something other than quick uh and that that's really out of scope we should just be like hey we can get what latency we can over that and get those use cases the other thing that i think that we sort of fuzz sometimes on this and and don't talk about that we should take an account into the design very much is the relay issue so all the large-scale existing deployments of these very things streaming or interactive have these i mean the cdns have cdn nodes of some type they might be very much http caches they might be some sort of extended support to http type cache that's designed specifically to work with hls um they might be something completely you know internally designed twitch may have designed something works just for them right you know those types of issues on the interactive media we call these you know cascaded sfus and if you look at how conferences like large scale conferences that are involved people across multiple geographies work there's always multiple relays and distribution points and sfus involved now there's a tendency to say well let's just pretend those don't exist in our design we won't think about them and then we can insert them in later and i don't think that that's working out very well for us i think we need to explicitly consider the relays in the design because they they really change it they're one of the complicated parts of the the cost points to change um they also you know the our current sort of you know i right now there's a lot of people figuring out how to you know remove all their private keys from their tls private keys from their cdns in russia before they're seized um you know like just this faking the origin in a relay may not have been the best design model for it so i think that we do need to think explicitly about how these work and what we need to do and"
  },
  {
    "startTime": "01:14:00",
    "text": "recognize that for this to be a successful deployment they're one of the people that have to have an incentive for the relays to deploy and work um this you know a lot of the stuff too of you know that came up earlier about webrtc or not i mean i you know i think we could put anything in a browser as long as there was an incentive for the browser to put it in there and it meant the safety and security constraints that the browsers promised to their users as part part of it going on in so i think changes are possible there too including doing a whole new protocol um i also think that everything we've been talking about here just trivially you can map it on top of web transport uh or on quick raw you know that's the type of thing that could be worked out very much in the working group much later what we need to agree here is just sort of the overall scope of these things kind of coming together so uh with that i want to just sort of end with where i think loosely we should be going and having this and um that is i think we should have a sort of north star of an eventual solution and architecture that solves a lot of the hard use cases that we want to be able to deal with and that's really important to have the incentives to make all of this and the incentives to deploy it those types of issues now that said i don't think we have to do all of that day one i think day one we can start on some of the use cases that are most important to people which have really been over what were the pub arrows in all of these diagrams uh of how to push this stuff around and you know i i i think that where we need to be there is we need options around figuring out how we prioritize those how we put them in streams how we put those and things and we need to rely on the fact we're sitting on top of quick existing congestion control i think that a lot of the current discussions we had today are printed pretty lacking on discussion a little bit about how audio fits into some of that and probably some of the proposals you know that that's how audio fits in there is going to require options for datagrams in some cases or on streams i think there'll be some things where you can you need to choose both of those"
  },
  {
    "startTime": "01:16:02",
    "text": "uh content naming and how we do that we sort of brushed off i'm not saying we should do signaling but i do think that you can't if you want to build an interoperable device that connects up to somebody's ingress streaming server or not you you have to understand like you know people say oh it's just in the uri or something yeah yeah that's great that's fine but you have to say how that is and you need to realize that how we expressed uris and stuff is one of the major costs in some of the existing cdn deployments and maybe that the incentives to cdns are designing that a little bit more optimized for cdns instead of forcing onto cdn's something that was not designed for them to start with or whoever's doing the relay node type ideas uh i definitely think that we need to design the ingest and the distribution to work together i actually don't really see any reason their protocols would be different for pushing the video around but maybe somebody can convince me um and uh there's the question about webrtc i mean my view is not that we couldn't do this on webrtc i mean webrtc is a beautiful cathedral and we could definitely add some more gorgiles to the edge of it to do whatever we want the problem is is just the complexity the size putting into clients the the getting new things out so i think that that is one of the biggest concerns in thinking about whether you should do this work or not as well can you just already do it some other way and if the answer to that's yes but my view is it's pretty pretty complicated and that really the rtp was not particularly well designed for a large scale distribution out in relay content zone and i know that rtp was designed for multicast and really what i'm proposing here is an application level multicast but i think rtp and its practical usage has moved a long ways away from being a multicast or an application layer multicast network over top of it so i just think it's a very very difficult thing to map over to what we need here particularly on the distribution site"
  },
  {
    "startTime": "01:18:02",
    "text": "and with that let me just go to questions yeah this is clarifying questions we'll soon go into the discussion part so if you have any clarification questions to call them please go ahead spencer [Music] spencer dawkins this may be a clarifying question or the beginning of discussion but i turned my hand on to ask to talk ask luke about uh his comments on latency and uh i appreciate uh cullen putting up a slide about doing things with no relays uh and for the reasons why he was you know why you named that um but luke's categories included a thing called international and i you know don't have to have an answer here but i think the question is worth us asking what does that mean does that mean uh that it's a long way from our cdn or that it's a long way over our cdn with and uh over a lossy network at that point or something else but like i said your your comment about doing things without relays i think really speaks to that and i think i think that's yes stephon is still here so i can say the draft that he offered to write may"
  },
  {
    "startTime": "01:20:00",
    "text": "be very very useful for that as well thank you i will say something about that i mean i i think that it's very easy to design something that works well on good networks and that comes back to uh some um stuff somebody said earlier like you can just ignore you can just send stuff as fast as you want and just sort of ignore all normal things about networks and it works a lot of the time it really does um the problem is it's the hard cases where it doesn't work i mean if you have if you have a service that doesn't work in five percent of network connections and look just sending with no considerations whatsoever will fail more often than that it's not going to be a viable service i also think you need to be talking about is really a lot of countries so i'm very familiar deeply familiar with uh deployments that work in you know most of the countries in the world and what it takes to have a provide a reasonable quality interactive media in all of those environments um to do that and this is very common across all of these major services and even different things you see people today using at least about 100 points of presence for their cdns or sfus or whatever to get the latency low enough to deliver the type of things they do today and that's not just for web conferencing that's across a wide range of things that are interactive you end up about that many points i think we're about to see a massive change in the general cdn networks where the number of points of presence that you have just goes wildly up if you look at delivering something like stadia um or that sort of interactive gaming type thing i i i my my belief on that is you need a lot more points of presence than that and that we're going to get them and so you're going to see people selecting a cdn like which cdn they use like in a very small area be able to choose from different ones versus just i got us west and that's close enough i mean i think it's going to be down to you know per city levels or even lower so i think that that's one of the things that makes this type of protocol exciting now is to be able to take advantage of uh a cdn market that's going to rapidly change over the next seven years right so i i was uh so i've been thinking about this"
  },
  {
    "startTime": "01:22:00",
    "text": "as a transporter i'm sir sit down you're rambling come back and i think i could cut so there are people in line behind me for clarified questions so i will i will step to the back of the line for them yeah so julius um yes so that's just a quick comment when you're speaking about doing a group of pictures over one stream i'm a little bit concerned about how it can generalize to svc so when you're doing this pc you really want to avoid being blocked by a higher layer packet you want to avoid suffering from head of line blocking and i'm a little bit concerned that we might be painting ourselves into a corner here by making this choice you're 100 right i totally believe in scalable codex i believed in them for way too long and they keep not deploying but i mean i i think yeah and i think this is exactly the work the working group should do is nail down how does this really work if you look at our actual proposals both going back to ripped and then coming into the quicker stuff i mean yeah we want to have different we want to be able to put different layers in different streams with different priorities to be able to deal with that type of stuff so um take that as a gross oversimplification what i have on that slide you're 100 right okay luke quickly i wanted to answer spencer a little bit and add maybe a little new requirement sometimes relays are expensive sometimes like we have for twitch we have a lot of broadcasts that have zero one viewers um we don't want pay to send the data over a backbone um over these relays so one of our requirements is that yes literally they can download a stream like over the atlantic right like rtt is huge uh there's a lot of congestion because they're using transit but that's way cheaper for us"
  },
  {
    "startTime": "01:24:00",
    "text": "pros and cons thanks uh sanjay quickly hi kalan this is sanjay mishra verizon um very good presentation and i think obviously it's very exciting to see some of the conversation here but um looking operationally um folks that are actually implementing uh real-time distribution with webrtc today um and doing a lot of investment in that so what would be really for them to sort of turn around and say that you know this really offers me something better and i need to redo my distribution so so what's in there you know is it the congestion control that's going to be the thing that would cause them to move or what's the consideration here for folks that are doing investment now in webrtc to consider and move thanks so you know like i work on webex which has a huge investment on webrtc and being able to use that to bring everyone in it's been it's amazing good but the the thing that really moves us is dramatically lowering the cost of scaling out the distribution while still keeping the latencies we want um it would be you know the thing that would really drive us to to move to this i think there's other people that are very much coming more from the streaming direction where they have uh you know a great distribution protocol right now um and they're struggling to have a a good ingress protocol and some of them are finding webrtc too complicated or some of them uh are finding the other options that they have as an ingress protocol are not really driving the the real-time latencies they want and being able to switch back and forth some of these use cases but i think you are hitting on the part of the problem here which is do we have something that is enough different and enough interesting that it will really take off over webrtc over just using webrtc to do a similar thing and that's that's uh that's a hard question to answer without digging into a fair amount of the work"
  },
  {
    "startTime": "01:26:00",
    "text": "okay so thanks colin and we're going to move into the sort of general discussion we've got about 25 minutes for general discussion uh there's just want to acknowledge there's a lot of people here there's a lot of different backgrounds and use cases that are important so um everyone just you know and some people very passionate about their favorite protocol and whether it scales so try to keep it civil and you know try to bring you know new information that's gonna like help drive the work forward and please be brief and try to avoid rat holes and buy trades thanks now welcome to queue up spencer that guy i'm spencer dawkins and justif just to follow up on uh my second thing with cullen was that the iab has spent a certain amount of time thinking about centralization and so the decisions we make about about operating without relays and what the requirements really are and things like that may have pretty profound implications for how centralized the internet becomes either more or less thank you thanks bernard yeah um what i've heard is uh and i've heard this in other places as well is that there is a real uh interest in getting a new ingestion protocol for the reasons that ying described so i think there's definitely something uh that needs to be done there but one of the things i've noticed in reading the requirements for different ingestion protocols is they are subtly different so i'm just wondering if for example something like wrist seems to have slightly different requirements than what we're talking about here so i'm wondering if we can bring some of those folks in so we get a very clear problem statement"
  },
  {
    "startTime": "01:28:01",
    "text": "there thanks thank you tom hi there tom hill bt um i was actually quite interested to see the hierarchical unicast delivery sub-pub mechanism uh in the the architecture draft for quick r has uh has there been any discussion i can't see any at present but has there been any discussion on a multicast delivery model as part of this and or is there any part of the architecture that precludes that from being possible in the future colin did you want to jump in and answer that real quick uh i do um yes um and that there has been a bunch of discussion about it i i don't think it really has resolved and it comes down to an argument about how deployable multicast is and people always say multicast is not possible to deploy while selecting their printer within dns and watching their video for video on demand over a multi-class service run by cable companies so uh i think that that there is so the way i have described quicker is there's many ways to think of it it is both a you know a name data networking type protocol but it's also an application layer multicast and it's also pops up us i think that if you had the quicker or model like that working as an application layer multicast if you were in a constrained environment that actually supported real multicast you could go to one of these relays and the relays could reflect the data into true proper multicast inside of that environment um and then pull it out uh one of the reasons that i haven't been pushing building that as much is multicast on wi-fi now like if the more we move to wireless and what multicast means on a wi-fi network or a cellular network is it's a pretty bad story so i think it all gets complicated but it's"
  },
  {
    "startTime": "01:30:01",
    "text": "one of the things that i really like about this architecture is it opens the door to having some of the segments use real multicast thanks steven so the uh stephen wenger so the question here was where should we focus on and my personal preference is to focus on the way uh in the distribution side of the story to get to go away from the three seconds or so latency that we have at the moment when we switch channels on a tv right that's just so ridiculous if we could do something there that's where the money is on the interest side i know we are not in the good old times when when the tv trucks were driving around and satellite links were used and so on but it's still really small and i also question a little bit whether real standard solutions are required there i think the world has survived so far quite well on a handful of let's call them industry standards uh and call them standards for for because it sounds good yeah but in fact it's it's basically uh documentary implementations right and that was good enough and i don't see a reason why why an organization as big and cumbersome as us needs to get seriously involved there so i would i would go for for the distribution that's where the money is thank you thanks stuart just a quick comment i've been sitting here wondering whether to say anything and i think i will someone made a comment earlier that"
  },
  {
    "startTime": "01:32:00",
    "text": "congestion control is overrated and i've heard this from other people so i just want to describe that the engineers that have said this to me are the same engineers that write their own protocol that runs over udp and separately they report yeah sometimes it works and sometimes it doesn't sometimes it just spectacularly fails and doesn't work at all and nobody knows why the network's just not good enough um and they don't do not see the connection between these two statements so so i am very optimistic about uh media and other things running over quick for people who for whatever reason feel like tcp doesn't meet their needs quick has much richer semantics with multiple streams and priorities but it has a competent congestion control algorithm underlying it all so that's why i'm very optimistic that many of these homegrown protocols running on top of a competent congestion controller will be better than running over raw udp and sometimes they just don't work and nobody knows why thanks pete stuart's tall uh pete resnick um so i've been sitting here still trying to figure out what this intrinsically has to do with quick it sounds like there's new protocol to be written um at upper layers i know it's quaint to talk about layers anymore but and and it sounds like quick has some stuff underneath it that will help those protocols work better but beyond an applicability statement that says this is why you should use quick underneath because these things in this higher layer protocol are going to work better or work at all this doesn't seem like a quick thing this seems like media protocol stuff"
  },
  {
    "startTime": "01:34:04",
    "text": "exactly i i may just jump in and express a personal opinion which maybe which is that um quick brings a lot to the table sort of off the shelf that can be reused and that it can lead to simplicity at higher layers that may come at a cost as well but i think that is you know sort of it's one of the people who got like us talking about media over quick i think that's one of the drivers for why quick is built into the name of there right and you know i i don't want to dismiss that at all it's just i don't think that this working group should limit itself in the sense that if someone comes up with the cool way to use sctp to do that and is deployable i don't know how that works but okay god bless them they can go ahead and do that but that this working group shouldn't be thinking necessarily in terms of quick but in terms of the underlying structure that it needs yeah thank you um hi um a couple a couple of points just responding to pts that i totally agree um so when we proposed quicker uh one of the things that was called was media delivery protocol um and and the initial uh uh thinking was around udp there was no quick as such uh but but that goes to the point saying that it's a media relevant protocol that this this working group should solve and yes if we can provide uh the benefits we should definitely use use and the future versions of quick provides additional benefits that gets uh looped in uh into the delivery protocol and also i i feel that uh it would be worth for the group's time to see what problems and use cases that we bring in that today's solution do not solve our inventory solutions do not uh scale out or in whatever uh problems that we see like if it's a simple question about why something like low latency dash cannot work on the distribution side uh"
  },
  {
    "startTime": "01:36:01",
    "text": "and at the same time some other things i would also like to know about is how do how does the internet security come into the picture would just be that something like uh traditional resolutions it would be hard to get something like that working or not and also one of the things i am very curious or interested to solve is that how do we avoid multiple uh encapsulation decapsulation of the media that's going from the ingest to the distribution from the from to the to the to all the viewers and and not having to have uh multiple layers of encapsulation capsulation and have efficient pipelining across these intermediate nodes that way we can we can achieve really good quality and latency combinations as well thanks thank you i remind you about being brief james my response to stefan earlier around focusing on distribution and not focusing on ingest is that there's this two two things that should be considered uh the first one is that ingest protocols are becoming very diverse and very uh spread out covering a whole bunch of different use cases and maybe coming up with something that can harmonize some of those problems down would be very very useful it would certainly be useful in my domain uh satellite trucks permitting uh and the second thing to consider as well is that a lot of distribution protocols end up getting reused as ingest protocols hls dash and rtp rtm ping all end up getting reused as some means of contributing into a distribution platform right there's many cdns and other services that have got uh that as a thing and you've got to ask yourself well why are they doing that and the answer is is that if your input protocol and your output protocol the same then it's easier to reason with the architecture of the overall system and and how things function as well so if we"
  },
  {
    "startTime": "01:38:01",
    "text": "we don't focus on the ingest up front then we certainly run the risk of it being bodged in in the future thanks victor i want to comment on the question of quite quick specifically and not other protocol so i've at least with google we have at least three different teams that are dedicated to running uh maintaining and transfer stack there is a quick name there is a team that does work on improving tcp congestion control and other aspects in linux kernel and there is also a team that works in rtp and or some other protocols like sctp we also use in some cases and supporting those is not free it takes a lot of effort it takes a lot of very specialized knowledge uh and a lot of speak investment is specific types of codes like cpu optimization and we spend a lot of time optimizing quick for cpu because delivering video is very is at scales that youtube does just takes up a lot of resources uh in that sense you being able to build upon all of this effort rather than building something new from scratch is a very important plus and dash over http free at this point is an extremely mature technology and the reason i believe that we should do this is not only from standpoint that this is a good idea but"
  },
  {
    "startTime": "01:40:01",
    "text": "this is also something that many people in the industry independently have built facebook built trash uh twitch built warp and they work on this fundament on fundamental is the same principles as they use quick avoidance of head of land blocking to bush media and i believe this means that this is the right direction because multiple people independently came up with a solution and it is worth standardizing it so thank you victor please be brief uh we are closing the queue very soon uh ted hardy speaking and i came up to kind of pick up on something that magnus said in the chat a good while back which is that fundamentally what this really is a discussion about whether it's time for a new media delivery protocol and the point he made in chat was that quick coming to the fore has kind of created a straw that broke the camel's back or i would say that broke open the dam of discussion about whether or not it's time for a new media delivery protocol and i think most of the people i've heard at both in the in the mic line and on on the chat have basically said yes it's time for a new media delivery protocol and the question is why is it that quick broke the dam and webrtc did not and i think the interesting thing about that is actually to go back to the point about multicast models multicast models are fundamentally a public publication that subscribe somebody publishes a stream and you subscribe to it and those publication subscribe models especially with the kind of fan out that typically occurred in application layer multicast are much easier with quick because of the infrastructure that's been built up in the internet so it's it's simply the case that cdn"
  },
  {
    "startTime": "01:42:01",
    "text": "infrastructure is friendlier to to quick because it shares h3 as one of its use cases than it would be to building something new on top of webrtc so i think the question we really have in front of us is do we want a new media distribution protocol and i think my answer is yes do we want it to be based on something where the fan out possibilities of publication subscribe have this scaling property of quick and i think my answer to that is yes and then i think we have kind of the question of if we're going to do multicast and unicast for this how many of these use cases are actually in scope and i think my answer to that is again yes if we're building a new media dis uh distribution protocol it should actually cover all of the use cases that have been described because then we get the best network effect of building on that infrastructure and building on the code bases that are there so uh people have said we we named this wrong sorry we bike shedding the name of something is one of our favorite occupations at the uh at the itap uh if you if you want to call it new media uh delivery protocol yes or no buff i think at the end of the day chairs your your your your question will be very quick because i think you'll get a resounding yes to its time thanks mel uh miles and eddie i'd like to briefly touch on three main themes that i'm hearing from a lot of people so uh one on quick uh versus non-quick two on ingest versus distribution and three on um latency trade-offs so on quick versus not quick i think all the discussion about webrtc and uh god forbid sctp directly uh um or srt or other things i think a lot of people have already done a lot of soul searching and we arrived at quick for a reason it wasn't that we ignored uh a lot of us have"
  },
  {
    "startTime": "01:44:00",
    "text": "deployed you know uh as higher scale as possible all those other solutions and we've arrived at quick for for very good reasons so i don't think we need to rehash all of that and for the ingest versus for the contribution versus distribution i don't think that there is really that much difference when you look at the uh all of the the different needs at the lower layers and even at the app layer between those two cases there's a few nuances contribution you know may have more nuances about both live and pseudolive that i want to get this stream up reliably anyway but give me the live edge as fast as you can but also make sure that i can give full you know contribution uh you know at a delayed time that's the only nuance for a contribution and distribution the scale is obviously a difference but when you look at the lower layers of the protocol required to deliver those things there's really not that much difference and the latency trade-offs i keep hearing the trade-off between latency and quality i think those are you know false false trade-offs i think harold put it well but i don't i think was kind of lost in the room the channel is your limit the channel governs your quality you can't get better quality by adding latency if you're talking about a short-lived you know five-second ad maybe but not these long-lived flows that we're talking about we're talking about distributing events we're talking about live streaming you know many minutes or hours worth of content you can't get better quality than the channel by uh by just increasing your latency so that's a false dichotomy what latency um impacts is resilience how how resilient is your delivery of that um and so we need to take latency as a as a trade-off between resilience not as a trade-off between quality and the same applies for contribution and distribution ultra low latency can be achieved with the best quality that the channel can provide in both of those cases thanks i'll just say we've got five people in the queue and five minutes left before we're going to end the"
  },
  {
    "startTime": "01:46:01",
    "text": "discussion so 60 seconds david go all right i'll speak really quickly davidskenazi quick enthusiast uh first off i'm not going to beat over the dead horse of let's do this over quick of course quick is awesome let's do this over quick people have said why and that's good second as speaking as an internet architecture enthusiast i'd like to offer perhaps a suggestion to the proponents of this buff um we've seen that like not going into the buff questions because this is not working forming but clearly there's interest and it sounds like it makes sense for this to happen in ietf and there's people who want to do the work that's all great what i worry about is we keep hearing like all these like use cases which are awesome i would love to see a list my concern is i'm not entirely sure if we can agree on a list of requirements so what i would recommend for the proponents is to have side meetings and refine this list uh in in and if you can agree on some uh on a list there that's something that we can then have a walking group forming buff because if we agree on a set of requirements that means we can tightly scope this for a working group and then start building a solution that meets those requirements right now i'm not enough enough of an expert in this space to be able to tell if all those use cases have a single set of requirements and that worries me a little bit so that's what i would love to see thank you thanks uh cohen um i just want to speak to the ingress uh issues and you know the this comment made earlier about like hey it's the distribution that's really where the money is or whatever i mean yeah there's obviously that's a larger scale side of it but ingress is where we're failing the worst right now i mean um if you're working if somebody are using non-proprietary setup for their system applications probably the most common ingress is rtmp and i mean that's that's really old it's out of blind blocking over tcp has all kinds of issues people have mentioned and it's"
  },
  {
    "startTime": "01:48:00",
    "text": "not like you can build a device that will work with anybody's ingress i mean when we do our tmmp ingress to facebook versus youtube we have to do it differently and like you have to understand all of that with those things so i really think that we can't just pretend like ingress is so currently solved right now i think it's it's an area where you know the state of the art is like hair on fire and that needs to be part of what we're looking at thanks all right thanks colin justin why quick uh question yeah i think everyone here is very excited about the fact that we're now going to have a protocol that's extremely widely deployed both across clients as well as in public cloud architectures and that that protocol is capable of doing uh you know unreliable delivery and so i think we're all looking and saying well what can we do with this and it's interesting to see both rush and warp as ways of leveraging uh that sort of unreliable delivery feature within quick to do things solve problems they're being faced in the streaming world um i think we're also interested in trying to use it for other types of use cases like some of these 50 millisecond 200 millisecond uh latency problems but we haven't yet seen the solution put forth um that basically speaks so here's like a need and here's how quick solves that and i think we're all kind of really excited as i said because of the quick deployment we know there'll be all these things we can do but i i think that you know we may be wanting to focus on these concrete things um that sort of applied in live streaming rather than some of those or future possible gains of making web rtc easier to deploy uh just because we have these concrete things in front of us and hopefully as time goes forward we'll also see ways of how quick makes webrtc easier to deploy it's max yes magazine but this is the questions we'll at least do some polls on after we have gone through the queue"
  },
  {
    "startTime": "01:50:05",
    "text": "uh hi this maximum vision srt developer so if anyone wants to talk about it you're welcome so i just wanted to add on our use case we are currently one of the most popular protocols for our live video contribution and distribution and mainly for enterprise and i just wanted to tell that the topic of congestion control is very important but i would propose not to like uh in terms of mediocrity and protocol be developed here not to take all the responsibility for congestion control and for latency because when we talk about enterprise contributor uh he really has some idea of what he's doing he has some idea of the network he has some idea of the quality and if it wants to contribute really high quality of its video but the transport layer limits it and it drops this kind of stuff it might be a temporal congestion right but you're dropping the quality like for five seconds and it's really a major thing that should not happen and instead if you could sustain this temporal congestion for some time it would be better and that's what we see uh that's that's one of the reasons why srt is currently so popular and important uh among enterprise contributors is that we trust them more we let them uh decide what to do thank you thanks and fantastic i got the last word [Music] so we have to learn from our experience with doing real-time media over the internet over the last few years which is after all the basis of all the all the stuff we're doing here now because that's media congestion control needs to open up so that application protocols"
  },
  {
    "startTime": "01:52:01",
    "text": "can do more intelligent things than just throw packets away or cue them for later delivery so designing the interface between congestion control and media is right at the core of what the itf should be attacking we have groups do that know congressional control very well we are groups that know media at least somewhat well and making sure we draw on the experience of the last few years of low for whatever value of low latency streaming of media in designing that interface to congestion control i think that's a core deliverable for whatever comes out of this effort thanks yes so um thank you everyone who's been very civil and i have nicely contributed discussions here uh i think we start with these three questions you see a bit of of henry use the hand racing to show han's tool to figure out so even those in the room you need to use your retecco light or fall to give you input i will start the first question now so um and this is really just to figure out give some indications i think uh to verify what's i think what the discussions show that the interest etc but um sorry just quick clarification race hand means yes do not raised hand means no"
  },
  {
    "startTime": "01:54:00",
    "text": "ray sound means yeah yes thanks yes [Music] it's still ticking up here i will give it a few more seconds um yeah i will end this now seems to be quite significant 55 that race that i'm saying yes there's something to be that's not met they don't don't see it i mean i have a clarifying question about that i realized i didn't understand the question i know is this conversation we're having right now between you and i live yes people are saying yes yes colin yeah i mean okay so i i think the thing is is that i i just want to make sure that later in the minutes we don't redefine live to mean something completely discussed than what was in the box right that it includes things like what we're doing right now in meat echo so and then i i think that you know so i just want to have that point in there okay go ahead go ahead peep it quickly i'll repeat what i yelled from the floor which was simply what do those 13 people mean who said no [Music] i mean if those have input they it would be good if they could post why they don't believe etc or what i mean it's it's i interpret as my personal implication of that those 13"
  },
  {
    "startTime": "01:56:00",
    "text": "are saying they don't see there's a need to do something there's no use case they see that can't be met that's my interpretation so let's go on to the media and the eskian side here or contribution what we call it so and and yeah raising hand if you believe there's not there's use case that's not met so [Music] okay i will soon end the poll but it's still taking up okay i'm ending it now or sorry can you for all the polls put the results in the chat because they're no longer accessible just so we have them sorry yeah um okay uh thank you i'm ending this so there is some people feel like the third question is okay well let's discuss the third question before um but but just to summarize here it's um slightly less number of responses but they're even more for that that's within just it's not there's not their use case that's not met today with today's protocols so ted uh uh ted hardy speaking uh as a couple other people in the chat i think three might be a little bit confusing and i'm suggesting a friendly member should work on these two use cases be"
  },
  {
    "startTime": "01:58:01",
    "text": "done together as a friendly amendment [Music] yes i'm editing here let's i'm trying to get it should work on these two use cases be done together what's that producer that's a reasonable sets of use cases two sets okay i will start the poll now erase on this yes [Laughter] while not participating chose a lower count which means that yeah okay i think i'll very soon close it um so get your pulse result all seen thank you and this had 58 participants 50 saying yes and it's saying no um i think it gives a fairly good"
  },
  {
    "startTime": "02:00:01",
    "text": "indication of that some joint work and then their interest a significant interest in both of these two sets of use cases so i think the next steps really is what i lied out there saying okay can we scope this um and that the proponents etcetera will start looking into okay discussing on the mock list seeing what we can do working towards maybe shorter or suitable place but yeah so um mari do you have any comments as a.d for this buff no i don't think so um you think you've got the next step that i would be expecting i'm good with what we have here okay well that's we're just at time now i really want to thank everybody for coming for participating i thought it was really helpful session it was good to see so many people here i want to of course thank the ads thank the presenters uh thank my co-chair um and everyone have a great rest of your week here [Music] all right [Music] had like back to bed and immediately closed his browser"
  },
  {
    "startTime": "02:02:05",
    "text": "hi it's nice to meet you in person yes uh well in the time of make sure we sanitize it that's going okay thank you guys for coming um nice to meet you in person thank you well i think hopefully this will generate some discussion on the list um which has been a little bit sparse and then i mean i would like to see you start to put together a charter i mean and if we can agree on and let people argue about what a charter should be what it should be like on the list if it goes well then we don't have it we can"
  }
]
