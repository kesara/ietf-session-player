[
  {
    "startTime": "00:00:10",
    "text": "thank you all right shall we start Chris you ready super hi everyone I'm care Patel and we have here Natalie with us who's our cider Ops secretary and I have Chris Morrow who's our my co-chair um joining us remotely Chris you want to say a quick hi hello super um this is a note well um most of you know what this means this is about IDF policies and more importantly talking about code of conduct as well as patents amongst many ITF policies that's been discussed for those who are new at cider Ops working group you want to pay an extra attention to this and on the agenda we have five presentations we have uh Tim talking about challenges and Lessons Learned in deploying a couple of rfcs um 8492 8181 and 8183 then we have both Igor and sriram talking about an update on Seth uh using bgp uh for aspa and Roa then we have signtel that Tom's going to talk about again update on ASP verification from sriram and finally job is going to talk about update on RFC"
  },
  {
    "startTime": "00:02:02",
    "text": "6482 base any questions on agenda before we start super team you're on I don't know how to clicker one just tell me yeah hi everyone um I've said 20 minutes on the agenda but I'll try to be shorter maybe we'll have some discussion though let's see um I wanted to talk today about um the experience that um well I've had firsthand in the last couple of years implementing these rfcs we need next slide um before we start I want to say a big thank you for creating these rocs I think by and large they work actually very well we see a lot of deployment different implementations different instances um so I'm not here to say we have an immediate problem that you know there's well I hope it comes across although all of this stuff essentially works I think there are also things that we can improve um next slide please um if the numbers don't mean much to you and I can I can imagine they don't um there's three rfcs I named so 8183 is about the exchange of identity certificates essentially between different parties in the rpgi so a child CA needs to talk to its parents to get certificate signed needs to talk to its publication server to get its uh signed content published um there's an initial setup for that that's 81.83 and then there's the"
  },
  {
    "startTime": "00:04:01",
    "text": "provisioning protocol 6492 and the publication protocol 8181 next slide please so as I just mentioned is there anything missing well yeah I think that um based on experience some things might be missing other things can just be improved and the urgency of these things well I guess that's to a degree of opinion but also uh it varies a bit from topic to topic but I'd like to go over the well The Fairly long list of things that I wanted to say next slide um so starting with the general protocol um what I have found is that the definition of the CMS messages and the identity certificates used in the in any communication is fairly well Loosely specified and this can lead to some interrupt issues because you know you don't want to just accept everything that's possible on CMS or certificates so you kind of narrow it down and it makes sense to do it in a similar way as what we're doing for the resource certificates but then you know you find that some implementations use additional things and then you need to go back into your code and make fixes at an ad hoc basis so this can be a bit annoying um replay protection can be improved I think because currently this text that says that the the signing time of the CMS cannot regress essentially which is uh which is okay but might not be enough for example if I ask my parent for my resource entitlements and I get a message from before but it's not regressed it's just the same message then technically speaking I should accept that so A man in the middle could essentially keep me from learning changes um identity key role is a thing at scale so one implementation that I are in"
  },
  {
    "startTime": "00:06:03",
    "text": "I should say one deployment that I work with have uh well over a thousand delegated chart CA so they're on their own systems and they need to do this exchange initially but what if you want to change your identity key for example because you wanted to start using an HSM and you weren't before do you go out to all of these Cas and ask them to do another another exchange that's it's it's difficult from a scanning perspective signing algorithm uh for the messages it's all RSA 2048 and sha-256 and I don't think we have a plan for changing that we may want to think about it next then um more on the control of the messages error messages there are quite a few error messages and some of them are really useful but I think there's also room for improvement there rate limiting is something we may want to think about and then I don't know there may be other things a lot of people have run into that I'm not aware of next slide please now more specifically on the publication protocol so there has been from time to time people suggest that maybe publication server should be more proactive in what it accepts from uh Cas uh relying parties cannot trust the repository inherently they need to do their own validation but still if a server would you know apply some hygiene there is some attraction in that so things you could think about is like should a server protect against certain object types syntax should it be um insist on the consistency there's actually an error message for that already so um but then if we go down that road we also need to consider the risk of server errors so what AP publication server has a has a bug was just you know not up to"
  },
  {
    "startTime": "00:08:02",
    "text": "speed with the latest development and it starts rejecting things then how does the ca deal with this so it's not as trivial as we might think um quota is another thing um it's hard to say what quote I should be probably it's something that is on a per publisher basis um but then again yeah we'd like to protect against uh Cas just publishing a million objects and causing relying party software all over the world to download all that stuff and I think it would be good if you have something in the protocol for that um server notifications or resync these question marks are about a specific um outage that I witnessed where essentially there was a configuration mistake applied to the publication server and it had to be restored to a previous version from backup and when you get done is that Cas are out of sync with the publication server but they don't know they think I published new manifest URL everything is good I'll come I'll come back tomorrow but the rest of the world sees the whole manifest URL and they expire so should we proactively pull the publication server or student publication server maybe send a message out and say hey I'm back up and this is the state or I don't know maybe we need to think about that next slide similarly in the provisioning protocol the when our resource is safe to use that's also a a concern um specifically when will my certificate with new resources be published now this is written in the in the CPS but I can't really pass the CPS I don't know where it is and I don't know how to pass PDFs so um but yeah if you get a new resource to me if my parent gives me a new resource and I delegate that resource to a Chelsea before relying parties have seen the certificate issue to me then not only my"
  },
  {
    "startTime": "00:10:02",
    "text": "child is invalid so similarly removal of resources it might help if there were would be some advanced warning um similarly to before how frequently do we pull the parent for what the entitles are have they changed or shoot maybe the parent have in notification mechanism like we have for example in RCR that they can say you may want to talk to me algorithm agility there's a document describing what we would have to do if we wanted to use other algorithms in in the rpki and essentially is based on having separate trees for a while and a new document that defines flag dates for going from one to the other um I don't really want to go into the detail of that right now but the point is more as a child CA I have no way of knowing um that this is going on what I'm supposed to do so probably if we're going to do this we would need something that okay A little bit of detail the response that the parent gives me is essentially these are your resource classes and your entitlements your resources in each resource class so most likely you'll need something that says here you have resource Class A and you can do RSA there you have resource Class B and that's where you can do elliptic curve or you know whatever it might be but something like that we will probably need at some point because currently we have a document that describes how we could do algorithm roles um but I don't think that in practice we can make this work at least not for the world where you have delegated cas and there may be other things of course which other people have seen next slide please um now what would be requirements if we think about changing all of this"
  },
  {
    "startTime": "00:12:00",
    "text": "I think it's really important that we basically don't need anybody behind at least definitely not from the start so we would need some kind of graceful negotiation of protocol of capability or capabilities or something um what might help is that we stay to the current protocol as closely as possible because it's less work but of course provided that we can do it in a safe way um and then other questions uh questions come to mind like okay if we look at all of this and other potential things that people might think of will we go for a new version that has tries to fix all of the issues or which might be hard or you know is there a way to to do things incrementally and say okay these things are really important can we address those first um what I propose this is the last slide actually um and to start with the bottom well I'll start with the top I'll get to that what I propose now also after talking to some people is that um even though you know I I would love to move fast and design a thing and have something great I think the proper thing to do is to uh to make a document that really defines the problem statement in these different areas uh that lists requirements of how we might move forward and that can also be used to discuss what the priorities are that people you know feel do we have consensus on that something is an issue uh maybe we don't have consensus on certain things and you know it's hard to work on that that's probably a good starting point having said that um the identity key role is a operational issue that's that I'm facing so I need to do something a little bit more proactive there now if we can have a discussion and it goes fast and perhaps we can have something there that is really within standard but if not and"
  },
  {
    "startTime": "00:14:02",
    "text": "I still need to do this then what I would propose is that I make an informational document that describes how this works potentially get a external auditing on it as well preferably I would get review from the working group though and I would feed back into a ietf standard hopefully plus I will also commit to that if we do have that I'm more than willing to change things that I have done as a temporary measure uh to to follow the the standard to be now the final thing I wanted to say is some of you might have seen the document that I submitted with just myself as author having you know where I documented some ideas some ideas also resulting from discussion with other people that is not that document that is just a document that I wrote because I wanted to have something tangible so I could have discussions about you know what are possible mechanisms we might want to think about uh there and and it has helped and serve that purpose but to be clear that is not a document that I'm proposing to write in the first Buddha point um and that's it really so I guess my question to the group would be um do you agree that's um going for a problem statement on requirements documents in this case is uh is a valuable exercise and would you be willing to uh to contribute to it yeah and that's that's it super thanks team okay and we can take this on mailing list as well okay yeah thank you"
  },
  {
    "startTime": "00:16:10",
    "text": "I think thank you um hey everybody I'm Igor lobashev and last date of in Philly we presented a new proposal for a source address validation algorithm that's using a bgp as well as rpki data we received quite a bit of good feedback at the mic and after and so today I'm going to talk about what's happening next with this work uh can you do next slide actually next two slides next so I'll start with a just quick recap of barsev uh algorithm um not going to go into much details look at the Philly presentation from work next all right so we have a pretty nice long pedigree of last 22 years since 2000 when BCP 78 was published which says thou shall do Source address validation 3704 invented a feasible path RPF method 8704 didn't enhanced feasible path and well here we are today with barsev so and um the main advantage the main innovation of barsev is augmenting bgp based methods with rpki data uh just like 8704 it starts with let's find uh the customer cone for the interfacing question for the customer or peer interface and it's also using aspas data from bgp in a more advanced way but uh fundamentally aspects from bgp but also looking at spa data as available"
  },
  {
    "startTime": "00:18:01",
    "text": "uh once customer cones been built it's gonna find all the prefixes that belong to ass in the customer cone using prefixes from bgp announcements and also low generator data um so what's the advantage of the biggest Advantage is that the data available to barsev is more than just the data that was previously available to algorithms that only use bgp and that means that for networks that use some sort of traffic engineerings and their prefixes or as numbers don't show up in bgp we have a way to augment that data and build a better self-filter list looting it rpki um in some future if customer code has a perfect adoption for example of aspn and raw uh burstev can then build a perfect filter chat from rpki data um but it doesn't have to be we don't have to wait for that future because it's perfectly happy to augment rpki data from BJP next next slide thank you so again quickly going through bar 7 operation step one build customer cone uh start with just a single as number that's on the other side of your interface peer interface or customer interface look up the test number in aspa a customer of relationship or a lookup or end lookup that as number in all the asps that the router has received and looking for what is the previous as number in the asps should be stated that we're looking at every single bgp update message that's"
  },
  {
    "startTime": "00:20:01",
    "text": "available not just from this interface so anything that received from any of the customers appears even your provider if you get a full table from them all right so you discover some as numbers iteratively repeats the process when you can't discover anything new you're done you have your customer call and then uh for the customer Corners numbers you look them up in raw uh find the prefixes you look them up in BJP update messages uh for uh bgp update messages whether originating as is in your customer cone you find the prefixes um at this point it's kind of implied but a good idea to State explicitly that the inputs the bgp uh data that you are looking at are should be pre-validated so using rpti ROV or any other validation method so don't use invalid bgp data for the uh for serve all right so you combine the two set of prefixes found from row and from bgp and that's your sub list next as you can see um you don't need a widespread deployment of Rowan and spa for it to be useful um barsef Can happily take data from bgp the only place where rpki data is useful is for networks that do some sort of fancy traffic engineering so the prefixes Nas numbers do not show up in a BJP feed most likely those networks that do this kind of stuff are more sophisticated networks and that's most likely translate to them being more likely to actually publish information into rpki uh next all right so um what's new next"
  },
  {
    "startTime": "00:22:02",
    "text": "uh most of them feedback we received uh revolved around uh uh things that we published in uh I did the new chapter like section 6 for operations and management considerations next so first of all it's very true that we're using raw and aspa information that was not really designed for it and honestly bgp was also not designed for sale but we looked at it and it seems to us that it's sufficient uh for the purpose but there was a suggestion so um that what if we actually try to introduce sub-specific sub-specific objects row like and like they're very much like raw in Spa but they designed for Sev and there is clearly Merit um in to the idea of let's use information specifically designed for the purpose but there is also clearly a cost to that so one is you double the number of objects and two when you it's always a pain to keep the tools synchronized for probably 99.9 percent of the cases so we tried hard to figure out I mean find examples where asking um the operator to publish um rpki data that's specifically for sale and that they wouldn't other publisher otherwise like when would it be harmful and we couldn't come up with good examples with us the mailing list with we still haven't received any good example so we welcome further discussions further ideas next um the other feedback we received was around uh we need to give much more implementation guidelines to the"
  },
  {
    "startTime": "00:24:01",
    "text": "implementers um this stuff I mean rpki is not guaranteed to be 100 available or even consistent so must fail open um it's the case for the traditional rpki ROV case something fails bgp would still work and for Save case something fails data forwarding should still work um so some of our suggestions so number one is if a repository is unavailable when you're trying to refresh your cache um then assume that all previously valid signed object is still valid you and so it effectively extends the expiration um the idea is that most likely the objects are still valid um and number two is if you have successfully refreshed your cash but now some previously valued objects have disappeared um you should you probably it's a good idea to actually still use them from the local care until they expire unless they're on the crl the idea is maybe the there is some sort of temporary inconsistency in the repository in the file system or some sort of synchronization problem and honestly if an object is not expired uh it should be put on the crl if you really don't want to have it uh but more ideas definitely welcome next um just a quick blog for asba adoption uh it really works well it's good for its purpose of uh detecting BJP route leagues but it's also very good for sale and there are updated draft next and so we're asking for more discussion more feedback uh with uh"
  },
  {
    "startTime": "00:26:01",
    "text": "the new version of the draft has uh addressed a bunch of feedback we received let us know um is it good do we need to talk to talk more with you and since we did get some engagement from the community we would like the working group adoption thank you chair do you want to go first yeah sorry I screwed up I haven't done me that sucked into the media account thing so on slide 9 and 10 I think it's size 9 and 10 real quick I have some feedback so um Igor sorry to do this here um I need to describe to you some internal cases use cases we have at Akamai where this is not going to work um just on the networking side um and stuff and then on so I'm happy to talk to you offline about that but yeah or if you if there's something you can talk publicly about I'm happy to have it here too yeah so I mean so we have internal interconnect customers uh that might get incidental Transit from us uh out to the public internet if we have one of our clusters that becomes disconnected from our Global backbone based on how the aggregate ipspaces announced and so it's possible that in those cases we might direct a customer to a location that is off net either because we've taken it offline for maintenance or some sort of other activity and so um The Source address may be the our customer's IP address that we're not really intending to provide transit to but we also don't want to break because if there's an external system directing them to talk to a specific server hypothetically speaking that system may not know what is Real Time connected to the backbone or not in the routing information so that's uh that's something here that would make it challenging for us to go to our external network providers and ask them to implement this on our ports"
  },
  {
    "startTime": "00:28:01",
    "text": "um talk offline and get yeah to get better idea about it but so if purely talking about um publishing reluctance to publish aspa data well and raw data for theft purposes um well it may be that we have a customer that is a dark origin or something like that where they don't want us to announce their IP space right so so there's that and then on so but we can talk offline about that and then on 10 uh the next slide you know the concept of using um you know stuff that's still valid but kind of ignore the expiration there's precedence for that in the DNS and the um use tail um definitely you know that and so I think that that makes sense so I just wanted to add that in thank you hi Anthony um I have a question you know when we are implementing currently customer accounts on basis of bgpa 84 there is a warning here for people for customers that have a quite large customer call a number of prefixes reaching more than 5 000 and I mean our own custom account is like 1.3 million prefixes um how is Boris have going to change calculating prefix filters for that so I'm assuming that you have a lot of customers and your customer call like you said is very very large I mean clearly all the theft techniques they work best closer to the age so I'm not sure how well like a tier one network would be able to do anything like that um you need to I mean at that time you know like at that point you might even need to think about the size of memory"
  },
  {
    "startTime": "00:30:01",
    "text": "on your uh so I was wondering is is there a change in in this approach comparing it to for example calculating it from our AR data well it almost doesn't matter where you calculate it from if you really have a very very large cider list uh then you have a very very large cider list to consider exactly uh right so and so there's no inline processing or something like that it uh the expectation it doesn't really have to be computed on the route or it could be computed on a server next to the router using the feeds and just feed um to the router uh through some other means through bgp I don't care in other words if we can't do bgp 84 right now for some customer cones we can't do it with ourself as well so it's not it if if the question is basically a huge cider list and you don't know how to implement it uh this doesn't tell you immediately how to implement it maybe there are ideas but this is trying to come up with a more accurate save list if you could install it okay thank you hi um Nan please go ahead oh sorry hello from Hawaii technology that's the one who share our preservation uh what I want to say is that what happened during the evolution of South surprise validation mechanisms uh what I found is that we are considering more and more information to generate accurate as a way roles in particular scenarios at the beginning we can manually"
  },
  {
    "startTime": "00:32:00",
    "text": "configure Excel rules to filter particular Source prefix we need to update these rules in time when the preexix changed and then we have strictly your RPF we can generate these rules by considering local Fable and this rules can be generated automatically but under a symmetric routine is not accurately enough so we have enhance the urpf and enhance the RPF we consider local rib information so we are considering more and more information into consideration when we generate uh actually rules and now we are considered uh more extra information besides the local variable to generate a more accurate rules and uh okay congruent uh in other words if we want to generate accurate as well as we rules we need to import extra information and of course extra cost will be imported so there is a trade-off between the potential benefits and the extra cost thanks certainly yes we are trying to get more information than available in bgp and there is a computational cost there is a cost of maintaining your pki caches but the idea that people are going to be doing it anyway to process uh rpki for bgp purposes so yes this cost is amortized sure job Snyder's honestly um section 651 suggests that you should refresh daily but there's existing work that recommends refreshing at least once an hour preferably once every 10 minutes uh and I don't see a justification to"
  },
  {
    "startTime": "00:34:00",
    "text": "deviate from uh what is already the established best practice in that regard thank you yeah that's the very good very good feedback I mean when daily was just taken without just an initial placeholder happy to change um and then the militant side of me trips over ignoring expiration dates um I would not Implement that in my validator I see no justification for doing so the document already describes a sort of fill open mode where it's suggested to fall back to enhanced urpf or enhance feasible urpf because that downgrade is better than suspending Seth entirely and I think it is very good to consider a path towards a fill open of sorts but ignoring objects uh either being deadlifted from a manifest which would cause them to not appear on a crl but it does mean that the ca refocused the Roa or the exploration that is is unhelpful I think so I I would warn against changing the rpki validation algorithms as we understand them today thank you so the recommendations were not for processing um the traditional rpki ROV uh but only for theft purposes uh the concern is that if you start ignoring pretending objects don't exist for example you fail to refresh your cache and the object is expired in the meanwhile that falling back to your uh to enhance the RPF that means that some of the data that was available from rpki will no longer be available and your server list will become smaller and that's what you don't want so you want to err on the side of"
  },
  {
    "startTime": "00:36:00",
    "text": "yourself list being bigger than it has to be then accidentally smaller than this has to be because smaller than it has to be is following it's failing closed but what is the garbage collection mechanism because I do understand your concern that it is operationally potentially a little bit nicer to to be more permissive than strictly needed but somewhere in the decision path there there is going to be an event horizon where you go left or right right so for instance if I see a traffic stream coming from you towards me and I I own the IP space and I want to block it and I remove the robot authorizing you to send traffic for the source and others continue to use that Rover that now has been removed from the system pretending it didn't expire or pretending it wasn't removed put it in the crl but then you are changing some fundamental parts of how rpki was designed to work the crls are shrunken both based on on what has now properly expired but also if it's not listed on a manifest you don't need to add it to the crl so crls in the rpki are fairly small like it's an average of say seven to ten entries per crl and this is possible because manifests are strictly uh interpreted so now you're you're changing a few complications of the rpki uh in a way that I think uh is I think it's a great discussion I mean we can definitely see what the uh guarantees of consistency that you could expect from the repo versus um what's yeah so if we can believe that Reaper has a good consistency that we will not accidentally drop objects then"
  },
  {
    "startTime": "00:38:01",
    "text": "we don't need this recommendation manifests Were Meant to uh provide very strong guarantees about the Integrity of the repository okay but then you still have your your concern about hey maybe I want to operate things on different timers um and to overcome that concern you might want to consider defining a new science object where the expiration date is what you want it to be if for some reason the expiration date of roast is unsuitable for this protocol's purpose a new object could be defined that that has slightly different rules um but and and if anything it would be good to uh put in the internet draft uh just the concern of garbage collection being stricter than is uh applicable to this particular uh case but then also find other ways to to do to shrink the the set filters uh as time progresses well thank you that's perfect feedback that's that's awesome so we'll definitely work with you and come up with something improved thank you for your time thank you I sorry you want to wait uh that is a queue sorry about that um Ming you have a question okay uh uh a very good simulation is pretty good Improvement of course but I have a question in page 10 you said that we use rpk as the uh tours to find hidden prefix right so in this case if rpk is failure I think that will make this perfect will Lobby uh included in sap table so in this case it seems like"
  },
  {
    "startTime": "00:40:02",
    "text": "the nickel traffic would be you know be blocked this kind of risk right I mean right so it was a little garbage but I think I understood the question as uh right if we're augmenting a table from rpki data then rpki failure will be will put that prefix at risk and that's why we have this implementation guidelines so some of it looks like it's pretty clear and the other one we need to have more discussion on but that basically points to the need to make sure we're very careful at managing our cash so that any failure doesn't result in failing clothes for any prefix if I understood the question right thank you Doug you're next I just wanted to make the point that you know as far as rpki objects staleness I think we have to follow what the basic rpki validation algorithm is doing and what route origin validation is doing if for no other reason that you know subtle issues of rpki objects staleness is something that's typically in the purview of the validator and this algorithm operates on bgp ribbon data and those two data sets are typically on different platforms so I don't I mean it sort of aligns with jobs comment that if we're going to change the semantics of behavior around staleness we almost need a new object which I think we agreed to stay away from so we can have the discussion about the"
  },
  {
    "startTime": "00:42:00",
    "text": "treatment of staleness in rpki validation but we should be consistent with whatever is going on in ROV yep so definitely just like I said to uh your brother that's good good feedback and we'll definitely come up with something that makes sense these you still have the com okay thank you thank you um very good thank you very much and so um the adoption that we're gonna make a call on the list sure we'll take it and I encourage you to take the discussion also on the mailing list definitely thank you okay Tom you're next okay thanks um next slide okay so so to recap briefly on this this document defines a new type of sound object called a trust Anchor Key object or attack object and that can be used by trust tankers to communicate ta certificate URL changes and taq changes to relying parties so the aim here is to simplify the key rollover process get support of some sort into relying parties and that in turn will help with HSM vendor lock-in next slide please so this was presented at the last meeting as at version 10. there were three main changes between version 10 and 11. uh the first was to note that"
  },
  {
    "startTime": "00:44:00",
    "text": "summer line parties that can't support uh Automatic Transition can still get most of the benefit of the model here by doing a sort of semi-automatic type thing so for example RP card client is a relying party that can't do Automatic Transition because by Design it's not permitted to update the key material that's being used but it can still fetch the attack object validate it alert the user to a new key um check the acceptance assignment period and then when that expires it can request that the user update the key material manually uh the second change was to note that our attack object distributed out of band is not somehow uh more secure or more reliable on account of it being signed it's pretty much just a telephone in a different format uh the reason it doesn't matter that it's signed is because if the relying party trusted the the signing trust anchor it'll be getting it inbound and the third challenge was to add some text to the security considerations around what we're calling for lack of a better term temporary ta compromise so this is where a trust anchor is using a device like an HSM that permits key signing without actually having access to the raw key and attacker gets access or rather control of that device somehow but then the trust anchor is able to regain control over that device so a trust anchor in this situation might think great I've got control back everything is fine but at least in the presence of attack objects there are some scenarios where that temporary access can cause long-term problems so by documenting that trust anchors can consider what to do if something like this happens and update their processes accordingly next level for 11 to 12 uh it's a bit simpler the only change there was to add a comment"
  },
  {
    "startTime": "00:46:01",
    "text": "build to the Tik structure so as to align it with the tell file format from 86. there's been some implementation work since the last meeting uh Apex code has been updated for version 12 of the draft uh job did some tech object validation work in rpqr client which is now in openvsd proper uh and Tim also did some tech encoding work in a branch in group and jobs and Tim's work was very helpful in finding problems in the opening implementation work next slide please some things to discuss uh Ross Housley had a suggestion on the list about adding some clarifying texts to the signed object registry at Ayana uh that's uh or rather on the author's side we think that's a good idea in principle but because it's not strictly related to what's happening in this document we think it might be better off as a separate thing uh job had some job had a suggestion about removing the TA compromise section which was added in version 11. because it's kind of hard to talk about this uh clearly it might just confuse people and it's not strictly necessary to uh to the document as a whole on the author side we're fine with that um but teas did indicate on the list that he thinks it will be worth keeping that text and then the third suggestion was to it was also from Joe adding texts about certified destruction of key pair material uh again on the author's side we're fine with that teas did indicate that the term certified invites questions about what certifications uh and so on and it might be better off uh to avoid that if we can thanks a lot please apart from resolving those issues uh there are some other suggestions from"
  },
  {
    "startTime": "00:48:00",
    "text": "Joe that are uncontentious so we need to update the document for that uh it needs some editorial work particularly on the server side of things the process uh for Server size for server-side implementations to follow is a little bit uh strewn around the document so that needs to be Consolidated and there needs to be some text around the purpose of the acceptance timer just to make it clear what what that's about and more implementation work would be good too so particularly on the server side and that's it thank you that is a commenter question probably from Russ so let's go ahead please so you're absolutely right about uh my suggestion being like really small but um what I did is I looked for the next document that's updating that registry because the previous one was already an authority eight and Warren said I won't get it they did the process so this seems to be the next one and it seems really really small thing to do a whole RFC for okay thanks okay thank you water yeah thanks Russ said part of what I was going to say of you know we couldn't put it in another document that was North 48 because that burnt me badly once before so the next document that goes through if it couldn't include that would be great um could we go back a slide um what I was wondering is if the compromise text could maybe just be stuck in an appendix as sort of like then it doesn't need to be as clear and it's just a sort of like here are some additional information that where it can stay the remove the tear compromise section maybe that could just be an appendix instead if that"
  },
  {
    "startTime": "00:50:01",
    "text": "works for nope doesn't work okay I'll just clarify my main concern about key destruction uh here the limitations that I've seen with the various vendors that we have looked at for hsms is mostly that you cannot uh be certain that there's no copy of a key somewhere somewhere else so we can have a certified process that shows that if that really is the only box we have deleted the copy that's in that exact box which you include in the process but you kind of really have guarantees about the thing meaning in really uh gone and it gets really Murray if you if you want that so that's why I was opposed to that okay thanks thank you yes uh so good morning uh good afternoon everyone uh this is sriram from nist I'm going to talk about uh the updated aspa path verification draft today next slide please so I'll quickly we'll quickly look at changes in version 11 that was published a few weeks ago um and compare it to version 09 we skipped version 10 because uh it was submitted uh and then we had a few more changes to make um so we submitted version 11 soon after um we received some good comments on version 11 already um on the working group list uh in the last couple of weeks uh we'll take a look at those comments and then the next steps next slide please"
  },
  {
    "startTime": "00:52:02",
    "text": "so just to recap uh we are doing aspa based path verification because it has the benefits of detecting and mitigating bgp route leaks and also uh does the same uh for forged origin hijacks so essentially it's a basic form of path verification does not do a complete path verification but establishes that that it is a feasible path and it is free of any route leaks and also catches forged origin route hijacks next slide please so the changes in version 11 compared to Version 9 UM the algorithm needed some corrections and we realized that I made a presentation about a year and a half back uh at ietf 110 uh that is the three Ram one reference um and that that pointed to some enhancements that were necessary uh in order to get rid of some mix up between the invalid outcome and the unknown outcome and we took care of that in version 09 but but additional refinements were also necessary and they are now in version 11. uh these additional refinements are in the form of as set handling route service route server as how to treat that uh some some other refinements related to clarification about applicable fee Safi and a statement about as configuration and we in addition to these refinements we devoted a good amount of effort to get like pretty good text Clarity uh throughout the document but there still needs to be done a little more next slide please"
  },
  {
    "startTime": "00:54:03",
    "text": "so on the as set handling we had a pretty good discussion and feedback on the working group uh a few months back uh the pointer to that discussion uh is provided at the bottom of this slide uh so now uh based on um the working group uh discussion and general consensus uh the presence of as set anywhere in the bgp path would make the path invalid per aspa verification algorithm so that's in the draft in the in version 11 next slide please so for that out server as also we had a good discussion on the thread on the working group list and the thread is provided at the link to the thread is provided at the bottom of this slide as well uh we had basically on based on that discussion it emerged that we had two choices uh one uh a choice a is to add the RS ASN to the as path in the case of a transparent as and in this case we can apply the algorithm for Downstream paths if we go with Choice B we would remove the rsasn from the as part in the case of non-transparent RS and apply the algorithm for Upstream paths uh so in version 11 we included Choice B they are they are equivalent they give you the same results uh for the path validation uh under uh under any scenario so they are equivalent we chose uh Choice B uh in addition to that we should also mention that now the draft makes it clear that NRS client must include rsas in its aspa and also an"
  },
  {
    "startTime": "00:56:00",
    "text": "rsas must register nas0 aspa that would facilitate the path verification unambiguously and it will make it work right next slide please so we just have a clarification um about the office had some feedback on this and we have since included this statement um so basically it says that the procedures described in the document are applicable only for fe1 fe2 meaning ipv4 and IPv6 and in both cases with the Safi one or unicast and the procedures must not be applied to other address families by default next slide please we also have a statement about as Confederation which is new um and that simply says that the ass on the boundary of an as Confederation must register aspa's ASP is using the confederations global ASN or Global ID and the procedures for aspa based path validation in this document are not recommended for use on ebgp links internal to the confederation I think that's uh cleared enough next slide please so uh like I said since we published version 11 a few weeks back we have received the comments uh from a couple of people and that includes uh Claudio and also Rich content from Charter a good set of comments uh both of them and I have responded to them on the list and those are comments that we can incorporate into the next version version 11. uh in particular Claudio read the draft uh very carefully and he"
  },
  {
    "startTime": "00:58:03",
    "text": "offered a very good extensive set of comments uh his main difficulty with the draft or main main suggestion is to improve the readability so he found that the draft by itself uh especially in the algorithm description uh it gets a little in intricate and at that point he made use of sriram one uh the my ietf 110 presentation so that has a nice set of figures explaining the algorithm it has a good it has good notation to begin with and based on that it builds on the description of the algorithm so he he found it essential to make use of that in order to be able to under understand specifically uh the the algorithm description uh in the draft and that's where he said it can be improved a whole lot so what I'm proposing as part of the next steps is that we can follow the notation and style in Sri Ram one reference uh to better describe the algorithm uh that's quite doable and it will make the description shorter and more concise and also um also clearer and we will publish a version 11 a version 12 sorry in the next few weeks and at that point we will invite some more feedback uh from the um from the working group and hopefully then it will be uh it will be in good shape uh so at that point uh job has suggested that we should wait a few few months and solicit implementation experience reports and then we can proceed to uh to working a group last call um right so and then I have a couple of uh slides backup slides if we can just"
  },
  {
    "startTime": "01:00:00",
    "text": "quickly move to the next one the one after that so in case you are interested and want to understand the I how the RS as is treated and how these two Alternatives the choice a and the choice B that I mentioned earlier if you would just want to get some clarity on that I won't go through this slide but you you may look at this slide uh to to to figure that out and please ask me questions if if you have any on the list or or one-on-one so thank you we can move on to the questions Joe Snyder open BSD it is Our intention to implement verification uh in the next three to six months which is fairly soon so that's where my DSI and recommendations comes from to wait a little bit with working with last call and so we finished that implementation and I hope some other thunderstorms so um start working on this in the next few months uh thank you y'all uh I should add that um at least we already have implemented uh these this pretty much close to this latest version 11. um um aspa path verification and we also have a number of uh test cases that we run against uh to to verify uh the the implementation so we have that all available on GitHub and we will come we welcome anyone interested to pick that up and make use of the tests uh or or make use of uh or make use of our implementation uh for if you're running"
  },
  {
    "startTime": "01:02:02",
    "text": "some experiments you're welcome to do that in addition to that I must also mention that uh that Claudio uh zika he also mentioned uh in his comments on this version 11 on cider Ops list uh that he is implementing it so it would be nice to I mean we know that that there is another effort as well and job mentioned uh another one as well so it's good to see that there are multiple implementation efforts already on way are available or on way thank you um um so on the cas side we've done an implementation based on the syntax that was in one of the documents but I think we need Clarity on whether that's going to be the actual syntax basically that's what's stopping us from doing it on the validation side because you know experience of implementing something that wasn't solid and then having to revert it wasn't very nice so if you can have Clarity on that I think we can do more work on the validation side as well editing job can respond uh Joe Snyder's uh I think in terms of the profile uh the profile in my perspective is now stable there's multiple implementations uh specifically your ca implementation helped me develop my validator implementation the current's profile drafts contains I think references to seven or eight implementations that to some degree have tested interoperability with each other uh so as far as I'm concerned unless there is a grave grave mistake in the profile that needs addressing uh we should stop"
  },
  {
    "startTime": "01:04:00",
    "text": "touching the profile and the contention between the current version of the profile and the one before that is of a somewhat cosmetic nature and that to me does not count as a an urgent passengers him to to change uh and then from a CA side um I am I am ready to publish as per profiles conforming to the latest profile in the public rpki and we'll do so in the next few months for a talking profile anyway we now have a implementation of the current drafts as per profile in the CA environment available on our testbed and we will publish a documentation how to create objects soon it appears to be interoperable not causing problematic validators however one implementation will complain about unknown objects super and my my comment would be that the contentions that we are seeing on the profile side maybe that's something we should take the mailing list and discuss um also come into my fellow idea chairs this is something uh uh idea should be looking at at least and reviewing it so Chris and I will follow up with you folks thank you sriram thank you thank you Joe hello everyone loud and clear yep I wanted to present an update on a recent effort to do a best version of"
  },
  {
    "startTime": "01:06:00",
    "text": "the document that specifies the route origin authorizations profile rc6482 this next slide please the best effort started because I noticed an oversight in the original specification with regards to the mandatory presence or absence of as identifier extensions and I felt to myself well I'll just file an errata next life unfortunately it was shut down to me this seems a little bit of an arbitrary decision by the powers that be I feel that what's currently deployed in a while in terms of objects in the repositories and how validators react to them and other erratas that have been approved similar to this one it should have been verified but it didn't so next slide please buckle up here we are able documents next slide the best document is available for your consideration I started with a for bottom copy of the original RC to really try and make the changes from revision to revision as minimal as possible so that the whole crowd can follow the story and see that even though some of the changes might seem intrusive that all in all these are very uh nuanced changes to tie up any and all Loose Ends the original RFC presented next slide please the goals of the best documents as far as I'm concerned are to clarify uh that that as identifiers should not be present I want to strengthen the asm1 notation"
  },
  {
    "startTime": "01:08:00",
    "text": "uh pull in the verified Errata that that appeared so far extend the documents a little bit by providing an example that people that are implementing roast may find useful and uh above all maintain full compatibility with the ecosystem as we currently extend the uh understand it I will go through these points one by one in in the next slides next slide please so the roast payload contains essentially two elements one is the origin as and the other element is a list of IP prefixes and as we validate roast in an rpki cache validator uh all the IP prefixes in the payload must be contained in the 3779 extensions of the ee certificate and the parent certificate of that ee certificate and that's parents certificate Etc on the other hand the origin as or as ID as is it's called in the profile is an arbitrary value that can be set by the resource holder so I can impose anybody's ASM or even private asns and it's really up to the IP resource holder to decide which ASM is the one that can originate the prefix in other words validators do not check whether the asid in the payload is contained in the 3779 extensions in the certificate chain and I've seen multiple uh implementers that that made a mistake in this regard because um from the original specification uh the the whole notion of as identifiers is is not described and since in the payload there's an AS"
  },
  {
    "startTime": "01:10:00",
    "text": "and potentially in the ee certificate there could be something as related it is intuitive for people to assume that maybe they have some kind of connection and by explicitly documenting that the as identifiers extension must not be present I think it becomes easier for developers to understand that the asid is not part of something that is uh verified to be contained in the chain of authorities so that is what motivates uh this particular uh next slide please about the feasibility of disallowing the as identifiers extension in Roland ee certificates there are currently zero roas in the wild out of more than 130 000 rowas and none of them have this particular extension um I looked at various uh open source CA implementations and none of them set the extension the as identifier extension on Raw ee certificates and on the validator side uh most validators will ignore the extension if it is present and one validator will consider the robot invalid if the extension is present so I would argue that is entirely feasible to add this additional constraint because it doesn't seem to stump on anybody's feed next slide please all right next topic strengthening the asm1 notation um the original Roa asn-1 notation was I think written in a time where uh there was less understanding of of all the powers that and features that asm1 can offer us and what the benefits are in uh"
  },
  {
    "startTime": "01:12:01",
    "text": "being very concise with with constraints so for instance um the asid is an asm1 integer an asm1 integers can hold very very large values think larger than 64 bits and also negative values so I think it is super helpful to be very precise in this profile and explicitly disallow negative values or values larger than 2 to the power of 32. um the slide you're looking at what I've done is uh the the rats colorize text uh enclosed in square brackets is what is removed and the green colored text in the curly brackets is what is added as its replacement so let's go over these changes one by one in the the the the in the container that that has the optional version attributes uh the asid and the adder blocks A Change Is made that ipader blocks cannot appear unlimited amount of times uh but it can appear only once or twice the reason for this constraint is that the address family is contained within ipadder blocks and all robot producers currently will add maximum insert to IP other block structures one for V4 which can contain multiple V4 prefixes and if V6 also is a V6 prefixes also appear in the roller payload another ipadder block is added inside this ipadder block it specifies ip6 and then a list of one or more ip6 prefixes so this feature of Ip editor blocks"
  },
  {
    "startTime": "01:14:01",
    "text": "being permitted and unlimited amounts of instantiations is not used by any CA implementation in retrospect I think it shows that the data structure of uh rowas should have done an inversion where for instance the API is is on the outside of the container but that type of change would prompt us to do a row of a profile version bump and that would then we would not meet our goal of compatibility um all right so IP address is is a a quirky change but it works out with everything that's deployed in the wild uh then asid being constrained to zero up to two to the power of 32 I think is fairly obvious um the next change is in address family inside the Roa IP address family structure where the API is uh no longer permitted to be two or three octets but can only be two octets the reason is that the robot specification you need three octets to encode both API and Safi but you're not allowed to encode the Safi so according to the normative natural text in the original RFC the only possible outcome is is two octets so it seems a bit silly to me to on The Wire permit three octets and then error out because you specified the third octet uh so again this this is a change that is perfectly compatible with with what's deployed out there uh you're not allowed to specify surface uh so we clean up the room that would allow you to express Safi onwards to max length inside the Roa IP"
  },
  {
    "startTime": "01:16:00",
    "text": "address sequence next link again was an integer unconstrained so it could be negative could be really large but the reality of the situation is that max length cannot be smaller than zero cannot be a negative number that would not make sense and it cannot be larger than the maximum prefix length of an IPv6 address which is under 28. there are additional constraints uh for instance in the case of an uh ip4 Roa IP address block the max length value can at maximum be 32 unfortunately uh it's super complicated to express this in asm1 and while we do have a draft profile that that introduces API context dependence constraints my personal take is that the ASM one is utterly unreadable to both humans and most open source compilers so I'm a bit hesitant to go down that path then the final constraint is in the IP address bit string again an address is if it's ip6 address the address is going to be a maximum of 128 bits if it's ip4 it's going to be 32. it definitely uh is not unlimited and never larger than 128s uh and and these constraints uh are expressed in the natural text in the original RFC to to some degree but I think it is helpful to also uh repeat these constraints in the asm1 profile itself so that the next person that takes the asm1 profile and compiles it into source code uh gets some benefits of these constraints uh Jeff you jumped into qdf a question specific to this or at the end"
  },
  {
    "startTime": "01:18:01",
    "text": "all right next slide please all these changes are 100 compatible with all robots deployed out there there are no Ross that carry more than two IP other blocks there are no row ads that have negative or or larger than two to three uh as IDs and max length uh an IP address also are within these constraints so I I think there's a good justification to to accept these changes to the asm1 profile because it does not break anything we're currently using uh on the wider internet next slide please in terms of incorporating verified errata these were super easy I just copy pasted a sentence that the inherit element in the 3779 IP address extension is is not allowed the asm1 has been uh verify to compile and be complete in the latest version thanks Russ for your help on that um and since the the document now in its native form is an XML uh to RFC version free documents the table of contents is automatically generated and I got to take off this Errata without actually doing much slide please I included an example in as an appendix in in the draft this is just the payload of a Roa and I provided a standard Unix utility invocations to demonstrate how the hex the Dr encoding uh transposes to to Output that is a little bit more human readable and my hope is that if somebody writes a validator or a CA implementation that an example like this would help them"
  },
  {
    "startTime": "01:20:02",
    "text": "and then again there's of course in the repositories there are hundreds of thousands of examples so uh it's it's more there for completeness next slide please um and with that I would like to open up the the microphone for comments feedback uh also please email to the mailing list or if you want to have a private discussion email the office Alias directly or if you like using GitHub use that Jeff Jeff has mostly nitpicky type stuff so the main reason historically at least that constraints were not thrown at a lot of asn1 stuff is the tool set anybody that's tried to actually use any tools that do ASN one parsing usually end up tearing their hair out uh swearing at the authors no trying to actually dig through necromancy manuals to actually figure out what the apis are supposed to be so while I agree that additional constraints might be handy then you know your real question is whether or not your implementation is going to slow down because you're increasing the difficulty to get the job done two binder points on some of the items you had in there so like your as numbers you're trying to actually restrict them to be positive integers as big as an as number can be that's great uh one of the weird side effects of leaving it just integer is what happens if you want to do strange things like we've affected them with slurm and other Scrolls where we stick weird as numbers and they're like zero if you have a negative number is a signal that allows you to Signal stuff later very similarly like uh by not being prescriptive about the length of the addresses you're allowing for something that's not V4 or V6 to eventually become specified um but in my last comments really directed towards you know the original spec encoding an Effie as an octet"
  },
  {
    "startTime": "01:22:02",
    "text": "string was always stupid you know it's an integer and it's uh you know bounded to two two bytes uh if I respond to that the constraints as used uh can you go back to the ASMR slide please one more one more the only two constrain types that are used here are a size limitation and a value range limitation and those are supported by uh all the asm1 compilers I'm aware of so yeah Yep this is much later than I mean the original robot specification is I think a little bit more than 12 years old so times have changed to some degree uh where we had more difficulty was instantiating new classes to the API contextual context dependent uh size limits and there we finally found the compilers uh uh like asm1c have trouble uh understanding what is happening but I'm I'm very confident that these constraints are supported by the vast majority of the ecosystem any other questions comments what is next with us all right what else ah so Russ Housley there's um when we were working on three seven seven nine like snack was the most common open source asn1 compiler and uh it only did the 1988 version of the Deus unlock and so all of that so um when you look at this I think if"
  },
  {
    "startTime": "01:24:01",
    "text": "you're using a tool that understands it you will get a benefit if you're not you're going to have to put code around it to make these same checks anyway so I see no harm uh in in putting it here where they get a decode error or you'll get a checking of consistency error but either way you're going to have to perform these very same checks and as you have said I already compiled the module um with the addition of a single semicolon that was inadvertently dropped it works could we go back to slide I think three so yep you're right that was a judgment call on rejecting that um arbitrary judgment call uh I think allowing a comment is less of a big change than this but I'm super glad that I rejected it because this looks like a much better outcome so if I know this is going to be the outcome of hit reject faster but yeah if it was a judgment Hall and I agree that that uh taking this opportunity to do a best document is is a better outcome uh so I I just wanted to you know you were sitting right in front of me be a little bit snarky about it sure you have 10 questions no I have uh just back to the slide 10. um uh you know that had the ASN notation so the one the one problem I'm kind of having with this is I mean I certainly agree that we're not going to have negative as numbers but you know in in looking at this I'm trying to think of the future as well you know if we're talking about asn1 compilers and you can encode that well we may not like you know dates back to the 80s uh you know in some of these cases I'm"
  },
  {
    "startTime": "01:26:01",
    "text": "also trying to think about the future so if we look at an equivalent time window in the future could we have IP addresses or ASN values that exceed this range in the future that by doing this we are constraining ourselves from doing something creative when we when it be may become operationally necessary and and that is one of my concerns in trying to make a change like this is you know by by going and doing these things if for some reason we need to go and change all the asns or move them all to 64-bit or some other future number even though today we may not imagine 32 to be you know you know so far out of range I I have reason to believe that we would still be using the same dgp4 protocol plus plus and that time frame and so it's hard for me to believe that we would since the universe of people using this is even smaller than the universe of people using bgp4 plus plus um that we wouldn't that we would want to construct you know to implement a constraint to say hey this should only be 32-bit um especially in you know this because this is an outside encoding encoding this isn't a wire encoding this is wire encoding this is not a wire encoding for the purposes of this data storage you know for actually signaling in in the protocol um in in this place because this is something that gets passed through that you know 1983 or you know or later decoding engine for asn.1 if we want to use something other than asn.1 like you know the tlvs that we use in bgp protocol like maybe we should"
  },
  {
    "startTime": "01:28:00",
    "text": "be doing that but if we're going to be relying upon asn.1 which is what For Better or For Worse x509 relies upon uh you know I I'm concerned about constraining the numbers even if today it seems rational because it's a valid range um but by going and doing that we're going to foreclose that and make it much harder for people in the future can I uh interrupture a beautiful monologue um if as and numbers are specified to be say 64 bits or 128. the path to extend this profile is fairly straightforward on line two you see a version uh and if the semantics of of the data structure elements would change such as hey asns can be larger nowadays uh you would use a new version number uh that has new so I'm sorry is this saying that you're going to increase the version number here to do your change now these changes all reflect what is currently version zero as deployed in the wild is there a reason to constrain it now to to perhaps unconstrain it later it's it's already constrained in all RP implementations it's just not in the spec and if we want larger as numbers which might happen all we need to do is Bump the version number and say asns can now be 2 to the power of 64. or we file a bug in the Implement in those implementations to say please accept the valid numerical range from the asn1 encoding there there's more Plumbing if ASL numbers are extended like RFC 3779 would need to be updated sure uh but the list of things you know I'm just wondering do we want to be adding to that list of things in the future that would you know future debt when it's not necessary now"
  },
  {
    "startTime": "01:30:02",
    "text": "and I haven't heard a compelling reason to make that change other than well people implemented it this way now maybe for this thing that isn't also I've you know you claim isn't being used either so I'm also wondering why we're spending a lot of time mucking with it sure George Michaelson AP Nick I actually have two points they are unrelated I I think there is significant benefit in being narrowly specific and are arguably prescriptive in binary structures like these because the primary risk here is Bad actors not good actors and the negative number wrap around the unexpected Behavior and the we didn't expect you to use it but we didn't Define it concerned me from risk and I know that that's the hand waving we're a little unknown risk statement but I do see this as bad actor threat we've got people with implementations and they don't have defined constraints in the ASN one if it turned out writing a negative number caused an out of memory event and it was in routers that would be very unpleasant I'd rather rewrite specs which narrowed that opportunity that's my personal belief and if what you're doing has significant on The Wire binary compatibility for good actors it does have a certain zero cost quality and that again is good and I would I would absolutely wish to applaud and welcome the word cross does using a well understood validator to check Behavior I think is a huge net benefit to the community so thank you for doing that so now I want to make another comment and it is a personal comment I wish you to understand that this is not a reflection of AP Nick and it actually is not directed at you it's a comment to the chairs and to the idea it's something I said to Warren informally"
  },
  {
    "startTime": "01:32:00",
    "text": "over breakfast that I think should be said publicly in ietf in the wide is insufficiently understood and I feel documented and irrespective of the merits of this proposal I believe risks of contention and dispute around how this is happen actually is a problem that should be addressed and it is a working group chair matter and an ID matter it's not a matter about authors it's about what should the ITF say is how these things are done it is a personal comment and it does not relate to the specific work thank you any more questions or comments for jobs yeah no one thank you Joe uh yeah final sentence uh to to reiterate my goal with this best document is to tie up any and all Loose Ends uh so for instance there's a part of the specification that talks about two out of three permutations with a certain data structure I intend to add the third one for completeness uh and at some point My Hope Is that we we have understood all loose ends and and documented everything that must be absent or present and then go for working group last call uh so if you want to contribute to that effort please help review the document as it is and uh send comments concerns or paragraphs to the offers thank you all thank you and that's a wrap we will see you again in Yokohama till then stay safe"
  },
  {
    "startTime": "01:34:02",
    "text": "thank you thank you problems well absolutely desire to specify only what's found there there are extensibility paths motion and X-Files yeah I will not make sense foreign"
  },
  {
    "startTime": "01:36:07",
    "text": "it's hardening the system in a negative or that makes sense that's necessary of course the Next Generation every five generations now and that's that's what I'm unnecessarily conclusion is regardless is still need to be done me this question is"
  }
]
