[
  {
    "startTime": "00:00:45",
    "text": "[Music] [Music] [Music] hello Janet can you hear me whoa [Laughter] okay I will try with this one I think it would be better so Martin please yeah Martin Dali okay so with respect to diameter security when would that be discussed up in the beginning or a and the wrap up on the diameter of security we will discuss this point at the end of the session do you at least have some points on the I think it\u0027s better I think it\u0027s a mic but not on the status was the working group documents or non-market good documents so basically we have to document our that our block "
  },
  {
    "startTime": "00:03:47",
    "text": "in the editor queue mainly due to the shepherd the laziness myself so I need to review one of them and this one is so both of document are linked to the third one and this one needs a review and to go through the AG and so there is no issue with this one it just because one document is missing so it\u0027s a misread for the load information an agent of a load both are linked to the rate control algorithm so this will be done right after this meeting Steve please Donovan a little net you get the editors wrong and the list excuse me what an editor\u0027s analyst is now not correct I\u0027m the editor on load information I\u0027m not the editor on 400 6 bits okay just an it okay thank you after that we have three working document for we just that this is quite I would say unclear so we have firstly a dynamic to group sing any you would like to say something for the Aug review okay the first one would be the diameter group singing that teasing the working group for wine now we have received a new version of the document and from the author\u0027s point of view it\u0027s quite Tibble but only from the author\u0027s point of view within more review we were expecting additional reviews but it was not the case so the next step normally should be working with high school and in high school but maybe Marco you would like to say something on that not much to say as I said before last ITF we published two new updates on covering all the comments that have been made before it so some of them have been checked on peer-to-peer phases some in discuss and confirmed on Manliness so I think doc administrated for detail expert review and we\u0027re fine and happy with updating again but we need comments to progress on them I think that from a technical point you will receive one good review from Steve but not so much except this one okay so we will see what to do but from from a working group point of view I could ask for working group last call but that just need to ensure that we will have enough people to review it and just to ensure that at least the content of the document reflects the working group consensus but we see that with the ban after after this I will I will go "
  },
  {
    "startTime": "00:06:49",
    "text": "through so the overall break control is still rights so for this one the what is missing is the it\u0027s it\u0027s what is missing here the write-up for for this document just to push it forward at the agency please it has finished record wescorp alright only right rate has finished working group Weiskopf yeah yes just to be able to send it to the for a HD review so my test so the last one is the four six that was in the working group last call and I yes it\u0027s where it\u0027s so it this one was we had one working group last call we had some comments but fairly so it was only about some references and so on but not a deep review of all the technical content and so on so it\u0027s something that I was I was I was waiting for some first input on the main needs to be able to react so I\u0027ve sent it right before somewhere some comment because it was not it was not really pointed out by someone else but I think that for this one we will need further review it could be through I think that the working group logical process anyhow for time is not so relevant but we can initiate a second one based on the new version on the version 3 publish and capturing the government receive so far for the first working group last fall but we will need anyhow additional review because an especially from SF 5 guys so from the 3gpp because there are the main users of this of this one so I think the work done so far is good but we do need just to ensure that what it is puts here and especially is the new additional the new additional ADP\u0027s and so already fulfilled the requirements any specific reaction on that after that we have the diameter will be P level security so the end-to-end the security for the emitter so for the time being we have no draft anymore because the the last one expired uni will not have so much time to work on so I think he I will not speak for him but sir it would be a bit too to contribute on this one but he is not able to support the workload alone what we on Thursday I will go to the SAG meeting the security meeting and I would ask for support from the security "
  },
  {
    "startTime": "00:09:49",
    "text": "experts so the idea will be to to to motivate people to work on this on these documents the solution itself doesn\u0027t have to be a diameter specific we can discuss with them if they have if there is something easy to do in this area for the emitters that could be applicable to diameter and based on the output we will see what to do and also after the discussion on the working with Ben please Ben I would personally like to see this completed but I wanted to ask a question of this room if anyone in this room going to deploy it if it were implemented if it is completed mal Jun Jie\u0027s Martin Dali I mean okay so the next round of sec Cedric they just finished the ss7 of vulnerabilities yada yada and the next two years is going to be looking at diameter so what I plan on doing siii is coming up in three weeks and I\u0027m bringing in a discussion paper basically telling them of the status of this work here and where you know it gets friendly talk at that meeting you know and ashley has been brought up in the conversation of you know 5g or overall securing the system I\u0027m gonna let people to know it\u0027s it\u0027s now or never forever hold your peace time so unless a comment will be from the GSMA point of view we have received an LS so this one had been sent to 3gpp and diameter just to be able to work on this solution because JC may at least from recoverin point of view would like to have a solution to recommend in their specification so at least from from start point of views are expecting a solution after that I think it will depend on the type of solution that would be a dot if it is something that it is easy to implement people will be because we have ongoing deployment ongoing operational networks for yes please this is air campaign for mission-critical deployments there\u0027s a two security domains one of them being the operator domain and one being the mission critical operator domain and we\u0027ve looked at securing the I had a message or even a I would say component "
  },
  {
    "startTime": "00:12:49",
    "text": "to a sip message level securing these where the message is has to be end-to-end secure between the client and the of said the host and the mission-critical service provider I wonder if there are Triple A requirements where you you can\u0027t expose certain a VPS to the operator i I mean that for the most part be the operations that we\u0027re using triple a for have to do with the access but I\u0027m wondering if they\u0027re Triple A requirements that are emerging mission-critical I think we should check that actually from the requirement point of view both should be should be available with this solution after that it will depend on the type of solution that we will adopt at least as a first as a first step but normally you should be able to secure the signaling on only between gateways for instance and you you can assume that your your the different system are secure enough to get rid of any security that you may have also a real end-to-end secret end-to-end security exchange between any diameter clients and any diameter server mutton this okay so I mean not to have an SI 3 discussion here I don\u0027t think that is the case I think that if you look at European operators in particular ok so if you look at 401 401 specifies that equipment manufacturers must support IPSec for communication within the EPC it is optional to deploy and I believe that optionality equals nobody has deployed and so therefore with Olano for 5g one of the things that that\u0027s being fought the most by the European operators is pulling the security further into the EPC and not at the you know B as an example because they view their vulnerability to the fact that the EPC is open to the Internet and so some of these some of those last couple statements aren\u0027t true and what you\u0027re saying is true ok so at least been you know that there is an interest and we will see and actually from a personal point of view and Union or so we would like to see this work done and and yes I have been I observe an interest in using the work we will also need to observe an interest than doing the work hopefully we\u0027ll get some bail from Sargon that well on this "
  },
  {
    "startTime": "00:15:49",
    "text": "I can I can come at this point he is a key issue it was the same for unis that it\u0027s more security discussion than a diameter stuff from personally I could help on the diameter aspects and and how to convey the information you\u0027re defining the ADP and even figure out how to behave but on the security aspect itself we we we need some some expertise coming from the security area yes Eric so just for my understanding there was a mechanism and there was largely to blame for it for for using an S mime encapsulation that that became historic at some point it was deprecated okay it is said in the new version that this CMS mechanism is deprecated and and it is said also that a new mechanism will be defined but outside the best protocol and also new drafts are based and new RFC are based on these common things and to end security is an issue but we will have soon a solution but the soon now is maybe ten years uh not yet not yet but so I will skip the description on the other one because we have at least on the new on the new proposal and also an inversion for Policy group and predicted units we have a specific one I\u0027m assuming that the one on domain arbiter mask filters and peer flow must be traits are just okay so let\u0027s jump to the presentation and you would start with the medic unit and policy groups yes please the floor is yours thank you we\u0027re just gonna run through kind of the updates for these two so next slide please so predicted units if you recall our motivation was the fact that we\u0027re using nfe virtual functions basically and typically right we have authorization endpoints as a part of this inside our virtual network functions what we\u0027re kind of starting to do is divorce the CPU that computes the memory that is the i/o resources as we virtual eyes away from the authorization so we see a much bigger distance it used to be we had fairly predictable Hardware and sizes now our thing kind of fairness and so what we\u0027re trying to do is basically sin anytime an authorization occurs basically a successful CCA OCC are we\u0027re trying to add some extra information in this case time add a match to the amount of resources that may be utilized following the tradition of used service units and granted service units we just "
  },
  {
    "startTime": "00:18:49",
    "text": "call it predicted service units with the full understanding that the units that we actually may send back may not relate to the actual grants credits maybe for example in terms of bytes or time however predicted may actually be more granular based upon the system something that\u0027s meaningful to the employee so for instance disk usage Network IO some other metric but we wanted to basically inform these virtual functions a little bit more about the usage that they can expect this is especially important during peak times for the VNS so this then allows us to basically have the service if you will select if it needs a new scaling event other things you know if you can think about nfe in the bigger picture they can ask for scaling events or they can just stop actually you know accessing or granting requests into the system so this allows us to avoid the situation that we\u0027re currently in today which is I have to overload a virtual network function in order to realize I\u0027m in trouble or at least hit a threshold toward an overload before I realize I\u0027m in trouble so and just for reference it\u0027s still about three to four minutes to spin up a vnf virtual machine so this sort of information is important to us anyway so that\u0027s the the base motivation with this and it\u0027s essentially a VP\u0027s that are carried along during an authorization next line so in version two all of our updates were editorial we were asked in the last meeting to really put that information in the introduction and so we\u0027ve rewritten that introduction we\u0027re a bit more dictatorial about this and then just as a refresher right this is essentially the the same old same old in terms of units the only difference is we have the time of day in there as far as an EVP to give you the idea of conditions and then our assumption with the series is that there\u0027s non-overlapping times with respect to any particular unit it\u0027s all in the spec pretty straightforward at this point we\u0027ve had a couple of updates before the last meeting we had updates during the meeting I have not received any new updates so what I\u0027d like to do is push for this being adopted either out on the mailing list because I we\u0027ve had quorum issues in the past but push for this to be adopted as a worker item on the mailing list after this meeting if there\u0027s no other material updates here what do you think you so for my information and and for the group who has read these documents the predicted units okay for them so I wasn\u0027t there the last meeting that\u0027s at least what was the UM I think there was some interest expressed I think the larger discussion in the meeting was just the lack of the number of people I think it focused more on that and the level of energy so I and I would argue that two "
  },
  {
    "startTime": "00:21:50",
    "text": "of them were just waiting for a plane but that\u0027s a yeah okay so I think it forward and I and I at least I understood the principle and also the proposed mechanism so it\u0027s good what we can do on this one would be to at least to check on the on the manning list so what we could do is check on the manganese if there is support in to us for working group adoption I think it\u0027s if it is the case I think that there is not so much to do on this to team on that just to push it forward yeah but it will depend on so on the on the next on the next discussion on what to do with the ongoing work and you working yeah absolutely understood all right for educational comments no so we supposed to go on with the other one so policy groups this is actually the harder of the two so the conceptually what we\u0027re doing here is two things so in the user plain we\u0027re essentially adding metadata as another filter through the policy group matching mechanisms and then what we\u0027re doing you know at the higher level if it\u0027s actually implemented before the user claimed we\u0027re essentially using this as a pre-screening mechanism to figure out what filter IDs apply the good news is it\u0027s very consistent in terms of the back office where you know if you think of old service order codes or bit sets or really just flagging rules to policies and then using masking to figure out you know when a particular product X or Y implement several rules rather than mapping that as individual IDs for just one of these as mask operations this is as we did our work and everything what we found out was happening in our policy decision points anyway and we also notice we have some other updates I\u0027ll talk about here a second terms of examples of where we found this actually being applied particularly in open flow switches in the metadata fields so we also wanted to pick up some of the common naming that we kind of enjoy over in 3gpp as well for grouping so this is all about efficiency for those who are familiar 3gpp charging characteristics it\u0027s there it\u0027s just a little bit more generalized next slide so this is once again an editorial update we didn\u0027t change anything structurally what we did was we reworked the examples based upon the feedback once again from the last meeting so next slide so as a reminder this is the relationship model and here we talk about policy entities being kind of your rules and having this bitmask assignment and then the the group mapping type users have memberships to multiple rules either through the domain which is very similar to a base name as well as the the bitmask value so this is your base relationship model and that was added prior to the last meeting so before that ins alt text and definitely not a flood inspect to read so next slide so we have "
  },
  {
    "startTime": "00:24:50",
    "text": "a couple of examples that we\u0027ve added but this is more of the visual so one of the issues we encountered was over on the left side so in Sdn switches we\u0027re typically limited in terms of the number of tables so we talked about Sdn switches serving a large number of customers one of the typically issues we run into is the default rule so the match for any any traffic and then applying different treatments in the typical world if we just stick to the traffic headers themselves these rules overlap and we need multiple tables what we did by adding essentially the metadata information basically the match type and the membership values in here is we\u0027ve added them in this example to table 0 on the rights and so what you see is essentially and we\u0027re just dividing this up by subnets but what we\u0027re essentially doing is we\u0027re table 1 would actually figure out what subnet you belong to then apply your subnet specific policy we\u0027re able to with this metadata value essentially to concatenate that into a single table so you see if we\u0027re in subnet 1 we set the membership values initially so we\u0027re just setting up basically the bit mask involved and in the next table we\u0027re actually concatenated so you\u0027ll notice that these rules no longer overlap in table 1 on the writes we do have essentially the any-any traffic descriptors in on table one on the right and we\u0027re actually now relying on the match type and membership value to really drive essentially the base so in table 0 you set that membership value of the user here you then match against it in table 1 over here this allows us to go to a two table solution and like I said effectively or justic the filter and really what it boils down to is especially nasty end switches you know we have the metadata field we\u0027re really just clarifying how you would actually use your metadata field and tie it back to a customer and so this allows us then to kind of reduce the number of tables so this is just one example so next slide please this is a much simpler example if you\u0027re not doing this in the decision point you can actually apply this type of logic upfront or excuse me in the enforcement point you can apply this decision up front at the decision points determine that at runtime also cross-reference it with your time of day if your your enforcement point doesn\u0027t actually enforce time of day and then send down the active rules for the enforcement point so this is a very simple example but what\u0027s important about this is it\u0027s using the exact same fields it\u0027s just basically pre executing if you will that metadata match and this really just boils down to the fact that we do not change a user\u0027s relationship with their products or services typically during the session when we do that we typically do a reauthorization as an operator so this allows us in certain decision points to execute here so we\u0027ve got one example where we show it basically expanding the filters in the enforcement point we\u0027ve got the "
  },
  {
    "startTime": "00:27:51",
    "text": "second example where we\u0027re socially you know what I\u0027ll call a much simpler enforcement point enforcing it at the decision point then sending it down so those are your two easy applications and you can imagine combinations thereof and all sorts of optimizations taking place if there\u0027s multiple levels of hierarchy but we just wanted to keep it to a couple of examples here rather than spending a long time writing a dissertation and an RFC next slide so we\u0027ve talked a little bit about applications today we actually do use this so as an operator you know when and I\u0027ll give one real-world example I\u0027m quite sure you can guess which operator it is both about 60 million devices there were 1200 rules or at least what was thought to be distinct rules upon further analysis you know 1200 rules in open flow across sixty million devices is a little bit daunting and just not going to work at the end of the day it just isn\u0027t happening so we ended up using the metadata as we described along with the binary masking so we have in we\u0027re a mobility side in this implementation three table downlink design once again where we match the destination IP set the metadata in the open flow if you\u0027re familiar with that and then carried that over had the original role plus the extra metadata to do the deduplication and in executed we\u0027ve done the similar design in the uplink this just kind of gives you the idea values in terms of how we approached it and you can talk to me later offline if you have more questions about it a similar design of the uplink basically the 1200 rules once we were deduplicated became about 200 and then we found some other optimizations as well and I hate to say it but really we like to think we do lots of things arguably we really only have 12 actions so there\u0027s only 12 to 16 rules that really make up that particular what we thought was 3040 distinct products so this is where you know as marketing you think you\u0027re this big as Network designers you think you\u0027re this big as engineers you think you\u0027re this big and then upon further review you\u0027re doing like maybe twelve or sixteen distinct activities so we\u0027ve been able to use this technique and apply it in our systems today on Sdn switches and reap the benefits so it kind of gives you an idea of where this is I think next slide so next steps this is not a fun document it\u0027s not an easy one so I\u0027d appreciate more reviews I know we had some good feedback in the last meeting before we actually even bother with any sort of last call I\u0027d like to hear more from the group so TV adoption so question for the floor so who has read the documents three "
  },
  {
    "startTime": "00:30:51",
    "text": "and so first of all any question for clarification or comments at least have one it\u0027s your so you said that you may so when you said adapted in this purpose of things that the the use of beijing\u0027s was introduced in 3gpp so we pick membership domain okay not to confuse right so base name right is more of an organizational concept of rooms and here we\u0027re trying to pull that up in sort of the metadata extraction because we did not want to confuse people who were already actively using basically as a convenience mechanism but we wanted a simpler a similar aggregation concept and still maintain backwards compatibility so we don\u0027t want to decoy and say it\u0027s something completely different we want to keep it separate enough so that because we do use base them as an operator so we wanted to basically point out its it\u0027s similar and for backwards compatibility purposes it would be a separate AVP so we also gave it a different name since we\u0027re talking about sets and membership we just refer to it as a membership domain so what about so I think that as you said I\u0027ve read the document but I think we need I would need also another review to be able to understand because actually I understood the exactly the mechanism arriving at the end of the document especially when the ADP defined I think it would be useful to have more review and we see what to do with this one again yeah I would my ideal situation I\u0027m still not happy with the examples I would like them to be straightforward all the way through the documents in transition but trying to describe that and set logic and everything you know as a single author it\u0027s it\u0027s a bit tough yeah I think one of the point of so would be too soon so maybe it is there but to clearly clearly identify why it is not so easy to do it with existing mechanism today yes what we are losing a chip yeah and yeah and I think quite simply and we distinctly say it in just a few sentences but but basically all of our filters today are based on time and what\u0027s in the packets and unfortunately when the times the same and the rule is particularly like a default rule unfortunately that commonly that\u0027s the most common overlap we have in the system and so and so that\u0027s just one example but that\u0027s usually where we run into but thank you see something that you can understand easily when you see the definition of GBP boots India rationale the creation of all right "
  },
  {
    "startTime": "00:33:55",
    "text": "thank you recommendation say you have about 15 minutes good I can ignore most of the errors then this will be made thank you so next one so you know as I apologize but I wanted to be a little bit operator specific so we\u0027re doing a few projects here and actually I\u0027d said you there\u0027s an update but we\u0027re doing a few projects here in implementation and open source and we just basically notice that over the wire everything was great but getting to consistency compatibility above the wire line was a real pain and so I am fortunate enough that I get to torture my developers because I also get to do research on them so what we did was we tracked under two repos what was coated each night and what was coated each release and I was able then to track back what was happening when an error occurred or a test incompatibility occurred and what we found was 85% of the time and inconsistency above the over the wire or due to an error or a lack of inconsistency or some ambiguity in the text of the spec that it was associated with or you know and the developer I would say filling in the blanks rather than coming to us and asking the experts what the question was they would fill in the blinds so this is more about people being very nervous and trying to get code done then it is about aspect being right or wrong so that started a campaign we essentially thought we you know as a litmus test to prove the developers wrong you should be able to go from spec to code very quickly and do most of it not to automate the developers but to really validate that specs can be a validated and B our lettuce tests would be automatic code generation so next line we were terribly wrong and our hypothesis that we could do this consistently quickly and efficiently or in some cases at all we ran into over nine hundred and seventy different errors looking at thirty different specifications we categorized them in large swaths hey excuse me and this is just the highlight so even in the document just so you know I don\u0027t list all 973 errors I only for your pleasure only go through broad swaths of categories so please keep in mind it\u0027s not an exhaustive list in the document but we wanted to get this out to the group so in this task of going from spec to code the first thing we found was too "
  },
  {
    "startTime": "00:36:56",
    "text": "many table formats we couldn\u0027t argue that it was a change over time because some of the specs that we looked at started and ended at the same time so it was just inconsistent table formats for things like defined in imported a VPS the imported a VPS were actually very tough some specs reused a VPS that didn\u0027t have import tables so we would try to actually build and compile a spec and then realize we didn\u0027t actually have the right AVP loaded or this spec was wrong or the number was wrong or the version was wrong so that slowed us down there were some outright errors particularly in 3gpp and billing in release 14 a good example is a lot of the a VP\u0027s driven by extra services for release 12 - or at least 13 are mentioned in 32 - 99 and even in 298 right in terms of the over wire format but they do not appear anywhere in any other document they\u0027re around typically machine type communication prosy has a lot of a VPS defined in fact there\u0027s an entire swath of a VPS where we found that there was an attempt to add to the 3gpp registry and then it looked like the CR never made it through and so when you actually go look in for the AVP codes than missing almost 64 ATP\u0027s so there\u0027s errors and we\u0027ve gone back and looked through the documents as a group so it\u0027s a bit challenging to get implementations and we\u0027ve documented as much of that as we can but these are all typically we found actually just machine errors right so exactly tried to do something or there\u0027s a lot of document references and we chase documents but this is pretty pretty typical across the spec so keep in mind we\u0027re looking at a lot of specs as well so didn\u0027t know I can pause any time for questions itself it would be wonderful if you could send some of these errors to essay 5 charging or if I you know even if you don\u0027t delegate there we don\u0027t so but the other side was as a stream of consciousness we decided to put them in here because we also wanted to write we at least want to get them out somewhere and I apologize my particular group does not but we wanted to get that out in the open so um a lot of this is also the fact that if we talk about programmatic verification there\u0027s some things you just can\u0027t get from us back so an app ID it\u0027s there but try to find a consistent way to find the app IDs that are in there try to find you know so you start tying all this data together in the name of this right textually as a human you can read it but what you sit down and say how would I write code to actually parse a document validate it like we do with knits and verify that it\u0027s good things start you know we things start missing in the documents the next one "
  },
  {
    "startTime": "00:39:56",
    "text": "so our methodology basically we create a we split you know not because we need to but because we\u0027re experimenting here we pull out the AVP defined tables the imports tables we pull out metadata based upon the the app IDs and then we convert the document to text we actually have some Python that converts it into the die a fuzzy format that automatically generates it and then just as a secondary validation we have exporters that export that to free diameter library code to verify that we actually can compile something and we might have an idea of what we know what we\u0027re doing and this forced us into individually tackling the problems a lot and it also forced us to really do things that we wouldn\u0027t know as a human we don\u0027t care about but when we start running a compiler we totally do so this really forces items we did in several cases have to add items as a appended file and I won\u0027t even begin to discuss you\u0027ve seen in the document we recommend not using emotes that\u0027s how bad it got for us and there\u0027s lots of inconsistency there so this is kind of the base design for that next slide so we did have some unexpected use cases in terms of a VPS that refined further refined an existing grouped or command so because we didn\u0027t have the linkage back to the original a VPS this is actually really hard in terms of setting up the library in the code and populating libraries at the code level and so without that information what we found was we generated some really nice code that was completely useless because what would happen is it would redefine the AVP in the library and basically rather than saying I extend the AVP which many libraries support it would actually attempt to re define the AVP in the code and get a conflict so for you know the API soft where that was incredibly important although it\u0027s easy to detect because if we look at the a VPS that are imported we can realize it\u0027s defined and imported it\u0027s a very simple situation we got a lot of inconsistencies we\u0027re in this situation because it\u0027s defined in imported sometimes it showed up as an import sometimes it showed up as a defined and sometimes there was no mention of anything which then you know left us in the software with some things to deal with and there\u0027s no concept right of imported commands so even don\u0027t even forget about and just forget about it so next slide so you know I hate to say it I\u0027m an editor of four thousand six so I was not immune to this we found a couple of errors a typo so this is not me going around and trying to be yeah actually I thought I had a good spec I\u0027m just I\u0027m just happy I found like only two out of the whole thing so and will will correct those by "
  },
  {
    "startTime": "00:42:57",
    "text": "the way with the cumulative update our plan is to correct those two items because they are editorial as well as the the update on the reference in four thousand six and in the next revision excellent you know the first line just says it all over the wire diameters fine trying to programmatically pull out in numeration z-- if you want a root canal every time you do it out of this back you go for it part of it boils down to your right we don\u0027t specify the label formats we don\u0027t specify the orders and it shows even in some documents we would have and it it showed the age the longer the document the more revisions so so typically we this doesn\u0027t happen in RFC\u0027s because right we\u0027re kind of a one-shot and multi revision documents we would see three or four different formats for enumerations in the same document so the longer lived your application the higher probability you got wild inconsistencies with enumerations we also see characters that are invalid in any language there are some labels that have slashes in them but one so there\u0027s there\u0027s no hope of generating any code let alone parsing so we did see some crazy things just plain old references the web pages with like some anchors that kind of got you close typically referencing OMA integrations we did see reference to registries which we would expect now they weren\u0027t actually enum registries they were you know different protocol registries and other items so and we didn\u0027t see reuse of enums which was interesting where they would either extend or say we were use this enum but not that one value which I\u0027m not even sure how we would go about enforcing that in the library so just generally what I\u0027ve got my group doing is trying to avoid enums at all cost at this point so next slide recommendations this is really simple just go for consistency and spelling in case and you\u0027ll go a long way just get rid of the spaces between all your dashes that happens a lot more than you think that also helps we need to decide whether or not we\u0027re Kate we\u0027re truly because the thing is diameter over the wires fine but we need to probably decide are we really case sensitive when it comes to these IDs for the names or my favorite one is could we please get consistent naming on the command ylabel versus the command acronym versus the actual command definition I would urge you to look at re authorization request and the number of ways it\u0027s actually specified there\u0027s a running bet on my "
  },
  {
    "startTime": "00:45:58",
    "text": "team as to the number of new ways every time we learned in aspect that really extended one more time by the way so far the answer is 15 different ways to specify reauthorization requests in text so far so good anyway there\u0027s a lot of things that we do this don\u0027t try to use underscores unless it\u0027s part of labels ready I won\u0027t go into the details but it makes life interesting for software in general and n constants but it\u0027s kind of this next slide we do have some some other recommendations in terms of tables this just kind of gives you the basics it\u0027s a lot of what we\u0027re doing in the latest specs but it\u0027s just the point out kind of formats the next slide kind of shows you a couple example formats that seem to work really well ok thank you so this kind of shows you some things that work out pretty well in terms of formats that are easy to parse if you\u0027re wondering there are over 19 known regular expressions in our software it\u0027s actually just do tables for defined tables we reduce that from 28 so we actually cheat and reformat we drop columns we think we can reduce that to 12 we\u0027d like to actually just reduce it to 2 going forward so we understand backwards compatibility but this is a lot of work if we talk about programmatic verification next one important a VP\u0027s if you import them please notate just please do it I won\u0027t go into a long diatribe but it\u0027s really inconsistent with the full understanding when you\u0027re refining it it\u0027s a little murky right that\u0027s an open question what you should do when you\u0027re actually running this right probably adding whether or not you\u0027re changing the N bit semantics which by the way per that the guideline definitions has other rules that kick in but at least noting that there and the table so we understand that we should be doing some compliance checks which has also caused us some errors will help us out next level group a VP\u0027s and really commands the refinement we\u0027ve talked a little bit about that a couple things we want to note so we do assume a precondition of star of actually inclusion by the way there\u0027s a couple violations of that where there is a refinement and that and I will fully admit we accidentally attempted to do that in four thousand six and said what was noted so it does happen semantic rules for refinement right stay within the range of the original right if you\u0027re further refining the range just stay with the next why we did add a new item to our metadata into the header format we added refines and then the the app idea refining into the CCF in our intermediate format and that by the way took care of this issue for 99% of our actually all of our use cases so this just lets us know it\u0027s a refinement which then allows us then to properly generate code so once again it "
  },
  {
    "startTime": "00:48:58",
    "text": "doesn\u0027t hurt diameter over the wire but it does help in code generation as well as the engineer understanding that it appears in this map it will also appear in a different app ID that\u0027s okay that was intentional furthermore it was changed in the new app now arguably we thought at one point that we only refined the original app ID we were completely proven wrong with TS 2906 one I would urge you if you want to look at an extreme example of refinement use cases and and some very interesting ATP\u0027s that\u0027s probably the one that took us the longest over a couple days of just trying to figure out what was going on there and bringing it in but it\u0027s definitely I would call it the extreme example of what you can do with diameter also to me at least academically the most interesting but by adding this we were actually in pretty good shape now one of the more open questions is if you could do something like limit the range and say there\u0027s only five ATP\u0027s at it versus the starter that\u0027s an interesting question from a you know a programmatic point of view computer science point of view I don\u0027t know from a practical point of view I\u0027d ever want to say you can have up to five other ATP\u0027s inside this but it\u0027s it\u0027s definitely an interesting thought is that extensible would we ever allow that is that something and actually I don\u0027t know is that something that\u0027s permitted could I have a star 5 and then AVP no I have no idea how to even enforce that but that\u0027s a whole it\u0027s something that you can defined when you get decide that when defining the group baby so okay well please don\u0027t do that that would be yeah so I guess it\u0027s possible but I would be afraid that would be a lot of semantics checking so next slide so this is an example of how we\u0027re applying refines so what we do not supply the appid we look for the original definition where no refined statement exists that requires a search we could actually probably be more specific and add another keyword called defining or defines which actually just says this is the definitive route for this code but that once again helps out with this so this is a practical example from 29 128 in terms of refinement so you know and the rules are in there so next slide I was to jump into the conclusion because yeah yeah yeah absolutely so I think we get yeah we\u0027ve got there\u0027s there\u0027s a lot of little stuff in here there\u0027s some some great open questions like this do we go back and fix the things we\u0027ve identified that\u0027s a that\u0027s a more working on in question and more about the future of the workgroup I also don\u0027t want to go back and fix things that we\u0027re really not carrying forward so I would argue it\u0027s at the very least if we "
  },
  {
    "startTime": "00:51:59",
    "text": "commit to it it\u0027s a case-by-case basis based upon where the the errata applies and I\u0027m more than happy to go back and file where I need to the errors but we wanted to get it out and get it disgusts more than anything else we have a class of unit thirty twos that we\u0027ve seen called pseudo enumerations where wherever it looks like in the spec they\u0027re trying to avoid enums altogether and do we want to support that as a use case that seems to be kind of the current trend and some of the later specs that are fairly useful and then what do we want to do undefined and so those are kind of the open questions so next slide so um summary here is is diameter with a wires fine this is evidenced by the fact that we all interoperate together but going from spec to validation respect to code that\u0027s an entirely different story and so there\u0027s an opportunity for for doing better we can add a few things into the form oh it\u0027s entirely optional and we can even implement some of the other formats these are all minor items they don\u0027t affect anything over the but if we decide to maybe do this as like a style convention as opposed to something that\u0027s that\u0027s normative we can talk about the tool whether it\u0027s at 3gpp or whether it\u0027s part of the I wouldn\u0027t make it a part of the nits process right where you kind of get rejected but maybe a validation tool I know Ben was in the back has a lot of yang validation and stuff that also does this so we know it can be done we know it can work through a process but it would be nice you know I think as an author regardless of where I\u0027m working to be able to validate so as far as the document like I said we wanted to get this out what we do with it is there and I think the other recommendation is enums genomes are a pain in the rear and I would argue I know Yanni talked about it the last meeting but enums are much worse than I anticipated I I know he knew this I you know I\u0027m feeling the pain now as a managing a team of ten people doing developments related to diameter and I would urge regardless of where it\u0027s done we should probably think about tackling enumerations in terms of formats and getting getting it better I\u0027d like to see some of the other use cases picked up as well so any questions any question or comments who has read the documents without and I know uni and Dave Dawson who could not be here also looked at the document as well although it was much rougher form so from from a practical point of view I think that you you as the user I think that the lot of stuff so I think that there is a two type of two types of mistakes first of long "
  },
  {
    "startTime": "00:55:00",
    "text": "first of all would be about the documentation issue it\u0027s more about how it is find senators like for instance when you say we find it\u0027s not totally refined it\u0027s because in 3gpp specifications that just I the EVP that they are using yes yeah they are doing the same for the commands and but you\u0027re right when you have coding people just taking the specification and and just not reading anything but just yeah just blinking yeah yeah because from their point of view they just want to get to the the actual grammar right the code and then be done exact and then they\u0027re surprised when it doesn\u0027t work because they\u0027ve well and they\u0027ll always tell me I followed the document yeah so I think that at least so so for this one I think it should be taking to account when when actually drafting a new a new specification and so on for the existing one I think it would be actually difficult because it\u0027s just a documentation issue but whereas there is something to clarify I think that it can be done either by an errata within a chair because when you said it will not fix normally people using the Arabs documents need to go through the errata to discover something is is wrong in the existing spec and it could be through CR to assuage EPP spec but we can see okay so um question then for you or for the group right this has a lot of errors in it and and I once again wanted to get the information out would this be more appropriate as something informational of which to drive requirements or recommendations kind of like the app guidelines as a style guide I don\u0027t want it to be normative and prescriptive but but where how would this document logically programs I think that this kind of document could be so from my point of view if in depending on what we would do after that but it could be just for information because it should only work it could be at least useful information for the community and it could be also even referenced by other things that there is no guideline here just to highlight what could be wrong or what could could cause some issues if you are not taking care of this type of stuff for instance and then it\u0027s a way to describe the levels and so on yeah so I would probably then based upon that change the style it\u0027s right now it\u0027s more of just a it\u0027s actually an air of this I can tell you from from a large repository but turning it more into a little bit more of a report style but actually it would not be it could be it could be good but it would it would not be the first time that way at this time but the comment ok chief it could be some we have some document just explaining all the issues that we had for implementing such protocol without any rationale and so on just to expand we\u0027re ok had some issues and that\u0027s you you will you would know you would decide yeah okay I will I will not "
  },
  {
    "startTime": "00:58:01",
    "text": "propose to to to jam to so thank you lying I will skip the milestone just to highlight the fact that there is nothing new on this one and I would just open the floor for discussion about the future of the working group so from so we have three remaining active document from working with documents the other one are either already RFC\u0027s or are or idea AHG level or almost sorry yes so we have few people in the room not so much activity on the on the mailing list and we have an additional issues that I will not be able to to go on my March chairmanship and I\u0027m working there is to me too much work with those guys in 3gpp no but honestly with the 5g stuff we have a lot of coordination internally and across the companies and also we need to set up columns one so at least I will not be available as I was in the past and and we have the same issue with the Uni so I moved from a company to another company so it will not be able to go on too much on this one so we have a clear issue here about about the future of the working group so what should be the next step except closing the group and what if it is a case what to do with the existing yes so if I remit this is then if I remember correctly the existing documents were all either post or near working group last call right yes so it is conceivable that those could be you know in the hands of the released to IETF last call or in the end of the area directors before long what\u0027s your timeframe for needing to step down as chair I think that my personally my main concern would be to know exactly when to stop adopting new working new documents just one shoe that\u0027s if we have a time flying or at least a work plan to say we need to conclude all this work and after that it\u0027s done I think even four unit will be achievable because we aren\u0027t the same in the same situation so the two drafts that are candidates for adoption the pretty good you need to know that I think the predictive units one would be quite reasonable they\u0027d be sponsored the group winners came out on the border from my perspective because that kind of hits a little deeper into the where you use the protocol but that\u0027s not out of the question right that we could we could consider "
  },
  {
    "startTime": "01:01:01",
    "text": "that as well I think I would need to look more closely think more closely about that for me to do it I also don\u0027t have to be the eighties sponsor right but assuming it\u0027s me I would look a little more closely at least for the group one I think we need the additional reviewing to see if there is interest in at least some people to be too willing to work on the right and even if we Hades Hades powerful thing is I like to have people review them but actually even lying said that if we need not only to review them but to comment and to contribute to this one if there is something to modify even in the shape of the document so for sure we need someone to work on and and as I said the end-to-end security from my point of view need to be need to be done so we will see after the Thursday meeting we sag if we have a people volunteering to work on so I would our yeah for that I would say let\u0027s see what happened to SOG if we actually see some realistic interest it you know maybe that\u0027s something we spent up a little mini workgroup for maybe that\u0027s I don\u0027t know how to somebody we lady sponsor or not I want to talk to the security 80s and I think Ben was maybe Sadie sower or the other possibility would be to this is just speculating we chartered I\u0027m with new chair to just do that which it kind of the same thing it\u0027s been a yep and do any work I think that if it is only I would say only it would be at least something that we will need to do anyhow but if it is only to go on with the existing one even with the new adopted at least one or two I think it is achievable in in a short timeframe I I would need to confirm with uni but at least for my we will need to share the work and we will need all volunteers because it\u0027s it\u0027s not only about the chairs it\u0027s also about people reviewing that right defining a new charter with a new chair I think only for that would not be will not be useful and and this feels like something the Mac might not happen in security area they may just to get the interest ago in there but we had to talk with the security agents okay I will talk I will ever talk also with a cat in before before the zagnut II so can i summarize that at least at this day so and obviously if there is something to do and there is interest and so on it would not preclude to reopen a new group based on this one or in a new in a new in the new context but to summarize the discussion is there any any other comment on this one so the existing document the existing working group document will be pushed forward at least for the predicta clinics we can we can "
  },
  {
    "startTime": "01:04:01",
    "text": "we can it will be confirm on the manganese but we can we can assume that this one would become a working group humans and for the group one we will check and after those documents we will close the working group yes yeah it would be we will we will we will confirm it on on the meninges but I think that further predicted the units I think that for the this one it would be there is interests and it should be straightforward so there is no reason to not adopt union but what was your point been so what is it been again my point was that if those two drafts predictive definitely and I and maybe group could be candidates for just 80 sponsorship hello it\u0027s fine if we want to go ahead and check on group interest and adopt them they can still change ad sponsorship if we decide to shut the group valve okay it\u0027s just let\u0027s keep in mind that I I\u0027m very skeptical skeptical about bringing new work into the group right now unless we see some okay so that\u0027s two remaining there so the individual drafts will be four will be candidate for an eighty sponsoring yeah I think the first one is a good candidate for that the second one I need to look at more closely I think about okay thank you so thank you for being there and so I receive the feedback that I was looking for so thing for that and so see you I don\u0027t know why exactly but actually drinks a week for a drink thank you it was "
  }
]