[
  {
    "startTime": "00:01:25",
    "text": "[Music] [Music] [Music] "
  },
  {
    "startTime": "00:04:27",
    "text": "[Music] [Music] if you have this article you should it\u0027s a lot of fun I thought of for that it\u0027s a lot of joy and fun you should you should read this with a lot of popcorn or with scotch your preference but it\u0027s fun I\u0027ll move on go search for it having an open Internet it\u0027s just fun so we\u0027re gonna start the iccid a meeting and idea 101 thank you all for being here early in the morning I know it\u0027s Friday and it\u0027s early in the morning so we\u0027ll try to keep this short and sweet before we start we need a few people you know the standard questions raise your hands now if you want to be a JavaScript don\u0027t make me choose somebody javis five thank you sir Matt I need one more person four minutes and I have a link here for to use etherpad so people can contribute so you won\u0027t be the only person taking minutes but I need a point person we can do a crowdsourcing experiment to silence it\u0027s like nobody can hear me I can see anything right now minutes minutes minutes Michael why aren\u0027t you raising your hand it\u0027s been a long time since it Akmal it\u0027s an icy crg somebody minutes please if you don\u0027t need minutes I\u0027m gonna spend five minutes asking the question or should we stop taking minutes in this meeting because it\u0027s really not a meeting I don\u0027t mind doing that having that conversation either are you gonna take minutes thank you sir you just "
  },
  {
    "startTime": "00:07:27",
    "text": "avoided this conversation so welcome again we will start quickly I\u0027m gonna hand out blue sheets in a moment please sign it please add your name and your affiliation to it in the meanwhile you can all read the note well I\u0027m sure you\u0027ve never read it before so take a moment to read it if you haven\u0027t dug them out well you actually should read it it\u0027s about IPR disclosure and I an idea of participation and it applies to this meeting so please make sure you can look for IETF note well and you will find this that\u0027s about all I\u0027m gonna say about this now so if you haven\u0027t read it please go read it here\u0027s the agenda but I think there\u0027s at least one modification that I know of we\u0027re gonna have Paul Paul are you in the room there yeah thank you presenting some initial proposed I Triple E standardization work which Paul and others who are proposing this in the I Triple E thought would be of interest to this room and they wanted to basically give a short bit summary of the work that they\u0027re proposing there so that we\u0027re gonna start with that and then Ingemar is gonna talk about VBR condition control for and l4s support especially for 5g networks Neil will give an update on what Google\u0027s been doing with PVR in the past four or five months Praveen is bailing on me so I\u0027m gonna call him out west I\u0027m not gonna call him out I would I want Praveen to president let that plus plus in he will hopefully do that again in the future but he\u0027s not going to be able to do it today so that\u0027s off the agenda and we have Michael shanira from Hebrew University to talk to us about performance oriented condition control it\u0027s a it\u0027s an exciting and a different form of condition controller than we\u0027ve seen in the past and I\u0027m excited to have him here so I hope that generates a fair amount of discussion I\u0027m not going to close that one very quickly because I think they\u0027re going to have time today to spare and then we have a very short presentation on in on a new idea for doing condition control and bandwidth canteen networks by ancient so we\u0027ll just show up with it\u0027s time for your presentation in the meanwhile if there\u0027s nothing that you want to bash in the agenda we will start do you wanna start Bob no yeah I\u0027ll advance that\u0027s "
  },
  {
    "startTime": "00:10:33",
    "text": "probably the best thing okay just real quickly show hands how many people were in TS vwg okay so this is the Cliff Notes version of the presentation I did yesterday so well but I think I may have some clarifications on the some of the comments or questions as well so as mentioned we have a proposed project and I Triple E 8o 2.1 for congestion control algorithm or congestion management algorithm I guess I\u0027d say and it\u0027s proposed meaning it hasn\u0027t started yet but it has is in the process of approval and one of the things that we need is more review and feedback and because it\u0027s intended to cooperate with congestion control from the IETF we thought it was very important to do this presentation so next slide please so back in November 802 don\u0027t want to agree to create a project the project authorization report that\u0027s a proposal it goes through various levels of approval and ultimately the working group votes to create that project so we decided to delay this project until July timeframe in order to obtain more simulation data more approval more review from others so again this is why I\u0027m here the motivation for this activity is documented in a report pre standardization report from the I Triple E 802 the link is here this is an open process that involves end-users and standard and vendors to kind of define the requirements of the next generation data center so have a read of that and and send me comments so that\u0027s where we\u0027re at so what is this thing we\u0027re talking about next slide please so first of all we are amending 802 dot1q which if you\u0027re you know familiar with 802 that is the book that described Ethernet switching which is really the fundation for chips that are used in routers data center switches and routers today as well so it\u0027s all sort of related and the scope of this project is specific to data center environment so within the data center so we\u0027re not talking about across wide area links and what we\u0027re doing is we\u0027re trying to isolate flows that are causing congestion in these environments like rocky v2 or you know high-performance data center computing kind of artificial intelligence types environments so the way this works is that this bridges but I\u0027m calling them bridges because that\u0027s what our terminology but again fundamentally there are layer 3 switches routers you know they have Ethernet links the Asics have effectively one set of cues on on the port\u0027s the so they\u0027re all shared between the layer 3 layer 2 functionality but what we\u0027re doing is we\u0027re going to identify flows that are creating congestion in the data center and so a lot of questions came up about well how are you doing that well one of the challenges we have in 802 dot one is is finding the right level of specification and allowing "
  },
  {
    "startTime": "00:13:33",
    "text": "implementation flexibility as well so we don\u0027t typically try to over specify things we\u0027ve specified behavior as and whenever we can and so we\u0027re assuming in this scenario that there\u0027s already many mechanisms that are in chips today that are identifying congestion because you\u0027re doing things like marking the ecn bits and so we\u0027re going to use those existing mechanisms or we will use mechanisms that we have defined so there there are there are techniques in in 802 dot1q nonetheless you find the flows that are causing congestion and for those flows what we\u0027re going to do is adjust their scheduling adjust transmission selection so we\u0027re going to move them so to speak to a different traffic class and then we\u0027re going to schedule and mix in that traffic class in a way that doesn\u0027t reorder packets but it effectively delays those flows that are causing congestion allowing the indian congestion control loop time to react so the concern is that as data centers get faster and bigger that they\u0027re not going to be as reactive as we want them to be there\u0027s gonna be a lot of packets in flight with 100 gigabit ethernet and that we want to absorb those as we can and giving ecn time to do its rate adaption so that\u0027s the the high level and another trick and reason why we want to standardize this is we intend to provide a signaling mechanism on a hop by hop basis to the upstream neighbor to identify those same flows so that way that upstream neighbor can also do the same and and provide the isolation that allows them even a little bit more buffering and so it can kind of propagate back the hope is it never propagates all the way back to the source but because we want ecn to do that I\u0027ll explain more on that so anyway the scope is really about reducing head-of-line blocking for you know as separating the the flows that congested from the non congested and as I mentioned it\u0027s intended to be used with the higher layer congestion control defined here next slide please so kind of the the assumptions of the the lossless data centers of today that a high performance computing or again storage Rocky be to storage and vector environments is that the network is a layer 3 network first of all that all these links and boxes are really routers and the links between them are subnets it\u0027s a class network fat tree and we are using ecn as a you know DC TCP or rocky v2 with the DC qcn as an end-to-end congestion control mechanism that that that may be application specific so for quick for example or for for rocky B 2 there that\u0027s more or less an application thing if you want to create a purely lossless environment there\u0027s the the ability to use per flow or priority based flow control so in other words pausing the frames and on a per class "
  },
  {
    "startTime": "00:16:35",
    "text": "basis this is often not discouraged or a lot of times people don\u0027t like to do that because it creates that of line blocking but that is your sort of last effort last-gasp to never drop a packet if we\u0027re really run - buffers and we don\u0027t want to drop and we\u0027re gonna do flow control and again these are all using typically code points as a way to identify the different traffic classes and maybe as opposed to VLAN tags these days next slide please so 802 that one already has two sort of existing congestion management tools I just mentioned priority based flow control so typically we have eight classes of service or eight queues if you will you can define those queues to have individual flow control and so that requires extra buffering you know so that you have Headroom so typically people don\u0027t turn this on on all the classes they may turn it on on one class or something like that but again the concern is that it creates like a head-of-line blocking scenario so as I push back I\u0027m blocking the entire traffic class so if I have multiple flows sharing that traffic class and they may be going to different destinations I\u0027m fundamentally stopping them all so that\u0027s the the negative downside that we see that causes some buffer bloat causes backup and congestion spreading so in general we see maybe priority flow control being used only at the edges to keep from over running Nick\u0027s not so much used in the core of the network but if you want a truly lost environment that\u0027s sort of your last your last tool in the bag next slide please there\u0027s another approach that 8:02 had defined for non IP type networks if you will so this is where your your fabric is a big layer to and you\u0027re doing something like Fibre Channel over Ethernet or maybe rocky version one where you didn\u0027t have IP II didn\u0027t have therefore congestion control so we were trying to create a layer two lossless Network and here we actually signal a congestion message all the way across the layer to fabric to a source and have the NIC so to speak perform rate rate control itself again that was sort of divine for these non IP based protocols didn\u0027t necessarily see a lot of use there was a mechanism defined here to identify flows causing congestion that\u0027s one of the mechanisms we could reuse or for our proposal next slide please so congestion isolation so first of all the goal here again is to work in conjunction with the higher layer congestion control we want we want the stacks the endpoints to do their rate adaption in the way they do but we want to provide assistance or more time for that control loop to work and we are doing that by creating yet another control loop so there that\u0027s the the tricky part the goal is making bigger larger faster data centers that aren\u0027t that reduced loss or having a loss do "
  },
  {
    "startTime": "00:19:36",
    "text": "this in a flow agnostic way when when observation is as we get faster links like hundred gigabit on and higher port density we\u0027re seeing less and less available memory in those switches on a per port per pack you know gigabit per basis so we are pressure on switch buffers so we don\u0027t can\u0027t just throw a memory at the problem and primarily reduce flow control and head-of-line blocking so next slide please ok so just briefly kind of a unless you will wait till the end to take comments um briefly how this is going to work so imagine we have an upstream switch in the downstream switch obviously we\u0027re going to have a full fabric here and there\u0027s traffic coming from all directions but just to simplify the discussion so as in the downstream switch as frames begin to congestion creates frames begin to Q in the egress of the switch the logical or physical egress of the switch this is there\u0027s some thresholds here at which point we will be detecting congestion and this is where mechanisms that are being defined in IETF could be appropriate or ones that we have already used we\u0027re not necessarily specifying that the specifics of that yet that\u0027s an option but once we\u0027ve identified congestion and I can kind of flow that may be causing it maybe it\u0027s sampling we will notify we will remember that we will keep track of this flow and subsequent packets for that flow that are coming in we will now queue in a different class in a different traffic class and we\u0027re going to do now we\u0027re going to schedule that class in a way that avoids that out of order packets but the key is that we\u0027re effectively sort of delaying that and using buffering in the switch to delay the flow that\u0027s causing congestion now this might be an elephant flow statistically but we don\u0027t know but hopefully we\u0027ve got the right the right flow next slide eventually as that congested queue begins to back up we want to tell our upstream neighbors so that we can use his memory as well so we this is where we would send a signal we call it a congestion in isolation message which is effectively a message that includes the flow information so the upstream switch can identify the same flow and and it will do the same thing next slide so then that that upstream switch will also be isolating the flow and that\u0027s where we effectively now are eliminating head-of-line blocking so the the non congested flows the mice the green the good guys there now you know don\u0027t have the congested flow in their path as much so they\u0027re able to to eliminate the head-of-line blocking the next life now if you truly want the lossless environment and we start to backup the congested queue then we can pause that queue and at that point we\u0027re blocking the entire class but the goal is that in that class is is effectively only the congested flows David wanted to quick "
  },
  {
    "startTime": "00:22:36",
    "text": "check on what\u0027s going on here are you doing dual queues are you doing queue pairs within class or do you need a second priority or class to odd to make this work right so one of our objectives is to try to make this as easy for existing implementations to use and so the objective in the idea is to use an existing traffic class so we would be using a new tree an existing one that\u0027s already there because and the reason why is because of that last step if I\u0027m going to pause the congested queue I need to actually be able to and use the existing pause mechanism I need to have an existing class okay so is that is that step four pause causing both queues or did you put the top queue into what\u0027s effect is a separate priority right yeah it\u0027s a good point it\u0027s it\u0027s pausing only it\u0027s a it would puzzle initially would pause only that queue however there could be some packets still in flight you know that there\u0027s a possibility that we might have to pause both queues it really depends on what is what residuals are left in that nonk ingest okay sorry I think I\u0027ve got the answer which is every time you want to use this for a traffic class you need to put you need to assign another one of the eight traffic classes to the congestion isolation mechanism yeah now it would be possible to have say two or three non congested queues and only have a single congested queue but you would have to coordinate that but it doesn\u0027t have to always be pairs but if but the expectation is that it would be a pair as you\u0027re mentioning okay next slide so we have some early simulation results I\u0027m not going to go into great detail above it the links are here but we do see our objective remember was to reduce the use of that pause frame because that has been known to cause problems we did see some reduction of that and improvement in flow completion times particularly for kind of quote mice frames if you will this sort of naturally or statistically does of myself in separation if you\u0027re assuming that you know the the elephants are the guys that are effectively causing congestion you kind of statistically get that in a lossy environment we also saw a significant reduction of packet loss so we were by getting the flows that are causing congestion out of the way we reduced packet loss and that really has a huge benefit on the mice and control frames so again the details are there and we\u0027re gonna do some more there was some questions about what you know the modeling and as usual I and I would love to work with anybody here on simulation if that was an option okay last slide so what are the next steps so again we\u0027re socializing this with with many people we need technical review we\u0027d like to do additional simulations all alternative switch architectures that and different congestion control algorithms I\u0027m very interested how this interplays with "
  },
  {
    "startTime": "00:25:37",
    "text": "things like bbr or other time-based congestion control schemes or rate based not it\u0027s not explicit marked and we would hope to progress our motion and start this project in July so how can the ITF participate so we already have an IETF I Triple E inter working relationship with that would be a great topic for that to discuss you can email me directly comments and I\u0027ll be happy to work with you engage in and do whatever we can to vet this idea a little bit more and then we also have as I mentioned there is this paper that we remote where we\u0027re motivating this problem and we\u0027re talking about the use cases and would love again collaboration on that it\u0027s an open process anybody can work on that we have vendors and we have end-users today so have a read its ads as well and that\u0027s it thanks to eat comments Matt Mathis um it sounds like this method is focused on there being specific pairs of ports that have hot flows between them well I showed it when I described it I\u0027m only showing an upstream in a downstream but the reality is the cloth fabric so they\u0027re kind of coming from all different right I was gonna say I\u0027m not aware of that many environments where it\u0027s a single port pair and and a much more common case is what we call the in caste problem where you have every input port is landing on the same output work and then correct okay yeah I used that simple diagram as to illustrate the mechanisms because the formal problem is probably got a lot less traction as something but the latter problem the real the in caste problem correct it might make sense yeah if you have a look at our paper we talked about the key problems and in caste is is there any one of them Bob Michael I just wanted to know most your presentation is about this is what we\u0027re gonna do and then at the end you say and we want to try different queueing disciplines different traffic mobiles all the rest of it are you saying you might not do what you\u0027ve most mostly described if someone comes up with something better or you\u0027ll definitely do that but you might do other things or yeah so the key part as I mentioned in 802 standards is we don\u0027t we\u0027re not you know it\u0027s the balance of how much of it is known versus how much of it is not known what that\u0027s what the answer is and I mean we have a pretty good idea on how we want this to work and integrate with the what\u0027s ship models and things that exist today but it\u0027s it\u0027s not done the typically what we do is put this project authorization request which defines the scope and like you know and we\u0027ve already gone through defining objectives and goals we can entertain multiple proposals for how to achieve those and bake them off and do all that that\u0027s "
  },
  {
    "startTime": "00:28:37",
    "text": "that\u0027s not part of the process so that\u0027s against soliciting these ideas and much an outcome be that are cheaply standardized is a couple of couple of mechanisms then the sort of market decides between them yeah that a couple of switch mechanisms you know and they can be put in different yeah yeah yeah or we can say yeah you can say here\u0027s one idea but it\u0027s open to use others at the end of the day we want to see this external behavior we want to see you know no repack and reordering or some level low level and we want to see a message being sent that gives me enough information for an upstream switch to do you know so that that\u0027s where you have pure interoperability and then the behavior can\u0027t obviously can\u0027t conflict with one another but try to avoid implementation details if possible is that if that makes sense yeah but but what I mean what I meant by - not necessary - in the same network so that one network could use one and another network could use another but switches have got them both you said I mean oh yeah yeah Singam are next okay and is just about the presentation of this VBR and it\u0027s a useful FRS with alfresco support and it\u0027s a few experiments and findings and it\u0027s quite sort of recent results and it doesn\u0027t claim to be the D solutions and there are other solutions also in the pipe okay microphone yeah and connect the next slide and there you probably have seen all this there\u0027ll be all what it stands for and what it is and it\u0027s implemented by Google and and the scope of this work is to modify bbr with the l4s support it doesn\u0027t address this the packet loss issues that are identified with short or ism and the reason why I wanted present is that these changes are very minimalistic and it\u0027s a very easy to implement it in real and these are simulations not not distress it that it\u0027s not no no real test beds that are run with this okay do it like this do it my cat fell on it and - it\u0027s a modification start all done is that do use Alfred supporters is added and this ENL code is from the TCP DCP "
  },
  {
    "startTime": "00:31:37",
    "text": "TCP and it does this special thing it doesn\u0027t do accurate easy on and the main changes the function will be your update bandwidth and totally you update the bandwidth computation and remove the number o see more packets in our equation and divided point in factor 4 in this case and it\u0027s not 2 or 4 it doesn\u0027t really matter that much and and there\u0027s only additional state arrivals the total it\u0027s a delivered C needed while deliver is specified and delivered is the amount to deliver the packets and it sort of update the delivered and is and see more packets to take next slide please and the game cycle is changed from a torta tease to 3 Ortiz and also reduce the gain gain variation a bit and it sort of the game variation is essentially haft it gives less jitter and but sometimes he can give the slightly slower rate increase and in or in some cases if they\u0027re competing competing flows in the same boat the next return they get the worst sort of a convergence the mean or TT probing is removed but that is for simplicity to make it work and one reason why you can remove is that the alpha risk is very short or a 0 Q delay but in the end you may still need minority probing for instance if you have 5 d FR g dual connective whatever flows to change from LTE to fight un or any kind of changing or titties the bandwidth window is the max bandwidth window is reduced to 2 or titties or 3 or titties and one warning here is that the two short winner can actually reduce performance for a pre-application limited traffic and it also gives some more noise in the bandwidth but the benefit is that you get the more a quicker rate reduction if you get to congestion and additional one one is that you exit PBR start startup if you experience more than one or two T or C mod packets that\u0027s wrote it and can take next slide and this is sort of essentially if you look at for instance BB or update bandwidth is that only the boldface read the port is that ad in the code but you need additional code in a TCP input dot C and TTP rate sort of account for this earmarked delivered packets but all in all this is a very simple modifications and gives a reasonably good results already the next Lloyd and here a comparison between BB or and it is the BB or evil and simple both the neck with "
  },
  {
    "startTime": "00:34:39",
    "text": "the minimum or TT 20 milliseconds and Arnelle 4\u0027s mark tresor leau two milliseconds on them and what you can see is that the both the track the band would changes and marked in red here quite well and the BB or even in some cases you may not see it in some case it\u0027s a slightly slower in the rate increase and what you can also see that BB or has the kiss or TT probably that reduces the throughput quite a lot indications it is not present in the in the l4s adaptive version and if you look at the cue delay you can see that 12 additional well for s mainly reduces to scan standing cue it also reduces the delay spark but that is more attributed to a sort of mean max bandwidth window so al fresco Slee reduce this tanning cues faster okay and this is an example when it\u0027s optimal prior phantom queue and this is total mind interpretational phantom queue in reality is Tandy her slightly differently we did a virtual cues on we can see that you managed to reduce queuing delay to almost zero yeah what we have is the actual zero ization the way or the packets and you sacrifice some 10% pen with what you get more noise in the throughput also can of course say that this is well through postmaster over a very short period is like 20 milliseconds so I will actually sort of changes due to OTT is that next slide and if you look at multiple flows it can this is sort of a simulation where you have five flows that are either corner to pass on arrival process process we don\u0027t in 1000 point two floors per second so they\u0027ll appear at random the first blue Flo appears to me on the red at some random intervals and you can see that the BB or it doesn\u0027t manage initial to sort of keep the queuing delay low and that\u0027s quite natural because you have this late converse at wanted children let\u0027s go but it stabilizes talked a while at least and the flow rate will converge or relatively well like next point and if you use this with l4s then you get two slightly better flow convergence and you reduce the cue delay but it\u0027s not ideal ideal issue they\u0027ll have something like two millisecond q delay but it\u0027s so more like five millisecond PQ delay and what you can see also the newly-arrived flows the ramp up more cause as long as that\u0027s because you exit this beep your startup mode next Lloyd unless the only hundred "
  },
  {
    "startTime": "00:37:39",
    "text": "songs don\u0027t provide a phantom key and you get slightly better convergence but it can\u0027t really say that this sort of a just happenstance or so and it\u0027s slightly lower Q delay and that\u0027s it sort of you don\u0027t reach 0 Q delay with phantom to you if you have more flows that are competing the bottom item that there\u0027s something that needs to be addressed audience should get worried even lower key utilized and then we have an exploit let\u0027s talk about try to mimic case very good for in for instance the beamforming in orange totals get slightly worse beamforming and you lose the BM slow it\u0027s done this will put gradually decreases or the course of 400 milliseconds and you can see that the BB or Evo here managed to keep the queuing delay lower than PB or the BB or sort of a has spikes up to sixty minutes you can kill delay and and it but you don\u0027t reach the 0q delay with vb or avoider it\u0027s up to like 20 milliseconds if you look at the next slide you can see the reason is that the max panel it is slightly overestimated at first yeah 1.05 second you have a too high bandwidth after that you get on bunch of seeing more packets on the bandwidth is pushed down and then you have the it\u0027s this process repeats itself so more work is needed here to get a better bandwidth estimation and the trick is here that you can of course make this make this more coercion but then you can easily end up in a problem problem that you get the worst convergence if you are more flows that compete for the same bottom actually it\u0027s a excuse between the devil and the deep blue sea in some cases real litterbot ott fairness in this case I must say in this case it\u0027s very short to test and you can see that PBR you is it will expect it behaves as expected that if you are the longer or tt stand you get the lower share of the bandwidth PB or on the other hand total doses complete the opposite if you\u0027re shorter or tities command compete with a flow with a longer oddity to get the lower share of the bandwidth it\u0027s a it\u0027s a feature a BB or II we in this case it sort of makes things it sort of flip this upside down or it turns your throat or right side and then another example where the sort or entities on a comparison I still takes a nice slide you have roughly the same behavior and it can with a shorter OTT scenarios annual water tea unfairness differing oddities and they get a better or TT "
  },
  {
    "startTime": "00:40:39",
    "text": "factors with another with that is no real for a smoking sorry finance and conclusion after it it was quite easy to modify PBR for this case but a probably a better way to do it with more sort of a brainer then more thinking involved conduct and then if you additional Adele Presley you get good convergence with multiple flows competing on a certain box makes it it keeps defending you small or where it\u0027s more with the phantom cheese and you get a certain degree of jitter and that is a result of the bandwidth probing that is still present in this with an app to BBB or and what you need to be a in this implantation need to be a bit aggressive in order to get your fair share and about bottleneck otherwise you will install if your to coach doesn\u0027t need your than we do we go away sloop it will go away that\u0027s about it you have a few minutes for questions you chunqiang thanks for this work very appreciative you\u0027re the small change about easy in and and seems very interesting I would love to try it on our environment too there is only one change I\u0027m a little bit afraid off which is to exit startup or slow star on the first rung of experiencing easy end I think this will cause the same problem the baby are trying to fix originally which is an hour on the Google back on when links you have this elephant copy flows and they\u0027re getting all this sort of bursty losses or likely bursty easy and signals because of some Incas happening or you know just bursty small mice traffic coming driving by we call that you know lost by the drive-by traffic so VP I might be exiting the startup phase us essentially slows are very early and then we have to rely on this either 1.25 can or in your case one point 12 percent again to slowly grow up to the Penguins because it could be or could be right about that and that could be an issue yeah yeah so that part we might need to you know discuss further yeah but I think that the the rest looks not very interesting thank you so Gauri first I like this piece of work I like the idea of being able to take some signals and really figure out what to do rather than just carry on blasting into the internet so "
  },
  {
    "startTime": "00:43:40",
    "text": "this looks really cool I appreciate this and your benefit seemed to come from multiple things so one of it was using ecn as a signal at all which applies 12 for s but would apply to non l4s it would apply to a boar other ecn methods it\u0027d be kind of nice to know if you have a feeling for how much comes from easy an and how much comes from bbr yep and I mention it briefly honorable evil one a sort of a trick to that universe was a short or bandwidth window and that sort of reduced of the length of the various parts and the other one is to reduce understanding Q that you could see in the BB or example one will do all the slides for your changing bandwidth take some mana be the first craft on the first craft be with slides yes and you can see it after the blue graph a sort whether the cue delay gradually goes down with BB or and that they circulated that is attributed to m4s marking that it comes to cue delay real fast but the window the delay spikes and possibly also the height of the last parts that is the chain difference there is due to the shorter bandwidth window okay I buy this as being as being a very useful thing on its own and I like the phantom cubit as well which is probably something that is a separate thing but it really helps convergence so thanks very much for the talk Andrew Gregor Google I\u0027d like to second Gauri\u0027s thing about change one thing at a time so that we can see with what the impact of each of them is now I mean is it exploration this is this is great work get me wrong but from an engineering point of view ought to be very nice to know what the contribution of each of the individual variables is and I have one to add which is that one of the things that stands out particularly in this plot is the impact of bebe as Max filter window consider using something like a percentile tracker and softening it there\u0027s if you search around you\u0027ll find that there\u0027s a way of tracking and given percentile over distribution which costs about the same as in a WMA they\u0027ll need to be a little bit of numerical fudging to deal with the fact that the percentile is not actually what you want to track you want to track an estimate of the maximum but you know fudged like that because that could be bbr a significantly softer response to events in the network and probably eliminate most of those spikes in the bottom graph in the slide you chhanchhan um yeah I agreed right now the VBR max filter is a "
  },
  {
    "startTime": "00:46:42",
    "text": "it\u0027s a stream or filter and I think the men sort of benefit of this ECM work is to actually reduce that effect of max filter because one problem is everybody\u0027s probing and once a while everybody get a really good penguin and they stay at that but who is this easier serve as a like a guidance to say no no the you you are way too extreme because you know I\u0027m try to start thinking you all right you have to reduce your way and that\u0027s why it would achieve great queuing delay I do have and I also agree with Andrew suggestion we are looking to use different type of filter for the Penguins um I have a one question here so I noticed l4s uses sojourn time as a marking special but in most in our data center we still use instant as cue lens so what\u0027s your saw that if instead of using sojourn time you use simply like over X kilobytes of key well market what would the result look like mmm i protonate to come back with the answer yeah okay yeah it would be going to see that yeah yeah Neil Cardwell I also wanted to thank you for this work I think it\u0027s really great to see people experimenting with simple ways to fold in ecn signals to the VBR algorithm or framework I had a question about or some comments about the the modifications to the algorithm if it\u0027s possible to go back to the slide that\u0027d be great if not that\u0027s fine in looking at the relationship between the the gain cycle that\u0027s talking about the bandwidth probing schedule and the bandwidth filter window I would encourage people who are experimenting with variations on a PBR to keep in mind some of the design relationships between the timescale in which you probe for bandwidth and the time scale of the bandwidth filter window in the you can take a look at the EPR internet draft it talks about some of those constraints where you want to basically remember a bandwidth over a time scale that includes some bandwidth probing in order to get robust behavior and not sort of fall over if you see delay variations due to cross traffic or or radio bandwidth fluctuations whatever and then the second comment would be I think one thing with the EC n base bbr one thing to keep in mind would be that over the lifetime of a bbr connection in the real world we might have the bottleneck shift from a bottleneck that supports l4s ECN to bottleneck that doesn\u0027t support it and it would be probably important to think about how these mechanisms could function if the bottleneck shifts in that way so that the behavior is still reasonable if "
  },
  {
    "startTime": "00:49:43",
    "text": "there\u0027s a non ecn bottleneck that becomes the bottleneck so for example the min RTT probing might need to be always kept there in some forum to make sure that the to a propagation delay estimate maintains anchored to reality but but overall thank you very much for this work I risk I really just wanted to comment on your point about by its versus time in in the Q marking and it\u0027s fine to use bites if it\u0027s a fixed-rate link but if it\u0027s got multiple queues and the right varies depending on the other queues then you really want to use time but that\u0027s this and there\u0027s those papers that give you performance results on that if you want to I can point you at them well add yuku or if you\u0027ve got priority queues you know in other words if they\u0027re if the output rate of your particular queue varies depending on the traffic in the other queues then you can\u0027t really set a byte limit because you don\u0027t know the rate of your particular queue and so it\u0027s better to do it in time they all have multiple yeah and and this is what these papers show but but you can get a lot better performance if you use time not it\u0027s the same as you know coddle and pie and everything they will use time yeah for that reason thanks for the interest on me I thank you so much income up that\u0027s good work and a good presentation next up Neil I would upload the slights that\u0027s what I\u0027m gonna do out here okay so I\u0027m Neal Cardwell and I\u0027m gonna talk a little bit about recent work that we\u0027ve been doing on bbr at Google this is joint work with my colleagues at Google working on the TCP and quick VBR implementations including the folks you see here van and you Chung Ian China and the rest of the gang next slide please so I\u0027d like to start with a super quick overview and status of bbr which most people here are familiar with probably from previous presentations and then I\u0027d "
  },
  {
    "startTime": "00:52:44",
    "text": "like to spend most of the time talking about work that our team has been doing on dealing with paths that have a lot large degrees of aggregation and then close with a few thoughts on packet loss and ongoing work next slide please so just a quick overview in status we\u0027ve talked about PBR at the last couple ITF so you\u0027re probably familiar with some of these the bbr is used for TCP and quick traffic on google.com and YouTube and internally at Google on the when backbone between data centers this source code is available for Linux TCP and for the chromium quick implementation there\u0027s also work going on implementing bbr for FreeBSD TCP at Netflix we\u0027ve got a couple internet drafts documenting the the algorithm and then there\u0027s you know some slides and and videos from previous talks next slide things so aggregation so what do I mean here basically there are a number of where you can get data and/or acknowledgments that are sent in birches bat bursts or batches and generally when this is intentional it\u0027s been designed for some kind of amortization of overheads to increase efficiency so this can happen both in shared media whether it\u0027s Wi-Fi cellular cable modems and it can also happen to reduce the per packet overheads using offload mechanisms in order to take a batch of packets on the wire and treat them as a single entity as you pass them down through the stack on transmit or back up through the stack on receive and so basically you see that on a lot of Ethernet links and so between all of these links and all the various techniques they\u0027re using this turns out to be a very widespread phenomenon so to handle aggregation bébé are sort of decomposes this problem into two separate but related problems the first is how do you bound your sending rate based on the estimated rate at which your packets are making it through the network basically your bandwidth estimate and one of the two drafts we have out talks about that approach that we have been using so far for delivery rate estimation and there are links there and there\u0027s ongoing work on improving that basically a couple different approaches we\u0027re looking at but both have the flavor of looking at bandwidth over longer timescales and then the second part of the problem is once you have a bandwidth estimate how do you use that to sensibly bound the amount of in-flight data in the network which is to say how do you set your Sealand and as a first approximation the "
  },
  {
    "startTime": "00:55:45",
    "text": "initial release the VBR basically has a sort of simple approach where it generally bounds the Seawind to twice the estimated bandwidth delay product so twice the bandwidth of times the min RTT of course this can run into challenges if RTT is noisy so here\u0027s an example next slide please so this is a trace from a case where there\u0027s a Wi-Fi bottleneck so it\u0027s just a sender one Ethernet hop Wi-Fi access point Wi-Fi network laptop receiver and here this is five seconds of traces showing every RTT sample that we get and it\u0027s a bit small I guess but you can see that the the RTT who sort of bounces all over the place from four milliseconds at the bottom all the way up to the roughly 80 milliseconds so though the RG T\u0027s can be kind of noisy here and so this is one of the challenges and we can sort of drill down and get a sense of what\u0027s going on if we take traces at at both ends awesome next slide please so I wanted to spend a little time doing a deep dive on one particular choice just to illustrate some of the issues that you run into and and how they impact the the approach we we took for dealing with this so here\u0027s a single trace we\u0027re going to be looking at and this is just a very simple TCP transfer of 20 megabytes this is a Linux TCP ebbr or sender going over the public Internet to a cable modem and what you know typical Wi-Fi home network and a laptop receiver and we took some traces on the sender and the receiver with TCP down and next slide please so if we look at things from the perspective of the receiver it looks kind of reasonable the data is mostly arriving kind of smoothly this is a time sequence plot x-axis is 2 seconds of worth of time y-axis is 20 megabytes worth of data you can see the data arrives fairly smoothly and then the TCP sender is in queueing acts smoothly there are a couple of flat spots that are kind of interesting then those turn out to be from when the sender is seewhen limited or a receiver when delimited which we can see on the next slide next slide please so this is the same trace but from the perspective of the receiver and that\u0027s this one\u0027s a little more interesting you can see that basically the sender is largely sending pretty smoothly but then there will be long periods where the act stream just sort of disappears for a while and the acts go silent and in those cases the sender sends for a little while at it\u0027s estimated bandwidth but then it runs into its Seawind bound of twice the bandwidth delay product as I was saying and and then eventually after a while there\u0027ll be a big burst of Acts that arrives the sort of vertical blue segments here a little while later and we can zoom into the the circle here "
  },
  {
    "startTime": "00:58:45",
    "text": "on the top right and the next slide next slide please and this is what it looks like from the sender\u0027s perspective zoomed in so we\u0027re sending along in this green transmitted line that our estimated bandwidth and then all of a sudden the acts disappear and we get a flat segment in the horizontal segment in the blue line and there\u0027s sort of silence in the extreme and eventually we get a big aggregated ACK the bunch of data acknowledged in a single burst which allows us to continue sending and it\u0027s this kind of the phenomenon that we see over and over when there\u0027s aggregation next slide please if we look at that same moment from the perspective of the receiver it\u0027s a little bit interesting the big aggregation effects that are visible to the sender in this case are not showing up at all on the receiver on the receivers perspective what happened was during that time here where the the lines are smooth and diagonal the the data arrival is smooth and at the receivers in queueing TCP acts on its own receiving host smoothly but basically the the it as we could see from the previous trace actually those acts apparently are not making it out on the network and so you know there a number of phenomena that could be happening here but it appears that basically the the Wi-Fi access points data transmissions are monopolizing the link for a while and the result of this is that since the acts are not making it out on the wire the sender as we saw on the previous slide runs out a Seawind goes silent and that gives us this flat horizontal section following there where the link is underutilized because the sender stop sending on next slide please so how can we how can we deal with this so at a high level bbr is sort of aiming to keep the minimum amount of data in flight that\u0027s required to achieve the available bandwidth that it sees and in the first arrive of bbr at a high level the the Seawind is generally bounded to twice the bdp and that\u0027s because basically there\u0027s one bdp that\u0027s budgeted for utilizing the pipe and then one bdp that\u0027s sort of budgeted for a delay variation that you can see in the path and that\u0027s sufficient for fully utilization if and only if the delay variation is less than or equal to the men are TT but as we were just seeing in previous slides obviously delay variation can be quite high so what happens is you if the delay variation is higher than and as we saw the sender exhausts it\u0027s see wind and under utilizes the link and the lower the min RTT is the more likely your you are to run into this situation let\u0027s see we lost okay I think you so and in fact this can happen even on the public internet because often content "
  },
  {
    "startTime": "01:01:46",
    "text": "distribution networks will try to place their they\u0027re sending hosts close to the user for a good user experience so the min RT t might be just one to twenty milliseconds whereas as we just saw that Wi-Fi delay variation can be quite a bit higher up to say 80 milliseconds next slide so to deal with this the in the past half year or so the bbrt MIT has implemented an an explicit aggregation estimator that is basically asking how much excess data can be acknowledged as a sort of aggregate over the time scale of a single flight of data or a single sealant and it does this by sort of estimating the windowed maximum degree of aggregation that it\u0027s seen recently and then uses that estimate to provision or allow itself if needed extra in-flight data to so they can keep sending during these long silences in the extreme and it does this by basically saying what\u0027s the extra amount of data that we acknowledge acknowledged over some time window beyond the expected amount so specifically we can calculate the extra amount of act data as the actual amount that was act - what we expected and what we expect to sort of the estimated bandwidth times that sampling interval and then we can take the maximum we\u0027ve seen over some reasonable recent time window and and add that into our seal and that we allow ourselves and there\u0027s a course a question of how we sent is recent and here we we basically tried to keep that window at roughly the same time scale as the the estimated bandwidth filter and then as a sort of safety mechanism sort of bounded that extra act amount by the the Seawind since of course you over the timescale of a single flight you shouldn\u0027t have more delivered than your Seawind obviously so next slide please so here\u0027s just a quick way to visualize the the mechanism here the idea basically is that on when we\u0027re sampling the extra mouth that\u0027s been acknowledged we take the actual amount that\u0027s been acknowledged just sort of the entire blue height here and then we subtract the amount that we expected to have acknowledged which is the the green vertical segment here and we computed that from the estimated bandwidth times that sampling interval on the bottom there and that gives us the if we subtract those do we get that extra amount that was act which is the red part here and that\u0027s the part that that we allow as extra Seawind essentially so next slide please so this um there\u0027s code for this that\u0027s already been deployed for the quick chromium implementation of bbr and you can click through the link there to see that and then there\u0027s also a tcp plantation that\u0027s being rolled out on "
  },
  {
    "startTime": "01:04:47",
    "text": "Google comm and YouTube for some global experiments and then here just some quick examples and it controlled the environment where you can see the kind of levels of improvements sort of this in this particular network the for example in the Wi-Fi network when using 2.4 gigahertz there\u0027s sort of a 4x increase in throughput and then in 5 gigahertz is sort of a 10x increase in throughput system significant improvements next slide please so in addition to the the work on explicitly estimating the degree of aggregation in the path a second mechanism that we\u0027ve been working on recently is a mechanism to adaptively drain data from the network to maintain shorter queues on a more regular and basis so in in bbr in the steady state there\u0027s this basic approach that we call gain cycling where you in general pace at some pacing gain times the current estimated bandwidth and you spend a little bit of time probing for bandwidth by sending with a pace and gain above one a little bit of time draining the extra data in flight from the network by pacing lower than the estimated bandwidth and then holding steady for a while sending out exactly the estimator bandwidth and in the initial release of VBR the the drain phase was held generally for about 1 times the the men RTT or the two a propagation delay estimate and that can run into issues where if the bandwidth if there are variations in the available bandwidth then that can end up pacing out more data until the we reached the Seawind and so there ends up being twice the estimated BGP and fly so the new mechanism that we\u0027ve been experimenting with we could call drain to Target which basically instead of holding the the drain phase for only one min RTT it sort of adaptively holds that state until the the in-flight drifts down to the estimated bandwidth delay product in order to drink try harder to adaptively drain more of the excess packets out of that so in various rounds of experiments we\u0027ve sort of found that you need to be careful how you do this so there seemed to be a couple ingredients to making this work well you sort of need to bound to the amount of time that you spend draining the data because you spent too long then you can run out of good bandwidth estimates in your filter so you need to make sure that you do probe for bandwidth within the time scale of your bandwidth estimating filter and then we also noticed that you seem to need to randomize the phases of the gain cycling and this kind of approach to avoid always having the elephants and mice probing and draining in sync and "
  },
  {
    "startTime": "01:07:49",
    "text": "then we found that it was practically speaking it was necessary to deploy this in tandem with the aforementioned aggregation estimator so that if you\u0027re trying to robustly drain data out of the network excess data out of the network you do need to sort of allow yourself to put more data in if that extreme dries up and that and the acts disappear for a while so anyway so we\u0027ve been experiencing small-scale experiments with this on YouTube and seeing nice reductions in RTT and packet loss and we\u0027re in the process of doing a global experiment next next slide please so there\u0027s also work on bbr going on outside the Google teams as I mentioned there\u0027s work going on Netflix there\u0027s also there was recently an implementation of bbr for ns3 that was released there with a little tech report accompany the links are here the VBR is also one of the many interesting congestion control algorithms that the Stanford team is testing in their really neat system called Pantheon I encourage you to check that out next slide please and then what to share a few quick musings on on packet loss actually okay sure about the aggregation thingy yeah you mentioned that it is bounded by Seawind is that including the receiver window in that state because it\u0027s not explicitly mentioned two slides two slides earlier belief or three shots yeah or the next one before that yeah so right now it\u0027s not explicitly bounding it\u0027s off by the receiver window when it\u0027s doing that filter we could certainly do that as well we could say okay this should not be bigger than the Seawind and it should not be bigger than the Arwen\u0027s is that what you\u0027re suggesting so if you if you\u0027re running if you have this aggregation aggregation phase obviously the X do not come back to the to the sender right so in here you could exceed the receive window that that the sender knows at that point in time right yeah well the so it the obeying the receive window is sort of I think it\u0027s sort of a separate or orthogonal issue obviously we we never ignore the see the receiver window so it seems so implicitly ha notice at all states yeah yeah so they\u0027re they\u0027re separate mechanisms in Linux or quick I assume that make sure they never disobey that received window yeah so from my perspective the with packet loss as a signal the question is not really whether to use packet loss as an input signal obviously we know and Kubik used packet loss as a signal and even all of the versions of bbr that we\u0027ve experimented with also use packet loss "
  },
  {
    "startTime": "01:10:51",
    "text": "as an explicit signal i think the key question is really how can we use packet loss effectively as a signal to adapt to the transport senders behavior and I think there are some really interesting fundamental issues here is where packet loss results from congestion there\u0027s still the question of time scale so to take two examples for exists if if the buffer is only one percent of the bandwidth delay product then you have situations where a link can be 99% idle even if you\u0027ve got around over the even over the time scale of a round trip with packet losses is you can take there can be a burst that can fill that tiny little queue take some losses and then people back off say and then leave the the the link idle for the rest of the round trip and then at the other end of the spectrum you could have a buffer that\u0027s nice and big say it\u0027s a hundred percent of the bdp and you can still have if you look over a slightly longer time scale of say ten round trips that link might be fully utilized for the first round trip but then completely idle for the next nine round trips and so I think given these kinds of issues I think part of the key question is on what time scale should a congestion control algorithm repro before bandwidth once it\u0027s seen a loss signal and there\u0027s sort of an interesting tension between minimizing packet loss on the one hand and maximising application performance on the other hand and now obviously most of the time reducing queuing and packet loss generally reduces the application latency but up to a point but if you if you try to absolutely minimize queuing and packet loss it doesn\u0027t always mean you\u0027ve maximized application performance so to minimize that absolutely minimize queuing and packet loss so you essentially need to send quite slowly and probe for bandwidth slowly and rarely and that can cut your loss but it can also mean that you\u0027ve you\u0027re not well utilizing the network and can thus increase your app latency and it\u0027s kind of an interesting trade-off and we\u0027ve actually seen this repeatedly as we look at tuning congestion control algorithms and in data center workloads we\u0027ve seen it with several different congestion control algorithms not just PBR anyway so those are just random musings next slide please so in conclusion you know we\u0027ve talked about the status of the verse first version of bbr and we\u0027re actively working on improving it we know we\u0027ve got some work left to do it\u0027s there\u0027s work on going on in several places and right now at least at Google "
  },
  {
    "startTime": "01:13:52",
    "text": "if focuses are sort of how to deal with aggregation how to deal with the packet loss signal I talked in November about one iteration on the VBR responds to packet loss and then we were also working on the dynamics of sharing with loss based congestion control and EQ MEMS and there\u0027s as I said work going on on VBR at netflix as well and of course we\u0027re always happy to hear ideas see patches or tests or experiment results look at packet traces all that sort of thing next slide please yeah so that\u0027s all I have and like to thank these folks and there\u0027s a link if you want more info and we\u0027re happy to take questions and comments yeah Toby Holland journalism and for the Wi-Fi tests all the examples you show have the TCP sender at least one Ethernet hub away from the Wi-Fi link have you run experiments where the BBS and there is the same machine that has the Wi-Fi device in it we we have run results we have run tests with that but I don\u0027t have any traces or numbers for you on that help for that as well your your order of magnitude improvement does that come in those cases as well some some like what I\u0027ve seen is this phenomenon where because you don\u0027t have enough queue in the Wi-Fi device you get less aggregation because there\u0027s not enough data to build the aggregates right and BB I was especially that was especially bad for bbr so I was wondering if you right now that\u0027s a great question we um I think yeah we\u0027ll have to go back to the off to run those tests as well and see see how much it helps for that case as well I think it will help to some extent I think the open question would be how much is left on the table after this patch but thank you that\u0027s great point they get girl Matt Mathis I wanted to add a couple of footnotes and about some other things the issue about aggregation is actually a really critical problem in this community and the issue is I believe it to be true that the majority of humans on this planet are behind links that exhibit this ken\u0027s it behaves and the constraint is that if you have a curation going on you don\u0027t have a way of precisely controlling delay you just don\u0027t have enough information and this is a fundamental problem here that in places where you have full duplex wire line connections it\u0027s easy to imagine precisely regulating queues and that is not the case in wireless and not the case in most of the southern hemisphere and not the case in lots of places and so we\u0027re looking at a divided problem space probably and the other thing is I\u0027ve become very concerned recently "
  },
  {
    "startTime": "01:16:53",
    "text": "there ruminations about loss all of the plots we\u0027ve been seeing here have been what I would call bulk flows not transactional workflows and I think there\u0027s a lot of stuff there that we haven\u0027t even begun to look at Thank You modules in more party I\u0027m the chair of the coding group research group and this with my chair hat and I\u0027m talking here actually in our group we started investigating how to deal with loss by using some form of loss coding especially inside quick and also how this interacts with congestion control in particular so I think we\u0027re interested but I think there could be also some collaboration that could be possible okay thank you very much if you could email us that would be great Alan thank you like Kevin Smith Vodafone I\u0027ve got two quick questions so first of all on for example the Vodafone UK 4G network today would Google services be using bbl yes they would thank you okay the second question is regarding loss and how you deal with loss this is maybe an open problem which is it D Evan are you attempting to use a way to distinguish between loss versus the fact that the radio layer may well be retransmitting at the at the physical layer one layer two layer there two and a half for example right no we\u0027re not attempting to distinguish those in in VBR but what I would say is that typically what we see in traces of cellular traffic and Wi-Fi traffic and our team everybody on our team has spent a lot of time looking at the traces typically what we see is that the the link layers in radio in cellular and Wi-Fi networks tend to do a very good job of link layer retransmissions that completely fill in most of the holes most of the time yeah so in our experience looking at the traces from TCPS perspective what we see is high delay variations like the traces that we saw here and we don\u0027t see those manifest as packet losses at the TCP layer so now if I Cal be interested in exploring that further ISA great thank you I\u0027m inserting myself from the QR Andrew Andrew McGregor I noticed there was periodicity in the flat spots in your Wi-Fi trace and the periodicity was a hundred and two point four milliseconds so what was happening was something and attached to that access point was in how safe mode and what you were seeing was the timer took the access point to drain his passive cues after a beacon what sort of device you think may be in power saving mode in this case I don\u0027t know it could be anything it could even be that it could even be your bbro endpoint having part of its traffic power saved which is not supposed to happen but does okay because in the receiver side traces it seems like it\u0027s pretty smooth yeah okay yeah "
  },
  {
    "startTime": "01:19:55",
    "text": "me I\u0027m gonna take off that can happen the other observation is maybe packet loss you need to think about the situation that shallow buffer packet loss is generally pretty much plus on distributed you know random drops other causes of packet loss will have strings of correlated losses and maybe we need to treat two and three consecutive drops is different from one as a heuristic toward looking at the whole distribution hold distribution and time of the packet losses which tells us about the the cause of it and if it\u0027s buffalo\u0027s the integral of the tail of the distribution of rates going above what the buffers can sustain yeah I agree that there\u0027s probably some signal in there that can be looked at it it might differentiate yeah it might be really hard and so I think that that\u0027s probably gonna be a long term ongoing research effort and the question would be what\u0027s the simplest mechanism that you can deploy that will behave reasonably even if you may not have a a complicated mechanism to distinguish different kinds of packet loss but yeah I agree that that\u0027s an interesting research area um Jenna I\u0027m got participating on the floor um I just want to make a general comment and this is entirely my anyways I want to make a general comment about loss rate I don\u0027t I don\u0027t like the word loss because it\u0027s really regions missions that we\u0027re counting here not losses but but beyond that I think there\u0027s something important I think there\u0027s something important in that neurons which is that sending additional retransmissions I think your slide on the meditations on packet loss is spot-on there\u0027s a very clear tension here between application performance and most pertinently right now for me and I think for most work that folks are doing in transport now it\u0027s latency and there\u0027s a tension between latency has observed by the application and retransmissions that a sender is willing and able to do more aggressively increasing retransmission retransmissions in my opinion should be completely reasonable and it should be reasonable thing that we are willing to give to gain some latency back or gay or to drop some very some latency as was and in that spirit I want to encourage folks to think about the mechanisms in the work that\u0027s happening here as not to try and reduce the retransmission rates all the way back down to what we know and are used to in Reno or cubic but to something that\u0027s reasonable it doesn\u0027t have to be the same as what cubic or Reno has and that should be perfectly fine as long as it\u0027s not 50% obviously at that point the "
  },
  {
    "startTime": "01:22:55",
    "text": "trade-off is it\u0027s not a trade off anymore at that point or you\u0027ve gone way past the line but I think it\u0027s completely reasonable to to increase the retransmit rates significantly from where where they are right now for for you know cubic in order to do to get back some of that latency thank you all right thank you very much Thank You Neil it\u0027s Michael next let\u0027s see you know [Music] maybe what go for it Michael great hi everyone thanks a lot for coming and thank you Jennifer the invitation so I want to talk about a little nervous begin to the money oh sorry can you hear me now okay I want to talk about PCC or performance-oriented congestion control next slide please and PCC is a collaboration between my research group at Hebrew University and Brighton godfrey\u0027s research group at urbana-champaign and everything I\u0027m about to tell you is based on two publications one from 2015 at used ANS di and another that\u0027s upcoming also a tennis I next slide please and PCC is currently being evaluated in various places and hopefully soon more as well more will follow next slide please so before diving into a PCC I want to say a few words about what TCP we think does wrong so think of a scenario where you have a flow you have a connection F sending at a specific rate R right and yeah and there\u0027s packet loss right there so there\u0027s many reasons why this might have happened and there\u0027s many things you can do in response and yeah so so you experience packet loss what\u0027s going on in the network so one option is that your the flow causing congestion that is you\u0027re a big flow you\u0027re bombarding a bottleneck link somewhere and for that reason there\u0027s congestion in which case you should decrease your rate by a lot even for selfish reasons right a different option is that you\u0027ve just exceeded some shallow buffer somewhere in which case you should decrease your rate by just a little bit now yet another option is that there\u0027s some you\u0027re an insignificant flow there\u0027s some other flow that\u0027s big and that\u0027s "
  },
  {
    "startTime": "01:25:55",
    "text": "causing congestion right in which case I would argue you need to maintain your weight right certainly you don\u0027t need to decrease it by a lot and the last option which this is not an exhaustive list but the last option here is think of non congestion laws right due to physical layer corruption due to the medium due to handover in mobile networks whatever right if if you get lost that\u0027s random then your fail you\u0027re alone on a link and one out of every 200 packets is dropped I would argue that you need to fully utilize that link right so in that case you actually need to increase your transmission rate until you actually do start the stuff for congestion losses so yeah next slide so the but the the the key point here is that TCP is design philosophy is such that you hardwire a mapping from packet level events Lizz a packet was dropped I received an AK the RTT of a packet was more than some bound to changes in the congestion window and no such hardwired mapping can be optimal across all of these scenarios right so stepping back from this the abstract question is what\u0027s the right rate to send that and the key observation is that it\u0027s really hard to figure out what\u0027s going on within the network but regardless of what\u0027s going on within the network there\u0027s something you can quantify that is if you send it a specific rate you can quantify the outcome in terms of the implications for performance and you can formulate that so yeah so specifically what PCC does is it tries out different rates so you send you pace yourself you send at a specific rate you wait long enough thing RTT are slightly over an RTT for acts to return for selective acts to return and you gather statistics meaningful statistics that pertain to performance like what\u0027s the last rate what\u0027s the throughput how fast is latency increasing during this monitor interval etc etc and then you derive from these statistics a utility value so you aggregate these statistics using some utility function into a numerical real value that intuitively represents the score in terms of performance that is how happy am i with what just happened right so for now think of a very simple utility function that\u0027s just the throughput times 1 minus the loss rate okay so that\u0027s maybe that most basic utility function one can employ in this case it ignores latency and it\u0027s pretty straight forward so what PCC basically does is it divides time in two consecutive intervals and each such monitor interval is devoted to trying out a specific rate so you can "
  },
  {
    "startTime": "01:28:56",
    "text": "think of this as a sequence of micro experiments we test different rates one after the other and so at first we said that a rate of r1 we we collect statistics we aggregate these statistics into a utility value you one then we sent in a different trade or two we\u0027ll do the same thing an aggregate now we have a different utility value you two and the crucial point is how do we determine what the next rate should be so there should be some online learning algorithm that determines based on our previous interactions with the network based on the different rates we tried based on the different statistics we collected based on the different utility values what should happen next so at a very high level this is the PCC architecture so you you learn real performance in the sense that you you choose a rate you gather statistics and you provide a score that indicates the performance level you control based on this empirical evidence and this we are going to show reals consistently high performance and going back to TCP there\u0027s two crucial aspects here so one is that thank you that we gather meaningful statistics right so the fact that a packet was lost doesn\u0027t tell us much the fact that the loss rate is half a percent does actually mean something the second thing is that we apply an online learning algorithm to adapt our rate so I\u0027ll talk about what that means in a few slides yeah can you still hear me cool so here\u0027s so the first version of PCC presented in 2015 called PCC Allegro employed a fairly simple rate adjustment algorithm so there\u0027s a lot of details but I want to give the high level picture so think of sending at a specific rate R and deriving a utility value of U right so now you need to decide whether to go up or down right so you\u0027re in this decision-making mode well-liked and the question for pcc\u0027s perspective is will I gain higher utility if I go up will I get better performance or will I get better performance if I go down right so the way PCC v1 handles this is it does random control trials but it probes high it probes a lower rate and based on empirical evidence it decides in which direction to move so in this case for example say that you were sending at a rate of are you try a higher rate of R times 1 plus Epsilon the lower rate of R times 1 minus Epsilon let\u0027s say you do to experiment right you so you send twice at the higher rate and send twice at the lower rate and in this case as you can tell in the figure the results are pretty conclusive it didn\u0027t have to be this way but in this case it\u0027s pretty obvious that\u0027s I gain higher utility when they sent at a higher rate and so I will move up now obviously there\u0027s a lot of details I\u0027m omitting because firstly I wouldn\u0027t want to go into this decision-making mode each and every time "
  },
  {
    "startTime": "01:31:57",
    "text": "right so PCC doesn\u0027t do that PCC basically enters the decision-making mode where there\u0027s evidence that the utility that the direction is no longer the right one and I\u0027m not going into the details about how we choose this exact step size which is also adaptive right but this is the high level picture the high level picture is gained empirical evidence right and move in the empirically maximizing direction which translates in PCC to the utility maximizing direction so one thing I want to point out is that even this fairly straightforward scheme is enough to distinguish between some of the cases and actually all of the cases we\u0027ve seen before so think of the scenario where I am a large flow causing congestion and the opposite scenario where where there\u0027s random loss right so one out of every 200 packets is dropped an expectation regardless of what I do right so in this case utility behaves very differently right if I\u0027m sending at a certain rate in the first scenario and I increase my rate then because I\u0027m the flow causing congestion loss rate is going to go up right I\u0027m the problem in the second scenario if I increase my rate the loss rate is the same right because it has nothing to do with me and this is translated two different choices of rates because the utility maximizing direction in the first scenario is going to be decreasing your rate whereas in the second scenario it\u0027s going to be to increase my rate because I\u0027m getting more good foot and my law rate doesn\u0027t change so one important thing to note is that congestion control here is handled differently than in TCP so the question is where\u0027s the congestion control so in TCP congestion control is exogenous to the protocol I mean there is we hardwired into TCP behavior that says if you experience packet loss interpret that as congestion and once you interpret that as congestion react in a certain way so PCC doesn\u0027t do that and thank you so in PCC again please yes so in PCC we have different different centers each employing an algorithm that maximizes its own utility and the crucial point I think is that selfish doesn\u0027t mean aggressive if my utility function is such that I don\u0027t want right that I suffer from excessive loss right then so will my behavior so my behavior will be consistent with that so we think of this interaction between flows as inducing the non-cooperative game we have different senders they optimize their own utility functions but if the utility functions are such that the result is resulting equilibria are good for "
  },
  {
    "startTime": "01:34:58",
    "text": "example where the loss rate doesn\u0027t exceed a certain bound then that\u0027s what we were going to converge to so a lot of the engineering is devoted to figuring out what the utility what the right utility functions are right so with utility functions induce good equilibria so in PCC v1 this was handled in a certain way so going back to the utility function the simple utility function I mentioned before where it\u0027s basically just the throughput my minus some penalty for loss right these utility functions aren\u0027t actually going to behave that well but when you plug in another factor and and you can look at the paper the formal analysis right another factor that penalized is loss further that what you actually get is a unique equilibrium that you\u0027re bound to convert through that\u0027s fair and where the loss rate is bound is bounded sorry yeah so here are some experimental results so what you see here is a side by side comparison of PCC and TCP cubic on sending on a so we\u0027re standing on a single link and Flo\u0027s come and go so first we have so the x-axis is time the y-axis is true put the red flow enters and it\u0027s alone in the beginning then the green flow enters and so on and so forth and then they start leaving all right so what you see in the bottom figure is that PCC basically converges to a stable equilibrium where each of the flows gets its fair share and stays there until the situation changes there\u0027s until another flow is added or one of the flows is removed so here\u0027s another experimental result another depiction of experimental results from the paper there\u0027s often a trade-off between convergence rate and how variable your rate is upon convergence it\u0027s so often you\u0027d have things that converge fast for where you see high standard deviation once you converge or they are the opposite I we\u0027re probing is slow but then you might stay at a fixed point so what you see here are different protocols and where they\u0027re located in terms of convergence time and the standard deviation of the throughput upon convergence the y-axis and below at the bottom you see different points corresponding to different choices of parameters for PCC so what you can see here is that for quite a few choices of parameters you actually get better trade-offs than most existing algorithms out there so here\u0027s one more result I wanted to "
  },
  {
    "startTime": "01:38:01",
    "text": "show so this is this is an attempt at quantifying reactivity to rapidly changing Network conditions so you have an emulated environment where every five seconds the network parameters change drastically so the bandwidth changes the RTT changes and the loss rate changes and what you see the blue line is the optimum sending rate in hindsight this is what you should have done had you known right that this this these would be the exact network parameters so what you see is that Kubik performs really badly on this example it\u0027s because it doesn\u0027t contend with loss well TCP Illinois performs somewhat better and PCC Allegro is actually very close to the blue line right so it pretty much is able to track the optimum in this example and I\u0027ll actually revisit this example later when we talk about PCC v2 because there are some subtleties involved here okay so we\u0027ve run a lot of experiments in the cost in the in the course of the last three four years and all of these are documented in the paper so please take a look and I\u0027d be happy to could you go back once I think\u0027s are are documented in the paper we\u0027ve we\u0027ve run experiments in many different emulated scenarios and also in the wild and one thing I want to note is we\u0027ve run experiments from in in in in the context of you know inter data center intra data center satellite and force etc and what we see is that we get significant performance benefits in each of these scenarios but one important thing to note is that this is the same PCC implementation but each and every time we\u0027re comparing against the TCP variant designed specifically for that environment so so this is the same black box PCC implementation so here\u0027s a here\u0027s a graphic portrayal of some of the experiments we\u0027ve conducted these are 510 source target pairs so each and every line represents comparison of PCC and TCP Kubik between the Associated points and the color index is how much higher is the throughput right so in the median it\u0027s it\u0027s 5x yeah okay and this is a my favorite figure from the DNS di 15 publication so this is sending a lot of data over very long paths so this is entering 100 gigabyte of data over very long path you see from you de to Berlin from you de to China from Illinois to Reseda so the green represents TCP cubic "
  },
  {
    "startTime": "01:41:02",
    "text": "and yellow is taking a flight right basically and and PCC is the blue yeah this is the Internet in the 21st century so there\u0027s uh yeah there\u0027s a lot of work for that firfer this workgroup so a few words about deployment and deployability so there\u0027s one one interesting observation which is that TCP is a really bad learner from an online learning perspective because of this hardwired mapping so for example it doesn\u0027t do one of the most basic things that you\u0027ll find in any online learning algorithm which is trying to close the feedback loop with the environment so this is what I mean by saying by saying that when I go up that\u0027s the loss rate go up that\u0027s the latency go up or do they stay the same this seems to be crucial for dictating what I do next right but in TCP because you\u0027re reacting to a packet level event right that information is not available to you so but the the more important observation from a deployability perspective is that the TCP sender it might be the bad learner but the TCP receiver is really trying to help it right so when you think of the TCP receiver it\u0027s actually throwing a lot of status ahh-choo right which you could theoretically utilize and from a deployment perspective what that means is that PCC doesn\u0027t actually need to change that\u0027s right PCC doesn\u0027t actually need to change the TCP receiver so the receiver can still run legacy TCP you don\u0027t need to change the application all you need to do is change the part of TCP in the kernel that adapts transmission rates so that that\u0027s the part you need to change this can be implemented as a kernel module and and and this is basically what we did so this is the high level software architecture but but I think that\u0027s the that\u0027s the main point the main point is it that all you need to do is generate the kernel module that just changes the algorithm TCP employs to adapt rates so so this was in 2015 and since then we\u0027ve continued working on on PCC quite a bit and the reason is that PCC Allegro is still far from optimal and the more challenging the environment is or the more challenging the applications are the worse it performs so in particular it suffers from some a suboptimal convergence rate we in we did incorporate the some extent latency into PC cv ones utility function but but it wasn\u0027t done in a very principled way and we actually did very little experimentation there was no formal analysis of this it suffers from back performance in mobile networks and for LT a for example I\u0027ll revisit that and also from some optimal quality of experience in the context of video streaming I\u0027ll revisit this point as well so in so in an upcoming publication "
  },
  {
    "startTime": "01:44:05",
    "text": "we\u0027re presenting a pcc vivace which we\u0027ve already experimented with and is being evaluated in several places and thanks yeah and pcc vivace is another embodiment of the same high level architecture but there\u0027s two fundamental changes that pertain specifically to the utility function framework and to the algorithm the online learning algorithm that we employ to adjust transmission rates right so uh yeah Thanks so we\u0027re changing the utility framework and we\u0027re changing the online learning algorithm next so without diving into the details and soon soon the paper is going to be available online and the code also a few words about what we did so so the new utility function framework incorporates latency explicitly and that\u0027s required required changing the original utility function of PC cv1 quite a bit and is intended to provably provide better convergence that is both faster and more stable another thing which is a pretty crucial feature of PC cv2 is that different senders can employ different utility functions within the same broad class of utility functions without compromising convergence right so we can tweak the utility functions of different senders to accommodate their different performance needs right some I care more about latency some might care more about bandwidth right while still guaranteeing that they converge to a stable state and while also enabling us to reason about what this stable state might look like another thing that changed is the online the simple online learning algorithm that I described before where basically you had random control trials and you were probing in both directions trying to figure out what what the right direction is so again there\u0027s a lot of details some pertain to how do we handle noise right that is how we are gathering statistics but these statistics might not be accurate think of a scenario where I\u0027m sending at a low rate if I send just a few packets it\u0027s really hard to get to get accurate statistics right and there\u0027s other scenarios where this might happen so this is one thing we needed to address there\u0027s also various issues related to the step size but one thing I do want to to mention is in PCC v2 called PCC vivace we don\u0027t just use the utility function to determine what the right direction is we also use it to determine by how much we should change "
  },
  {
    "startTime": "01:47:07",
    "text": "the rate right and this is done by looking at the gradient of the utility function right that is if I increase my rate if at this point in time the gradient looks like this right I should increase my rate by quite a lot right as opposed to maybe I\u0027m very close to a stable State and I should just probe a little right and go up by a little bit right so this is this is so this you this online learning algorithm is intuitively based on doing gradient ascent on the utility function and there\u0027s a lot of results in online learning theory that show that not only does this result in performance gains for the individual sender but also if different senders interact and they play well with one another so it\u0027s interesting to contrast PCC with bbr because these two reflect very different design philosophies right bbr tries to track the bottleneck bandwidth and TCP and PCC employees online learning and try to learn from empirical evidence but they\u0027re also somewhat similar because both rely on pacing and both rely on going beyond packet level statistics and trying to gather statistics at longer timescales so one thing I want to mention before we go onwards is we have compared against BB rv1 and I know that the BB our team has been working on various improvements and once VBR v2 is available we\u0027d be thrilled to play with it yeah so going back to the figure we saw before where we Twite try to quantify how we active different protocols are to changes in network conditions so again we have a network we have a single link right and the network parameters change every five seconds and what you see here as the dashed black line is the optimum sending rate right this is what you should have done in hindsight so notice that both PC sees that is both PCC Allegro the old version and PCC vive the new version track that line pretty well but also notice that PCC Allegro the old version often overshoots right and when it overshoots that results in excess latency because it\u0027s it\u0027s sending above the raid and packets get queued etc etc PCC vivace does much better because most of the time the vast majority of time it\u0027s close to the optimum but it\u0027s beneath the dashed black line bbr has somewhat bizarre behavior here as you can see it it sometimes dips and take a walk takes a while to recover I conjectured that it is associated with scenarios where the behavior of this network deviates from the network model that underlies PBR bid be variant it\u0027d "
  },
  {
    "startTime": "01:50:08",
    "text": "be very interesting to to see how BB rv2 does in this context yeah so one important thing to know they said is it possible just to ask on the previous slide what you were showing gory first and are these each individual runs against exactly the same channel model yes change and they change by choosing a uniform uniform e ik lambda ma parameters within within the specified ranges right and then you basically just run these protocols against exactly this oh sorry about this okay but yeah but but just to repeat my answer so so this is an emulated environment every 5 seconds we choose parameters uniformly at random from these ranges and we run all of these protocols against the exact same choices of parameters sorry one at a time right so one one thing I want to mention is that this this the way you react to changing network conditions has implications for quality of experience so here\u0027s an experiment that\u0027s basically doing the same thing only now we\u0027re measuring quality of experience for streaming video and we\u0027re measuring that in terms of the buffering ratio which is the time you spend buffering at the client out of the total time right so ideally the buffering ratio will be zero right lower is better so what you see on the x-axis are different bit bit rates the for standard bit rate and the y axis is the buffering ratio so the upper line is TCP cubic as you can see bbr the significant significantly better and pcc vivace is actually quite close to the bottom in this example and and once again these are emulated the network conditions but we actually also ran run this experiment you can see that in the paper on streaming video from AWS to residential Wi-Fi right so there there are such experiments as well but if you next slide please another interesting phenomenon is that as the number of complete so the previous slide was for one flow but as the number of competing flows of video streams increases the the gaps actually become bigger and the reason is that stability and convergence play a role right you want the flows to play well with each other so so this is what you see here so the bbr line goes somewhat higher and the pcc line is still pretty much close to the bottom yeah so here here\u0027s another set of experiment this time we used Mahamaya the network simulator to replay Verizon LTE traces "
  },
  {
    "startTime": "01:53:08",
    "text": "and and what you see here is different points on the trade-off between self-inflicted latency and throughput so it\u0027s very easy to do well in one of these individually right if I just blast away I\u0027ll get perfect throughput but I\u0027ll I\u0027ll generate a lot of latency for myself right because packet we\u0027ll be buffered on the other hand if I send one packet each RTT latency will be perfect but true put will be horrible right so different protocols exhibit different trade-offs so what you see here is that cubic gets really high throughput but it\u0027s lost based right so that the latency is horrible PCC v1 which didn\u0027t incorporate latency and actually even once you did incorporate latency it didn\u0027t actually do that well is not much better in that respect bbr is better but still suffers a 1 second latency vegas i would say is a better trade-off and sprout is a protocol from MIT designed specifically for this context which i think is it\u0027s probably the optimum trade-off in this in this figure at least I don\u0027t know how close it is to be the optimum in general a but it\u0027s important to know that it\u0027s specifically for this context and also requires changes in both sides it\u0027s like explicit receiver feedback so you can see that PCC vivace it Eanes more or less the same the same level of cells and so the latency and the summer flow is stupid yeah ok so there\u0027s there\u0027s more things you can look at there\u0027s various demos there\u0027s there\u0027s a additional leading material Thanks so we\u0027re still very actively working on this both on the algorithmic side and on the implementation side so we\u0027re still seeking better online learning and utility frameworks I think that PCC v2 vivace is is a step in the right direction but I think that there\u0027s much more to explore there\u0027s the question of how this should be adapted to mobile networks there\u0027s the question of what the interface with the application should be what would video where PCC looked like and one thing where we\u0027re working on now is building an open-source consortium that will center around an implementation in quick and a PCC kernel module there\u0027s a lot of more details in the paper including all the different parameters of the of the different simulation emulation and empirical results are presented and that\u0027s basically it thank you very much and that this um I was trying to figure about your question about why bbr did so poorly in that one graph and I realized that VBR has in it an explicit assumption that the men are T T is stationary and just just varying "
  },
  {
    "startTime": "01:56:09",
    "text": "them in our titi out of five seconds steps time intervals is guaranteed to screw up VBR and one of the things that we haven\u0027t done yet which is going to come up at some point eventually is is how badly do we screw things like Leo but in terrestrial routes this is sort of an unrealistic scenario thank you all right I\u0027m gonna thank you very much for well I\u0027ve got a question about the switch well I think it was a switch part way through from selfish utility maximizing - everyone uses the same utility function is that true or do from selfish utility maximizing - everyone uses the same uses before you started talking about what utility function would be for everyone right alright so so in in PCC v1 we\u0027ve hardwired a specific utility function into the protocol and and we chose that specific utility function because we could show both formally and empirically that you get good equilibria but in this non-cooperative game that\u0027s induced by it and in PCC V - one of the things we\u0027ve managed to do was to allow you to tweak your utility function without compromising convergence so you no longer have to you can there\u0027s there\u0027s different knobs in the utility function that reflect how much you care about latency how much you care about good put you can play with these knobs and not compromise stability okay so my specific question was that if you if one cares about throughput and latency hmm and another cares about throughput and not latency surely that that one that doesn\u0027t care about latency is gonna cause the other one to keep starving itself and and then you don\u0027t have to compromise the latency eventually if it wants some throughput which is sort of the whole problem that Vegas had and everyone else has right yeah so yeah so I think I think that that goes beyond PCC as you know that\u0027s inherent right that is you can play with the utility values but but this this is a but what you\u0027re basically doing is you\u0027re you\u0027re choosing a different balance of how much you care about loss and how much you care about latency as a signal yeah right and in that I agree if you ignore latency completely you will end up taking over all the buffers and this is sort of like why I\u0027m it given given that latency if you don\u0027t care about it it also doesn\u0027t matter if you don\u0027t have it you know in other words having low latency right is even if you don\u0027t care about it it\u0027s good for everyone else wait so so there you do need to somehow design the things so that you you keep the latency low as long as you don\u0027t compromise your only utility function just for everyone else that does one low latency yeah yeah okay "
  },
  {
    "startTime": "01:59:11",
    "text": "the same thing applies to right right pravin from Microsoft one of one question I had was you showed the optimal rate in retrospect how did you come up with that so if you know exactly the network parameters right then then you can figure out you know what the bandwidth is you know what the RTP is and you also know what the loss rate is all right so that means that you can try to operate it the exact operating point where you\u0027re fully utilizing the bandwidth and you\u0027re not going into the buffer okay but if you had multiple flows with different utility functions would you still be able to compute this instantaneously like later later after the road oh if you had multiple flows and you were playing with you emulated yeah links that\u0027s a good question what you could compute is their fair share at each and every given point in time so you could argue that ideally right there would be let\u0027s say that it\u0027s five-second interval so they actually do have enough time to converge so most of that time you would want them to spend at exactly their fair share of the optimum okay one thing that is interesting here is that because you have added now latency as a as a part of the utility function is that you know less than best-effort congestion control likelike that it would be a good test of whether the utility function can mimic something like less than best effort so that would be a very good test to do for this so this that\u0027s a very interesting point so we haven\u0027t actually it\u0027s an interesting research for us so we haven\u0027t actually tried seeing whether we can emulate other protocol as well by choosing utility functions that match them but I think for LED backed right where you you want to do best effort until someone enters right I think that is something that you could actually probably capture when other thing I want to note about the utility function which I think relates to one of the previous questions is if you\u0027re lost based purely lost based then you\u0027re aggressive towards legacy TCP if you\u0027re purely latency based right then Kubik might kill you right because you have this this utility function where you can play with the different parameters you can you can strike the right balance between the two because I didn\u0027t talk about TCP friendliness right which relates to your question but but what we actually show in the paper and we discuss TCP friendliness is that even the purely latency sensitive debauchee yeah that performs better than cubic in the wild even though cubic is supposed to kill it right but it\u0027s I guess there\u0027s enough spare capacity for it to you the game sure I think this kind of ties in very well with you know applications that want to pick their utility function like there are scenarios where you want to make that trade-off but like this goes back to like Bob\u0027s question on what should be the default because most applications today are not "
  },
  {
    "startTime": "02:02:11",
    "text": "providing that information so what would you pick as the default to do on the best on the network so in the in the paper we we had two default choices one that\u0027s lost based and the other that\u0027s latency based in an ideal world everything would be the PCC vivace latency I think in terms of you know the function to choose to build into PCC vivace and you actually get good results for for both so I would argue the one that cares about latency is one that won\u0027t end up filling up buffers so that\u0027s the one you would probably want globally thank you what I saw from UCL I have two quick question first all of the experience that you are showing us is 5-second info right in these specifics what if one second or less than that so because when the network becomes highly dynamic right all of those learning things that you talking about doesn\u0027t work so I think that there\u0027s there\u0027s two issues here one thing that we\u0027re currently exploring is the behavior of PCC so this is joint work with Huawei that implemented this as a kernel module and and the behavior of PCC in mobile networks right which i think is exactly what you\u0027re talking not a lot more wonderful so but there might have some dinah where the RTT might be way too slow to react and and you can actually get get significantly better results than the existing protocols because unless you have in network cooperation but this is not a question of online learning right unless you have in network feedback you\u0027re going to react at the timescale of an RTT and of the question is what do you do and I would argue that online learning is what you should do in that context you shouldn\u0027t hide your responses regardless so what is my concern is so you ended up with the Wraith right which you thinks that is a right rate and the network have a transient congestion right so when the transit congestion happens you\u0027re gonna your sending rate is quite high comparison with other flows which react quickly to the transient congestion so you\u0027re gonna be taking all of the bandwidth so you\u0027re sort of penalizing when the network becomes highly dynamic you are penalizing other flows competing for us which they are non non PCC flows by and so this is my concert rather than the online learning so I\u0027m worried about the race that you thinks that these are maximizing the utility functions and at that time net will become you are in the highly dynamic network yeah then the sending race that you thinks is right maybe is too aggressive to the computing for us so but I think that isn\u0027t that is dependent on your choice of utility function right so take for example a choice where you shy away from confirm latency the latency increased no matter what right in that scenario you\u0027d be you wouldn\u0027t be sufficiently aggressive right so so this is a matter of figuring out what the right utility function is okay my second question is have you considered any processing costs for this "
  },
  {
    "startTime": "02:05:15",
    "text": "algorithm to run because my in my past experience you required a lot of CPU - yes the world so we\u0027ve so we\u0027ve actually there\u0027s there\u0027s a few variants of kernel modules of this at this time and one thing that we we\u0027ve done and and we owe Google a big thank-you for that is is we\u0027ve leveraged the changes that bbr write have inserted into the module I think in that respect the costs are approximately the same thank you Yoochun Chen from Google I love the PCC work in fact internally we often compare PCC with PBR and yes there are a lot of similarities and there are differences I want to ask for the utility function have you consider beyond the RTT ray and laws like application level metric for example videos probably care about the quality of experience the most so so we\u0027ve we started considering this I think it\u0027s a very important research direction right now we focused on transport layer metrics but I think and there\u0027s some and there\u0027s some deployability issues here as well because you may need the clients cooperation right to reveal some statistics but maybe you can infer them from the sender as well so I think it\u0027s a it\u0027s a very interesting direction we have not yet plugged in a utility function that reflects the quality of experience at the client I think I think that would be great to do yeah I think a lot of video clients actually can sort of feed a quality of qoe metrics easily so they\u0027ll be very interesting the other is also a similar question that have you considered beyond the transport layer but also beyond the flow level control for example if you ask they are sent her application what they care about the most they would take tell you it\u0027s the tail latency of my our pcs across 10,000 connections I don\u0027t really care about your flow level seeing I only care about the last one that straggler issues so so so can we build a congestion control using your framework but it\u0027s not just managing per flow by looking at a little bit like the co flow kind of concept from from Berkeley to say can we build a congestion control the optimized Taylor agency for a particular job now you have not done that but it seems like you could apply the same the same ideas but but I I I think it\u0027s it\u0027s worth it\u0027s worth exploring we haven\u0027t done that we have looked at what PCC might look like in the data center environment as if you want to to incorporate DCN and if you wanted to employ incorporate receiver feedback but you\u0027re you\u0027re talking about a different angle yeah because if we head home usually they use flows are just a channels and they will spread their requests among the pool of connections so to them optimizing per is really the "
  },
  {
    "startTime": "02:08:17",
    "text": "lease of their their interest yeah but I don\u0027t see a reason why we shouldn\u0027t be able to incorporate Co flow information but that is that is something we should definitely explore thank you Spencer Dawkins is wearing a few hats and it probably doesn\u0027t matter which one this one is right now I have a question probably in the form of suggestion which you know take it for what it\u0027s worth and please don\u0027t try to answer it right now because I hate people who like blow up your presentation with a question that you know it\u0027s like no one would ever have thought of the suggestion was I wonder when you all will be ready to look at things like PCC with the kind of rich understanding of what you are running into for a modder protocol like quick which that my first boss in 2002 got shut down we cut down because we couldn\u0027t tell the difference between loss and congestion and you know some things remain constant over you know decades and that\u0027s one of them so up let me so I ask you to consider that but maybe more broadly for the group here not the next ITF but in the next year or two is to be thinking you know is to be thinking you know people have been working on TCP for a long time because TCP is TCP and everything yeah I mean basically everything else is either UDP or blocked and firewalls but we\u0027re trying in the transfer area in the IETF to get to the point where you can you can roll new protocols without having upgrade operating system kernels and without having to upgrade middle boxes and stuff like that and you can roll new congestion control mechanisms and I would you know I would offer that as a possibility I would also offer as a possibility thinking if you you know the Internet is a big place right so it\u0027s actually hard to work it\u0027s not hard to make big improvements on something that smart people have been trying to fix for 40 years but are there are there specific corners of the internet where what you\u0027re thinking about would be more valuable than other corners it is there a way for you to isolate what you\u0027re trying to do to those kind of corners I have some theories about that doesn\u0027t matter you know doesn\u0027t matter what they are and smarter people in the room we could probably give you twice as good a theory over lunch but I transport okay I can I can take what I "
  },
  {
    "startTime": "02:11:17",
    "text": "could put that eye on for saying there was transport area director I would like for people who are thinking about research to be thinking about stuff that\u0027s more than the next the next operating system update you know which a couple of year of timeframe but you know I mean we\u0027re gonna have it we\u0027re gonna have an internet for a long time you know and I know that I see crg as schizophrenic and it was before Jonah was here because beacon because we rely on we rely on ICC RG so much for guidance and in the IETF but you know and please don\u0027t stop doing that because the Internet is a big place and there is you know you will probably be running TCP next week too but to to you know to be maybe a little bit more schizophrenia can say you know what are we looking at this relatively short term it has to work everywhere because something\u0027s gonna be blood something\u0027s gonna go in the Linux kernel and it\u0027s not gonna know you know if it or it\u0027s not going to get deploy at all they\u0027re like like like cubic I mean that was the deal right you know you put you put a cubic in the Loess kernel and then it has no idea who\u0027s who\u0027s trying to talk who\u0027s trying to talk to it or from where so that you know that\u0027s a use case and just thinking especially if you guys are talking about like really different kinds of congestion control mechanisms like PBR and PCC and the next five that somebody will come in and want to talk to a Java about you know maybe maybe maybe that maybe that\u0027s something that\u0027s worthy of really smart people time in the research community as well and thank you for letting me decide which add hours worth thank you just for the record Alison\u0027s not in here and my co ad just left before I was in the mic line so completely unsupervised at this point so my name is Nicholas Quinn I\u0027m working for nests which is a French National Space Agency and when I look to all curzon the comparison with your proposal and all the optimized DCP versions have to say that when we have 10 megabytes links we can actually get that capacity so just that\u0027s the other side where you say that we have two 17s more capacity with your proposal which one inside so if you go up in your presentation have again again again again up and down the other way round sorry that\u0027s it 17 I "
  },
  {
    "startTime": "02:14:18",
    "text": "look at your paper and basically you compare that with Ebola which is not deployed in peps and we have specific things so basically you may want to revise these message because I don\u0027t like think that this curve is 10 megabytes and satellite only one and we can achieve that which was Omega bustling so I\u0027d be happy to take this offline because it\u0027d be interesting disappearing why would be at the benchmark you haven\u0027t and another that was a comment and I have a small question on how you actually evaluate the loss rate the loss rate so we\u0027re using selective axe okay so you assume to drift exactly so we\u0027re using selective axe and we use that because cumulative is just not enough to make it and also a fail and also the poem that we selectively knacks you may not be sure that you have actually losses good pay P make buckets maybe we use a software to win such missions such as in cellular networks and those are pockets that are being being submitted they are not just due to congestion so that is yeah I mean so right you may want to consider that and I don\u0027t know how you can basically in your models consider that your measures you are may be incorrect and fee if that is resilient or not so this this is a very good point so right now one thing we\u0027re looking at very closely is is really mobile networks and satellite networks and the specific characteristics oh yeah so if you were me I\u0027d be very happy have we have platforms if you want to try your sing on our real satellite things cool we can take that offline if you want I\u0027ll ping you after okay go thank you well thank you so much this was the session goes on really oh good lord apologies my sincere apologies this is me basically without adult supervision this is why we needed it it isn\u0027t well well then that\u0027s it folks and I\u0027m gonna bow just so the last presenter that we don\u0027t have the ten minutes to to do the presentation I\u0027m sorry you had a chance to do doesn t CPM so I\u0027m not gonna feel that bad but well we\u0027ll see you all next time thank you oh my you "
  }
]