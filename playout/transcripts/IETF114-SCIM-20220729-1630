[
  {
    "startTime": "00:00:06",
    "text": "yes what can you see me typing oh then you're the one okay wow"
  },
  {
    "startTime": "00:02:06",
    "text": "oh okay we're at time uh those who are remote can you hear us okay yes all good all right can you see the uh the slides that are being shared yes yes yes excellent okay let's go ahead and get started welcome to the last session and the last day of the ietf 114. yay um so you are in this really simple cloud identity management so i'm not going to read the official name uh because that's what we do so uh this is what we are covering in the session if you're not expecting to talk about skin you're probably in the wrong session next slide um by now you should all be very well familiar with how the ietf works and the participation which we note in the note well so i will just skip and let you read it through the meeting materials next slide um i do want to provide some meeting tips and i know it's the last day but we've been asked as chairs to remind everybody of the meeting tips so for those who are here physically in the venue make sure to sign into the session via either the little uh video icon or the on-site tool icon on the webpage under skim"
  },
  {
    "startTime": "00:04:00",
    "text": "that way there's some binding that acknowledges that you were present in the session um please use the meat echo to join the mcq especially those that are here present that way we can respect the order um for those that are participating remotely as well um for those here who are present turn off your audio and video that way we don't get feedback and the other important one is please make sure you keep your masks on the only ones that can remove their masks are those who come up here in present um where pink x marks the spot uh that is the only instance in which you can take your mask off for those who are participating uh remotely make sure you have your audio and video um off unless we the chairs recognize you to speak or present during the session um and we still continue to recommend that you use your your headset next okay um i think we're all friendly in this room so uh there's this is just a reminder for the code of conduct treat each other with respect keep it professional um keep the discussion on point next slide okay so do the orders at hand uh thank you judas and pam for being the jabber scribes really the note tapers i guess i can update it now since we're no longer using jabber um the minutes uh the link is there for those who want to track the note takers and feel free to augment um if you see things that are missing so anybody can go in and edit the minutes as well"
  },
  {
    "startTime": "00:06:00",
    "text": "uh the meeting material there's the link there and hopefully if you're already here you know the meet echolink next slide for the agenda today we do have a pack of set of items that we want to cover rather than going through the actual drafts i've asked pam and phil who has the first adopted draft that being skim events to just give a brief update of where we are with those particular documents for the protocol and schema we've had a lot of discussion on the particular items that we want to focus on so rather than just saying we don't quite have a draft yet um danny and janelle if she's on will talk about the different particular features capabilities use cases that need to be addressed given that there was sufficient discussion on the email thread and how we do um beyond the actual bootstrapping or provisioning how we do the updates the scalability and performance for those updates uh that's my terminology in the thread there's been discussions about filtering how we do coordination um we've allotted more time to talk about that and the other term that would use pet was pagination so we'll spend a significant amount of time doing that um phil i forgot to update the agenda um that was my day i was going to update the slides so what is in the actual agenda under the the mean echo is correct um so phil we're giving you five minutes to just give an update on the draft itself but we're giving you 15 minutes so phil has uploaded it in the meeting materials there is a"
  },
  {
    "startTime": "00:08:02",
    "text": "presentation that phil will walk us through for us to kick-start the discussion of what the use case is that leads us to the discussion of pagination filtering eventing and so on and then we've allocated pretty much the rest of the time to discuss and hopefully get to some points that we can reach consensus on a path forward for those items so any comments or updates and i apologize i meant to update the uh i did that anyway i don't know why it didn't go um on the agenda if not oh thank you so that is the updated agenda that i have on the powerpoint just didn't get the update if there's no comment or further agenda bashing we can go ahead and get started so with that pam you want to come up and give us an update on the use cases documents hi everybody can you hear me okay looks good yes yes so the number one news is we have a use cases document but where we have uh it in hack md format right now the link is going to go in the notes and so we're not going to discuss it today because um obviously no one's had a chance to review it but we will start the process right away of reviewing that document on the on the list so uh it's work that we literally got done here at ietf let me just tell you quickly about the rationale that we're using this was part of the review we did at the last the last plenary as well what you're going to see in there the is"
  },
  {
    "startTime": "00:10:01",
    "text": "terminology we've tried to define terminology that was not defined in the protocols themselves so in rfc 76 43 or 44. so you're going to see a definition of a of a skim service provider right and of a skim client and of what skim schema is and of what the skim protocol is where possible we've taken those from the protocols copying them across and then we're trying to boil them down to make them very brief but they may be wrong and there may be cases where actually there is a definition and it just didn't come up as the authors of this this new draft were casting about there are also definitions so there's basically two types of definitions one is you know the things used in the protocols the second one is industry definitions that would be relative to the use cases where people might implement skim so those are absolutely up for debate uh i put in um slightly provocative definitions so that people would know to to react to them um so don't be afraid to go in and change them we that's the whole ideas that we iterate and and then the second piece is about so the concepts are generally around uh units of work that implementers might want to perform and then the last piece is business scenarios and the goal there is to define end-to-end activities that an implementer might look at and say yes this is something i want to do so the link is in the in the meat echo i will leave it at that so we stay on time and um if anyone wants you know to take the pen or do a a large revision we're going to have the hack md open for a couple of hours so that anyone who's here and has time can you do it themselves or even come talk to us so danny and i are the original originators and then we did do a review in the skim side meeting yesterday so others have had who happen to be in the room have had some uh review of it"
  },
  {
    "startTime": "00:12:01",
    "text": "how's that any any questions before i get kingdom off the stage anyone online who would like to ask a question feel free to get on the queue if you have any comments okay perfect uh looking forward to feedback on the list then there we go uh just one quick addition to what pam said uh yeah we'll leave the hackmd open for a bit and then uh either today or monday or somewhere in between we'll uh publish the first version to the data tracker okay yeah and that's at the point where we'll post to the list yep just a reminder when you when you put it in the data tracker just um post to the mail list and solicit feedback we will definitely do that okay thanks everyone thanks pam okay next up we have janelle and danny hello everybody uh can you hear me good cool uh okay uh so yeah uh is janelle in the chat i didn't see her i'm assuming cool okay uh so yeah my name's uh danny zolner uh along with janelle who unfortunately couldn't be here today uh we have sort of thrown our hats in the ring to be the editors uh for a big body of work that comprises uh changes in additions to the uh skim schemas and protocols both eventually the main documents themselves as well as helping to shepherd a bunch of extensions that will add new functionality next slide please uh so our rough agenda for this uh chunk of time that we have we have some topics that we'd like to talk about that do currently have drafts although"
  },
  {
    "startTime": "00:14:01",
    "text": "several of them have expired uh one or two have been re-you know revived in the past day or two uh and then there's a whole bunch of topics that we'll move on to that don't currently have drafts and just at a conceptual level i'd like to talk about uh see if anybody has opinions interest and uh if anybody wants to stand up and uh you know volunteer to either uh you know author edit uh just contribute ideas whichever uh so yeah we have this agenda here and we can just move on to the next slide and we'll dive straight into cursor-based uh pagination uh so yeah first our section of uh topics is drafts next slide please uh so uh we do have a draft that's out there uh it's expired currently uh by matt peterson uh on cursor-based pagination uh their application is listed here uh at a high level it introduces a few new uh parameters for queries and response attributes so things like cursor count next cursor and previous cursor uh and the at the high level the use case is to improve uh sort of you know massive scale operations when a client interacting with a skim service writer needs to be able to traverse a very large set of results so just sometimes we're looking at millions of results over on the right hand side we have an example you know just get users count equals 10 and the key thing is highlighted there uh where the additions would be uh that next cursor uh and the cursor when provided in uh the query url for a subsequent query would uh return another set of results um so i guess i'll pause at this point uh if anybody has any comments or questions i believe we have time carved out later specifically to talk about pagination so actually probably just get this slide but uh yeah i know there's been some discussion"
  },
  {
    "startTime": "00:16:00",
    "text": "on the mailing list about uh this approach versus a skim events based approach uh but we can cover that uh in the allotted time later on in the agenda i guess uh so without uh any questions in the queue i think we just moved to the next slide please and uh so next uh one of the topics that's in the charter is uh multi-valued attribute pagination and filtering uh phil hunt who's on the you know on the call uh he wrote this draft uh phil if you would prefer to speak to this at any point feel free uh okay fills in the queue yeah it's still there um i think it's based on um extending the complex multi-value attribute sub-attribute filtering and applying the same technique to the attributes parameter and in addition to the attribute sub-attribute filters then you can also say what what pages of that you want um so those two things kind of go together um and it's really in a case of the group saying hey we like it or if there's enough interest to do it um the only complex thing that really came up in the past when we first proposed it was how to deal with knowing how many rows there are and that's up for some discussion and that's probably about the only thing we have to sort through on that spec okay oh and to clarify and to clarify just so everybody's clear what we're talking about is paging of attribute value roles not paging of resources so those are very different things thanks all right uh yeah thank you phil"
  },
  {
    "startTime": "00:18:01",
    "text": "uh so yeah the most common example of uh where multi-value attribute pagination would come in uh would be uh if you have a group with a million members and you don't want to receive one response with a list of all million members but instead break it up into smaller chunks and that's exemplified in the request on the right hand side of this i know from the from the mailing list there's also been some feedback from matt peterson who again cannot be here today uh on a different approach uh which would be to represent group memberships as uh sort of as two different sets of resources like slash group memberships and slash user groups to represent the memberships in a group and the groups that a user is a member of respectively i i personally don't have a strong opinion on one way or the other on which way to go that's just the other sort of you know idea i guess that's been put out there and uh phil go ahead hi um there already is an attribute under the user profile called groups which you can find out what groups a user is a member of so we don't need a new object for that that's already inspect or were you talking about something else i think he was talking about something slightly different um it was uh an approach to not need a multi-value activity um i i believe the the approach was so rather than having because in both cases today uh group memberships as well as the user groups attributes that sort of displays the other direction of all the groups associated with users um those are the multi-valued uh attributes on an object and so when you have a large series of results you run into the situation where you would need uh the multi-valued attribute pagination graphs"
  },
  {
    "startTime": "00:20:00",
    "text": "uh whereas the proposal from matt peterson wants to do new top level skin resources that represented the memberships either you know as members of a group or groups that they use as resources or like results that we've returned so these existing pages and logic rather than multi-value attribute pagination i don't know if you re-entered the queue but if you haven't no i left them on no so the question is uh so yeah anyways at a high level that's this topic at least uh the next slide please uh so one of the other drafts that uh has been out there for a while it's also currently expired is a draft on soft police uh this fairly straightforward graph it adds a new uh resource to our sorry a new attribute to resources uh which i believe is is self-deleted and then you can also query uh with uh and correct me if i'm wrong phil i might have gotten this one wrong i know you were an author on this uh even if cortez has the name on the graph uh but there's also a query parameter that is uh is soft deleted equals true uh i guess rather than you know um attributes are or whatever it is uh the the other form of filtering we're explicitly calling um uh like a filter rather than you know is software being so danny let's pause for a couple minutes because um phil unless you were able to capture it you can go ahead and respond but um i'm gonna ask danny to pause for a couple minutes because"
  },
  {
    "startTime": "00:22:04",
    "text": "my understanding is that the issue right now is that the skim contract says uh skim protocol which is essentially alcohol contract says that once you delete a resource if you try to do a get resource you're supposed to return a 404 it's supposed to act like it's gone and i think the idea of a of a parameter that that sort of says um yes i know i'm querying a deleted resource but i want you to return it to me anyway that's basically essentially what's going on um i haven't delved much further into it than that it was really something morteza was uh looking for um there may be other ways to do this but um if you the key is that if you're doing a get against an object that was deleted the server would normally say 404 not found that's what it's supposed to do that's what we decided so now we're we're trying to say yes but also check to see if it was a soft delete and return it if it is so we have to have something to override the behavior okay we're going to pause one thing i'll say is we should also think about whether that's a security issue that only certain clients should be able to execute that query we're still listening phil it's just i'm pausing because uh we have two av people here that that's all i wanted to say is there might be a security consideration that being able to troll through old links might cause problems and it it might not be something that a normal client can do"
  },
  {
    "startTime": "00:24:01",
    "text": "i haven't thought through it though yep no that's well noted and it's been noted in the minutes so thank you for that testing testing is that better for those who are remote can you hear danny any better i i can answer daryl daryl's question go ahead and answer daryl but okay what is the so the question was what is the expectation go ahead what is the expectation of delete on a resource with soft delete equals true parameter is it a no-op or a hard delete um that's something that the spec would have to define because the first delete that you did which is an http delete turned it into a soft delete because your server supports that then you'd have to ask the question well if i delete it again does that wipe it out so that's something the document would need to spec would need to describe yeah it may already i was looking through it earlier this week i believe it says if you do another delete on the resource while using the install deleted equals true parameter that it that transforms for soft deletions through partners but it's not okay so um pam wants to speak but as a minute taker she hasn't had the chance so i'm gonna give her a pass go ahead pam so i think there's an issue there because it may be that the client isn't"
  },
  {
    "startTime": "00:26:01",
    "text": "capable of of full deleting like that so so that you know just because you want it to be hard deleted doesn't mean it's going to get hard deleted so i assume there's some interaction there i i recall sorry i guess this is nancy as an individual i recall the discussions at identiverse that there needed to be clear distinction and definition of self-believers is hard to leave so i think that's something that needs to be i i would agree and i think to the point that pam raised uh whether or not the stem service provider is capable of either soft deletion or hardware both is probably something that could be straightened out with uh if it doesn't already exist in the core schema our before protocol today for normal hard deletion uh both would probably be represented inside of service rider config elements uh as part of that okay we've we've got michael prarock sorry if i butchered your name on thecube no totally fine uh michael brock here uh yeah i just wanted to state the clear need for a soft delete because there are a lot of audit log type scenarios and read only type of pen stuff for instance some of the items we're looking at in skit and some other areas where there's going to be a hard overlap that may require the ability to say yes this is soft deleted but it may not actually be possible to clear that piece of information so now i'll speak as a chair we just need to have alignment on what it means to do soft delete which is hard to leave yes yeah and uh as a chair would you say that that needs to be figured out in the individual graph that we're submitted or potentially you can put it in the draft"
  },
  {
    "startTime": "00:28:02",
    "text": "go ahead phil um i i'm a little bit confused because um it's always true right now that a server can soft delete behind the scenes um so the client asks for delete and the protocol says that as far as the client is concerned the object is deleted now whether it actually is or not that's only something the service provider knows so really the issue is why does the client need to request a soft delete or not that's a policy of the service provider not of the client so maybe we need to work on the use case because current signaling as a contract is if the client asks for delete as far as it knows it's a hard delete whether it actually is deleted or not that's there and the requirement i understood before was really the case is during life cycle management of the user you want to open up the opportunity to re resurrect the user to maintain the resource identifiers and stuff so that links don't get broken that was sort of the broad use case that i understood from morteza was that resurrection becomes possible and again the client doesn't know that it's just that the workflow on the server will say okay i've already got a match for that based on hidden data that it knows about and you resurrect that account and now that account just exists so the next time the client queries it it's there um so i'm not sure i understand why a client needs to flag flag it at all this is this is aaron i have a clarifying question on the protocol that we're looking at on the screen is this protocol only describing the ability for a client to query a possibly deleted record to to have it potentially return that actually it does still exist is that the idea with this protocol"
  },
  {
    "startTime": "00:30:00",
    "text": "we're not actually describing the the ability for soft deletion to happen in general but the it looks like i'm looking at what's on the screen correctly it's just the ability to query for soft related records that's how i understood it aaron i i would agree we're not defining the actual mechanism of sort of soft deletion and what it means to the application but rather if the application has their you know their own concept of soft deletion that that term at least is i think at least somewhat uh in agreement you know across a lot of identity systems might be a too controversial statement but at that point that it's um this is this draft is to finding a way to for the client to learn that from the so one thing i think we have to keep in mind is that in a world where the skim server is always the authoritative source and the skim client is always not then that's fine but in our case like we now have cases where the skim client can be the authoritative server right so so the question becomes if you know for example if it's your idas platform that is the skim client right there could very well be a use case for them to to be more prescriptive and to demand a hard delete that sounds like a different draft in that case then right yeah yeah okay so again this gets back to the use cases and clarification and requirement sorry i'm trying to hey yeah and this is connor from octa and i'm just kind of echo"
  },
  {
    "startTime": "00:32:01",
    "text": "i think this would need more details on the use case because even for that sort of resurrecting a temporarily disabled user case like there's already the status field that we have on objects that can be used for that so i would be curious to say what specifically would need that soft delete info as opposed to that go ahead phil yeah i was going to add that that it might not be so much as a special flag or we could look at it but it might be that when you go to do an ad you want to say i want to re-add with the i want to create the record but i want to resurrect the old identifier that's really the issue because under skim protocol right now when you create a user the skim server is required to assign the identifier where the client can't say it what the client's saying is i want you to restore your old identifier which was this and that might change the problem a little bit or change the solution a little bit uh does anybody else have uh any i guess comments questions on soft deletion or should we move on i think we should just go ahead and move on okay uh uh also i guess test uh people remote uh is this microphone working okay cool i guess okay yeah go ahead cool um oh boy this is small text for for being here um so i i this slide has a few suggested"
  },
  {
    "startTime": "00:34:01",
    "text": "additions for soft deletion i think given the current uh state of we're not sure that this is the right fit just at a concept level uh we could probably skip this slide if anybody's interested they can go review the meeting materials later uh so yeah i think we can proceed to the next slide we'll uh okay so the the next topic and this is i believe the last one where there is an either current or expired draft available this is a draft that i wrote late last year on roles and entitlements uh the url is definitely wrong there oops but at a high level this draft aims to add two new resource types uh to the to the schema which would be slash roles and slash entitlements uh the purpose of that is to provide uh a way for a skin client to go and query a list of all of the available values for either roles or entitlements that would be accepted for the uh the respective values on the uh the user resource uh so a problem that exists today is uh especially in applications that are linked to a skim server where uh you know what we'll call them the multi-tenanted applications typically they're you know sas in nature uh the customer or like on a per tenant basis can customize what roles are available in their application uh and the skim client today has no uh like protocol or schema or you know"
  },
  {
    "startTime": "00:36:00",
    "text": "skim standard way to go and discover what are the available values that will work uh in these requests and it therefore either creates sort of an out of management problem where if a new role is created in the app it also has to be created somewhere else or it leads to a whole bunch of failed requests when the skim client doesn't have the correct data and is sending uh requests to the skim server that are you know deemed invalid due to uh you know disallowed values um on the right hand side uh there's an example uh just you know it's it's a resource very basic uh has things like you know value display uh just as the the the user resources roles or entitlement attribute would uh on that side um i two nights ago published a new version uh which is currently at increment zero two uh that actually has a few new features as well i didn't include those on this slide uh but it uh those features revolve around uh sort of role or entitlement hierarchy uh in the event that uh there might be a role that is actually made up of several smaller or like less permissioned roles uh so that you can understand uh sort of how the different uh like how the structure of uh of permissions or roles or licenses or whatever the thing that is being represented uh works in that application uh as well as a few uh attributes that help to uh represent the availability of those on a numerical basis it's more towards probably the use case of entitlements uh thinking of uh instances where entitlements may be representing a paid license in a service and you may only have 100 or 1000 or whatever number of seats uh so the ability to know what is the total number of uh users that can have this value and how many currently have it uh are also added in the zero two uh version"
  },
  {
    "startTime": "00:38:01",
    "text": "uh and that's all i have to say on this one if anybody has any questions um if not i think and to so to close out that section um i i think the hope is that for some of these topics uh maybe we slow down a little on say soft deletion and figured out as a group uh given that there are current drafts uh i would like uh discussion to pick back up on the uh the mailing list and for us to get to a point where you can do calls for adoption on some of these existing drafts or figure out why uh you know what they can't be adopted and go fix those problems so that we can start as the working group having uh more adopted drafts to work through um so just keep an eye out on the mailing list there will be some emails uh pertaining to most of these drafts that we just covered uh in the next uh few weeks uh next slide please uh so going through some other topics so these i think for the most part align with the charter that we have for the working group today uh however there are no drafts that have been written uh so this first one is around change detection or delta query uh the use case would be uh similar to uh the you know crystal-based pagination uh it's a tool to help with large scale sort of manipulation and tracking of data it's particularly needed in pool based scenarios where the uh the data is sort of maintained and changes on the scam service provider and is then being retrieved for some other purpose by the skim client uh such as you know a"
  },
  {
    "startTime": "00:40:00",
    "text": "human resources provider where their data is being retrieved for use elsewhere um currently there's uh and i think an option to do a get based on the meta dot last modified attribute uh to detect changes uh however that doesn't uh fit all use cases uh as systems that are sort of like distributed systems such as a lot of uh you know cloud like as a service systems uh may have time drift that uh causes problems with getting extremely accurate sets of results based on time uh so with the the delta query it would help to uh to provide a way to accurately get all changes since the last time that a request was generated uh the example at the bottom is just one possible format i could take you know get users with the parameter of delta token equals and then a randomly generated grid that i put in there um there i know in the mailing list there have been some other you know conversations and uh there are other thoughts on how to approach this uh this is just one of them uh and i see michael's in the queue i'll move from this microphone because he's here in person yeah thanks aaron uh just wanted to speak to the usefulness of this especially when you look at some of the bulk update type nature of the items we're dealing with or mapping over to real world type scenarios digital identities it can be quite useful to be able to say hey i need to see every company for instance that like an entity was registered for an identity was registered for since the last time there was an update or some event that occurred right so it's a highly highly useful law capability so uh is this microphone syllable"
  },
  {
    "startTime": "00:42:02",
    "text": "uh okay so switching microphones cool um thanks if you guys um so uh yeah the the next topic would be the human resources schema this is also part of our charter given the close relationship that uh data originating from human resources our human capital management providers uh tends to have with other identity systems and we are you know system for cross identity management after all um there's a desire to get a unified generic human resources schema uh for skim so that human resources providers can start you know labeling the same sets of data that they may have in the same way rather than everybody you know labeling different attributes different things when they serve the same purpose um so this one does not have a draft um i've mentioned this recently on the mailing list i believe one of the critical things that we will need here is to get involvement from a significant number of human resources providers uh to provide their feedback on uh you know sort of the shape of this schema as if a if it's just sort of identity knowledgeable people who don't necessarily exist in the human resources or human capital management world we might get it wrong and it might not actually help with the problem we're trying to solve uh any questions on this one uh okay in that case i will move to the next one please uh so the next topic would be uh account status context and so this uh lines up with the discussion on soft deletion pretty well uh currently the the only real uh information that you have about a user's status in a lot of ways is the active"
  },
  {
    "startTime": "00:44:01",
    "text": "attribute which is a boolean so it's active true or false uh there's a proposal to expand this out perhaps with a new let's say complex attribute to support active that would be called something like account status where you can see things about that user are they a pre-hire are they on leave unpaid or paid have they been terminated uh there's i think a desire to align some of these states with states from the shared signals community as well um [Music] but uh yeah so this is a topic that uh we would like to see a draft for as well and i believe it is also a part of our charter um i'll pause for five seconds for you know hands to go up otherwise uh next slide please uh okay so the i think we're down to the last two slides for this section so uh there's an improvement to the protocol that i would like to see uh around reference urls and uh there have been discussions in some of our interim meetings before possibly also on the mailing list uh so the key example that uh that i have would be the photos attribute uh in the core user schema uh so the photos attribute is a complex attribute but underneath that sort of the main sub value in that complex attribute is an attribute of a data type uh that is called reference uh which there are actually very few of in the uh in this comes back relative to things like you know strings uh so a reference attribute needs to point to another resource somewhere uh and there's gonna be multiple types there in the schema spec you can go look it up uh the problem with things that are of url formats specifically like the url to somebody's"
  },
  {
    "startTime": "00:46:00",
    "text": "profile picture is that in a cloud like sas internet-based world the systems are communicating over the internet so if as a cloud idp i am on one side of a transaction that's happening over the internet uh and i am communicating all the user profile pictures for an organization uh vscam and i'm giving urls the service provider that is being told those urls will then need to go back and ask for the pictures uh and the spec doesn't actually clearly specify today uh the existing spec on what happens after those urls are provided uh does the app that's consuming them just sort of hot linked to them forever or is the skim service provider expected to do like a fetch and then store it somewhere locally on their side uh so if you know the cloud app represented by the service provider uh wants to you know hot link forever that becomes a problem i don't think uh it would be a very popular solution either for performance or just you know cost reasons uh but the the actual big problem that i'd like to see there uh see us solve here is for these urls if as the cloud idp i provide a url you know something dot jpeg open to the internet how do i make sure that only the skim service rider that i sent it to is able to access it uh the skim standard today doesn't talk at all about securing these uh these urls um the i believe the intention of the original authors was to leave it up to the implementers unfortunately that's led to not you know uh just there's not a whole lot of use profile pictures between uh uh cloud idps and service providers today uh bill uh my understanding is it's just a url so if you are going outside the spec and pre-fetching the data as a service provider yeah you're opening up a can of"
  },
  {
    "startTime": "00:48:01",
    "text": "worms because now you are republishing the picture um so what what was discussed originally was just publish the url and it's it's the client that receives it that has to have the credentials to go and pull it um but but the spec is silent on that for a reason because there wasn't consensus on that so it's just a url um it makes sense that some will want to add value and do that but then they sort of have to solve all those problems that you're alluding to that's just the historical perspective oh yeah thanks phil um yeah i i agree that it's a can of worms there's a lot of sort of questions there about who should be doing what uh i think that a consensus is needed though and that's sort of what i'm calling for here uh just because looking at the number of collaboration apps that exist out there today uh there's uh just in sort of anecdotal experience myself i've had a number of conversations where uh folks that have uh you know that i've helped with integrations would really like to consume profile pictures but there's not a uh a way that we deem secure today uh this is uh me speaking as a product manager at microsoft uh there's a security problem that we would like to see solved here this week we don't uh you know feel that it the it's quite there yet and the uh we need a scalable way to to do this versus uh leaving you know certain decisions up to uh the you know any given set of uh implementers that are working together that's the internal problem essentially yeah go ahead dory hey or steel from transmute um i guess i've seen versions of this problem solved in a couple different ways so i just share you know briefly the some of"
  },
  {
    "startTime": "00:50:02",
    "text": "some of the"
  },
  {
    "startTime": "00:54:35",
    "text": "oh hello those remote can you hear us all right so already i was just saying sometimes the network can go down and you'll be trying to resolve your url reference and it won't work and so one thing i've seen folks do in that case is embed url content so going from a reference to a value with data uris or things like that and then the other system is a trucks trusted high available proxy and there's privacy issues on both sides of this problem so i just wanted to share that in other sort of higher security scenarios where you're worried about dereferencing being observable you might want the embed by value solution and not the dereference solution so this is uh josh baum from rodrigo um i just wanted to might say my take on"
  },
  {
    "startTime": "00:56:00",
    "text": "this is that even though there was no consensus as i just heard that at least it should be in the security considerations probably to just mention i don't know something about it okay uh so yeah this one uh as we even just heard from the feedback i think there's a lot of potential solutions on how to address this um i don't have a great one to propose myself but it's a topic that i would really like to see a group of people uh find a good scalable solution to uh so i'll try to continue this this topic on the mailing list but i would really love to get a draft written at some point for this next slide please okay just so it's on the notes um phil commented on the chat that um it could fit in the best practices uh such as uh the slide right now um yeah and so uh this is i believe the final slide of sort of you know the random sport of drafts that are there uh okay uh so yeah this is the last in the pile of assorted topics and drafts uh so uh i i believe there's interest uh in uh sort of tightening up some aspects of the sim standard uh with regards to security uh you know the scam was written in 2014 2015 that was published in 2015. uh the internet and the world have changed since then uh so just some examples of things that could be profiled and uh you know strongly discouraged in a modern security profile bcp"
  },
  {
    "startTime": "00:58:00",
    "text": "would be uh to drop support for basic auth when uh authorizing the skim service writers uh to drop the password attribute off of the user resource uh and to sort of clarify on this one i'm not necessarily proposing a hard you may never do this ever uh there are always going to be sort of the outliers in the edge cases uh for instance uh skim talking to some gateway that's talking to a really really old like you know mainframe or something where the code on it is never going to change but for one of the really common scam use cases of you know uh cross domain you know identity exchange uh you know sort of the the cloud flavor of it um passwords really shouldn't be going around uh was you know federation has taken its place as as the you know the way to go uh and even just you know it's looking at the basic off piece uh there's there's bear tokens there's oauth there's other ways to purchase over uh i guess clarify basic auth think like username and password um there's a if you can't tell by my lack of you know precision on these terms i don't really live in the auth world uh so i'm adding to uh yours's comment a minute ago the thing on uh sort of uh reference url security may also fit into this uh but this is a thing also you know very happily welcoming any contributors co-authors et cetera but this is something that i feel is needed phil did you want me to channel your comment or do you want to get in the queue okay go ahead phil i think i've got it okay um yeah uh we talked about this before and it may already be in the security condition"
  },
  {
    "startTime": "01:00:01",
    "text": "security considerations and i'll double check that but there there are really two scenarios one is um authenticating so you can make calls to skim as a client which is which is using an http authorization header and it's not a great practice i agree to use basic auth and probably you should never allow that but there are valid use cases we're working with the password attribute and that's why it's in the schema and the idea is that a lot of people were using skim as a directory and their open id provider would call in to skim as a secure client using its credential and it validates that a password matches and usually that's part of a whole mfa strategy so i would say it's a harder question to remove password from from skim schema than it is to simply say it's a really bad practice to use basic off with skim and i'll double check what's uh 7644 says about it right now yeah and the approach even for password may be not to remove it but to just very strongly advise against it unless you like you know we can sort of again it's the best practices so to find good reasons right so here's what the spec says right now usage of basic authentication should be avoided due to its use of the single factor that is based upon relatively static symmetric secret and so on and so forth so the spec already says should be avoided so you can get the only thing stronger than you can say is must not and and redraft the whole spec if i'm understanding this correctly these are two completely unrelated example suggestions that danny is presenting one is about the client"
  },
  {
    "startTime": "01:02:02",
    "text": "authenticating itself to the service divider and the other is about just data moving around so i don't think these were linked together as part of the same part of the same issue and again i don't think it's a necessarily the best time to debate these particular issues right now this is more just to get the ideas flowing yeah my intent is to to make sure we move forward from the current spec then try to fix something that's already in the spec that's all uh yeah i i'm done i'd actually fully catch phil's last point but if it's in the notes i'll read it phil could you repeat that last point please yeah i was just saying that that i i just wanted to point out that if the spec is very clear right now although it does not say must not it says should not uh and it's not even capitalized uh it should be avoided um if if people feel very strongly i wouldn't be opposed to banning it um but i think there's a there's a it's a much more difficult question on the second issue of you get rid of password entirely and i don't think the community is ready for that there's still ldap out there for god's sake so they haven't got rid of that um and so i think that's sort of a key part and and i'm hearing a lot of people really want the opposite they want credential schema to have more sophistication about credentials and they actually want not just the password but they want all the password management features standardized so password policy when was the last change"
  },
  {
    "startTime": "01:04:00",
    "text": "how many failures all those kinds of things it may be that that whole password is subject as part of a whole mfa schema discussion that we should start per your point that things have changed in the last six years yeah and uh that was all i had so i believe this is the last slide of my set and we can move on okay thanks danny okay so next up phil you can start with a quick update on the draft i don't think it will take you that long then we can move to the next uh that's probably uh 30 seconds um the working group had a call for a job for adoption for the events draft that was passed i asked for prior to posting it as for co-authors and thank you nancy for agreeing to to help me with the draft we could probably still use a couple more co-authors what that usually means uh is that i'll be talking with the authors more directly at each publication cycle and hopefully getting help in writing or in editing and so on so on and so forth so if anybody wants to co-author please let me know um i just need a clear uh indication that you want to do that um i know there were a couple other people but i wasn't able to a time of publication get a confirmation that they wanted to be an actual author and i will clarify i will speak now as a participant um since i'm now an author for this draft i will not be speaking to this draft as a chair i will relinquish my role as chair to aaron for this particular"
  },
  {
    "startTime": "01:06:02",
    "text": "okay mike go ahead this is phil i'm interested in helping out so uh you hook me into whatever process you have in terms of respect yeah that'd be great and as for the draft there was very minor changes uh a couple typos and other things um i think we have to start the discussion phase next before you see any major updates so that's it for that okay so we can move you to the uh to the deck the coordination use cases deck thanks um what i thought i saw at least in the last month a lot of duplication of efforts going on or parallel streams that were starting to appear and i had was under the impression that we had made a decision but it seems like we need to revisit that uh so i took the liberty of writing what i saw as the use cases um for for events and for uh paging and comparing the things i also heard even today things like change detection that could be covered for some people by the skim events draft but you may still need change detection as a polling technique um so that's something we we should sort out so i thought the best way to get"
  },
  {
    "startTime": "01:08:00",
    "text": "through this would be to talk about the use cases what are we trying to solve and then we can sort of say okay what approach really works i also want to understand the full set of requirements and also i think one of the things that's been pointed out that the environment just the world of directory services dramatically different um so we now have broader security threats there's signaling going on that we need to do the scale of servers is much much bigger than we had with ldap uh and we have to think differently we now have multiple administrative domains so if we go to the next slide i'll get into get into this a bit so the overall scenario was there's some relationship between domains and this might be a single service where there are tenancies talking to another administrative domain a corporation there may be multiple clouds involved so for now i'm just going to talk about two at a time and it doesn't really matter what they are but the point is there are two separate domains there can be life cycle relationships between users in one domain and another such as an employee when the employee leaves the employer the account let's say at salesforce.com needs to be either deleted or suspended or soft deleted um so that's a big question right there i just gave you three possible outcomes at uh at salesforce that might happen in reaction to a change in status and a parent domain um and that's one of the things that's important to observe is that it's sometimes it's very complex now to go the old-fashioned way of ldap but saying one"
  },
  {
    "startTime": "01:10:02",
    "text": "domain controls the other because what we have is a concept of independent control and slightly independent life cycle management so while there's a relationship a trigger that goes between the two independent action is now more important than ever so we'll go through this um let's go to the next slide so in cursor paging the idea is that the domain on the left is periodically asking for a logical copy of the entire database on the right so that it can do reconciliation now it may do that in one call or it may actually use paging which is what's being asked for so it can get through the result set one thing that is a bit of a misnomer is people are assuming that they need to do that because max results is in force that's kind of a mistake because max results is an optional feature that the server can use and also max results should limit the total results not the page results so i just want to be careful to make sure we understand that but nevertheless you can accomplish synchronization or coordination between domains by doing a polling technique of retrieving all the entries every periodic 15 minutes or every hour or whatever the cycle is and then do reconciliation to keep the domains coordinated what is good about this is that the reconciliation process is deciding what changes in domain b mean to domain a and then deciding what to do in domain a on the sort of circle arrow on the left and right side um i'm just indicating that those domains are are running independently they have their own value add and there are changes that occur independent in each other that's very different from the old world where we had let's even 20 years ago we had meta directory nothing happened in domain b"
  },
  {
    "startTime": "01:12:01",
    "text": "unless domain or a ordered it that doesn't happen today we do have independent operation operation there are also cases that i've seen where there may be mutual uh replications so take this diagram and flip it around where domain b wants to know about changes that are going on in domain a just as much as domain a wants to know about b so next slide in the event based system uh it's really the same things the same environment and conditions except this time when a server in domain b processes a change it issues a security event token and which is actually just a jot and sends it through a transport mechanism uh which could be a message bus where it could be uh it could be the set transfer protocol that's that's already been published and it sends it over to a receiver on the in the client domain side on the left that reconciliation process then happens as it receives each event two things that are that are majorly different one is the event can be transferred in real time as soon as it occurs on any particular of say 100 servers in domain b that server can publish the event directly or it may route that event to a dispatch server which collects all the events as a stream and then sends it to its partner that's really up to the deployer to determine that but the idea is as close to real time as possible you get change notices coming to the other side those change notices can be notices about the skim resource itself this piece of schema this attribute changed and so forth or they can be things like security event they could be account status change events"
  },
  {
    "startTime": "01:14:00",
    "text": "they could be risk events that says there's been a password reset against this user which which then the receiving side can decide what to do and they might not go to the skim server or the directory on their side they may actually send it off to their security team for action there and they there's some other proprietary action one of the things that comes from security event tokens is that each of these messages are just a simple statement of fact it's always the receiver that decides what to do in its domain and to infirm meeting meaning from that event that's very different from the world of scam where we tell the other side what we want through a create post or delete or patch request in an event you're just getting information that you decide to act upon so next slide so these are the cases i've covered there may be more um but let's go through them next slide so one of the cases that does keep coming up is how do you initially set up a server how do you bootstrap it or recover it i'm not sure either draft right now addresses the problem uh it's partially addressed in the current spec with the bulk operation but in reality people are probably doing exports to jason transporting that file securely to the to the receiver side and then they do an import when that happens we can talk more about this issue but i think it's a it's a special case problem that depending on what you have may be solved in different ways so for example if all of your nodes in your skim service provider are communicating and replicating using a message bus these days often the message bus technology itself is your recovery mechanism so if you lose a node you"
  },
  {
    "startTime": "01:16:00",
    "text": "simply go to the message bus and reload the whole message bus and you're up and running um that's one of the techniques there are many other techniques um but that's sort of an issue for product managers i'm not sure it's an issue for us to have to deal with when we're talking about that kind of recovery cross-domain recovery is something we need to talk through as a use case and flush this one out a little bit more or cross-domain bootstrap where you're just setting up a new domain so i'm a new customer at salesforce and i need to set up my tenancy over at salesforce how do i do that quickly so that's that's one case so the next slide so in this side case we have we have uh something goes on in the um controller side and it wants to uh reconcile a change that's concerned on a slave side and reconcile that change so it wants to uh pull the change information across and decide what to do locally to keep in sync or vice versa it may also decide to make subsequent changes on the remote side depending on what the policies are but we we're not talking about specifying what gets done next so this is another case i think this is important because where we're in the past with ldap we didn't standardize replication because typically uh customers would be deploying one server across all of their one vendor one product across all so what each product chose to do didn't really matter or impact replication"
  },
  {
    "startTime": "01:18:00",
    "text": "i don't think that's true today based on the provider you're using might influence which skin service provider you software you implement and if you're using multiple cloud providers there's probably lots of people who are going to run into the case where one server is running one service product and another service running another and it occurs to me that internal domain replication is a challenge another thing that that typifies this environment is that you're all in the same security domain schema is likely to be the same and your goal is to copy information between nodes in its whole form so in this case when you send a change you want to send all of the information all at once when i'm cross domain i know that the receiving domain might be interested in the change but i don't know necessarily what information they're interested in so that the concern with cross-domain coordination is don't just send them all the changes that occurred just send them what they need to know in order to resolve the issue in internal domain will publish all the replication information so that the other node can be quickly made current that's really the difference i see between the two cases next slide um more and more it wasn't the case a few years ago more and more uh the notion of whose master and whose slave is getting weaker and i'm seeing more cases where people wanted to have bi-directional event coordination um so that's just something in the back of my mind that those two diagrams for both paging and both uh event delivery you might want to start thinking about throwing away master slave and start"
  },
  {
    "startTime": "01:20:00",
    "text": "thinking about uh that it's bidirectional you may have to set policy that says one domain is authoritative over rolls but the other domain is authoritative over photos who knows but um it's a little bit more complex than we had even six years ago thanks next slide this sort of flows out of the risk sharing events under open id um people are interested to know about higher level events that come out of a skim repository such as a password was changed or an account password was reset um or suspicious activity there's a bunch of high level events that may be detectable or maybe being tracked in a skin server we don't have necessarily directly all of the all of the data involved in this because uh as i said we haven't standardized things like password failure accounts and things like that but certainly the side that wants to share that information has built that information with a combination of skim and other services and they have that data and they want to be able to share that so that receiving clients can know that they can take independent action um so the other thing so the scenario that risk sort of worries about is if somebody adds a new authentication factor they want to make sure that the person adding the authentication factor wasn't somebody stealing the account and so they changed their security policies temporarily on the receiving side to enable account recovery for a period of time and then once the factor is laid down and it's working they now know that the account is good to go sometimes they're just taking that information and saying we're going to reset our login sessions and force that user to log in so that's what signaling"
  },
  {
    "startTime": "01:22:02",
    "text": "is all about is the ability to take independent action by the receiver and decide what's appropriate based on your own local policies next slide i wish now i'm looking at this slide on my desktop it's quite small uh i tried to go through a comparison i i will first of all say i'm biased because i started off by looking at paging um my chief concern with paging is that you're only doing it on a certain frequency it's periodic uh whereas events you're trying to get make sure that it gets delivered as in close to real time as you can and the effort you put into making sure that happen will get you closer to real time if you decide you're not going to do that you can still uh throttle event delivery through whatever mechanism you want to use and say no i'm still i'm just going to pull for events every 15 minutes you can go both ways with that but on the left my concern has always been downloading the entire data set particularly if you're talking about billions of users is is a a real challenge in terms of raw cost um data exposure you're exposing all of your data every synchronization cycle [Music] i would prefer to see a way in in the skim coordinated event spec in fact the changes don't initially at the event stream doesn't contain raw data it just contains a notice that an identifier resource has changed and then the client can go and find out what that change was and that's how it works so we we share minimal information in that profile and i think that's why it's a better draft i think also that"
  },
  {
    "startTime": "01:24:00",
    "text": "the ability to leverage that event mechanism to do things like async sing async signaling um so the bulk request has completed um to send security events such as risk signals and other events creates a lot of value and it sort of fits in with a general pattern between open id oauth and skim as a three-legged security system uh that can be coordinated and so that's where i'm coming from um so that's it and and uh i also would like to invite danny and i wish matt was here to also add where i've missed uh the benefits of theirs i do think the the paging protocol is relatively simple to specify but it's much harder in my experience i know of a lot of databases that can't support cursor based paging or if they do you get problems like thrashing because what they end up doing is maintaining a copy of the entire database in memory and you that leads to swapping and other things and if you're a service provider with many tenancies on the same server you haven't got a lot of memory or a lot of disk to hold multiple copies if if you've got 50 clients doing paging at the same time you won't have enough memory at all um i still think you could avoid paging by doing a get as long as the service provider agrees to give you unlimited search results you could still do a cyclical get and that can work and that is really simple um but it still has all the problems of you can only afford to do that only"
  },
  {
    "startTime": "01:26:00",
    "text": "every so often once a day or once an hour and that leaves you a one-hour gap and is that good enough um so so that's it for comparison and i hope that helps okay we only have like a couple couple minutes behind let's let danny respond and then we'll move on hey uh danny with uh microsoft uh so i i did have a chance to speak with matt peterson uh before this uh he's unfortunately not able to join us today so the one of the problems and it comes down to either we would have to you know specify it in the uh in the events draft or it's left up to the implementers is that in some implementations of uh like shared signals transmission once the signal has been provided and the uh the receiver and i apologize i'm using the wrong terminology uh once the c receiver is responded with uh 200 okay the transmitter may not have a that obligation to actually hold that message anymore one of the benefits of first-year-based pagination is that uh the for a limited period of time that same cursor can potentially be replayed uh to in an event that there's uh some sort of you know infrastructure problem the vm hosting this has gone down uh you're able to get your data back uh i it's solvable on the shared signal side as well uh other things sort of in favor of cursor-based pagination uh so just really the infrastructure required like you as a skim service provider uh the implementing another you know parameter set of parameters some attributes uh to support cursor based pagination it is still going to be the more simple option compared to having to set up uh whichever elements are needed uh infrastructure wise for the uh the shared signals processing uh and as a"
  },
  {
    "startTime": "01:28:01",
    "text": "counterpoint to uh sort of the the the downfalls from like a software engineering standpoint on cursor based pagination um i i so matt's feedback uh to paraphrase him was that um there are also a number of uh existing uh databases you know used in identity systems where they natively do cursor-based pagination and even today to do index-based pagination they're having to store their results in memory in an index format because that's not natively how they work with today um i feel like i had other points i'm losing my trying to thought slightly not sure why don't i why don't i respond to the first couple you made and then you can add yours because i don't want to lose the thought um on the set transfer we did a lot of in the in the security token the id token working group which which sort of took the skim group and a number of other groups to work together on a common spec we talked about this and the issue from any service provider was the sheer number of events that are flowing out and being able to persist those for long periods of time becomes untenable or let's just say we didn't get consensus on that what the group decided was is that the transfer spec give you guaranteed transfer because it's not just a 200. the client responds it acknowledges that an event was received the responsibility for recovery then becomes the receiver's responsibility so once that client receiver says yes i got that notice i'm acknowledging it that tells the service provider it's now allowed to forget about the event so that's the way it normally works in in practice there's nothing saying that the publisher of the event can't keep the event indefinitely uh there's nothing saying that or not what what was wanted was the ability for the publisher to only have to hold events"
  },
  {
    "startTime": "01:30:00",
    "text": "for two or three days the idea would be uh you're sinking let's say with azure between google and azure and azure goes offline or google goes offline for three days when a call comes back up will it get back in sync that was the level of recovery people were agreeing to but if we're talking about recovery of a lost server that needs to go back and figure out a month's worth of history since the last backup uh that's a different thing and it would be up to the receiving domain to figure out how they're going to manage that recovery issue anyway so again making recovery the client's responsibility gives the client domain full control over their data set and how they do recovery and it lets the side that's publishing in the event not have to worry about the internal problems on a receiver domain uh so that i'm just telling you what the design is that's it okay thanks phil uh well so i just wanted to to say we're transitioning and we are behind scheduled but the whole notion why i put this presentation ahead danny of this was to trigger the discussion of the use cases as well as the requirements as we see them that leads us to the notion of we've already adopted the skim events draft there's been discussion about adoption of the other two drafts or potentially right and so this is where we allocated 40 minutes we're now down to 20 some odd million minutes right to get into that discussion of alignment to drive consensus sorry um amongst the participants here of at least getting alignment and agreement of these are the use cases and and that are driving the requirements that we need to address in the working group"
  },
  {
    "startTime": "01:32:00",
    "text": "so i think with that i mean i i'm actually fine because you've started talking about the pagination right but danny i had allotted for you and janelle to help lead that discussion um it's too bad matt's not here so it's not just you and phil but others p please feel free to to jump in as well uh yeah i i agree i wish matt was here because he can speak to this better than i can um so yeah i guess the so the use case behind pagination um and just like full transparency i i work with skim almost exclusively in the scenario of we'll call it a centralized client working with a service writer or a set of service providers um and in that case usually the client is acting as an authoritative source trying to push data elsewhere um in those scenarios i think pagination is needed uh for instance i guess so it's not just push it's also pool data potentially where you know data's going from somewhere else in uh but in either case uh in instances when you want to uh not only sort of uh you know make a suggestion on the state of things you know you're sending the data that you have and there may be other data in a system uh in when you as a client need to uh sort of be able to see the full state of the external connected skim system uh precisely the problem you said of you know millions of results uh i don't think cursor-based pagination alone solves that um it's probably cursor-based pagination with uh some sort of delta query together allowing to paginate a set of results and even if you have five million results if you're able to first say give me the results that have changed since the last query and then break them up into smaller chunks uh it's"
  },
  {
    "startTime": "01:34:00",
    "text": "sort of uh like it correctly wrong isn't that how old that lets you do it like it's it's not really reinventing the wheel in a way right yeah i i don't know if there's a an ldap uh user here that can speak authoritatively well my my concept was the uh thrill here again um the cn equals change log thing in ldap was rather uh um in my experience uh a description of how not to do things because it was a common changelog you you it was very hard to implement good security on that because it's wide open and if you have multi-tenancies it just gets it becomes very very complex to secure and it also has a lot of high value data in it um i i would prefer to avoid that pattern if we can um because i think skim is supposed to improve on ldap not repeat it yeah um and so i can't speak authoritatively just because i'm starting to get outside of my expertise but i'm uh my understanding is that there are a number of other rest apis whether for identity or something else where a delta query mechanism does exist like sorry i don't think we're treading uh you know brand new ground so much as adopting a solution that's used elsewhere um the exact engineering pitfalls i don't think i'm particularly qualified to discuss um but i so i guess so to pivot slightly um i think one of the points of contention uh is uh sort of the the scope of where uh where the events draft is useful versus"
  },
  {
    "startTime": "01:36:00",
    "text": "cursor-based pagination and i think there are systems where they could in lieu of you know a delta query and cursor-based pagination they could just use an event flow to to keep track of things uh but there are also systems where that's not really feasible uh and so the the high-level concept of uh security event tokens containing skim information um i i'm aware of like other i uh you know uh people conceiving the idea besides you as well phil although you're the only one to actually publish a draft and in the use case that uh the the other group that i've worked with uh was aware of our i guess was focusing on was more around a skim service provider communicating high priority results uh or high priority uh data that could not wait for that next you know pulling cycle that you mentioned here maybe like three hours away uh so the classic example from sort of an identity like synchronization provisioning standpoint being um a client normally pulls data from a human resources provider and uses that to you know do things downstream and they do that every three hours and the uh the skim service writer which is the human resources organization they uh have you know breaking news that uh a certain employee has been fired and needs to be terminated immediately and you know like whatever happens after somebody's fired you know what the facility will call it and uh so alerting those high priority changes uh is the use case that um you know sort of me speaking as an implementer rather than an author or anything uh look first saw when reading your draft so okay um i i just want to add one thing yeah so phil i i was just gonna channel in the uh in the chat when ting mentioned"
  },
  {
    "startTime": "01:38:02",
    "text": "uh i think cursor pagination and events both have its unique use cases um in the area of data sync i'm just gonna read it i felt we need both even though the changing event published should be the primary mechanism to sync the data efficiently um so that kind of triggered a question in that danny if i'm understanding correctly you're also describing the so phil has the general requirements vis-a-vis he's articulated the use cases vis-a-vis the requirements from an abstract you're describing the anecdotal pragmatic of this is how the deployments are using it today with those anecdotes right um and then given when king's comments my question to the group that i will pose is this isn't the first comment that has come through saying well maybe we need both mechanisms so i will pose the question to both the implementers as well as the users of these solutions if the solution requires support for both of the use cases meaning if the argument is we need this mechanism for use case a we need this mechanism for use case b if there is a deployment that needs to address both use cases for me the question is both the implementers and the deployers are they willing to absorb i'll call it the overhead for just generality of having to support the two mechanisms and i i'm not expecting an answer now"
  },
  {
    "startTime": "01:40:00",
    "text": "but it is a question that we as a working group need to think about right as we're saying well we need both meaning two protocols two mechanisms right yeah i guess since i'm standing up for the mic i will sit down in a second um i speaking more as an implementer i so i agree that i think both are needed um i sort of just described my thoughts on the use case and it's a bit more narrow than some of the examples that phil has given uh whereas we we haven't really ever looked at it you know speaking for microsoft as a sort of a a wholesale you know like replication uh feature to move all data around potentially you know like all the changes versus high priority ones i there's definitely a strong preference i don't i don't think i can speak to say we would never implement uh you know to the like the the level of uh what phil just described as examples but i think there's a strong preference to be able to do that purely with uh like operating with the rest api as a client versus also having to have the receiver in place so you're saying as an implementer you would be okay supporting both mechanisms uh not quite so there are there are certain use cases such as the human resources high priority uh change notification of a termination where implementation makes sense uh at the scale of uh sort of all like replication of changes from side a to side b happening through events rather than through a polling model uh my sort of you know rough understanding not a hard statement or commitment is that as microsoft as an implementer we would prefer to uh act primarily as a client and just pull for those changes using cursor-based pagination a delta query and so on"
  },
  {
    "startTime": "01:42:00",
    "text": "rather than also having to have a listener for the events to come in okay and uh i guess just one final clarify so in the in large distributed cloud systems i think there's uh maybe a slightly higher risk you know when you have like a thousand or ten thousand or a million or whatever little containers running they're one of them may go pop off and die and get reprovision so in any scenario where you're receiving something like that um the the polling model is a little safer i think in when the client is a cloud distributed system uh because you can go and remake that same request if something happens to one of your many uh little nodes that are running and i'm not a software engineer so if i'm wrong i'm sorry i'm saying okay phil's been patiently waiting in the cube so phil um yeah i i wanted to point out because people were looking for a compliment to skim events there's actually two other mechanisms we haven't discussed as well that can be used one is rfc 7232 which people haven't really looked into because skim's an http profile and it does mention e-tags and what e-tags give you is the ability to put http preconditions on your re on your request so that means you can say um get this resource uh as long as it's changed since i last queried it if the e tag has changed the e etag is just a hash of the resource so if the resource has changed give me the resource so that's what happens on a get and then on the modify you can put a precondition that says this this condition only applies if the resource hasn't changed underneath me so that's one of the techniques that http offers and the skin protocol spec does specify that"
  },
  {
    "startTime": "01:44:01",
    "text": "the other mechanism that skim offers if you want to know what's changed is you can do a general query and query metadata last modified and say since a certain date um that might be somewhat more crude than people want but those two things are sort of there and that's what's in the back of my mind as the complement to the event spec if i need to get a list of identifiers that have changed since its last date i could do a skim get ask for attribute id and last modified equals metadot and that date um so so that's already possible in the current spec we don't need a new spec for that okay danny you're back in the queue and pam so you are um yeah i i think there's hesitance to lean on the meta dot last modified thing uh i mentioned it you know 45 minutes ago probably because in distributed systems there's way more like times to be kept track of potentially just it's harder to get more like to get a certain level of precision and results if there's time drift between different systems um and then with e-tags uh my understanding of it um so when you mentioned on the mailing list somewhere in the past month i went and did a bunch of looking because i've never had to interact with them that's a resource level hash so it doesn't necessarily help with the the delta query on a large scale of you know if i have a human resources platform with 500 000 resources in it that i'm trying to create like i i can't go and say give me anything that's changed in you know the past uh since i last talked to you three hours ago whichever right it's i want to go see is danny or has danny changed in the past three hours i'd have to do a whole bunch of requests or bulk requests or whatever"
  },
  {
    "startTime": "01:46:01",
    "text": "i'll it fell i'll just respond um i don't know that precision matters on last modified you could certainly um add a few minutes to that date and you'll get more data than you necessarily want but i don't see you losing data it's certainly better than querying everything so that's that's there and etag yes that's the limitation is it's a resource level hash so it only works on specific resources that's it for okay go me okay pam dingle uh sorry i forgot to say that before uh from microsoft um so what i've i think we can debate till we run out of breath but we really need to get this data in the hand of the engineers of the that are going to implement so can i suggest as a next step we create um an analysis and maybe it's this comparison chart but with all of the options instead of just two of the options and we actually create a survey that we can send to as many engineers as we can that we know have implemented skim 2.0 and try to get those um you know get answers back in a format then we could collate to understand what it means it's possible it's it's uh the notion of posting in the mail list and then for those who are participating in the skim to solicit that feedback from their implementers i call them deployment or slash customers but yeah just from my perspective i think if you go with paging nobody's going to bother with event signaling it's it's it's it looks like it's easy i tried to implement it"
  },
  {
    "startTime": "01:48:02",
    "text": "in a scalable way i've run into a number of problems all the research i have on databases say that cursor cursors are for subsets of data not whole sets of data because you either end up locking all the rows in your database if you've got a billion entries in your database and you're trying to lock all the rules likely won't happen and then the only other way is what some servers do is they create a virtual copy and they run out of virtual memory which is a hard thing to do and a very expensive thing to do and that's why i've been saying it's a denial of service problem that you will have to address because if the server runs out of memory that's a great way for a hacker to shut you down uh that was a problem in ldap that i'd like to keep out of skin um so all those caveats um i think if we choose multiple methods there has to be a strong reason for paging to exist outside of uh coordinated replication otherwise you're just going to have half the community just one spec half the community does another and you won't have interoperability and you'll end up either implementing both um and be frustrated by that so i'd rather have one spec that's why i post a question right does it seem like events in solving a particular use case that is useful and so for the implementers that have to support all the use cases are they okay supporting them danny you were in the queue hi uh danny's owner microsoft again um oh boy what was i gonna say um um um okay so the as i said before the the use case that i"
  },
  {
    "startTime": "01:50:01",
    "text": "see and like the reason why i think uh shared or the the the set skim thing would still be adopted by some people is that very specific problem of a service provider needing to urgently communicate something back to the client um i can i can definitely see your concern of if we go ahead with cursor-based pagination that and you know we'll say the combination of cursor-based pagination and delta query because they sort of jointly solve a lot of the like large-scale problems that people would not necessarily implement shared signals to do that instead um so not being able to speak you know other than channeling matt peterson's words for instance on the ease of implementation uh of one versus the other you know like the database and blocking memory and all that um i i suspect that the majority probably would go with the cursor-based pagination and delta query unless they were operating uh more as a source rather than a recipient of data uh it's still in a pool model so you know hr provider problem needs to tell somebody that a user was terminated um like i i've always like i've had trouble following in the email threads at first just because the i think the use cases that i've seen you envision phil for uh share it for the shared signals have been much wider than we originally um you know speaking for on behalf of my colleagues at microsoft that we had envisioned ourselves oh one thing you're in the queue oh now we can we can hear you but uh if you can speak a little louder would be great uh okay uh"
  },
  {
    "startTime": "01:52:00",
    "text": "let me see can you feel me better now yeah we can hear you fine now okay so basically uh we are doing a lot of integrations with certainly the turkey sources and without other hr systems i think our experience at least for my uh visibility is that uh we integrate basically start large system a small system as well so my my feeling is for one uh probably all the included usually start with a search for platinum means we call the plague nation because that has other use cases like certain use cases usually they start with that with that and just publish that api rather than in systems and we start from the usually exclusive interfaces that we integrate with them but later on if they do see use cases for the event when they're running stability issues they start to publish those data changes that sounds like that you uh like for some real-time notifications through your channel right uh so so i i feel that i'm not sure as a as a standard or community do we want to give people choices because in reality people always start with like a curse of purging and the late army's unions right that looks like a common pattern i run into what i did with all the integrations of like my country theology integrations that's just that's a common reality i'm sure the community as a standard body do we want to force a particular solution more geared towards the large systems or do we want to give people choices that's one the second question is that um like like i said they usually uh start with uh with a search-based or cursed based approach and later on their ad events channels or when they run to escape issues or when they really require real-time notifications in terms of mechanisms or events usually there could be like a like a published pub sub systems or they could use uh uh like uh uh human hooks to notify us on you know you demand like"
  },
  {
    "startTime": "01:54:00",
    "text": "hr systems to notify a termination or something like that using your time with even hooks so uh those that clapping we saw just share some of the experiences we had here so thanks quentin in essence that's the question i posed earlier right in the survey to the implementers right um the other suggestion i might make um and that goes to phil danny and and to matt when he comes back is um the comparison chart that phil started is a really good way for us to provide that succinct data if you will and so i'd encourage you to to do an update to it um based on danny your yours and matt's feedback and alignment and then we can bring it back to discussion because i i don't know that we we have the answers that we need now to get us to a path forward any ah okay so when ting the note takers are requesting that you look at the hedge stock document to make sure that they have appropriately captured the points that you wanted to raise uh second i will take a look and another point i want to mention is that also we run two cases where we have to do a full like full import like export import in order to verify some kind of fix some issues on the data stream in the young channels we run into some of the issues where in the data that like a delta changes that we just on the youtube channel sometimes we're going to issues"
  },
  {
    "startTime": "01:56:00",
    "text": "cannot be resolved we have to resort to the back to the full import or resolve some of the bugs or issues of your channel you got two thumbs up from danny in case you didn't see that one any certain comments any other comments or feedback on this topic oh danny's getting up uh one final thing this is actually a thought from matt peterson when we were discussing this earlier this week uh is that i i think we look at the uh of the capabilities provided by the events draft as an a potential optimization to things like change detection and uh and whatnot versus a replacement to it okay given that we only have three minutes left i will uh close this topic and do a quick chairs update we are behind on our milestones the chairs do need to update the milestones and pam i think when you put the use cases aaron and i can come back and give you a swag on when we can have you target to a working group last call it may be a living document because of this use cases and we're still teasing out requirements it may be a question that we put to the mail list of just deferring publication of that until we believe we've agreed on all of the use cases meaning we can continue to work on the extensions that we need protocol and schema okay so that's number one so that one we may stretch out on the milestones"
  },
  {
    "startTime": "01:58:00",
    "text": "um we had said that we would publish a protocol and the schemas given the discussions and the way we're moving forward i see the path of a lot of drafts so we can take that up in discussion in the mail list of whether there is one or several um that said we cannot update those milestones until we we have that discussion so for that we need a couple drafts and i believe danny you've submitted one of them for the roles and entitlement um so if you can just as the author you can post on the mail list and request feedback and comments um for the chairs we look for at least three individuals who are not authors of the draft to provide any feedback uh well let me rephrase constructive feedback either positive or not for whether this draft should be considered to be well i believe the comments so far the drafts are considered to be in scope but more of whether it can serve as a seed a starting point for us to move forward and progress on a particular topic okay um so i think with that i may just adjourn us unless somebody really has anything burning they want to say in the last 30 seconds going once going twice all right thank you all for participating both remotely and in person it's been a good ietf 114 um we will try and schedule an interim"
  },
  {
    "startTime": "02:00:01",
    "text": "likely in september i think is what we were discussing so um we'll put out a doodle poll to pick a day and then from there post that interim we hope to see you in ietf 115 either remotely or in london thank you everyone thank you to the notetakers pam pam you are incredibly awesome no i i like i mean for me as a chair to go back and say what is and i did fix um"
  },
  {
    "startTime": "02:02:08",
    "text": "oh okay thank you um okay"
  }
]
