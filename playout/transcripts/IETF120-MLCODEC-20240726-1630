[
  {
    "startTime": "00:00:47",
    "text": "I hope that construction will come through the... I remember this yesterday better the meeting room than the hotel room Good morning Yes, and we will need neither hopefully We were rescheduled from a small block of time, I believe, to"
  },
  {
    "startTime": "00:02:05",
    "text": "and we'll get this Indeed We could have probably squeezed into it one hour block, but just to give comfort and avoid any cutting people off at the longer slot"
  },
  {
    "startTime": "00:04:11",
    "text": "Just give it one more minute, start five past. That's what I was thinking Okay let's get started. This is machine learning for audio coding If you're expecting to be somewhere else Yeah, okay. This is machine learning for audio coding If you're expecting to be in a different session, you're in the wrong place note well this is an official proceeding of the IETF so your activity here is subject to, a number of IETF policies, which are noted on this slide. You should be aware of these policies before participating in it activity here is subject to a number of IETF policies, which are noted on this slide. You should be aware of these policies before participating in any IETC activity activity Those policies also include standards for conduct, including anti-harassment and so you should be aware of those as well We all want to treat everyone with respect and decorum"
  },
  {
    "startTime": "00:06:03",
    "text": "this is the last day of the meeting so hopefully people are already familiar with the tools. But in particular, if you're in the room, you need to log in to the online tool in order to register your participants in the room and also to get in Q cue for the mics if you seek to do so Yeah it's going to be fine. Apologize for the construction We aren't compressed today for time, and so if people are you know, need anything to be repeated because of background construction noises, please just ask for it to be repeated Stefan, but unfortunately I think it's right above us Can you close the upper doors? Can we get anyone to volunteer to take? notes for today? There shouldn't be a significant amount of note-taking required Just capture the major items and the notepad is already pre-populated with the agenda So it should just be a few sentences Thank you. Thanks, Jonathan Okay so do we have any agenda bashing? The agenda as we have set out today simply as the updates on a number of the drafts that people have been working on are as we have set out today, simply as the updates on a number of the drafts that people have been working on around the working group, the OPEs extension mechanisms, the deproduct"
  },
  {
    "startTime": "00:08:00",
    "text": "redundancy and the speech coding enhancements we have time available, so there should be plenty of room for questions if any come up Okay well, no agenda bashing, we can hand it over to Tim All right if you guys can still hear me Yes, you're loud and clear. And I'm going to hand you control of, uh, control of your slides. Oh, great Now means I've got to figure how to use that. Yeah, you should have control there you go. Yeah, OK, I got arrows. I can do it All right. All right so this is the extension draft um there's been no changes to the actual draft for two meetings now. So it's submitted a zero two version just to refresh the x-free date Um, so diagrams from a previous presentation basically showing the actual format of an opus frame and how the extension fit into that And so the thing that I want to highlight on here, for this presentation, is the amount of overhead it takes in order to actually send some extension in information So if you're just sending a single frame, you have to use a code 3 packet which has this extra frame byte count in it in order to get the padding bit to turn padding on So that's one byte overhead You need to signal the padding length, which can be multiple bytes but it's at least one byte. And then in the actual padding itself, you need"
  },
  {
    "startTime": "00:10:00",
    "text": "to signal an extension ID And only after you've done all that, you can start actually signaling extension data So at a minimum, you have three bytes of overhead if we look at the multiple frame case, you already have the frame count by, so that doesn't count as overhead But now for every frame, you have to include both you know the extension I as well as one of these separators to mark the boundaries between frames And the first frame has, again, the padding light so you have two bites of overhead for every frame So sort of look at a move motivating example. This was the reason this is at a motivating example. This was the reason this issue was originally raised. One thing's that people will do is they will set P time to 60 million to try to reduce header overheads, right, from RTB and ITP and EDB and all that And so when you do that, what often happens is you can free up enough available bandwidth to actually use hybrid mode or even Kelt, which leaves you encoding 3 by 20 milliseconds frames in a single packet So now if you want to add extensions to those frames, we take the minimum amount of extension information that we can send that has an action payload per frame. So we say one by extension on each frame, you wind up having two bytes overhead for each one byte of payload so for the three frames it takes you nine bytes in order to send three bytes of data you know when when you're in the compression business that's not not great right right?"
  },
  {
    "startTime": "00:12:00",
    "text": "so it occurred to me that a lot of these bytes are in fact pretty low entropy and easily predictable so that we ought to be able to do better than that And so the proposal I came up with was this um, is a new repeat these extensions extension So we don't, you know we may not necessarily know exactly how extensions are going to be used, but it seems like a pretty common use case that you're going to want to use the same extension over and over again at every frame So this adds a new mandatory to implement extension with ID equals 2 When you encounter it in the current frame, for each frame after the current frame in the same packet, and for each extension, that you've already seen in the current frame, you decode an extension of the same type for the next you know those following frames And this happens right away, so as soon as you see that that equals 2 extension, you go through and decode all these extra extensions for future frames If there are short extensions, they use the same L flag as the original, because the L flag is in the extension ID byte, so we're emitting the ID bytes for these repeated extensions and just coding the payload payloads If it's a long extension, then you, explicitly code a length for each long extension Unless the L bit was zero for the RTE extension and you're on the last extension. So for the very last extension of the packet, if it's a long extension, you don't need to code length So the same as normal On the other hand, if the RT extension had the LFlex set to one, then after you've done recoding all these repeated extensions, you go back to coding"
  },
  {
    "startTime": "00:14:00",
    "text": "extensions for the original frame C C And finally, if it had L equals 0 and it didn't end with a long extension, so you didn't wind up consuming the rest of the packet and there's still some more data, then continue decoding extensions for the next frame So it basically allows you to sort of save a separator from byte to advance to the next frame in that case if you already were on the last frame then you know you do the encoder muscle with zero decoder must ignore the rest of the packet Not that there's really a reason to use, repeat these extensions in the last frame is there's no frames to repeat things in So before I move off this slide is, did I confuse? everybody or any of that makes sense? sense? Jonathan looks maybe perhaps confused All right, well, I'll maybe go through the example and, or, yeah, if you want to say something like too you can ask. Sorry, I forgot to use the cue Yeah, I think I, the general idea looks good. I didn't quite follow the algorithm, but since you're the one of us implemented I don't care that much Fair enough So basically, to go back to our 60 millisecond packet with 320 milliseconds frames example now if we want to send a one byte extension on each frame, we have to spend one byte to code the padding length. That didn't change One byte for the extension ID in the frame frame, and then one byte to signal the repeat these extensions extension and then you just"
  },
  {
    "startTime": "00:16:00",
    "text": "immediately code the payload for the other other extensions without having to signal any extension ID or any frame separators So in total, you have six bytes So, Tim, can you go back one slide, please? Sure So maybe to help some of jonathan hui confusion because I think maybe I had some of the same confusion on his, even when I read it before still not sure even after your description is the position of this repeat this extension extension significant are you trying to signal that the extensions that came before this were only once one shot and the ones that came after this are repeated or it's not the position that's significant? in the extensions, it's the position in the frame that's significant that previous frame in the Opus payload don't apply, are not having this repeat extension applied to them so so let's second thing you said is correct. The first thing you said was only almost correct but it's the other way around so so the position in the frame is significant and it basically says you're repeating all of the extension that you've seen so far in the current frame right so so if you then continue coding extensions in the current frame after that, they don't get repeated. So basically the position in the frame marks how many extensions in that frame you want to be repeated in future frames frames Okay All right, so kind of to some"
  },
  {
    "startTime": "00:18:02",
    "text": "summarize, the reason that we want to do this is that, you know, we can reduce overhead even if you just have one extension of it appearing in two frames, like you're already saving at least one byte The savings scale with the number of frames and repeated extensions It applies to any extension, like you don't need to have extra extension IDs to register a CPC signaling, which I think some other people have proposed an alternative ways to solve this problem It integrates well with non-repeated extensions like DRED So for example, if we went back, to that three frames, in one packet with one byte repeated extension if then you wanted to add a dread frame to them in the first frame it would wind up coming after all of the repeated extensions And so you can code it without signaling the length, because it's now the last extension of the path even though it was on the first frame And finally, the reason to do it now is because if we did it later, it would be it breaking change to extension parsing So we kind of need to decide if we want to do this before we finish the draft So the reasons not to do this is, you know, it is additional implementation complexity I did wind up putting together a patch for it before the meeting here. I think it's like it it is additional implementation complexity. I did wind up putting together a patch for it before the meeting here. It adds like 100 lines of code for the decoder and maybe something similar to that for the encoder So not the most complex thing in the live opus code base um Question is, is it worth that for? you know, the, is this a niche enough of a use case? that it's not worth it? um and then you know"
  },
  {
    "startTime": "00:20:00",
    "text": "finally you get this sort of weird thing that the extensions for a frame are no longer guaranteed to be physically continued So like if you just enumerate them in the order that they're in the packet, they won't come out in framework which is maybe a little bit weird I don't think it actually breaks anything So, I don't know, how do people feel? So has anyone actually looked at this? proposal? Oh, yeah Tim just presented it jonathan lennox, I think this is an audio code complicated algorithm for making smaller as the name of the game So I'm okay with implementation complexity. I would suggest you make sure you have a you know, the document give up full set of examples that can basically turn into a unit test to make sure your parser is correct. Yeah, yeah I already have something like I think I have more lines of code for the test right now than the actual feature Joe Moore Just to say that to me, the actual format makes sense, I think among the reasons not to do it like having things contiguous is not an issue for me. I think the only real the only real thing we want to evaluate is the"
  },
  {
    "startTime": "00:22:00",
    "text": "implementation complexity. I've glanced at the source code, but I haven't actually read the details yet to see, you know, how likely it would be, for example, that another implementation would make errors and all kinds of things. I think so I want to take some time to evaluate that, but to me, I think that's really the only thing that is worth checking, otherwise I have no issue with that extension just seems to make sense Yeah, there are a few tricky corner cases so like you are going to want to have good test for this Can you comment a little bit on the things? that are tricky and what happens if you mess them up? up? Yeah, so just stuff like so one of the influence one of the implementation details was, you know, in order to avoid in order to avoid allocating array to save all the extension that you have to repeat, like when it's parsing them, it iterates over the ones that are in the packet that it's already decoded, right? So if you have that iterator go on to the repeat this extension ID, if there's padding right before that, you don't want to repeat the padding, right? Right? You mean you don't want to repeat any padding extension? in the frame, even if there is some of that? correct um so now, if you then try to use your logic to say, you know, am I on the last extension that? I'm repeating to know if I know whether or not to code use an L equals zero flag for a long extension? you have to make sure you're not looking at the padding as the last extension so that was one bug I had an initial limitation"
  },
  {
    "startTime": "00:24:00",
    "text": "So there's little tricky things like that Um there any, like for example, we don't say, right now you've not mentioned what happened if there's more than one RTE in the same frame, like should we? disallow that to reduce? the possibility of problems or like our than one RTE in the same frame? Like, should we disallow that to reduce the possibility of problems or, like, are these two ways of uh i mean like you you you can say in an encoder much not do it but like this decoder still has to handle if it gets a packet that has that you know those bytes in it. So, so correct I just you know say you only repeat extensions after the most recent RT extension that you saw so basically every time you see when it updates the pointer to start repeating from And so it'd be kind of silly to do it multiple times in the same frame, but it doesn't hurt anything And I assume if you have more than one extension of the same type in the same frame, they both get repeated Yeah, you're repeated multiple times times Yeah, I think one thing that would be nice, like at this point, it's given how things are, like it's possible we would end up like not you using RTE in any of the extensions we have right now So I don't know if there's like a way of making sure that you know, anything that gets deployed that is supposed to support it you know, it actually works if it's not used for like let's say the first thing that uses RTE is like five years from now when like"
  },
  {
    "startTime": "00:26:00",
    "text": "we wouldn't want everything to be broken with respect to that Yeah, I mean, that's that is a real concern, but other than writing good tests I don't have a great solution for it So that's just echoing from the chat, Mark Harris had a similar concern about whether or not this is already solving something that people are running in chat, Mark Harris had a similar concern about whether or not this is already solving something that people are running into. So Tim, what was your motivating use case for this? Did you already see that this was useful? for the current extensions? Or you're thinking just for future-proofing? that the overhead when you have many extensions is too much? high? so so the original motivation was you know one of these enhancement extensions that adds side information right? Because like you're you're really competing with just spending more bits on the original opus encoding in terms of, you know, what improves the audio the best right so you want to keep that overhead as minimal as possible. I think the issue is that, at least from our latest experiments the enhancement information is already not paying for itself Because the no side information enhancements are actually performing a bit better than expected feel free to tell me if I'm summarizing that correctly, John Mark I think Yan can confirm also, but that's about what we got so far. Yeah, so that's why Jean- says, you know, this might not wind up being in the initial deployment I think those aren't the only reason you might want to do this. Like, I also think of stuff like dynamic range control metadata you know there are certainly going to be other extensions that that certainly going to be other extensions that, you know, people want to encode on every frame"
  },
  {
    "startTime": "00:28:00",
    "text": "And as jonathan lennox says, we are in the business of making things smaller I'm sensing a little bit of concern over the complexity, maybe would it help people if, uh, uh, of concern over the complexity. Maybe would it help people if, Tim, if you'd be willing to actually write up the spec text? for this so that people can actually try to gauge what the real complexity is from the spec perspective. I know Jean-Marke is trying to maybe look at the code too, to see, you know, in pseudocode, what does this look like? But I think maybe people will have it more informed opinion about is this really complex or is this really rather trivial if they saw the actual spec text instead of just a slide about it see if that really muddies up the spec or if it's pretty straightforward yeah i can do that Okay, so rather than just throwing it into the spec, just please post some suggested spec update text to the list. Yep. And then we can make a call about whether or not we want to go in this direction Seems reasonable to me So, just sort of tie this back into the extension ID numbering proposal from last time, which I did get any feedback in this sort of shows how this would fit into that proposal proposal So even though the original proposal I said we were splitting the extension ID space into short and long extensions We actually kind of have three spaces. One of the sort of structural extensions, you know, the"
  },
  {
    "startTime": "00:30:03",
    "text": "012, which includes the padding and the frame separators and now this extension And then you have the range for short extensions and the range for long extensions So I don't know if people have thought any more about that or had opinions. I think Mo, you had some alternative ideas that you started talking about the mic last time I never saw anything on the list Amia, as in the individual, I was thinking of a different map mapping, and I thought I had actually sent that mapping to the list. Oh, maybe I missed it was thinking of a different mapping, and I thought I had actually sent that mapping to the list. Oh, maybe I understood. Yeah, but I think you're proposed mapping you know works in the equivalent ways some not opposed to it I would just make sure that people understand what the current limitation are, how big the extension space is for the short and the long, and whether or not people think that's adequate. And I thought there were also some other, even that space looks rather big you know, the 8 through 64, I thought there was also some other conditions on that and that you were still subsetting even that space am I miss remembering that I thought there was some some other options within each of those ranges that was also still subspacing those You might be thinking of the length right? So the so the idea for the eight to sixty three range the the length is given by the the last two bits of the extension of ID. Okay, yeah, so okay really is subspaced by a factor four For the first time, it's subspaced by a factor of two for the second one. Yeah, I mean, I sort of"
  },
  {
    "startTime": "00:32:00",
    "text": "expect that for short extensions, you know, most use cases of that are going to be with a figure length So like you really can have that many extensions, but like you everybody wants an extension with two bytes in it, you know, you're correct that there are one quarter of many of those is available Jean-Marc, go ahead I'm noticing this six to six and seven being reserved with length reserved. Does that mean we're not? defining the length for these? and what would happen if we decide to map something there? Yeah, so I think this goes back to that original question I had asked in the first meeting about, like, do we need unsafe extension? um and so the idea here would be that that you see something with one of these extension IDs, just throw the whole packet away, throw the whole extensions away because you don't know how to parse them right? Because if we had wanted to add this, this extension, you know, like, after standardization, like this is a use case where we would have needed that Okay, so it's basically one, well, two reserved code points for unsafe just in case. Just in case yeah. I mean, like you could just some something else with it, like, define a specific length or something like that, but that was just my proposal No, I mean, because I'm not against it I just wanted to make sure we understand what it is"
  },
  {
    "startTime": "00:34:00",
    "text": "Yeah, it might make sense in case there's like another mechanism indeed, because you just found it one Are there any objections to going with this proposal from anyone? Hearing none, Tim, I think you're okay to go ahead and write some text up from this and put it in the next version and if people have better ideas, they'll bring them to the table All right. So any other questions? Oh, thank you. And just remind people from last time that the reason why we kept this draft open was because we wanted to get some more information from extension authors about how they plan to design their extensions and what they need from the extension mechanism. So that's it's largely been dormant for for a while but we wanted to make sure that we actually had extensions looking at it and exercising it before we froze it I think we're getting closer now to freezing it And if you keep it open, I'll keep coming up with more problems to solved Okay, close it now close it now All right, thank you you All right, so apologies for the title. This should read Draft IETF, MLC Kodak, Opus Dred, 01 now Do I have control over this? Yes"
  },
  {
    "startTime": "00:36:02",
    "text": "So quickly changes to the dread draft since the last IETF The draft now clearly explicitly says that the amount of redundancy is unbounded despite the fact that, you know, there's limit to how much is useful There is a should on having one redundancy per packet, and it should be the first frame You now must not have more than one redundant extension for the same frame frame The encoder should not encode leading and trailing silence Any SFU should forward the dread from the last active source and any mix should forward or re-encode dread So these were all discussed in the previous meeting and the previous meeting and the update was made to the dread Dreddraft Dreaddraft Whoops. So now we get to the fun part of how do we actually write the Dred specification and what to specify So this reflects what's in the document. Not all of that has been discussed in the group, so, you know, consider it as a straw man The idea was to have a normative feature decoder but the encoder being provided only as a reference So what that means is we would be defined the computation that the feature decoder the DRED feature decoder needs to do"
  },
  {
    "startTime": "00:38:00",
    "text": "a bit like we described, you know, how the opus decoder is supposed to proceed It would be done not as source code, but as text, as we discussed previously But then we would also have the complete itself based on test vectors for the decoder a bit like, again, like we're with doing with Opus Now, in terms of how this is specified, the idea is for the parameter, the parameter, entropy coding distributions you know, the probability of symbol zero the decay rate and so on those would be in the draft. They are right now in the latest draft, and that takes up about eight pages as like eight big tables After that, the decoder architecture itself would be defined with a mix of text math and student that, the decoder architecture itself would be defined with a mix of text, math, and pseudocode. It's basically, you know, what layers go where with what kind of activation and so on And after that, we're left with all of the decoder weights. Those do not fit the they do not fit a draft document The original opus was not great with a base great with the base 64 this would be way worse so the proposal is to define those in a binary file And my proposal is for that bind file to be the same that we have the same format as we have in the current sort of reference implementation of these things, mostly you know, the format is not so bad and it would basically reduce the probability of making errors, as opposed to having like two different formats formats Two questions would be one, whether we use fixed point or floating point"
  },
  {
    "startTime": "00:40:03",
    "text": "to specify these weights. Right now in the draft, I'm linking with, I'm linking to a binary file that is based on 8 eight-bit weights and it's a bit under one megabyte so going with floating point would be a bit under well, 4 megabytes And then the obvious question of where do we publish? that file that we need to solve need to solve I don't know if people wanted to comment now or after I'm done with the present I had like three more slides I think we should probably come back to this point because I'm pretty sure that there'll be some issues right around how do we do this people probably aren't thinking about it yet, but once they realize the implications they'll want to discuss it So let's go on with the rest of your material and then we'll come back to this one. Okay So this is the Is the rest of the other about how to specify this? Yes, all of it is about how we're going to specify this So this is the proposed by binary weights format. It's based on 64 bit, sorry, 64 byte aligned chunks with you know, a header ID that says this is weights, this is a constant one, a version number zero all the time a type that says whether it's floating point or integer, the size like the number of weights in the array, the block size rounded up to 64, a name for the weights, which is basically a string up to 44 bytes and then blocks of data align to 64 bytes"
  },
  {
    "startTime": "00:42:00",
    "text": "So before you go on, John, Mark, Tim, did you have a question on the previous slide or this one? It was on the previous slide. I just wanted to say that you know, I'm sure John Mark will not be surprised, but I am a fan of fixed point over a floating point Noted One thing to keep in mind is that these are trained as floating points and, you know, things being neural natural they tolerate errors and like the fixed point is just like dumb rounding of the floating point Like I said, also depends on whether we want to perfectly match one or the other or just have like the current, what we have in opus which is just like a margin of error that makes sure that all kinds of different implementations will pass Okay another aspect in the dread decoder is the vocoder. So the first part I discussed is the part that takes the bits and generates acoustic features which means essentially Kepstrom and pitch. And then we have a vocal that takes the Kepstrom and pitch and synthesizes actual audio from there. And the proposal here is to not have a normative vocoder. And the reasons for that is, you know, first thing the vocoder right now is the part that dominates the complexity of the whole process It's possible to switch vocal and we've actually done it a few months ago when we switched from LPCNet to Fargan without having to change anything else and things just work. So I think we want to leave the Flicketer there. And, you know, vocoders are still improving quite a bit, both in quality"
  },
  {
    "startTime": "00:44:00",
    "text": "and reducing in complexity. So I really don't think we should really freeze a vocoder there But now, what? do we somehow specify or what do we say about the vocorder? There's a few options You can probably think of more mixes of those The three main ones would be either define some minimal objective metrics on the vocorder synthesis. So saying based on these features, the vocorder needs to achieve an objective quality of this based on some kind of objective metric we would need to list or define Another one is to see that perform feature analysis on the vocoder output should match the input. So you have a bunch of acoustic features you synthesize with the vocoder, and then you try and go back to these features, and it should kind of match with some kind of leeway or something. This is something I've only recently tested with Fargan and there seems to be some kind of alignment, but it's definitely not that close, despite the fact that it sounds fine And the other option is sort of completely leave it unspecify and just define how the features work and then say, you know, use whatever vocoder matches these features And I know this is wishy-washy I haven't figured out exactly how this would be said exactly So these are kind of the options there And not similar more open questions to discuss I mean, the other stuff is all things we can discuss"
  },
  {
    "startTime": "00:46:00",
    "text": "First, the how do we handle the block sparse weight matrices? Right now, the decoder network is block sparse, so it has all the matrices are based on blocks of eight by four network is block sparse, so it has all the matrices are based on blocks of 8x4 and each 8x4 tile can be all zeros or something non-zero non-zero So some of the options there we can just specify everything using a dense format including so you know all the zeros would be in there. That makes for slightly bigger a slightly bigger file about roughly twice the size like the decoder right now is on average about 50% sparse So we can specify them using a sparse format There's already a sparse format supported with the in the format that I described earlier. Or we could decide to just not use sparse matrices in general and train something dense It would increase the complexity for anyone who's able to support a sparse format There may be like some accelerators or whatever that don't support that format where it would decrease the complexity That's kind of the trade-off. I still think that sparse is useful here because sort of the lowest common denominators is going to be things like ARMCPUs and that sort of thing where we're able to take advantage of this 8x4 block sparse format and those are the ones that will be more on the edge of the complexity as opposed to anything like super fast where you"
  },
  {
    "startTime": "00:48:00",
    "text": "don't care. And it's about 50%. So if you can't take advantage of it, you don't take a huge hit Sorry, Tim, go ahead Yeah, I hope just going to agree with you that you know, if you already have an accelerator, like the complexity is probably not your top issue Yeah, I think we probably wouldn't want to change like a 10x hit, which is why so far I've gone kind of conservative on the sparseness But, you know, I'm happy to take comments there there And yeah, the last thing is do we say anything about anything normative about how the features are meant to be computed? 18 of these 20 features we're austin wright now, they're just kept trying so it's mathematically very easy to describe what the encoder should do with them and what the decoder therefore should do attempting to match On the other hand, the last two especially one of the last two is pitch based on a neural pitch extraction. So that one would be a bit trickier if we attempted to define the exactly So that's it for what I had to present. So I guess, Mo, for example Yeah, one quick question back on the, on the sparse weights. Is this, is this a had to present. So I guess Mo, for example. Yeah, one quick question back on the sparse weights. Is this a result of some aggressive quantization or how? is the reason for the sparse? that you're getting today? getting today? So this is trained and for with a sort of enforced reason for the sparsity that you're getting today? So this is trained and forced sparsity, and the idea is that not all the"
  },
  {
    "startTime": "00:50:00",
    "text": "not all the connections in this neural network are equally important So if you force some of them to be zeros, you can get benefits compared for the same number of non-zero weights you can get better performance with a sparse network So with nbyte you know, an N by N matrix if you make things sparse, you degrade quality a little bit but you don't degrade as much as shrink the N to have the same number of non-zeroing weights. Okay But these are really forced. They don't do just happen out of thin air and training Yes, hi One comment, jan buethe, one comment about the feature computation being normative or not. I think if they are not normative, then there should be some quality tests regarding however you compute the features This may already be necessary because we don't specify the encoder. So if you just get a reference encoder and anyone can have a different encoder, how is it ensured that quality of the decoded signal is? good enough? And I mean, I'm generally skeptical about whether you can easily change the features, especially pitch with freezing, so to say, the decoder So it's the question whether it's worth the effort to do the question of whether it's worth the effort to put such a test in place at least i think one would need such a test Yeah, I mean, I think the question of features is tightly tied with what we say for the vocal"
  },
  {
    "startTime": "00:52:03",
    "text": "For example, if we said you know, this is the vocoder you're going to use and freeze it, then the features would automatically be frozen through that vocoder, like the entire deep code would be frozen, considering we don't do that, then we need to figure out something there. Yes, but I mean, so to say you could have different features for a new vocoder that works well but then um i mean you would distribute this to decoders that might have a different vocoder. So in a certain way, you have to make sure that once you start changing these features, that it works with all possible vocoders being deployed. So I think the same implications are already true for not having a mandatory encoder encoder I mean, maybe the risk is not so big there because it's trained to match a certain set of features but once you also allow start a allowing having different features, then I wouldn't dare predict what happens to an arbitrary vocoder features, then I wouldn't dare predict what happens to an arbitrary bulk order. Yeah, I mean, the combination of the feature definition and the vocal order together needs to be sufficient for interoperability And exactly how we shift that is the sort of vocal question I was trying to discuss here here So Jean-Marc, you have several questions on here And I assume you're looking for input from the working group about them, but do you have an opinion? or a recommendation on any of these issues? You mean the one?"
  },
  {
    "startTime": "00:54:00",
    "text": "that are on that slide? Both this and the question of whether or not the vocoder is normative or not. Yeah So on these here about sparsity uh i think we should keep the sparse we should keep sparse decoders with how we spend them exactly, you know, weights or ends with or without sparsity I don't have a very strong opinion in there. I would tend to lean towards matching what the implementation has, so maybe something close to what is right now in that file, either as float or in, or actually I can do both as well like that that is also easy to have I would tend to go towards something like that mostly again because it means it's less likely we'll have errors because the implementation actually uses that format. So, you know, if the implementation works, then the file is probably good But in terms of flow versus in versus both, I don't have a strong opinion there Okay I think if you have opinions, and a recommendation, I think it would be good to state the recommendation on the list or in the draft and then have people, verify or object to that because I'm sensing there's not enough experts to actually you know, making a well-informed decision about this or to propose their own variants. So I think it'd be better if you recommend something, then people can analyze that recommendation and and dig out whether or not it's reasonable or there's a problem with it"
  },
  {
    "startTime": "00:56:00",
    "text": "it Okay, in that case, I think I might as well, if it's fine, like I might as well, like try and write the draft based on my recommendation and then people can bash the draft and change whatever is there. Yeah, wherever you see some degrees of freedom or some where you had personal deliberation about a design decision you can note that. And then people can look at that and see if they agreed with your decision there So by far, I think the biggest open issue though is whether or not the vocoder is normally specified And I think we need to close on that first Where are you leaning? with that right now? Yeah, so on the vocoder, definitely, I'm definitely leaning towards not specifying the actual algorithm in terms of the requirements from the vocation vocoder I'm still trying to experiment a bit I'm kind of leaning right now in the direction of of you know, having some kind of test sequence where we have existing features and we say the vocoder needs to take these and output something that is within a certain quality and within like certain boundaries so that would indirectly define the features themselves Like we wouldn't necessarily say this is how you come the features, but if you want the vocoder to, behave correctly, you pretty much have to compute the features that way Yeah, so like test conditions or evaluation criteria Yeah. Yeah, that's very common in a lot of other work okay that's a good start"
  },
  {
    "startTime": "00:58:05",
    "text": "Is this all of your questions? So that is basically the main, the, the, the, is basically the main things I wanted to discuss at this point, like I think maybe when we go further in there, it may raise more questions questions jonathan lennox, so going back to the specifying the weights, that kind of feels like what you actually want to do is specify a my mind type for this file because it feels very similar to like a defining it and then you can have like a you know application slash opus thread weight mime type or something like that. Because it just, I mean, maybe it's not if it's only ever going to be one of them but it still feels like a media type. I mean, the are, to be clear, these are never meant to be distributed or go on the wire or anything This is similar to source code. So I don't feel like there's really a need for a media type or anything wire or anything. This is similar to source code. So I don't feel like there's really a need for a media type or any of that. Like this is definitely not a good format in general It's just about distributing something once So a question about, I'm sorry go ahead, Greg So, Jean-Marc, you would expect even that implementation, would not necessarily even use those weights directly, but they might convert them into some, you know matrix format with a different precision or whatever's efficient on that implementation Is that correct? Well, so the format itself is designed to be efficient on the existing implementations we're dealing with, but if someone were to implement that like on some kind of accelerator or the would probably like do things completely different change the order or whatnot. Yeah, so it"
  },
  {
    "startTime": "01:00:00",
    "text": "using the the format you're currently using just guarantees that the file you create definitely were for at least one implementation Exactly. But good. But yeah, someone else might use something different derived from that data. Yes, absolutely or go with floating point or fixed point or both or 16 16-bit or anything, yes Jonathan, did you mean to be in the queue? Okay, so trying to back up to sort of, first principles here before publishing the weights and making them some frozen normative things are you going to should we specify the model and training set that those weights came from? That's also another good question that I did not, I forgot to raise in there So I can say right now that all of the data is publicly available Now, what do we want to do exactly? And the scripts are also publicly available, the exact options that were used Most were default. I don't think the exact details are publicly available I don't know what would be the best practices that you guys would like Ideally, it should be repeatable, but there's a lot of random stuff in the training So I don't know exactly how you guys would like to do it. Probably, there's a way to very in the best case there's at least a way to verify that based on the data and the model I have if you try and train that further, it will stay where it is"
  },
  {
    "startTime": "01:02:00",
    "text": "which means the gradient is minimized. So there's good indication that this is how I train them But it would be hard for someone else to get to the exact point as opposed to just verify it to local minima for that training condition Yeah so if you can document in the draft how someone could reproduce these weights, I think that's, that gives a lot more confidence in the community that this is, you know, this is something that they can reproduce change if they, if they want, you know, iterate on It's, I'm very, I'm very hesitant to publish something as normative no one has any idea how it was arrived at it almost you know smells like a malicious vector Fair enough. Shub this binary in your systems and you're good and you'll have good speech Yeah, I'm not to, speaking as an individual contributor, I wanted to concur with what Mo was saying there i think in in like artificial intelligence right now, there's a, there's a big kind of open debate about what counts as open source. And this is made really complicated by the fact that many, many of these ML things are using terabytes of training data and hundreds of millions of dollars worth of computer time to train them. But that isn't the case for, for this work. That, you know, it is reasonable to be able to distribute the training data. And as I understand it, the amount of training time is really not that considerable has gone into it so i think it would be highly attractive if you know, to the greatest extent that the training process is made repeatable for this. And if there's technical reasons like, you know, you can't predict the rounding behavior of the GPU used to train it and so you won't be bit exact. I don't think that's the end of the world"
  },
  {
    "startTime": "01:04:00",
    "text": "but the closer that we could that we could get to reproducible training on this, I think, would be highly desirable Well, the thing is the initialization is itself is random. So and that, you know, you change the exact version of the toolkit that's going to differ. So I think... Yeah, but that would be easy to fix right? You could just specify the initial random state as like the output of a particular random number generator. Yeah, but what I'm saying is if you try to generate the same thing like two years from now, you won't be able to because of versions, because of everything So my thought there, maybe two ways we could okay there's three ways i can think of of the top of my head where we could go. One would be have some kind I don't know if there's a way to have like an IETF trusted system It's set up with a card like a bunch of people hit the command line, see it train, and everyone agrees that everything is good That would be one Another one is, you know, I can have the recipe out there, train things and then, you know, based on the data, as I said, like, you can't prove that this is how I got to that mark train things, and then, you know, based on the data, as I said, like, you can't prove that this is how I got to that model, but you can continue training and see the gradient is zero on that data in other things things don't go anywhere. You know, this is a local minima for that data. You don't have absolute proof that I did not do something strange but made sure to be in a local minima. Honestly, I don't know how to do that, but I can't prove it's impossible That's another approach and last one most complicated may be feasible is we have like a few different groups that try"
  },
  {
    "startTime": "01:06:00",
    "text": "to get exactly the same versions, the same initiative conditions and try to train exactly the same way. I don't know if we would succeed I mean, I just, I don't understand why you couldn't you know, set up a virtual machine image or a Docker image and then adjust the computation graph so that the initial ran values are just you know they're just their output of the SHA-256 of ML code or something, right? Like, uh so that the initial random values are just, you know, they're just their output of the SHA-256 of ML Kodek or something, right? I'm just saying there's a lot of random stuff and I don't know. May I comment? on that? Sure, go ahead um so i just wanted to say there are long blocks about me making network training repeat and it's really notoriously difficult so you have to track down all the random states and then you will notice that some of these libraries they spawn sub-processes and then they don't get the seed and so on and so on So I think the general verdict is that it not feasible And I mean even who knows you can have a different platform I think this could I call it's compiled as you push it to the GPU. The version changes. You have different rounds of errors that can bring it into a different direction So this degree of accuracy, that's lost I would say so basically, you need a good quality metric to see you trained five different models and you basically get the same result that's that's achievable but I'm pessimistic about this You have a Docker image and no matter where you run it you will get within an L2 distance of epsilon the same way weights out But yeah, I think Yan also makes a good point, like I said you know, you can verify the gradient is zero, but you can also verify that the"
  },
  {
    "startTime": "01:08:00",
    "text": "training loss is at the same level that you would get with an independent model So it gets pretty tough to fudge at this point And I don't know if I may ask also who would be interested in helping or participate? in that effort to make a model that's reasonably verifiable Thanks, great Yeah, we have one hand from Greg. I think I would also be willing. If anybody in a we have a lot of remote participants, if we could just, you know, indicate in the chat whether or not you'd be willing to tackle this repeat repeatable repeatable weights if I may also clarify a few things in terms of what it involves it take all of this the current models have been trained with one 49, NVDA 490 GPU, a single one with a machine with like a hundred and nine GPU, a single one with a machine with like 192 gigabytes of RAM, but it can probably work with much less than that and training data is like 100 gigabytes range maybe a bit more a bit less but like kind of. So I think to Greg's point the short of it is this is something you can do with a beefy workstation. You don't need a GPU cloud you know, or you don't need an entire company to be able to do this type of training. We're not talking about L or you don't need an entire company to to be able to do this type of training we're not talking about LLM level training this is something you can probably do on on high-end gaming rig Yes, this is literally done on the machine I have in my office office Or alternatively, like, do we have some kind of, trusted machine where we can?"
  },
  {
    "startTime": "01:10:00",
    "text": "perform this thing? and or neutral or work? whatever you define that? Oh, the vaguely annoying thing in terms of the training is not everything is perfectly automated Some of the steps involve, like multiple iterative manual command lines where you check that things are converged, and then you stop, and then you restart another one and so on It's not great, but there was no trivial way to automate that It's a few command lines So this is currently not in the current draft, right? The model. Training procedure is not enforced at all. Okay Can we get that document itself? somewhere, if not directly in the spec? somewhere that we can reference so that people that are interested in this? reproduction can try it Yes Is it fine, like right now all of the code? for that is in a Git repo? I can provide the hash and the link link and then maybe like the draft or some document in Git can specify the exact training procedure or something like that. I think at this juncture we're not even concerned about the specification element of it, but just the participation right? That contributors can go try it out and go like ah, I can't reproduce this or whatever And then from there, we could talk about what shows up in the spec you know, once we've, you know, actually got something that could be specified If we all, if everyone tries and we all come to the same conclusion as Jan, that this is just intractable to get"
  },
  {
    "startTime": "01:12:00",
    "text": "to get something with reasonably small deltas, then you know that's good input that's good information, and then we can decide what to do with the spec text Well, I think the spec text is kind of independent of that like it's more about how how we or the spec set, one part of the spec is to say that the weights are not dodgy and the second part is how do we specify these weights? And I think how we specify these weights is an independent question from proving they're not dodgy Does that make sense? yeah it's not just dodginess it's it's you know um opaqueness. You know, we don't want to be delivering standards that no one can understand and no one has any idea, you know, they're just opaque And, and how did you design this? How did you arrive? at this? You know, we don't want a standard like that So if the most optimist scenario, if Yon is wrong and we can get people to actually, you know, arrive at similar, you know, weights with a defined process mostly automated, that would be the ideal case Then we could specify that. And then the weights are just an artifact, and we don't have to actually normatively reference the weights because they're just an artifact of the process Well, two things here. One is that even if we succeed, if we succeed there is absolutely no guarantee that people will be able to succeed a few years from now The second one is there's still the question of, you know, how do you balance, like, you need to use these weights as opposed to you need this behavior, you know, like test vectors and that sort of thing. And then the weights, you know, can be"
  },
  {
    "startTime": "01:14:00",
    "text": "rounded or ultimately I'm not sure I would go there, but like would we even want to say this is the behavior and if you manage to get that behavior? from different weights, it would be good I'm not convinced on that one, but I mean, you essentially argue that if you accept that people may use different precision versions of the weights right? Well yes and no. Like if you, again, you look at the how we specified opus, like, we say that decoder needs to do this this this and this but, you know, you have the fixed point decode and the floating point decoder and they all pass. So there's like, there's a whole continuum between, you know, I don't think we want bit exactness, for example example But going as far as saying you can retrain a totally different weights that would be a another another thing like we would need to have like to be really confident in our test vectors, which especially for ML, I'm not sure I'm that confident Okay, so I think we have two decision points here. First is whether or not people are comfortable with just specifying the way weights as is as a binary blob and opaque and no way to reproduce that's the first decision point are people okay with that and if not what people like to see the process? for being able to generate those and the model architecture and training? data to arrive at those? And then secondarily, how does this actually get specified whether or not we can do reproducible? builds? Do we actually specify the normative weights? or do we specify the procedure for generating them as normative?"
  },
  {
    "startTime": "01:16:00",
    "text": "And then finally, if they are if they are normative weights and it's a binary blob that we're standardizing, how does IETF actually procedurally have something like that? How do we, in a RFC, normatively say this binary is the spec? And you're saying it's far too much to embed in an RFC in any way correct? Yes okay so we've lost Murray We'll have to take it up with the AD when we get there, but let's first... And to be clear, if we standardize the procedure, it would be even worse because instead of being one or four megabytes, it would be gigabytes of data data Sorry, I didn't need to cut you off. Okay So jean-marc valin you provide all the information? that we can start those experiments, is either on the list or in another drafts or appointed? to a GitHub repo or something that has the information for people that are interested to get started. For the training procedure, yes. Most of it is already in Git It's only the exact command lines that are not And I'll try and provide that maybe on the mailing list for now OK Okay. All right, and the chairs, we'll talk with AD about, about how I'll try and provide that maybe on the mailing list for now. Okay. All right, and the chairs will talk with AD about how, in the eventuality of having to specify a bunch of normative weights somewhere, how do we reference that? Oh, yeah, John's not exactly. And I guess the other question, people won't honor auto-evaluate is, is the training data set? chosen well? I mean, is there's like some part of some type of human speech that is been left out or something that this would not? handle well? Yeah So the training set is a whole bunch of"
  },
  {
    "startTime": "01:18:00",
    "text": "CC license data sets that are be found on open SLR sLR.org They represent many different languages I had that in the paper, but if They represent many different languages. I had that in the paper, but it's 30 or 50 languages or something like that most of which are not English So the fact that we've been testing most with English and that it works fine gives us reasonable confidence. They're basically most of what we could find in terms of free training data I'm sorry, Jan, you were in Q earlier. Did you mean to be there? and drop out or did we forget you and slip you? Oh, um Q earlier. Did you mean to be there and drop out, or did we forget you and slip you? Oh, just a small comment I wanted to throw out another idea. So in case you want the whole training process to be transparent, still I would be very happy about being wrong about the reproducibility But in case I'm not, what you can do is you can actually make the I mean, you can basically save all the data and update steps that you do And this way you can at least value a training procedure. It's a ton of data but I think it would be still practical So you would basically it would save every bed a training procedure. It's a ton of data, but I think it would be still practical. So you would basically, it would save every batch that you put into your model. You would save the set of model weights and you would say the update step. And that means that everybody could trace back where did the data come from what was the gradient that was computed you can validate this within the precision of floating point precision or whatever. I think we're talking about hundreds of terabytes"
  },
  {
    "startTime": "01:20:00",
    "text": "here. I'm not sure whether it's hundreds of terabytes, but it's 120 epochs times the dataset size times the model size 10,000 different, yes, but I'm still saying So it's, in theory, it's possible. It's a ton of data, but yeah, I mean, it would be possible just wanted to mention it Yeah, so we'll increase the eye IETF's bill exponentially next year when we published this so Colin also offered to help navigate the IETF process for standardized and referencing something and housing something that's highly available for everyone And everyone, please keep in mind, although we're just specifying you know, this opus redundancy here, I think this is setting a precedent that we want to be careful about how IETF or any standards body standardizes any ML anything is not, it's pretty fertile ground You know, there's not a lot of precedent for how to do this well and effectively. And if we start off on the, you know, on the foot of, oh, I would just specify all these binary weights and we put them in a in a not a lot of precedent for how to do this well and effectively. And if we start off on the, you know, on the foot of, oh, I would just specify all these binary weights and we put them in a, you know, uncontrolled archives that may or may not exist, you know, if someone pays the bills or doesn't, it doesn't seem like the best way forward for other for other, you know, standards. And while this doesn't seem terribly, you know I don't see a lot of threat vectors on this. Imagine if the imagine if this work was you know, to standardize an animal model for you know, for good or bad URLs You know, and when we say, oh, just take this by binary and we'll tell you whether or not it's a good or bad URL Just curious, is there precedent at IET? for, you know, the main thing that comes to mind is this"
  },
  {
    "startTime": "01:22:00",
    "text": "you know, magic value? for some NIST random number generator that was the discovered later to be like you know, not randomly obtained But is there a way at the IETF where, you know, some random value was needed for a standard? and someone like verified or a grant? on a way to determine this value? maybe we can reuse a similar process Colin, go ahead Yeah, just to answer that question, I mean, I think you see this in the security protocol. Somebody may have mentioned earlier on this call of, you know, we take some strings that is hard to manipulate, like the name of the working group or something dash 1.0, like some string that people sort of agree on and then we, you know, feed it through a common like Shaw 1 or some common hash algorithm and we assume that that's something that couldn't have been manipulated. But people get so wrapped up around arguing this, and I just come back to the I mean I think the best, I think that if we can get this to be not too dubious of values and we can expect why they are and some people has a chance of like there was a couple people validated sort of our thinking and way we got to them that we can probably get that through the process and i 100% agree with mo that we're just we're in all new territory here it's going to be you know, really interesting to see what happens. And we're going to be setting what we do here is going to set the precedent for a lot of, for other ML things that come in IETF later so we can expect a lot of pain and suffering on whatever we do Great And a follow comment on the random aspect I'm not aware of anything that's ever truly random people do have some TRNG hardware but it's very rarely used and never ever used for these generic systems and these training systems. These are all pseudo-random generators and you just have to find where the seed is"
  },
  {
    "startTime": "01:24:00",
    "text": "and where the randomness really comes in is timing effects If you have non-deterministic timing effects between you know between parallel processes that's where the true randomness comes in and that is very difficult to get to be fully deterministic like Jan said So I'm kind of on the same page as him that the would be a death knell for us to be fully reproduced but I'm hopeful that that doesn't introduce so much variability that you still can't arrive at very close results Okay that's everything for your Jean-Marc? Yep All right, Jan, you're up Okay all right, I have arrows. Okay, great Yes, I wanted to give an up update on the speech coding enhancements for Opus So that's a progress report Nothing happened on the draft side except for an update of Jean-March affiliation to keep it from expiring expiring I will basically, so basically this presentation has two parts. The first one is I want to resume talking about requirements for speech coding enhancement methods to be permitted in the Opus decoder The second part that relates to questions that were raised at the last IETA meeting, so people asked about real world or out of domain signals like FAF farfield recordings or music or double talk And the best answer I could give was haven't had tested this yet. So now I can give some feedback on that. And then I will talk about next step"
  },
  {
    "startTime": "01:26:00",
    "text": "also I want to point out that there is an attachment containing some audio exam that I sent over the reflector earlier today So let's look at part one, the requirements. So the aim of this draft is to specify requirements for speech coding enhancement methods So far, not much happened on the draft side because most of the effort went into developing these methods first But now Opus 1.5 is released and it includes two options speech enhancement methods that lays a note effort went into developing these methods first. But now Opus 1.5 is released and it includes two optional speech enhancement methods, that's lace and no lace. That means we now have integrated methods for testing these requirements what happened so far is at IETF-118, I presented a study of different object quality metrics that could be used to assess the performance. And one, rather simple method emerged as a problem candidate a modification of the current Opus Compare tool that's used for decoder control and one rather simple method emerged as a promising candidate, a modification of the current opus compare tool that's used for decoder conformance. It correlates reason well with quality and it's simple to explain, standardize and to use There was one better metric that was nomad, but this is a neural network with 60 million parameters. So I thought it would be best to try and go with the modified opus compared um at least for starters So the next question on the agenda is what test data to use? The conventional approach is to use a clean speech data sets and to apply controlled degradation So add noise conditions, simulate different rooms reverb, etc But there are certain drawbacks related to this. The first one is that"
  },
  {
    "startTime": "01:28:00",
    "text": "there is really not much free multilingual high quality data available, so the best data sets they are proprietary And also it's not clear how well these simulated degradations reflect real use cases on my search for test data, I came across an interesting alternative that would be the Mozilla common voice data set. That's a pretty large data set of user-generated clips So the quality it's typically people sitting in front of their laptop or using a headset or whatever, just recording their own voice and uploading it and it's all merged into data sets So that means the quality is on average not great, but it can rate from pretty good to noisy to reverberance far field, near field, everything in there And I would argue that the recording scenario are actually very relevant, because people would record these clips the same way they would probably use their computer for internet communication Also, the data covers 100 plus languages and it's gathered from all over the world And this might address some concerns regarding real world data, so are we testing? on relevant data so what I did is I run a test I downloaded this rather big data set. I did some pre-processing. So testing is done on a per-language basis from every language I selected 10 samples, 5 five female, five male, sampled uniformly from different sources to have enough variability That's done by client ID and languages that are that have not enough samples or all the samples"
  },
  {
    "startTime": "01:30:00",
    "text": "from the same origin that they were excluded so that left in the end 81 languages or dialects. The items were converted to 16 kilohertz, so it's a white-band test and normalized to minus 6 decibel full scale And the script for item selection that on the opus branch, on the opus main branch The test setup is straightforward so test clip goes into the reference encoder It's decoded by reference decoder and then enhanced decoder and original reference decoded and enhanced decoded output are samples are compared to and relative delta distortion values is computed, I will tell you now what that is So for this relative delta distortion that's based on modified opus compare, modifies opus compare is a degradation measure. So it tells you how far you're signal is away from the reference with some perceptual waiting and so what I took for a relative delta distortion is basically to take the difference of the square root of the degradation of the reference decoded signal minus degradation of the test decoded signal and normalized by the degradation of the reference decoded signal. And the idea here is that if you have small degradations, then you want to put a higher weight on deviations than if you have high um distortion to start with For test criteria to pass the test, I picked two different criteria, one is basically on a per item base So maybe one thing to point out. So this"
  },
  {
    "startTime": "01:32:00",
    "text": "relative delta distortion it means if it's large, then test is closer to or if it's larger zero and large than test is closer to the original than reference. And if it's smaller than zero, then it's the other way around so the distortion is getting bigger So the first criteria that limits the worst case relative delta distortion so the minimum distortion is supposed to be large than a threshold. And the second one limits the average relative delta distortion, also supposed to be larger than a threshold the test scripts to run this test is also on the Opus repo, so it should be possible to reproduce it Now, as test conditions, i used as reference encoder and decoder just opus 1.5.2 and the reference decoder has a decoder complexity of zero The next test condition lays is the Opus 1.5.2 decoder with deck complexity set to 6 and no lace with deck complex to 7. Similar to the first study at IETF 118 to simulate some bad models I basically use lace and no lace, but with weight files that are not fully trained So they're just trained for a few steps and there are some examples how it sounds So it's not catastrophic the degradation um but there is an audible audible degradation and quality it's also included in the attachment Okay, results um i picked some thresholds that basically are designed to let lace and no lace pass and let bad lace and bad no lace fail"
  },
  {
    "startTime": "01:34:00",
    "text": "And they are theta min equal to minus 0.1 This means that um we allow or we allow to the modified opus compare value to increase by about 10% or better to square root of that by 10% and for the average it's set to minus 0.0.0.2 so it allows for the distortion increase of 2.5% And this way, basically all good methods pass and the bad ones fail for lays at 6 six kilobits per second, not all tests fail but enough that you would reject that method And I mean, it's looks a bit questionable that you have to allow it increase in distortion. So what I did is I also included the worst case items for no lace, which usually has a larger distortion value assigned than lace. And first personally, I did not find any perceptional degradations to me they looked they sounded slightly better but they are included also in the attachment for others to review Okay. So that's it basically for part one, the test part um now about part two that addresses the question about our of training domain signals So to refresh your memory training data as really clean speech it has been tested on noisy speech already that worked, but it hasn't been tested on music or double talk of half recordings So basically the most interesting thing for you to do is, I think, listen to the samples in the attachment but i want to give so to say a very short"
  },
  {
    "startTime": "01:36:00",
    "text": "overview how these items were generated what the subjective impression is, and all report the modified opposite compare scores So as a music example, I took a show violin clip and it's noteworthy that the fundamental frequency range of the to I took a short violin clip and it's not worthy that the fundamental frequency range of the tones are within the range of human speech And what I observed is first that the item is also classified as speech, which is not surprising but the output of the legacy opus and encoder, decoder, that's not great And if you wonder what the reason for that is, it's that the pitch detector misdetects a lot of pitch values at the encoder the good news is the on the listening impression, both lays and no lays improve on that quality, but still, even with no lace, the quality is not great But a good sign is that it indicates that lays and no lays, they are robust to pitch prediction errors. That's also reflected in the modified opus compare score so recall that better, lower means better and at least lace and no lace receive lower scores than opus For a far field recording I used an artificial item, so I took a clean speech clip from the Ivo Squam CD and I processed this with a real room impulse response. Again, the observation is that both lays and no lays improve on opus and the quality is, on average, a lot better than for the music sample And a third example is double talk. So for double talk, I basically did a stereo arrangement of two clips from"
  },
  {
    "startTime": "01:38:00",
    "text": "the EpoSquam CD, added a little bit of stereo reverb, and did a down mix that basically simulates two people two talkers in one room, and performance is also quite good both opus performance and lace and no lace make it again better. So all in all, that's reassuring, and it's also reflected in the modified opus compare scores However, you can note that usually modified opus compare things that no lace is slightly worse than lace, although it's the other way around So it's not perfect So the conclusion from the experiments are that the test on common voice that successfully distinguished good from bad enhancement methods So admittedly, the number of test conditions is not very large If somebody has an idea how to yeah, how to better ways to simulate a bad model, I would be very interested in hearing it The second thing is that in the current setting, the thresholds must allow small modified opus compare degradation But the listening impression is still that enhanced signals usually still sound better but feedback on that is very welcome So worst case examples are included, as I said Regarding the critical test items it was quite reassured to see that Opus 1.5 enhanced methods also improved that Also please hear for yourself And, yes, I mean modified opus compared in sum is moderately successful in confirming quality improvement of enhancement methods i'd say So the questions that arise now"
  },
  {
    "startTime": "01:40:00",
    "text": "is, I mean, it looks like a viable path for is, I mean, it looks like a viable path forward to say, we take this comment voice, test sets, these 81s, we could add a small clean speech test to that we could add that this is a actually a typo I didn't want to write farfield but the double talk because far field that's in common voice And we could also express some single instrument music clips to add free three test conditions to these 81 from common voice And then we could say we do have a test dataset um yes but I would be very interested in hearing your opinion Do you think test coverage would be good enough with this? Is the method acceptable? If that's the case the next question will be, do we want to have a reference encoder decoder, like in the current set? which would mean that we basically have to provide storage for 120? megabytes for audio? or do we want to provide to provide really test vectors then that would basically increase by the number of test points plus one times 120 for audio because we would have to basically store the decoded out reference decoded output clips on top of the clean ones um there could also be a hybrid way of encoding bits streams but allowing a reference decoder then we would just have these 120 megabytes plus the storage for bit streams, which is smaller And then the big question is, where would we host this? data?"
  },
  {
    "startTime": "01:42:02",
    "text": "No worries. We'll have a multi-terabyte array for Jean-Marx spec So this will just be a drop in the bucket Yeah, training execution trace, 400 terabytes no problem, we just, this is a rounding error So Jonathan, so is this, I guess my bite taking a step back, is this actually standardizing anything? or is this just, you know, things? I use to make Opus better? It's standardizing something. It's basically standardizing. It has a set of requirements that once your enhancement method fulfills this, you can put it into the Opus Decoder and it's conforming to the specification. Okay, so it basically it's what has to be done to an opus decoder so you're still conformant for the purpose of the patent licenses yes there. Yes. Okay, can you further clarify? I thought earlier we had the earlier Tim made a point about um side information versus no side information. Clearly, the ones with side information would need some standard, right? Yeah So, I mean, that was already a spoiler from Tim. There was a first attempt A colleague of mine carried this out to you site information um just to add some information at the end code and add this to no lace And it improved quality a bit, but not enough to pay for it itself so the first attempt failed but that doesn't mean there will be other attempts that will be successful and in that case that would need to be specified, yes Also, if we wanted to do something for instance about this pitch problem that on this violin sample that could also be something that"
  },
  {
    "startTime": "01:44:00",
    "text": "might be worthwhile looking into, so to improve the pitch detector at the encoder not making any promises just saying it might be interesting Jonathan, are you meant to be in Q? Are you? Pleasure, Mark Just to give some context on the difficulty of site information, specifically for the case of speech this is completely different from the other use case we've not worked on so much yet, which is to do it on music. But for speech, what happens, is we can do lower than six kilobits per second. Just because the encoder won't do it And to some extent, like no Nolase has been working so well that at 9 nine kilobits per second, it's very close to transparent. So the range where this would have an impact would be between like 6 and 8 kilobits per second and we have like about one kilobit per second of signaling at the minimum that we would need to do so that leaves like a very narrow range maybe there's something we can do, but I'm getting slightly less optimistic and that that's kind of where it comes from I thought you were sending entire voice signals in one and two kilobits now Why can't you just enhance it? That's the problem, though, because one or two kilobits is what the enhancement signaling takes up. You just send a whole other voice signal So just to clarify jonathan hui question then, will there be anything for us to standardize if you become of the opinion that a non-signaling method works reasonably well?"
  },
  {
    "startTime": "01:46:00",
    "text": "well? Jan, that was a question for you Oh, I mean, a conformance requirement is a standard, right? To the best of my knowledge Okay, so you would like to specify how to measure whether or not you have actually have a good effect? Yes I mean maybe not to matter whether you have a good effect, but just to have a safeguard that you don't put something in that doesn't work or isn't general enough I mean, if people are fine with a lot whatever, that's also a way to go go Jean-Mart? Yeah, I mean, I think there's just like a few things we want to guard again like one obviously is people deploying like anything without validating anything. Another one would be like systems that rely on a modified encoder and interpret things differently and that sort of thing. So it's just a map be like systems that rely on a modified encoder and interpret things differently and that sort of thing. So it's just a matter of setting a boundary of what is okay to do and what is not okay to do Tim Tim, are you trying to talk? Yeah, yeah, sorry I was just going to ask if you thought more about a extension that would turn this off on a frame-by-frame basis Um thought about it some time ago, but I don't see the use case that much, I have to say"
  },
  {
    "startTime": "01:48:03",
    "text": "mainly also because I haven't found a problem with the enhancement methods yet. Yeah that does make it harder to figure out of this use Yes yes so at the at the moment I don't see the use case For that, all right Jean-Marc Yeah, I can see a few issues with turning that on and off. Like one is that unless you specify an extension that says turn it on for now or turn it off for now, which itself is probably not a good idea because you could lose the packet Otherwise, you end up spending quite a few bits signaling that that you know it may not be worth it so I'm not going to it's a good idea unless there's something really compelling that I miss You mentioned the violin sample. Have you done much testing with putting music through it? Is that that strikes? me as a case where you might have more cause to turn it off. I have a question sample. Have you done much testing with putting music through it? That strikes me as a case where you might have more cause to turn it off. I haven't done more. I mean, basically the question is if you have general music then it's less likely that the speech coder would code it So that was the reason for going with a monophonic signal and to see what happens there But I'm very happy to put through the whole squam CD with all the music clips, including Abba and what and shared the results I'll put this on my tiduit list, okay okay Drum art"
  },
  {
    "startTime": "01:50:00",
    "text": "General comment about music like, we're talking about something also for very low bit rates. That's where it has an impact So if you're at like 24 kilobits, lace and no lace and all these, they should really have like very little impact And if you're coding music at six kilobits per second, like it can do whatever. It's going to be bad no matter what so i'm not again not concerned you get what you deserve Okay, so, so, Jan, you're not asking for adoption at this point, and you're just going to continue generating on these two models? Yes, yeah think so. I should have some more drop text before considering adoption That's my feeling Okay OK okay what's the name of the test suite that you were going to run through? The name of the test? When we're referring to this music stuff that was the squam it's the squam CD. It's from the European Broadcasting Union is for signal quality assessment material material um ibu s q a m that's about 50 clips or so with music speech single instrument recordings Okay so thank you, Jan. We'll look forward to your update Thank you So that's the end of our agenda Does anyone else have anything else they'd like to bring up? All right, hearing none, happy Friday, enjoy the rest of the"
  },
  {
    "startTime": "01:52:00",
    "text": "IETF and enjoy your trip home or enjoy Vancouver Thanks, everyone And thank you for the notes, Jonathan Thank you Is it going to be tight for you? No, well, I think I can't make the difference"
  }
]
