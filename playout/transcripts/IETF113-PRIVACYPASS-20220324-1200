[
  {
    "startTime": "00:00:13",
    "text": "all right um welcome to uh privacy pass at uh ietf 13. um this session is being recorded as usual um you're probably by this time all familiar with the notewell um if anybody isn't you can quickly get a refresher here or look it up online as um so for for participants in the room if you want to speak at the mic please add yourself to the queue and also you need to log in to the uh meeting using the barcode or the qr code up there so that we can get you counted on the blue sheets the attendance all right uh some more information about the agenda we don't think we need to go in there so for our agenda today um we do need note takers is there anybody who's willing to uh take notes in the cody md either online or in the room here let's see if i go to i can't quite see if anybody respond in chat okay thank you jonathan all right and then on the agenda today we have um looking at our adopted drafts and going through some of the changes that have been made there um we have a draft on"
  },
  {
    "startTime": "00:02:01",
    "text": "rate limit tokens that we are that we may adopt so we'll have a presentation on that and then if we have time i i don't know if mark is here but um there's some updates to the centralization problem we weren't sure if we were going to have time for that presentation so i don't know if if mark is going to attend or not but if he is we can get get him to give us an update are there any other additions or modifications to the agenda or ben did i leave anything off awesome mark is here all right um we only have one hour so let's see if let's see how fast we can move all right well then let's go and get started with um the uh core drafts chris do you wanna are you gonna share yeah can you stop sharing yeah i can just do this i think that'll do it i need to thank you [Music] oh sorry um okay uh morning everyone uh afternoon everyone i guess um uh joseph this is just gonna be an update on the the two core documents the architecture and the um the authentication scheme documents so for the architecture the the biggest change that went into the last revision was uh sort of an exploration of deployment considerations"
  },
  {
    "startTime": "00:04:00",
    "text": "for uh ways in which you would use privacy perhaps in practice and what the implications are on sort of the privacy posture of the protocol with respect to clients um we'll kind of go through the main highlights of those deployment considerations here and talk about next steps for the draft after that as a reminder um uh the the architecture uh draft since the uh as of the last meeting was sort of updated such that it's now split into um uh like two sub-protocols in which uh there's this this issuance protocol that that clients run uh with an ad tester and issuer for the purposes of acquiring tokens for use and interacting with an origin via the redemption protocol later on in the architecture document describes what the role of the tester is and the azure is in the issuance protocol as well as what the role of the origin is in the challenge and redemption protocol tried to orchestrate things such that the redemption protocol is really simple and lightweight and doesn't really put a lot of burden on the origin conversely the issuance protocol of course sort of encapsulates all the complexity of new token types and the uh i guess relevant privacy properties um of those different issuance protocols um and uh and uh yeah that's pretty much sums it up i guess um the the overall flow of the the the protocol sort of looks like this uh if you recall so um the the origin of the server on the right-hand side um generally would ask the client you know it when it wants to you know wants the client to present a token simply ask the client you know can you"
  },
  {
    "startTime": "00:06:00",
    "text": "attest to some particular property and give me proof of that particular property via a token and then the the client would then go off and interact with the issuer and the tester through the issuance protocol to produce a token and then present it back to the to the origin and the trust relation such that the the server trusts the issuer that is issuing tokens in this particular arrangement and the issuer trusts the tester that is attesting to particular properties on behalf of the client and in the latest version architecture document we've introduced this this notion of context um [Music] so for example during the redemption protocol um there's a redemption context and the redemption context sort of encapsulates all the things that the origin of the server would see about the client during its interaction with the client for the purposes of redeeming a token so that might be the origin name itself because the origin as information about that particular redemption context might be the time stamp of the redemption event um might be the client ip address uh you know whatever information there is about a particular client interaction when redeeming a token with a particular server likewise there's um an attestation context that is present during the uh or sort of encapsulates all of the you know perkline information that the attester sees during the issuance side of the exchange side of the interaction um it's the exact same sort of thing so it's the all the information that the tester sees about the client including timestamp the event all the the client's ip address whatever other relevant metadata or information is you know representative of that particular issuance event if you take a step back and ask yourself you know what is at least in a deployment of privacy pass what is meaningful privacy"
  },
  {
    "startTime": "00:08:00",
    "text": "um because uh you know uh for most uses of privacy pass we're trying to use it for uh purposes of improving client privacy mind you there are applications of privacy password you may not care about privacy but for this particular uh you know scenario uh we are so in the the meaningful privacy we claim in this particular setup is that there's no single entity in the system uh that can link per client information and per server information sort of across these contexts this attestation redemption context and of course the the way in which you deploy privacy pass has an implic has an impact on you know uh who sees what in the interaction um and uh also has an impact on sort of what the client has to do when it's interacting with these different parties in the system in order to achieve this goal so take for example what we call the joint deployment model which is basically how privacy pass is deployed today where the client interacts within a tester and a server [Music] for the purposes of solving captchas as an attestation mechanism and then spending tokens as a as a way to demonstrate that they solve captures at some point in the past in this deployment model the a tester and server are the same entity um uh and so they have uh basically a shared view of the client during both interactions um uh they share the same ip address or they they will they will share um their view of the the client during uh the at the capture solving process as well as the you know presenting the token or the redemption solving process um and uh as a result sort of meaningful privacy in this particular arrangement means that the client has to either separate itself um across interactions over time uh sort of um"
  },
  {
    "startTime": "00:10:00",
    "text": "so that across different redemption events or across different attestation events it appears as sort of a different client or over space such that um and when interacting with the attester or the server it appears as a different client or it's sort of unlinkable with respect to um an attestation and redemption event so uh if you're looking at the time separation aspect of this um this is where things like you know um uh our unlinkable tokens you know sort of the basis of the the privacy passport will come into play um a you you can't we we use like unlikable tokens that are issued through like blind signature protocols or opr protocols or whatever such that the a tester and server can't link a particular issuance flows to a particular redemption flow or go they're sort of separated in time on the space separation side um a client would sort of separate itself in space by using for example a proxy and connecting to the attester or connecting to the server indeed in the initial motivating use case for privacy password um tor users would go uh and interact with the um uh with the you know users with users of tor would be you know being faced with captures over and over again repeatedly um being forced to um uh present tokens uh they would they were already sort of separating themselves over space um but you can use other proxies if you know reasonable for your particular deployment model uh yeah ben go ahead hey you said time or space here um i guess i would have expected time and"
  },
  {
    "startTime": "00:12:00",
    "text": "space because uh whether i see a consistent ip address over a long period of time or i don't know your ip address but i uh but you're the only client who's who's uh requesting and using tokens in some short period of time it seems like either one of those is uh well i think um sort of implicit here is that you're more than one client interacting with the system um right otherwise like that none of this really holds that there's no really privacy to be gained so certainly if there's only one client that's asking for tokens and spending them doing any sort of separation over time or over space doesn't really help so right i mean that if you don't have separation over time then you're you're sort of necessarily the only client that's because in any sufficiently short time window there's only one client sure and that's why in this particular model non-interactive tokens sort of like are the most sensible variant in particular because uh spending a token um or redeeming a token does not mean that you like went off and fetched in real time it just means that at some point in the past you have fetched a token so um uh there's like a sort of a natural separation between the two over over time um whereas if this if uh like all of these [Music] if if if there were interactive tokens rather um uh then there would be sort of no um time separation and you could link attestation and um uh and redemption together uh presumably by like time stamper or what have you right it seems like you could you know in that context you can do that linkage even if you have space separation so it's not that you need this is why i'm saying it's not that you need one or the"
  },
  {
    "startTime": "00:14:00",
    "text": "other it seems like you need both anyway i won't help so recall the the the meaningful privacy here is that you're not being you're not able to link per client per server information um so although you're able to like link these two events together presumably by the time stamp um you're not linking uh uh you're not sort of revealing any uh per client information by virtue of using um a sort of uh a proxy and interacting with the cert for the tester server in that particular case so yeah um okay moving on um uh there's also um there's another another other deployments in the in the draft as well uh deployment models in the draft as well another one uh we talked about is this split deployment model um and this is this is uh useful for different attestation mechanisms that are less privacy friendly um like say for example the client is demonstrating that has you know ownership over some specific type of application account and it's like specifically logging in with that account um in this particular model by joint we mean that the sort of the tester and the server are run by two different non-colluding entities and as a result they don't share the same context with respect to attestation and redemption and so the the sort of bar to meaningful privacy is kind of lowered in a sense and just means that the attestation in this particular case can't reveal any sort of per server information um because the attester doesn't have uh the sort of the shared view of the uh the redemption side that otherwise would in the joint deployment model and likewise that redemption doesn't reveal any per client information"
  },
  {
    "startTime": "00:16:00",
    "text": "so the excuse me the interaction between the client and the server during redemption just needs to sort of make sure that there's no client information revealed for example using a proxy to interact with it so that you know it's a particular unique ip address or whatever stays hidden and then during attestation you just need to make sure that whatever issuance protocol you're using doesn't leak inadvertently the the origin name or anything else in particular about the redemption context to the attester and uh this sort of has implications on you know how the asians protocol works in particular the insurance protocol should not reveal um any things that are you know particular to the to the origin um uh during issuance but um thankfully all the issuance protocols like the the blind signatures and the oprs and whatnot um naturally sort of hide this information by virtue of being blind signatures or oblivious suit around functions so yeah i think next steps there's there's an open issue right now um for sort of addressing the double spend requirements uh when you're using uh cross-origin tokens um across different uh or double spend prevention requirements excuse me when you're using cross-origin tokens which means that like say for example uh you have two origins that both accept cross-origin tokens they both have to sort of share double spend prevention state otherwise um a client could spend um a uh token at either one of these particular servers um and i don't think we explicitly sort of make that obvious uh in the draft right now"
  },
  {
    "startTime": "00:18:01",
    "text": "um so there's just an issue to sort of call that out there is uh also some existing sort of privacy parameterization in the draft which sort of describes you know if this is the sort of like size of the anonymity set that you want for particular clients here's how you should arrange your issuers and arrange your testers and whatnot but it's still sort of [Music] kind of highly dependent on the previous incarnation of the architecture draft so we just need to kind of go through and update that um and then there's there's been a sort of a long-standing issue to address centralization although with mark's draft we may be able to or may consider just simply punting discussion there or conversely folding his his some of his text into this architecture document and closing out the issue i think at that point um we'll have sort of discussed and covered all of the different architectural uh properties of the system um uh that are you know relevant to uh how you would deploy how you use it and and what the the resulting privacy posture is or uh for clients i think we could either park it or uh move it into working group class call and focus our efforts elsewhere on issuance protocols um but that's pretty much it um anyone have any questions before i hand it over to tommy to talk about the architecture or the authentication scheme if not uh tommy jeremy stop sharing so you can pull it up or tell me just advance for you i can just say next slide it's it's fine if you don't mind yep that's fine okay all right next slide great um so i'm going to talk about the authentication scheme this is the document that we discussed at the interim meeting it is newly"
  },
  {
    "startTime": "00:20:01",
    "text": "adopted and we did actually publish a zero one version uh just this week with a couple of the changes that i will talk about today now these are mainly minor terminology changes to some of the fields in the struct that should hopefully make it clearer and the other thing we wanted to cover today was the fact that we want to be able to stabilize this challenge and response format to make sure that people doing deployment experiments and interop experiments have something that we are comfortable with next slide okay so for the changes in the zero one document um the main thing was renaming some of the terminology there was a field in the challenge structure before which was a redemption nonce um oh ben in the chat is saying that the audio is not very clear is that true you are breaking up occasionally okay i'm trying to mute on me all right um i i will just try as best i can i'll speak a little bit more slowly so the redemption nonce is renamed to the redemption context generally this is because this field was not necessarily a nonce it's really just some servers chosen context that they want a token bound to and i want to point out that the fact that you have this redemption context in a challenge doesn't actually make the token issuance"
  },
  {
    "startTime": "00:22:00",
    "text": "interactive as in saying that the client needs to fetch a token immediately it's just saying that this token is bound to something that the server knows for purposes of double spend prevention and it's something that isn't exposed to the issuance protocol so it's really just between the client and the origin that's doing the redeeming to make sure that you're not spending a token that someone else got and then one other minor rename is that there was another confusing context name in the actual token struct that was given back to the origin upon redemption and this was renamed to challenge digest because it is indeed a hash digest of the challenge next slide all right um then the other thing we want to do is talk about stabilizing the format of this challenge and response we have several implementations um that have been testing with interop and to encourage the deployment um of these experimentation between these the authors would like to essentially hear any issues with that format now so that um we don't have to worry about changing it too much later next slide so just to review what the current status is of these structures in the challenge so this is all using http authentication with the scheme as private token it has a challenge and a key that are passed there and then the challenge structure has the token type which defines which issuance protocol we would use it has the issuer name which tells you who who is allowed to actually get give you the tokens on the other side it has the newly renamed redemption"
  },
  {
    "startTime": "00:24:01",
    "text": "context which is optional and is essentially just some random server chosen context that they want to bind this token to and then also an origin name to scope this token to a particular origin upon redemption and that's it for the challenge and the next slide is the redemption side it's the same scheme for actually sending this authorization the token type is there again there is now a client generated nonce which is a true nonce that um is is used to make sure that this is a unique token as part of the issuance protocol there's a hash of the challenge structure there is the token key id used and then there are all the bytes that are specific to the issuance protocol this is based on the rsa blind signature the oprf etc um i see a question from ted in the chat uh ted would you mind maybe asking the question at the mic uh ted hardy speaking if you wouldn't mind going back a sled i was i was typing slowly so it's actually about this uh for the opaque origin name which is optional uh if you wanted to have something that covered both youtube and you know google search at the moment you could leave this out and the redemption context would handle it you could present it and neither one could could do it but i was wondering whether it be another option to consider and maybe it's not needed is to allow opaque origin name to to"
  },
  {
    "startTime": "00:26:01",
    "text": "have more than one appearance in the struct so that you could specify a a list of origin names that are covered for when you have cases like that where the redemption mechanics in the back end are likely to be the same um but the origin names are not so i'm not sure this is worth doing that's why i just put it in the chat to kind of as a side comment but that was the question got it um yeah that's a great point of an interesting feature that this could have as you point out it could of course just use cross origin tokens but then depending on what your issuer is this that could be a much broader pool of origins that it would be shared with and not just google and youtube the other approach i imagine could be taken is that the client could know essentially when it is safe to do cross-origin for a single origin similar to how i can do connection uh like http 2 connection reuse if the uh if two origins are covered in the same certificate of the server that i'm talking to potentially so um i think i i think you're probably right there that the there could be something else it uses to know whether it's safe to do the cross-origin um but the the mechanics of this in the back ends of some of these are going to be a little bit wonky because in some of these cases the same redemption mechanics are going to be used for something like gcp so you could have instances um that are being issued by google where the redemption mechanics definitely do not want you to use the same redemption context as the google properties themselves but"
  },
  {
    "startTime": "00:28:00",
    "text": "they might be the same redemption mechanics in the back end for um uh origins that are just you know living in their cloud so if if you if we're not going to use multiple origin names then i think one of two things will happen either cross-origin is going to be very very common to to major services because they many of them have more than one name from the point of view of http origin or you're going to have to have some other system to kind of figure out oh okay what i'm going to actually use to figure out whether it's cross urgent safe is my last contact for them what all of the subject names were in the certificate or something like that um so it there's some trade-offs here and a simpler trade-off might actually just be to say opaque origin name can and can occur multiple times because then if somebody wants to um to scope it to a specific set they don't have to rely on either previous um contact by the the client or um maybe a try and fail with cross origin that was to the same um set of servers but to different um actual redemption context so just just a thought got it yeah no that's that's good we we will take that as a uh as an issue to look at stephen i think like even beyond like wanting multiple origins like the definition of an origin in privacy past is slightly different from like what an origin might be on the web like it might be a site or it might be an origin so i think ben mentioned the like ideas leave origin to be an actual opaque blob that depending on the use case like it might be interpreted as like a literal origin or it might be like a set of things or a site and leave that up to the use case rather than trying to put it in the stock"
  },
  {
    "startTime": "00:30:06",
    "text": "all right thank you okay let's go forward a little bit all right um and yes that was steven so the origin behavior essentially the server behavior of what to do in this is if you want to challenge it should be very very simple you essentially just need to choose who your issuer is one or more of them and what token type you want to use so what issuance protocol you can choose to be per origin or cross-origin and to the discussion we were just having maybe you know there is a in between you essentially you need to choose what is that value of kind of what you are binding your challenge to and then you can choose the optional context that you want to have so for this the context really is about the state that you need to keep for double spend prevention if you don't have any context then the state for enforcing double spend prevention is not really tied to what's in the challenge but you need to have some broader state that you keep upon redemption to recognize identical tokens coming back to you and that is certainly doable but it can require more work on the origin the benefit of having the context-based token is that they can tie specific client session properties or other properties that they want to the token to make it a smaller set of"
  },
  {
    "startTime": "00:32:02",
    "text": "things amongst which you need to do double spend prevention i think the next slide has some examples that chris gave so of course you can have an empty context but you could have some very simple mechanics like uh doing a hash of the client ip address or the client ip address subnet so that you can just compare these tokens to other clients that fall within that subnet or you could have something associated with your state with the client so if the client has a long-lived connection with you that's doing http 2 or http 3 maybe you have some state associated with that that you just keep for the lifetime of that session and then you can just guarantee that this token is only valid for the lifetime of that session all right and then on the client mainly what the interesting things that a client needs to do is manage what tokens are cached and how they are reused and recognizing when which which tokens are eligible to be reused and what cash you need to pull from when you don't have a context then it's very easy to cash it if they are context-based then you um have generally shorter caching lifetimes and we had a recent issue from something like chris pointed out that you want to probably clear any cached tokens whenever you clear your cookie state or something else that would otherwise be changing the client state and then the other thing that you want"
  },
  {
    "startTime": "00:34:00",
    "text": "to do on the client side is to verify that origin name information to make sure that it matches and this again to the point that ted brought up is where if this is expanded to include multiple things it's some verification that what the challenge was bound to actually represents what you um the this the state you think you have with the server to make sure that you're not going to give a token to the wrong server all right next slide all right so based on this discussion it sounds like we have you know one issue where we want to kind of dive in a bit more to this origin name if other people see other changes that to the formats that would be useful that would be great to hear now um and other than that we plan to continue polishing the document and doing interrupt testing if people are interested in testing with this let us know any other questions okay any other feedback on either of these two core drafts all right all right chris are you presenting the next draft as well uh oh i'll share slides um tommy and i are gonna kind of do it together"
  },
  {
    "startTime": "00:36:01",
    "text": "yeah we'll go back and forth all right so next we'll talk about a uh secondary issuance protocol so the main core issuance document talks about blind rsa blind signatures and oprf and these are just very very basic usages of those protocols but we have another issuance protocol that has been defined specifically to allow per origin rate limiting next slide please so rate limiting is a very very common part of fraud prevention and anonymous access across web and in apps and often it's something that does rely on tracking cookies or client ip address and so it's not a great thing for user privacy and just first to give some background on how this is commonly done um with something called token buckets uh chris is going to walk through some examples yeah um so uh token buckets or leaky buckets um you know choose whichever one you like uh using toca buckets here so i think it maps better to the analogy of uh uh the sort of the use case here um so like for as i guess a reminder or refresher for people who may not know um a token bucket is a sort of a just a process or an algorithm for sort of enforcing uh rate limits um uh and you can think of it as a like a composition of two independent processes um one of which is a process for sort of replenishing tokens in this bucket um sort of like adds a new token to the bucket at a fixed rate"
  },
  {
    "startTime": "00:38:01",
    "text": "um and the bucket has a particular size or a capacity after which if it's full you can't add any more tokens if it's empty you can add the tokens and then you have another process that removes tokens from this bucket and this uh this process that is uh removing tokens is typically um representative of something that wants to like access a resource send a pack out on the network or send an api call or do whatever basically um and the the the token bucket just uh the internal check is basically you know are there tokens available to service this particular request um if the answer is yes as in uh that the number of tokens in the bucket is not empty um the the request is serviced um if it's not it's dropped on the floor um leaky buckets um are sort of the mirror image of this and also commonly used to implement rate limiting but as i said um this is i think a simpler mental model internally if you're to sort of open this up um uh take a look at the sort of token replenishing process first um when a token bucket is replenished the very first obvious thing is that the the bucket that is being replenished has to be identified um so in this particular case you can think of it like you know there's a hash table inside and the hash table has a particular index and the value associated with that particular index maintains the count so in this particular slide here the the bucket with the index one two five blah blah blah whatever um is incremented with uh t tokens are replenished with t tokens um uh and uh previously where it had n now has n plus t pretty straightforward on the redemption or the sorry the resource request side um similarly the bucket uh has to be identified um and uh"
  },
  {
    "startTime": "00:40:03",
    "text": "so that involves going into the hash table um and then depending on how many uh tokens your particular request corresponds to maybe it's like you know if it's a packet size and bytes uh there needs to be n tokens or whatever um here we're just saying that each request counts as one token um the the the algorithm uh identifies the bucket decrements the the count by one um and if it's uh greater than zero surfaces if it's not it just and drops it on the floor um and uh that's basically it uh the it's pretty straightforward the the you know you have to identify bucket and either you increment increment tokens or decrement tokens um uh and uh act accordingly or service requests accordingly back to you tommy all right thank you so that's how these schemes normally work um so why is this interesting for privacy pass so specifically using these rate limiting schemes which are used for both fraud prevention as well as things like metered paywalls really break down when the clients have more privacy because they end up sharing the rate limiting buckets this is because of tor or proxies or vpns or just being on a shared ip on a public network and a basic privacy pass token is useful for the cases where i'm just going to get really gratuitous captchas um but it's not always enough um both for some functional use cases like the metered paywall but even for some of the just the captcha"
  },
  {
    "startTime": "00:42:00",
    "text": "prevention cases so a basic privacy pass token says that a device or a user passed some check but it doesn't stop that device from overwhelming um what the service would allow if that service wants to have a pretty low rate limit so i could have a bunch of legitimate devices that are being used as a click farm or captcha farm or i'm just trying to get around something like a metered paywall and so we have a concern that in many of these cases we're still going to degenerate to people being blocked even if they're using basic privacy pass next slide so on the left we have our basic tokens and these are about attesting to a user device legitimacy that you could do a captcha etc and it's good for replacing the captcha to improve the confidence around a specific user or device and then the rate limited token variant does that but it also is attesting that your access rate for this origin was below a certain threshold and this um adds mitigations doesn't completely solve but it adds a lot of mitigations against devices being used as a click farmer captcha farm and it also allows you to work with things like metered paywalls even without giving away user privacy next slide so the the rate limited token issuance protocol is extending the basic issuance protocol that's used"
  },
  {
    "startTime": "00:44:01",
    "text": "for rsa blind signatures it shares most of the same structure and it essentially adds a bit at the end that the attester and issuer boxes will use and so this does rely on the attester and issuer being separate which is not a requirement for basic privacy passes and in this model the tester which is the thing that sees the client identity maintains a counter for how many times a given client has accessed an anonymized identifier for that origin and so these attesters are trying to learn a stable mapping between the client and the origin uh based on a per client see it's anonymized using a per client secret and a per origin secret so the tester cannot learn what you're going to just that it is a unique thing within a given time window the issuer is responsible for providing what the rate limit is and telling the tester during the issuance protocol by the way for this for the thing that this client is accessing they should only be allowed to access it uh either you know let's say three times per hour or five times per month you know it could be a fairly wide range of rate limits and the tester is the one responsible for failing the request if that rate limit is exceeded next slide um yeah and so the the main interesting part here is how you do this without revealing to the tester what the uh per origin information is next slide and i think it's back to chris now"
  },
  {
    "startTime": "00:46:02",
    "text": "yep yeah thanks tommy um right so as time you said the the main challenge in the protocol is to basically compute the stable mapping because we want to do it in such a way that the the sort of buck identifier is private to the attester in particular private in the sense that it the tester doesn't learn the specific origin for which the bucket corresponds to um but it does have assurances that it is sort of the the same origin for a given client over time uh otherwise it's it's difficult to reason about or difficult to say that this is meaningful rate limiting on a per client and per origin basis um uh so to do so sort of the the way the rate limited issuance protocol works is a part of it um is that it computes this this the stable mapping which is um you can think of it's just a deterministic function between this client secret and this origin secret where the client has and maintains the client secret and the issuer has and maintains the origin secret a tester has access to neither and therefore can't compute the mapping and uh this mapping is basically uh you can use it as an index into whatever other data structure you'd like to use for enforcing rate limits um so if you think back on the uh the token bucket example you would use it as the for example the hash table index that would use to store and associate your accounts uh or your token accounts for that particular uh or client per origin bucket um and so the flow would be the tester computes the secret arc computes his mapping looks up in this table and then basically applies the rate limiting policy and if you were to map this to sort of the flow the issuance flow between the client and tester and the issuer sort of hand waving massively over sort of the underlying details in terms of how this stable mapping is computed"
  },
  {
    "startTime": "00:48:01",
    "text": "reserve those details to the document basically the the the tester's job is to uh in in interacting uh in completing this protocol between the client and the issuer is to compute the stable mapping decrement account um or rather that should say increment count you'll find that the the slide is hilariously wrong in terms of like the the algorithm but like hopefully the idea is clear um basically compute the mapping apply the the the algorithm um and either eject accept the request um and and forward the token response back down to the client um or drop down the floor sending the client down an appropriate error code um and this uh you know the the close reader will understand why this is wrong uh in terms of like what the what the check is actually doing um but i think you should get the idea um so the i think that the the novelty is that we build on the this new scheme that we're presenting at cfrg next uh for uh computing these stable mappings um uh in particular uh this uh what we call a signature scheme with key blinding it looks something like an opr but it's not quite an opr if you're interested in sort of learning more i encourage you to take pop into the next session and check it out as tommy said it does require a split deployment for meaningful privacy in particular because the issuer does need to learn the origin name in order to associate and use sort of the right origin secret for computing the stable mapping and as a result uh the attester um who cannot learn sort of per origin information uh thinking back on what we think of as meaningful privacy uh needs to be a separate entity uh there are uh today a couple different"
  },
  {
    "startTime": "00:50:00",
    "text": "operable implementations of this particular variant um uh using the the signature scheme with ecdsa um previous incarnations of this draft used eddsa for the crypto um but we've since changed that we could bring it back uh i guess we're not particular in the type of crypto here um and there is a security analysis underway for sort of the both the new underlying cryptographic scheme as well as sort of how it plugs into the the larger rate limited issuance protocol and what the resulting privacy properties are so at this point um as i said without going into sort of the crypto details i think those are uh perhaps more useful for offline discussion or um you know cfrg in particular um we want to know uh if the working group is interested in adopting this draft and i will stop here and turn it over to joe and john or joe and ben sorry okay ben hi uh this is just as individual i want to understand a little bit more about the um about the proposal and use case so let me um let me ask why can't we um why can't we do this with the standard or previous privacy pass construction so for example let's talk about the the rate limiting paywall case if i'm a magazine i could act as both issuer and uh and and validator and users could make an account with my um with my origin and then i would say you know you have a free account so i'm going to issue you uh three tokens a month and then i'm going to execute a redemption on uh you know to to do a token spend"
  },
  {
    "startTime": "00:52:00",
    "text": "event every time you attempt to view an article and so you'll run out of tokens then when you try to get more tokens that you know i can i can trigger the rate limit so um you know why why isn't that the architecture yeah that's fine um so you mean yes you're right that if i'm willing to create an account with the origin and you could definitely use it for that case this model is more for when you haven't created an account because if i if i can create an account with you know the new york times i can already read my articles and that's not an issue i think the more interesting case here is talking about the cases where you're trying to rate limit for the fraud prevention case let's say the actual account creation time right so if what i'm trying to do is prevent you know device farms from creating a bunch of abusive accounts that's a case where you know today they may use captcha plus rate limiting on ip addresses etc and the basic token issuance there isn't always going to be enough because they that will still not give you confidence that these aren't abusive devices so it's it is it's for specifically for the anonymous case yes sorry i wasn't clear when i said an account here uh i didn't mean uh i didn't mean an account in the uh that is in the origin um i mean uh"
  },
  {
    "startTime": "00:54:00",
    "text": "i mean an account that is uh that is not linkable to the origins but it is in a the account is in a separate origin uh but it is uh but it is providing the tokens it is acting as as issuer in in standard yeah okay that's essentially what this is then um but so in that case if i have an account that says oh you get to read articles or you get to do this um that thing if it wants to rate limit to you to say i will give you only three right um it kind of needs to know what what those are for like unless you're setting up a new one of those accounts for every service so like do you think i have like a new one-to-one mapping of like some other rate limiting service right but i i think the issue then is at that point it it sounds like it would be fairly easy to know when i signed up for that service like oh this is the service to give me tokens for the new york times and so that thing does know when i am getting those tommy you're saying that the the thing that has the account the free account as ben described it would then basically learn the browsing history of the client i guess i guess you could get the tokens earlier still but but still like if it's enforcing you're having this per origin state somewhere like essentially someone is needing to keep track of these are all of the sites you want to"
  },
  {
    "startTime": "00:56:00",
    "text": "sign up for accounts on or all the sites that you want to be able to read articles on and partly what i'm looking for here is a way to take that state and shard it out and push it into an entity that is accountable and funded by the um by the origin as opposed to um having it all essentially centralized in a single attester which has to maintain this like potentially unbounded amount of state for like all of the different origins that the user is potentially active on okay um so okay um i mean stepping back to the question at hand uh i'll just say i um you know i my my feeling as individual here is is that um i wish i i could see a little i wish i were a little more confident that uh that this is the simplest solution in the in the broader architectural context uh tommy are you in the key okay so jonna yeah that was just to respond just wanted to quickly respond to ben's comment then i i i think i would 100 agree with you uh that if there was something simpler we should absolutely do the simpler thing and i would encourage folks to think about how this could be made simpler and share that either on the list or in github yeah i i just wanted to go on that that oh john are you done i'm sorry"
  },
  {
    "startTime": "00:58:02",
    "text": "yes i assume you're done thank you um um the the key challenge here um was in that in trying to uh build in a mechanism for uh sort of keeping the state um uh we did not want the the thing maintaining the state to be able to effectively reconstruct um any any like browsing information or any sort of pre-origin information um about the the the thing that it's enforcing rate limits for um uh indeed like earlier designs uh were a lot simpler um uh and but they enabled the attester um i don't know if it was called the adjuster at that particular point but they enabled the attester um to learn that information um and in in trying to address that uh this was the this was not the first solution that we came up with but is it i think it is the one that has um uh the desired functional properties as well as the desired privacy properties um and and just to heavily plus one to what jonah said if there is a simpler solution we would love to see it um but the the the the core challenge i think remains the same um so okay i think we're just about out of time so i think we'll have to continue this this discussion on the list and and maybe we'll try to get an interim before the next ietf to discuss this and and some of the consolidation issues and other topics all right thank you very much and for folks in the room if you didn't check in i think you might still be able to check in uh i want to make sure everybody we count everybody"
  },
  {
    "startTime": "01:00:01",
    "text": "thank you these masks increase the concentration of carbon dioxide makes it a little bit a challenge oh thank you very much those in the room uh thank you"
  },
  {
    "startTime": "01:02:24",
    "text": "you"
  }
]
