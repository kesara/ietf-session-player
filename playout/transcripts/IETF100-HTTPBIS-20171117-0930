[
  {
    "startTime": "00:00:04",
    "text": "or importantly and I can do a pronunciation test just the Declaration of become Bluestein English these these folks are here to help us keep a professional atmosphere and if you feel that is not going on in this working group weren\u0027t any work need group to attend I encourage you to bring those issues to your chairs and if that\u0027s not appropriate this all right administrivia blue sheets marx started those around yes people come in now please make sure you sign them because with the attendance we have today your vote will count scribes we have a scribe be in sweat we have a jabber relay Barbara stark and then we\u0027re gonna move on to the agenda and any bashing anyone may want to do how much for Monday session this is fundamentally we\u0027re going to just work through our active drafts find what work we have in front of the working group I hope you can move a couple of these items to last call and then after that we have one item in time committing earning http/2 and first before we move on to the extra graphs we can take a moment to recognize the things that have changed instead of dr. Johnson some last we met so we\u0027ve adopted BCP 56 this but you won\u0027t find in data tracker yet because we actually finished that adoption while taken I could have closed been all too busy to actually do the paperwork this week but we\u0027re gonna treat that as active and origin frame has been sent to the iesg and they\u0027ve begun their IETF last call on that topic the first message from our area director on that one has appeared on the mailing list this morning so that\u0027s good and I believe something else well it\u0027ll come literally so that\u0027s good one in two out that\u0027s you know kind of the right direction of eventually getting those towards a reasonable to the documents did you come okay so with that being said the first thing though does anyone would anyone like you ash the agenda it\u0027s always a big billion moment me I can\u0027t when someone steps up to the microphone and says throw that all out this is what I want to talk about all right that being said we\u0027re going to start with one expect CT so mark you\u0027ve been talking ball with Emily on this do you want to Emily wasn\u0027t available to to present her in Singapore but she did provide us sort of a readout on where that is we\u0027d like to maybe March and go through the summary that Emily sent and then we\u0027ll sort of open the mic or any comments people have on let\u0027s work you know the relevance of its work given the "
  },
  {
    "startTime": "00:03:04",
    "text": "timelines involved in the changes in the in the CH so this is the summary that Emily sent us I won\u0027t read it to you I don\u0027t think I have too much to add really I think that the state were in with expect CT is we\u0027re gathering just a little bit of implementation experience Emily said that she felt it would be best to talk about it for a while and allow folks to make sure that it made sense as and specified and I think we\u0027re near the end of that place we\u0027re probably going to go to welcome to Brussels fairly soon I\u0027d like to hear any comments without much trial I think I mean I do have maybe one comment you know we anticipate changes in 2018 amongst you know comes other experiments what they feel the relevance is heard an election decide will be and what another to deploying after excuse me how long will have to live without their efforts the committee discussion so we actually jumped ahead because Emily\u0027s comments were on the chairs flies and somebody jumped right into them because they were next but the agenda actually had us talking about I\u0027m early data in HTTP which which I think we should do so Martin from on to say a few words of where we are and where we would ya like to be yes Mountain Thompson I think this is done we\u0027ve had a great a lot of input from the working group of rare to see this sort of document have input from I guess 10 people at least in very short amount of time and we\u0027ve got now I think three implementations that I\u0027m aware of and intent to implement by a couple more so all signs are that it works and we\u0027re about to see TLS one three go out so this is a finally time to turn the crank and spin out some sausage air patrol so I read this in a plane over I have a few minor comments that I think aren\u0027t quite right but I think it\u0027s basically ready and the picadillo will be a boss call activated Tony probably I will um yeah I think the document looks good I\u0027m just having read through it there\u0027s one thing that\u0027s conspicuously like not mentioned is other things that do really data that other than tea last 1/3 of like you could do this over just TFO do we want even just a mention of that to "
  },
  {
    "startTime": "00:06:05",
    "text": "acknowledge that that exists and there it just seemed a little bit conspicuous I mean everyone should be doing TLS but so I think that the primary focus of this is on the attacks against the fearless belly dancer if hope does send things early up but that doesn\u0027t really change the security posture at all so I don\u0027t think it\u0027s a particularly ok but yeah that\u0027s fine a in case other people reading this have the same thought though it could be interesting just to mention that there are other verticals and that they do not have this problem so when we adopted this document we actually set a time line saying we would like to open the last call during the Singapore meeting so Singapore meetings like open the last call we\u0027ll keep that up on the list or you know a couple weeks kind of thing see if any more of them any more ideas trickle in but with that being said I mean I think they\u0027re great you\u0027ve been great experience the other no well okay short tight document I got input from a wide variety of people we\u0027ve met our timelines on it and we\u0027re gonna get it and either thoughts on early data for anything we\u0027re okay with 425 response with that being said next on the docket is random access in live on which I think we\u0027re gonna try and do remotely Craig we don\u0027t have remote presentations so Craig just tries to speak we can it\u0027s the please speak button we hit your red button and let him speak we have a backup in the rooms referring you here we go can you hear me good morning it\u0027s very dark for morning here okay so this is just as supposed to be an update on where we are with random access live resources the request at the last meeting was to that was to basically ensure that some intermediaries were able to operate well with well with this so I guess you\u0027re advancing slides oh right I can\u0027t advance slides from here don\u0027t think okay go bad so this is yeah this is a brief recap of what we had basically the the idea here "
  },
  {
    "startTime": "00:09:06",
    "text": "if you recall was to use a very large use a very large numbers arranged and basically indicate an indeterminate links a pending resource if the server returns to you your large number you can assume that they support this functionality and if you slip flip to the next slide usually a server that doesn\u0027t support this this functionality should basically give you the hand of the content instead of this instead of your very large number um having some break up here ok so anyway we\u0027ve prototyped this if we go forward to the next slide I don\u0027t know what do we do questions along the way if anybody has any questions or at the end I think given that you removals even to the end I\u0027m cool either way so ok so here\u0027s some pointers to we have a live server people can try basically it\u0027s it\u0027s feeding a nasa-tv stream that supports this lie of appending functionality so you can go and actually try grabbing some of the data or whatever and as well as a couple reverse proxies we\u0027ve set up and source code so if you want to go to the next one here\u0027s an example session with that server this you know basically looks as you\u0027d hope it looks it should look exactly like what was uh you know prototypes and what\u0027s in the RFC we return you know this is just this is using plain old curl so plain old curl works with these dreams just great it\u0027ll just continue to download the the content as it goes iffy so this is this is the request response if you flip to the next slide we can see the server and the client kind of doing their thing here the the hash marks represent data being transmitted and the dots or pauses on the server and you can kind of see here\u0027s curl showing its bitrate outputs and you kind of see it vacillating and bitrate as it as it sporadically gets live data sli pointed that long said of hash marks there at the top that would be like your random access data you know like if you\u0027re doing video that could be R keyframe or whatever it is you need to kind of bootstrap yourself anyway uh and this would just keep running basically forever if you were to go grab this you would just have this growing file on your system that you could play in VLC or something like that in this case because this is the NASA TV stream is a "
  },
  {
    "startTime": "00:12:07",
    "text": "is basically one continuous transport street so ok so go to the next one next slide so this is our small little summary we have details on the following slides if we want to go into them showing showing our basic results the interesting thing is how well how well kind of squid and varnish actually worked I wasn\u0027t really expecting them to I was expecting them to work the same way that that farnoosh works in default mode which is basically is if you do a request it\u0027s going to make this it\u0027s just going to give you a static chunk of the content representing where that is at that particular point in time but the important thing is it\u0027s a coherent you know by tax equals of the origin data equals by Tex of what you download so there\u0027s no coherency problems or anything there that we\u0027ve seen and still working on calcloud where we\u0027re testing so so I don\u0027t know real quick I guess we can go through what what those requests look like it\u0027s kind of the same thing if you go forward a slide or we can skip this do x critical we have time now we\u0027ll go through a quick if you can forward the next slide okay so this is you know it\u0027s basically the same sort of thing hits hitting the reverse proxy with Kuril command this is varnish and out this is out of the box or nisshin\u0027s I\u0027d call it and you can see here that we have our origin server which is a live server and then varnish in front of it and it\u0027s basically giving us back this this fixed number as the range and down below and also it\u0027s not giving us a star to represent intermediate content so a client who\u0027s implemented against the RFC would basically operate on this guy\u0027s at these static content and say okay this you know I can\u0027t do any fun stuff with aggregating content with this server but the intermediary I guess there aren\u0027t any problems with the you know burnish doesn\u0027t break the data that\u0027s cache is is you know byte wise correct that sort of thing so we flip ahead to the next one so this is varnish reverse proxy "
  },
  {
    "startTime": "00:15:07",
    "text": "configured with this range support basically config file changes and this worked remarkably well although what I\u0027m trying to do is to resist looking into optimizing in the caching as interesting as that would be really we were just trying to check to make sure that again varnish doesn\u0027t break and we aren\u0027t returning bad data and in that sense everything was was cool it basically you know it acted as a proxy and it looked exactly like the like we were that we were operating on the origin server so that was a nice surprise actually so we can go to the next slide here this is an example of what that what that looked like it\u0027s similar to the live range you can see here we at the beginning and see the bitrate is goes from being fairly high because we\u0027re getting random access content and then it Peters out as we start to go into the live content and that stabilizes more towards the bitrate of the nasa-tv upstream the the only the only side effect of this seems vni like I say I hadn\u0027t gotten too much into it and it may have to do with the way that it\u0027s that I\u0027ve configured it but it seems like it was that varnish was just trying to get even after I cancelled my session it was just continuing to try to download live data like it was going to do it indefinitely I didn\u0027t wait I I should have let it run a little longer but Darsh acts paying for the bandwidth so I didn\u0027t want to break his bank account by running it for like a day or something but anyway that\u0027s uh that\u0027s that\u0027s what that looks like for varnish with range support so you go ahead to the next slide squid reverse Park proxy actually operated just like the varnish plus range support it actually was telegraphing through the indefinite range requests go ahead and flip slip to the next one and he slides just aren\u0027t big enough okay same same story basically here we accessed a whole bunch of random access content in a bit rate Peters office it hits as it was hitting the live content and it actually and squids a little bit better in the sense that once once the Kuril closed its session closes its session the squid closes its connection "
  },
  {
    "startTime": "00:18:10",
    "text": "with the origin server and stops buffering so it was really really promising so all in all so that\u0027s basically it I guess there\u0027s just comments and questions like I say I would one thing I\u0027d anticipated is again it\u0027s we weren\u0027t checking these to make sure that they were being very optimal about how their buffering data or that we aren\u0027t completely blowing their you know the proxy cache is you know storage or anything like that this is just making sure we aren\u0027t destroying them and we\u0027re getting good data okay thanks before we take how much from the floor I will just say that the witcher\u0027s this is currently slated as experimental and the chair sort of intention is to open our last call if with this meeting and start taking those comments yeah so Martin Thomson as an experimental draft I think this is plenty of testing I would say that maybe we could say the experiment has worked and we might want to go propose standard on this even if other people are amenable to that this is good work thanks for doing this all Craig I\u0027m a little nervous about the varnish thing but I think that\u0027s just something that those guys will have to have a have a look at and maybe maybe make a few tweaks so that they don\u0027t open themselves up to denial of service but the I\u0027m maybe they it\u0027s not a real problem and maybe they\u0027ll keep trying for a little while and and give up eventually that it would be interesting to know what what\u0027s going on there but ultimately the choice did avoid something like this with something like varnish is something that a server is going to make so they\u0027ll don\u0027t be wanting to test that the code works we have an RFC describing how this works then I can test that too so I\u0027m not particularly concerned about the few little oddities in the behavior there things didn\u0027t break the bytes didn\u0027t get truncated weird ways or mashed against I can imagine all sorts of funny modes for this but this seems fun and the regarding varnish - it\u0027s important that that wasn\u0027t the out of the box configuration that had the issues it was it was a it was special changes there\u0027s that were intended to do range support and there were lots of caveats on that description of that functionality in terms of it not doing coalescing and things like that and yeah I really hope that what happens is people go forward and start doing those things start doing coalescing and and supporting sub-ranges on cache content and that sort of thing "
  },
  {
    "startTime": "00:21:12",
    "text": "okay sorry oops so I think yeah our hope was to also finish that CloudFlare love like a string but hopefully we\u0027ll probably get some results from that also because that\u0027s kind of the more interesting you are the more common use case that may happen in kind of a CDN kind of environment but probably provisional results on the mailing list but I\u0027m hoping that there should be enough to kind of at least issue a last call we can\u0027t really keep the last call open until that and I was thinking um yeah I agree with Patrick thank you for introducing this work this is absolutely what we asked you to do and more and that\u0027s great I think going to last call now is probably appropriate and that sends a signal that we\u0027re serious about this and maybe I was just thinking as well keep the last call open for a bit longer to give the different server vendors and so forth a chance to take it seriously so some mark what about the question of experimental verse yes I remember I don\u0027t remember a discussion about the intended status of this we we we spoke about it experimental I I think we need to go away and maybe have a bit of a chat but I\u0027m not necessarily against the standard yeah yeah yeah we we have discussed the issue before so I should bring up the thread and see how it a lot will be arrived yeah because it there\u0027s this pattern in the organization where things that people want to propose like this you know they\u0027re tested and they intend to deploy them and they they say experimental just to avoid the level of review that people have lower expectations from an experimental draft so that the level of reviewers and quite as thorough and I think in this case the the document is a high enough quality and the the testing that we\u0027re saying particularly if we can get a little bit more information in a reasonable period I think it really perfectly find it just go propose standard after all it is just the proposed ended yeah I\u0027m less concerned about the status the document I think that that\u0027s probably true but like Patrick says let\u0027s go and see how we got where we got where we are and I particularly want to make sure that we get folks like Apache and other CD ends and and you know the varnish team themselves just to have a look at this and make sure they\u0027re comfortable with it right I think with proposed standard you know the question of other and 10/2 implements becomes a stronger question and we might want to research a that\u0027s already a clarifying question about the aspect of this that deals with the representation where you have sort of the deletion at the beginning right so there\u0027s a description in section 3.2 of how this works when you have a resource where there\u0027s sort of a time window you can go back in time a certain distance but not all the way back and so it presents this mechanism of saying hey this is live and there\u0027s a certain buffer in the beginning that moves overtime and I\u0027m trying to figure out what that would look like when we\u0027re "
  },
  {
    "startTime": "00:24:15",
    "text": "talking about what I put in the cache for the resource when I\u0027m saying okay I was told that it\u0027s available from this bite range to an indeterminate length I chose something later than that bite range as a first how do I know the last possible moment I can go in backhoe and the answer to that mean may mean you don\u0027t right in this particular case wherever you start from unless you start a new request and get a fresh response you have to assume that they putting it into your local cache from that point on you\u0027re not going to be able to go and back though but if that\u0027s gonna be the case that draft might want to say that out loud just because otherwise if you if you choose to pick someplace that\u0027s not at the beginning of the of the of the range that you\u0027re supplied in order to get closer to the current place you you now know you can\u0027t go backward in time anymore even if the server could have done it at the moment you started so I\u0027m trying to figure out kind of concretely what I would put in a TR to suggest the language or for that and I it\u0027s not really coming to me but I think it might be useful kind of working out what those semantics are and having some language in the draft that just says hey once you picked a range if you want to go backward in time from the range you\u0027ve got to start over to figure out what that new range looks like dark you can adjust at us so if I understand your question correctly what we\u0027re saying is that I initially found out that this was the starting range and then at some point of time later on I may want to make a request for that range and it\u0027s not if I don\u0027t get close to the mic please sorry I\u0027m looking at section 3 tattoo at the at the example that\u0027s at the top of page 7 or I guess at the top of page we\u0027ve just the world um and what I\u0027m showing him is that the way the content ranges is displayed you have a range followed by the indeterminancy marker and if you don\u0027t go to the beginning of the range to start my belief is that you don\u0027t necessarily always have access to that same beginning of the range because the description says this is probably a sliding window happen just based on the range request semantics is if the server is able to satisfy you want a part of that range it was under 206 with the range that it sends back if the server cannot satisfy anything it supposed to sign I think a full 16 range not satisfy whatever the range not satisfiable coordinates I\u0027m done okay so you\u0027re saying instead of actually adding text to the to the draft you just use the existing cement that says range not satisfied and even though this was the "
  },
  {
    "startTime": "00:27:16",
    "text": "original response you just have to at that point as a client figure out I I give up now that\u0027s no longer satisfied okay yeah that\u0027s that\u0027s that\u0027s correct as long as there\u0027s some overlap so a way to think about as long as there\u0027s some overlap you should be getting a partial content but the included is there in the response is the area that\u0027s overlapped right in the in the content range yeah so not in some Sun I think we\u0027re we\u0027re talking about things that actually kind of unlikely in practice the the number of resources that have bits of them that go missing over time is relatively Q and the number of people actually doing things like coalescing of byte ranges and things like that have also relatively few it\u0027s good to have it out in practice but I think we have all the existing mechanics in 70 to 35 I think it is that covers most of this stuff up again it\u0027s a number the rain just got a little bit 33 yep I\u0027d say yeah 416 covers basically everything you need to know and Ted\u0027s question for me kind of brings up another aspect of this in that I think where we\u0027re getting comfortable with the safety of this another aspect is cache friendliness and and in my experience caches and range handling are all over the map and so I\u0027m also like a little feedback from from cache implementers - you know if this if they need any more explicit signal or anything else to make this more efficient for them but that could be part of last call feedback I think but I was actually comforted that the answer was its defined by seventy two thirty something right instead of a new semantics for this which is you know make a really good answer okay seeing no other questions what we\u0027ll do is we\u0027ll open the last call then maybe in the working group and maybe work through these last few niggles next on our agenda for this morning was supposed to be expect CT but we\u0027ve already talked to that so we\u0027ll move on to have a common structure which has seen a fair amount of discussion since we last Matt I\u0027m along with proposal from Mark called structured headers which may be a way forward on some of those issues so he\u0027ll he\u0027ll talk to that so yeah we adopted jfv a long time ago from Julian which described how to use adjacent serialization of the json data model and HTTP headers after a lot of discussion there was a feeling that that wasn\u0027t the appropriate way to go and and pauline and comp came up with the header common structure which was his sketch of the beginning of what he wanted that you know that that sort of thing to look like and that got enough consensus where we decided to adopt that and in favor of "
  },
  {
    "startTime": "00:30:16",
    "text": "JFD and then it sat there for a while paul had some other stuff that he had to do and you know I personally felt like we needed to move forward on this I had some ideas that I wrote down as structured headers for HTTP which you see here and then what I had it sketched out to the level that I was comfortable with I went to Paul Henning and said you know what do you think is this you know the next step forward he agreed to come on and co-author with me and we\u0027ve been refining this in the background and we we announced at the working group where we\u0027ve got a really good amount of feedback so talking to Patrick I think the question is do we just make this version Oh two of the structured header draft do we want to actually swap it out or whatever I think that\u0027s just mechanics but I think the question to ask here is putting aside all of the discussion around the minut details of how we represent numbers for example which could easily consume this session and into next week is this the general direction that we think we want to go in and and that\u0027s the feedback in the discussion that I think is probably appropriate at this point I\u0027m very much like this draft and since that it tries to cry by the best practice instead of trying to invent new things so in that way I think we should not put up this draft thank you and and to be clear it is still very rough the algorithm is in particular are just kind of thrown up there and maybe they stick maybe they don\u0027t but we wanted to get the conversation because because you all right since I rather on this I\u0027m not acting as a chair in this our chair tells us that we have a extremely time bounded hey I did wander you know as chair open the floor for comments on the draft in particular we do have enough time this morning and it\u0027s if anyone has strong opinions on any of these matters it\u0027s not a bad moment to hear it but this will be time bounded I don\u0027t see people running to the microphones I\u0027m not all that worried about how big an integer ought to be but if you have feelings I\u0027m I\u0027m interested in maybe census of the room just a show of hands of who has read the potential subsequent draft here structured headers and wants to make a comparison to the previous afternoon see if that\u0027s the direction they want to go in because we we saw a couple nods but I\u0027m not sure we really have enough people participating to make an informed judgment so have a show of hands of who\u0027s sort of read both of these drafts that\u0027s like six maybe okay yeah we may "
  },
  {
    "startTime": "00:33:19",
    "text": "need more comments from the list yeah Thompson to be fair there\u0027s a lot of a lot more people who aren\u0027t here they\u0027ve read read the drafts because a lot of the discussion on the list is from folks who are not in this room I\u0027m from kazuo here who\u0027s been involved in deciding how long the number should be of course I\u0027m just trying to figure how to weigh the input we\u0027re getting today yeah and personally I think this is this is much more promising than what was written in the other draft but I think it probably needs it a good deal of work particularly in the areas of things like the numbers so that we can I guess narrow it down to the set of things that were comfortable with I think there\u0027s a little bit of work left to do that that said that what could happen in the in the working group I would rather see this be adopted as a replacement for the structured headers be able to the other Paul Hennings draft rather than just leave this hanging because we then have this hanging common structure thing that just I don\u0027t think it\u0027s particularly useful at the moment I mean that we had grand aspirations for it and that\u0027s great but it didn\u0027t really eventuate and we really started to discover them all those places where it wasn\u0027t really going to work for us this is maybe closer right so that\u0027s clearly one of marks motivations yes we had many chair discussions about how to move this work forward and it\u0027s just too big and parts of it of too little value both existing things ability opted and the hope here is that this is more focused on something meaningful an unsolvable problem okay that being said I even less anyways I think further comments on that my only comment is um I\u0027m really happy with the level of engagement on the lists about the draft that\u0027s what\u0027s giving me a hump here that would need to continue and need to broaden beyond just number formats yeah Martin Thompson I would caution against things like bike sheds being used as signal yeah so we\u0027ll see how this works out I have hopes as well but you put a red rag in front of a bull and now you\u0027re celebrating the fact that it charged you so not dead yeah so and I hate to put people on the spot but you know Julian you\u0027re a ton of expertise in this and I know you\u0027re sitting there listening to us on the other side of the planet and I\u0027m wondering if um this would be a good time for you try me and if you\u0027d like to perhaps red button meantime my coin yet Sakura Google I progressed because I didn\u0027t throw up the discussion on the mining case but is there any reason for restrictions on the length of various types that\u0027s one of "
  },
  {
    "startTime": "00:36:21",
    "text": "the discussions I think we need to have there were certainly from what I read on a list significant sentiment against having lengths on individual fields to talk to that I think we should remove those because it would break a lot open this thing I\u0027m starting catch last part I think like the increments would break a lot of these two departments 20% very large traders so someone times and I think the the comment here is that if we if we were to retroactively we apply these constraints to existing headers we would find ourselves in some sort of pain but I don\u0027t think that\u0027s the intention of the draft the intention of the draft is to say if you\u0027re going to define a new header field maybe you want to use these recipes and doesn\u0027t say that you have to use these recipes either it just gives you a you know this is good practice if you don\u0027t die verge from that good practice be certain that you really want it it would be wonderful if we could clean up the mess that is header parsing for all of HTTP but that is a much larger task that we don\u0027t feel more capable of doing right now so this is really just uh just to stop the pain at least for newer headers so on jabber from Julien he says I think this goes into the right just discussion would be good to decide on adoption soon haven\u0027t let it in depth yet thank you oh and we have yo hi so I haven\u0027t gotten into the details of the draft but what I want to emphasize and something I just recently realized that kind of structure would enable us to provide significantly better HVAC compression for parameter names and would relieve pressure from giving new drafts very short parameter names in order to save bytes you\u0027re saying that we could take advantage of structured headers to in an in a newer version of header compression yes I\u0027m saying that once we have structured headers we could have HVAC to which adds parameter names static parameter names into the static value in that thank you okay so I will come up with a plan for how we move forward just from a you know procedural point of view out that out to the list or comment next on our docket this morning cache digest for e2 and "
  },
  {
    "startTime": "00:39:22",
    "text": "kazoo has got a presentation well he comes up this is one that\u0027s been with us for a while and I hope her we seem to be nearing the finish line now we\u0027ve made some changes so hopefully this will be enough to get us over the endpoint so high and to change how fast is the same ding cash dices settings permit so it\u0027s pretty simple it\u0027s a way for the client to say that hey I\u0027m going to say any cash digest so so the server please wait decide standing water push and you see my cash status and the client strategy will be to send a - that is frame for every quest that was the news origin and or if it doesn\u0027t want any cash digesting to send an empty cache that is frame with research plug-in gaining that I\u0027m not going to give you any indication and the next one is more debatable and it tries to change the digest algorithm from gone cold asset that we use now to taco filters and motivation behind that I structure on the site and send it wait necessary rather than returning through the cache to be the digest when sending a spring and so on the browser side you essentially have a filter that hash table that implements event handler for turbans and one is in certain events when a browser a new object is added to the browser cache it has an insert ribbon and when I entry get affected I should have an eviction even so think it is to create a hash that could be sent to the server and and also hatch that to see if only those two events and the other aspect of the change is that since there would be only the instruction and young-shin event you cannot determine whether if a entry within a couple filter is purchased a next week so thanks Europe we now have some final preliminary numbers regarding the size of the please note that this is a preliminary and we might further tooth on the side "
  },
  {
    "startTime": "00:42:22",
    "text": "and as you can see the size of storing 3000 edges with a probability of about 6 cubits or five kilobyte at minimum and for comparison and GCS well just feel fulfilled in this next week and the other issue that we need to discuss is about losing the distinction between fresh digest and still dodges and well the data is very small and it\u0027s all bad we can assume that there be something like twice as much of still digest so looking the distinction would mean that the browsers would be forced to send a flash and the other issue that we don\u0027t even know how because we owe the ncdb do we haven\u0027t and on the server-side computers has a good characteristic insisted you don\u0027t need to decode the dice before using this is of course if the digestive Saint and Congressman consisted the you orient URL and a tag needs to be well then the two properties are needed to check if and that could have implications on certain caches because instead of just look looking at the URL you need to actually fetch the object and check the exam to see if you you should push them next please so that\u0027s the summary of our stages and so I think that we have three options now one is to replace and if we are interested in taking that new doctorate I think that we should at least wait for water and the second choice is defined both algorithm or possibly just reserve a seal that can be used to indicate which algorithm it\u0027s been used and the third option is to stick to using GCS and even if we decide to stick to using GCS it couldn\u0027t be aim for the calculated approach because personally I believe possible to adjust the approach so that you could generate a GCS from is something like a computer stuff and we also need to discuss about we are already two years since "
  },
  {
    "startTime": "00:45:35",
    "text": "sebastian Decker\u0027s no affiliation I would like to know if there\u0027s any attention to implement sort of signaling between the client and server when a cache object is inserted or purged well so a browser can resend your cache sizes if it changes to the cache but as long as the only changes it receives for their origin that if the brothers connected is from that connection then there\u0027s no need to signal changes because you can a server can seem dead what it has it\u0027s my understanding that you can do multiple origins within the same cache digest frame how they are you feel so my bishop Akamai I think the replacement with KUKA filters makes a lot more sense because you can just have one object and keep it up-to-date rather than having to crawl through your cache and generate it or even to call the table I really don\u0027t like the idea of having two different algorithms that clients now have to signal which one they support which means servers support have support both or they risk having to match with clients I don\u0027t think there\u0027s enough wind from one over the other at least in my understanding to support both but on the whole I think this is useful work and I think we do need to figure out how to push three or four vard Nottingham I\u0027m an author on this draft so I\u0027m not that guy who sits up there I yo could could you actually um I you know you\u0027re wearing your chrome hats at least partially for this interaction is that correct to some extent yes so my chrome my oh my hat okay um from my perspective personally you know the the problem that we\u0027ve had in this draft for it zero points out it\u0027s long lifetime is that you know we have been had trouble getting clients interested especially browsers implemented interest in implementing this so if there\u0027s a good chance of getting browser implementation or at least some good experimentation bug on with cuckoo filters then that make me think maybe that\u0027s a good idea and and if the working group decides to do that I\u0027m happy to take my name off the draft and and have y\u0027all go in as an author because I think obviously he is gonna be "
  },
  {
    "startTime": "00:48:35",
    "text": "generating a fair amount of text in the draft and what I like about that is is that that\u0027s a we have someone at least partly representing a browser and someone representing the server side working together because this is a collaboration between the two sebastian i just wanna say that i agree with that and also that I did a experimental prototype about a year ago using the GCS but purely as a serviceworker not in integrated into browser and the results of that or my experiments at least were very positive in terms of performance although I do think that I sort of misunderstood the intent where I did not intended to replace the thinking time of the browser on the opening with nection but I used it to sort of replace webpack bundling assets and drastically reducing the reload time of a front-end application basically so I think there are experiments that prove that this is very meaningful work so we\u0027ve had a couple of comments about how yeah that\u0027s a while I mean I would also know it\u0027s experimental so we don\u0027t have to know at the perfect the enemy of the good here so I would encourage you to you know have the courage of your convictions and make a decision under that experimental manner we obviously need a new last call right for that going forward do you if you have thoughts on what you would like to do of these three options so of the three you have down here but you\u0027re hopeful you can go with a based upon that experience just thinking out loud we I haven\u0027t heard of any interest of anyone generating the frame with GCS although you seems to be using the header would it make sense to publish the document we have without the frame and just describing way to do it with headers and serviceworkers maybes experimental and then see how cook the filters goes for the you know more integrated browser integrated flow you\u0027re still alive did you want to say anything else i I think we\u0027re placing GCS with cuckoo filters is worthwhile for the reasons "
  },
  {
    "startTime": "00:51:36",
    "text": "you mentioned for like because I basically I don\u0027t see the GCS approach being adopted and I didn\u0027t see the Google filters approach thickened up then I\u0027m not sure what would releasing GCS as a header would give us over the current experiments but as long as it\u0027s done in a way that really confusion around cache digests I\u0027m I\u0027m fine with me Thank You Sebastian I don\u0027t have any particular opinions regarding GCS versus cuckoo filters I do have a experiment that I\u0027ve been working on that hackathon and I\u0027ll hope to conclude that soon to compare the two if that helps at all okay if there are no further comments I think about Tootie to the list too and and big the authors become how they want to update the document thank you I make one last call for blue sheets Sushi\u0027s blue sheets there\u0027s one still out there somewhere mr. Barnes I\u0027m looking at you and I want up here okay after cash digests we have clients so uh it\u0027s on the big board up there maybe Michael wanna talk to talk to this so we got this update from Ilya who is the author of the client specification and he could not be here with us today um client hints is still progressing I think Ilya is still working on it in a couple of different directions I mean this is another document we\u0027ve been working on for a while and we\u0027ve been trying to get right most of the discussion recently has been around except CH lifetime and except CH to figure out how the mechanism for advertising this works and especially around privacy considerations there been some concerns brought and and hopefully dealt with if you\u0027re not familiar with this I\u0027d encourage you to look at the most recent graph and the discussion most that has been happening on github I think that we\u0027ve probably got another cycle or so of work to do on this document before it\u0027s really ready to go so maybe early ish next year and also um depending on the adoption of variants right now this document refers to key I think Ilya has said that if we adopt variants or swapped it out for the variants I\u0027m encouraged that the "
  },
  {
    "startTime": "00:54:36",
    "text": "movement on this document is now around fundamental issues of negotiating client Hinson\u0027s lifetime and that kind of thing it is not amongst the details of the set offense that provided back and forth because that would just be able to beam document it would never never converge and we seemed to have exhausted most of those issues around to the fundamental one so I think there is actually hope now that we\u0027ll get this out and maybe variants not blocker but the contemporaneous document that needs to happen to make it go forward Milly is not with us but are there any other thoughts name hasn\u0027t on the claims work sir Mountain Thompson there was a thread on the list about a geolocation header that hoped to use this mechanism I didn\u0027t respond to that thread but I have opinions where are we going with with this long term because the current structure of the document is pretty well-suited to the sort of things that it describes that the new header fields that it includes but once it gets outside that comfort zone it may not work very well unless we bring in some more examples unfortunately I think geolocation would be an awesome example to test this out and a terrible thing to add so I don\u0027t know where we want to go on that but I agree that working through the the fundamentals here is where we\u0027re at and it\u0027s not quite there but I can sort of see an end in sight so it\u0027s not the other thought that I had was is this experimental is that the is that the aim yes okay that makes me a little bit more comfortable that about some of this stuff the the security and privacy considerations are considerably better than take word before which is probably the main reason why you would sort of hold the publication or something like this but the rest of it that whether it\u0027s fit for purpose I think we have a lower bar if it\u0027s experimental I do think we\u0027ve gotten into the habit a little bit of misusing the experimental state or something we\u0027re not quite sure about but let\u0027s lop it out there and see what happens yeah yeah so what is the experiment exactly yeah I\u0027m sort of wondering whether the experiment is does anyone actually want to implement this other than the author of the document that may be the question well we can ask about proposed standard experiment can we publish it and then everybody else I probably have to implement it yeah well "
  },
  {
    "startTime": "00:57:39",
    "text": "yes um just regarding Martin\u0027s comment about geolocation headers and I think privacy in general I think we will need at some point to distinguish capabilities that are available for active content versus passive content and we already started to address that and the recent except CH lifetime draft as well as there\u0027s a has a proposal to add something to feature policy that will enable exposing of the client hints info to select third-party rather than providing uh you know providing access to that in people to all because on the one hand providing that information to third parties is a very important use case and on the other hand it\u0027s very easy to abuse it so this is work in progress and I think that uh you know capabilities like geolocation even though I\u0027m not necessarily convinced about that particular use case I think we should address something like that and be able to guarantee privacy restrictions around it I don\u0027t know what guarantee privacy restrictions means I mean these mechanisms obviously have a negative impact on your privacy I mean it\u0027s not a like every bit every bit you leaked out of the browser without cause you know that wasn\u0027t a server to you know to to ask for specifically is the privacy issue and we\u0027ve seen that you know I\u0027m and so I mean the uh you know we\u0027ve seen number of cases where servers interrogate stuff from the browser they have no actual intention of using and they use it for finger ring purposes whatever it is he\u0027s the classic example here the only reason we know that\u0027s being misused is because you can observe servers interrogating for it and then just discard and then just scan the data and so yourself in a position we are the where basically the server says is please send me a pile of stuff there\u0027s sensitive finger brain data but I might happen to use I might not use to condition my site um and you cannot tell what I\u0027m doing essentially removes the ability of researchers lenggang partner Ganon to find out when this is being misused um so IIIi are not like you know to stop people from doing things that are shitty because that never works but "
  },
  {
    "startTime": "01:00:41",
    "text": "um I don\u0027t understand what the like what is it experiment this seems to me to be putting their stamp on this practice and I understand with experimented with other than like well privacy people like freak out so when we adopted the document um I think there was pushback not on this aspect but just around general implementor interest mm-hmm and we adopted it as experimental as we have with other documents in the past because we weren\u0027t sure if it was gonna really get broad adoption I think that\u0027s probably not a good use of the experimental State separate from that the privacy and security concerns we\u0027ve had a fairly involved discussion around that I\u0027d encourage you to look at the document and look at the current approach and we can if you want to raise issues let\u0027s work through that I have seen interest you know on the browser side yes we have one browser who\u0027s interested that\u0027s primarily but other folks are interested from other perspectives especially people who want to do content negotiation and not touch the content sure I\u0027m not denying I mean like lots of things would be convenient like we really convenient like I said she any kind of events my hard drive that even looking me for you to write I mean the the standard whatever you want I think like like as far as I know there have been discussion about this but mice brain specially Murthy and people with ain\u0027t saying these aren\u0027t brothers use when they obviously are in terms of geolocation my inclinations is that we shouldn\u0027t be adding new things to this document new axes of negotiation but I do take Martin\u0027s point that if we have a broader scope of potential axes that that\u0027s the bezel and in forming this framework yo yes so regarding Eric\u0027s points there is an opt-in so this is not about send users private and for users identifiable info to servers everywhere the servers have to opt-in and clients can for example refuse to respect that opt-in or track if that opt-in also correlates with a very header or a variance header once that\u0027s a thing and you know note that this server is asking for data that it\u0027s not using and therefore it is suspicious and for example can go on some privacy list this is generally the approach that we\u0027ve taken is that client hands should not expose information beyond what\u0027s already available in JavaScript and if you see places where we deviated from that goal let us know "
  },
  {
    "startTime": "01:03:47",
    "text": "my objection is that there\u0027s a difference between active and passive fingerprinting and that way and wait for you to say that the standard we\u0027re gonna apply is that with permission we\u0027re gonna have we\u0027re gonna have passive fingerprinting for anything which you couldn\u0027t got my JavaScript was like not appropriate standard that\u0027s exactly what\u0027s wrong nor by the right what I\u0027m saying is that it\u0027s not 100% passive because there is an opt-in and you can track its usage so if someone opts in or doesn\u0027t use that data for the intended purpose then you could potentially mark it as no matter then like then if the JavaScript you use this data privacy Google I\u0027m sorry I when if the user is opting in to your location in the permission prompt how is that passive non and I\u0027ll answer that because you give the site permission once and that persists now you have active use of that permission on every single request as opposed to just at the point where the activation of the the request was made so that the material difference here is that when I when the site asks for geolocation it asks the location and gets it that\u0027s that\u0027s an action that can be tracked but one when you have this sort of capability you have a one-off opt-in and then you have geolocation being provided on every single request even if the server owner uses it on two of them but right now the prompts also have an always just remember we don\u0027t have geolocation okay well not discuss stuff that we\u0027re not unhappy right there\u0027s offline and Negro persuade to me they won\u0027t okay so I would say one of the takeaways here is we need to open a github issue attached to client hints covers this issue and the working group will have to come to consensus on whether or not that security considerations can be you know amended to actually deal with it or whether the issue is you know just overly fatal I think for the moment and we should track this with you know initially to make sure um consensus is reached I thought there were no open issues in his track well well we\u0027ll double check and make sure that\u0027s the state when anyone else we have time in favor on us has our last comment here or we can move on okay so speaking of a long time drafts we have RFC sixty to sixty-five this known as the cookies rewrite and is Mike with us on who is going to speak to progress and some progress has happened since their "
  },
  {
    "startTime": "01:06:48",
    "text": "last will get together is Mike in the audience there and meet at home we can\u0027t scroll we don\u0027t know my quest are you there alright Mike does not appear to be online so I know Mike has made some updates in this document his task is to integrate the resolution of the issues that we discussed which is mostly things like data as well as the major document proposals that we accepted earlier on in this process and I think he is integrated almost all of those I\u0027m not sure but I think his intention is is that he will have a document ready for working group last call in the near future so hopefully we\u0027ll either be talking about this would then I towards working group last call in London or before that would any well we\u0027ll wait for Mike to give us an update on the list since he\u0027s not here that\u0027s a good question actually um we\u0027re going to need people to review that document once it\u0027s ready who\u0027s intending or willing to review the cookies draft once we have it in a good State I see a smattering of hands we need to increase that number very light smattering of hands so thank you Jeff Hodges thank you Mike Fischer but anyone else like it thank you by raising their hand Monson um we\u0027ve been having a discussion internally about expiration times on cookies and how cookies are expired and I\u0027m wondering whether or not there\u0027s going to be another thing to add to the pile in a very short amount of time what we seeing just to give a brief flavor of the problem is that different browsers have different policies with respect to retention cookies and that different limits on the number of cookies they\u0027ll accept in the size of those cookies and various other things and we\u0027ve recently increased our number to match that of chromes but you know we had this whole priority discussion it sort of plays into that there\u0027s some really awful side effects when you start expiring cookies that aren\u0027t expired and having some sort of common strategy across user agents would be kind of nice so nothing yet at the moment we have someone looking into it but we may want to talk about that at some point yeah they definitely does lead to real-world failures yeah it\u0027s kind of awful actually the situation that we\u0027re in "
  },
  {
    "startTime": "01:09:48",
    "text": "because once you start expiring cookies in different browsers all bets are off you have no idea what you can rely on anymore it\u0027s really kind of random actually so when we discussed cookie priorities um my perception at the time was we were very close to adopting it but we just didn\u0027t quite make the line are you saying that maybe we should reconsider that um you know now cookie Priorities was not the solution that we\u0027ve discussed and it\u0027s probably better to if we actually come up with a proposal and and rather than go back to that yeah I sent you\u0027re not quite ready do you think we\u0027ll be seeing a draft soon or just an email message or I think it\u0027s gonna be in the form of an email message because I don\u0027t want to write a draft until we have discussed the the broad-strokes at the problem it\u0027s very early days the thread started this week in fact so well wait for that then I I don\u0027t think you\u0027re in danger of the cookie document shipping you know in those kinds of timeframes yeah so just fair warning on that one that\u0027s something today okay which brings us to BCD 56 bits its inaugural appearance on our active graphic Jentezen Ark has the presentation here is authored this room he gets to stand up in stretches line at the end of the week it\u0027s no longer a pink box it\u0027s a pink angle so it\u0027s kind of sad down there so um BCP 56 came about around what was that mm ish 2001 somewhere in that time frame and it\u0027s embodied the the best thinking about how to use HTTP a substrate at the time how people use HTTP has moved on considerably since then and so we adopted very recently now this document I think this is really important because now we have a large number of IETF working groups creating new protocols that use HTTP for some value of use and I don\u0027t know how deep we want to go into this today I really just want to get people to start to look at this document I know that I have a lot more work to do on it it\u0027s very sketchy and very bare bones now and I think it needs a lot more examples and a lot more text explaining why things are the way they are and and but I want people to start looking at the principles in the document to make sure we have good agreement on those principles I don\u0027t know new so the first "
  },
  {
    "startTime": "01:12:58",
    "text": "thing here in in section 2 is you know we need to decide when this document applies and so at a high level the approach that I took was if you\u0027re using port 80 or 443 or you\u0027re using URLs with an HTTP ich scheme or if you\u0027re you you identify the protocol as HTTP using one of our H al pn identifiers or if you\u0027re using the message formats we described along with the registries that kind of fill those up you\u0027re using HTTP in this document applies does that make sense let\u0027s go on down and then we go into the general you know this is mostly advisory here in this section I think you can scroll on down just kind of getting people used to how we think about HTTP how we think about our links on the web how you can you know why you don\u0027t want to just use HTTP as an RPC because you know hopefully you\u0027re using HTTP to get some of the value out of it rather than just tunnel through you know firewalls which i think is maybe another discussion for another venue as to whether that\u0027s good practice or not that seems to be coming up a lot too and then down in section four are the the more specific recommendations how you specify that you\u0027re using HTTP what documents were you refer to stuff like that keep on scrolling um how you define HTTP resources so you know resources are one of the high-level concepts that we have in HTTP and and and you know we\u0027re encouraging people to specify their use of HTTP in terms of resources keep on going and then how to handle URLs how to discover URLs how how we you know we use I don\u0027t think I actually use hey to us here because I\u0027m trying not to talk about rest religion in this document but there\u0027s a little peek of that in here in a couple of places talking about how to use URL schemes when it\u0027s appropriate to create a new one which is not terribly often and how to choose a transport port methods status codes this is all fairly straightforward a few that are on HTV for a while header fields their definition referring into the stuff we already wrote in 7231 and then there\u0027s some empty stuff here about payloads and interoperating with browsers because that\u0027s one of the big benefits of using HTTP and I think I want to talk about things like cores their access control authentication application States and then the boilerplate after that I don\u0027t think the check to see if there any of Hennessey\u0027s never again lots of references so many references No okay so that\u0027s kind of a high level of parts of the document as I said I think it needs a lot of filling out it is that let\u0027s going to be a fair amount longer I thought bucks want to help out without that\u0027s great but right now I just want to make sure that to validate the kind "
  },
  {
    "startTime": "01:15:58",
    "text": "of core print that it\u0027s talking about and I\u0027d love any feedback now or on list or privately so one of the reasons I was interested in seeing this document adopted now is you know there is contemporary Gnaeus work unite yeah you know having J map for a couple iterations now and it\u0027s there\u0027s a group that could use a document like this to refer to and as it check I\u0027ll have a designer thing them doe obviously this time around as documented and Joe it\u0027s one of my objectives to move both of these in four words of a consistent with each other and even less obvious things like you know dkg had a proposal in deprive that was really really would have fallen under the auspices of this document under your definition of using HTTP that would have qualified right and so I think it\u0027s interesting actually in a you you remind me J map was especially interesting for me because you know the initial kind of visceral reaction was oh what the hell are they doing HTTP and through discussions with them it\u0027s very clear that they have very good reasons for the design that they\u0027re using and it is not a traditional use of HTTP it is they have requirements that are not practically met by HTTP today and so that tells me that we need to make sure that that it\u0027s clear that there are cases where you know it\u0027s still okay to use HTTP you know in perhaps non-traditional patterns and there are reasons for that but then you need to make an informed decision when you do that and it also is interesting because it indicates to me there\u0027s there\u0027s opportunity for new HTTP extensions to meet those kinds of use case in the future so there still may be some interesting work to do yeah so this is not the world\u0027s most exciting document but I would like to put out a plea to the working group that you know the amount of accumulated knowledge in the working group about these topics you know is more amongst this set of practitioners than really any other set in the world and so reading this even sharing in terms of anecdote you know where suggestions have worked and haven\u0027t worked in the past can highly inform this and make this a more practical document perhaps that\u0027s predecessor and that Billy is this point but what it\u0027s updating so you know I think marks signing up to do all that sort of the heavy lifting here but if you can make sure you read it and just have even general commentary on what it\u0027s suggesting I think they\u0027ll be very helpful and can be successful let\u0027s you know as you know as the Shepherd of this documents what I need to hear is is that diversity I can give you points to make it work is this document also intended as a kind of best practices for so-called rest up the eyes I don\u0027t think that\u0027s an explicit goal I think people could use it as such in many ways but that\u0027s a much larger task and I think we "
  },
  {
    "startTime": "01:18:59",
    "text": "need to assemble more of the components to be able to do that I have another draft or two that kind of push us in that direction but they\u0027re not for this working group this is really that the target audience for this is IETF working groups and IETF efforts that want to use HTTP as a substrate protocol not necessarily the entire world if the rest of the world gets some benefit out of it that\u0027s great but it\u0027s not an explicit 10 Rd incipient Yeti I I did actually want to talk a little bit about the best practice at aspect of this because I think BC P 56 was put out with that status because the author mistakenly believed that it would serve as a gatekeeping role and I think we were all aware with the divergence between actual reality and BGP 56 but it it did not serve as any realistic gatekeeping role and in general the ITF has moved away from the belief that controlling registries or making best foreign taxes is is an effective protocol stick for the wider development community I would thoroughly agree with the need to deprecate BC P 56 as it currently exists as a result of the mismatch between its aims and how that document evolved or how the ecosystem evolved over - um but I might ask you to consider whether this would be better off as an informational or standards track document rather than a BCP if it also obsolete obsoleted BCP 56 because I think that there are there are some places in which the exploration of what the options are does not require them to be best current practices to be useful exploration the document might get a bit broader and a bit more useful if you weren\u0027t trying to keep it the things that were DCP ich that\u0027s a fair point I think it\u0027s worth discussing okay it\u0027s nice to have a meeting where you don\u0027t have to just cut the mic lines alright that brings us into overtime we\u0027ve got one topic left to discuss so you know well and have updated the work they\u0027ve been doing on HTTP to cross stream compression you have you can put yourself in the queue if you wanna get your prison speaker this is work that has been presented in an earlier form in a full stay at least once has been discussed more than one time and there the merit here is is fairly obvious especially as you talk about very small "
  },
  {
    "startTime": "01:21:59",
    "text": "resources and the trend is towards smaller resources and many of us think that\u0027s a actually a useful thing for web architecture the the the ability to have you know compression apply a cross stream gives you substantially better compression ratio results and the authors have shown that you know in in past past experiences yeah on the other hand there is no working or there is no document author or working group chair or area director who wants to be the one that says oh compression and encryption that\u0027s okay to put together this time right so everyone remains very nervous about you know the prospect of that and the the security analysis of that that accommodation so you know the chairs that actually sought out reviews outside of this working group since we last met and some experts in the field to do some analysis on the mitigations presented and you know in this work and other similar work and we have been unsuccessful so far and getting anyone to sign up to to take on that work maybe because he also don\u0027t want to be the person to say compression and encryption it\u0027s okay this time I\u0027m not sure they haven\u0027t given that other reasons I somewhat suspect that in case so if you gentle members of the working group have contacts or suggestions of experts in the field that might be reasonable to reach out to who\u0027ve done this kind of web security work in the past who would be you know well-regarded in their conclusions we would love to have a conversation with them that being said I\u0027m here live has you know made some updates to the document and to their security considerations and I think we should give it some Arian to help further understand what we\u0027re talking about in the currency of the artis yes so I ran and I don\u0027t want like I\u0027m not a security person per se but I ran some analysis of the various risks and potential mitigations and that analysis is now part of the document but I think the highlights there would be that mitigating attacks that our cross origin is most probably simple because the client can we say that there is no compression context sharing between different origins and be done with that it won\u0027t hurt the use gates all that much and will significantly increase the our ability to protect users they\u0027re the part where mitigation is a bit less obvious is for same-origin secret leaks and it\u0027s like it\u0027s very hard to actually "
  },
  {
    "startTime": "01:25:05",
    "text": "protect against the same origin secret leaks even today because in most cases an attacker page can fetch fetch the secret content and examine it that way but right now an attacker can protect itself up like a page can protect itself against such an attack by various by restricting across the origin sending CSP headers that restrict the use of fetch for example so if that is indeed the case then compression compression dictionary attack can reveal such secrets but I\u0027ve stated like I\u0027ve put together in the document a few ways that enable us to potentially mitigate against that one of them is heading of transfer sizes in certain cases another is to limit this compression to non credential fetches which would at least today significantly restrict its benefits but it\u0027s significantly better than nothing if this is the only way we can mitigate it against that attack and generally I would love to get some more feedback on those attack vectors on mitigating pins and the overall security story from people who do that at their data and one more thing regard related to this space we have allotted from Google\u0027s compression team online who\u0027s been working on shared dolly which shares some of the aspects like it can potentially if you look at it the right way addressed some of this same use cases while also addressing different use cases it\u0027s also something in this piece but not being said are there any comments in the general space yeah as I said I think this is a problem that htv-2 would like to solve which is you know we have not adopted this draft on security concerns but we keep talking about the topic because I think it is meaningful and it is a in a way I meaningful criticism of HTTP choose with something we we should continue to talk about mark "
  },
  {
    "startTime": "01:28:09",
    "text": "mark mark as time is an HTP t HT t Wow avert an independent issue um and I would I wouldn\u0027t you I don\u0027t ya Mon Thompson I think Patrick basically summarized it right from the outset yeah there are various things that we know that we can do but there are also a large tract of things that we just have too much uncertainty over and it\u0027s funny because some to some extent we have this problem within resources that intermix secrets and attacker controlled information but once you start crossing resource boundaries now we\u0027re talking it\u0027s a it\u0027s a very different game at that point and an opening that that potential up is somewhat worrying particularly when things can be applied generically ian\u0027s but google I did have a question if it sounds like there there are two kind of mostly unrelated but potentially problem solutions in the same space one is this graph there is written up here and another one is this kind of custom bratli dictionary or custom other other dictionary approach do we have an idea of the relative compression efficiency of the two approaches compared to one another because I mean I think it seems like it\u0027s easier to analyze the publicly shared dictionary option than the cross stream compression option like and so if they\u0027re similar from an overall performance perspective I mean I like the yeah the cross stream one it also has a negative from a quick perspective that it\u0027s like a huge band mask like monumental so like you know if we could push towards something it\u0027s a little tighter I think the question is not the compression efficiency but the you slightly different shared dictionaries address the use of relatively large websites who have a large amount of repeat visitors and therefore download the multiple visits from users where the compression like refreshing context in h2 proposal is mostly addressing the bundling uses so right now people many small files over h2 and these are "
  },
  {
    "startTime": "01:31:13",
    "text": "significantly better compressed one bundle at the same time running a custom granularity and it would be great if we could tell developers uh bundling and rely on the transport to solve that problem though the use cases are different shared dictionaries out-of-band share dictionaries can potentially give you better performance but they have other this is thoroughly are not necessarily compatible with all use cases yep so mountaintops of basically what I was gonna say Vlad did some analysis on on this and found that the opportunity cost of sending a large blob of shared dictionary ahead of all of the these small things that we wanted to compress wasn\u0027t particularly good for the performance when you didn\u0027t have some sort of prior loading for the dictionary so there\u0027s something to keep keep in mind with all this the performance here is actually pretty amazing the the problem is balancing that out with with other things I don\u0027t think there\u0027s been any question about the the performance benefits of this and if you haven\u0027t read Bloods previous analysis the performance of it it\u0027s it\u0027s very interesting yeah and in persuasive you know as far as my codes yes there\u0027s also a third entrance in this that\u0027s been kicked around and I\u0027m not sure public that is but there\u0027s a growing set of interesting problem but none of them have security solutions that much different than what we\u0027re seeing yeah and part of the problem here is that we just don\u0027t know what\u0027s in responses fundamentally the problem and we don\u0027t know how to analyze them in such a way that we can actually make sense of it there\u0027s someone in queue you might be needed your mouth is moving hello can you hear me yes okay I am loaded from the compression team in Google\u0027s Irish working on shared broadly we are creating better dictionaries for broadly compression and looking into the shared dictionaries and we also have made a spec a draft of a specification available currently on github.com slash Google slash broadly slash French specs and we are interested to collaborate with this efforts do you have opinions "
  },
  {
    "startTime": "01:34:13",
    "text": "consideration no no strong opinions right now thank you thanks and if if you could if you could send a message to the mailing list yeah thinking it and if you want to consider it as a proposal it\u0027s probably best to see it as an internet draft thank you yep with that unless anyone has any further topics on compression and encryption I think we\u0027re wrapped up or any HTTP topic because we have a few minutes for open mic if you\u0027re anything to say so Martin time to just tweet back on that last comment the spec that the the proposal is as written a patch on the fetch spec so think about that for a moment yeah my little minds blown it\u0027s incomprehensible as a result anyone who\u0027s read fetch will understand that it\u0027s it\u0027s great for some things but not great for others going once going twice Aira germs thank you see you all in London hopefully where\u0027s that blue sheet someone waved a blue sheet give me important where we\u0027re trying then we\u0027re trying yeah we\u0027re sorry we know is disappointed because I get so I think we should we should just have a discussion I mean I to me if we\u0027re not going to get any progress then come in it and you know and if what I what I would like "
  },
  {
    "startTime": "01:37:22",
    "text": "you "
  }
]