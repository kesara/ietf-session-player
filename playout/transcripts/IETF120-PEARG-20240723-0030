[
  {
    "startTime": "00:00:32",
    "text": "using out of it, exactly Okay, I think we can start start I think we can start. Hey all, welcome to the pair G session at IT 120. I'm Shaban and I and Sarah have been coaching this group for a while And yeah, looking forward to the two talks this evening and this is the note well please do note it well It covers various topics such as Pat patents and code of conduct by participating in this meeting and generally the IETF week you agree to follow this. And this is a number of BCPs and RFCs that govern behavior at the IETF Please do take a look at some point. And if you do have issues or concerns about behavior, please talk to the Ombuds team That's why they're there. And we do need a note taker. So to be have a volunteer in the room or remote someone could ask you to anyone in the room willing to volunteer to take notes"
  },
  {
    "startTime": "00:02:14",
    "text": "We do need a note-taker What was the fun part? Oh, thanks Alison Great. Alison, thank you All right, please do sign into the session via Datatracker or the QR code that should be up there somewhere in the session This helps us keep track of the participants at these research group and working groups meetings. And if you're in person, please do still use the Meetecho Light client in order to join the MyQ or participate in show of hand so that it's fair for folks who are remote And yeah, keep audio and video off. If you're not present And yeah I think that's it I can give a quick update about a draft that we've been working on I don't know if Mallory's in the room Oh, yeah I am Yeah, I can give a quick update as well, but you've also done work on this, Chavon. So I mean, I think it was a couple of meetings ago. I think it was in Prague where I sort of maybe asked the chair I know you didn't have a session there. I don't think you did, but just whether or not you thought this draft was getting close to ready and for research group last call because I think we've gone through and just the draft It's fully flushed out now. I think all of the seconds have been written out We've had some great review from folks whom we back actually cited. So work the other have done that has shown up in this draft, we've had reviews"
  },
  {
    "startTime": "00:04:00",
    "text": "from those experts. They've improved the draft And so I encourage people to look at it in its current state If there are questions or anything on direction let me know, especially if that's direction and suggestions from the chairs but it's in its tenth you know, actually technically 11th version now so yeah Thanks a lot, Madrean. Thanks for driving this work Yeah, I took a look as well. And I think the chairs intend on doing a research group last call within the next few weeks. And unless people have like strong opposition or based strong opinions on this right now, please do let us know very soon. That would be great Yeah Great. All right. So let's move on to our first talk, which will be by Stefano who is joining us from ICMO, if I'm not wrong And it's very late for him. So thank you for presenting yes you're not wrong hello from Vienna Let me share the screen again Thank you should be able to see my slides, right? Yes All right, let's start then right? Yes. All right, let's start. I'm going to present you our work called Moulting Point which stands for mobile evaluation of language transformers a few things about me I'm a machine learning researcher at Braille Software and I also hold a visiting researcher position in the University of Cambridge. Prior to that, I was a research scientist at Samsung AI and I did my master's in the university of Cambridge. Also, I used to work at CERN back in the day. My interest"
  },
  {
    "startTime": "00:06:00",
    "text": "generally revolve around the areas of distributed mobile systems efficient deep learning as well as federated learning The brief overview of our talk today is going to be like some background and motivation on like deploying machine learning on device the challenges and directions that exist. Then I'm going to present our work on MelodyCoint and last I'm going to close with some key takeaways and top and challenges in future directions. Right it is has been in the past that there is a trend of networks, as in mobile models, are getting larger and bigger in their pursuit of file accuracy and we're entering NIRA that is that of hyperscale models At the same time, a device are getting ever more increasingly powerful in their attempt to be able to execute this networks So we have various use cases coming up ranging from self-driving cars to immense in painting and outpaces painting, web page interaction with various assistance assistance as well as voice assistance such as the case of Brave Leo in the browser or a Google assistant on pixel 8 So some trends are that there are in your architectures in the blocks such as l lamps diffusion models nerves and state models At the same time, we have multiple neural networks recorresiding on the device, either in the same application or across different applications, but running at the same time And also we have the pressing need for privacy preserving machine learning either what are we talking about, inference, or when we're talking about training or personalization of these models on the user device"
  },
  {
    "startTime": "00:08:00",
    "text": "on user data so a very brief introduction on transformers. They were introduced by Google a few years back, 2017 Their great characteristics are that they can attend to long context dependencies They parallelized sufficiently well so we're able to train them quite efficiently. They support various modalities, such as sequence of tokens, which can be text, can be speed or like a conversion of the vision and they can scale to large sizes without performance plateauing. This means that more data you fit the better they become This is what they look like. I want to just pay attention to the main architecture that it consists of an income and decoder. And the decoder is auto-regressive. That means that when you have a sequence of time attention to the main architecture that it consists of an encoder and decoder and the decoder is auto-regressive. That means that when you have a sequence of tokens, it generates the next token and it goes in it iterative manner. So x4x1 up to xT, it generates xT1 plus 1, and then this goes into the sequence of token and it goes to t plus 2 as you can understand there is a lot of compute being the same so there is a key value cast that of optimizes a lot of computation and saves a lot of computation and pre-fill operation fills the cache, whereas generate production the next token The former is a compute bound operation, whereas the last is usually a memory bond operation and sometimes terminology that is going to be useful later on is the complex size, which is the maximum window of tokens is that the transformer can attend to. It's the maximum generated length, which is the maximum generated tokens in the output. And the end of sequence token, with effectively a special token that when it when the LLM is generating that it stops generation right large language models"
  },
  {
    "startTime": "00:10:00",
    "text": "um exist from various players in the in the market accuracy efficiency and cost are the key characteristics and the training process consists of pre-training on very large corporate such as like the web then fine tuning on very specific annotated datasets for downstream tasks and usually are related for RELAEI which is human feedback or AI for alignment to either human expectations or safety this kind of values. So how can one deploy a lens in the browser? We envision two main modes. One is the application level and the other one is at the website level. The application level an example, is integration with the browser itself So imagine Leo running on premise but for inside the browser, whereas at the website level, the custodian is effectively the website and the browser is simply the vest that is running everything through Webto-Beer usual the custodian is effectively the website, and the browser is simply the vessel that is running everything through Web2Bu. So, one of the challenges Definitely one challenge is a system origin 80. You have various tiers of devices coexisting out in the world along with previous generation devices There are simultaneous workloads as I said before not necessarily neural-based but also non-neural compute you have the download latency of the model parameters as well as the initial latency, so loading them from memory into, from section memory to main memory and another problem is the sustained performance where the peak performance may not be guaranteed under continuous inference So I'm going to start now with some"
  },
  {
    "startTime": "00:12:00",
    "text": "molding point, which has been accepted to Mobbicom in 2020 The code and the paper can also be access to the line Right, this builds on top of previous work of mine but I'm going to speak a lot about this These are just for reference The question is, why should one deploy on device? One is for decentralization, democratization of access. So instead of having only access to this resources through a black box API, you know, are able to effectively control how they're using these models Next is privacy and personalization So they're able to be the custodian of data and not have their data ever living device and the device premises and they're able to personal these models on their own tasks And last one of least, it's sustainability in terms of you don't if you have easier tasks that don't need that many resources, you don't need to run it trillion model parameter just because it's the one available. So you can customize it effectively the model to your needs So there is a trend going from large language models to smaller language models and these are usually going from like several billions to like a few billion, like one billion or two billion models this is how the mold infrastructure looks like you can see two families of devices. One is the phone device you can see the bottom, and the other one is its edge devices signified by judgment on the left upper left side. The difference phone devices that you can see the bottom, and the other one is its edge devices signified by Judging Lamp on the left, upper left side. They differ in terms of like how their capabilities, obviously but also how we measure energy The mobile phones or connect are, well, we have removed the battery we are doing a battery bypass procedure and we're connecting the monsoon power monitor through probes to the battery controller doing a battery bypass procedure and we're connecting the monsoon power monitor through probes to the control to the battery controller and we're able to provide power and measure how much power"
  },
  {
    "startTime": "00:14:00",
    "text": "is consumed. At the same time, for the Jetson boards, they have integrated probes where they can measure their energy And the whole process of automating interaction as well as measuring and gathering the performance memory as well as energy data is coordinated by a raspberry Pi. We also have a the LLM Ben where we're collecting the models, the benchmark pineries as well as the output reports. This is what it looks like in reality and these are what each device is doing Also we have an infrared thermometer for being able to measure the thermal behavior of the devices So the benchmarking process looks like this For a brevity of time, I'm not going to go through every bit of detail, but effectively we have models that we're downloading from repositories be it hiding face or another source We're converting and quantizing it depending on the frame, the packet frame at hand, and we're going and we're evaluating this based on a evaluation suit We're using Lucidias LM evaluation tool and report accuracy on several tasks These are in order to be able to coach the impact of quantization on the accuracy of the tasks that is These are in order to be able to go to the impact of quantization on the accuracy of the tasks that the model is going to be tackling At the same time, we have a building process where it builds the binary for the application and through automation we're able to interact realistically with the application on the device and to measure the characteristics that we want to measure The devices, models and frameworks that we have traced are depicted in these three tables. We have"
  },
  {
    "startTime": "00:16:00",
    "text": "high tier and mid-tier devices across the board on Android iOS and Zetons We have models that span from 1 to 13 billion model parameters and the frameworks, the back-end frameworks that we support is MLC and LAMA CvP, LLNFARM is at the front of iOS on top of LAMAS We divide two families of BAT benchmarks, macro and micro benchmarks, macro run at the conversation levels. they're more realistic, whereas microbeats benchmarks are more deterministic by having always a fixed set of inputs and output totals The throughputs and discharge rates are depicted now on these various graphs On the left side, you can see the throughput and on the right side you can see the energy efficiency of each model on different devices and frameworks Pre-fill as I said before is faster than generation because it's computer bound. So the D.P is not, or the CPU doesn't wait for the memory to bring data into the compute device, the SOC You can see also that there is a lot of hydrogenation across the board based on the device, the framework as well as the model Generally, smaller models behave better, faster and are more efficient You can also see the some peculiar sorry, the Lama CPP on Android was not really optimized for GPU performance and this is why whatever you see in Lama Cp here is run on CPU whereas whatever you see on MLC is running on GPU. We have numbers for GPU in the paper and you're welcome to, you're welcome to see Also, CPU was not as efficient as GPU and yes"
  },
  {
    "startTime": "00:18:00",
    "text": "so let me just uh breeze through this In terms of power over time, here you can see depicted how the wattage flux fluctuates over different operations during deployment You can see the model loading time. You can see various inferences that happen in between, and then we're analyzing the last the sixth generation phase here, where you can see the pre-fill and generation phase One thing that is quite what was quite interesting and surprising was that the power grow of phones nowadays tend to be much larger than 10 watts that used to be the case So iPhone 14, for example, was reaching a Mac maximum of 18 instantaneous and 13.8 watts of sustained and galaxies 23 was 8.5 and 14 watts and this device managed to run from 5 and 13.8 watts of sustained and Galaxy is 23 was 8.5 and 14 watts. And this device managed to run from 500 up to 600 approximately uprooms until the battery was depleted from full In terms of quality of experience, you can see the model loading latency here and you can see that different devices offer somewhat different loading times however you can see that there is an artifact here where the Android devices offer larger GPU memory memory, unified memory, across the board, so they're able to also load larger models but it's noticeable to the user. In terms of sustaining inference, we tried to measure how performance fluctuates when you're running continuous workload and we saw that thermal throttling is being invoked on iPhone 14 Pro whereas this was not the case on judge on devices and we verified that with the events on thermal throttling is being invoked on iPhone 14 Pro, whereas this was not the case on judges on devices and we verified that with the events from the API, the program API of iPhone Generative 2 and we verified that with the events on from the API that program the programming API of iPhone to see that this is the case and on the right side"
  },
  {
    "startTime": "00:20:00",
    "text": "you see that after a full conversation with Jeffrey 3B billion model parameters, the iPhones were reached 8 degrees Celsius In terms of micro benchmarks, we went and analyzed various operations and how much of the compute is being spent across different operations fused operations here for MLC you can see Fused operations are operations that are more opt optimally defined by merging different operations of the network And in figure two you can see very clearly the memory boundness of the generation, where the GPU effectively waits for memory I.O. and doing no-op so it's waiting Last we also measured the impact of quantization across different datasets Datasets span across tasks of natural language inference and generation and you can see that different quantization levels quantization methods, as well as different parameter uh like different parameter models different impact so there was an interesting dynamic in terms of what is the case last we verify how, so if we don't only, we're not only limited on the premises of the device and we can span across this different devices, so float to ambient devices such as the Jetson and Anons, we want to verify how computation works how well it works it's significantly larger than the on-device computation and also it becomes more efficient You can see also here different TDP modes that this Jetson supports, which might be effectively different devices that coexit at the IoT domain. So imagine that such a device could be, for example, collocated"
  },
  {
    "startTime": "00:22:00",
    "text": "with your TV, for instance, if we're talking about the consumer home The summary key takeaway is are that we performed the first study for the feasibility of deploying LLMs realistically and mobile and the devices that can act as a basis for future research. We offer an open benchmarking framework that people can use and extend that allows performance energy and thermal as well as accuracy evaluations of transform models on devices And the key takeaway is where there is performance heterogeneity, that the on device inference is still memory bound but also thermally bound. Quantization can come at a non-negligible accuracy cost and tractability does not necessarily imply deployability so the quality of experience is severely affected on model that are more than 3 billion parameters in terms of future directions there are various things happening in the field. We have noticing model changes, so the rise of smaller language models and that are used for specific agent like behaviors and your routing across different models for invoking specific behavior as well as moving from you unimodal to multi-model models such as models that can interact with vision for example as well as text on the model compression side of things we have lower-rank approximations actually on top of quantization that already existed taking over as a performant variant. And we're noticing also broader support for dynamic graph models such as models can do early exit skip decoding or speculating decoding In terms of execution, we expect that the new generation of NPU hardware will be specifically designed for efficient matrix and matrix to matrix multiplication, as well as offer more efficient operation support"
  },
  {
    "startTime": "00:24:00",
    "text": "and this see this will boost the energy efficiency and the thermal behavior of the device Last in terms of deployment, we are already noticing the hybrid collaborative execution with TEs An example is Apple intelligence of loading paradigm And we are also in this anticipating a deployment paradigm where effectively the backbone of the model can be part of a middleware from the operating system and then different applications will be able to tap on top of it and build different adapters, for example, for downstream use so these are giving different dynamics some acknowledgments to my collaborators and yeah, I'm open to questions. Thank you Thank you all, Stefanos. Do we have any questions? either remote or in the room? I have a question to start us off maybe It seems like you were evaluating open source models on these devices largely. Do you have a sense for what? the model performance might look like for the LLM shipped by the OS? by the device? to manufacture themselves like Apple or Google? That's a good question. I'm not sure so for Google on that definitely not sure what they will be seeing. That's probably going to be a Gemini variant that they're already talking about, but they don't publish about Usually it depends on so effectively if it acts as a foundation for others for other applications to build on top they need to have a generalizability aspect"
  },
  {
    "startTime": "00:26:00",
    "text": "covered. So the more parameters that this backbone have, the better it will generalize across different tasks obviously but at the same time I don't expect this to be large than 10 billion model parameters because it won't be supported by devices for Apple Apple's case I'm pretty sure that like the first variant of Apple, intelligence is only supported on the 50 Pro line, which comes with at least 6 gigabytes of memory So that's my that's my understanding right now for things to come also I expect that the first variant will have first part applications first and then third first variant will have first-party applications first and then third-party application might be supported along the way Can you say more about that, sorry, the first-party versus third-party use cases? Yeah, the native application will be the first clients for this paperwork and then third-party applications will prop say more about that, sorry, the first-party versus third-party use cases? Yeah, the native applications will be the first clients for this paperwork, and then third-party applications will probably take over Gotcha, um other questions for seconders? Jeff go ahead Have you looked at the accuracy of the ends versus the performance that you are evaluating? especially against trying to break the model, bypassing the guardrails, or having hallucination into the answer? Because I mean, those answers are like very low value but still consuming resources So did you make any attempt to measure that?"
  },
  {
    "startTime": "00:28:00",
    "text": "we did not this would only be covered from how quantization can affect this behavior so for example if a model is alternating more when you're quantizing to a lower bit width, that would be covered by the accuracy method But other than that, we did not notice. Well, we did notice such a behavior to be honest it was on three quantized models If I remember correctly, it was some variant of Gemma, but I might be wrong about it but we did notice this this behavior yes and when a model hallucinates, we also notice that it tends to be more verbose than necessary so it produces more talking Cool if there are no more questions we can move on to the next talk. Thanks a lot, SEPNOS. Let's have a round of all applause. Thank you. Thank you So hello everyone My name is Anton Fusanko. Today I have the pleasure to present you work that I've been doing with my colleagues, Kluji Yanone and Malkei-Richach on the Aryan Protocol, which is a privacy-preserving network layer protocol So in this presentation, I would present you this protocol and the reason why we did the design of this protocol is the following. In fact, we wanted to design a data plane network player protocol allowing a source S to send the package to a destination D anonymously"
  },
  {
    "startTime": "00:30:00",
    "text": "And by anonymity here, we mean that the source goals is to present an adversary in the network from associating it with the packet destination. So we don't want to someone observing the packet in the network to say oh this source is talking to this this this this and we want to all to prevent two consecutive packets to be associated with the same flow in the networks that means that if you prevent association of two packets to a free then you make attacks based on traffic flow creation more difficult to perform So in the threat model for this protocol design we consider a global attacker that is observing communications on all the network links. So if you see on the picture on the right, of the slide all the links are colored red because we consider that the little attacker can wiretap the links and some nodes some routing nodes in the network can be corrupted, meaning that an attacker has control of the links. And some nodes, some routing nodes in the network can be corrupted, meaning that an attacker has control over the cryptographic materials of those nodes Given that we want to operate at the network layer, we have some performance constraints. In fact, if you look at the literature and we'll talk later about routing, you have several designs for onion routing protocols that use public key cryptography. The problem is public key cryptography is that it's very difficult to operate that line rate on the road so in the design constraints of our protocol wanted to use symmetric key cryptography we'll see that we do that as the expense of some anonymity protection, but performance requirements were quite strong for us And from a performance constraint, we also want to"
  },
  {
    "startTime": "00:32:00",
    "text": "change the possibility to have the possibility key with every packet to preach attacks So, as I mentioned, in the previous slide, we, when prevent attacks. So, as I mentioned in the previous slide, when we tried to design our protocol, we quickly went to onion routing and we found a very good definition of onion routing in a paper for christian kuhtz and martinbeck and torsten shuffer called breaking and partially fixing, probably secure new routing, which I encourage you to read because it's a tremendous paper So this paper gives a formal definition of new routing as a set of three primitives, a key generation algorithm sending algorithm to form the onions that you are going to send over the network and an algorithm called PROC Onion, which defines how a node in the network process a packet to translate it to the next node So, associated with this formal definition of a new routing, we have three formal properties for onion routings and ensuring those formal properties translates in your protocol being able to protect the anonymity of people involved in the communication So those three formal properties are layering the anonymity of people involved in the communication. So those three formal properties are layer and linkability. In fact, in layer and linkability, we mean that an adversary that is accessing onions before and after an honest relay can't determine whether the two onions are related or not. So it means that a packet that is crossing an honest node can't be a packet in ingress packets and igress packets coming in and out of an honest node can be associated. So it prevents a"
  },
  {
    "startTime": "00:34:02",
    "text": "attacker to perform correlation attacks the second formal properties that we are interested in in onion routing is taland indiscrimination. It means that an adversary having access to onions after an honest relay can't figure out which was the road taken by the onion before the honest relays So when you see egress packets from an honest node you can't see whether the packet has taken a long path in the network, a short path in the network, and you can't figure out where it was coming from. And the last, but not least, property, because it was, it has been used for several later path in the network, a short path in the network, and you can't figure out where it was coming from. And the last, but not least property, because it has been used for several attacks on practical onion routing protocols is onion correctness onion correctness means that a node that is receiving a packet needs to be able to compute to make sure that the packet is that it received is authentic that an attacker has not modified part of the payload to perform correlation attacks. This union correctness property is very important because it has been misregarded for several years and in the literature about privacy preserving network protocols there are lots of attacks that are performed that are based on the manipulation of the payload and trying to correlate payload manipulation in and out of a network to correlate packets together So this onion correctness properties payload manipulation in and out of a network to correlate packets together. So this onion correctness property is very important So now that I set the scene for what we wanted to achieve and what was onion routing in general, I will do a quick review of state of the art and I summarized it in the"
  },
  {
    "startTime": "00:36:02",
    "text": "following table. So in this table, we see a list of work from state of the art. You have several protocols like Sphinx, Tor, or ornets, taranets, and a set of properties. So the characteristics of the protocol and the security properties here And from the state of the art, we see that the perfect solution as a green light on all those properties but in the state of the art we see that no protocol is addressing all the properties that we want to achieve with privacy preserving network clear protocol So in particular, first, things and which is one of the major, the major onion route protocols that has been presented in a paper from 2012 and it's enhanced versions that has been announced by christian kuhtz and her colleagues Those protocols are not usable at the network layer because they are using public geography From a security perspective, they are very nice, especially the analysis version like this income, but from a performance perspective, it's not feasible to run those protocols at the network layer Thor is very wide used and very very popular when you talk about privacy present protocols. The issue is that you have a number of attacks that have been performed on to talk about privacy presence protocols. The issue is that you have a number of attacks that have been performed on tours that prevent them from being completely secure and operating TOR as a natural clear is a challenge And you also have protocols like Onet and Taranet that has been designed in ETH Zurich in the frame of the future network architectical science Scion. Those protocols are also very important but given"
  },
  {
    "startTime": "00:38:02",
    "text": "the design, some security properties are not in short so yeah so this per op integrity integrity is the fact that you can compute on the packet authentication tag that allows you to detect manipulation of the packet. So the possibility to perfect correlation attacks based on manipulating the payload so on it, Sphinx and the Tor are not immune against those attacks And in Thor Ornett and Taranet, the fact that you use a cell ID or metadata in the packet prevents those protocols to be protected against the fact that a malicious node can correlate packets together if it has access to the perfect material of a given node in the network So now that I presented the shortcomings of state of the art, I will try to present you how we aimed at harvesting those shortcomings in IM So in IAN, we want to prevent against the attackers that I described earlier and to avoid having to rely on the SIDS third party, we used a source routing model So using the source routing model allows the source to control the path that is going to be taken in the network, and the source is able to determine the trust it has in the route it received route it received from an oracle. For now, we are continuing doing it now considering it an oracle We are going to use sequential encryption on on packets so yeah"
  },
  {
    "startTime": "00:40:02",
    "text": "every node on the pass the packet will be re-encrypted And we'll see how the integrity of the whole packet will be checked at each step and for performance reasons, we are going to avoid as much as possible the use of public equity periphery because it's not feasible to do that for all the packet We'll use an Sphinx packet at the beginning to set up master symmetric keys between the source and every node on the path and then we are going to use the key duration mechanism to derive per packet symmetric keys that we are going to use to generate the cryptographic material that is used to process the packet packet So in Aryan, we have included two technologies that we think are participating in improving the privacy properties of protocols from state of data art. The first, technologies that we designed and included in Arian's design is the anonymous key reference. In fact, if you remember in the state of the art, I mentioned that for instance in Torr the use of a cell ID was opening the possibility for an attacker to perform correlation of two packets to a given flow. In fact, in Tor's design, packets belonging to the same flow share the same cell ID a given node And this is a major attack vector for Torr, and it has been used with several attacks that have been published And we struggled a lot in the design process of variants attack vector for Torr, and it has been used with several attacks that have been published. And we struggled a lot in the design process of Arian to avoid having such mechanism and such identity for metadata allowing to an attacker to correct packets to the same flow and"
  },
  {
    "startTime": "00:42:02",
    "text": "the light came with paper called Anonymous authenticated encryption that was published in 2000 I think And in fact, in this paper this paper they say that they are going to use the reference of a key by saying that the trailing bytes of an encrypted content will contain all zeros and if you have a bag of keys and you are and you are and you are trying to decrypt the fact, the encrypted content, you realize that you have used the proper key when you find the trailing zeros this is not an acceptable method when you try to encrypt and decry packet at line rate using symmetric keys because of why method when you try to encrypt and decry packet at line rate using symmetric keys because otherwise you would use public geography. So we modified the method of it in the following way. In fact, we inspired from the enigma attack, the enigma yellow attack that at the beginning of a package we use the same bite pattern and we encrypt it with a set of temporary keys So from the master keys, that we change between the source and a given node we derive a set of temporary keys and we encrypt the beginning of the packet with those temporary keys. And we store the results of those encryption in a dictionary when we receive a packet we look at the first bites of the packet and we compare it in the dictionary that we got to determine whether it matches an encrypted pattern that we have pre-computed When we find this pre-computed pattern in our dictionary, we can retrieve the temporary"
  },
  {
    "startTime": "00:44:02",
    "text": "key, and we can use this temporary key to process the cryptographic materials that we have going to use to process the packet on the network can retrieve the temporary key, and we can use this temporary key to process the cryptographic materials that we are going to use to process the packet on the... The advantage of this is that you don't have the same reference in clear in the packet because you are using encryption of a given pattern so the pattern changes with the clear in the packet because you are using encryption of a given pattern so the pattern changes with every with every packet so that's that's fine and it can be used to create packets together it's a moving reference so it prevents link packet to a session for an external observer If we are in a state where there is a loop on the pattern, then we change key and we go back to a new cycle The second technology, we used in Arian is the routing element chef shuffling. In fact in private preserving network protocols, using a source for writing approach one aspect is that you have to store the route in the header and the length of the road needs to be hidden to prevent an attacker from getting information from the observation of the length of the route To do that, Sphinx use last in first out data structure, which is very interesting but processing this last-in-first structure on a router is time-consuming because we have to copy the structure and rewrite all the structure on the route This involves memory and the router don't like copying and changing large amounts of data in packets at Earth. So instead of doing this, we have a routing element vector In this element, we place the"
  },
  {
    "startTime": "00:46:02",
    "text": "segments that are describing the routing instruction a given node as to you at slots that are given by a pseudo random permutation and when node processes a packet, it can decryptomy given by a pseudo random permutation and when a node processes a packet it can decrypt only a slot plus the routing element vector in the same go to retrieve the routing information and send the packet afterwards So now that I gave you the description of the primitives that we have been using, in the design of the ion protocol I will try to define you with hands how processing the process goes. If you want a formal definition, of the protocol, you can go to the archive paper that is listed in the agenda of the meeting So, we come from, we state that S wants to send a packet to D and S has exchanged a set of master keys, KSA, KSB, KSB, KS C and KSD with A, B, C and D KSD, with A, B, C, and D. Okay, so the exchange of those keys is done with another protocol that is out of scope of this presentation for now You can use enhance things as I mentioned earlier So first, S will create the packet to send to D through AB&C and c to do that it will take a two pass approach. First, it will retrieve the address of all the nodes ABC and d and the keys and it will initialize a routing element vector, which is this vector of bytes of a fixed length and it will compute"
  },
  {
    "startTime": "00:48:02",
    "text": "so during the permutation over the address to place the address length and it will compute so during the permutation over the addresses to place the the addresses into segments here that are placed in slot positions that is given by a pseudo random permutation so here you send the vectors that segment A's position is here B C and D okay so now, from the master keys that has been exchanged between S and A, B, C, and D, we compute a temporary key using key direction mechanisms such as the UKP key duration mechanisms used in credit cards and with temporary keys that we retrieved, we compute absurd random permutation of the length of the packet plus the length of a round element vector here. So we have those byte strings we keep them, and we see we have those byte strings, we keep them, and we start processing the, creating the packet in the following way so we start from the vector of bytes that we initialized and we place here a segment A segment is a prefix with a fixed pattern that we are going to encrypt to reference to our key And a segment A star. Segment A star is the address of the next node for segment A, so the address of B and the and the authentication tag set to zero because we are going to include an authentication tag later So we include segment A star here and we exhaust the whole packet with Rho KA include an authentication tag later. So we include segment A star here, and we exhaust the whole packet with whole KSA header. So it's part of it's this part of the pseudo random bytes that we generated with the temporary key. So"
  },
  {
    "startTime": "00:50:02",
    "text": "we'll free this key this this this vector here and we continue the process with by placing B at the slot and and zowing with raw K sb, etc up to D. Yeah. So at the end of this process we obtain a byte vector in which the which is initialized and sequentially encrypted with the Rho KS header part but the problem is that with this structure we don't have the possibility to get authentication tag for the whole packet so to include this authentication tag in the header we are going to add the page the whole packet. So to include this authentication tag in the header, we are going to add the payload at the trailer of the packet and we are going to exhaust the whole packet here with ROKSD header plus ROKSD payload So we obtained this byte vector here and we're going to place the segment D star at the beginning compute the mac of the packet here and replace segment D star here with the same with the authentication tag we computed Xord with ROKSD routing element. So the first part of the pseudorandummentation that we computed earlier. And we are going to copy this part this byte part in the routing element vector at the slot of segment d star so we do the same in the same way with c b and a and in the end we obtain the following packets so here we have"
  },
  {
    "startTime": "00:52:02",
    "text": "segment A with the authentication tag that there's been encuted once. Segment B with the authentic tag encuted twice, C three times and B four times So here notes that segment N with M contains the encryption tag and segment N star doesn't contain the encryption tag, so the encryption tag is set to zero so now s is ready to send the packet to A. So it's going to send a packet to A here So when A receives the packet, it reads in the common header, the slot position of the routing element vector, the routing element that it needs to read in the routing element vector So it says that it's in position 0, 1, 2 3, 4, so 4 at the beginning of this routing element, it takes the first bytes and try to match it with the table it has to reference temporary keys If there is a match, it retrieves the key compute the pseudo random permutation that is used to exhaust the table it has to reference temporary keys. If there is a match, it retrieves the key, compute the pseudo-random permutation that is used to dissolve the packet, and dissolve the routing element with the first part of the pseudo-random permutation to retrieve segment AM From this, it can compute the authentication tag to see whether the packet has been managed or not, if the packet has not been manipulated it can exhaust the whole package with the trailing by packet has been manipulated or not. If the packet has not been manipulated, it can exhaust the whole packet with the trailing bytes of the pseudorandum permutate it computed to obtain the following set of packets and relay the packet to the next node B relay the packet to the packet to the next node B. B will process the packet the same"
  },
  {
    "startTime": "00:54:02",
    "text": "way, C will process the packet the same way, and the packet will reach D. So at this stage, D doesn't know whether it's the destination or any damage intermediate node. So when D will receive the packet, it will catch the first bytes to reference the key, retrieve the key, compute the pseudonal permutation, and when it computes a segment DM it relies that the next element is himself, so it means that it's the destination. So at this stage, D can compute the authentication tag for the packet verify that the packet has not been manipulated and decrypt the whole packet or transplanted it to the upper layer. Okay so now that I described you by hands how the protocol works how does it compare with state of the art? Did we manage? to enhance situation a bit? So here at the bottom of the picture you see how we perform with iron whether we are using the routing information vector with the pseudor random permutation of the position or without using the routing information vector meaning that we are using the life photo data structures that I told you we would we would like to avoid the reason why I had this comparison here is that in discussing with a reviewer in conference when we submitted the academic paper for this research work, we realized that the use of the routing information vector was leaking some information, some privacy information So, in fact, you could you could say that two packets in and how to honest router would not be belong to a same flow is they use the same position in the routing element vector. So this leaks some in information about for"
  },
  {
    "startTime": "00:56:02",
    "text": "an HVacker willing to correct packets so this is why we mentioned that protection again flow correlation and so to correct packets so this is why we mentioned that protection against flow correlation and the source passing this security is partial here the limitation we have with the perfect protocol is that given that we are using symmetric cryptography, it's impossible to have source stain and distribution limitation we have with our perfect protocol is that given that we are using symmetric cryptography, it's impossible to have sourceding and indistinguishability because a node that is processing two packets with the same master key knows that the packet comes from the same source, maybe they have taken other paths to reach it, but still we know that the same source has generated those two packets. A way to mitigate this at attack is to have a set of keys between one source and one nodes and you can shift keys for some packets but it's taking it lot of performance burden for ensuring this property Just a time check, John, you have about three minutes okay then I will jump to conclusion Okay, so where do we stand? So we did the academic part of the work on this protocol, and the reason why we wanted to present this in IATI is that we think that this protocol can be adapted in IETF protocols In fact, we are thinking about design ARIAN either as an IPV6 extension header inspired by the SRIPv6 header or as a new quick version taking advantage of the work on masks on the max quick proxy and in particular in the forwarded mode the Mask UDP relay method We have a first implementation of Aryan in Rust"
  },
  {
    "startTime": "00:58:02",
    "text": "contributed by Mail Kerrichard, which is using our model SRV6 header in the implementation We show from our benchmark that the processing of packets is quite efficient compared to compared to Sphinx protocol and with think that iron could be uh building block in a more global complete privacy preserving a layer three set of networking protocols So thank you and I'm willing to take questions I see that jonathan hui in the queue, so please Just, we have about two minutes, so please keep your questions and answer for sure. Go ahead, Jonathan hoyland, Claupler. You mentioned that you have formal proofs of security on your previous slide I'm surprised by that because the anonymity trilemma says that without a certain amount of cover text or a certain amount of latency introduced it's impossible to be secure against correlation attacks In fact, yeah, it raises on of latency introduced, it's impossible to be secure against correlation attacks. In fact, yeah, it rather than formal verification, it's a formal description Formal description is a photo protocol. Oh, okay. So, but formal description, formal description of the protocol, sorry. Ah, okay, so, but... How do you, how do you know that or other, without adding those two things? how do you prevent correlation attacks In fact, uh, reasoning behind being behind the following. In fact, by using the anonymous Key Reference, we try to get rid of metadata that is allowing correlation of packets So this is where the"
  },
  {
    "startTime": "01:00:02",
    "text": "started from And in fact, in our form description of the protocol we show that if you cross an honest node, with the gaming proof you can't have a correlation between the inside and outside package So imagine I you have your network and you have the attacker is the first router and the last router And it does a very simple counting attack, it just says, we know that the source sent 10 packets we know the destination received 10 packets right it's doing that attack uh yeah i sorry i know i know i get your point. In fact, this protocol has to be used together with uh traffic jamming or packet pacing sort of algorithm to prevent the pacing from being an attack vector. In fact, what we try to achieve with our Arian is that structurally in the protocol, we get rid of metadata that allows correlation attack to be performed based on the structure of the packet. Then you have all the traffic dynamics based attacks for packet correlation This is orthogonal to our work, and we see think that in the literature you have several works on a packet sizing method so when you pad the packet, you can adapt the padding to the kind of network that network traffic that you are transporting and you don't sacrifice too much performance to prevent a correlation attack. But this thread of work is very interesting. We think our work is compatible with this work but we didn't uh in include this in our protocol Okay, thank you all. Thanks, thank you"
  },
  {
    "startTime": "01:02:02",
    "text": "to both the presenters for great presentations and yeah, thanks all for coming up. See you next time And So thanks to Alison for taking notes and Colin for being the representation for the chairs in the room Thank you everyone Thank you"
  }
]
