[
  {
    "startTime": "00:00:04",
    "text": "All right, everybody, everybody here Thanks for joining CFRG Research Group meeting for IETF 120 Vancouver Are your chairs, two of which are here? alexey melnikov nick sullivan, who's me, and Stanislav Shishmayev He is online on The Meetecho Mute Next slide, please All right, so just some administrative reminders here. The session's being recorded We do have a minute taker in Hedge doc Follow along. Additional contributions are welcomed Here's the participant guide. These things should all be clear to you for being here at the meeting at the IETF for the last three days The minutes are available, right? here. And next slide, please All right, this is the Notewell This is the IRTF note well, but it does follow the IETF Intellectual Property Rights Disclosure rules these things should be well known Read these. If you haven't read the read the read the And we'll move on from there. There's also co- of Conduct if the IAT and IETF, follow this, we have RFC 777 anti-arassment The Ombuds team is available. If you have any questions or questions, either about this policy or anything to raise regarding it Next slide, please. As a reminder, this is the IRTF, not the IE to raise regarding it. Next slide, please. As a reminder, this is the IRTF, not the IETF. We're very closely linked, but the IRTF focuses on long-term research issues in parallel with the IETF, and we collaborate with them as well. As a reminder, the IRTF public informational and experimental documents"
  },
  {
    "startTime": "00:02:01",
    "text": "and the primary goal is to promote the development of research and collaboration and teamwork exploring these issues that connect cryptography to the IETF. There's RFC 74 the primary goal is to promote the development of research and collaboration and teamwork, exploring these issues that connect cryptography to the IETF. There's RFC 7418. Yep All right. So the agenda and slides are available here The Datatrackers on this link And here is our outline We have a very packed agenda. So I'm going to try to speed through this intro. We've got six great talks about different whether it's individual drafts or adopted drafts or new ideas Is there any agenda bashing? going once going twice? Thank you All right, so I'm going to do a quick overview of the document status There's been one new RFC since the last IT you. All right, so I'm going to do a quick overview of the document status. There's been one new RFC since the last IETF, which is Frost, so congratulations to the Frost authors for getting this through Yay. All right There are no current documents in the editors queue, but the ICA review, there are two things that we've been working on, one for a while, one for not as long but the properties of AED algorithms is, if there's any, any last words on this for anybody working on 80 AEADs, this is sort of a last call And Kangaroo 12, the hash function, extendable output function is also right at the end. There are no drafts tonight IRSG review and there are two in the IRTF chairs queue As a reminder, we have quite a few active drafts within the CFRG so I encourage these authors to continue working on them converse about them on the list. All of these documents are updated and none of them are currently expired"
  },
  {
    "startTime": "00:04:04",
    "text": "We haven't adopted any documents recently and there are no documents in adoption call, but we do have two expires drafts, pairing friendly curves and BLS signatures and there's a number of inactive and archived drafts that were adopted in the past but haven't been continued continued This is a new slide. There's quite a few erratas for a lot of these documents. So what the chairs would like to do is invite folks who have expertise in these areas to take a look at any errata that is referenced that hasn't been resolved yet. And if you have a contract, or a confirmation of any of these, these would be appreciated, but in the next few weeks, the chairs are going to be going through and attempting to close or address all these, and there's three pages of them So there's several drafts and also quite a few new erratas that have been raised since the last one so including seven 8032, 80, and new ones with respect to the 9,000 series, so HPKE has quite a few and on the third page we also have two new ones for 9497 So any help? with closing these would be appreciated Next slide, please Okay, this didn't update on my slide, my page, but um do have a crypto review panel. These are these volunteers are also heroes along with our note takers. We have a current slate of members. Several of them are here as well. So thank you everyone for the contributions on the Crypto Review page And as a reminder, the Crypto Review Panel,"
  },
  {
    "startTime": "00:06:01",
    "text": "not only helps out with finishing the documents here in the CFRG, but also is a resource for the IETF for it asking questions or clarifying things with regard to how to safely use cryptography and IETF protocols Once you'll see you With that, there's no agenda bashing. Let's kick off the first presentation There is only one revision, right? Thank you, sir me? Yeah, so my name is Klaasamjid and I'm going to present in partially blind artist's signatures based on the draft that's linked in the slides. It's a joint work. It's scott hendrickson, christopher janz Wood, and Kevin You. The names are in alphabetical order. Next slide so over the course of your stock i'll go over some motivation for blind signatures, partially blind signatures, RISO, blind signatures, give a high level overview of RIS blind signatures and how do we change them into supporting public metadata? and then go over some benchmarks for our implementation and then talk about current status of the document and the implement next slide so blind signatures are pretty well motivated at this point they are using privacy paths and under the hood in Google VPN in iCloud private delay in avoiding repeated recovery are pretty well motivated at this point. They are using privacy pass and under the hood in Google VPN in iCloud private delay in avoiding repeatedly capture solvings and as an anti-fraud measure in private click measurement and as denial of service to defense and door and so on next slide so for partially mine signatures, they also have a fair share motivation recently there was a public metadata issuance draft adopted into the private pass working group, which depended on two"
  },
  {
    "startTime": "00:08:01",
    "text": "token schemes that support public metadata One is privately verifiable variant, which is based on verifiable oblivious pseudonym functions, which have a flavor that supports public metadata and the second one was a public verifiable variant, which was basically partially our sublime signatures the draft that the stock is based on moreover blind signatures when you enable public media with them they offer additional utility for example you can think of your tokens needing some expiration time and you can essentially tie this expiration time cryptographically with the token if you enable public motor data data. Other than that, you can also do things like service multiplexing where you can essentially maybe you want the same signing key to issue tokens with different egress policies and using public metadata you can achieve that. Moreover, you avoid the one key for public metadata approach which can have a drawback like you have to fix your public media choices ahead of time, or you have also like if you have so many keys for public metadata then key management becomes a headache in production deployments next slide So for brand signatures, last year, an ROC was adopted, was published based on the fact that it's a simple scheme. It has a one round scheme with the server token insurance is stateless and it has by widely supported public verification. So it's national to think of a natural to think of standardizing the way variant, the public metadata variant for RSI-9 signatures Next slide. So now I give a quick high level overview of RSIBline signatures next slide so for example you have a client in a server in this diagram. The server has the public key and a secret key, which sends a public key to the client The client wants to blind sign a message Next slide. For that, it will use the blind protocol. It will take the public key and the message"
  },
  {
    "startTime": "00:10:01",
    "text": "and output a blinded message and then inverse. It will see its inverse in its state and send the blinded message to the server The server will execute the blind sign protocol taking the secret key and the blinded message and output a blinded message to the client. Next slide Now the client will run the finalized protocol taking in the public key, the message, the inverse, and the blinded signature and output a final signature. Next slide. So a couple of things to note here is that the input message is not directly blinded, it's just encoded and the RFC suggested PSS encoding for that. Secondly, the signature is verified as subroutine of the finalize protocol. The finalize only takes the public key. So signature is like publicly verified which is a good thing about this protocol Next slide. So now I'll talk about partially blind art signatures how do we change the scheme to that? Essentially, this is based on works from Abe and Fujisaki and Abe and Kamenish in 1996 and 1997, respectively We have an accompanying paper that is currently in submission which sort of has the accompanying security proofs and security analysis of the scheme, along with that is a minor things. So, to enable this public metadata in blind signatures, RC blind signatures, we assess essentially add additional parameter to all the protocols, signing, signing, finalizing, errorifying. We basically pass the public metadata to them. Next slide The main tweaks here are the main core things here to note is that instead of input message just being encoded by PSS encoding, we augment the input message with the public metadata and then encode this this augment input message using PSS encoding. Secondly, you do not use a standard public key, R is a public key you derive the public key using the public metadata. So here this H function that takes the public metadata, there's a type it should also take RSA modulus then. So and for implementation ease,"
  },
  {
    "startTime": "00:12:01",
    "text": "we use HKDF in practice Now, the output of this HKDF needs to be co-prope to 5N, where N is the RSA modulus And in order to do that, we do two things. We use strong RSI modulus, which is a product to safety primes of a certain size. And we make sure that the output of HKDF is size restricted according to the size of the safe primes. And also it's always an odd number so that it stays co-prime to firefine Detail of this is in the draft as well as in our paper if anybody wants to take a look. All right, next slide Other than that, we proved one more for unfurability for this scheme which basically means that if I asked the server for end tokens on passage and public metadata of my choice, I cannot forge an additional token and also unlinkability under the same public metadata, which means that if I ask the server to sign two blinded messages under the same public metadata, and then the server turns me to sign if I ask the server to sign two blinded messages under the same public metadata, and then the server turns me those signed messages, and I run the finalized protocol on both of them and shuffle the results and send it back to the server The server should not be able to link back the final signatures with the inputted, blinded messages that it received from me at the beginning We also take care of domain separation where we make sure that all the RSA moduli, sorry, we make sure that for the same public metadata and different art moduli, the output of HKDF will be different So the direct public ease will be different. And then we also make sure that input to the hash function in the input in the message augmentation as well as in public key derivation or domain separated by using standard techniques like adding tags and stuff like that and moreover as the public key are non-stander sizes we may we added an allow service section to the draft. Next slide so a little bit about the benchmarks of the implementation"
  },
  {
    "startTime": "00:14:01",
    "text": "Next slide So on the right-hand side of the column, we have the partially blind RIS signatures, which have like the blind and the finalist protocols hover around a millisecond and the blind sign hours around 4.3 milliseconds. We have two implementations that are currently available online. One is in C++ and the other one is in Go and I thank Chris Wood for providing that implementation Next slide And current status, next slide sorry. Yeah, so at Google currently this scheme, we are using it under the hood in Chrome IPE protection project in Pixel VPN because it's publicly verifiable and it fit our bill there. And there's a third-party audit of the cryptography involved it's unpublished but it's a available online if somebody wants to take a look Furthermore, we have an academic paper that with the company's security proofs and security analysis, which is currently in their submission, and we put an e-print out last year but it's a little bit outdated and we will planning to kind of update hand in hand with the submission So it should also be available in a few months the updated version the outdated version is still there if somebody wants to look at and yeah i guess the question becomes is anybody interested in adopted? Yeah document? Yeah, next slide next slide Yeah, thank you Any questions or comments? We've got a few minutes for conversation. I know there's several related works that have been explored by the CFRG Yes, Scott Extremely minor question I thought I heard you say that the signature were unlinkable by public data"
  },
  {
    "startTime": "00:16:01",
    "text": "by the server. Did I hear that correctly? So they're unlinkable The unlinkability proof, like, assumes, under the same public metadata, you cannot link them, right? Because if you have to signatures with different public metadata, you can already distinguish them Okay, but doesn't the serve the blind server need to under know what the, uh, public data is to sign yeah that's what i'm saying Like, if your messages have different public metadata, the server already distinguishes them. Oh, okay So what you're saying is that even if they have the same public data the sign or can still you still has basically blinded. Yeah, yeah, yeah, under the same angle they're blinded my i misunderstood thank you very much Sophie? Hi, yes, I'm Sophie from Google Do we have any plans of making this PQC? Like, I think there's some lattice-based schemes that we could use instead of RSA. Do we have any work in focus for that? We, I don't think we have explored that right now now Watson, Akamai, my question is, is the security model? just one more signature for it? My question is, is the security model just one more signature for each metadata, or? do we also want to say that you can't extract a signature for different metadata if your own to say that you can't extract a signature for different metadata if you're only allowed to get signatures for a certain? set of metadata? Sorry, just to clarify, you're talking about the one more unforgeability? Yes uh, I'm asking, you know, if I'm allowed to sign under HMD1 and HMD1 should it be hard for me to"
  },
  {
    "startTime": "00:18:01",
    "text": "get a signature under HMD3? No, I think the security definition right now, it like covered that case. Yeah like, covers the case where you cannot forge an additional signature for any public metadata Yeah there's a, um, there's a, um, recall, there is an additional caveat here that I think the scheme probabilistically uh fulfils their definition here Does it answer your question? Or I think I need to read the paper and think harder I'm just sort of thinking about the same thing it happens if you do PKCS 1.5 without that initial bit, where there's sudden relations among the hashes that you can expect I'm not understanding the question because of the noise in this. We have a little bit of noise in here, Watson. Let's take this question to the list and also continue discussion about this draft Scott, do you want to coming to the mic? Okay, I believe this is a Watson's question Is there anything to prevent, say, two metadata? from having one being the exact multiple? of the other, like one metadata hashing to? seven and another one hashing to 21? All right, right. Okay. So we analyzed in the paper we analyze exactly that case where you can possibly potentially launch some sort of that attack and it turns out that there are some safeguards around that particular attack A, there's like the probability of finding those"
  },
  {
    "startTime": "00:20:01",
    "text": "particular ease because we also see restrict E according to size of the same prime and the size are large enough that the probability that you will run into that case is into two to the minus seven or something depending on the key size obviously that changes depending on the key size and secondly if you additionally had set up if you want to if you if you know your public metadata set at advance, you could essentially sanitize your key in such a way that you know that this case will never come up. Okay Yeah, Chris Wood a question on process because the privacy pass document that depends on this is now a working group document um what do we think the plan is for this one? It went through an adoption call last time, but it was like, there was not a lot of support for it just because, well, A, not a lot of people chimed in, but it wasn't necessarily required at the time because we were was no IETF protocol that was using it, but that has changed So I'm wondering, get more clarity on like what the actual next steps are This, you know, blocking ICE work on IRF work is seems not quite right So there is going to be a meeting or a president on Friday about potential new processes around crypto dependencies within the IETF and as of today there's no change, but I would encourage this conversation to go to the list yeah but in general I think we'll as chairs, we'll be mindful of this, that there is a potential dependency and try to expedite a minute I see see. Second Thursday and then I"
  },
  {
    "startTime": "00:22:01",
    "text": "Thank you. Thank you Bert Yeah, for the note taker at SAG on Thursday Thanks very much to the chairs for this opportunity to present I'm Berk-Kaliski and presenting joint work with Swapmeal Chef, Andy Fraggley, and Joe Harvey We'll talk about Merkel tree ladder mode at a highlight present Berkeliski and presenting joint work with Swapnilcheth, Andy Fraggley and Joe Harvey. We'll talk about Merkle tree ladder mode at a high level two things, interested in CFRG adopting Merkel tree ladder mode as a draft And then secondly, a more general question that I'll explain interested in CFRG working on modes of operation for signature schemes as an area to provide advice to protocols that are integrating some of the new post quantum cryptography standards So on the next slide, please So a year ago, Joe Harvey presented on this topic I'll give you a high level of how we see Merkel tree ladder mode. What you'll see here is a collection of three trees that can be used to authenticate four nodes. The first one has eight nodes. The second one has four nodes under it, and the third one leaves rather four leaves and the third one has two leaves and this in some places in the literature is called a Merkel Mountain Range through of people that have arrived at this kind of construction were used it particularly for an evolving message series when it's expensive to compute the signature. So the use case that we focus on in our research is DNSSEC If you're continuing to expand your zone and add more records it's convenient to be able to organize those as leaves of a Merkel tree, and as you add more, you get more internal nodes and we structure those in what we call a ladder on the left hand side, and the ladder has rungs in it So think of the rungs as the designations of specific nodes in your merkle trees or in your murkled forest"
  },
  {
    "startTime": "00:24:01",
    "text": "If you sign all those rungs, you can then authenticate all the leaves And the goal is to have some stability as the merkle tree forest evolves so that you can reuse old ladders when possible I'll take that as a short introduction If you want to take away something simpler from this slide, just think in general of using Merkel trees to authenticate multiple messages with a single signature on the root of the Merkle tree. We would call that a mode of operation. It's a way of using the underlying signature scheme to be able to authenticate multiple messages with a single use of the signature Next slide, please. Joe Harvey presents a year ago, IETF 117 and there were three next steps, request to review the draft We were planning to release an open source library, and then we plan to publish a draft on the using Merpeture that or Method. We've done all three end of the set. We've done all three. Next slide The, can you be a little bit closer to the mic. The Merkel Tree Ladder Mode draft has been updated based on feedback and evolution of the post-quant standards from NIST. And we had a hackathon at 118 on the Merkle tree ladder library and version three of the Merkle tree ladder draft just published now aligns with the way that NIST is approaching what we would call modes of operation so for those of you are following the NIST PQC discussions on PhIPS 204 and 205, you note that NIST has introduced a pre-hashing variant plus a pure signing variant To many of you familiar with that, you've seen the mailing this discussion So in order to support applications that want to reduce the operational impact, on the underlying signing process, there a pre-hash mode. You hash the message and then put it in your underlying signing. That's attractive if you're underlying signing is in a hardware security module, and it's particularly important for schemes where you make two passes over the message"
  },
  {
    "startTime": "00:26:01",
    "text": "For instance, in Sphinx Plus U-Hash wants to get the random then you hash again with the randomizer. So now if you have a multi-megabyte file, I have to send it twice into the HSM, assuming the HSM can't remember what was sent in the first time. So the pre-hash lets you do that outside, or maybe one pass of sending it in to do the hashing So as I show on the slide, NIST defines pure hashing as a way of formatting the input to the underlying signature scheme prefixing it with the 0-0 byte, and then that's followed by an optional context string If you prefix with a 0-1 byte, that means that what's going into the underlying signature scheme has been prehashed So pure, just directly the message, pre-hash, the message, prepended by a 01 object identifier context and so on. There was a lot of good discussion on the NIST PQC mailing list about the pros and cons of this approach and including a panel discussion that we participated in at the NIST PQC conference in the spring I'm not sure what the final outcome will be when the NIST 204 and 505 are published but seem to be leaning in the direction of supporting these. But if you've followed the discussion, you know that there's some important trade-offs going on Should the FIPS themselves specify both of these? modes? Would an application use both of these modes? How do you distinguish between these modes? And these are general questions that come up even as recently as Falko Strzinski's attack on PKK and CMS, if you remember what was taken place there, you had either a hash being directly in CMS, if you remember what was taking place there, you had either a hash being directly input to the underlying signature or an authenticated attributes structure where one of the attributes was the hash. How do you distinguish between the two of those? So one way is to distinguish cryptographically as NIST has proposed a zero or a one before a structure so that you're never signing something that's ambiguous in its interpretation Another way is in your application only use"
  },
  {
    "startTime": "00:28:01",
    "text": "one of the other modes. A third way, is for specific key to say this key should only be used one way or the other, so you leave it to key management Well, that's related to the general question when you're using a key What is this key used for? Because a key can see anything. How do you interpret the thing that's signed by the key? if you change the context of the interpretation? something that's signed, meaning one thing is now in another language or it's in another format syntax and so on In the discussions that I followed on the NIST mailing list, general sense I got from those who didn't want to see the 0-1, 0 and the 1 byte put into the standard was let protocol designers figure out how they want to use this let them distinguish between pure and prehashing and something else. Well, that feels like it needs some guidance. So whether NIST standardized on zero and one or does something else, it's good to have some guidance to protocol designers who are thinking about using signatures in multiple different ways sometimes hashing beforehand, sometimes not, or other modes of operation Just elaborating on this a bit more in our comments to NIST, we said for the pre-hash mode, the pre-hash exactly does that. It hashes only. Depends on collision resistant. Now, no doubts about collision resistance whether that will be secure, but we have history that's suggests sometimes when you assumed collision resistance, later on that didn't prove to be the case and some randomization was needed to protect you against the predictability in advance of what was going to be signed That was the Shaw One story. There's no input in this in the 01 case of a of what was going to be signed. That was the Shaw One story. There's no input in this in the zero one case of the public key that's being used. So any collision that is universal can be applied to anyone who's doing the signing. Those feel like design considerations for additional modes of operation. You want to one with randomization one with public key input. That's a long way of expanding on that bullet. But that's the motivation for saying, Merkle tree ladder mode is an example of a family of ideas"
  },
  {
    "startTime": "00:30:01",
    "text": "a family of how do you use this underlying signature? scheme that's being standardized, as powerful as it is in different ways, prehashing or hashing multiple messages together, how does that evolve over time? so we followed a similar technique with the latest draft of Mercantrilateral mode and decided to reserve the values 80 and 81 hex figuring they were far enough away from zero and one that they might not be reserved. There's no reservation scheme for that yet, but we wanted to be visibly distinct from the zero and the one if we're using the underlying signature scheme and the input is that hash of a Merkel tree ladder that represents multiple messages There's more details in the draft around that. We've also released the open source software implementation and if we go to the next slide and an internet draft on using Merkel tree ladder mode with DNSSEC If you were at hot RFC or at the hackathon this weekend you saw we were demonstrating and initial use we have a zone file that's up and running and when you look at the zone file, you'll see that one of the signatures is very long. It's an S LHDSA signature on the Merkel tree route ladder that represents the whole zone. The rest of the signatures are short. They're more in the size range that you'd expect to see for DNSX signatures running over UDP and so on. Okay, next slide Just a couple of final points here So previously disclosed the same statement Veracine has announced public royalty-free license following IETF note well and so forth, and you can find the IPR declarations for that. And then final slide our request would CFRG be interested in taking up MTL mode as part of researching modes of operation in general the MTL mode document has a lot of materials in it right now because we wanted to be as complete as we could on how it's specific how it works with different underlying techniques security considerations, and all those kinds of things"
  },
  {
    "startTime": "00:32:01",
    "text": "But the larger question is, how do we think about modes of operation for signature schemes? I think it's important with the arrival of the prehash versus the pure mode, with the size trade-offs that are unprecedented in terms of the operational impact and additional understanding of security proofs. This feels like the right place to pursue that further On the DNS side, we're continuing to work on the DNSSEC use case and obviously we're open to other use cases as well Thank you Okay, you mentioned that you were extending the prefix bytes to T include 80 and 81 I believe that the, in this API will also include a domain separator Using that, setting that to a specific value served the work just as well and stay within the NIST API? Good, good question. So the zero and one bytes as well as the 8081 are part of the domain separate So the NIST domain separator is zero followed by context or 1 followed by OID followed by context. Okay, maybe this is kind of then, but that's what I meant You have context as freely submittable setable by both the signer and the verify I believe. Quinn, am I? that's what I meant. You have context is freely submittable, uh, setable by both the signer and the verifier, I believe. Quinn, is, am I remembering that correctly? correctly? Yeah, the context within the signature Verify APIs please come to the mic for uh conversations I think I understand the point. The domain separator has the zero or one at the beginning. Context optionally, the OID Yeah, I was thinking. Yeah, we could use the context and this is all flexible, right, as we're defining it. The reason that we adopted the 80 was to be visibly distinct"
  },
  {
    "startTime": "00:34:01",
    "text": "from the other ones because NIST has already defined what is zero and the one looks like zero is pure hash zero and one is pre-hash requiring a hash function. So we did something different. Go ahead Quinn Dangan NIST, I don't really exactly the values for each of them in my head right now. But you know, numbers, for this thing functions that the purpose is, right? But I thought that basically whenever you sign, you verify, you got to select a context yeah and my point was do you don't need to extend, basically, introduce new ideas. You can do it the same thing with existing ones Yeah, okay. I think that's a good follow-up discussion with NIST on the intent of what 0 and 1 prefixes mean okay but I'm open to that system, but it's an excellent point on interpretation, but then think more generally if there were other modes of operation how would you specify those? Would it be different bytes, different content? and so on? Hi, russ housley, Herbert The, yesterday a new mail is was created about post-quantum signatures in the DNS set context. I think that was started by one of the security ADs And this technique looks very interesting in that context. So I'm really excited to see this group pick it up Thank you, Russ Thanks, Berg Thank you"
  },
  {
    "startTime": "00:36:07",
    "text": "Thank you this is Ruder Gepp I'm sorry, my camera could not be activated And, yeah presenting draft IPPM capacity protocol in here, the crypto options we choose. I'm with Dutch Telecom and I'm not a particular security expert so I a particular security expert. So I'd be grateful if the note taker or the chair helping me if there are comments which I have to work on that would be really great The second day early review we got was made by Brian Reese. And that took a while now and we solved all comments and by now we asked us to please present our crypto approach in the CFRG as he is not aware of any other implementation which shares the approach by us So what we do is we use to 256-bit H-Mack as SHA-256 author to authenticate for authentication. This is completely standard conformant and what is special about our approach is if authentication, is combined with encryption We specify that the second half one bits of the H-MAC SHA-246 orthiage Digest, is used as in a initialization vector"
  },
  {
    "startTime": "00:38:01",
    "text": "for the 128-bit AES mic CBC encryption All right, what we did so far is looking for standards which say it's either a bad idea or this is feasible And the properties required for CPC IV following this, SP 800, minus 388, the IV must be unpredictable now we believe that the result of the H-MAC meets the unpredictability requirement but brought us to look for confirmation or contradiction, of course, if that is correct. So the key to the H-Mack is unknown to an attacker attempting to de- decrypt the message and the output of H-MET minus S-H-A-256 is a uniformly random value. So that is all I can tell you and that is all I've collected and I saw that I get quite a lot of time for discussion so please, if you have to make statements start now Hello, watson ladd, Akamai It sounds from the description that you're taking the H-MAC of the message, then using that as the IV, and then CBC encrypting the message This is not, the security properties you're asking for with encryption aren't just can the attacker decrypt? Because of the CBC you have to put in padding there are padding vulnerabilities that can be exploited unless the encrypt message is authenticated. And so, you have an issue with that"
  },
  {
    "startTime": "00:40:01",
    "text": "Yeah, as I mentioned, I suggested note take notes so I can get that written and then see how to work on it. I'm not a security expert. I really don't start to discuss this. It will be shameful notes so I can get that written and then see how to work on it. I'm not a security expert. I really don't start to discuss this. It will be shame for me and annoying for you you jonathan hoyland, Cloudflare So this isn't unpredictable at all because if you send the same message twice, it'll be encrypted, oh, sorry, it'll get the same H-Mac, which means you're have the same IV so definitely not unpredictable unpredictable Well, the H-Mac, how much is the, how big? is the probability that an H-Mack appears twice? if the content of the message does? 100% a hundred percent it's deterministic it'll do the same thing twice twice why Also, fine On the mic. On the mic, please Deterministic encryption is not an issue. The vulnerabilities for cease mode encryption, come down to encrypting different messages. If you just encrypt the same message twice, well, quite frank if the attacker wanted to, he could present the same message twice to the receiver and to do whatever evil thing whatever evil thing you were expecting him to do So the question on the slide is, is this unpredictable? And my answer is no. I like whether that has other vulnerabilities is the answer to the question on the slide is this is not unpredictable there is actually a mode that has an RFC that does exactly what you want, which is called ASE"
  },
  {
    "startTime": "00:42:01",
    "text": "ASI-S-I-V, and uses CTR mode instead of CBC mode. Otherwise, it's want, which is called ASIV, and uses CTR mode instead of CBC mode. Otherwise, it is very similar to it doesn't use HMEC, it uses CMEX but if you want something that is a deterministic authenticated encryption mode you uses CMA. But if you want something that is a deterministic, authenticated encryption mode, use AESIV. I don't know what I've seen on where it was, but you can look it up Yeah, I don't know That will not be vulnerable to to like padding oracles and that has like a ton of security research behind it it's a good mode. It has an RFC. Use it. It will solve it your problem Okay, we've cut off the queue. Thank you so much for bringing this here. It would be a great to have a cryptanalysis session in this room but it might be preferable to do so on the list going forward. And we do encourage groups who try to put together new security mechanisms to get them reviewed through the CFRG Mailing list is probably the first step Yeah, it would be great no take a new security mechanisms to get them reviewed through the CFRG. Mailing list is probably the first step. Yeah, it would be great no take has been picking up everything which has been said on the micro because that would be really helpful for me So then thanks and Okay, we'll make sure to update the notes uh with from the transcript in order to um have a complete complete outcome for you. Thank you. Thank you Husha Nice, you're on on arm. Hello, everyone I'm going to talk about a mode of operation that uses ASGCM, which is called"
  },
  {
    "startTime": "00:44:01",
    "text": "double non-derived key GCM and in short DNDKGM. So next slide please. First of all, here is a slide to indicate what limitations of ASGCM we see and what are our goals. So on the right hand side, I'll just skim through this short nonces, 96 bit nonces. If they are randomized, they would collide after so many times. And if you want to keep the collision probability of the nonce, which is a catastrophic event, to be below 2 to the minus 32 then you need to abort the key, rotate the key after 2 two to the 32 times and this is a very small amount of usage for a given key certainly in a cloud encryption environment The other limitation is the birthday bound, right? We are using AES with a hundred and twenty You can this is a small calculation if you want to keep the distinguishing advantage below 2 to the minus 32 you need to you cannot encrypt more than 2 to the 52 bytes And last, but not least, there is no key commitment It is easy to find too maliciously to find the two keys such that and the ciphertext. One ciphertext would decry properly under two keys. So on the left-hand side, these are our goals to encrypt at least two to the 64 bytes to use random nonsense. So you see this is not possible to do right now with ASGC The security to use only a single security assumption, basically that AES is a episode of random permutation. This is something every, every believes to enjoy some kind of a NIST compliance umbrella by claiming this is not really a new mode, it's just a way to use a"
  },
  {
    "startTime": "00:46:01",
    "text": "ASGM and bypass the limit limitations. We want of course good performance. We want to also get optional key commitment and minimum effort. So we want to be able to leverage all the software base, outdoor base can find and leverage the same so next slide please So a derived key mode is a general concept Actually, it is already being used in AESD slide, please. So a derived key mode is a general concept. Actually, it is already being used in AESGMCM, and basically it says you take any kind of an AED, input nodes additional data and the message you start by deriving deriving says you take any kind of an AED, input notes, additional data, and the message. You start by deriving a new subkeys from the norms. So you see, okay, this should have been some kind of an animation but but collapsed. So you take a root key, you apply, some kind of a PRF to the root key in order to derive a new session key and also a key commitment value if you want this And then you are using the derived key with the underlying scheme and the you want this and then you're using the derived key with the underlying scheme and just apply this so basically it converts a mode into a multi scenario where hopefully every key is being used only one and no collisions on the keys So and then you'll ask what what nonsense? you going to use with the underlying scheme, and how long is the non? So we are going to separate them these are two different questions and basically we're going to use the fixed nonce"
  },
  {
    "startTime": "00:48:01",
    "text": "Take all zeros and the nonce for the compound scheme would be long So next slide please. This next slide. So how long should the nonce be? So this is just a very quick computation If we want to use it for 2 to the 64 times and have collision probability below 2 to the minus 32, you can see that you need at least 160 bits in the nonce. We're actually taking more. We're taking a one bits, but here is the challenge. It is the challenge. It is longer than the block size that the underlying block size of AES. So are going to accommodate this Next, next slide, please So this, oh no no all right all right all right please. So this, oh no. All right, all right, so, so, so I'll think will say in 192-bit nonsense do not collide after two to the 64 times i mean collision probability is very small. If you derive 256-bit, keys, they're also not going to collide and you cannot do some kind of offline guess on the thing. So you can see now that double non-dK GCM is actually a selection with 24 bytes nons deriving 256 big keys and using a fixed nonce of all zero 12 byte zeros, with the underlying AESGCM. So the idea, nons are going to collide, derived keys are not going to collide We are using only one assumption, it's only one primitive, which is a AES, but we still need some kind of beyond-burtly-bound PI"
  },
  {
    "startTime": "00:50:01",
    "text": "in order to do the derivation because we cannot take just something, just a standard PRF because it's not going to to to the derivation. Because we cannot take just something, just a standard PRF because it's not going to sustain 2 to 64 times So we need the beyond birthday bound derivation based on AS. Next slide please Okay, I don't know. Just so based the idea is to take a long norm split it in two halves Each half would be shorter than the block size of the S128 bits apply some kind of a beyond birthday bound PRF and then sort the results And miraculously, this one works, this gives you beyond birthday bound PRF Now we can use if the two halves of the nonsense are shorter than the block size, we can easily use a AES. Okay, next slide, please So here is actually a picture of how it works We start with 24 by bytes, split them into two 12 bytes Now we feel safe. We have four more bytes that we're going to one three bytes are going to use, to be used as a con configuration. You see this is a configuration on each half, we're going to apply XORP permutation in order here to derive 512 bits. So to do XORP this means that you need to do five in independent AES calls on each totaling 10 of them And then you sort the result and you have the first stuff would be the new, the fresh derived key The other half would be a key commitment string"
  },
  {
    "startTime": "00:52:01",
    "text": "Okay. So now what happens if you don't want key commitment? Of course you don't want to pay 10 AES, right? Because many usages don't need the key commitment So next slide, we'll show you this So yeah, do the same thing. Just don't do five AESs with the SORP Just do three on each side. And here is some, you see that there is a different config string the reason is that we want that if we don't want the if we don't want the key commitment we want the the derived keys are going to be different So with this type of a domain separation between the two cases, we're going to get different derived keys and different key commands in the previous case and no key commitment here So if you are sending a message and your intention is to have key commitment and somebody drops it, it's not going to work because the key derivation is not going to be consistent. Next slide please. Okay a warning. Is it also possible now? to use the 192-bit nonce just as a counter? The answer is no. Well, because you can just do some kind of manipulations and get the get dependencies on the on the underlying blocks so if you really want to use this mode with a counter, you need to free for example, you need to freeze half of the nons and then run a counter a 96 96-bit counter on the other half. This would work Here is the question Are we sure we want to use this mode with a counter? with a"
  },
  {
    "startTime": "00:54:01",
    "text": "with a with a counter nons? This would be a little bit wasteful because you're using only half of the nodes but it is possible next slide So there are four variants right now So we config that is all zero this would give us a standard AE8 interface because there is no key commitment and it just the standard AEAD If we, you configure it with one then we have key commitment now we can also have two versions of a counter half counter option with and without kikkimi commitment. So basically there are four options right now. Next please Okay, you can see how it works. We have the general framework of the derivation. We can use it with and without key commitment and with random nonce or with the counter nons and we have all of these options They are actually controlled through the configuration three bytes And okay, next slide please I don't believe this. All right So performance. We said we want something something efficient. So this is the performance on the short buffer. I'm coming to open SSL. Open SSL is slow because of different reasons, but we have our own optimized, optimized ASGCS and optimized, it's called Haberdasherry, it's a library, optimized DNDK GC so we see that there is some kind, some difference on short buffers. This is because of the overhead Next slide. So now we have"
  },
  {
    "startTime": "00:56:01",
    "text": "a medium buffer like a one kilobyte. So we see that the difference decrease still open SSL loses because the key setup is very, very slow in open SSL. Next slide please A long buffer, 16 kilobytes buffer. You see that this is almost the same and this is exactly the idea. That asymptotically, if the buffer is, if you're processing a long enough, message, then the overhead is amortable and you get almost the same thing now Open SSL catches up And last, the next slide okay this is the asymptot I took one megabyte buffer so you can see. So you have some of overhead if the messages are very short and the difference between DNDKGCM and AES-256 GCM becomes a small and smaller, asymptotically it's the same performance. Next slide, please Okay, so if you want details, where can you get? code optimize code, there is a library that we call Haberdasher. It's open source high performance assembly implementation of many things several things, including AES-2506 GCM, ASGCM CIV SivMac, this is a kind of a MAC scheme, and the DNDK, so you can get it all and compare. There is a Git repository If you want more information, more information there is a talk that I gave it the real world crypto. So you have the video and the presentation at the NIST workshop another presentation And there is also an internet RFC draft that I'm going to"
  },
  {
    "startTime": "00:58:01",
    "text": "actually update after this session but there is already a definition And now next, next slide please. This would be the last slide Well, the last slide is a thank you, but but so one before the last slide. Here are a few questions that I brought to discuss. So the first so remember we have the double nonce is 24 bytes deriving 32 byte keys a fixed norms, well, actually this defined everything in double X4P derivation. So is it true? reasonable to support all four options? I can certainly sympathize with two options which are with key commitment and without key commitment because of course you can do you can say we only have one mode. It always gives out key commitment but you can ignore this but then this is wasteful because you're using 10 AES calls for the derivation where if whereas if you don't want the key commitment you can do it less so the first two configurations first two options actually are a reasonable choice, but now the next question is should we allow or support? also the use of a standard counter nuns? with this construction with and without so the key commitment? And if we do this, which is a better option i mean you have to freeze half of the half of the Otherwise, you can do some manipulations and the security proof doesn't work So there are two choices right now I'm, and right now I'm writing the RFC with the first one, where you have all ones all ones on the on one hand"
  },
  {
    "startTime": "01:00:01",
    "text": "and right now I'm writing the RFC with the first one, where you have all ones, all ones on one half and then a counter because this, you know jumps into your eyes and you see immediately if somebody somebody some somebody is trying some kind of manipulation with this counter mode and then you can immediately reject but it might as well be like a long counter that starts from zero and whatever and it would never spill over so you can just zero this half it's all the same from the viewpoint of the secure proof. Should we allow to use different configurations? with the same root key? Okay. Or a root key would be defined together with a specific configuration That's another question. Now, output format, should it be ciphertext concatenated with the tag? and concatenated with the key commitment or the other way around? and in general should we were about different parameters because we can use a shorter root key. For example, 24 bytes, like 192B root key and shorter derived key shorter renounces and we can actually commit with less with a shorter key commitment but of course once you add this you you get an exponentially large number of options and in general, I hate options, I want the smallest amount of options. So these are the questions for discussion. And the last slide. Next slide, please. This is the thing okay you see okay so thank you very"
  },
  {
    "startTime": "01:02:01",
    "text": "much and I think that the the Q there are PowerPoint version of your slides uploaded. So if people can have a look later, they can see animations and hopefully it was messed up with the pdf conversion from yeah thank you very much for the talk um listening to your use case it sounds very much like the use case for tweakable block ciphers, which there are a number of bits of research into, but your method unlike tweakable bits block ciphers, require recomputation of the key schedule, so it sounds a bit less efficient Is there a reason you're not going for a list-approved tweak? block cipher to prove? Well, basically, I'm wanted to use the general framework that is already used in ASG CMC, where you have a derivation. It's a general concept. You can apply to anything And most importantly, I actually wanted to run a to run ASGC, the classical standardized AESGM under the hood So without making any changes to that standard, and in a way, to say, this is a way to use AES-256 GCM. You just use it in a way that every time you encrypt something, you are using a one-time key and the key is being used only once. Okay martin thomson, this is very interesting I particularly think like the idea of being able to use the one input key basically forever, which is one thing this could provide. And the random noons, I mean right so i do think daniel's concern here is one that I share, the applications that I'm thinking of, would very much benefit from being able to to avoid the overhead on a per message basis"
  },
  {
    "startTime": "01:04:01",
    "text": "of having to manage the new key each time To give you a bit of colour on this one, we have a lot of people who are looking to use TLS over very, very long periods of time and want to avoid some of the complexity of the re-key that's going on there. And so if you can avoid, having to re-key by having something of this general shape, then there's a possibility that you can avoid some complexity in the process I don't know, I cannot change the change the whole concept on the fly, but the but this, I just can tell you that the whole over with the 10 AS is a 170 cycles. Right, it's not a lot and maybe that's not a big deal On your question about, do people want to put in a countenance on this one in that app application, yes, they very much would And they would probably like to do the same sort of thing that TLS currently does with its nonce just on a larger scale and X or the counter in with the thing, but that work in this case? I suspect it would. I don't see a reason why it wouldn't work And by the way, I forgot to mention, if you're freezing half of the norms, then you can do the left-hand side derivation once and then apply this. So this would be just souring some, some string. So this is a way would be just soaring some fixed string. So this is a waste, but this is the cost of trying to use this Consum construction with the long norms and the count Yeah. And so in that context, having key commitment is not particularly interesting, but there are other contexts where the key commitment is very interesting. Yes, and this is why this build two options. In most cases, you don't really care about commitment because All right. Thank you. Thank you very much Yes, we are done. Okay, so thank you very much and I hope you"
  },
  {
    "startTime": "01:06:01",
    "text": "post the good version yeah both versions are available on the Datatracker All right, Stephen, for farrell So this should be, well this could be quicker or slow, I guess, we'll see There was interest in CAMs and combining cams for various reasons. The chairs formed a design team. We had a bunch of calls. Next slide The people who could make the calls are listed here Next slide So I just want to check how many people can of saw the mail to the list and kind of read it? Okay, not that many Okay, so basically the design team we had a bunch of calls. We were basically not asked to produce a way of doing chem combiners, but to try and figure out requirements of the research group by address We look back through the list discussions, there's valid and conflict requirements. Looks like a single answer, it's not going to say of doing chem combiners but to try and figure out requirements of the research group address we look back through the list discussions there's valid and conflicting requirements looks like a single answer is not going to solve every everybody's use cases There's a bit of a lack of clarity about some of the security properties we might want and there's lots of choices and we you know some people want to get this done soon Next slide So the next slide, the output is the following, but we mailed to the list a few weeks ago ago so first basically the design team recommends that the research group might want to produce a document that has the following bits in it. So the first part is basically identifying which security properties are IETF relevant There are papers that have specified like more security properties that you could have here. It's not clear that all of those security properties are relevant to an IETF contexts and there's just so many it's confusing. So a bit of text that kind of clarifies that and other kind of issues to do it using them. Next slide And then the second one is to provide the second"
  },
  {
    "startTime": "01:08:01",
    "text": "part we'd like to see in that document is to provide an overview of kind of ways you could do this and and then to have some concrete combinations that are just to provide the second part we'd like to see in that document is to provide an overview of kind of ways you could do this. And then to have some concrete combinations that are justified that way. So next slide. And design team figured that these are the set of things, the set of combined kems or hybrid kems that would make sense to define in a first instance And so include the definition for those of the documents with some pseudo code, etc next slide include the definition for those of the documents, with some pseudo code, et cetera. Next slide. And basically there's a, you know, there's two at about the roughly 128th security level and one higher If there was no need for the first one, which is, the P256 one and then the design team said, it would make sense to omit it, but we thought that probably there is some people on it. And there are other kinds of things people would like to do. And the design team figured, doing those three listed first and then doing other stuff like would make sense and i think that's it is it and there's another slide okay so i think if the research group were happy to proceed on this basis then the we'll be back over to the research group chairs to go find some victims to write the documents. And the rest is just time for people to say that's terrible that's okay we're happy, we're unhappy or whatever Hi, I'm Deirdre, I was on the design team I think we should do this and I volunteer as tribute to help write the document Chris Wood I also think we should do this. However, I don't think steps C should be in that same document. I think it would be useful and advantageous for us to pull the actual concrete Thank you instantiations out into separate documents because that's really the important part here. We want to get things on unblocked for HPK and TLS as soon as we possibly can and trying to gate all"
  },
  {
    "startTime": "01:10:01",
    "text": "of that on something that's more general purpose, more research-y is it feels wrong. So I would strongly suggest that like X-Wing is today, keep it in a separate document, get the code points based on that. Make sure it's consistent with, you know, whatever the good vibes are in the other document. But please do not hold up the code points on a research document. OK okay so just to summarize you're okay with the kind of content you just wanted to set the documents. Yeah, yeah, correct. Because like, I only care about that actual instantiation at this point. I just want to be able to go to the registries and be like, can we please get our code points? We know what we need for HP key. We know what we need for TLS. Let's just make that happen for the other applications and other protocols that want to use KEMS and might depend on different properties yeah, that's going to require some of the work that's laid out here. Like we're going to need to figure out what those actual protocols require, what's sort of binding they need, and it's perhaps a bit premature to you know, allocate code points for those use cases but we know exactly what we need for TLS and exactly what we need for HPK right now. So let's just let's get I saw a couple of people nodding as Chris was talking talking Yeah, I mean, encrypted client who uses HPK, which requires only in the CCA2 secure chem, does not require anything beyond that Some chat about encryptly clients, though Any other feedback are people generally happy? I guess it would, I presume it would be useful for the chairs of this chat about encrypted clients, though. Any other feedback are people generally happy? I guess it would, I presume it would be useful for the chairs. Maybe you can do a poll thing and say, is that a poll thing working? Watson first, but... Watson has a question, but yeah. Oh, sorry, yeah, uh, uh, uh, what's that I like concluded this and have a plan to write the thing. I agree with Chris that we should try and organize it so that we can on unblock groups as quickly as possible, even if there's"
  },
  {
    "startTime": "01:12:01",
    "text": "some more contentious things waiting in the back background. We really are seeing people want to deploy these on very close timescales Okay. Paul paul hoffman. I was sort of one of the people who got this going on a private message that with the chairs and such. I'm perfectly happy to have it be separated. There's no reason to slow anyone down with where they're going. I disagree with what Watson just said I don't think we're on. I don't think that many people are on time time frames. And so doing this, right, getting these sussed out might take a year or two. There will get there's going to be plenty of time for people who aren't almost already there who are thinking about later applications that need KEMs to then look at this and go oh, this is how we want to do it. So I think this, it would be great to do it this, separate things out, let people dissolve from the base document So did you want to, the book was question is, are people happy? with the proposed CAM design team plan? Yeah, Modulo, I guess Modulo Chris's comment on Okay, so can we also just have a raise the hands again? Like, who has read the output of this email? Okay, it's more than the last. Okay, so that's more, yeah, more than before so so so we can actually have uh shall we delay the session so more people can pitch out? Yeah, yeah, I think some of the, the, the at the, at the line there, uh, reflected tension between having a code point versus having what they're, CFRG is supposed to do is generating research documents and recognition so I would encourage that tension to be fleshed out. This is not a bring you"
  },
  {
    "startTime": "01:14:01",
    "text": "algorithm and get a stamp code point group So that may be a reason why splitting it out is not in line with the process of the CFRG If I may comment on that, the CFRG and the processes do not require this group to bless anything for HPK or T TLS. The registry and the registration policies only require a stable internet draft and then review and approval by the experts So we could literally just take X-Wing today and go get code points We're just, we don't want to do that, but I'm saying like it just does not need to go through the CFRG to RC RC Publication comes with some expectation of stability that some of us might prefer to rely upon and so if we took a draft and grabbed a co-point and started deploying things, there is I guess heightened risk that this group or some other in the process of working through publication would change things in ways that would cause interoperability problems So the preference, why are we allowed to do that? That's a longer discussion that I'd prefer not to have in this, but we can take it offline Rich Yeah, one of the TLS rich registry experts. We already have a Khyber draft Zero is one of the registered code points in the effort changes. We can talk about it offline or you can examine the IAT history. I think splitting up does make sense Does or does not? Does. Sorry should close the poll The poll says 20 click. The poll says 22 yes, zero no and let 10 no okay is this"
  },
  {
    "startTime": "01:16:01",
    "text": "question account for the split or not I I assume this includes the idea of splitting. It does, it includes split? Yes. Okay. Great We'll add two more to the yes then for me and Mark. Okay. So yeah Well, yeah. Okay, so my conclusions, people are not unhappy and I guess it would be preferable Okay, and is there anybody else who would like to? volunteer as a co-author with our? wonderful? volunteer? We can bring that to the list, and the chairs will assign I'd be willing to help do something. Okay, thank you All right, let's do the last One second Okay, this is a rough sketch of some ideas that may move a possible look at a HPKV2 or however we do V2s in CFRG. I know in other words groups in the IETF, they do things like this is and blah, blah, but just a bunch of ideas that motivate like a bit of an overhaul because we have a couple of ideas. Next please ideas Next, please. So, I was here, I don't know, last meeting, the meeting before that, discussing how if we want to include any new chem in HBKE, that's not the written into the document DH, chem, we lose a lot of binding properties that are already guaranteed by DHS or at least we lose the guarantees of"
  },
  {
    "startTime": "01:18:01",
    "text": "binding properties that we get by using DH Chem, which is the one that is deployed and basically all deployed in some of HBKEV1 nowadays That means that you don't have a guarantee that the public key that's being used in HPKE is bound into the shared secret that you're actually using to do your actual encryption. You don't have any guarantee that the ciphertext or the, you know, the other effect into the shared secret that you're actually using to do your actual encryption. You don't have any guarantee that the cipher text or the, you know, the other ephemeral public key in the case for DH Chem is bound into the actual key that you're using to do these things And this opens you up into reencapsulation attacks or other kinds of attacks that you don't have to work about with DH Chem the current only cipher suite we have defined for HBKV1 HBKAV1 also does not define a signature-based auth cam. It's all DH-KM-Oth-KM modes And if you want to do something in the future where this is hard to do with post-quantum variations, this we don't really have an option for that. And third, we don't have a way an HPCAV1, to mix in multiple rounds of encapsulation or off-encapsulations or having multiple pre-shared keys into it. There's just like one go. You have a one shot or you just have one round of what whichever mode that you're doing, base mode, off mode, or using pre- pre-shared keys and you're done. And if you want to do anything more complicated, you just, that's all at the protocol layer and you're sort of doing a nesting doll of hbkees it's not built into the thing. Next please So to fix the binding issues in a new version of HBKE, we'll call it HBKE in a backwards compatible way, we basically force all the chem, cipher suites that use of HBKE to do all the bindings stuff that we do for the DH Chem variant for all the variants. So if you have a chem that defines encapsulation and decapsulation like ML Chem like X-Wing, or your favorite variant of a hybrid or non-hybrid chem, you"
  },
  {
    "startTime": "01:20:01",
    "text": "will have, you will define a new encap with binding or we do it for you Where we do the encapsulation against an encapsulation key, we serializer encapsulation key, we create a chem context where we have the encapsulation this cipher text of the the chem, not the whole HPQ, so ciphertext, and your serialization of your public key And then you have the shared secret from your encapsulation. The whole KEM context, which includes your public key and your cipher text and use that to derive your shared secret that you're actually using to do your HPK encryption at the bottom layer And then the other side is basically doing everything the same thing to actually compute the shared secret where you're binding to the public key and you're binding to the encapsulation key and you're binding to the ciphertext with the shared secret from the encapsulation. Fully backward compatible with DH cam and HBKV1 and the same thing can be done in the off-encap and off D-KK mode while also binding the sender's public key So we can get that to it. Next fixed two is actually including a new signature off-cam mode. So this includes your sign key and your in-cap, your receivers, encapsulation key. And so we're basically doing if you have a chem that has encapsulation and you have a signature scheme that gives you keys and gives you signing and gives you very verification, you have, you do your encapsulation as before You serialize your public key. You serialize your signing key your, sorry, your verification key and you can cat all of the ciphertext, the encapsulation ciphertext, the chem public key the signing, verification key, smush all that together, and then you sign that with your signing key, and you can cat concatenate your encapsulation ciphertext and your signature and then you derive your shared secret for the whole actual HBKE from the shared secret of the chem and that whole ciphertext which is over"
  },
  {
    "startTime": "01:22:01",
    "text": "also over the signature and also all of the sign key material. And then you do everything the same on the other side you just verify the signature before you actually finally compute the shared secret and output the same secret This also helps to mitigate these key compromise impersonation attacks that are not addressed in HBKV1 at all by including a special one shot mode, where we can include the hash of the payload as well into that SIG cam context where we're computing in both phases and this is also included in the signature um and when it prevents some of these insider weaknesses Next Cool And then the third thing is making more generalized keys schedule to allow multiple numbers of N-caps and multiple numbers of possibly off incaps if you wanted to We allow you to be repeated any number of times so that you can incrementally construct and bind a context that includes all of the elements of the Ncaps that we're actually using And if you think this sounds a little bit similar to some of the TLS-13 key schedule, you would be right The advantage of defining this combination in inside HP, HBKE, would that we could follow people to combine any number of PQ and non-public key cams, signatures, and pre- keys in one-shot HBKE construction as opposed to kind of putting it outside and leaving it up to a external protocol to figure it out. In practice, people likely want at least one, at most one if he helm shot HBK construction as opposed to kind of putting it outside and leaving it up to a external protocol to figure it out. In practice, people likely want at least one, at most one Diffie Hellman, one post-quantam, one signature maybe one post-quantum signature, or sorry, one traditional signature, one post-quantum signature, and maybe one pre-share key. But even that's a lot. So a generic composition may have value. So next So this is a bag of stuff"
  },
  {
    "startTime": "01:24:01",
    "text": "Do we want to try to write something up and just be like, hey, this? is like a stab at HBKV2? Should we try to break them up into extensions and each one of them be its own? thing? Each of these requires a new security announcement and each one of them be its own thing? Each of these requires a new security analysis. I know that Carthick and Francis are interested in spearheading those um although they are because they are backwards compatible, we can reuse a lot of proofs for HBK new security analysis. I know that Carthick and francisco arias interested in spearheading those. Although they are, because they are backwards compatible, we can reuse a lot of proofs for HBKAEV1 for the pre-existing DH cam interested in spearheading those. Although they are, because they are backwards compatible, we can reuse a lot of proofs for HBKEV1 for the pre-existing DHS case. So it's not like starting from scratch we're able to reuse a ton of stuff. That's about it like thoughts, feelings I hate it. I love it. Yeah You have a fast growing cue. That's why I was, I was signed What's that? Fast-growing queue of people oh yeah we've got only five minutes left yeah a fast-growing queue, that's why I was, I was saying, you know this. What's that? Fast-growing queue of people. Yeah, we've got only five minutes left, so let's lock the queue. There's six people in it Seven now. Chris Patton, so I would be supportive of cutting a draft with a fix one. I don't think we should I don't, I don't think we should be doing all off-chem. I think there's certain protocol details that are fine to be left to the protocol So yeah, that's my opinion All right. Hi, um, I like, uh, I like fix one and, uh, and a bunch of a rota actually in a fix. We've got a ton of errat for for HPK1 so those two things combined I think would make a nice bundle of work and then the other two things could come as later Thanks. Chris, my Wood, my understanding is that fixed one is not actually fixing a problem the as carthick suggests and demonstrated on the list, I guess, the proofs for HPK do not require any anything beyond a CCA2 secure cam. They don't require any specific binding properties. And this isn't something that"
  },
  {
    "startTime": "01:26:01",
    "text": "just holds for DHKM. It holds for any chem that is CCA2 secure. That's what the new proofs say What I would say is that we have guarantees of properties that we didn't know that we relied on via DH Chem that you try to swap out the chem in your Cypress suite and HPE for any other one, and you have to go revisit that. That's what he did. He did that in the proof. He swapped that up for something more general just a CCA2 secure chem and the proofs follow through but proofs for in cca not for binding properties the only property that HP purports to provide? is CCA2 security for public key encryption. Right, my argument is that we inadvertently have been relying on DH cams properties, not in CCCA for binding to identities. Which applications have been requiring? or depending on properties more than CCA2 security? The ones that have been using HBKE for encrypted client, hello and MLS Tree Chem have through their protocol, saved themselves from rebinding attacks and reencapsulation attacks by doing things at the protocol level. I don't think that's accurate for EC What do you mean? attacks and reencapsulation attacks by doing things at the protocol level. I don't think that's accurate for ECH. The analysis for ECH only depends on a CCH security chem. Right, but these reencapsulation attacks would be present if they didn't, if they swapped out their chem, in HBKE for classic McCle Right, but these reencapsilitation attacks would be present if they didn't. If they swapped out their chem in HBKE for classic Michelese, and they weren't also committing to the public key as part of the transcript, they would be open to reencapsulation attacks I'm not sure I agree with that but um i will yield my time yeah let's take that to the list and it should be noted that there are, yeah there is errata for the HPCA end at the very least, this presentation raises things that might be useful to include in errata Next, after Chris is Watson oh Watson I think that if you have an application, that's using off-chem, it"
  },
  {
    "startTime": "01:28:01",
    "text": "might not know that it's when to need two separate keys instead of one, and you have to worry about binding them together. So I don't think fixing up off-chem to use signatures. It's going to be practice be backwards compatible i think if we have a rata accumulates it's a good reason for Abyss. I think the other stuff should be done as extensions and evaluated on the basis of is this extension valid? valuable? valuable? stephen farrell. Not necessarily against this. I'm also not convinced it's absolutely needed If we do this, I think we should look at all of the various modes and see is anybody using them? Because I believe there's a bunch of combinations that nobody used at which point we could maybe simplify things instead of just only making them more complicated So yeah, I think I agree with Chris that these and I think it was with Watson, that these don't look like fixes. They look a lot more like extinct right? HPK never proposed to provide binding properties, you know, inadvertent did with the H-chem. So like maybe it would be nice to extend it as you've proposed to get those binding properties. I think it also looks like extensions in the sense that at least the of the things you've proposed here fit around the core HP cooperation like the first thing you propose like is how you define it cipher suite, right? Like, you can define a cipher suite around your case to have the nice binding properties when you flow it through HPK right kind of what i do with ML cam through hbke which raised these ideas, yeah. Yes, and you can do like a PSK combiner that feeds nicely in, right? So it all fits a around. So I think maybe this, the bundle of stuff could make sense as like some recommendations for how to use HPKE and get some extra stuff out of it, but it doesn't feel like a fix or a V2 to me"
  },
  {
    "startTime": "01:30:01",
    "text": "I think Chris and Richard probably said everything I was going to do I want less like Stephen. I want less in HPKEE than there is right now There are too many bells and whistles already And adding more max. So we're at time. And there's, yeah, the queue is closed we can take this conversation to the hallway and thanks everyone for attending thank you thank you I didn't fully guess what Chris was saying in terms of like I have a question the thing we're going to have two dogs in the I think there are at least two dogs, but there is wonderful about the plan in terms of like, I mean, it says in the thing we're going to have two dogs in the I think there are at least two dogs but there is one that is one of the separate ones a separate one instance. Yeah Can you we are probably going to be discussing like, just a lot of mind distracted distracted I mean, we'd like that people you have someone else"
  }
]
