[
  {
    "startTime": "00:00:01",
    "text": "pause your video. Feeds, incoming video feeds, any of them that you don't need just to make sure that we're kind to the bandwidth. As much as possible. Okay. And then We were, asked it the last time why we had a picture of just the GABA and not our traditional picture of a, a mascot for the local sports team. It was because the GABA actually has more than one sporting event that occurs to it, but we were asked to select it. And so here you have heater. Who is the mascot for the Brisbane Heat, the local cricket team that does play at the GABA. It does remind me of Syndrome, personally. I don't know what it that for rest of you or not, but, feel free trying to sleep through in this, your cricket coach as you go to bed tonight. You have no doubt seeing the note well by now, but we do like to remind you every time that there are a set of rules that we operate by. Those are reflected for you up here. They include the standards process, the work of your process, anti harassment procedure procedures, the code of conduct The copyright information on how you deal with patents and participation here at the IETF And finally, our privacy policy. If you have questions about those, please find a member of the ISG and or your favorite lawyer There is chat built into the media code tool. You're also welcome to use Zulu the deck OQ will be used for managing anybody who needs to make an intervention, we have already gotten a slide. Scribe. Excuse me. Thank you very much, Martin, for signing up for that. I think, I think, I think, Many people get their start of scrubs then move on up to becoming working group chairs or area directors. So I just, I just like to say, we really appreciate Martin, taking the job."
  },
  {
    "startTime": "00:02:00",
    "text": "There has been a serious, refactoring of the agenda based on what happened on our previous meeting, the Scribe search were already done. But we're going to, have the lessons from implementation has now been cut down to 15 minutes, bandwidth measurement in MOQ, 15 minutes catalog and WERP updates, that was something Will was gonna do yesterday, but we're gonna try and get in, at lightning speed today. Or I should say work speed. Then the lightweight encryption for mock objects in 10 minutes. This is subscribe and fetch follow-up this is the PR that was just landed shortly before the working group. Meeting started. So if you have a chance to go and look at the PR, that'd be great. But, we're functionally not gonna try and get through the whole thing today. And last 5 minutes for planning and administer administrative. Since this has recently been bashed, I'm kinda hoping nobody has bash again. But if anybody does, now's your time. Today. One one question. Some of our conversations yesterday. Maybe as part of the priorities presentation or one of the other presentations, we can Is it possible to get a feel from the working group about what is necessary to go into the draft. To have a productive interim on priorities whenever that happens. Let's see if that falls out from the first one. If it does, that's great. Otherwise, will will will have to see about fitting it in Again, it's a pretty tight agenda, and that could be a pretty big topic. In the meantime, let's go ahead and ask Suhas to take floor. Hello. Can I have your How can everyone hear? Oh, so take that, can you share? Yeah. So is my audio coming through? Yep. Yes. We can hear you through us. Cool. So I I I have a delay here before it shows up. Okay."
  },
  {
    "startTime": "00:04:00",
    "text": "So I'll be talking about, some experiments that we did while encrypting the mock transport protocol, especially some learning from our experiments when we try to build a simulcast based multi quality video delivery or mock protocol. And the focus of our experiments was on the real time use cases so that we can, think someone can think of a conference in application that's built or mock. And this deck basically talks about next next slide, please. Tech talks about, like, multiple things in in, like, what what are the very high level start with, you know, how did we realize the simulcast implementation or mock, basically, how do we reuse the, mock data model or how did we use the quick mechanisms there to kind of make the stream mapping work And once we started implementing this and started trying out, we tried, the implementation in different enrollments. Some examples include a running wired and some included running a stable network like an enterprise network and some many of the experiments that we ran are involved, asking people to run from home Wi Fi Networks. And the more and more we ask people to run in the in this kind of different environments, we learned the network changes in everyone's home, and the wifi network is does not is not stable and there's random losses that that's thrown at you and also the things like, when you're walking around with a mock based call, you see a Wi Fi because you're walking away from your Wi Fi access point, there's a suspension that's happening and you walk back things all all all altogether things coming up, with your network up uplink on the downlink. And, we also observed whenever the Wi Fi driver on your laptop scans the for the access point It does it induce latencies. The packets get held in your Wi Fi driver, and it will be released all of us that are in a burst. And these kind of things, we are we are not aware until we started implementing and, putting things out for testing. And that's when we realized, you know, what kind of"
  },
  {
    "startTime": "00:06:03",
    "text": "we can use. They can use priorities here to help in some of those things. Can we use stream APIs to reset something Copat. So basically, we tried various tools And under this not so good network network conditions, what kind of issues we observed and what tools work and what tools did not work and the change that we have to make to make it work for the real time use case simulcasmiccategory. Next time. At the high level, what is the ambition here? The ambition here is that in a typical conference, you have or, you have a participants with are capable of sending multiple media types, audio and video and with video, they're also able to sell multiple multiple qualities. The reason why we do that, this is because we would have receivers with different capacities some are on mobile, some are on high end devices. And we would like to reach, the producer's video to, multiplicity of of receivers And in order to do that one, we basically, have, typically, you have 3 qualities, like, 360p at the load f and some 20p at a height f unlike 1080p at medium and 1080p at the high def. And here, the idea here is that, like, when we when you basically support this kind of multiple layers, we want a behavior to expect where if the network goes bad, we want the lowest the highest important out, like, the audio and the load f video to go through when you have when you're constrained on bandwidth or for any reason, And on the other hand, if if you have if the things get better, we want the high quality go to the go go to the receiver. And we would want most of these things to be done without requiring explicit selection of media. So the basically ask, learn from what the quick condition control is saying and react that one. At the same time, we want this decision to happen, close to real time. So the that it's it would be useful. Next slide, please."
  },
  {
    "startTime": "00:08:04",
    "text": "This basically shows that the the point that I made where a source with multiple callers and receivers going to relay, and each receiver is capable of receiving some set of the qualities are not depending upon where they're coming from, their network, hop, the issues on the condition conditional links, or it might be the pure wire the poor Wi Fi network scenario where what you're you're you're on a call and someone just starts watching Netflix and runs microfavor 1. You'll you'll see all kind of losses happening, and we won't be we want to be able to support, this kind of different set of receivers, and being source being able to multitask those and to release reach these receivers. Totally where they're coming from. Next slide. So now having such a set, like, what our experiment was the and to basically realize it or the quick, the idea here is that the source who has these multiple qualities and media types, we'll send all the media on a single quick connection and we map the media to quick streams datagrams for audio, and we assign, priorities to streams and data grounds as well. The idea here is that we want to use the condition control to help us guide, when to scale up or scale down, I I'm make make some decisions based on that one and ask quick to schedule the most important streams first. So whenever a quick quick stack is transmitting setting the priorities based on the application preference here, whether it's the expiratory, the most important one, and hide if we'll be the least important one. Based those priorities on your objects on particular streams. And ask quick to schedule it properly so that now we can behavior that we want, under different conditions. Next slide, please. And, in order to kind of realize,"
  },
  {
    "startTime": "00:10:02",
    "text": "handle kind of different networks, conditions that I explained. What we experimented first was, have one stream per group mapping. Here, the idea here is that, like, if you're, say, sending, 3 streams high quality, low quality and medium definition. And at any point in time, you see because of the network condition, Your network, you're not able to push through, particular quality. You basically reset the team at the at that group boundary and wait for the next next, group to start. The the idea here is that, like, if the things are already bad on the network, we don't want to add things that does not really matter, for the end to end user experience to have the end users to have, what do you say, the continuous smooth user experience. So the the the first of the one of the tools that we tried was if the stream is really falling behind, the HD is falling behind because we are unable to push this through struck behind retransmission or or something like that. You just reset the stream so that the 360 p video and the audio can go through. And also, on the on some of these cases because of how the condition control works, the receiver's side, you do get mix mix of frames. Depending upon how quickly your condition control reacted before you stop sending the things that could, you know, make the things worse. The risk will still get the mix of strain. So our our our navy implementation on the receiver side was if you get, like, multiple qualities pick the one that's best at the point in time and and and, render that one. That way, and we have some some tires some way to not have jarring effect on the receiver side, but the receiver makes a local decision. Saying that, you know, that point of time when I have to render, I pick the best quality that I have, and I'm going to render And there's some cases where We also had this scenario. If the receiver is seeing consistently court is coming off the, bad, frames coming off the quality that you're basically don't unsubscribe because it's not helping. So it's kind of it works"
  },
  {
    "startTime": "00:12:00",
    "text": "send her reacting to the network At the same receiver choosing the best thing can do at that point in time and also make a decision that, he really not helping 1080p, then don't even bother to ask it, about that one. That's one thing. Just a quick thing. We we have the queue, but I'm gonna ask the to wait because we have a really tight timing, but I'm also gonna tell you Suhas that we're 12 minutes in and you've still got 4 slides to go. So you might need to go fast Sounds good. So when we reset audio or datagram, and we set the priority at track level and we apply that, to that that gets populated to different objects, based on that one. We put next slide. So what did we find? On certain drops, in a network or capacity or or due to the Wi Fi delay. We saw condition control will notice it off delay around 2 quantities. When, when when that happened, you basically end up making wrong decisions and you end up queuing the frames, like, SD packets and into the, into the queue into the network where this shouldn't have been gone. This cost basically random losses, not just for the high definition packets, but also for the definition and this ended in recording, more retransmissions and more, delays across all the streams. Next slide, please. So what are the things that helped us fix is that, having the retransmissions respect the original stream practice really helped us a lot because, when things got there, network is when and and you're able to send more data, the thing is if the retransmissions did not follow the original stream priorities, then again, you're getting mixed up data sent over the network. So one of the things that we implemented was having the retransmission also support the original priorities because of that, we're able to basically have after the things come back, still have the priority order maintained. And, also, we worked on making the in the peak of, basically, crucial work Christian worked on making the condition control react faster."
  },
  {
    "startTime": "00:14:02",
    "text": "In the sense that we it basically, we made some changes BBR straight machine to handle, suspensions better and how to detect and react to the bandwidth changes slightly faster that would help us in this in in making this. And we would like to propose some of the incremental changes to the BBR in the upcoming IT of Next slide, please. This is right. It's a teaser, where we we basically say, the before increments and after increments. And and we if if, like, if you look carefully, you'd see the reaction time at the RT, at the below graph would show the major change that happened before and after where we are getting closer to, like, we can, like, 102 or 200 milliseconds if you're able to react in the worst scenarios, on the network. Where in earlier case, it was like on 2 seconds, depending upon how the Wi Fi network be heard. Next slide, please. So in general, we for the upcoming idea, we we idea if we would want to bring the permits that we did, by maturing the BBR to make it more real time friendly. And also, we would like to contribute, you know, how the the standard setting the stream practice actually helped, in making some of the decisions that relate to decide percent and when to send next. Any questions in case I've got Jenna and Ian in queue. If you're you're gonna queue for this, please do it quickly because we're gonna also ask everybody to be very quick. Because of the tight timing. Janae. Thanks so much for that presentation. The first thing is that just say, PSA, everybody should be doing similar priorities for etransmissions as what emissions because otherwise, it exactly ends up in the situation that you had. So that's that's a good catch, and you should absolutely do that. The the question I had for you is how often do you mean, I mean, I mean, I mean, I mean, I'd love to dig into the traces with you on this one because it it seems to me that there's a lot going on clearly, but also, How often do you have multiple qualities coming in at the same time. You have simulcast, which obviously you're gonna have multiple qualities coming in, but I don't know how you separate out"
  },
  {
    "startTime": "00:16:01",
    "text": "when you are in a condition, How do you know that you can't support just one of those qualities? Right. Well, because we are we we implement the priority queue. So prior to q, so reate it. When when the pick up with medical tells that it has does the thing to send, we send it in the priority order. So if the car network is really congested, they were, they were, they were, enough, you end up, you end up, you end up, backing up the sense. low Oh, I see. I see. Okay. That makes That makes sense. Thank you. Thank you. And Sweat, Google. Thanks for sharing number 1. So the priority scheme is the kind of existing ish send order stuff that's in the current draft. Right? It's basically an miracle. That's publisher driven? Cool. Alright. It's, okay. Yeah. Yeah. If I if I may The the permission, the point of scheme is pretty much that of HTTP. I didn't catch it. But is it's it's like HDC Okay. Yes. That's what I I mean, the thing in the current draft thought. Okay. Perfect. is fairly similar as well. Next is who's canceling? Is it the, subscriber or the publisher. The subscriber or the publisher basically resets the stream if the queues backing up for a particular quality. We have some like, the the depth of the cubase on that. And the subscriber can also react subscriber happens later. But but if sees less getting a lot of back end going on, but, the stream is reset bite the sender the publisher. Okay. Based on purpose traffic. That's what I inferred. I just was curious. And then the last thing is, Thank you for any input you have to BBRV3. I actually think it's a fairly good basis for a congestion control protocol that would work all well on department. But I do suspect that there are some small changes that will be necessary but thank you for any contribution you wanna make. And, yes, you're right. Thanks. The wrong working group. Thanks. Lucas. Hi, Lucas."
  },
  {
    "startTime": "00:18:03",
    "text": "I had a clarification question. I still got answered, but then I got confused because the response of this is like HTTP is confusing me because there's multiple ways you can do HP priorities. Whether they're they're sequential or incremental effectively. I I'm presuming this is trying to serve things sequentially. So the low order incomplete before the meet, the medium quality in complete before the high quality is that correct? So that that's correct. Like, we the lower order has to finish first and then the VA month and the RFF. Yeah. Okay. Cool. Thanks. Okay. Victor, sorry. We did close the queue before you got in it. So I'm gonna ask you to take your question to either the chat or the list. There are several other questions in the chat, Suwaj if you wanna give those an answer. The next person up is Ali Hello. Good afternoon. Can you hear me? Yes. We can. Alright. Perfect. Alright. My name is Ali, and, I'm here with my, PhD students, Safesh, who you met with in Danish for those who were in Danish. So we are, we've been doing a lot of, testing and, research on bandwidth measurements, and we are gonna share some results today. Next slide, please. So this is a this is a dispute that we established and then, you know, the the paper related to this test bed some results will come out next month in ACM, MMC I can share the early papers, with those who would like to have a copy So we have a live cap, and then this was also demoed in the ACMI High video last month in February in numbers as well. So we have, ffmpeg, live encoding and packaging. And then we use mopup, from Mike English, and then we have origin GS as the you know, dash, origin server, HTTP server,"
  },
  {
    "startTime": "00:20:01",
    "text": "through Mercury. We have traffic shippers, traffic shapers, for independent traffics like TCP and UDP. And then we have open source HTS client and more player and then redouble of some, you know, some monitoring tools to record things and so on. So today, I'm gonna just talk about the mock, the orange pipeline. Next slide, please. So, traffic shaping just for those who wonder, we apply TC, which is the defector know, traffic shaper, tool in the Linux systems. So there are some details over here as well as in the paper. Next slide. So this is in a closed environment 3 computers, source relay, and then the pop, declined, test live source is coming in from FFM, you know, FFM back here through the webcam. Segment durations are 1 second, 1, 1 go per second, and then one stream per cop as well. Chunk duration is is a Seema Packaging. So that's, one frame per, one one chump per frame. And that's, you know, 30 chunks per second. And then we have 4 video representations based on the HLS recommendations And, this is also I need I would like to emphasize that this is an ABR system. So we are doing client side adaptive streaming. This is not just a single stream or simulcast. So this is, rate shifting on the client side. Next wife, please. So this is the this is the user interface for those who haven't seen number, we measure everything and then post them in real time. And, you know, I mean, into demo, we also attached GS next to it low latency dash. You could appreciate the low latency fee, nature of media work week. Next slide, please. Okay. So we measure the bandwidth in two ways we previously, it was last year in Seattle. We gave some,"
  },
  {
    "startTime": "00:22:02",
    "text": "you know, initial results and assist on the couple of things, and, we fix some issues. So first, we have the passive bandwidth measurements passive means, this is as we download data like video or audio. All the audio is not included in this testing at the moment. We we try to understand what the bandwidth is. What the available bandwidth is and then how fast we can actually receive the data. So, there are some equations. There's there's some technique over here, but this really has something to do with, you know, monitoring the incoming data when they were generated. Supposedly that's, corresponding to when they were transmitted from the from the origin, from the publisher. And then, based on the time that they were received, we tried to figure out you know, what the link can actually support. So this allows us to figure out even if we are streaming it, streaming it per second video right now, you know, maybe the channel is going to support 13 or 30 megabits per second. So we can actually understand this. And then there's a paper on this as well that was published last year. Next slide, please. So this is this is the result the the blue curve over here is showing, you know, staircase downshipped and then upshift. So we are reducing the bandwidth over time. And then we are increasing the, bandwidth over time. Here, dropping from 20 megabits all the way to 2 megabits per second, and then staying in every bandwidth value is 6 or 10 seconds, if I remember correctly. And this is what we can measure from the ABR system. So this is not using any radiator, but just incoming downloaded video data. As you can see, that that's a pretty good job. Except that there is a one overshoot around 8, 6 megabits per second. And then this is something we are, we are working on. This is a this is a This is an overd estimation problem. And then this happens when the client dev shifts at the same time when there is a"
  },
  {
    "startTime": "00:24:02",
    "text": "betray change in the network. So this is something we need to fix. But otherwise, it looks quite promising. Next slide, please. This is, again, showing the same Paradigm, but here now, the green curve is showing measured over and applied. So, So if if we measure 5 and then the applied rate was actually 5, so mean, the network limit was 5. So this should be around 1. Right? So we want a flat could incur, but as you can see, there are some overestimation, as much as, like, twice, when, declined this you know, down shifting, because of the in, you know, lack of bandwidth in the network. Otherwise, pretty much around 1 ratio 1. Next slide, please. So then the other approach that we also spoke about, and then discussed a bit in a Denver meeting was the active bandwidth measurements. It's called Probing. So this is not using the actual video or audio data coming from publisher, but this is using some just some dump data, in the order of, you know, tens of kilobytes or so. And then we can do this on a probing channel And then here, we established that channel and subscribe it from the client. This is something we already implemented, and, we can share the quote for those who are interested in. So we we subscribe, and and we send a prop size indicating how much data we want for our, for our prop, and then we also indicate the prop priority this is a very interesting, investigation point because if you want, the pro priority to be the lowest we will underestimate available bandwidth in the network. If if it is the highest, we will over estimate So finding the right pro priority level for the prop topic is definitely something important. So here's an example. Subscribe pro channel"
  },
  {
    "startTime": "00:26:00",
    "text": "40 kilobytes. And then I want you know, high priority. And this is not a continuous subscription. So, subscribe message is gonna get you one pro. You wanna do it again, in one second or in 2 seconds or in 10 seconds where or whenever you want, you need to subscribe again. And then you get a subscribe, okay, message, and then some dump data in in the amount, and then you measure the bandwidth based on that. So the prop frequency and the priority and then, you know, payload size, adjustable on this prop channel. Next slide. Here here is some results as opposed to overshooting the bandwidth in the, passive measurements here, we have a bit of under estimation, you know, undershooting problem. So the the bandwidth is usually, you know, just below what's available in the network. And, but otherwise, it does still, again, a pretty good job, especially when the network conditions are improving, probing works relatively well because you are already opening up some extra space for the for the probing traffic and, you know, it works just fine. During time shift, it's not really working very well, but maybe that's something most of you need to fix, you know, Rica. Next slide, please. And, this is, again, the same plot with, measured where over applied ratio. So, again, we want 1 here instead of overshooting, we have undershoots. As you can see, as much as, like, you know, we'll, you know, we'll point 4. So it's like, percent less bandwidth measurement in some occasions. And, still this is promising, but, you know, it's not necessarily much better than the other issue. I guess it kinda depends on whether you wanna undershoot or overshoot. Next slide, please. So our conclusions for now, available bandwidth"
  },
  {
    "startTime": "00:28:01",
    "text": "can be measured on the client's side in a passive reactive way. Obviously, it's not perfect. This is not really, you know, this can be really a perfect measurement anyway, but, I mean, it does a pretty good job And for ABR purposes, and our client still works just fine, yes, there are some stalls or maybe skipped frames and things like that, but still, it it gets the job done. Now, his major months, have several advantages. There is no traffic overhead. We are using the incoming data. Right? So there's no additional track, like, a prop track that you need to maintain or you need to create or you need to subscribe to. That's a sup also plus And we don't need to find out a good value for, oh, what what what, you know, how big a proper message I should use or how frequently I should prop the network, or should I use high priority or low priority for my prop those questions don't don't, don't apply. But then active measurements have advantages too. So, they can't complement passive measurements. So our r Our idea is to go within a hybrid approach. So do you mostly passive measurements and then do only active measurements when necessary. And an active medium is also worked reliably when there is nothing sufficient incoming data. So, so, that's one of the issues with the passive measurements. If if you are just receiving audio data, for example, from the publisher. You know, passive measurements you know, failed largely to actually figure out how, you know, how fast the network can, send the data to you. But, with active measurements, that's where we can actually use them. And pinpoint the exact value or a close, you know, a good value for the for the available network. Now This is an ongoing discussion, obviously, but I, raised my, concerned about sender side signals before. Again, you know, I am not really against them."
  },
  {
    "startTime": "00:30:03",
    "text": "If there is a conscious window of signal coming from the sender side, we will be more than happy to use it for the client side, measurements, as well as for ABR purposes. But, we are not really, we are not really planning to, replace all the client side measurements with center side signals because that's really the cap And, that's, actually, that's not really a good estimate. For the network bandwidth. That's what the sender thinks that it can send. But doesn't necessarily mean that's what the network can send. So that's it. Tech. Thanks much. very I actually don't see anybody in queue, but I imagine will want to dig into this and may contact you afterward. So, thanks very much for presenting. Alright. Thank you. The next first step is Will, Yes. Come on camera. Okay? Firstly, let me start with an apology for, the prior session that was a calendar, Matthew, on my part, I had put the meetings in very early when they were first scheduled. Into my calendar. And after they were moved, I thought the calendar entries would move, but they don't. So, reminder to all to check. I appreciate the time slot today. Can you, we're we need to get your slides up because they're on yesterday and set it today, which was our mistake. So if there's any vamping you can do, to use the time efficiently. Evansions. cover So I'm gonna It's very, very quickly, 10 minutes, 2 subjects. The first is catalog and update on that. And the second is WAR which is the beginning of the actual streaming format that's gonna run on top. Of media over quick transport. There's a lot of discussion. Most of the debates you hear in these calls are about the at at the transport layer, but we do need to start thinking of how we're going to move media,"
  },
  {
    "startTime": "00:32:00",
    "text": "over this binary transport. Who's Hey. Will, since we were still trying to get your slides up, can we actually put Colin in your slot right now and then have you Yeah. That's fine. We can start looking fine. Do you need me to resend you the slides? Oh, wait. Wait. Maybe. Do you wanna just Actually, I I do a share screen. So much Okay. Apologies Nevermind. for only having some of our ducks in a row. Okay. Okay. Scroll down, please. So the agenda, I wanna present, PRs and changes happened since, last ATF 118. The NPRs that are currently open that you can comment on or review and prove so that we can merge. And I'll talk very quickly about a job let's scroll to slide 3, please. So we did merge in based on discussion of prior IETF support standard mechanism for updating JSON documents and catalog is a current form is a JSON document. There has been some feedback from this that it might be too heavyweight for some applications. I think"
  },
  {
    "startTime": "00:34:01",
    "text": "but I I would like whoever is unhappy with Jason Pipes to open an issue and propose an alternative because the trouble to this And I feel patch and call it something else, which wouldn't really be a step forward. So this PR has merged, but we're open to an improved update mechanism. Next slide. Yes. This PR got merged. I think it's a really good one allowing relative name. So previously in a catalog, you had to specify the namespace but now we allow it to be inherited from the track that is itself describing the catalog. So there's there's an an implicit, name space by the mere fact that you're you're reading a catalog track, and all tracks within the catalog and inherit that. And I think this will allow for decoupling the the actual catalog contents from the namespace be very useful when we get in a flexible distribution. Of course, you can specify the namespace within the catalogs. You haven't lost anything. We've gained a feature here. Next slide, please. This was another request. So we're on to open PRs now. So there was a request to move the fields to, and IANA the idea of being that if we wanted to add new catalog fields in the future. We didn't wanna have to go and and do an RFC update. So I've created and I I own a registry called MachCOM and catalog fields. It's gonna be managed according to specification required, which is it's slightly below RFC required. I wanna be open to there being other non IETF groups who might want to define, catalog fields here. And it's basically all the existing fields, will be populated as initial entries this register. Next slide."
  },
  {
    "startTime": "00:36:02",
    "text": "PR 40 is to add a new field to indicate a delta update so to be expected. This was another requested optimization. A player wants to know, does it need to expect incremental updates. So we have a flag now on the left hand side support delta updates. True. It's Berlin. And it's particularly useful if we look at the right hand side example, this is This is a catalog that is describing other catalogs. Is one of the capabilities of catalog. And you notice it describes 2 catalogs, and, one of them supports Delta updates 1 doesn't. So we have schema where you can offer 2 versions of your catalog to clients and maybe your simpler clients it can't handle delta updates. Subscribe to 1. Your more complex or fully featured clients can subscribe to the Next slide, please. And this is PR42, I started implementing a a a little parsing class for the catalog. And I quickly ran into the notion that we have an an an an assumption that any track properties that are floating around loosely in the root are to be inheritable by old tracks. And this this I think is inflexible because in the future, it's gonna inhibit extensibility with that as some Right? Because we might wanna put other fields in the route that should not be interpreted as track fields. So if they are to be inherited, then I wanna put them in a in an explicit So there's objects called common track fields. It is what it's the name is what's on the can, and anything inside it is explicitly meant to be inherited by all trucks. This allows for compactness of, catalogs, especially where we have lots of entries sharing a lot of common information. So if you like this idea, give it a thumbs up. If you don't, please suggest a change. This is just a picture scroll down, next slide. It's just showing the the same situation as above."
  },
  {
    "startTime": "00:38:03",
    "text": "The right hand, this is just the evolution we've come from and PR 42 is the situation on the right have an explicit object holding our common track fields. Next slide, please. And there was a call for adoption following, the interim that adoption passed. So thank you everybody. There is Now there isn't actually the new drafted head. That, link, I tried updating it. I'm getting a push error, which I will resolve chairs, would actually like before we before we move to a different, ID I would like to merge the open PR, so we're not in the super weird state. So maybe we can debate that, but I will work it out with the chairs And then once it's, in board and adopted, I think we can move forward with optimizations. I don't think there's time for questions. That's the last slide. Are we gonna do warp as well, or we had a We only have a couple of minutes Is there something Yeah. I can do a walk in a few minutes. Alright. That Can you bring the slides up? More speed. In insert bad war puns at this point. Okay. So, what dropped up? Update for Brisbane, agenda. Let's scroll down. Go to the picture. So WARP as a reminder, WARP is the application layer, it's sitting on top of mock transport WOP is an example of a streaming format. There can be many streaming formats. There may not even"
  },
  {
    "startTime": "00:40:00",
    "text": "deliver video. They can do other things, but WAF is gonna be an example a streaming format that can deliver both C map based and lock packaged content. So scroll down. It's comprised of reusable components. There's 4 drafts right now. There's catalog draft, which I just reviewed. There's CMAP over mock transport, which is available, There's lockoverlock transports, the Thanks to Moe and Suhas. They they just released that. That's currently one document. I know there was discussion in the prime meeting. I I missed it. I still hope that lock can be separated into a packaging component, just for the how to package the media and then a a And then there's gate there's a separate, draft for the warped specific logic and there's 6 issues open on that. So that's the layer of the land. The collection of all of these is what we term walk. Next slide. There was a PR 20 that's removed the packaging definition and moved it to an independent drop just that was basically executing that separation that I just spoke at on on the diagram. I I won't get into the details, seem as very useful for media that's used elsewhere in the industry. So I wanna make sure that We have a compatible Victor for getting it, into and delivered efficiently. Next slide. Period on 19. This was just Again, the mechanism, I changed walk to pull out any catalog specific stuff and refer them fleetly, to the catalog draft, Wolf applies additional restrictions. For example, that the name of the catalog must be catalog. So that we can find it. That's not a requirement for catalog, and there'll be other changes to walk"
  },
  {
    "startTime": "00:42:00",
    "text": "may also add additional custom fields for content protection and those were discussed at the interim as well. Next slide. So timeline proposal, this is still in the work, and this is what I wanted discussed most, but there's there's not time right now. The notion that we need to tell players not so much for real time communication. But certainly for interactive levels, live sporting events, and certainly for vod files, We need to be able to communicate to the player what's the relationship between media time, wall clock time, and group number? So proposed a a simple track for doing that. Adjacent derivative of it right there. But if you go to the next slide, and and I introduced this at the interim as well. If we want to extend Jason, Jason Pap seems overkill because in this case, we could modify the past, right? So we're an append only operation. So something as simple as CSP would in fact, I think suit this very well because a Delta rugby could be a new line that we just add to the end. We append it the end. So we can actually is it just real quick interruption? If there's any way you can get to then quickly because we're just running a little bit behind. There's a couple more things we need to get Yeah. I can stop here. This is the, I think the last slide. So just a summary of core issues remaining. There's a lot of core issues remaining So if you're interested in what, please get involved. Thank you. Alright. Thanks, Will. Next step is now we have Colin. Run. Oh, no. Keep the slides. It's not hard to beat the slides. It's up by much. Okay. So I'm gonna be talking quickly about this. Secure objects. And this is, next slide. This is a way of"
  },
  {
    "startTime": "00:44:03",
    "text": "Encrypting the objects. That's that's all it is. It's it's just straight up symmetric encryption. I wanna be really clear. This is, this isn't to replace anything that we use in CMAF content and protection or anything like that. This is purely for places where you're not using that and you wanna do end to end encryption. Primarily some of the sort of cases that would probably be associated more with lock type things. So a real goal on this, particularly for the the real time stuff audio in particular is to really minimize the bandwidth overhead that we're expanding these objects out with each time that we do it. The technique here, there's no real crypto that we're doing here. We're just saying, use the standard crypto IETF uses, these HKTF to expand out keys and any AED scheme. I'd like to sort of highlight the point here that, you know, irate early here, the IETF does not define APIs they do. Fact, 5116 is nothing but an API. Anyway, moving on from that, we want to, so that's that's the what we define it in terms of here. And where this would is set up to work with MLS. But there's no reason it couldn't work with some other key negotiation scheme if you were using it. But only works with MELUS. So the next so that's basically design calls, minimize bandwidth, The relays don't know anything's different. It just looks like a mock object that they're forwarding like they forward any other object, you know, it doesn't change any of the group IDs or anything like doesn't change any information the Relay look at. Look at. It's only the endpoints. Next slide. So the the trick with AED, AED, crypto, though is we, we assume as a starting point here that we're going to have a key that we're encrypting the media and that over time, that key may get changed for various reasons. It might be that a given track always has the same key. But we wanna have the option of being able to switch a key mid track for some reason, not maybe in a conference, if somebody leaving the group or coming back to the group. So there's also a key ID that goes along. The key ID is just, you know,"
  },
  {
    "startTime": "00:46:00",
    "text": "numeri, you know, and if ink ink, implementing integer then indicates the generation of the key and is identifier for the key. Because, you know, we have that. So the prerequisites, we have that. We basically use the the full track name, the keys, ideas, the cipher suite and some of these things to drive an actual encryption key that we're going to use. We also generate a nut. So we generate this out of using the group ID and the object ID mix with some salt. But the the the key point about this is that for these types of encryption algorithms, that nuns has to be not used multiple times with the same key. The key is unique to the track. And we're guaranteeing that the group ID and the object ID inside that track are are you in So that that's sort of the main, characteristics that we use of these existing IDs that allow us to, basically reduce the amount of information we have to send substantially and actually sitting the stuff across. So what we end up in the actual payload, we have the normal whatever is in the you know, mock envelope and things, at the beginning. We're taking what what's the data we're going to encrypt. We're adding before a key ID, which be a, a warrant, so probably one bite in most cases. We're adding the new encrypted payload, which will include an authentication tag at the end. So the amount that this is expands things is different. It's basically the size of the authentication tag for whichever crypto algorithm you're using, the padding of it, and the key ID. So it's a pretty minimal expansion, of where to do Last slide. One more slip. Yeah. So we're talking just to give an idea of what that looks like, if we're talking about the, like, low bit rate codex, so we're seeing codex that are going you know, sub 3 kilobits per second now and sound quite reasonable. And, you know, we we see some of the early versions of things coming out ahead on that. It's that's only 12 bytes per packet. At at even fairly large at 50 millisecond packets at 20 millisecond packets"
  },
  {
    "startTime": "00:48:01",
    "text": "it's, you know, it's less than half of that. So even adding 1 byte plus some tags or something does substantially up the bandwidth of that. And that's really why the focus on what bandwidth So this is a really short, simple draft. It's just at the sort of early stage of people starting to think about it and look at it. Wanna get people's attention on it a little bit, and that's where they are. Questions I can answer about that. So, Jonathan, in the In the chat, you said suggestion Jason Streaming from it? Did you wanna Oh, for the previous one? Okay. Sorry. It was delayed on my thing. I was wondering why, you know, For for this one, I said GCM SST, but get the tag shorter, but that's a separate So say that one more time. Well, this is what I said. I Mike, I said, in the jet GCM SST, which is the new proposal for shorter tax. I I that mean, is that a AAAD, c Christopher, we could totally use that. We could also use the counter mode version. About it. But, yeah, definitely would support both of those. We'd have to figure out which crypto suites we wanted later Thanks. Okay. Thanks, Colin. I think now, Ian talked a few minutes about, the subscribe fetch follow-up. Did I get that right? Don't think there's any slides. Did you wanna talk to anything in particular or you're just gonna Put heater back up. Yeah. I didn't have slides. I'm happy to answer questions. I mean, a number of there are a number of design details in there that, like, I'm attempted to make a call on. One way or the There are a number of things where you can spell things other. a number of different ways as SUS noted. Like, you know, whether it's something's open ended, do you indicate that in, you know, number of groups you wanna fetch or do you indicate that in type field or something like that. I'm not hung up on As an individual on kind of any of them, as long as we can come to a conclusion on all of them that, like, makes really clear, like, what everyone is supposed to do then I would be happy."
  },
  {
    "startTime": "00:50:01",
    "text": "But, but if there's any specific feedback about aspects of it, that people wanna discuss In particular, things that What work? Or Things are problematic. I cannot aren't like editorial. Kind of commentary or, like, framing commentary, then This would be a great time to discuss it. Just put it up. So if you if there's anything you wanna have people look at Not no one's raised anything. I think that Is overly concerning so far? But I think Luke hasn't had enough time to look at it. It's also think in the middle of the night, maybe there I mean, Luke just joined the queue. Yeah. Just just joined the queue. I I found an issue. Basically, I just don't know what you're for the HLS dash use case. The reliable live how you're supposed to use fetch and subscribe. And I think just writing that out just as part of this proposal would help because I think it makes it really complicated. Yeah. That's a good point. Are are there any other folks who've either read it or have wanna just clarifying questions up there. We do have a few. We we've been going at lightning speed, but we're actually doing okay right now. So you have somebody to ask again, Colin. I was gonna say, I mean, I mean, I think that that the classic HLS dash streaming cases is dealt with really easily on this. And perfectly reasonable for Luke to ask of like, Hey, Claire, you know, let's, let's walk through this, but, I'm glad to send something an email to the list trying to walk through that I think it totally meets those needs. So I'm very much in favor of like, move quick on this. And then as we, you know, it's gonna make it easier to deal with all our other PRs and stuff we have this in, if we find mistakes, then start changing okay. Will? I might just to comment on fetch, I I I like the separation, but I read through the the minutes from yesterday. It seems fetch, is just centered around the live here. You can either get the the"
  },
  {
    "startTime": "00:52:01",
    "text": "the the group boundary before or the one that's going to come there. Right? And and the comment was that This meets the majority of starting media use cases. I think that is true. Real time communication. But that is not true. For all other latency schemes. In other words, 2 to 3 seconds media. What you typically want to do is go back more than one group That's a very common starting case. So I and the proposed solution, which is that I I do the subscribe for the live and in parallel, I do a fetch once I know what the object is for something to is behind is is make 2 operations. You have to queue up data. It's is just an ugly start to what will be, I think, a very common starting use case. So I I would really like to extend The subscribe to include more than one back rack which is pretty much where we where we had it before, but I think there's good reasons for for being there. And I don't believe it places an any undue burden on the relay. 3. It's just an issue. It So, I completely agree that this is an important use case, make sure it and works well. we need to So the intent of going back one full group One of the reasons that that was added was because If you go back one full group, Hopefully, that is enough Data, You can fill the pipe. During that RTT that it takes to get the thing you are going to fetch so the fetch, they subscribe fetch sequence that takes an RCT, an extra version extra RCT, but it shouldn't matter because if you want More than one group back, it's hopefully going to take more than 1 RGT to deliver that amount of data. And so there should be in in would expect in most certain circumstances, no practical, practical latency and that's kind why I suggested that is is being a a reasonable approach. I mean, we can we can do something else, but that that was kind of my thinking."
  },
  {
    "startTime": "00:54:01",
    "text": "Yeah. Okay. The trouble is you're blocked. Right? You're getting the later data first, but you need the earlier data before you can start. So having to hold it and and that just seems a an awkward construct for for a start use kids, but Yeah. We have a few more folks in the queue, and now we are getting I'll I'll Got it. to the end of our time. So, John, Jana and Garth, I don't agree that this is ugly. I think this is actually clean. And I'm happy to talk about why I think that but I won't spend time on it here. We spent time on it yesterday. And the the key is that it doesn't seem clean if you don't think about congestion or priority condition control or priorities. If you include those in your thinking, it might start to make more sense. It's a question of composability with, co primitives, basic primitives. And this, I think, can work. If it doesn't, we can fix it later. I had a second point. Oh, the the the second point was was going to respond to Luke's question about, HLS dash. Did I hear correctly that Colin you were going to write out something for HLS dash, and then Luca's gonna comment on it. I just wanna make sure that we have something here to most forward. Hi. Sure. I think I, I don't know who needs to do it. I volunteered to do it if somebody else with me. Great, but I think we need to do it. It definitely is a valid if we can't, accurately describe that. And if people don't agree at work, we have a problem. I agree. And I think we should we should get that nailed down ASAP so that we can get that PR to move ahead. Okay. Thanks, Suhas, and then Colin, quickly, please. Sure. I think that this this PR is heading in the right direction, and I agree You're Sorry. a little quiet. So you can get louder. Okay. I agree that this is adding the right direction, and I agree with Janelle that this makes things clear. And also a lot of open, edge cases that we had, which the the way the subscriber is defined today, one API to do everything, end up in implementation complexities, especially when you have multiple relationship chain, and make proper decision. And and, I I did read, a 5"
  },
  {
    "startTime": "00:56:02",
    "text": "left some comments on 409, the issue. And I I I think, the the mal that that's multiple things has been raised there, and we need to kind of address them. But regardless, I would say as a group, we need to figure out how do we move forward on this and not not stop the progress here. Thanks. Okay. Thanks, Colin. Quick comment that is wanna remind people to, to remember that, like, if we did the thing where you could do a subscribe, we're going back more than 1, you could go back, let's say, you know, 5 minutes or something. Or 5 seconds, whatever a large group size is. We ran into that problem that if multiple subscribers, different clients all did that at sort of relatively the same time. We didn't know how to conflict, deconflict, all those, overlapping requests. So we had a bug on that issue, and and that's I I I don't think we have a solution to that problem. Alright. That's why we moved away from Yeah. I'll also note that, MoQ does not have a concept of time. And so, though, people discuss time as though it exists, it does not exist. Is not real Can we the minutes please reflect that time does not exist. Yes. Thank you. For something for something that doesn't exist. We're close to out of it. Can I ask the area directors and the incoming chair to come up? As it's, the last thing on our agenda, was chairs dash 5 minutes, and that's because we're making a change this We're also making a change to the area of directors ahead. Get up here. Yeah. So, This is the last meeting of MOU under art. It is moving to Witt So your new area director will be ahead. Has been a pleasure working with you all as much as I have provided not so much document support, but logistics and so forth. So, good work so far. Keep it up. Keep him happy. Also this working group chartered in October excuse me, September of 2022, and Ted promised he would help for a year. He has helped us for a year and a half. Thank you very much, Ted, for all your help. As soon as he loses his dot, this will be your new other other co chair and all that opens tomorrow"
  },
  {
    "startTime": "00:58:00",
    "text": "evening. So you only you're stuck with me and Ted for another 24 hours. And after that, push all the magic buttons. So see what happens when you scribe. Yep. It it's true almost everywhere in the IETF. You take on a little bit of work and they give you more. Is about AppNex. Yeah. Yeah. This is exactly what happens. So thanks, Mohit, for for your service, and I hope we'll have a good time. With Martin. You know, it's all about edit things. So I don't need to do much job. So, yeah, Get back to work Martin. And I would like to extend a very special thank you to Ted for all of the help he's given to MOU starting with the working performing buff. And then through, you know, this has been my first, job behind the desk, and, it could not have gotten where I've gotten without Ted. And so Ted also is getting a bar of mock chocolate. To, thank you for for what he's done. Really have to get care of, and then it'd be really mock chocolate. Well, thank you very much. Well, it's and, yes, and that means he's gonna be our scribe at the next meeting. Paid in advance. Did you wanna Yep. say anything? Thanks, everybody, for being here today and for your contributions to Muck. We will see you on the list. Take care."
  }
]
