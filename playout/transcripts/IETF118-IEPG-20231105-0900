[
  {
    "startTime": "00:03:24",
    "text": "Hey. Want to really named Thomas Kaufman cracked. So there is Oh, well. It's Thomas Graff, Fuh, Fuh, today. Yeah. Thank you. Alrighty. So"
  },
  {
    "startTime": "00:04:00",
    "text": "Hello, everybody, and welcome to AEPG 118 at IETF 118. So IEPG is the internet and plat internet engineering and planning group. And this is a sort of unofficial thing that meets before the IETF actually starts It's supposed to be more operationally focused than most sort of I ETF meetings are or working group meetings are, and, hopefully, we will have something interesting to talk about. The agenda is often filled with DNS and BGP. But this time, it is somewhat different. We do likely have some extra free time at the end of this. So if anybody wants to talk about something, know, and has an interesting presentation, we might still have time We are officially in the Warren waffling for a little bit part of the presentation. Somebody pointed out that I miss that there's an extra off F and Thomas's name, apologize for that. But this is our agenda. Does anybody have any agenda bashing? Did I miss anything that I was wished put in and didn't Nope. Is this thing on? Great. Alrighty. So Without further ado, We will have Mister Jeff Haas, talking about packet discard reporting. Oh, I will mention this is the new meet echo interface. And so you know, who knows if we're gonna get the buttons right? Okeydoke. Let's get going. Good morning, everybody. I'm Jeffaz. I am here to talk on behalf primarily of John Evans, who's the main author along with Alexander about some thinking about what to do about packet discards. And in"
  },
  {
    "startTime": "00:06:00",
    "text": "interesting things we should be doing about it this presentation will also be given in a slightly different context that upstatewg. I'm here primarily because you know, this really is intended for operators, and we're trying to get people involved in what we're doing here. So We've had a lot of presentations in the last couple of, IEPGs about things that happen when traffic is getting discarded and you don't know really wise. You know, a lot of those have been in the context of, you know, very strange ipv6 things, but, no, if you're an operator. You're often chasing down why is things just falling apart to my network? Where where's the it's going, where are they dropping, in Probably finding these things can be challenging depending on context, you know, whole purpose of a network is to get the packets from a to b and packet loss is the one bit of signaling we have, you know, that the network isn't do we get job for some reason. M You know, the challenge here is the packet losses, you know, some regular portion of what the network's supposed to be doing anyway. If it actually doesn't get there, maybe it's not supposed to. You things like discard routes. You may have, you know, firewalls. But for the most part, if you really expecting traffic to get around. You want to make sure that it is, and when it doesn't, That's interesting. Maybe you should do something about that. So the goal here is how do you minimize the anomalous cases, you know, things where packets are supposed to get through and they don't And You know, this is the standard feedback loops you expect off of you know, trying to actually do something about packet mitigation, for losses if you're an operator. Oh, you're paying attention to some sort of source of data, you know, that's telling you that something's going wrong. You are ideally doing some level of organization of that to help you figure out what it beams,"
  },
  {
    "startTime": "00:08:01",
    "text": "and then hopefully that methodology you're putting together will tell you that you should don't do something about this. And this is again where I'd say, you know, please talk to John, when he gives presentation that APAWG coming up later this week because is not theoretical. No. John and Alex are actually doing stuff with this inside of their network Amazon at scale, using automation. So this is not just some theoretical bit of, information model we're talking about. This is there's some steps that try to actually make these things a little bit easier for operators. Yeah. 2nd half of the problem statement, you know, How can we actually report the packet loss? You know, and the challenges are, you know, what are the accuracies that we're looking for, you know, where we're getting the data from? And then, of course, you have to provide enough context. So this is all about organizing information. Where is this at? What is the cause? Is the cause something that's in one of the bins that we're tracking and based on that, is there something specifically we should do about a different type of these things. And, again, motivation here is They are actually taking this information and using this you know, in a backwards fashion for reasons to trigger mitigation. This And for the most part, only a small number of mitigations that they have in their specific bid, you know, with We have sort of overlapping work that we've been discussing from Juniper. Where we're actually Warren, we are judging you for your ringtone. I have some overlapping work inside at Juniper where we're actually now getting deep into the hardware and talking about, well, we have programming errors, and this is a class of things. Oh, they're they're not trying to do something quite that deep at this point. They're a little bit more vendor neutral. And for the most part, this is the kinds of things that, as operators, you're going to do. Would you start seeing some box or some link in your network, being crazy. Going to take it out of service in some fashion. And for the most part, it's the usual thing."
  },
  {
    "startTime": "00:10:01",
    "text": "Take the whole box out of service, take a link or set a link to auto service. If you think that this might have been caused, you know, due to some correlation to something you did, rovex. You know, if you have your favorite, traffic movement to technology, you know, depending on what you're doing, you know, this is obviously group that we talk a lot about BGP. You know what a lot of those things look like. You may put your into IGP overload. There's all sorts of things. And, you know, certainly there's, you know, push the button and say, hey, operators. No. Something's going wrong. Go look at this. So since this is intended to be used for automatic procedures, signaling of this information and you know, being precise about what you're doing you don't take the wrong action, especially if you're auto mitigating this is important So what do we have you know, in the current IETF bin right now, unfortunately, not terribly a lot. You know, out of SDMP land, we have ancient IF table mib stuff that says we have discards and errors and this is what we've been binning stuff in for years. And It's not terribly helpful if you're trying to do anything that's even the least about to settle. But as the tools we currently have, one of the discussions we have is we're working our way through no stuff in yang land is those should be between doer and smarter things. And maybe this conversation starts kicking off some of that discussion. You also have the problem that if you know, you know, all of you who are running things like IP fix, Netflow, etcetera, you realize that, between all your various types of reporting infrastructure, SMP, CLI reports that vendors don't necessarily been the same statistics in the same location. So what's going on here? Have you seen discards? Are these things cards versus the firewall counter or the discards versus packet drops? Cause you have a routing failure. No. This is part of the headache. And now we got"
  },
  {
    "startTime": "00:12:00",
    "text": "sort of motivation for this discussion is can vendors do things that are a little more consistent with each other? Maybe the operators a little less crazy. You have enough craziness of your own. You don't need to come in from the vendors itself. Important part is trying to actually bin things in consistent category. So Not every single discard is exactly the same. Some of these things. Are they full failures? Are they, you know, grayouts? Are you getting duplication. You see things like a interface and a subinterface are bending things the same way. And your counters was I like to yell at, you know, my own developers. It things don't add up to a 100. They're gonna drive people nuts because they're gonna be wondering what's, you know, the real truth here What happens if the reporting mechanisms across platform if you're using things like SMP are giving you different, levels the same statistic. Of course, you know, just Yeah. In general are not granularities we got that talking about. So, Symantics for the packet loss reporting just really are not clean. What should we do about that? So again, you know, John and Alex they spent their time defining some classification scheme, is the primary thing they want to talk about, which is ideally the seed for future work. They defined discard classes, again, working backwards with the things that are interesting for them from a auto mitigation, no situation. They're spending their focus on Finding the semantics of those Discar behaviors. And most importantly, they have, you know, across a number of vendors. I'll let them actually get the precise number at their own microphone. Actually have gone out and implemented this, you know, by various mechanisms, anything from scraping the CLIs various devices to getting down to the hardware levels when it made sense. And they basically have categorize these things into a large number of categories."
  },
  {
    "startTime": "00:14:00",
    "text": "This is the actual meat of the proposal here. It's just and what we would call an IETF, an information model, and I suggested to them as they were working through the draft semantics, agreed to help them sort of wrangle the IT into the things. Is is together information model Get a little bit of agreement about peep you know, among people that actually care about this stuff, as to how we actually track these things. What do you want the vendors to actually try to put these things to the same categories. Their their own opinion about this. They they They do this today. This is the scheme that they are actually using. This is not some completely vaped it in no model. This is something that they're using for their own purposes to trigger their own infrastructure And now, again, Where you put things is incredibly important There's a draft here. Please take a look at it, see what you think about it as operators, you actually want to organize stuff. Again, there's a presentation, ops, AWS, They invite additional, you know, discussion. And the purpose of this is not to end at the information model, is to take this long, the next couple steps, And we're either at at the presentation? No. Here we And again, you know, the steps for their purposes are eventually how do you take know, these various types of binned categories, and decide that you're gonna use these as triggers for remediation of stuff in your network. Now, again, there's, you know, at the very fundamental routing layers of things, there taking devices, links, etcetera, in and out of service, you know, Juniper Land, you know, we're also looking at these things for, like, hardware programming black holes and seeing can we actually, you know, fix those sorts of things that if you've, you know, suffered from a stuck flip queue of some sort. Understand the pain of this as well. And, again, this is not theoretical. This is stuff they're actually doing. So"
  },
  {
    "startTime": "00:16:00",
    "text": "the challenge for all these things is you have a number of vendors a number of platforms, number of ideas about how the you know, hoarding pipelines actually look like and how discards manifest within that pipeline, You have to limit the number of categories you're actually doing these things for. So it's was that model that was displayed the right model across the industry. You know, it's working for them. So the challenge becomes No. Classification of these things. And some of these things are probably good categories that expand on, you know, outside of, you know, these very simple, you know, model things, you know, like, TTL expires as an example. You know, are those things that are you know, related to your tunneling protocols, you know, is CPU issues related to firewalls. Is the no route that you have in there because of, you know, static configuration aggregation, something else. You know, how do these things actually manifest? And and Oh, at the end of the day, and especially if these things are driven by CLI behaviors, you can't actually detect wise, unless you know what the impacts look like. And that can be challenging at a per vendor basis. So, again, No. Is it, information model in there? Data models will come later. The discussion point eventually is what do we do with the enough, what you agree on, what these things look like. And, again, This is something they're practically doing today, this is something they would like to see more people doing, of, their own work. And people that are vendors Could we actually provide infrastructure for this? Does that actually look like? And an example of that know, it could be. Take these infomattels, put them into yang so you have a consistent way of querying these things. Or you will have them stream through ip fix. You know, how do we actually get the stuff off the box? And then, of course, the, you know, At some point, the secret sauce about what you do with these things from a dev ops standpoint. Absorbing this information, deciding that something bad enough is happening,"
  },
  {
    "startTime": "00:18:01",
    "text": "taking no network elements in and out of service. Now that becomes just another part of programming your net They also did an analog presentation. So, you know, that gives you a little bit of stuff to look at a little bit more practical and me waving my hands up here into the microphone about something else that they're doing. And I think That is the end of slides. So, again, the the request here is As operators, You know, these things are pain points that you're dealing with. Take a look at the info model, see if that fits, you know, your own triage procedures. And if not, maybe think about what they should look like, talk to John and us about, what should go into the info model. And then Hopefully, after a little bit of work, we'll decide this sudden, small amount of effort to put this stuff into Yang, put this in IT fix and maybe start pushing vendors to implement. That ends the presentation. Are there questions? Ben Wars and the coup. Q, Hi, Jeff. So It's on. Very quiet. So It's on. Okay. Thanks, Jeff. So I think this is interesting work, specifically to get some consistency But what I was sharing in the, chat is that we've got the IP fix forwarding status information elements, which contains a lot of the reason So it could be consume its management. It's for wallets, so it's dropped. And the drop, you've got already a lot of reasons. Like ACLDNI, drop and routeable adjacency back detail that you mentioned. So that's already what we started to do. So this is somehow similar it doesn't do l 2. Right? It's at 3 only. So there is at least a step in the right direction The advantage of IP fix is that you could define this for whatever key fields you want, so your for your own flows. Exactly. And and one of the discussions that's happening is as you're saying. Some amount of the stuff has some amount of standardization"
  },
  {
    "startTime": "00:20:03",
    "text": "The Juniper work as an example is a little bit deeper into the, you know, affording, you know, pipelines or know, the hardware for detecting the specifically black holding type situations, you know, figuring out the overlaps and not not just adding worse counters to add more counters. We don't wanna do that. Make sure that the counters are really consistent. They fit into the model. N, n if if picking up any one of these items, if that is the correct way to expose that piece of data, That's the right answer. So more just a comment than a question. I don't know. I'm gonna click share screen. It's probably gonna die. So, like, share screen except So, like, here is the IETF. Is that presenting, surface. It's not gonna show up. There we go. So, like, here's some pretty charts and graphs from the IETF meeting network. And there's some spots where we're losing some packet somewhere, and we've got no idea like, the closest graphs. I could draw was interface errors and interface drops and there's some set of numbers. Having better counters and stuff would be, I think, for that. Having the knock actually throw in their opinion would be great. And, you know, keep need to bend while here. If you already have statistics, you should be gathering, 2 Okay? Other questions. You know, again, I'll say WG coming up later this week. Look up, no, John. He's absolutely going to be here, and, he wants to talk to you about, where we take this work. Thanks. Awesome. Thank you And next, we have, I believe, Thomas Graf. Presenting on. Yes. See. Time. I'm assuming you will drive your own clicker thing. Yep. Cancel timer. Let me just stop a new timer for you."
  },
  {
    "startTime": "00:22:01",
    "text": "K. K. Good morning, everybody. I'm Thomas, and will be presenting with, Vincenzo together. Yeah. Yeah. Very similar topic than, Jeff was just presenting before. If you're talking about nitricornomology detection, So wanna bring all the different, metrics, together and also, I I collecting more and more incident data from networks. We realized that, we need to get better organized in order also to optimize, network anomaly detection so that at the end, coming from a, knowledge based, animal detection system, we can go with that additional annotation semantics, more towards supervised and semi semi supervised machine learning the challenge of today's network provide us. So first of all, what we monitor are, routing tables, VPNs, we do that, by taking data from the different network planes described in, network telemetry framework, We as operators are not good enough not fast enough. So, usually, when, outages are happening. We can see that the the scope of the outage the severance and also the duration, is getting longer So, what we see here clearly is that network operators lack visibility. And I think Wogan just showed us in the IETF network up. We don't know why we are dropping packets. That's bad. What collecting data means also we need to get organized so in a big data architecture, the organized data in so called operational and analytical data,"
  },
  {
    "startTime": "00:24:01",
    "text": "So for us, in networks, operational data means network data, which we collected through, network telemetry where we have clearly defined, semantics. Where in analytical data here at the IPF, we are starting to standardize, semantics and one part we bring in is, that we do the annotation of on the model detection data we wanna standardize. Another later explain why. First of all, what is anomaly detection it is in, in a nutshell, basically, we are monitoring changes in a network, And, we our, concern is whatever those changes in the network are actually impacting customers or not. And from there, we wanna get analytical insights in order to do the next troubleshooting step go down to the responsible network platform or network node where the traffic is being dropped. Or for instance, the the topology change was being originated from. We already presented, paper at the, applied network research workshop in, IPF on the 17 in San Francisco. There is a soon also at IEEE coming in more detail, paper on how using anomaly detection in a, in a network environment. Our motivation, of this draft is basically due, through network incident post mortems, we as network operator want to learn and improve and, by collecting that network incident date. We understood soon by having more and more such data, we need to get better, organized, we need to start to automate"
  },
  {
    "startTime": "00:26:00",
    "text": "the post mortem process. And one of the first thing to do that is actually have, well defined machine and human readable metadata, semantics, So that we can draw some conclusions there. When we perform testings with, with with network anomaly detection. And also, to get it's not only about collecting that metadata for for our own network because one network operator is still not having enough incidents by itself to get, to get enough information to to to further improve. It's better to actually collaborate are among the network operators and also among, academic research to exchange this labeled data and, improve the the research on this, on this area. So chief brought a very good example in terms of, we not only wanna understand that we are developing we also want a little bit no why we are developing and the same thing you can do with other network protocols as well. Just giving an example when I wrist of all, reachability I might wanna know from where the withdraw is coming from or why the withdraw is coming from. Yeah. Automoly detection can tell me that at the same time, appearing went down but the causality that the peering went down. And because of that, I'm drawing the the reachability that information is actually quite difficult to Dane, So getting additional information like what what Jeff just described before really helps and operate to draw this conclusion. And also,"
  },
  {
    "startTime": "00:28:01",
    "text": "nominally detection, can also bring some insights or what's happening at which point in time in the network and gives us some insight level this could be related or not. I hand over to Ah, sorry. One more. So first question, to the network operators in the audience. Is this something, interesting for you? To understand, why, for instance, packets are being dropped why ratability is being, based on So getting the causality of those network operational metrics To the network vendors, is this something which can be obtained from the network process. So for instance, a BGP process the trigger to do a very strong do we understand why we are destroying, that that that reachability Is this something obtainable on the, network device? Can be even maybe generalize this among different operational metrics to the academia if we would have labeled operational analytical data, from my network anomaly detection system, describing whatever operational metrics are an anomaly or a normal would that help your research to further improve network monitoring, And to everybody in the audience, having well defined symptoms, with symptoms, I mean, for instance, the action that we are act, action. So, changing a topology in the network. Changing the the the interface status. Or, also having, It helps spike in the network due to"
  },
  {
    "startTime": "00:30:01",
    "text": "congestion would that help in, in in the exchange of data among the different operators to improve their network to automatic detection systems. Thanks. Morning folks. To bring in a little bit of a machine learning point of view into the discussion as speaking about anomaly detection, right? If you like, you can, you can find an anomaly as a change, which, which can be concerning somehow. So if you speak with the machine learning community, you will find out that there are many ways to finalities and one way is divide them in global, contextual and collective outliers. And if you speak with the networking community, you will find out most of the anomalies we are interested in are primarily collective anomalies, which means These are those anomalies. That if you want to spot them, you have to look at multiple metrics, or multiple, measurements across the, the whole network. And this is way more import this is very important, especially from what Thomas the point of use was mentioned before. So, an anomaly can actually, propagate across multiple planes, wanna be able to track that and track some, somehow the causality, which goes from one symptom to the other. This is this is one of the reasons why, it's, we we have been working on defining this, young, data model or actually 2 data models. And, the way we are structuring this is, by finding in 2 different, 2 different entities as symptom and incident. So when we speak about the symptom, what we have in mind here is something like, you know, you know, we are, describing what changed in the network and what is the reason and the cause that, that that that might have generated that change. And what is the concern score, if you like, that"
  },
  {
    "startTime": "00:32:00",
    "text": "what what is the degree of concern we have in relation to that symptom or that change? We use tags. We we are tags to the model, which are there for, know, describing, Netroplane which action, which reason, which cause was observed during the symptom. And pattern might actually help like spotting particular type of patterns we identify in the in the symptom, things like a spike, a drop, or a mean shift and so forth. And, and there is a source there as well, like, why the source is important is because you can think of this model or as a as an an exchange mechanism between not just different human beings or network engineers, which are trying to explain symptoms to each other. But also to, exchange information between a network expert a machine learning algorithm. Trying to say, you know, you you can create labels and use those labels to train the algorithm, or you can get the output of the of the algorithm and validate these outputs through, a human eye is the reason why we have the source there, which we specify if the source if the symptoms have been defined by a human or a a machine learning algorithm, And the other entity, which is more on the analytical side here, is that what I was calling the incident, and, here in this case, like an incident can be defined as a list of symptoms, a collection of symptoms which are seen together. So, again, you're trying to find, like, the the correlation between all the symptoms together and, and you mark that as a, as a single incident, So bear just to just to wrap it up, we are very much looking forward to getting some feedback on this. From, from from the audience, the we hope it's clear, like, why having a standardized and, consistent way of exchanging information in relation to a normal deduction in network is, is important, especially between different actors. And we would be, very happy to have a"
  },
  {
    "startTime": "00:34:00",
    "text": "the possibility to have a working group with, heterogeneous people like from vendors, academia, and, and, and operators. Such that we can, together define a, structure and an ontology maybe for, defining that symptoms and and and operate for operational and analytical hopefully, this work can also help identifying any missing, information that we can get from the network to make this even better. So Thanks very much for for the attention. Tom's in the queue. Okay. Hello. Tom Hilton, BT. I actually quite like this work. I think it's quite interesting. I do have one perhaps philosophical problem. I I wonder whether or not How much are we asking, an individual device, which effectively exists on its own in the world. How much are we asking it to infer? So we're asking that particular specific device on its own to determine a certain amount or infer a certain amount of things from other events that it can see. Do we trust it to make to not make mistakes. How much how much can we trust something to not make very obvious overt mistakes that a human or even even a a a system out of you, both all of the routers, all of the end devices, a very good question. So what what what what we think is you. There is not one way to, actually get a besides visibility from the network, you need multiple angles. So one angle we see is with the collective outlier we are observing basically from the 3 different network planes"
  },
  {
    "startTime": "00:36:00",
    "text": "what is really happening in the network. On the other side, we believe that those operational metric telemetrics itself we could get more additional information, like what chief basically, shows us that we get more information on the causality having those two things together, the outlier detection itself. Which tells us this might be related. And getting from the devices in some areas where it's feasible, also getting the causality we believe that's probably the best way of looking at the network. So in effect. You still expect to have a super system that can understand holistic way, what going on, and it takes clues from devices, based on the metadata that they having having having have generated from from inference. Exactly. Or, to very simplify, what I believe is basically that's the single network node, out of does not know the entire network. Okay. By getting all the information from all the devices from the network getting that, on a central placed together. Perform on a model detection or some sort of analyzes You look at the network as a whole. Yeah. I I can still see a situation where if you have 14 devices, 13 of them think one thing happened. And then one device that actually knows what happened because it was the only one that tell, tell Do you believe the 13 or the the one. And that's the thing that I think starts to bother me is that if we start scoring this, based on a bunch of routers that don't know anything. Exactly. So we have to be very careful. Thank you. That's an interesting piece text."
  },
  {
    "startTime": "00:38:02",
    "text": "Jeff has, 2 things, sort of supporting, and having worked prior for Arbor Networks years ago. His point is very valid that usually takes the network or at least analysis of the network to come up with anomalies. So this is a reporting format for the network. I flashing it out based on those sorts of things we get. The real reason though here at the microphone is to ask either Warren or somebody else in the security area. I have a fake memory, of a working group that was closed that was being done recent ish that did the, security ticket analysis, no, framework. It's basically for automated, get reports. That's I knew something. Ah, so I didn't actually track the work terribly much. I glanced at it in the beginning because of, you know, what I used to do, doesn't that overlap a little bit in the sense of it had to come to some sort of, conclusion that something weird was happening before throughout a request I think that dots was more just signaling of Stuff. To a mitigation service being like, please mitigate my thing although Jen is shaking her head. So okay. I think it was more just like I've got a mitigation service. I'd like to tell it. Please block these sorts of things. I don't think it had far as I remember much, you know, magic automation and a lot of analytics post reporting Okay. Thanks. To to trying to this point, I think, like, 1 month Normally, it's an asthma and normal mode of operation. Right? I guess, yeah, you would I do not believe we can anytime soon, like, have a 100% artificial intelligence, machine learning, ability to detect but I think here you will need to, I guess, adjust each event, eventually, right, and turn that for for that type of events. You would trust one kind of devices more and for another"
  },
  {
    "startTime": "00:40:01",
    "text": "type of events. You couldn't trust anyone. You just probably still need to sign everything. And those people, Yeah. Because I I think it would really depends on the network. What's happening in the network. Right? Because from my experience, I cannot sometimes tell if it's okay or not. Unless I looked deeper and not actually in the network telemetry or anything from routers, but other systems like what kind of maintenance I have scheduled what kind of like, I might have, like, I don't know, like, confidence in the building. Which would look like anomaly in the trache level. Right? And it's something you would not never get from machines. Right? You you get it from talking to reception people. And find out, oh, we have, like, so many users today or, like, something that is a well known example of a massive traffic spike that a bunch of people saw because the Italian Grand Prix had just started. And, like, there's no way that a network anomaly detection would have that external And, yeah, Actually, many cases here, but when it cannot tell if it's a dose, or it's some event and people now want to watch the news. Right? Or it's Apple released an update, you know, laptop decided to go and download them at the same time. It might be also make a normal motor factoration. Yeah. So I agree. I don't think, yeah, there is a, like, silver ballot for this. But I guess, yeah, it would be really nice to have more data in some centralized system and teach it and introduce some custom rules for for each network may right, how to process those rules and what kind of signals Yeah. We want to look because, yeah, I guess your rules might be completely different one So again, I think philosophically, I like philosophy. These these devices are pretty stupid. Right? They're not very intelligent."
  },
  {
    "startTime": "00:42:00",
    "text": "There's no AI. Right. And instead of adding any form of intelligence or any form of insight. We're just asking them to take a bunch of events and infer something from it. Now, now can we, I mean, just one example Can we trust her? Any router, particularly if you've got a whole bunch of them, they're all running the same software version and all have the same bugs. Can we trust them to infer something from their own phib programming book. Yeah. If if there is a situation in those cases where all the routers agree and they say it was this problem, actually is it realistic that we can trust them to infer that correctly. I'm if we got to remember that if at any system that has a view of multiple things, including a human or a robot. Is a god amongst roosters. They are blind. They don't know anything. They only see what they can feel. That's it. So philosophically, is it reasonable to suggest that we ask them to infer more things, and we trust them to infer more things. When they really don't know anything anyway. And is it is it valuable to trust that information in a in a learning model or is it gonna set us up to fail? But it's it's a really Brilliant thing to start discussing. I'm not saying it's wrong in any way shape or form. But fundamentally and philosophically, I don't know whether or not we can actually benefit from asking a router. An individual router too. Make its own mind up about something What I can say is now doing for a little bit more than a year, automotive detection on Swiss COMS network, and we have approximately 14,000 routers onboarded and doing automotive detection, I can say that from the beginning, to now, our insights changed. So, like, certain things we result in the beginning. Well, that just happens maybe, once and, might be something special will be later, the more"
  },
  {
    "startTime": "00:44:03",
    "text": "postmortems you are doing, the more insights you're gaining from the network. You see that there are commonalities among, those different incidents and you start to see their that he's, some pattern there. There is something to to to be learned from. And I really believe I open up the scope and actually not having only one network operate the doing those insights and starting exchanging the data. Among the operators because maybe, one type of incident happen at one operator and it's gonna happen in the future on another operator we can learn from each charter. Doing that in a programmatic, sophisticated way I think in my opinion, that's how monitoring could be improved. And, right, if we can add something here. Sorry for just just one note. I think you can see this as the beginning of an iterative process where you know, network engineer can drive the learning process of automated algorithms you can guys relatively review the results you get validate them, and refine the the the major data that, like, the one that, as we described, and you can provide that back to the system which can learn from your feedback. So it's it's also a way to helped the machine, which at the, at the beginning might be completely ignorant about the network in general, and kind of trying the machine a charity but some other way to, to approach it if you like. Great. Thank you. For listening. Alrighty. And we have one more presentation today for your viewing pleasure. And that?"
  },
  {
    "startTime": "00:46:01",
    "text": "Is mister Jeff Houston on stalling protocol performance. Which seeing as I now have stalling somewhere will hopefully be interesting to me Would you like to drive the carousel? He can drive it I can drive it. Alright. My name is Jeff Houston. This is not a talk about the philosophy of network management. Or whether there's a got it routers or not. Thank god. This is actually a much more pragmatic experiential talk. Because a number of colleagues mine have installed Starlink. Bizarrely, one of them lives in California where evidently the wired connectivity is complete shit. And the only way that it can get decent connectivity is to actually go up. Which I find bizarre But nevertheless, true, and the other one, is my boss who has some property out in if it hasn't burned down already this week, somewhere in Outer Walk in Australia, If if the dish hasn't melted, I think it's still operational. This is actually about the the Performance of Star Lake next slide. Because it's billed Keep moving this forward. Yeah. Oh, okay. You didn't see next. I did. Did you? Did I say next slide? Thanks. I didn't hear him say next slide. Let's all say next slide together. Next slide, Warren, so so just a little bit of physics. Low earth orbiting satellites sheltered behind the Van Allen belt. That way you can kind of not worry too much about protecting those spacecraft from solar radiation. So the highest you can go is around 2000 kilometers. Once you get higher than that, They start to get far more radiation. And you've got to do a lot more protection actually on the spacecraft itself. If you go lower than a 160 kilometers, you run into 2 problems. One is you run into the upper part of the stratosphere and you start to generate some heat you start to lose velocity."
  },
  {
    "startTime": "00:48:01",
    "text": "The other thing interestingly is that At some point up there, Earth geography stops and you become out of space. Now all the countries of the world have agreed that outer space would start somewhere up there, But none of them have agreed at what altitude. So some countries, it's a 100 kilometers. Some, it's a 120. You're not only enough with some, it is indeed a 160 And and so if you at about a 160 as long as you don't communicate down, There are no laws. You're you're in outer space. You're in no man's land. Lower than that, No. No. Somewhere, someone on the earth, someone might claim that they earn a bit above themselves. So Leo is a kind of in that is part of outer space? So as I said, hi, hi, and I have to stop on grazing. But not so high at the Vanawan bill. At 550 Kilometers, is is where Starlink have decided to to station Now the geometry of that is Your 3.7 milliseconds up and down. So a full round trip, which is for is there for approximately 7 milliseconds. If it was on the horizon, Absolutely on the horizon, which is almost impossible. It would be 2000 and 700 kilometers, 2700 kilometers away but it doesn't quite work that way. The visibility of Starlink is from 25 degrees up across know, 25 degrees. And at that level, the, round trip time is is about 11 milliseconds. Next slide. Oh, previous slide. There's a lot of I've got the in on the slides there. What is it? StarWatch, the StarWatch app, there's a whole bunch of these simulate these things around the"
  },
  {
    "startTime": "00:50:02",
    "text": "Staling had decided to go approximately eccentric. So most of the spacecraft are degrees north and 56 Degrees south. But you do find a number of them are in polar orbits predominantly, and I believe to pick up Prudome Bay and the Oilers up there, giving them this because they pay big money. The other thing that also happens is that Starlink, when it launches a a vehicle, releases just a canister of these satellites. And you'll see on this map, there's one of them there, just to train of these things. Over time, they distance themselves out. So that's why you actually see those trains going. Next slide, Lauren, So this is the basic geometry of the Earth station to a satellite. 550 kilometers up, it'll span a circle of a radius of approximately 900 kilometers. Because at 900 kilometers away, 25 degrees up to get there. So if you think about it, a footprint with a radius of 900 kilohertz, where code 900 kilometers is a 2,000,000 kilometer radius, a 2,000,000 kilometer area of the surface If you say how big is the earth, Well, you need 500 500 of these spacecraft evenly spaced and you'll cover the entirety of the earth's surface. That's stretching everything. Reasonably, to get decent coverage from Alio you need between 6 20 times that number. So next slide. Starlene can now at 4 1000 700 or so in orbit their current filings with the FCC point to 12,000. They continue to file. They continue to launch more. The thing is with Starlink, and I'm not sure if I got this on the slide or not, because of the reusable vehicles, the cost per kilogram"
  },
  {
    "startTime": "00:52:02",
    "text": "or payload into orbit at 5 50 kilometers is approximately a tenth of everyone else. They're cheap. They're cheaper than we've ever seen before and the chief and then everyone else. So this is why, you know, they've got 4270 a month or so ago, and that number will continue to rise to at least 12,000 if they continue to file with the FCC, it will rise even more. Because There's no laws. Go for it. Next slide. So this is the other of you, And actually better as a gift. So what the hell? And I spent a lot of time animating this. If you look up, If you Look at 25 degrees and further At any point, you will see between 4047 spacecraft. Wizzing over. Effectively, They will take 3 minutes to go across your visible space. So they're moving relatively quickly. So what you actually see That's too hard to do it. What you actually see is a whole bunch of these things, the we are on the basically moving over relatively quickly between 56 Degrees North And South. This is California, this snapshot, And that's enough in the north the the top you actually see the 56 degree point So most of the satellites go up. It's north, and then they start going back down again. And now what I've captured there oddly enough is a train of satellites that are in a polar orbit. Next slide. So You gotta track them. And they swing across in 3 minutes. Which is really quick Now Starlink actually used a much more constrained tracking system that dishy will track a single satellite for exactly 15 seconds."
  },
  {
    "startTime": "00:54:00",
    "text": "And it starts at 2 seconds past but but but minute clock. And it'll swing to another one at 17 seconds past will swing to another one at 32 seconds and swing to another one at 47 and that's orchestrated by Starlink. So they're basically instruct both Dichy and the satellites where to lock in. The satellite, the the dishy on the earth locks into the satellite by phased array focusing. And for each lock, It has to lock for 11 degrees of arc and then it does another. Next slide. And you can see this. Because what Starlink do do? They don't tell you which satellite they're tracking. But they do give you a one way latency measurement. So if you fob away at the Starlink modem saying, what's your latency value? And you do this really quickly and constantly And then you apply UTC timings to this, and look at the minimum latency for every fifteen second slot You can actually say the point where you'd lock into one spacecraft Follow it for 15 seconds. And then lock into another. And follow-up for 15 seconds. So the blue line is the recorded latency. The red line is the minimum latency for that slot And I'm kinda wondering now because, you know, why the variation within that fifteen second slot And there's kind of two reasons why this happens. If you're following something from 25 degrees, up 11 degrees of arc, you will get some, difference in latency. Because as it moves closer upwards, getting closer to you. So there'll be a slight variation in the RTP. So you should find there are some intervals with the latency either monotonic that goes up or down. I can't see it there."
  },
  {
    "startTime": "00:56:01",
    "text": "The other thing is other people, you know, as John Paul Sartre said, hell is other people. That's my philosophical company. Contribution to the morning. Hell is other people, and the latency hell really is other people. What you're actually seeing here is a bunch of other people's traffic. This is California. Wide infrastructure is shit in that place, and there's an awful lot of sterling, evidently. What you're seeing is everyone else's contributions to your particular health. So it's pretty busy in terms of latency. Next slide. So how does Darling actually work? We'll Each spacecraft has 2000 Megahertz of spectrum. Splits into each channel. Each channel is 250 megahertz each. They use 3 downlinks and 1 uplink antenna Each one does 8 beams and 2 polarization. So you get 48 beams All steerable. Each spot will deliver up to 200 megabits per second but if no one is beside you, You can steer those adjacent beams to you. So at any point, you can actually get up to 800 megabits a second. As long as you, you know, make sure all your neighbors at the power cut or something. They're just not there. So they will adapt and adapt pretty quickly. So you get this kind of beamforming going through where you can actually borrow from your neighbors if they're not active. Next slide, So how do you do this? Well, firstly, the selling But secondly, there's some really neat little tools because if you actually use the the GRPC tools, and just fob away at the modem as fast as you can You get 3 metrics which are interesting. The downlink throughput, which is not what it achieved. It's basically the carrier signal. How much capacity is Starling aiming at you right now when the scheduled capacity. That varies, not by 15 second increments,"
  },
  {
    "startTime": "00:58:03",
    "text": "It seems to vary about 10 to 20 times per second. So it's astonishingly high frequency variation. Why am I telling you this? Because at some point, we're gonna talk about protocol behavior. And what I'm really saying is unlike fiber or wires or anything else you're used to Nothing is stable in this environment. So uplink and downlink change constantly and that's that what they call a popping latency. Which is the number I've been showing actually shows the latency of when you track onto different spacecraft. Next slide. So let's start putting this together and graphing it. This is over 1 hour. And the one thing you notice latency, never constant, never stable, It's just constantly frogging at 15 second increments since large steps, but below that very high frequency. Secondly, that reported bandwidth, all over the place. Literally all over the place. Next slide. Let's blow it up a bit. And even when you try and reduce the magnitude, you see this extraordinary amount of variability inside stalling. Next slide. So now I've gone right down into a small number of seconds. And you can see on latency, the 15 second lots, but look at the bandwidth. Even when I look at just a minute, The available carrier bandwidth is constantly changing. This is like 5 g hell. Where you get you get some channels and you get a few less, then you get a few more, a few less. And it is just constantly changing on you. Next slide. So when you've got that as your carrier, How well can you perform? So the first thing I did was bring up Speedtest. Why? Because It's really easy. Speed test is not TCP. I think it's packet pairing. And so when I run speed test, it kinda goes,"
  },
  {
    "startTime": "01:00:01",
    "text": "Well, across a relatively large period, this is about a month or 2, You should get between a 100 and a 150 megabits per second day and night from the downlink and the uplink in red about 30 megabits a second, right? Because you know, speed test never lies. Sit when it does. Next, Next one, and the latency number well, that's what that's what we said before. Latency of around 50 milliseconds. I'm not sure why, by the way, It is only round trip time, 7 milliseconds away in propagation, And there's another 40 milliseconds of we'll just tell under your packet. Because it's ours. And we're not gonna look, you know, let go of it quickly. We're gonna hang on to it quite some time. So there's huge amount of a latency bank inside Starlink. So that's what speed links says. Next slide. Speed test. I'm not sure that I believe speed test. So what I did was something a little bit different I do a ping every second. 1 ping every second. And that's 24 hours. So what you notice is of those 86,400 samples, it's precisely every second. There's around 4000 single packet drops. They're not clustered The single packet drops Right? And the second part is the jitter is extremely unstable. But we knew that anyway. So you get micro drops and unstable jitter. Next slide. So, okay, let's start moving data. You got 3 choices out there today, you've got Reno, you've got cubic, and you've got BBR. And this is kind of The theoretical performance if you didn't have, you know, a large amount of loss and you didn't have a large amount of jitter, first one is cubic. Blue And what you find is that Cubic."
  },
  {
    "startTime": "01:02:02",
    "text": "Starts to stabilize at a point where it believes it has capacity from the link. And then it slowly ups the pressure until it gets to packet loss, congestion. Once it gets that lost point, It then does a rate adjustment. The congestion window collapses. It drains the queues and starts all over again. So you get that classic behavior in the middle of that side. Which is the signature of cubic. Reno, is simpler. Additive increase, multiplicative decrease, long as you don't get packet drop, you increase the window by 1 packet every RTT, when you get drop you have it. Redline. BBR. There are no buffers. VBR runs blind 7 eights of the time. So for 7 round trip times, it goes I know what the rate is. I'm just gonna send. I don't care what the drop is of this repair on the fly I'm not going to change my rate. Every 8th RTT it ups its sending rate The original model BBR1 was up by 25% So it kicks hard into the buffer. If everything works fine, and I get back all the acts. You go, Yahoo. Let's go faster. If you have a bit of a problem and you lose some packets, which you know, it's not surprising. You just kick the network in the head was talking about every packet loss being a problem? They're not running BBR if it is because BBR really pushes every 8th RTT as hard as it can into that loss point to see if there is loss. It'll push very hard. BBR is very aggressive in that respect. And as you notice, It maintains constant pressure in theory. This constant pressure. There is no buffer formation. It's a beautiful protocol. Next slide. So let's run it on Starling. This is the same link. That speed test says, oh, yeah. 130, 150. No problem. Bullshit."
  },
  {
    "startTime": "01:04:02",
    "text": "When Cuba comes along and the red line at the bottom when I per 3 reports a packet loss event in that second, The blue line is is actually me reconstructing the actual bandwidth packet by packet across RTT intervals. You actually see a small amount of a standard cubic profile right in the middle, where it comes up stabilizes and starts to kick forward. But as soon as you get lost and don't forget, this is StarLink. Loss happens all the time, you never get it long enough for cubic to actually run fast. And right at the end of the minute, cubic is on its knees. It's all too hard. I'm gonna run it a couple of megabits per second because, you know, I just can't open the window. Next slide. Next one more. Could could could quick. Now It's very hard to say that there's a standard version of congestion control in quick. But they all say it's cubic. But I'm not sure they're all actually implementing the same cubic. In fact, I'm pretty sure they're not. And so whether you're running quiche or pico and so on and say, Pico quick and everything else, I think they all behave differently I'm actually running Pico quick inside a tool that I found. Thank you, GitHub. Kupir. And oddly enough, It starts with a really aggressive initial window size. 10 packets. And you notice that little bit of time, It's just jammed the network and jammed the buffers full but the loss event is overwhelming. The red line is just kinda going, oh, I'm not so good. I'm coping with is And so immediately, quick backs off like crazy, And within ten seconds, I'm back down at 30 megabits a second and falling. And it never recovers. Just never recovers. So in some ways, it's actually worse then the cubic we saw before And I think the reason why is I'm not sure that this implementation of Cubic is that robust."
  },
  {
    "startTime": "01:06:00",
    "text": "You know, it's just subtle variations in timers. Not impressive. Next slide. Then we come to BBR, BBR always amazes me that a not everyone is running it all at the time. You know, and b, why not? Because Look at that loss rate. It's constant. And look at what BBR says. Don't give a shit. You know, I'm just gonna run right through this repair at speed. And the other thing BBR says is, okay, you're gonna constantly vary the available capacity Not a problem. I will constantly vary according to what you're giving me. So BDR that looks a lot more like the capacity that you actually get from Starlink. So next slide, the moral of this story, which where I come to the end is if you really wanna use Darling, and make it sing and dance. You've got to run BBR. Standard protocols, Reno would behave like a real dog. I haven't even bothered plotting and cubic simply collapse because The carriage profile of styling is incredibly hostile. Incredibly hostile to normal TCP behavior. So if you actually wanna push the rated capacity through Stalin you need a protocol that doesn't behave like standard loss based congestion control. You need to actually go to a model like BBR that is far more opportunistic in in the way in which it takes available bandwidth and takes incredible sustained random loss and just simply pushes through it. And I think that's the end. Next slide. I believe that. No. No. Okay. So Stalin I don't think we've ever built this since about the 19 60s or 70s in communication systems. Because you've got this really high level of jitter. Really high. Incredible levels of micro loss. Not sustained first lost, but to single packet events that that lose."
  },
  {
    "startTime": "01:08:04",
    "text": "And and that's not something we've optimized our Flow Control protocols for because never seen it before. You know, this isn't fiber, you know, this isn't Kansas anymore. It's a completely different way of of looking at it. Lost based protocols because we always thought up until Starlink, that courage is pretty good. And that what we're gonna optimize TCP for is when I push the buffers to the point of overflow, I'm going way too fast and I need to back off. And and that's the entire congestion control argument how do I sensitize that algorithm? But that algorithm only works when the nature of the behavior is that the same order of magnitude in time Right? As your round trip type. Yep. If you have highly variant carriage behavior, highly variant And your round trip time is out there at 30, 40 milliseconds, you get 5 g. Because the round the feedback control loop will overcompensate and simply go too slow. That's the only way it thinks it can survive, and you leave all this unused bandwidth on the floor. Bad idea. So loss based algorithms simply pull back So if you're using this for short short based transactions. You're not trying to keep the thing running for 3 hours at maximum speed, but you're even doing video buffer refresh. You know, it'll basically work. Because you're only frobing, at the link and then backing off. So even if it behaves like a dog, you really won't see it. But if you line up a terabyte of data and see make it go through, you will notice so Zoom's fine. Large rate transfer not fine. Not fine. So the moral, I actually thought Quick would do better than it did. I believe Quic had a much higher sack Victor. Then stand at Cubic. But it behaves worse. So maybe I was wrong."
  },
  {
    "startTime": "01:10:04",
    "text": "Quick should have done better. It shouldn't. If you really wanted to work using standard loss control, CCP. Use a large Sac window because you're gonna get lost. And you wanna repair it in flight. You don't wanna back off. But better off, Don't use loss based congestion. Use BBR and just simply push through the entire system. And you get much better throughput. Yeah. You know, four people in the field, Oh, cool. That's right. They're not lining up the microphone, are they? Runsically the super Super interesting. Thank you. Just a few points. I think, speed test know if you're talking about speed tests on that. I think it uses multiple TCP connections at the same time, which means it'll sort of fudge all of this, right, because you Oh, okay. Yeah. I use Speedtest CLI. Okay. I I don't know how that works, but, if it uses multiple TCP connections or fresh g connections every now and then. It'll be in that sweet spot at the beginning what it does, like, slow start until it hits the first loss. Right? Other things, I do wonder whether, the 50 milliseconds is because the packet is ours, but it's because if we actually exposed 3.7 milliseconds. All of this stuff were just totally full apart. If you give if you give this, like, if you expose this really high jitter to to Cubic. I think it'll just go I mean, I think in particular, I think the cubic analytics kernel also looks at latency variation. Because it tries to detect sort of, like, congestion by that as well. I think, you know, I'm sure people can correct me. But again, if you if you exposes 10x latency delta, it it will probably just, like, work really badly. So I wonder whether that's What's that going on? That's a really interesting point. Don't forget the entire business case for laying the hibernier submarine cable across the transatlantic. Was 6 milliseconds faster than everyone else. All of a sudden you have something up in the sky, that is 50% faster than fiber"
  },
  {
    "startTime": "01:12:00",
    "text": "if only you could expose the rule propagation latency, they would take the entire world's low latency business. Daytime traders and simply monopolize it. And so I'm kinda surprised and eyebrows are lifting. That they're actually holding on to the packets. For much longer than propagation would suggest. And I agree in some ways that there is a trade off going on here between know, FEC and a whole bunch of other conditions that I thought the big game was going faster. And Stalin could but it doesn't seem to. We've got a pro if you had BBR, it probably would. I I who knows? I do have VVR. It does seem to lie. Who knows? Maybe in a few years, they'll sell a new class of service for the ultra low latency. Ten times the cost? I wouldn't put it past them. It's it's a $1,000,000,000,000 game low latency and and First of all, really interesting data. Can't hear myself, but I don't hear the knock so it works. There any documentation on how the physical layer works here? Because, Probably not, but you, you know, it's a it's a satellite. It's a shared medium. You cannot do, collision avoidance by listening for other people sending. You know how this this theory went that Google says nothing publicly? Compared to Starlink, their chat there is, you know, chattery as hell. They tell you everything. Starlink tells nothing. So what you actually all of this is reverse engineered in academic papers. There's nothing out there. So, yes, it'd be nice if they publish more about their protocols, their controller, I don't even know which satellite they're tracking anymore because they included that information. So all of this is is 2nd level engineering about what the hell are these guys doing? Yep. Yep. Yep. I I I expect that that's that's So if you but if you look at the upload the upload latency, if you look at other protocols like DOCSIS, cannot just all send random data, but you need to get an upload slot before you can actually sends data, if you send bigger chunks, So there may be multiple round trips involved with the satellites before you can actually send your brackets. So"
  },
  {
    "startTime": "01:14:02",
    "text": "it may be interesting to look at the physical protocols to do something similar and see if you can infer behavior like installing. Because what you may observe, what you observe with DOCSIS nowadays is that if you are trickling data constantly your latency is lower than if you actually are not actively sending data. It's an interesting point. I suspect and, you know, they published nothing. That the underlying technology is 5gfentocells in the sky. With, you know, a lot more power, a lot more antenna, a bit more control systems, but it's the underlying the technology is very similar. The other thing I suspect that makes life bizarre is Each spot being missing to square It's actually a variant signal to noise ratio as it passes through. And so you actually find is even if you're tracking in those 15 seconds, you get an astonishing variation in signal to noise quality. As the thing is zooming over and that will affect various signals and noise, the clarity of the signal the amount of signal it can actually push through. So all of those bits of physics impact upon performance. You know, and and and I I I don't design this kind of crap for a living. I have no idea how it works. I'm at the protocol at the guy. Wow. Why is cubic so crap at this? You know, so yes, trying to actually relate the 2 is it is is it piece of guest work right now. That's that's wonderful. Interesting. Your issue about if you pushed hard try to see if there's latency It's an interesting thought. I've seen in the papers is that Stalin command Each satellite to go to that user, that user, that user, in 15 second increments. So this is Starlink control system. That simply schedules capacity to dishes Yeah. It's mystery. That's at the moment. So, sorry, first, lovely to see the data. That's great. I've done measurements and"
  },
  {
    "startTime": "01:16:03",
    "text": "are the people who've done measurements that are on style links, so it's great to have more It's interesting to see how it's changing with time. You didn't cover the aspect like 6 months ago, it was different. This 6 months, it is different. Etcetera because I think it is evolving Unfortunately, it's evolving partly by satellite payload, so it depends which satellite is all for you when you do your measurement rather than Yeah. And we don't know which ones are which anymore, so this makes it really, really confusing. So Good to have the data. I'm a little bit skeptical about people who present things on quick and then use just one pick or quick, which is a particularly esoteric version of quick. So good to have data, but, maybe you'll get a different data if you use a different method different different different different implementation. There there are papers out there on the variations between implementations of quick, and these are fine papers. And what it points to is that once you lift a protocol from the kernel to the user space, All bits are off. Because your implementation, my implementation will have different choices. And to think that somehow you and I adhere to standards even as religiously as vendors do and they don't is kind of wishful thinking. So yeah, you know, trick is quick is all over the floor. And finally, BBR, BBR is a bandwidth estimation method. So it tries to fill use all the bandwidth for the flow. Whereas Cubic and Reno are capacity sharing their business participating and multiple people are going to be sharing the bandwidth. If you type multiple BBR connections up, they don't always play well with other people or with themselves, So That's a that's a different pack and a different slide set. I do have that. It is. Yeah. Something to be aware of when you tell me BBR is the best method because BBR might be the best for one case, and it might be brilliant. Exactly the right for that case. And in a different scenario, maybe you want something else. So"
  },
  {
    "startTime": "01:18:03",
    "text": "don't care about everyone else. It's all about my starving and my Oh, just use PPR then. That that will definitely help you. Absolutely. Hi. Greg Scholes, ISC. Thanks very much. That was very interesting, Jeff. Was just thinking about Flow Control algorithms and whether there's some scope for some research into industrial process control because that's been going on for decades decades. And, you know, there are some very complex bits and pieces out there and you can just buy an off the shelf thing that just self tunes and worked it out for itself. But so there's there must be some, you know, very high power maths going on behind that that has just been commoditize now. That could maybe put be put to use in this kind of a situation where you don't understand what you're dealing with, but it can work it out for itself. The philosophy of the design of TCP I'm getting philosophical here network people. So, you know, we're gonna we're gonna launch into is there a god of protocol the philosophy of the design was trying to achieve 2 objectives. It was trying to make sure there was no slack bandwidth on the floor. That in the absence of everything else, the network capacity was used. But the second thing was It was trying to be friendly to everyone else who was active at the same time. And the resultant sort of sort of the church of Berkeley TCP was this philosophical idea that as long as everyone approximately uses the same congestion control protocol, They'll end up being friendly to each other. And so you've kind of moved those 2 objectives. BBR throughout the second objective. It's not that friendly. Because its idea of using Yeah. Capacity. Was so totally different to the traditional model of TCP. And then when you start to talk about What's the feedback model that controls it. With Stalling, You're not on wires anymore. And the whole model of where TCP was does not fit in sterling."
  },
  {
    "startTime": "01:20:03",
    "text": "You know, that's what we're seeing here. And then it's a case of I change the world for Starlink? Yeah. Right. That ain't gonna happen. That how do I sort of integrate a styling link into a longer end to end is a really interesting question Because, you know, will be I'm not sure how many folk, but at least a billion probably behind Starlink, whether it's a year or 2, if Mister Musk has his way. And if it behaves like crap, then, you know, that's no good for everyone. Thank you. Yeah. Thanks. Runs according to servers clearly need to be able to change, congestion algorithm within a given within a given connection, see what works best. I'll leave that left as an exercise to the reader. I had a question, actually, you, you pointed out this 15 second switch where you, you change Right? I like, what happens while it's switching? Does it lose packets that have 2 radios? It attracts most of them same time? That that that that that that won't move forward forward forward. So that one, I'll have to move the dish. No. It doesn't the dish hardly moves. Most of this is phase gray. Next slide. Okay. Is a red one here. That one. I'm peeing every second. Every second. So I should get 14 clean and 1 bad it does a a switchover. Right? But don't get that. Yeah. I kinda get this dropped. And and there's no persistent time signal behind that the points where that one second ping actually goes away And so it's kind of a mystery to me y. And the modem, dishy, is telling me, bugger all. You know, all I dropped a packet, no such, no such report. So So it doesn't need time to lock on to the new signal or anything. It seems to pre provision. It's a big phased array antenna. And while it's tracking 1, it's getting ready for the next. So In theory, in theory, the switch over is seamless."
  },
  {
    "startTime": "01:22:00",
    "text": "In practice, not quite. But it's not related to switching. That loss profile is not a satellite switching loss. Otherwise, it'll be 15 seconds bang bang bang. Sorry. First again. Yeah. I concur. We don't see packet loss in handover. There is a small trading interval inevitably from the design of the modem, which you will see as it competence for doppler shift and retrkes. This is not visible at the user layer with the protocol at all. You do see planes and different satellites. If you actually very carefully, the RTT variance, you can actually see it bugs. He's mainly smooth out. This is an issue. No loss. That one. I mean, it is impressive how well this 6, It's amazing. Works at all. Truly. Yeah. Yeah. Hi, Bob Hindit. Different question. So they have different types of dishes, you know, the user dishes. There's you know, the one that's on a little platform, and then there's a larger flat one like for maritime, Giovanix Do you know if there's any difference in performance? There there's 2 services out there. Gory probably has more details than that, but there are 2 services. One is business. And it just seems to be a bigger dish with more antenna, which improves the signal to noise ratio and I'm not sure if there's any change 1. And the bigger heater. A bigger heater and you pay more money. And in in theory, outcomes more bandwidth. Underneath that, I think that's the only differences out there. The rest is how much protection you get from the elements, whether there's a dome, etcetera, but that's almost nothing to do with the signal. So maybe Gary has some It'd be interesting to see if this behavior changes with having a the bigger antenna. I have one of the bigger dishes if you wanna run stuff from it."
  },
  {
    "startTime": "01:24:00",
    "text": "My house is like near trees and stuff, so I had to buy the bigger one. And the trees went away. Yep. Sounds interesting work. Gary, yeah, the the maritime Legal Terminals basically do a lot more doppler shift compensation because the ships actually move the antenna base, and that is a huge issue in trying to get these satellites. There's more complicated electronics. If you get it right, then it doesn't change the signal. I've been tracking the dish itself. I mean, I had another animation of the dish actually says where the dishes facing, not the phased around tender, just the dish. And it's pretty stable. On a ship. But not on the ship. Only where it's stable, you know, not in earthquake zones. Yeah. Or in a normal place, the dish smoothly tracks from Horizon to Horizon. And this No. No. It tracks 11 degrees. Just try to 11 degrees. Yeah. This depends on where you are on the earth. No. The 15 second time increment. Yeah. 11 degrees is 15 seconds. Maritime terminal is better, doppler shift compensation. Okay. Thank you. Well, that's it. Thank you. Sweet. Is true. Anybody got any other loss, something, oh, can you have an animation? Does it worth trying find that and display it or whatever you if you want labs.ipnet./ labs, start even slash stomach. Yes. Smash Thonic. Animation time, Pretty big chairs. Go down all the way to the bottom. We Okay. So this is accelerated. It's not real time. Share share settings. Performance plots. Yeah. Hold on a second. Purity. And I don't know what it means, but already God, that's crap. That's the thing with the scrolling and the thing and the Yeah. Actually, it is very"
  },
  {
    "startTime": "01:26:03",
    "text": "Some reason, the video completely this is sort of accelerated time. But this is kind of the visibility if you look up. Starlink, the dish will not look at the sun, and it will not look where geostation result. So it will not actually track across the equatorial plane. Which generally picks you're in the northern hemisphere, it'll pick the northern hemisphere tracking the top half. If you're in the southern hemisphere, it will pick the southern because there are more satellites there. Because the poll is closer, you know, to each other than the equator. But like I said, Starlink doesn't tell me which satellite it's tracking, so I wouldn't have a clue that's what it sees when it just looks up. Which is cool. Hi, Jeff. It's Tom. From BTEC I have Just one quick question around the topology at the moment. Are they still using ground stations with Starling? Or if they switch to They're using both. And I had an interesting conversation with a gentleman from Mongolia where they're actually doing remote schools over Starlink. But for reasons, There are no worst stations in Ullandbator or in Mongolia. The They just stopped and reasons. So they're lifting them across to Japan. So into satellite, inter satellite to Japan and then holding them back. I think the Curibas Islands in the Pacific And if you wander into the bits of Australia that aren't burning, somewhere in the middle of nowhere, same kind of problem. There is no worst station you know, within 1000 of kilometers you're doing the links already. So they do have it. The latency is lift from we saw before between 40 and about 70. They lift to about between 55 to about 90. Which is not a lot. Because so if speed of light They're not that far apart. Seems to work. So it the latency is in general, worse when it's only about 10 to 15 to 20 milliseconds worse. Okay. The throughput and so on, it actually seems to be the same. And and does it improve on the jetta?"
  },
  {
    "startTime": "01:28:02",
    "text": "Not I haven't done extensive tests because I'm not in Mongolia. It's okay And lastly, have have you considered maybe trying any of the other constellations out there. This is purely opportunistic I just got a an intel nuc and put it right beside fishy in in someone's house. Which is what I'm thinking here with Warren, and it's just been, you know, playtime. There's nothing terribly structured about this. I was just fascinated about, you know, How fast does it go really? If I see an opportunity for you to try another one, I'll let you know. Thank you. I'm sure we can arrange something. Thanks, Warren. Oh, Hi, Taylle Royvononker, and I just wanted to say that I have direct experience with it, but not from a research perspective because as a rural remonter, with a really poor Internet access. We have found it to be a god zone. Right? Like, yeah, a a b a is as a consumer who just wants to, you know, my family just wants to surf the web and watch videos and everything. Delayed for. Right. It's just when you'd line up, a few terabytes of data and you look at the throughput and you go, why is throughput so crap? Right. Right. You know, that's when you find the thing falls apart. But short transactions, zoom, that kind of video stuff. It just seems to be seamless. Right. And so overall, I I just wanted to I'm I'm not I'm not gonna sell it either. Like, in fact, I'm also a member of the CV5 reward where trying to get regional fiber in But it's, you know, Yeah. In in practice, it does worked shockingly well. Base I was using it, I was a wireless link to a wireless link to a wireless link to a wireless link to a DS3 to the internet, and the wireless links are literally, like, 80211 link during the day, it would be 30 megabits per second? In the evenings, when other people use it, it would get literally, like, 15 kilobits per second."
  },
  {
    "startTime": "01:30:00",
    "text": "And stalling, made my life less sucky. It's not perfect, but But yet, was fascinating. Thank you. Thank you. Thanks. And that is the end of Oh, everyone, don't forget. There will be another IEP GME ring, in IETF 119 Brisbane where I work over the hill we are next time. Please think of interesting things to present."
  }
]
