[
  {
    "startTime": "00:00:04",
    "text": "I\u0027m a tree where we are so it\u0027s a little bit of motherhood and apple pie across all layers networking application and business and what well ITF does and could even do in that whole Stein domain from the amount of people in the room I\u0027m kind of staggered that means that that statement might be right or wrong so monitoring try saying set really a target for some real innovation I think over the past couple of years we\u0027ve seen that it is and let\u0027s go and as I said earlier on let\u0027s start across all layers and start with a networking layer there\u0027s most familiar to us and if I go back and go back a couple of years that\u0027s what we had right we had come online interface we had SNMP we had syslog and that\u0027s what we use to extract information from the devices and we said well that\u0027s not super sexy necessary necessary evil and to be honest also not really fast to go and extract data it never ever gave us a complete pick a complete picture of what\u0027s really going on on the box because if you look at the recent router there is something like two hundred and fifty thousand different counters available two hundred and fifty thousand different counters or classes of counters or types of counters because if you look at say bytes received in an interface that\u0027s one of them but there is kind of a raft of others what we were typically extracting was such really incomplete information and there was sometimes not even in CLI to get at all of that information and it was specific to a particular box as a consequence people weren\u0027t really looking at all that data because they couldn\u0027t look at all the data and was really hard operationalize and we made that overall thing more fuzzy by adding additional stuff on top so you pile not only per device information but also per flow information or even per packet information with IOM then it\u0027s getting even harder to operationalize so well we need to go and move to make this overall thing easier to automate easier to consume so that we ultimately are more easily able to reason about that Network which means well we had to go and give us a way to get the entire set of data that we have on a box so all these 250 thousand plus counters and get them in a digestible format over the wire in a common way so that we can go and operationalize that data and use the data as opposed to just sits there and I think this is pretty much what telemetry is if you go with a the original definition as you go to say Wikipedia or even what Mesa used telemetry is an automated communications process by which measurements and other data are collected at remote or inaccessible "
  },
  {
    "startTime": "00:03:04",
    "text": "points like the network and then transmit it over to some equipment where it\u0027s gonna be digested so we\u0027re gonna go and use this in a relatively loose way because telemetry is kind of one thing for one guy and another one for one and another guy so but it\u0027s basically gathering operational information on packet speed from applications and shipping it somewhere where it can be digested and let\u0027s go with that for a further time being for today at least with this benoit thank you Frank so we\u0027ll play ping pong game between the two of us during this session so okay we need telemetry so there are three neighbors for telemetry in the network first of all don\u0027t pull push it\u0027s right if you ever try to create a BGP table is an NP well you understand why now the second one is analytics right it must be antics ready I will cover that and the third one it must be data model driven because it\u0027s required for automation so let me show this to you right an interface we are networking people we know one interface is right show IP interface it\u0027s a gigabit interface great now well I receive a syslog message syslog message is like gig or Giga or whatever the mapping the name is T not even the same right now if I see if you know if I pop is MMP this is the if\u0027 index the unique ID for interface right now if I go into yang what this is the highest - index we were clever to make to have the same semantics between us an MP and yang great but that\u0027s a different field now let\u0027s go to what NetFlow an IP fix this is an interface but pay about patterns about that I mean we\u0027ve got the different semantics this is the Inger s interface or the egress interface even if we map it with the if\u0027 index you see it coming right now let\u0027s say that I need to analyze the triple-a information well you take X plus that\u0027s something different I see you\u0027re smiling already but I mean this is reality right in automation now okay I use radius guess what it\u0027s enough sport and I speak here about a very simple information which is an interface right we know when the face is but if we need to automate this this is very difficult so what are the solutions solutions are okay I\u0027m going to develop an expensive mission function that will match information from the if\u0027 index the eyes - index and if it come from a cystic I\u0027ll do a grep and hopefully "
  },
  {
    "startTime": "00:06:05",
    "text": "people will have implemented the right interface naming convention and if this is net floral say well okay there is extra semantics there or I\u0027ll be using the same data model in order to have the same semantics directly they one and as wide as data model your management is required what is important is the semantics right we don\u0027t care if you send this over JSON or XML or protobuf right this is irrelevant to me but having the same semantics is key the next thing we want is the analytics ready data okay great I\u0027ve got the time series of a counter okay will it come from what is the information for platform what form is this which iOS version is this like a deviation in yang is this like a specific image from engineering this information must follow the streaming information right same thing about the data manifests the how and the one the data were measured if you go back to the old net flow this was sampling without sampling random sampling whatever but unless he follow you unless you use information along with the streaming you won\u0027t be able to interpret this into your back leg right now you could say that data models are defines what you want to stream right but it\u0027s equally important how how do you want to consume information and this is tuning aspect so let\u0027s focus a tuning aspect there are plenty of tools for data models right I could have like 15 slide let me show just couple of them be I\u0027m to show us in a model to the three there is this catalogue there that gives you all the models that you could think of right including the one from different SDOs and I will come back that point right there is I Triple E and math and Bob and forum and all this so there are tours for that for telemetry there are some tools we\u0027re not there yet this one is the advanced that come Explorer you just go on a device and then you see okay what can you stream to me and from there you subscribe and you get all information directly to you the collector right another one this one is like 11 days old Telegraph right if you if you hated pipeline this one maybe is your friends that\u0027s the way to basically have model driven telemetry with what G RPC dialing dial out with TCP with gin am i right so there are tools there on telemetry but we\u0027re not there yet like Penang is d2 to start with for "
  },
  {
    "startTime": "00:09:09",
    "text": "yang models we need this for telemetry as well this one too I mean yes we all want you to go into this data modeling too and management in telemetry right this one will help the operators to move away from mission and PU IDs from eco IDs to telemetry yang telemetry so it\u0027s a mapping to say if you were using these Ioffe index and these counters these objects in SNMP this is the mapping to yang so there are some tools but we are at the beginning now if I try to summarize where we are in terms of network telemetry at least one part of it because Frank will be covering in the part under what\u0027s on the models we are there we have a specification of a yang of a Netcom for s confid cetera the tools we are there right we\u0027ve got reference code the industry combination you\u0027ve seen that with the catalog there where we try to correlate what\u0027s happening in open source in SDOs even for vendors etc so what you see as a theme throughout this session is it\u0027s not good enough to just do ITF specifications we need specification choose code and pollination industry because if I look at the right-hand side for telemetry well guess what we don\u0027t have the specifications yet after four years in the ITF right however we got some tools there and we\u0027ve got some code how come because the two that are in there and the coaches in there is not based on the ITF strategic edge that we don\u0027t have yet there are based on something else does it matter maybe not my point is that we need all of them alright so now let\u0027s move into octavarium was frank with in the during the presentation or after maybe it\u0027s maybe it\u0027s easier after so that we can go and condense this overall thing a little so from well what you did is go and collect information from an individual device and reason about that one day our up is well you might want to go and get a little bit of information about the overall network and how data has progressed and we\u0027ve been using ping and trace for quite some time and well more recently what was recently a couple of years ago even we did run way active measurements protocol and two-way act of measurement where you basically have a control channel between client and server and then instructing how the test would go and then you get more accurate information between these two devices I think what I want to go highlight these "
  },
  {
    "startTime": "00:12:09",
    "text": "tools are there were specified here and the other good thing and that\u0027s what Benoit was referring to in order to get these things deployed and useful we need something that looks like a reference implementation so that people can go and consume it om was done relatively early on as far as Internet to there was also t1 code out there you can grab it you can containerize it and you can ship it on well any platform that runs containers whether it\u0027s a router or an end host you can use that to do real accurate measurements these days fine now up leveling the conversation a little bit so we did observe and go and do what people typically refer to as streaming telemetry on a per device basis so getting all these 250,000 counters out of the box or a subset of those we can do active throwing but there is something missing right so how do you measure the life user traffic we\u0027re just kind of moving into the domain that I care about and that is in sitio a.m. which means per packet elemetary understanding what the packet does as it traverses through the network ie the customer traffic and not pro traffic so for those of you that haven\u0027t seen in situ OEM what is it does it do you receive traffic might be streamed directly from the host so you start this whole thing on the host itself or somewhere in the network editor main boundary you\u0027re adding metadata to the packet and you need to find a place in the package where the metadata that\u0027s the ongoing conversation we\u0027re having here and you\u0027re dropping in information that well it matters to you in order to understand what the packet does in the network if you\u0027re interested in understanding where the whole thing accumulates delay then you would add timestamps to it if you\u0027re just interested in how it progresses through the network you\u0027re adding node IDs to it if you\u0027re interested whether it follows the right path the path that you were trying to go and preset by using something traffic engineering be its segment routing be it RSVP whatever then you would go and put pieces of a secret hop by hop so that you can either later on reassemble the secret and check you do on the right path or you can\u0027t and then proof of transit fails that\u0027s what we\u0027re doing in the SOC working group right what we decided early on is well we don\u0027t want to go and get locked into one or the other well what I would call parent a protocol because many people are on different protocols but we want to go and have one set of data fields that everybody that is subscribing to this particular technology would support so we started a draft and that drive is adopted in AI ppm where we\u0027re just defining the data fields of what you carried ie timestamp node ID trooper transit marks sequence numbers if you need them and the likes so we have information that we carry on "
  },
  {
    "startTime": "00:15:10",
    "text": "a per node basis and I mentioned some of that we mentioned we carry things that are just applicable to a set of notes if you just want to go and prove the transit to a set of notes and there are certain things that you only knew you need edge to edge so if I want to go and derive the network traffic matrix maybe just all need its sequence numbers between here and there and I can understand where the drop is where the drops are and where packets are put in and spit out we can carry those things in various protocols and I think that\u0027s the journey at hand that we\u0027re doing and well that\u0027s a journey because there is many protocols and there are many protocols that have capability to carry metadata and there\u0027s many protocols that we can go and insert that metadata into and while I\u0027m mentioning a few there is a working group draft in an nsh and we are well debating how to get that into v6 and even more so in to v4 before is hard right everybody runs it but nobody well officially wants to go and touch it anymore maybe we need to do that so from a standardized generalization process perspective we want to go and draw a picture everything that is common is done in IBM data formats how do you operate the whole thing we need a yang model for the whole thing to end last point in order to operationalize I am and well you need to go and extract the data from the box again something uniform so that other people can go and digest it because that\u0027s just one source of information and we might have multiple other sources of information as we will go and progress and then well you need to go and do the uncap game an endcap isn\u0027t easy so we have individual working groups spinning on an end cap then you go back and forth between IP PM it would be nice if we had something that would streamline that process we don\u0027t have that so far but from a from a messaging perspective well we have this as a standards effort how do we get there we started this journey not with a document we started this journey with running code so when we first started to talk about I am at ITF in Berlin three years back not sure where we were in Berlin we started off saying well look at this we\u0027ve done an implementation in vector packet processing s open source and you can go and look at it and run it since then guys in Belgium at the University of liège so professor Benoit donae and an team they\u0027ve done a version for the Linux kernel and they\u0027re continuing to evolve that I just heard that they\u0027re implementing you\u0027re full version of the data draft so we\u0027ll have something for the Linux kernel that is pretty modern very soon and people have been embarking to go and put their overall thing into silicon based on that we\u0027ve done an open daylight implementation for kind of how to go and configure the whole thing that\u0027s based on a really old release carbon so that came out a while back and "
  },
  {
    "startTime": "00:18:11",
    "text": "it\u0027s part of the SFC code there but I wanted to gonna gain highlight we did the tool chain and it was really running code and then kind of how they embark into the standards journey and yes it changed but still I think it is what got people interested the next steps are operationalizing the thing and I think we\u0027re not really there yet we started the first steps but operationalizing means how do you go and consume this in an automated way configure it in an automated way so that aligns with everything else so I think the natural point is your favorite word it\u0027s not sure why right so how would you give it scorecard you know on spec on Tues you mentioned code and you know industry adoption I never ever thought about that question which is why we have a slide on it so yeah we\u0027re doing well on I think ITF specs it could go faster but yeah coordination is coordination tooling Wiese yeah not really yeah I could go and give me even a little bit of a C maybe because there\u0027s something in open daylight is it really maintained not so much we have reference code that\u0027s cool industry coordination I think we can go and chat about that later on and how far we want to go because I believe that if we can go and find more of a commonality between what happens at layer 5 and above from a tracing perspective and what we do at layer well up to 3 we would do a great benefit to the industry so let\u0027s go and uplevel the conversation to what happens at application layer and how do we rope application visibility into the overall flow so that we do something that is not there only for the art network operator but also for the app developer and the guy who runs the app be the CI CD person or some other operation your team because the funny thing is if you look at the questions being asked there are very similar so which is why well look at them right my requests are slow why who do I need a blame if I have my tracing tool it tells me the HTTP requests are slow eventually at school I already get that to that level but drilling down further can I blame the host sect the network or even do I have to go blame myself I don\u0027t know database lookup is slow why so all kinds of these why questions when it comes to I can\u0027t really reason about why my application just something that I didn\u0027t wanna go in half and if you look at the landscape today from an application developers perspective you\u0027re doing stuff in multiple frameworks and you have multiple messaging systems underneath yes well we\u0027re moving to a world where everything is a little bit easier with containerization everything runs on kubernetes eventually and yada yada yada we need to go and get there right not every single request yet is based on HTTP but what you do as an application "
  },
  {
    "startTime": "00:21:13",
    "text": "developer is today you\u0027ve gotta go and pretty much marry yourself with one of the many frameworks on the right hand side because you\u0027re gonna go and instrument your code with their api\u0027s then you run their libraries to go and get the thing into their particular tracing environment and stats environment and that means well you go and if you create something you go with that marriage and you keep with that marriage for quite some time and breaking up is hard can be done like in the real world but it\u0027s hard so if you look at this overall picture a little bit like you have the developer guy that needs to go and instrument his code then you have an instrumented library that typically comes with your vendor package today then you have something that exports this whole thing and in many cases even the tracing backend is linked to the agent at the exporter so all of that is one particular vertical ecosystem and well then you have the consumer on the other side so we we don\u0027t only have like in the network operating say a space the operator but world so half the developer that has a savior and the question is can there be standards because yeah well multiple silos everybody doing something in a similar way can\u0027t we just have one way to instrument the code so that I\u0027m subscribing to one API in my multiple languages and then well run from there and the answer is yes but it\u0027s threefold or multifold so one effort that Ben psycho-man side off on initially was open tracing and opens racing just just tries to standardize the API that\u0027s what they started off with so that you have a standard API and that you can have multiple people implementing the backend of that particular API so you instrument your code then you get from somebody else an instrumented set of libraries so that you can don\u0027t spit that data out they\u0027re also trying to eventually get to harmonize what\u0027s going out from an export format perspective what the format is really the the tracing piece the tracing API piece multiple people have started to implement that I said well and so I goemon started this whole thing so lights tab is obviously one of them that supports the thing but you have also open source frameworks like in C and C F we have Jager as a tracing framework Jager supports open tracing and multiple others like pick your choice there is multiple of them I\u0027m listing a few there is another group also in C and C F same forum different efforts that tries to go one step further don\u0027t only do the API but also "
  },
  {
    "startTime": "00:24:16",
    "text": "build a tool chain along with the API so that I not only give you the API and then say well vendor a vendor be vendor C please implement that in your particular ecosystem but go and build me a tool chain have a library an instrumental library that goes hand-in-hand with the API and an export infrastructure that can link to the various back-end systems so that I can own export to say well Jaeger again Prometheus a couple of open-source environments what also well more like proprietary environments well int the guys that complement this and those are third group by a Louis right right power I think that\u0027s the right name that is driving of dynaTrace I think he\u0027s the CTO of Donna trice driving the work of w3c where they\u0027re trying to go and specify well how do you go and transfer transport racing information or trace data using well parse off off the HTTP header so it\u0027s another portion how do you get that whole thing across the wire so how do these different guys apply in our framework so I try to going to pick that a little bit so open tracing is really drawing to go mostly focus on the API and how do you instrument your app that comes with libraries which is why they have a say there but it\u0027s a vendor that implements that open census really tries to go and do the overall developer ecosystem tool chain that they\u0027re trying to build and yes well all of them want to have a say of what goes on to the wire eventually from a trace theater standardization perspective are we there by no means so there is open tickets in in open tracing for instance on how to represent the trace data maybe with JSON on the wire and then well you\u0027re gonna go and spit that out into your tracing back-end and that needs to go and consume that and expose that to you and so all of them despite them quote-unquote not being necessarily completely on the same page have the same tracing model so you have a trace and that is kind of here we\u0027re just querying well we\u0027re trying to get data into into a particular cache so authenticating we\u0027re issuing a cache get then we have a trace on that trace reaches out to a back-end my see a query that ghost comes back so you see that darker gray that goes off to a different box and then we finally can go and update the cache the individual steps are so called spans and you can have AI hierarchy so that one trace has a child trace or one trace as a parent parent trace so that I can go and correlate these individual things later on from a tooling perspective into one big picture that\u0027s awesome and you have tools like Jaeger gain the CNC F project where you can go and look at these "
  },
  {
    "startTime": "00:27:16",
    "text": "traces and then you can drill down so you can see exactly on a per request basis what happens at what layer when do I submit a Redis request the HTTP HTTP GET call so i trace all the way down even with kind of details on kind of how long the call took from when doing so I get really detailed information on how that thing progressed so that\u0027s all cool now as I said earlier on maybe the HTTP GET request took us 652 milliseconds where did I spend the time do I blame Network TCP stack if the network which network element where did the lake you up what can be done attribute be attributed to the network and how so could we marry that and that\u0027s a discussion that the open tracing people and the open census people are really interested in heaven but the question is how do you go and get the trace ID that you have at the application layer down into the network layer and vice versa and make the to speak a similar language so that you\u0027re able to able to correlate what happens on one layer to the other layer so it would be really shit cool if you issue that TCP or their their RPC request and you have another try segment a child trace that tells you yeah we you went into the TCP stack then you went across router one two three you got that the the query on the other side responded to and then you went wire route or four and then you went back so that would have ultimate visibility into how these individual steps would work out the problem is well I have my trace ID in the HTTP requests eventually how do I see that at the network layer there might be multiple HTTP going over the same TCP connection so I can\u0027t really do a one-to-one mapping so let\u0027s say well I pass that on as an option if I open the socket so something needs to happen there right so we have to go and answer that question eventually if we want to go and provide for that level of visibility that everybody would appreciate the developer but as well as the operator the other question that we have to answer is can we get to a format where the backend system can ingest and digest that data in a relatively correlated and uniform way so if we\u0027re exporting Trice information from iom net flow information and well these tracing information and these guys are just about to embark to standardize that whole thing well wouldn\u0027t it be nice if we would be able to go and correlate all that and have a full view I believe so so how do we correlate these metrics to the backend format there is an open discussion it\u0027s still an open issue from an open tracing perspective it\u0027s open for two years now and well what I missed out on this discussion is what happens at the transport layer there was a really nice talk from Robin Marx on what "
  },
  {
    "startTime": "00:30:17",
    "text": "happens with quick so layer for IOM gives you a visibility up to layer 3 these guys look down to layer 5 well that\u0027s transport in between so if you enlarge the picture then you put the network into this whole trace data thing so we have trace data from the application side from these agents but we also have network trace data that would all be going back into the back-end system so that we can go and all correlate that vision is there everybody could say well we can export into CAF can correlate the various topics but yeah we need to go and get that done if you look at that scorecard that we had earlier on and then why I don\u0027t know whether you want to go to ask me again but I have one yeah who I have one so the observation is and I think that\u0027s very much driven from an app development perspective as a developer today I have to choose my framework and that means everything is driven by the tool chain open census open tracing w3c all these guys just came across because we have this landscape of of stovepipes that we want to go on well basically teardown if somebody says open tracing they don\u0027t necessarily say open tracing they just like say give me an open tracing system where are not kind of hooked up into one or the other silo forever so the observation is really that the how drives the watts and well again I think we\u0027ve done Network we\u0027ve done application there is one layer that well we may not talk about that much but but if Ben what talks to his SVP that\u0027s the only thing he cares about thanks Frank so oK we\u0027ve seen the network right the network we love right great we\u0027ve seen active probing we\u0027ve seen the application but the point is that as Frank mentioned we\u0027re missing the business view why do we do all this right because there is a business intense at the end and we need to create information at frag mention so basically the config information that may give us the service information along with the operational information and along with the business view right that might be coming from multiple sources the application active probing passive monitoring the operational information etc those have to work together for what a business intends that\u0027s why I believe that we have to add the service KPI the contacts information to telemetry so "
  },
  {
    "startTime": "00:33:18",
    "text": "that whenever we get it from wherever whatever way right and please don\u0027t let\u0027s not discuss prot above versus Jason right I we don\u0027t care but I need to get the contact information because if I\u0027m in my big data Lake and I\u0027ve got so much data then deformation the day that kills the data right I\u0027ve been there with net flow right years ago where we\u0027ve got so many flows it\u0027s not to be difficult and waiting to face the same thing take the twenty fifty thousand counters time in census in a router in a time series and make sense out of it so what you have to do is that we have to look at the service assurance parts right we need to tag in telemetry the context information directly so that whenever we export it we know what to look at for specific service and we\u0027ve got the three information of all services and we\u0027ve got the link with your pic information right and we\u0027ve got the full degree position on where the problem is this is tying telemetry to our intent right starting with service assurance right yes the dream and the vision is to do the closed loop automation we want together maybe you want to run so you want to walk before we run and get all the building blocks there now something reset will not going to spend time on this is the telemetry definition because I like the simple one we had the beginning a collection process of useful operational data now when I speak to some Mexicans as VP they don\u0027t care about my little time series of a counter right they want to say what is the business impact with your sorry which business impact can you have by sending telemetry right which used to information will you gave me for my business that\u0027s why we made it we make we made up that new keyword which is not their business telemetry just to say if you speak Tunis VP this is what he wants to hear and things such as okay in networks what type of devices have operators right is there dependencies can I do combined cells what about licensing if the licensing fine can I sell some more licensing can I enforce it what about the feature usage can I finally remove the 6:25 feature right what are the customers using this is the type of things that they mean by telemetry they\u0027re right this is reported value and that\u0027s something that we keep forgetting right writing a small spike in the ITF is great but if you don\u0027t see the big value it won\u0027t work the good news is that whenever they speak to me about their business telemetry and they call dr. Winfrey by the way right while we call its operational telemetry whatever the "
  },
  {
    "startTime": "00:36:19",
    "text": "good news is that we speak about the same information we just need to package it in a different way a kind of translation for business view right I might need the interface counters like the usage and I might need the license and the feature and inventory and all this so it\u0027s the same information just map differently now let\u0027s store all of these things together let\u0027s do this together so the conclusion for three slides you know we\u0027ve been discussing multiple aspects like at the network layer at the application layer at the business layer right and telemetry for me is a collection of many things so cover that one piece I\u0027ll cover the next yeah so I think we we need to figure out the what and I think we spend a fair amount of time off on the what and the what is described by a model our favorite whatever yang whatever if you want to go and express it somewhere else it wouldn\u0027t matter that much I think we need to really focus on the how because the how is from a developer perspective how I consume things if I want to go and consume the what the how needs to be figured out and those two things go hand in hand if you use on the how not on the what if I can\u0027t consume it no matter how well you describe it I\u0027m not gonna go and consume it and we go to the home and figure out who for right I think that\u0027s the three questions who do we need to go and talk to who do we need or unconvince who do we need Ali A\u0027s worth as an IETF in order to go and get these things this equation we kind of summed up so that I have something that I can go and deploy because the solution requires all of the three otherwise I think it\u0027s of a single silo we might not succeed I think that\u0027s at least for the telemetry business that\u0027s what we\u0027re seeing and I would say that you know we need like what specification and we\u0027re pretty good at writing RFC\u0027s here right check we could do that we\u0027re slow but we\u0027re good as Frank mentioned we want you to tuning the same time and the codes because otherwise we\u0027re going to fall into what has been happening that there is current tools but not for the idea of specs and guess what in this wall of automation open source people will go and try things and if it works well that works and if you say I\u0027ve got the perfect specification two years after well that\u0027s too late and the thing that we\u0027re missing some time is that we as a group we need to reach out to the industry and just thinking that okay we\u0027ve got our spec work is good enough now we need to go to all different SD O\u0027s and to the "
  },
  {
    "startTime": "00:39:21",
    "text": "consortium and etc and unless we\u0027ve got those four different conditions atom thing that we\u0027re going to be relevant or actually depending if your glasses are full of empty right but I believe this is what we have to do as an IT organization to get our solution adopted and if final remark Frank how do you want to go and do that so I think if I understand this this means specification is good but you really want to go and have a closer interlock again with running code right so that we go back a little bit into time where we had more running code and going that hand and hand with the documentation so that you have a bar where you say well if you have running code and the spec it mace might get more air time it\u0027s more relevant to to maybe discussions that we\u0027re having here and that needs to be balanced right so in certain place cases open source got it wrong and we\u0027re stuck with the wrong framework because well it\u0027s the only guy in town so there is not that that is not an either or that it\u0027s not a a discussion that well do this or do that but I think we have to go more carefully consider how we bring these three things together how we more frequently and better Li ace also at other quote-unquote standard bodies that might not be called an sto CN CF does these things they\u0027re not an S do but people care about it okay thank you excellent presentation a couple of points that probably missing Frank you mentioned that CN CF of course you mentioned open senses and open tracing and more all the back-end processing stackdriver wrong with years and more but what was missing in the bigger picture is if you plug in all these tools into a bigger architecture we know as a service measure like st you as an example so for application layer networking that framework is already established and CN CF is rallying behind that increasingly to have their tools graduate from that machinery like like kubernetes and st you and more so that is in the bigger vertical stack we need to correlate or create a link between layer seven application infrastructure or networking and layer two and three that Frank and I know you mentioned there is a disconnect absolutely that\u0027s a great point and then how to linkage these two of course your "
  },
  {
    "startTime": "00:42:22",
    "text": "colleague ed is trying to create something called network service mesh and then where that ITF can come in and perhaps take some set of requirements from the SDU and network service mesh and all other components that you mentioned create a list of those core requirements and identify the gap and based on the set of requirements develop probably some new API protocols or protocol extensions Thank You Maurice it\u0027s there the problem is there and it surfaces very much less you say in network service mesh where we are bringing together well the service mesh aspect which is an application layer concept basically add from kubernetes and the network world and the guys in network service mesh would love to have something like open says census or open tracing integrated so that you have visibility across the board all layers the problem that we have there is in the first bullet how do you own correlate what do you use as a correlation ID because your try segment ID is something that only lives up to layer 5 we have something else at the network layer but we have no way to DES to unlink that and that\u0027s an ongoing discussion so thanks for the question it\u0027s there people care about it we don\u0027t have a solution max power cable that very good presentation I really like it our members they you know operate large networks and something like this I think it\u0027s interesting not I don\u0027t have visibility on how they managed internally their own network so you know I\u0027m spitballing here but there could be an industry that you might have open years they listen to to what you\u0027re proposing and the advantage there is that we have our networks and to be a little more controlled you know we manage the network\u0027s and basically the devices that access our network so that might be some that very valuable in in our industry and you were talking about you know more industry connections I\u0027ll be happy to to help you with that and to reach out the right people yeah McDonough rich so you had there one in your table we were having multiple layers it was what and how and he said that the IDF has the what sort of like check but how was open it was the last slide in the first part of the Boehner\u0027s presentation yes well okay okay so what what I did this one and I want to measure the network and these days you "
  },
  {
    "startTime": "00:45:24",
    "text": "saw that there are many different ways of measuring the network and you can try it but it\u0027s really not there is not a unified way the OEM people are trying to do that and provide the some of those tools but you say there is what but there is not how I know that what is I want to measure the network what exactly I want to measure will depend on a case-by-case basis give me a common interface for the network measurement and then not the vendors although some of the vendors can also measure the network for their own performance you know for their own performance reasons to be able to improve the operations of the of the equipment but also the operator will be able to say I\u0027m measuring this and how I\u0027m using those measurements for my own business application is something that I will do we have many young device models in the ITF but we very we have very few service models because we don\u0027t have the right participation so when we are doing about the idea of specification we have to be careful which part we are working on and not trying to go into the area where we don\u0027t have the expertise so if we will try to do some specifications let us provide a common interface how to the measurement and not to say what to measure I would agree with that now the the point I was making about the feedback loop the closed loop this is biding the service KPI to your configuration and to your telemetry this is the way to do it what you\u0027re also asking is like the measurement let\u0027s say at the IP layer like the active probing that flag mention and Frank mentioned like yes we should have the same interfaces like you know a no like yesterday or a 141 or in situation yes I also want to get those api\u0027s because if I were to map my service up to my specific device pass I want to be able to test this actively and passively so one violent agreement of services overloaded in industry urban or a service is that top layer you can probably address that through your intent of your data information models but Yankees primary data model which is damaging to answer that and we spoke about a few times so it\u0027s answer will be about it routing is a service and it\u0027s being consumed by the high level services so when you talk about the service is anything then crossed that goes among multiple devices there are some services that can be specific for single device but anything that you\u0027re running between multiple devices you want to measure how that service is running across that being it like a routing or something others they\u0027re like elf like an l2 and l3 VPN which is using the routing in order to deliver that service so there are different qualifications of its "
  },
  {
    "startTime": "00:48:25",
    "text": "services and it\u0027s not overloaded you can be very specific when you\u0027re saying routing or a service or a VPN as a service or some others you just have to qualify it right so what they are means is service young models now I want to extend this definition because a service in this full picture could also be an application of Franky\u0027s mentioning all right thank you so much [Applause] "
  }
]