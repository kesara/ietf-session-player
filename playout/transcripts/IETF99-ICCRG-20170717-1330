[
  {
    "startTime": "00:01:05",
    "text": "[Music] [Music] Thanks [Music] good I think you should be starting all right let\u0027s get started [Laughter] okay wonderful welcome back from lunch please don\u0027t fall asleep standard rules apply if you fall asleep you have to buy me something later not if I don\u0027t catch you but pay it in to the front of the room all cell phones should be off what else I guess the standard note well stuff I\u0027m not gonna actually show you the no.12 I\u0027m assuming you read it somewhere if you haven\u0027t read it walk into one of the other sessions you\u0027ll find it there what I will ask is for a jabber somebody to look into jabber and come up and ask questions but javis cry I need a JavaScript we can move on without one you\u0027ll do it excellent and stupid gets a cookie I was actually gonna get you all those those things that kids uh playing with these days frigate spinners sha Shan Shan Shan Turner\u0027s idea oh no sorry rich solves this idea I also need a somebody to take down minutes I\u0027m going to start calling "
  },
  {
    "startTime": "00:04:08",
    "text": "out your names you will be embarrassed Marcelo you\u0027re shaking your head you know I take that as a yes I got a gun merits Marcelo excellent Thank You wonderful this works and with that I\u0027m going to try and plug myself in to show you the agenda the agenda is slightly different I\u0027m going to post a new agenda online it\u0027s slightly different from what we have actually you know what I\u0027m normally the agenda you get a bit of online I want to move quickly to the talks the agenda slightly moved I\u0027m gonna have to come up and do its presentation first but the rest of it is roughly the same I\u0027m also inserting role and bless for about five minutes right after the bbr talk those are the only two changes otherwise the agendas the same as what you\u0027ve seen and with that I am going to have Toki come up and start the first presentation okay well there\u0027s the agenda you can take a look a few wonders have your slides coming up sir annoyingly that happens oh one thing before before I start I am actually going to enforce time limits we can\u0027t get to the end of this meeting if we don\u0027t I mean with you already five minutes behind time I know that but we\u0027re gonna try and I\u0027m really gonna enforce time limits so take it away sir hello everyone am I in the box enough so my name is Tokyo Harlem Jorgenson I\u0027m going to speak little bit about some of the work we did on fixing buffer bloat and applying atrium techniques to the limit Wi-Fi stat so this was a paper that was presented last week at the Usenet general technical conference next fireplace and since there are strict time limits I\u0027m going to talk mostly about the buffaloed side of the issue we also looked into this issue of airtime fairness in Wi-Fi and you\u0027ll see a bit of that but I won\u0027t go into that in detail but first I always like to start these sessions by asking the room how many of you use Wi-Fi and how many of you are not listening so you may have noticed especially not so much here the ITF but if you have a home rooted at home sometimes the Wi-Fi is is kind of "
  },
  {
    "startTime": "00:07:08",
    "text": "flaky and one of the issues that poses is that we\u0027ve looked into here is buffaloed at the Wi-Fi link next slide please so I I assume most of you know what buffer bloat is in here but the thing we saw here was that we have these techniques hom algorithm for cheering algorithms and so on that we can apply to wired links that weren\u0027t really well for pretty much in most cases but for the Wi-Fi stat we still saw hundreds of milliseconds of extra buffering in the Stach even when applying state-of-the-art aqm to to the interface next slide please and so we set out to to try to fix this and this has gone into Linux versions four point nine through 4.11 and as you can see we we saw this nice order of magnitude reduction in in latency under load with with the solution next slide so some of the constraints and some of the reasons why previous solutions to buffer bloat do not work well for Wi-Fi bar these constraints in how Wi-Fi works and why has the concept of traffic IDs where packet going to different stations have to be Q together so that we can aggregate them at the tid level and we also need to handle reinjection of packets the transmission this means that we do not have just one managed queue at the interface level because we cannot just tell at you please give me ten package going to this destination so we need some kind of of queueing structure that allows us to split up package go into different destinations and then of course we need to treat the hardware APIs you know we need to run this on really slow really small embedded boxes and we also did not want to modify the clients to achieve any of this stuff but especially the first two points means we cannot use the existing buffer bloat solutions for Wi-Fi because they\u0027re simply not capable of Duras next slide so what we did instead was for the for the queuing part we açaí we designed a new queuing structure which is not at the interface level but which is sort of integrated with the mac layer in the linux kernel and it\u0027s based on fq coddle but in the the straightforward way of applying fq parliament would mean we would have a thousand views for each they associated to a wireless access point and that\u0027s prohibitive for very small devices so instead what we did was we we had a we designed a structure as you show it to you in a bit which has this global pool of cues that we then dynamically assign when we hash flows into them with an assign a queue to a station and that means we can reuse "
  },
  {
    "startTime": "00:10:09",
    "text": "the same pool of cues no matter how many stations we have and that means that we we can then support this perche idd queuing and scheduling that we need to accurate packets for Wi-Fi and then the the grayed out thing here is the airtime scheduler in the same work we also designed a scheduler that uses some of the same techniques from if you coddle ADR based scheduler to sort of make sure all station should get the same transmission time and I\u0027m not going to go into the details of how that works but you\u0027ll see it in in some of the year resort graphs next slide please so this is this is the Linux kernel peering structure and the left here is as before and here is after we modified it so the things to notice here is this curious layer this is where you could install your HMS beforehand but what we have down here at the at the driver level is another bunch of queues and these can be this is sort of the source of the hundred milliseconds of shearing latency that we saw before so what we did was we got rid of the cutest layer completely by bypassing it at the at the Linux API level and then in the in the Mac layer which is the the sort of the library that implements the Mac protocol for 480 to 11 we then assign the package to their T ID and then put them into these different views where we we have the the global pool of crew so I was talking about and then we we assign them to et ID and then we apply fq coddle between the queues currently assigned to this T ID and then there\u0027s an overflow queue in case of hash collisions but that sort of a minor detail and then the only thing left in the at line K driver is now we try pew which is where packets that fail to be transmitted get re-injected and that\u0027s basically just the priority queue where when you read when when you then transmit next time you will retry the packets before get you in other packets so this means we now have to manage to here this way closer to the hardware this is only to aggregates which depending on your rate is on the order of 10 or 20 milliseconds enough cases next slide then we did some evaluations of this thing where we sent data from a server to too fast clients on a slow client this was also to look at the DSM fairness issues and we\u0027ve we\u0027ve used the normal FIFO queue which you should all know by now this sucks and then we have a few kernel as sort of the best we can achieve with eight um latency and then we have fq mac as the restructures and then we have the airtime vanish curious as the last one and so looking at the latency first here we see that we go from of course FIFO "
  },
  {
    "startTime": "00:13:10",
    "text": "this this is a large scale obviously so we go up to almost a second of latency [Music] with with a fiber queue with fq caudal we reduce it somewhat for the for the fire stations we go down to about the 50 millisecond mark and then for the fq mac our nutrient structure we go all the way down to 10 on 20 milliseconds next slide please throughput also improves actually when when we apply this modifications and I\u0027ll go into the reason for this before of course just just applying a few coddle actually improve throughput but for the fast stations at the cost of the slow stations but the average is the sort of aggregate throughput for the whole network and the reason for this is on the next slide which is the the our time usage for each station so them so the percentage of time that each station Spence transmitting and the reason this improves from five ODEs who have to coddle to after Mac is that we actually increase the change base so beforehand you had a very reasonably small queue at the very low layers which would tend to get flooded with packet packets waiting to go to the slow station and that meant that the that the slow station would take up almost all of the airtime in these two cases it gets a little better here because you have better curing or you have round-robin curing at the upper layer but just applying the curing structure at the matte layer means that we can now have enough queue space to have always have packets for for all the stations available so this also this this is due to improved aggregation for the physicians and then as you can see when we apply the airtime scheduler as well we get pretty much perfect airtime fairness even though the the stations differ in throughput on in raped by an order of magnitude next slide we also evaluated the the impact on different applications of these changes we looked at HTTP load time and we looked at VoIP performance by calculating synthetic mass values for the different cases so the HTTP lost times as you can see like this is also a large Delta this is I think 35 seconds to to load a rather large webpage which will bring down to a few seconds in the best case so this sort of reflects the both the throughput and the latency so the small page here is dominated by the latency we can get down to almost zero in the best case and the large page also gains more from the from the changes in throughput obviously and next slide "
  },
  {
    "startTime": "00:16:10",
    "text": "so the VoIP test here I\u0027m not going to go into whether or not like this actually corresponds to actual voice quality but if we put that aside from them and assume that it does what you will see here is that FIFO is at 1.0 is completely unusable for best-effort traffic so this the best effort unvoiced traffic is the ADA 211 choose so in 802 11 you can you have average queue where you can get priority at the Mac layer and so the thing to notice here is that with these changes we now achieve better performance for best-effort traffic than we did before with this soft traffic that gets priority at the Mac layer so this means that we can now run our voice applications as best-effort traffic which is nice if we do not control the markings of the package so if they are removed somewhere in transit so this is pretty promising and I\u0027m like we have some plans to look into applications for this but just to show that for real application traffic we actually get quite a bit of an improvement as well yeah so as a summary we\u0027ve in this work we\u0027ve reduced Wi-Fi buffer bloat by an order of magnitude again and we get almost perfect at home fairness with the scheduler as well and this it\u0027s a lightweight deployable solution it\u0027s already in mainline Linux if you if you run the an open source firmware on your home router like open word elite you can get this now and run it and it really improves things and code data etc available bad but otherwise questions No how am i doing on time I have some more graphs I can show if you want thank you you\u0027re doing really well on time oh good and earlier up hi Randall Jessup Mozilla so it looks like this would have a significantly positive impact on Wi-Fi with WebRTC in particular because typically those aren\u0027t deserved and so now I would shoot the audio avoid law buffering but also video recovery time from packet losses or other things when you have a round trip to do it to do an AK or whatever should also be dramatically improved have you done any "
  },
  {
    "startTime": "00:19:10",
    "text": "testing with WebRTC yeah one other question I had was so there is one downside to this though not may not large which is the slow stations have you done any more detailed look as to what happens with different levels of slow stations and how bad the impact is on that it\u0027s in the graph you showed it didn\u0027t look all that bad in terms of throughput but that is something that you know that may impact some people in a slightly negative way mmm yes so for the for the first question yes we can now run web RTC over Wi-Fi while other people are using the network and downloading and I do this occasionally myself I don\u0027t have numbers specifically for for web apps you see in a test bed scenario but but from like I run this code at home and it works as for the other thing I think if you go I have some edge source lights so if you go the other way bit more bit more a bit more at this one so this was a repeat of the experiment with 30 stations instead of 3 stations and so this will sort of show you the worst case where the the dashed line here at the at the very end is now the slow station going from about 400 milliseconds to almost 2 seconds of latency in in the medium one and a half seconds so obviously we get a lot of improvement so if you go back one slide this is this shows you the the difference in throughput when there\u0027s 30 sessions so we get sort of a huge improvement in aggregate throughput but of course if you have a very slow station we are limiting it and we do this by basically starving it until it slows down and and this means this does work but if you are that slow station you are going to notice I guess and of course you get it you go into sort of a philosophical argument what notion of fairness do you really want to to apply to your network and so my reasoning is that since time spent transmitting on Wi-Fi is sort of the scarce resource this is what we should be enforcing fairness on where\u0027s the ADA to a lemon Mac by default enforces throughput fineness which is why there\u0027s anomaly sort of appears in the first place right that I I certainly understand that point of view from a technical point of view from a user level point of view that may not be this sort of fairness that they nest that a user necessarily wants to apply depending on the situation you know there are some intermediate levels between throughput and airtime fairness "
  },
  {
    "startTime": "00:22:12",
    "text": "where you allow slow stations to have some level of additional airtime in order to keep them usable okay well well not overly hurting the fast stations and that maybe wasn\u0027t yeah considering as an option so add some fairness actually translates to proportional fairness at the throughput level which is already a sort of trade-off between efficiency and and yes yeah go ahead I\u0027m done I\u0027m Thank You Randall can you ask your Western offlane and do I want to the next dog a little comment but sure I can do that go on we can make a quick comment before Neil come on Andrew McCrea just to say that the slow station problem is also rather mitigated if you have a multi ap deployment and that dot 11 R will tend to move the slow stations to places where they an slow much more vigorously in the presence of air time fairness and so actually you get better throughput as seen from the station if there is an alternate ap it can talk too thanks for that doc that\u0027s actually quite a good piece of work and we\u0027re going on to kneel and be be honor take it every meal thanks Jenna so my name is Neil Caldwell and I\u0027m gonna give a quick update on the VBR congestion control project and this is joint work with my colleagues at Google including you Chun and Stephen Sohail the quick PPR team which is in and our illustrious co-chair chana and Victor and then Ben Jacobsen as well next slide please so just a quick outline of what I wanted to cover we\u0027re gonna start with a quick review of bbr and its background if you want more details there are some links to previous IETF presentations the main meat of the presentation will be a quick overview of the new internet drafts we just posted about two weeks ago the first is a the delivery rate estimation algorithm that bbr uses and then the second is the PPR congestion control algorithm itself I\u0027m gonna speak briefly about active and upcoming work for bbr because we you know there are still things that we\u0027d like to improve about it and then I\u0027m going to give a quick deployment update which the the news here being that we\u0027ve switched a quick traffic from Google and YouTube over to using VBR just next slide please so just a quick background the the motivation for the bbr project is really the issue sort of fundamental issues we see with loss based congestion control meaning Reno and cubic largely and basically the "
  },
  {
    "startTime": "00:25:12",
    "text": "issue is that packet loss is not really a good proxy for congestion these algorithms sort of assume that packet loss is equivalent to congestion but of course that\u0027s that\u0027s not the case and really bites us in a couple of important use cases so for example often in shallow buffered situations or cases where there are link layers that are lossy you can get packet loss long before you can get congestion and you know in such cases loss based congestion control gives you a multiplicative back off every time there\u0027s a round-trip time with a loss and so you get pretty bad throughput there are some examples here so for example to get 10 gigabits over a 100 millisecond round-trip time you need less than 1 packet loss per 30 million packets which is tough to achieve operationally and if we look at more realistic loss rates that we see over the internet or over high-speed winds with commodities switches as shallow buffers then you see more like a 1 percent loss rate and there with 100 millisecond round-trip time you\u0027re going to get around 3 megabits with the loss based congestion control at the other end of the spectrum if you got really deep buffers then you know we have the the classic buffer blow problem because the loss based congestion control will keep ramping up until it fills those buffers next slide please so VBR so that stands for a bottleneck bandwidth and round-trip time around trip propagation time and the core idea here is that we try to build an explicit model of the network path that tracks both an estimate of the bandwidth that\u0027s available to the flow using a windowed max filter and then an estimate of the two-way round trip propagation time using a windowed min filter on the RTT and we update that model on every ack and then we use that model to control the sending behavior and largely what we do is is a sort of sequential probing process where first we probe for more bandwidth and then we try to cut in flight down to probe for the RTT and keep that keep the round-trip time low and then we also do that while being aware of the model itself in order to feed each aspect of the model the samples it needs on the timescales it needs and then putting it all together we try to seek high throughput with low delay and in practice what we see is that the algorithm can can give you sort of the maximal available bit throughput up to about a 15% packet rate just sort of a parameter of the algorithm not sort of a fundamental aspect of it and then it also - maintains a bounded queue that sort of independent of the buffer depth and if we get if we want to put it in context we can sort of quickly compare it to some more traditional congestion control algorithms that folks in this room are familiar with so you know the in terms of the congestion signal that the algorithm uses VBR as we just said uses "
  },
  {
    "startTime": "00:28:13",
    "text": "a model of the bandwidth and the round-trip propagation time whereas cubic and we know use packet loss vegas uses RTT and packet loss DC TCP uses ecn and packet loss and then if corresponding we look at the primary control or these algorithms use BB are actually it tries to use pacing rate as its primary control mechanism it does also have a see end back stop but it tries to do most of its action with pacing whereas the other previous congestion controls are built around a seal in this sort of limits the volume of data in flight next slide please alright so I want to dive into the first internet draft that we posted a couple weeks ago this is the delivery rate estimation internet draft and there\u0027s a link there or you can google it if the slides are not up yet so basically the the idea here is that on every ACK the this algorithm provides a sample that has two aspects the first is an estimated rate at which the network delivered this most recent flight of data packets then the second aspect is it tells you whether or not that particular rate sample was application limited by the sender that is the sending app ran out of data to send at some point during that flight and why did we separate this out into a separate draft well I mean we could have integrated it but our thoughts were that first this helps to break up the algorithm and just smaller easier to digest pieces you know we can think of bbr is having a sampling part and a modeling part and control part and this just helps it easy easier to digest also you can implement the bandwidth sampling separately and in fact in the Linux TCP code it\u0027s definitely a separate algorithm and then finally it\u0027s also useful to think about bandwidth estimation outside of the context of congestion control or outside of the context of VBR the other congestion control algorithms where I want this or adaptive bitrate streaming I want this for example to pick which bitrate to show the user next slide please so the the basic design principles that we were working from for this bandwidth estimator were first we wanted it to be purely passive in the sense that we wanted it to work with the acknowledgments that were already going to receive for the the data that\u0027s in flight for that transport connection second we wanted it to be generic it\u0027s in portable two different congestion control algorithms different transport protocols and so far we have a Linux TCP implementation a quick camp implementation and then there\u0027s a FreeBSD a TCP implementation underway at Netflix as well and we wanted to also as I said track which samples were application limited we wanted it to be relatively efficient so constant time for each act that comes in we wanted to "
  },
  {
    "startTime": "00:31:15",
    "text": "try to err on the side of being conservative and underestimate we wanted to make sure that we got feedback whether we\u0027re in recovery and getting sax or we\u0027re not and we\u0027re getting cumulative acts are to make sure we can try to get an estimate at all times and then we wanted to try to get an estimate that is over a time scale that is at least runaround trip rather than one packet to try to filter out some noise and if we think about alternatives the main alternatives out there for bandwidth estimation we considered were we\u0027re sort of packet dispersion metrics looking at the interacts facing and there are various approaches in that space packet pair packet trains and chirping but some of the challenges we saw with those kinds of approaches are that you know in the real world with cable modems and Wi-Fi and cellular links there are a lot of things a lot of funky things happen with ax compression and aggregation decimation strat tracks jitter and noise in the in the queues and so we\u0027ve found that we sort of took this particular approach to try to tackle some of those issues next so if we take down to the very essence of the delivery rate estimator I think this is a good picture to keep in mind the The Cove here that we\u0027re drawing has on the y-axis the amount of data that it\u0027s been cumulatively cumulative Leaney delivered over or acknowledged as delivered over the lifetime the connection and then on the x-axis we\u0027ve got time that\u0027s elapsed and what we\u0027re really trying to show here is the essentially the slope of the delivery curve and a key part here is what is the time interval over which we are calculating that slope and the key issue here is that we we calculate the the time interval we calculate this slope over time interval that starts from the most recent act that we have received before we sent a packet until the ACK for that packet do we have a laser pointer here or we can try anyway can we go back to the previous slide so the the data I didn\u0027t do the data packet in question here that\u0027s being sent is this one here and that\u0027s act here and then we go back to the act that was sent before Paquette and we use that as the start of the this great sample and then we just basically calculate an accurate that is the amount of data that was delivered between those two acts divided by the time elapsed between those two acts to give us that slope and that\u0027s the act right from this algorithm next slide so you might want you might ask well why can\u0027t we just use the rtt and what happens is if you try to calculate an accurate that\u0027s just the amount of data "
  },
  {
    "startTime": "00:34:16",
    "text": "delivered divided by the RTT you can see if we look at the same picture with that alternative attempt at calculating in a crate that actually gives you an accurate that sort of badly overestimates the the actual delivery rate because it doesn\u0027t incorporate the amount of time that was really needed by by the network to deliver all those bytes that you are accounting in your sample there all right next slide so one big issue that you run into when trying to calculate delivery rates in the real world is what you might call act compression there are similar effects going by other names that are have similar issues aggregation decimation stretch acts basically by all of these with all of these kinds of effects we have acts that are delayed and then they arrive in a burst or there\u0027s a single act that covers a lot of data that was delivered and this can be caused by the receiver can be caused by the middle box but the big issue is that these are quite frequent in the real world they\u0027re really common in Wi-Fi cellular cable modem links and the issue is that you can if you\u0027re not careful you can run into excessive I create samples so I\u0027ll give an example here on the next slide so here\u0027s a real-world trace where the actual bandwidth was something like 8.9 megabits but the a crate sample shown in red here is 27 megabits and that\u0027s because of the act compression that you can see there\u0027s a horizontal green section in the cumulative the ACK stream that\u0027s a big silence and it looks like then we get a burst of compressed acts all arriving at once and that results in over estimate in the a crate so next slide so the way that the algorithm currently deals with this is to sort of simply filter out the accurate samples that are impossibly high using the following observation so basically the accurate can\u0027t really physically exceed the send rate on a sustained basis for obvious reasons and so what you can do is for each flight of data that\u0027s delivered between sun sand and some ACK you can calculate the send rate for that flight and then you can calculate accurate for that flight and then to help filter out these implausible samples you can just use as the delivery rate simple the minimum of the send rate and the a crate and this tends to do a good and good enough job in most cases it can be improved it\u0027s not perfect it can be improved to filter out more thoroughly some of these implausible a crates and it\u0027s an active area of work for the team next slide John if you get a sec thank you so what this looks like on the picture is if we take that same example here we would look at the a crate which were I previously showed you and then the the son rate which is shown here in red and in this case we would use the min of the two which is is the son rate "
  },
  {
    "startTime": "00:37:18",
    "text": "and that gives us a safe in this case that\u0027s sort of an under estimate but at least we haven\u0027t overestimated and probably the congestion control algorithm certainly if it\u0027s bbr we\u0027ll be able to filter this out appropriately all right so I\u0027m going to quickly zoom through some pictures and go into the detailed notation here this is mostly if you\u0027re reading the internet draft at some point and you want to know what picture can explain a particular equation that you\u0027re looking at you can go back to these slides so this is just a quick way to show what the sundry it looks like it\u0027s basically the slope of this Green Line it\u0027s the amount of data that\u0027s acknowledged as delivered divided by the send elapsed time the amount of time it took you to sent that send that data next slide correspondingly the accurate is just the amount of data acknowledged it\u0027s delivered divided by the time it took to a call those packets next slide the delivery rate is just the minimum of those two rates next slide so the other thing I said that this estimator provides is a notion of whether a rate sample was application limited or not and it\u0027s a it does this with the algorithm a you could sort read in detail in the in the draft which all zoom by here but basically it let me go up to the next slide and this chart shows you a picture basically every time the application runs out of data to send in marks the application as a limited and then when all of those acclimated samples have are out of the pipeline then we can exit that preparation where the samples are marked application limited and then we can get back into this blue region where we\u0027ve got non application limited samples basically in the interest of time I\u0027ll skip over the details but the idea is that when your application limited you go to sort of idle or silent bubble in the pipeline and you just need to track when that pipe when that bubble has been acknowledged and it\u0027s no longer in the pipe no longer pulling down on your your rate samples next slide so let\u0027s go move on the bbr so the big picture here for bbr is that it takes as input these bandwidth or rate samples I was just talking about along with good old-fashioned RTT samples that your transporter protocol is probably already taking for retransmission timeouts pick supposes inputs and it uses those to build an explicit model of the bandwidth available to the flow and the the round-trip propagation time and then those two estimates get fed into a sort of probing state machine that increases and decreases in flight to try to keep a reasonable number of packets in the pipe and also also feed samples back into that model and the output of that state machine and all that together is a "
  },
  {
    "startTime": "00:40:19",
    "text": "pacing rate and a pacing quantum a chunk size that you want to use for your pacing and a congestion window our maximum amount of data you want to have in flight then that goes into the pacing engine which chops up the data stream into those quantum into those quanta and then paces them out at the given rate and makes sure that the volume of data never exceeds that congestion window next slide and so here\u0027s a quick outline of what we cover in the internet draft it\u0027s basically a lot of the same stuff I just showed in the picture so I\u0027ll try to breeze right through it we cover the network path model both bandwidth and round-trip propagation time we cover the target operating point which is that we want to try to maintain both rate balance to try to match the available bottleneck bandwidth using our pacing rate and then we also try to achieve a full pipe to try to keep the amount of in-flight data roughly equal to the estimated bandwidth delay product for our flow and then we use the control parameters at our disposal the pacing rate the send quantum and the C went to dial the knobs using the state machine to try to stay near that target operating point next slide so here\u0027s a quick overview of the state machine there are sort of four main states there and the state machine sort of implements this two-phase sequential probing and in the first phase we raise the amount of in-flight data in the network to probe to see if there\u0027s more bandwidth available and to get high throughput and then once we\u0027ve done that for a little while then we go into the second phase where we cut the in-flight back down both the probe for the round-trip propagation delay and to you know reduce the amount of data in the queues and get lower delays and we do that on two different timescales so there\u0027s the warmup period when we\u0027re we\u0027ve got a fresh new connection and we\u0027ve got this issue that bandwidth sort of spans you know 10 or 11 orders of magnitude from bits per second up to hundreds of gigabits so we want a rapidly probe of the network we do that in the startup state where we exponentially ramp up our sending rate doubling it each round-trip time as long as the delivery rate is also doubling and then we look for a full pipe using a sort of looking for a plateau in the delivery rate and then when we asked omit that we fill the pipe we enter the state called drain where we cut the pacing rate to below the estimated bandwidth so that the in-fight amount of the in-flight data is sort of gradually or other quickly really trains out of the network until we\u0027ve estimated that we\u0027ve pulled our in-flight data down to the bandwidth filet product and at that point we sort of go into steady-state where we do the same kind of probing but on a more gentle amplitude so the probe bandwidth state cycles the pacing rate up and down to do that probing for "
  },
  {
    "startTime": "00:43:19",
    "text": "bandwidth and then draining the queue and then if needed then we can also do sort of coordinated cut in in-flight to probe for the round-trip propagation delay and then the details for all of those mechanisms are in the in the draft next slide please so we\u0027re not done with VB are we we\u0027ve got a lot several things that we\u0027d like to improve several known issues or known scenarios where we definitely want to get the behavior to be we want to improve the behavior so the the biggest focus right now or one of the big focuses right now is on soon areas where there are high degrees of AK aggregation and we want to improve both the bandwidth estimate and you know as we mentioned earlier you can get bandwidth those were estimates in these cases and we\u0027re working on new techniques for filtering out these variations and getting a more reliable bandwidth estimate in these cases and then the second we want to make sure that we\u0027re provisioning enough data in flight in these cases is when you get these ACK a grinning cases you\u0027ve got often you got a very long silence sometimes tens of milliseconds even if you\u0027re a minimum round-trip time is one millisecond you might wait 10 20 40 milliseconds for an ACK and so you have to make sort of just estimation of how much data you want to have in flight given that behavior and then another area of work is BB behavior and shallow buffers there are some known issues where VBR can keep considerably more data in flight that you\u0027d like if there are shallow buffers it\u0027s still bounded to an estimated PPP but if there\u0027s if you don\u0027t have a BD P of Q to hold that then the packet rate packet loss rate can be higher than we\u0027d like to see so that\u0027s a known issue or there\u0027s active work and discussion and testing on the PBR dev list and then finally there\u0027s also work underway if to look at B be ours behavior in data center environments where there are large numbers of flows next slide so in conclusion there are two new bbr drops out we\u0027re happy to get people\u0027s feedback suggestions questions whatever we\u0027d love to hope to see he back the PBR is now deployed for quick as I mentioned on google.com in YouTube that\u0027s the latest deployment update and the character of the results is similar to those results we saw for TCP which we mentioned the last IETF so with that now we have basically all Google and YouTube servers talking to the outside world and then the Google Data Center when traffic and our backbones between our data centers "
  },
  {
    "startTime": "00:46:19",
    "text": "that way in traffic is also using PBR and then we see better performance than Kubik for web traffic video traffic and RPC traffic the code is available as open source with links in the slides and then work is underway as I said for FreeBSD as well you can talk to the Netflix folks and then we\u0027re actively working on improving the VBR algorithm as I mentioned and we\u0027re always happy to hear test results or look at packet races and next slide and that\u0027s it if we basically have a landing page here on the mailing list so if you search for PBR - dev you\u0027ll get the mailing list which has intro message that has links to the internet drafts the paper the code previous talks that kind of thing and we\u0027d like to thank the folks below who helped us get to where we are today and any questions Thank You Neil we have five minutes for questions Randal Europe to two quick questions the small the small buffer issue that still you\u0027re so looking at does that also include looking at things like aqm various variations on aqm and how well that work and what the second thing is have you done any comparisons with the some of the proposed algorithms for armed cat or how well it coexists with the armed cat algorithm or planning to do that yeah so the the work looking at shallow buffers would also in that effort also includes a QM I guess so far we\u0027ve done tests with with PI and Caudill if other people have other algorithms out there they\u0027d like to see testing with let us know or you obviously you can do the tests as well so yeah we are definitely looking at a QM as well as Java buffers since they have so much in common obviously and then we have not yet done tests with coexistence with RM cat that\u0027s definitely something that we like to do we had time although if anyone out there has cycles do you do that that\u0027d be great yeah like I did this is what is really hot like I\u0027m not a technical question but so I\u0027d share quick the working group and we are chartered to ship ITF quick with an ITF standard congestion control are the standard is I think quote-unquote but it should be in an RFC but time and time again quick DVR has been "
  },
  {
    "startTime": "00:49:19",
    "text": "as something you know gosh wouldn\u0027t it be nice if we could ship quick with PBR and and we can\u0027t because it\u0027s not currently sort of here but I\u0027m wondering it what\u0027s your feeling on timelines right do you think this might align it\u0027s maybe also a question for Jana because it\u0027s if it seems silly if we ship quick with RINO and then like do a rare for six months later and do BB all right well I\u0027ll defer to Genesis he\u0027s got a foot in both worlds obviously so July anger this is a deeper conversation I think not not because so I think Reno for for quick makes sense at the moment because the simplest thing to talk yes and it gives you completeness but I do think that there\u0027s something to be said about actually and I\u0027ve talked about this in the past about not actually having standards track condition controllers at all and having transports rely on sort of condition controllers that are documented elsewhere so I don\u0027t want to start it right but I think I remember that that PBR was going to come in in a trough or maybe it\u0027s even here and and I guess it would either go ICC RG or TCP M right if so it is it is now published okay so yeah it\u0027s published as I see CRG drafts as of like okay a few years ago so yes so I guess now it\u0027s then it\u0027s not here but I\u0027ve taken more than of those five minutes and I wanted something okay I\u0027m closing the mic at this point hey Neil thanks for presenting this to us yeah in in the references that you had you said there\u0027s a research paper do you mean they seemed cute paper yeah your reviewed paper yeah research is probably the wrong word yeah but that I was referring to the communications of the ACM paper area okay and we used that one for our student reading group at Akamai and I kind of quite enjoy talking about it okay but I was struck by I didn\u0027t see a lot of evaluation or commentary from your you as authors on prior work um do you know for instance the one that I brought up the students was the 2007 work by Cola and in Mary Vernon called TCP Madison that as a model based one yeah I thought it was a nice comparison in contrast to yours and I\u0027d like to know if he what do you think is wrong with that Oh God that caused you to do something different Oh and so I wasn\u0027t actually aware of that work you could email me a pointer Irina thank you I\u0027ll certainly do that one and it was also a UDP based implementation application the one thing I can offer that you did definitely improve upon is there\u0027s require to change in the receiver they they they could force an ACK yeah and that was the other aspect about where where are you guys in deciding that you\u0027re just doing something that\u0027s based on empirical things about what the real Internet today is doing versus what would you ideally like to change in the receiver that could make this maybe better right yeah no that\u0027s a good "
  },
  {
    "startTime": "00:52:19",
    "text": "that\u0027s a good question yeah so we wanted to start with something that could work with the deployed receivers that are out there now because we we did want something that would work for both TCP and quick but that\u0027s very good point that there\u0027s probably a lot of leverage to be gained if you\u0027re if you have a receiver population that you can iterate with quickly so there are plans to look at what we can gain by a receiver side bandwidth estimation for the quick transport implementation so it\u0027ll be interesting to see what sort of gains we can get in that case yeah um shaky Versova Dean I was curious about what Sabine del develop meant on when you don\u0027t have enough packet in flies to make up a reliable estimation for example if you have a sort of Bart is traffic like videos like - where you transmit a bars and then you wait and then get another and maybe between one the other you don\u0027t have enough data to estimate the round-trip or the throughput what\u0027s the state of the organ at this point what is traffic department what is the treatment of the the algorithm has for application limited traffic so you\u0027re saying that the doc part very bursty so that there is not enough data to make up an estimation like in Durham trip where you throw but I speed right so the the algorithm as it\u0027s currently structured does not do anything special in those cases what it would end up doing is it would start out with the initial congestion window picked by the transport implementation usually you know initial push condition when you have ten packets and then it would from there it would calculate an initial pacing rate using the regular using the hi what we call the high gain that can double the rate every round-trip time which is two point eight nine so it would calculate two point eight nine times the initial window per round-trip time so two point eight nine times the initial window divided by the round-trip time will give you the initial pacing rate and then every time you get a bandwidth sample that will it will look at that and decide if that\u0027s the maximum bandwidth it seen recently leaves that update its bandwidth and then it uses the sort of general approach of using that high gain times its bandwidth estimate to calculate the pacing rate so it\u0027ll be two point eight nine times or whatever the biggest bandwidth sample it\u0027s seen and while it\u0027s in startup it\u0027s not going to reduce its pacing rate until it is pretty sure that it has filled the pipe using its regular pipe filling estimator so those low application limited rate samples shouldn\u0027t cause it to decrease its "
  },
  {
    "startTime": "00:55:21",
    "text": "sending rate so it should should be on the whole for application limited traffic like that it should be on the whole pretty similar to what you would get with Reno or cubic in terms of the overall bit rate I would think all right with that please catch me at the home base or ask any of us questions about that I have an insertion into the agenda Roland has been doing some well thanking you Ron has been doing some experimentation with PBR as well so just a few minutes on this excellent agree yeah thanks for inserting me into the agenda talk so my name is Ron Bess and I\u0027m presenting some measurements that we did just only a brief heads-up this is joint work with Mario Martina so we did some experiential evaluation in the small testbed using our speeds of one gigabit and ten gigabit per second based on the Linux our 4.9 version of EBR and the testbed looks like this where we have sender with basically if we 10 gigabit interfaces software based switch in the middle and the receiver which is connected by one gigabit even as interface the round-trip time was 20 milliseconds and we use the small buffer which is corresponding to 0.8 VD P Mo\u0027s so the this is only brief heads up heads up on presentation so the full results will be in a research paper at the ICMP 17 and first we tried to show that VB is working correctly in this setup so single flow works as expected so we we get the the whole throughput here of the bun elect link and also you can see the the RTT is kept somewhere around the base RTT so we have the the probing phases here the gain cycling and then here for examine the probe RCT so um yeah that\u0027s working well but situation changes if we use multiple flows here in this case we used six PBR flows to per interface and as you can see this year is Sanders transmission rate actually in the purple line here is the total amount or the aggregate and that\u0027s way above one gigabit per second so VR is more or less in this case a little bit too aggressive and since is just neglects packet loss as congestion signal it doesn\u0027t back off here so in comparison to that next slide please cubic works as expected here so it doesn\u0027t send faster than useful and here in this case on you can also see it by the numbers "
  },
  {
    "startTime": "00:58:23",
    "text": "like cubic retransmissions for small buffers also here for 10 gig per second for example in comparison to bbr that\u0027s some orders of magnitude so um you know from our perspective maybe I neglecting packet loss as congestion is maybe not not the way to go we have more results in the research paper and we\u0027ll we\u0027ll post a link once we have written written up final version including the reviewers comments Thank You Roland we can dig a quick question one quick question daddy got it so this is a long pragma from paper I\u0027ve also been experimenting with BB r and I\u0027ve seen that in some percent of the cases with even two or three flows it gets into this type well one flow will get very high throughput and the other flow we get like 110 to 130 put another case if we\u0027re one from ABR we think on you know there\u0027s losses but I would say it\u0027s not congested and the other of flows will you know completely back half from that so I also have some concerns I let me you respond Thanks yeah I want to thank you all for this work and I just want also want to note that these are the issues that have been mentioned are well known and have been discussed on the VBR dev mailing list I would encourage people to subscribe to the mailing list and and check out the discussions there people the you there definitely this this issue that you raise here is the the one I was referring to earlier in my presentation about PBRs behavior and shallow buffers and this is definitely an active area of research focus for our group but thank you thanks for the for the grass thank you so much and we are now gone can you come in for a second I don\u0027t know which of these two presentations you\u0027re doing all right and cool has Conan\u0027s gonna be Conan Bob are both gonna talk about peace be proud because we are in Prague we have to do this you have to make this a tradition so every two years I think they\u0027re gonna have to talk about this be proud but take it away Khan it might be a disappointment because it is related to TCP Prague but it\u0027s not about it it\u0027s about the foundations where TCP prac wants to be built on and well anyway let\u0027s let\u0027s start in it so to give an overview of "
  },
  {
    "startTime": "01:01:26",
    "text": "what our current evolutions and sometimes revolutions so we see three streams of people working on congestion control that\u0027s the people who have the sender\u0027s under control they think sender only and they try to optimize and get as much as possible done when only changing the sender throughout the network vendors who want to differentiate in their products and offer services to - for congestion control to work better and then there is a third type where everybody has to collaborate together and that\u0027s the hard part usually where all the parties are impacted and that\u0027s the Alpha rests work and the TCP Prague for instance so in the sender only there is a clear migration towards bbr which is mostly delay based I could say and there is also the concern about ignoring drop but of course there is a network today which is usually not supporting a lot there is no easy end there is hardly any aqm so it\u0027s the obvious thing to do on the other hand there our actions going on in the aqm working group the second layer where we have pi by square f q Caudill okay next slide so also quite obvious we all want to improve throughput and avoid buffered blood and in the third case we also want to have really low latency and a new class and to be compatible with Torino that was the original strategy so next slide if we look more detailed and we also did some experiments this is how bbr works on tail drop we did a test compared on hundred megabit link on 20 milliseconds round-trip time so an bbr it\u0027s actually it\u0027s quite good work and it works very well it can achieve high throughput and full link utilization so this is so we have here four flows long running flows and they evenly use the link capacity and here we have about hundred requests per second in this case it\u0027s even 250 requests per second and it\u0027s a an exponential distribution above arrival time and a Pareto distribution of size and this is the completion time so and the minimum based on the round-trip time is quite low and and also the maximum is limited so that\u0027s that\u0027s okay and the queue size is also around 20 milliseconds minimum because bbr is more like trying to measure what is the "
  },
  {
    "startTime": "01:04:28",
    "text": "minimum packets in flight and what\u0027s and forces a little bit its amount of packets and everything which is above that is stacked on top of that so also the short flows that you see there they\u0027re actually modulated on top of this minimum queue size at bbr sets okay next slide so if we compare that with cubic cubic together with an AKM that\u0027s done the second line of evolutions you could say there we also have quite at the target here is 15 milliseconds on a PI a cone and there is with a reasonable drop probability of 0.01% drop we can keep the queue at 15 milliseconds and the dynamic flows they have also a good completion time there are some limited loss and you see that the the dynamic flows are actually moderated around the 15 Mille second target so that you could say it\u0027s a bit better or worse it depends on your perspective of course next slide this is where we have data center TCP on a step a queuing so it\u0027s also how do you could work in in alpha rest mode there you see that the completion time is really low latency the queue is with stepped result of one millisecond very low the the other thing is that the variations in for dynamic traffic or modulated below the queue size so and it\u0027s hitting a little bit on the on the utilization so that\u0027s another compromise but it\u0027s guarantees you really low latency no drop at all so that\u0027s the other part so all of them have their advantages and their possibilities next right but there are some concerns and so initially there was with reno 2 cubic some compatibility with with drop but today indeed like like was mentioned there is no compatibility with drop and there is of course then a kind of detachment between what\u0027s going on in this layer where we have a clamps and in this layer where we also want to be compatible with reno so in the next slide here is an example what happens if you put bbr on top of an ATM so bbr sets the target to 20 milliseconds and the aqm sets the target to 15 milliseconds so it\u0027s the same as before and of course bbr sets its target and the pi will increase the probability until finally usually VBR detects the "
  },
  {
    "startTime": "01:07:31",
    "text": "bottleneck as a pacer at the moment and there are very high drop probabilities and even so you see here the drop probability it is around 5 to 10% for several tens of seconds and then only 1% for the other tens of seconds so this is a problem that\u0027s right so if we don\u0027t do anything it will force actually operators to disable a queuing at the moment and we did also a tzn because it needs an AKM so it\u0027s a bit of a problem and a concern we think and it should have a solution and in the next slides a possible solution could be because it\u0027s actually not a problem well actually both we want to solve the same problem bbl wants to have as one of these main objectives high throughput under higher and trip times and high loss conditions but also offer s we want to be compatible with the classic TCP but we don\u0027t want to be compatible with the pathological cases so we also want to have high throughput under these circumstances so if both the systems could work on a corrected classic troop would behavior which works with high throughput under high loss and high round trip times which is again a possibility for compatibility between alpha resin and the the center only evolution bbr in the next slides so as a possible solution we were thinking on already four l4s but it\u0027s equally applied applicable for four PBR to have round trip time independence above a certain round trip time so that a flow with hundred milliseconds could be as aggressive as a flow of five milliseconds today bbr is already having an opposition or an opposite dependency on the round trip time so it\u0027s more aggressive the bigger the round trip time so to be equally aggressive is at least a good compromise on the other hand we also want to be less responsive for hydrops so at a certain probability bottleneck or below a certain probability of the network why not be scalable at that point so for instance if we make make classic congestion control responsive in a scalable way below 1% we could always get around 24 drops per second a very frequent signal "
  },
  {
    "startTime": "01:10:32",
    "text": "also with the round trip time dependence if you have 1% drop you could also get 30 megabits per second which is a Hydra probability 1% and you get reasonable high throughput as well so if next slide here for instance if we have round trip time independence above 5 milliseconds in this case you will see this is throughput so this is usually a rate and there are two types of rates there is the rate of the traffic itself and is the rate of the marks so how many marks per second you get so there\u0027s these set of lines operate these sets of lines are the the the drops per second you could say and as you can see what we proposed is to limit it so to always have 24 drops second if 800 milliseconds and 1% drop probability in general the Reno and the cubic so we\u0027re having here Reno in cubic lines the bottom lines here they have around one drop per minute which is not that much and it\u0027s even going worse you will see in the next slide if we can make them all round-trip time independent they will always get 24 drops per second similar with that amount of drop probability there will always be at least 30 megabits per second of throughput that could be achieved of course we can discuss about what should be the real thresholds or real values of these settings but okay in the next slide the resource shown previously you saw here the round-trip time here is the probability so here\u0027s this hydro probability and although rate and here is a low drop probability with where we expect to have high rates so if we see that Reno this is the bottom line here the throughput is limited so it\u0027s this amount of marks quite low with Reno we could achieve only 8 on that megabits per second on a 5 milliseconds also you know a hundred millisecond round-trip time that correct yeah here we could go up to 200 kilobits per second so on the 1% case here you again have the 24 drops per seconds and a 30 megabit so if you become scalable here you can have much higher throughputs where okay the throughput can be very high but this amount of drop probability means that you only get two drops per hour or after "
  },
  {
    "startTime": "01:13:35",
    "text": "or one drop every two hours so this is cubic you also you see there is not much difference between cubic and and Reno but if we make it scalable there will be always 24 drops per second okay so we we have a paper it\u0027s not a published paper it\u0027s it\u0027s it\u0027s on a website describing all the different scalability requirements and I\u0027ve talked about those two they were initially meant to be in the context of Alvarez but of course they\u0027re also applicable in in the classic DCPS so if you want you can read more in this paper we can also have discussions about this if you are interested in more of these experiments there are a few big videos that I\u0027ve pushed to youtube where you can see also experiments BB are running with connect um and BB are running next to cubic as well okay thanks thank you can share the questions after Bob\u0027s or there are no questions later to this otherwise yeah feel free to comment ask questions I\u0027ll transition to ok yeah so we inserted back into the discussion because because the the the tensions between these requirements still applied for all congestion controls that which is why we thought well we were trying to do a lot of work to work out how to make BB our meet these requirements particularly as couldn\u0027t said in this case where you\u0027ve got someone trying to help with putting in a queue mm and someone trying to use BB are where we have this totaling behavior so I\u0027m now going to talk about tension how these tensions might be resolved in the space where you\u0027ve got a bit more freedom because we\u0027ve created this other queue for l4s and so we can put up a better end system congestion control and a better aqm in so they work together rather than we have got any legacy to deal with in that queue except to do with its coupling with the old classic you and that\u0027s why we started to try and sort out bebe arse problems because that we were coupling to that and because it was ignoring the losses it was starving us in the other queue even though we\u0027d tried to get away from it he said when we had a lake um and it was also starving all the other computers and "
  },
  {
    "startTime": "01:16:36",
    "text": "things like that so this is now the context of my talk as opposed to Koons is more about okay so what are we what tensions have we got in this cleaner space I call it a clean slate an incrementally deployable clean slate right because it gives you both and what I want to try and do is explain some of the problems that we\u0027re trying to solve in the small amount of time we\u0027ve got and also because we sort of shifted our focus a bit to bbr over the last few months with with not actually got any code we\u0027ve got a lot of code that tries to solve all these problems and we feel we\u0027re still on the right direction but I wanted to try and just explain the problems and because they\u0027re they\u0027re actually problems that haven\u0027t really been articulated here before so the first one if you look at the utilization of three GE vs. four G versus five G though there\u0027s a nice study a reference one that finds that for G utilization has gone down and the variance of it has gone up and we also did some simulations on a millimeter wave wide area 5g radio network where we found that what you can get a really high capacity and what we measured that capacity by just flooding it with UDP with no congestion control on it to see how much came out the other side compared to how much TCP was giving you as your good put and so the message here is that the more radio capacity you get the less your utilization is by quite a large margin where it\u0027s going down to you know 22% here and you can see the bit the the whisker there represents the one standard deviation of variance in that capacity and that\u0027s why there\u0027s a problem and I\u0027ll show this when we looked into it what\u0027s going on so the whisker in the amount of capacity is is all the variation in the capacity and TCP just can\u0027t can\u0027t move fast enough to take it take advantage of that variation and one other point to know the last bulletin here the utilization was actually a lot worse but we manually fixed another problem so that we could just look at the congestion control problems so that we could just look at the congestion control problem so that utilization for 4G was actually only 43% but that was because of a combination of the problem here that of the variation of the capacity versus the congestion control and a problem with the receive window attuning where the auto-tuning couldn\u0027t cope with the dynamics either "
  },
  {
    "startTime": "01:19:36",
    "text": "so you\u0027re actually getting only 43 percent utilization of a 4G network and that that was measured using a real network you looking at all the traffic in that network Oh over a period of time so together we calculated that if for instance we could increase that utilization to say 85% not not a not a really ambitious target I I did a back-of-the-envelope calculation 1.1 trillion euros was spent on spectrum licenses in the European Union and if you increase the utilization of that from four 3% to 85% you\u0027re actually released 460 billion euros of investment by in Europe than that that\u0027s just Europe because I only had the figures for Europe so sort of utilization is worth fixing when bhangra at the cost that much right next so this is why half the reason why there\u0027s such poor utilization as I said the other half is just received window tuning which you think we could sort out fairly quickly and so this is the radio capacity and the orange is cubic TCP underneath it and it\u0027s essentially bouncing along under the minima because it\u0027s taking far too long to respond you\u0027ll see the this is in the 5g case this is a screenshot of this simulator which is simulating pedestrian mobility using three aerials each of different frequencies so there\u0027s sort of is a there\u0027s a low bandwidth flood of wide area flood and then there\u0027s higher bandwidth at higher frequencies that are much more directional so you only get them sometimes and so that\u0027s what\u0027s happened that\u0027s what\u0027s giving you these Peaks but essentially it\u0027s really only using the low bandwidth flood and this is why in this case for the simulation the utilization was 19% because you can just see the cubic curve just starting to curl up there you see there\u0027s each each of these grid squares is ten seconds and so that\u0027s over about 20 seconds it just starts to curve up even though the round-trip time here is 20 milliseconds so it\u0027s what 50 100 round-trip times is just starting to get moving all right and next so have a look at Wi-Fi as well attitude 11 ad is also the house band with Wi-Fi uten currently you just starting Val to buy these three gigabit "
  },
  {
    "startTime": "01:22:36",
    "text": "three gig in measurements in a static office environment but I should explain that attitude or 11 ad doesn\u0027t go through walls so it\u0027s intended that you have an access point in every room and also you\u0027re not allowed to move around no not true but other people aren\u0027t allowed to move around mobility is okay but shadowing is not so they mustn\u0027t get in the beam however it does have sort of it can pick up reflections and things and get round shadows right so with testing compound TTP over this we\u0027ve got a median good put of even in a static situation but with some humans moving around in an office you got two hundred eighty Meg out of a three gig channel sneaky bit second channel so again very low utilization with the beamforming we didn\u0027t do this ourselves but using the beamforming goes up to sixteen percent but it\u0027s still not really using all this hard work that we allow to people have done and so you know this is a message for people here this stuff is a challenge for congestion control people to start working on the dynamics next so we learned these lessons about ten years ago when everyone was doing high-speed high bandwidth delay product congestion controls and we learned you should not have too many round-trip times between each loss you know you that\u0027s the problem with Reno as it gets faster the Sawtooth get so big the recover time recovery time and so you can see here why we\u0027re getting a problem in that cubic was the result of that research but cubics now becoming unscalable if you Kubik here 100 Meg if you multiply its rate by 8 times to 800 Meg the the distance between or the recovery time goes up from 250 round-trip times to a thousand one hundred round trip times sorry 50 round-trip times it doubles yeah 250 round-trip times from 250 50 so I\u0027m using this variable V to mean the number of signals you get per round-trip time and so the trouble is it\u0027s getting smaller whereas data center TCP you can just see these little sore teeth here you\u0027re actually getting two signals per round-trip potato sent to TCP and so and it stays it to whatever the rate that\u0027s that\u0027s the point of why we call it scalable and it means you\u0027re getting a lot more information all the time rather "
  },
  {
    "startTime": "01:25:36",
    "text": "than just sort of having to launch off into space until 500 round-trip times later you get another signal and you don\u0027t actually know what\u0027s happening so next I\u0027m not going to go through this this is just a record some very simple maths that essentially says that you need to make sure in your TTP equation that the P the Lost level or the 80 and marking level the power of it is greater than 1 so it\u0027s Reno it\u0027s 1/2 cubics 3/4 as long as it\u0027s one Operator you\u0027re okay and that\u0027s what we mean by scalable congestion signaling that means your sorties don\u0027t get larger as you go faster and that also means they don\u0027t cause more delay variation all right next and so the point about this and how this links with the radio stuff I just mentioned is that when you get a capacity decrease you immediately get some signals and you can drop away very quickly may take one or two round-trip times if you\u0027ve got to drop down by 10 10 times the capacity or something or one tenth of the capacity but when there\u0027s an increase you just get nothing you get no information and so if you have a very frequent signal that\u0027s as in the DC TCP case every two round-trip times you\u0027re getting a signal when that goes away you know within one round-trip time it\u0027s gone away because you\u0027re not getting any these two signals every round-trip time whereas if you only get a signal every 500 round-trip times it takes you at least a thousand round-trip times to think hang on I haven\u0027t had the signal for quite a time now and I was expecting well after 500 I and and so if you\u0027re getting a signal very frequently you also know when it stopped that\u0027s the main point and then you can start probing and but yeah you have a few minutes yeah I think I may be done I\u0027ve got you know a couple of slides so next yeah we can go to next oh no no there was one very important point on that yeah and so with the the a crate measurement that is a way to to find out that the capacity has just increased but for us it becomes very difficult when we\u0027re trying to get the cute and nearly nothing to ever see that because you\u0027ve only got a few packets in the queue and so you miss that and you need something else so if we are you know for the problem we\u0027re trying to solve which is very low latency for the new applications like VR and all the rest of it we can\u0027t really use a great pacing sorry accurate measurement we\u0027ve been trying but it\u0027s it\u0027s it\u0027s hard so that "
  },
  {
    "startTime": "01:28:41",
    "text": "was really an explanation of why we have this first requirement that in the paper about scaleable congestion signaling so that we get enough signaling the second requirement is limited round-trip time dependence and here is a very very brief example of that and why it\u0027s never been a problem in the past because we know we\u0027ve had a problem with round-trip time dependence for years and we\u0027ve just lived with it and it\u0027s been fun the problem is that as your queue gets smaller it takes out a cushion that was that was protecting you from this so say for instance you\u0027ve got 200 millisecond round-trip time versus a two millisecond round-trip time flow so there\u0027s a 100 times difference if you\u0027ve got a queue of 200 milliseconds in a drop tlq your actual round-trip time is 400 and the other ones actual round trip time is 202 so you\u0027ve actually only got a difference of two and that\u0027s why we haven\u0027t worried about the problem it never gets greater than two right because you\u0027ve got this generally drop tail queues are built to be about the worst you know the rule of thumb of one bandwidth delay products so but when you queue gets down to here where we are at the moment with PI and suchlike you\u0027re starting to see round-trip time dependents getting a bit worse and and the Hugh\u0027s we want and we\u0027ve got in l4s you\u0027re then getting close to 100 times difference and so round-trip time dependence becomes much more important because the larger round-trip time flows starts to starve right and I\u0027m not saying this is an ante starvation so that not shouldn\u0027t be there this is an anti starvation requirement nots a strong pair that\u0027s required right but we have one minute next so have you solved that problem the tension between this scaleable signalling requirement for the capacity variation stuff and all the dynamics and this article dependence and if you rearrange the mass to make this clear in one case P times the window has got to be greater than a floor and the other case is going to be proportional to round-trip time they\u0027re clearly incompatible next slide so the way to solve this or the only way we can think to solve this sorry not the only way we\u0027ve thought of some other ways as well where this is the favorite at the moment to once you know your round-trip time at the start of a flow you workout this and then you use that as your constant for your additive increase instead of just two as in the case of DC TCP you get this value if it depends on the round-trip times you and then you get a floor about or very slowly decreasing congestion signaling at low round-trip times so you\u0027ve got effectively a floor to your signaling so you\u0027re getting still even at 10 nanoseconds you\u0027re still getting one signal every 10 round-trip times but you\u0027re getting your round-trip time independence at high round-trip times "
  },
  {
    "startTime": "01:31:43",
    "text": "and it\u0027s not actually completely around to it time dependent a low round-trip times so we know with that formula and it\u0027s implementable because you only do this calculation once or maybe twice if your Android time changes significantly so you know all we\u0027ve we believe we\u0027ve got a compromise between those two first two rather difficult requirements to reconcile last time we talked about unsaturated signaling we had a solution to that that your cue obviously gives you the coexistence and we\u0027re trying to work on fixing bbr enough so that we can still coexist with it and then here\u0027s links to the paper here\u0027s links to the videos that you chose and if this were the PDF we don\u0027t have time for questions thanks Bob and please grab Bob in the hallways and ask him questions we\u0027re going to move on because we have 28 minutes and three more presentations to do the next three presentations are over 10 minutes each so please try to stick to that said again I is going to go up next good afternoon I will talk about improving car network scheduling with the best limiting shipper first I will before diving into a purpose - I will do a quick recorder of the current architecture next time so the current architecture define suite classes the expenses for adding class the are short for adding class on the default class so the excited for adding class is for real-time traffic for example voice over IP and its schedule with the first priority of the priority schedule the other two classes a F and E are scheduled as the second priority of the priorities kadra using a red scooter the choose features of the red Ghidorah it\u0027s the fact that it limits the capacity available to the AF to prevent the starvation of best-effort traffic due to the elastic nature of the AF class so let\u0027s take an example here on the x-axis is the weight the relative weight of the AF class and on the y-axis is the IAF output right and so if we take an example for example with an EF input rate of 50 percent we can see here in red so desired behavior with when you vary the "
  },
  {
    "startTime": "01:34:45",
    "text": "the weight of the AF class so next slide if you want an AFL - trait of 25 percent you will of course that a weight of 0.5 to get 2500 g AF output right but if we don\u0027t know the EF input rate of it if it varies for example from 25 to 20 75% well you don\u0027t get 25 percent for the EF I\u0027ll try to get a Alka trait rendering from 12.5 to search is 7.5 and so the you get is a 25% of the capacity and you don\u0027t know exactly what is the output Reggie would get and so we can say that when the EF input rate is unknown the AFL petrest is uncertain and our goal here is to make the AF output rate more predictable we want to make it more predictable but we don\u0027t want to impact the real-time traffic the EF class and so what we seek to obtain here is the desired behavior here in red it is the same as before with the red color but we want a minimum between this desired rate and Z is the EF the sorry the residual capacity left by the EF input rate so here for example it\u0027s 25% with EF input rate of 75 percent so when we when we when we consider all the weight relative weight for the AF class you can see here here in grey the uncertainty area you find yourself in when you consider the AF output rate and without proposition you get a reduction of of the uncertainty areas so now that I\u0027ve presented our girl explain more in detail I\u0027ll put our proposition so it\u0027s based on the worst limiting pepper it\u0027s a credit-based sugar which is proposed by the time sensitive networking group and it\u0027s key idea is to use a credit contour to change the priority of the cube and so here you use the transmitted traffic to update the credit when the shaped traffic is sent the credit is increased with a rate I sent and when so credit and no on the other case so for example if no credit is no trend frame is transmitted or fee of his the trainees is the frame it from another queue then the pretty is decreased with the right of I either then for the credit change its opening two cases if so credit is high on the credit and if the priority of the queue "
  },
  {
    "startTime": "01:37:47",
    "text": "is high and the credit reaches the max level then the priority of that you becomes a low priority or if the priority of the queue is low on the credit which resumed level and so you can see that when the priority of queue is change it will it will reserve a sunset amount of bandwidth from up shape to and so we worked on the best limiting shopper and we did a formal analysis using network calculus with Adam medallion fabric on set we also did a complexity analysis and analyst to prototype and the multiple simulation rooms those results are currently under submission from this work we found out that the ballast has a low complexity it is a hardware implementable and it has a key feature it can limit the capacity available to the shape tube and so when we consider the BLS and the priority scheduler it they behave much like a read scheduler and so this is why we decided to try and change the read scheduler in the RFC to set bursts limiting sherburn stood and so here is our proposition we set the brass key limiting wrapper in the AF class so the EF class is not impacted by the change it\u0027s still the first priority of the party scheduler the AF class as the F class priority changes from two to four depending on the credits as I explained previously and so sometimes the default class for best for traffic as a priority higher than the elastic traffic and sometimes it has a priority lowers on the elastic traffic and this how so birth limiting shopper can limit the capacity a road to the AF class okay so we did several simulation using NS to first with the weighted round robin - for the red scheduler here on the left are the results as expected we have the desired behavior here in red and it is framed with the behavior when we have when we consider an EF input rate of 26 or 76% on the right who have the result using our purpose or proposal with the Burris and we can see that when considering EF input rate of 26 or 49 percent we have this as the desired behavior with a input rate of 76 percent however we have the desire behavior up to a weight of 0.5 and after that we have the output rate which is equal to "
  },
  {
    "startTime": "01:40:49",
    "text": "the remaining capacity left by the EF input rates and so I think you will recognize the previous figures with the uncertainty area which is much reduced with the burst riveting shopper compared with waited on ribbon and so we can conclude that the EF class is not impacted by the change the BLS parameter can easily be calculated from a current configuration when you consider an expected EF input rates when the EF input rate is known the BLS and the weighted round robin are the same AF arbitrate but when the EF input rate varies the range of possible AF output rate is much reduced with the bus limiting shipper compared with the weighted round robin and as a perspective we hope to propose a draft in the a creme working group thank you for your attention thank you so much is it\u0027s time for a quick question if there is one better snakes all right one quick suggestion you may want to actually talk to the PSU ewg working group chairs instead of the eqm want and they\u0027re sitting like that take it away okay so I\u0027m going to quickly talk about need and how to use it to optimize a mobile communication system Jenna so next the outline first some words about ossified internet then neat and how it maybe could be used to solve this and then need for mobile communication next okay the Internet today is kind of ossified and the reason for this is that we have used protocols and combination of protocols that has worked and good so everyone has used it for some things and when everyone has used it the kind of infrastructure has adapted to it and this is of course not good because things new things like new protocols and song and such are not supported by this adopted infrastructure and this leaves little room for improvement next so one solution this could be neat which enables the use of transport services instead of transport protocols it\u0027s an implementation of tabs "
  },
  {
    "startTime": "01:43:49",
    "text": "and it can provide services like reliable transfer most both communication low latency services need to tries to map application requirements to services so an application could say give me a low latency transport service and transparently to the application the new system will give you that kind of transport service so Nick tries to overcome the problem of ossification by providing a more expressive API that allows applications to say stuff like this like give me low latency service so that the application doesn\u0027t have to bind to a certain protocol at the same time the other point which we will see later is using a local and remote information to make well-informed decisions and thirdly you need users happy eyeballs to kind of test the services it creates next so quick example we have a application that wants to have a low latency service it asks to need API for that in JSON this requirement from the application will end up in the policy manager the policy manager will try to match this low latency requirements to policies that are stored in the PIP there the policy information base and also match it with characteristics found in the SIP the characteristics information base so in this PIP there will be a number of different policies policies for high-throughput communication for multi powers for whatever but in this case we want to lower latency so the policy manager tries to map this low latency property to different policies and different characteristics and build a list with different transport solutions it could be tcp with this and this and this socket options set over this interface because interface information and information like that are available in this characteristics information base and then the policy manager will try to resolve this requirement into a list and this list will then be handed off to the happy eyeball mechanism that will try to and the services so next and in this particular example we get a match because we have a low latency profile that says that we are going to use TCP as a transport and awkwardly enough this be no delay set to fools for low latency so it\u0027s kind of a typo but yeah okay so we kind of thought a bit about what to do with it and we had a bunch of mobile "
  },
  {
    "startTime": "01:46:50",
    "text": "nodes in the department with mobile broadband support and there was a Wi-Fi support so we thought hey why not test to use NIT and M participate here because currently and participe is not it\u0027s not optimized to this use case it\u0027s more of a general transport protocol right so there are some details in MPGs B that aren\u0027t perfectly aligned with a mobile node communication can use case so let\u0027s see if we can do something about that using it so you cannot have this set up we have three kind of main problems if trying to communicate from this client to the server so depending on the size of the object that you want to transfer for instance it can be costly to set up a multipath connection so in some instances it might be various to have a single path because it\u0027s basically not worth it the second thing is if we have link technologies that are very asymmetric in terms of performance let\u0027s say that we have very very very good Wi-Fi and maybe 2g or 3G connection then it might not be a good idea to include the bad one at all and just run single path because it can hamper the performance of the entire connection and then we have a third one that we absolutely don\u0027t have time with so I will skip that next this was a setup we had we had a regular server running Linux with multiple TCP and Monro node Monroe is the European project that builds a large mobile broadband testbed and these clients are kind of good small clients having both Wi-Fi and mobile broadband support and the good thing with him is that it continuously measures metadata about what kind of access technologies are we currently having on a mobile broadband interface signal strengths and so on and this can be just plugged into needs so that need knows yeah soon it has that information next the experiment was very simple that we did we have a client the client downloaded a set of files from the server using TCP and MPSP with the regular socket API and then we mustnít yeah so the first thing is it costly to set up well if you look at the graph you have the size of the different files in the x-axis and the relative download time on the y-axis and it\u0027s relative to TCP so for short flows you don\u0027t gain anything by using a participate you actually lose some but for longer "
  },
  {
    "startTime": "01:49:50",
    "text": "flows longer than 100 kilobytes in this particular setup you gained a lot neat on the other hand was able to track this at the whole time so it effectively built the transport service using TCP for the smaller flows and then participe on the larger ones and how is this done well this is a kind of simplified view of it but the policy manager and the policy that we made for this particular experiment was simply stating that if we have a small file in this case less than or equal to 100 kilobytes or we had the link technology on the mobile broadband in the face that was poorer than 4G then she was TCP as basis for transport service otherwise she was and participate in the left you can see the same experiment as previously and that was when we had the Wi-Fi and the 4G connection so when we change the 4G to 3G we can see that we are getting starting to lose a lot by using MP TCP regardless of the size of the flow here and it has to do with several things related to and participe and as you can see and it does not choose to use an participe at all in this case and this was of course because we specified that policy not to use it if we had a link technology worse than 40 but this is just examples of how you could make the policies and I guess we don\u0027t have time to choose to view the last one so we jump there so neatest implementational taps it can compose transport services based on application requirements and yeah questions or no we do have time for one question if anybody has one thank you for keeping it tight alright thank you / we did have sorry J holin on driver we had one question from John Leslie can we design how can we design this to minimize set-up time the famous business upper right it\u0027s for you yeah John "
  },
  {
    "startTime": "01:52:53",
    "text": "answer the question so again sorry I didn\u0027t the question was how do you how do you use this will minimize set-up time I mean you can it solely depends on the policy that you use so you can choose to use protocol that has less kind of set-up time or she was to configure a protocol with an option that gives it less connection time but that would be about what you can do right thank you I\u0027ll take it away Michael so hello my name is Michael met I\u0027m from University of Tuebingen and I present joint work with Nicholas side lot so this work is about activity-based congestion management for fair bandwidth sharing entrusted packet networks this work is inspired by congestion policing using congestion exposure that is work that was carried out some years ago in the ITF so this is not an evaluation of congestion policing using congestion exposure it\u0027s rather work that is inspired by its philosophy so the problem is we have a bottleneck link that is shared by multiple users where congestion may occur flows may use different transport protocols like UDP are not reactive or TCP which are reactive and that can lead to unfairness some users may transmit multiple flows others only a single one so how can we share fair share pend with fairly among users one solution is if packets can be related to users we can use weighted fair queuing that enforces per user fairness for bandwidth sharing if pegas cannot be related to the users weighted fair queuing cannot be applied and here ABC comes into play so this is the basic architecture of the ABC principle it is per domain concept we have access nodes and we have core notes the axis nodes they the user equipment is connected to these access nodes and the access nodes meter the user traffic and add an activity value to the packets this activity value is monitored by the current axis nodes and they run an ABC aqm to drop packets during congestion and for packets that have a higher activity value they get dropped with a higher probability than packets with a low activity value and "
  },
  {
    "startTime": "01:55:54",
    "text": "that leads to fairness between between heavy users and light users so that is the basic principle what is activity well every user is assigned a reference rate that is a fairly low rate which is below the fair share of every user of every user in the network and we kind of measure the recent transmission rate of a user and the fraction of this transmission rate and the fair rate of and the reference rate of the user gives you more less of the activity so the activity is a multiple of the assigned reference rate of a user and the higher the activity is the more them the more user has sent in the recent past well this activity can be metered by a scalable algorithm I don\u0027t go into details here what about the core nodes the core notes observe the packet stream they monitor the activity of the packets and take an average of them so they basically to a weighted moving average of all the activities observed in the packets then we use some existing aqm based again mechanism that is based on drop probabilities and the idea is now that we this what the drop probabilities in such a way that we get smaller drop probabilities for packets with a low activity and higher drop probabilities with four packets with a high activity and in some that leads to more fairness in our evaluation we we used a very simple a QM I don\u0027t go into details here and this is the experiment setup we have two groups of users we have one heavy user group which sends ten TCP flows and we have another light user group which sends only a single TCP flow and the traffic enters the network of a excess of an axis node that which does this activity metering so here the activity information is added to the packets and then we have a core node that is connected to the bottleneck link this core node sends us all the activities does the averaging and applies the distorted Petty drop probabilities here are the results the performance metric is the throughput ratio what is the throughput ratio the throughput ratio is the average throughput of the heavy "
  },
  {
    "startTime": "01:58:56",
    "text": "users divided by the average throughput of light users and if you have if you have an throughput ratio of 1 that means perfect fairness because the heavy users have the same throughput as light users as any light user here numerical results so the first the first row shows you the delay of the bottleneck link the second row shows you the use of configuration that gives you the number of heavy users and the number of light so it\u0027s one light up one heavy user and ten light uses for example and then we have a third role that is the differentiation factor this differentiation factor is a configuration parameter for the ABC a differentiation factor of zero means ABC is disabled and for that case we get a three put ratio of ten why well it\u0027s clear because heavy users send ten to have ten TCP connections while light users have only a single TCP connection then heavy users without doing anything in the network have a tenfold throughput so what happens with ABC with ABC we get a throughput ratio of around one and that requires a differentiation factor of three so that works quite well so what what else have we done we have considered the reference rate which is a configuration parameter for a system we understood that it needs to be set to a rather small small value which is way beyond which is clearly smaller than the fair share of the users what else have we looked at we have looked at the coexistence between non-responsive users say a heavy user that sends you repeat traffic without congestion control and and light users with only a single TCP flow on the on the x-axis you see the transmission rate of UDP users and as the transmission rate of this unfair UDP user goes up the throughput ratio also increases if you if we do not use ABC that means the UDP user can staff all the other users in the network and when we enable ABC on the network we can see that we still get pretty good fairness north order you also the unit user wants to send 5 10 15 even 20 "
  },
  {
    "startTime": "02:01:59",
    "text": "megabits on a 10 megabit link but the UDP user cannot hurt the TCP uses anymore with ABC another experiment showed that we can reduce the upload time for a light user with another parameter which is the burst tolerance so ABC has the burst tolerance that can be used to tune the system so that we can increase small uploads this work is related to the cost dateless fair queuing work from yen Stoica which was carried out 14 years ago the difference is that with this cost eggless fair queuing we required rate measurements in on the nodes while in this work we leveraged active queue management and this gives some nice perspectives in particular this work is extensible towards low delay communication so it\u0027s possible to enforce both fairness and low delay what are the use cases the use cases are basically those that we had already for congestion exposure it\u0027s fair rate sharing and improved quality of experience for light users there were some draft that congestion exposure may be applied in data centers there\u0027s also an RFC that connects may be applied for mobile access networks and there is another perspective namely ABC may protect light users against heavy UDP users in other words it may be used as a defense against denial of service attacks so conclusion and outlook activity-based congestion management uses axis nodes to add activity information into packet headers axis and core nodes use that information for preferential dropping in case of congestion and that enforces per user fairness so that heavy and light users can coexist in a fair manner the overall concept is protocol independent and it leaves the core network stateless I presented some simulation results that show that the that the differentiation is quite effective we understood the configuration parameters for example this reference rate which is most important and protection against non-responsive traffic is possible with ABC it\u0027s also possible to reduce upload times significantly which is which can be also tuned by a parameter ABC "
  },
  {
    "startTime": "02:05:02",
    "text": "supports different user profiles so it\u0027s not necessary to set all the users who set the same reference rate for all the users so different different exit different user rates are possible and envisaged use cases are the same as for congestion exposure for the work can combine ABC with PI for instance to achieve both fairness and low latency and we also would like to adapt ABC towards different service classes this work has been published at the Nam\u0027s conference last year and a preprint of this of this work can be downloaded from my website that\u0027s about it thanks any questions Bob very quick and your activity factor it\u0027s about relative rates yeah it\u0027s it\u0027s about relative rates here so it does that mean it\u0027s instantaneous it\u0027s what instantaneous yeah pretty much instantaneous hell okay so there\u0027s no history that so a light user that sort of doing a lot of short flows and versus a heavy user that\u0027s doing one or multiple big flows you like use it doesn\u0027t get any benefit from not being active at all if you\u0027re not I mean you you can have a history on it because you have options how you do the metering at the axis node and if you want to have more history there you need to apply a larger memory on the right metering and then you can have this memory but that sort of arbitrary what that time should be analyzed hmm all right thank you very much Michael and thank you everybody for for trying to stick dear dimes if you really I run over time by six minutes so please ask Bob for copies if you want any I\u0027m picking on Bob thank you everybody and we\u0027ll see you next time [Laughter] [Music] folks who are looking for the slides have uploaded them so you should be able to find them online now "
  },
  {
    "startTime": "02:08:11",
    "text": "if you go to blue sheets please bring them up here "
  }
]