[
  {
    "startTime": "00:00:18",
    "text": "Sam does my mic sound okay by the way it's been clipping recently in meetings you know it it might be just a hair over mod but no one's going to Care like say another couple words testing testing one two three testing it sounds like you're on a um laptop mic a speaker phone but it's I am I'll just put on my headphones no you're it's fine okay it's fine it's within the realm of normal okay that's all I think that's the best I I personally can hope for having just been on the sound crew for an event this weekend like they care more about sound we [Music] don't e"
  },
  {
    "startTime": "00:02:28",
    "text": "e e as I watch the trickle of people as much as I want St on time I'm going to give it another minute and a half e"
  },
  {
    "startTime": "00:04:18",
    "text": "all right colleagues telling you things you already know the session's being recorded um things that you say here if your you are your organization have patents on them those need to be disclosed um please be nice to each other be respectful to each other if you feel that that's not happening we have the ITF has UDS people who are happy to help um as of course are your chairs and um because we're an interim meeting we don't have to worry about a lot of the details but please don't have your audio or video on unless you are actively speaking um headsets are great um and then agenda bashing um the point of this meeting is to figure out whether the first RFC coming out here is going to support Heavy Hitters or not and if so how um those are the questions we want to answer today if we have time uh Jun has asked for time to talk about Pine um any other business or any agenda bashing go ahead Tim thanks uh so I think you correctly framed it as like we want we want to decide whether or not uh Dak will do Heavy Hitters um but we all the related discussion though is like what this working group might do about Heavy Hitters uh outside of the scope of dap specifically so I guess the pedantic agenda bash is that I think um we should like be prepared to discuss how we would approach Heavy Hitters outside of the scope of dab itself very good um but I'd be open to yeah"
  },
  {
    "startTime": "00:06:00",
    "text": "thank you anything else all right um Chris and Tim I understand you're splitting the presentation um I'll let you do slides all right I'm starting out here but I'm struggling to figure out the how to I click the button yeah hit it again okay I don't see do you need to stop sharing slides I did stop sharing slides I'm just having to find the button to approve it so you have to Mouse over Chris's icon in the queue and then there should be like a gr slides button there do it again Chris uh no need I have the popup now and I am sharing my slides right okay okay also we do have the um zulip room open as you as some of you have seen in case people want the back Channel all right sorry I'm trying to figure out if I could I guess this is good enough oh wait there we go cool all right okay let's get going um yeah so as was mentioned um today I'm hoping that we can decide if and how to support um Heavy Hitters um so uh I think everyone in this room I see has already seen this document um uh that"
  },
  {
    "startTime": "00:08:00",
    "text": "we've been working on in fact you know most of the people are here so um but I just wanted to give a quick shout out to the people who who gave feedback on this um uh and and helped us kind of figure out uh what direction to go in so um so let me let me like I think it's worth level setting anyway just so we all understand um the nature of of of what this would what what this would mean for dap so the problem we're trying to solve is uh we call it the Heavy Hitters problem you have a threshold T and each client is submitting just an arbitrary bit string and you want to know how many which bit strings were submitted at least T times so um this has tons of applications um basically you want to know like what is like kind of the most important thing in your in your metrics to pay attention to so I think kind of the the most common use case is like um what are what the use case to think about is like what are the most popular URLs that your that users of your web browser um are visiting so this problem uh is solved by uh this vdf we call popular one this is based on a paper from 2021 um and it was kind of like the first demonstration that you can solve this problem uh like in a performant way on the order of like say 30 minutes to an hour um so before this paper uh this this was not possible possible and the thing that makes it possible is this thing called an incremental distributed Point function so what this allows you to do is you have secret shares of some users's measurement and you can ask is this measurement prefixed by a a a particular string we call these strings candidate prefixes so uh what we this allows this functionality allows us to do is basically count up how many measurements of of these input measurements begin with a a a certain candidate"
  },
  {
    "startTime": "00:10:00",
    "text": "prefix um so we call this prefix counting so um yeah so all of this kind of happens what's nice about idpf is all of this kind of happens server side the clients upload the their reports and um the aggregators can do computations on them such that each can compute a secret share of the prefix count for each candidate prefix so the way we use this uh functionality is um the uh uh aggregator The Collector specifies this sequence of candidate prefixes um sends it to the aggregators the aggregators count uh compute their shares of the counts for each uh for each uh prefix and then uh send that to The Collector and then collector is going to use this information to pick the next generation of of prefix prefixes so basically if uh some prefix P has uh a a a count above the threshold um then uh we're going to add it's uh like p p plus uh zero and P plus one uh into the next uh set of candidate prefixes and so on um so just a kind of uh I think I think it's helpful to have an illustration basically we're doing this tree traversal thing so uh what I've drawn here is uh kind of the uh the the binary tree so uh each each edge of this tree is labeled by a candidate prefix and in the nodes we're going to put the pre the prefix count so um so uh this kind of table here I I guess you probably can't see my mouse but the table on the left side of your screen this is our kind of uh this is our our sample set of input strings uh and we're going to run Heavy Hitters over the Set uh with threshold T equals 3 so uh the first thing we're going to do is count how many uh inputs begin with a zero or a one um uh so uh in this case uh we have 30 inputs that begin with one"
  },
  {
    "startTime": "00:12:02",
    "text": "and only two that begin with zero so um two is below the threshold so we're done traversing this side of the tree the left side of the tree um and but on the right side of the tree we certainly have data that we're interested in so we're going to add its children into the into the set so we're going to Traverse one zero and one one and um in this particular example we see we get a count 16 and 14 um again both are in this in in in the uh in the over the threshold so we add their children to the Next Generation and so on and so forth until we reach the bottom of the tree so this is how kind of tree traversal works so at each node we're Computing the aggregators compute uh shares of this prefix count this uh that's shown in the node they have secret shares of this we add them together to we to to reveal the uh the count and then use that count to make a decision about what direction to go in next so does anybody quickly have any questions about that I'm guessing no one here uh has questions but maybe the chairs do um to be clear if your threshold were say let's say your threshold is two you'd go down multiple paths right yeah so if your threshold was two then uh going back up to here we would have added both 0000 01 one zero and one one into the Next Generation thank you and by the way for the record the chairs are concerned about the attendance today yeah I I don't know what to do about that uh um um but all right I'm gonna keep going um okay so um so the the next thing I want to point out here is in this traversal we've learned uh some information about our puts so the what we wanted to know"
  },
  {
    "startTime": "00:14:02",
    "text": "what what are the Heavy Hitters and The Heavy Hitters are these strings that kind of go along these uh these cyan uh highlighted edges so we have one Z1 one Z one1 One one0 Z and 11 one one1 those are all the Heavy Hitters but what you notice is that we've in in on on along the way to Computing the these Heavy Hitters we've learned additional information so um this is a inevitable this is how the algorithm works we can't avoid this uh without adding something on top of idpf that is popular um and it's actually worse than this so um something that we've been talking about for a couple of months is uh this a thread of what we called steering attacks so the observation is that in our threat model we Imagine The Collector is malicious controlled by the adversary and so is one aggregator so what we're making what we're assuming is that one aggregator is on US during this computation um so this means in our threat model that the the attacker gets to pick the prefix tree that we Traverse um it it learns the prefix counts first and um it it and it basically tells the aggregators where to go next so um uh yeah so we can partially mitigate this by just kind of uh adding some restrictions on how traversals can go um but the thing that we can't do anything about is uh this thing called an additive attack which I'm going to describe now um so here um so here we're our starting point is the same like we are we are just going down the tree and then we let's let's so we're kind of like going back to this first level um so we notice two and 30 uh and we're like well that's that I'm interested in what the that two must be there must be uh something juicy there so um what I can do is just lie about the prefix counts"
  },
  {
    "startTime": "00:16:01",
    "text": "so I add uh in this case I add one to the left child and I subtract one from the right child and uh now the hit count is three and I can and I can go to and I can go ahead and start traversing to the next uh level um so that's what I do um so I discover that this is uh yeah so this is kind of a noop level um but I need to keep going here because I need to lie about the hit count on the on the left uh once more um and we kind of do this um until we get to a point where like you know we've we've um we've learned something we weren't supposed to learn so we have uh found that 0 one0 is in the input set even though it's a it's not heavy hitting it's a it's it's a light hitter is what we've been calling it so um so uh yeah so that's kind of how the the attack works um does anybody have questions about this attack okay hearing none um so this is sort of inevitable in uh uh in our threat model um so um what can we do about it well the the thing that we know that we must do is add differential privacy to uh idpf in order to make them secure so uh differential privacy is something this working group has discussed uh there was a draft proposed a while back um um and hasn't gained much traction I think mainly because uh we haven't had an application but uh well here's an application we need differential privacy for this so what intuitively differential privacy provides is we're adding noise to the prefix counts that we compute at at each node and what this does is just introduce some inert uncertainty about the true hit count so"
  },
  {
    "startTime": "00:18:01",
    "text": "um I kind of flashed back and flashed forward um so the true hit count there you can see at Z 100 uh 0 one0 0 is is two and uh I don't know in this example I kind of like just just sprinkled in some Some Noise by hand we see that uh we have two nodes one is one has value one one has value minus one well like what do you know like what does an attacker make of that um the fact is that we do have non-negligible information that we're not supposed to have but we do have some uncertainty about whether uh whether what we're looking at is true um so it's obviously a lot more complicated than that formalizing this uh security property for popular one is is is is very very very much possible and we can actually have uh a security like a theorem that tells us how do we relate the parameters that we choose to do DP to the chances of an hacker succeeding at this attack so that's differential privacy um this is kind of the tool that we know we're going to need uh however I I want to I want to mention a couple of other things that have been brought up um really over the last year or so related to this problem um so another way we might try to mitigate this is by bolting on additional crypto so uh what's been observed is with general purpose NPC we can pretty much do anything we want in particular we can say like I have a share of the prefix count you have a share of the prefix count we want to check if the the the prefix count is above the threshold without actually learning the prefix count um and this is a this would be a useful thing to do um uh we it basically it means we don't need to reveal prefix counts to the attacker and this makes uh guessing harder uh in particular uh steering and tax harder um another way we might use"
  },
  {
    "startTime": "00:20:02",
    "text": "the trivial the way from the popular paper to uh do to add differential privacy noise is each aggregator kind of samples noise uh and adds it into each of its uh prefix count shares and this works just fine but we add more noise than we than we actually necessarily want so we can get better performance if we have a way of like uh sampling precise shares of DP noise um such that like we have a share from a sample of the distribution that we want uh I see a hand do I see a hand you do sorry I have to pick a nit you just said we get better performance and I wanted to clarify like I believe I think what you mean there is better sort of utility and that you apply less noise I mean I mean better utility I don't I don't mean that faster uh this would obviously be a lot slower uh so yeah as n as uh Tim points out um we are basically in the two- party party setting for for uh for MPC with dap um and uh this is probably twoo expensive in uh in the two-party setting um based on feedback that we've gotten in various in various channels but we we don't necessarily have a definitive answer but I also wanted to point out that um there uh Martin Thompson and gang are pushing on this uh new MPC framework that enables things like IPA um and and in this framework it it's might be possible that we can do things uh much more efficiently um and then uh just to kind of like round this out um there are also different approaches to Heavy Hitters that are possible so something that this working group has discussed is star um this did not gain traction ultimately because there is a Dos attack that was discussed and we couldn't figure out how to mitigate it at least not efficiently um the potential upside is that we get"
  },
  {
    "startTime": "00:22:02",
    "text": "than popler uh with uh within the same threat model and in fact we there's this recent iteration of star called pop star that kind of uh shows you can get uh basically the same leakage uh that you get with popler which is good um but still like you know it's still I we you know this this it might not be reasonable to to actually deploy this um okay so that's all I wanted to say about the this problem at a high level I think the the the main question is like do we actually are we ready to support Heavy Hitters and I just want to point out before I give it off to Tim uh uh this this is very much still a i it's a research question how to how to solve Heavy Hitters um we kind of know that uh we know for sure it'll require differential privacy we don't have a spec for this yet um it's also worth mentioning that there might be improvements that are possible um if we kind of go beyond the current framework that we have um so with that uh Tim I'll I'll leave it to you I'll I'll let you take over thanks Chris all right let's give Meo a shot I want to see if I can request the slides here not sure if you need to surrender control Chris Yeah Tim or Chris needs to stop sharing try it again Tim there you go neat okay where were we whoops that was it okay thank you um okay so I want to double down on what Chris was just observing um about how yeah doing Heavy Hitters effectively is there are some open research questions there there um I"
  },
  {
    "startTime": "00:24:00",
    "text": "bring this up in particular to draw contrast with the prio stuff that we have been working on where there I think we have a much firmer grasp on like how it works the security uh characteristic is and indeed how to deploy it um so I think this is very significant this distinction when we're discussing like in which draft shall we do shall we address which problems um all right so in this next section of this presentation um what we have here is a few questions uh sort of questions put to the working group group that we as editors of DB have come up with now obviously it's the um you know it's the responsibility of the chairs to sort of formally put questions to the working group um especially keeping in mind like the the attendance today but what we want to do as a document editors here is sort of get on the record uh first like the statement of the problem that Chris just gave us and now our view on what we think we could do about it um especially as we have sort of developed these concrete proposals about uh changes we could make to documents and texts we could go write so there's um a couple different questions we're going to work through here and what I want to do is uh yeah go through the questions and then spend some time fleshing out the proposals that we have for like how to address these things um and then once we've once I've done that then I think we can consider as a working group like how whether you know how we want to answer these questions okay so yeah the highest level question here is uh should dap distributed aggregation protocol support Heavy Hitters um all right another thing we have to tease out here right is that uh dap is not necessarily the only product of the PPM working group so even should we say that dap doesn't do Heavy Hitters uh that doesn't mean the PPM is giving up on the question right it would mean that we might Envision addressing Heavy Hitters in a sort of a parallel or separate document um uh which as we note here a lot of it would be borrowed from dap like for instance I think we we would come to observe that say the upload interaction for reports would look exactly the same basically and probably a huge part if not all of handling of aggregation jobs"
  },
  {
    "startTime": "00:26:01",
    "text": "between aggregators would look very similar um what ends up work behaving differently is sort of the The Collection back end um so yeah so the draft that we've got that does uh daap um it currently sort of is well suited to handle the pr3 family of eafs um mastic which is uh yet to be adopted anywhere which we could use for cases very similar to prio but in this case we'd be using a specific speically for attribute based metrics um and the the pine VF uh I forget what the pce stands for but it's inexpensive Norm estimation um which is specialized for machine learning model training um okay so this brings us to sort of three different proposal concrete proposals that we document editors have for how to move forward here uh in proposal number zero we would be we would say dap will not do Heavy Hitters and then we get to specialize dap to just um the case where each report or each batch gets collected exactly once um if we do choose to support Heavy Hitters then we have two different proposals for how to handle that which we'll uh cover in the next few slides and I would note here that um even should we decide that we don't want to do Heavy Hitters in Dap what we're laying out here in proposal one and two could still inform sort of a separate document that PPM would uh take responsibility for drafting um okay so if we do want to deal with Heavy Hitters whether in Dap or outside of it uh then the question is what are we going to do about steering attacks so over the last few weeks in collaboration with a whole bunch of folks that Chris acknowledged earlier we've been elaborating what are sort of the M Sam yeah should we go try and answer these in order or have discussion on them in order or do you want to try and frame it first I think it's worth uh going through the the list first yeah yeah so we want to like enumerate these"
  },
  {
    "startTime": "00:28:00",
    "text": "goals here of these mitigations that we feel are necessary and also I want to spend a little time just showing what are actually the document changes we're talking about here because I think that helps inform like does this belong in dab or does it fit somewhere else um okay so right so we covered steering Chris rather let us Chris steered us through steering attacks earlier and what we've come up with are is this set of mitigations that help to um eliminate some uh some attacks or some information leaks or limit others so what we can do is enforce uh we can require that the tree traversal is consistent which means concretely that every node visited has to be the child of a previously traversed node um this stops the attacker from like from say finding that there's a heavy hitter heavy hitting node let's say on the right side of the tree but then deciding like oh I'm going to steer down the left side of the tree of the next round because that seems more interesting um we can also have the aggregators reveal prefix counts at each level to each other um because then they could enforce the property that the weight of each node visited uh must be equal to the sum of its children um and this uh limits the ability of the attacker to redistribute weight across subt trees um similarly we can have the aggregators commit to shares of the prefix count um at each collection um the idea here is that we want to deny the attacker the ability sorry we want to give the attacker the least amount of information possible when it is executing the add excuse me additive attack that Chris described earlier um so if we don't force commitment then the attacker gets to see the honest aggregators uh share of the prefix counts when it's constructing its own share um and finally uh we can require differential privacy um which as we saw before mitigates additive attacks um and we should note also and again this is something of an open question um applying differential privacy noise"
  },
  {
    "startTime": "00:30:00",
    "text": "to aggregate shares can to some extent complicate some of these mitigations because for instance you want ideally you'd be able to verify that um two child nodes their weights sum up to exactly the weight of their parent but in the presence of differential privacy you can't have an exact comparison like that um so you instead have to Define that the weights have to sum up to the parent within some bounds that are a function of the differential privacy parameters that were chosen okay so now we're going to excuse me just taking stock of the slides yeah right now we're just going to look at the concrete proposals that we have for how we could tackle these uh how we could apply some of these mitigations in De so the first proposal uh involves we think the least intrusive changes to DAP they fit the most neatly into dap the way this works is that essentially as you would think of it today The Collector is responsible for driving traversal of the tree um so what that means is we have to allow uh each batch to be collected multiple times with a different aggregation parameter and each collection of a batch means going down one level of the tree um so the yeah so what's needed here the changes to a specification that are required for this are we have to introduce the ability to uh have aggregators hold on to reports across multiple collections of a batch and add code pass for running aggregation jobs that sort of refer to previously transmitted input shares um or report shares rather instead of having the them always included in line so the idea here is you know if I if I'm aggregating over bit strings that are 1,24 bits long um I'm aggregating over the same set of reports each time and it would be wasteful to have the leader retransmit report shares at every single level of the tree so this is just about introducing a message variant that says um you know aggregate over these reports that I sent you previously um there's also a number of tweaks we have to make to report validation rules um implemented in the leader and helper at aggregate job initialization time um these are spelled out in detail in a Google doc um which is linked at the end"
  },
  {
    "startTime": "00:32:03",
    "text": "of the slide deck uh for those of you who haven't already seen it or indeed contributed to it and this is the one that you sent to the list a week or so ago yes that's correct and there I see Chris Patton just put a link to it in the chat here wonderful um okay and yeah we'll get to trade-offs in a minute the second proposal is significantly more disruptive here we would shift the responsibility for driving veral of the tree into the aggregators so um in the previous setting right The Collector is involved at each collection of the batch and saying like Okay let's go down let's go down these um nodes that we believe they're Heavy Hitters in this proposal the aggregators um get to see the the prefix counts at every level of the tree and uh at that point they're able to independently decide whatever Heavy Hitters and whatever sub which sub trees is to Traverse so the collector would just say give me the Heavy Hitters please the leader and helper would then on their own do as many collections of the batch as are necessary and then simply deliver the set of Heavy Hitters back to the collector um uh let me finish this slide real quick Benjamin and then I'll I'll take your question so this requires all the same changes as proposal one but we also would have to define a new notion of like a different kind of collection mode um what Heavy Hitters would be one particular collection mode uh we have to introduce a commitment scheme between the aggregators and the other thing is that this introduces Heavy Hitters specific semantics at the DAP layer which to some extent pierces the uh the veil of abstraction of vaf um and we so we lose some measure of generality relative to where dap is today Benjamin go ahead hi uh yeah I have a couple of questions about this um why first of all does this really require all of the things discussed in proposal one um you know if the aggregation logic is"
  },
  {
    "startTime": "00:34:02",
    "text": "defined to be something that the aggregators do then can a dishonest aggregator really Mount one of these additive attacks or steering attacks it seems like you know that the the behavior is all prescribed uh within the specification and and the you know would fail it seems like it would some it would not it would fail to merge you wouldn't get wouldn't get sensible output uh let me flip back to that's the one okay so this is the slide that Illustrated how an additive attack would be run um on the prefix tree right so right cons consider that uh at this level of the tree the way the way that we have this Illustrated here right is that we're operating on we're doing this additive attack against like the real prefix counts um which are the sum of the aggregate shairs that were constructed at level one of the tree here so the aggregators in fact they don't get they don't get to see this what they get to see is the aggregate share that each of them constructed so instead of two and 30 right what they'll see is some some random looking value in the field um on the one side and then uh sorry same on the other side and when you sum those shares together they sum to two and 30 okay but okay so in that then then how is proposal two supposed to work if it continues to to use that representation how can one aggregator do part of the Heavy Hitters comput when it cannot tell the difference between heavy hitting and light hitting prefixes I see what you mean uh in proposal two the aggregators would reveal their aggregate shares to each other such that they could aggregators could independently then um each on their own decide like which are the Heavy Hitters and thus what are the correct sub trees to Traverse uh well okay independently in the sense that they each have a lot of information provided by the other aggregator that's right the important thing okay so if we did it like as daap today or as in proposal one then the collector has complete power to decide"
  },
  {
    "startTime": "00:36:01",
    "text": "what sub trees we're going to Traverse right and the idea is that in proposal to we ship that responsibility into the aggregators but on top of that enable either aggregator we give either aggregator all of the information necessary to decide which subt trees to go down so that they can sort of so that sorry neither aggregator is trusted to just sort of arbitrarily choose what sub trees to diverse but okay either aggregator can still do these additive attacks to sort of add weight to the wrong side of the tree how many rounds of communication does this end up with between the aggregators the same number of rounds as you would have had between the collector and the aggregators um which is to say the depth of the tree yeah uh okay and is that the reason why this is considered not a vda because of that uh you know if if we could do this without any without any communication between the aggregators um so they you know in a single round back collector would that make it qualify as a vda it's still a vda you would still be using like pop or mastic um articulated in the shape of a vaf but when I say that this pierces the um the abstraction what I mean is that like in da today or our our beautiful dream of dab was that it could operate completely generically over vs and that at most you needed to have this affordance of being able to collect a batch multiple times um and that would suffice for Heavy Hitters like right um uh and then the Hope was that so for instance in the case of heavers concretely what we're doing when we do multiple collections of a batch is going down levels of the tree yep dap didn't need to know about that at least we we we were hopeful that it didn't need to know about that um what proposal to concedes is like no we actually do need to sort of introduce those semantics at the DAP layer like the a the DAP aggregator needs to become aware of like what what are these intermediate collection results um and how dides that inform like the next thing I'm going to choose um so it's like some of the details of some of the details of what this the vda is actually doing um kind"
  },
  {
    "startTime": "00:38:01",
    "text": "of leak into the daap later okay thanks for that explanation okay um so that's that okay so then we have this yeah summary of like basically tradeoff prop proposal one and two so I mean not shown here as proposal zero because the trade-offs are bet just but like we don't really do Heavy Hitters um so the considerations here are yeah proposal one is less intrusive to the spec it like fits the model the philosophy of D better whereas proposal two is quite intrusive and disruptive um in proposal one uh The Collector still gets to learn all of the it still gets to learn the whole prefix tree um that we saw Illustrated in that slide earlier where there was the distinction between like what we wanted to learn and what we sort of unintentionally learned in proposal two however we can conceal um in uh all the prefix counts including the count of the leaves the actual heavy hitting strings from The Collector um because we've shifted the responsibility of treat traval into the aggregators we can reveal nothing but the set of heavy hitting strings to The Collector um uh proposal number two is probably faster this says optimal latency which is a bit strong but um the idea is that in proposal one you still need to have the collector in the loop for every collection of the batch um which isn't strictly useful because a prior it's doing this deterministic sorry the decision of which subtree to Traverse based on the current collections prefix shares ought to be deterministic so um adding adding The Collector into that Loop doesn't actually gain you anything except like more more latency more waiting on messages um as we were just discussing proposal 2 is less generic it is less General across BFS um so we would need to introduce some notion at the DAP layer of this Heavy Hitters collection mode and The Heavy Hitters semantics um to the aggregators um finally um proposal number two would"
  },
  {
    "startTime": "00:40:02",
    "text": "would complicate this optimization that we've been sort of hand waving around for a long time now which is level skipping so the short version of this is like um Heavy Hitters is about aggregating over bit strings well let's suppose that you have specific domain knowledge right that like this the bit strings that you're aggregating over are actually asky text so then instead of doing one collection per bit you might say do one per asky character um um and Skip like eight levels at a time so that's possible in proposal one because um we leave the uh the choice of the next set of uh of candidate prefixes to The Collector and it's not specified in the protocol which means that the collector could you know apply this like uh apply these application specific heuristics to choose these things if we wanted to do something like level skipping in proposal number two then the aggregators need to know what the strategy is um and that has to be sort of agreed on uh somehow okay so that's the rundown of proposals one and two um and the questions here so I believe yeah um last thing I'll note before before I ask chair some stuff is we have a couple things here to some more reading material um that give like a lot more detail on all this stuff um and yeah with that chairs uh how do you think we should proceed with these U with these questions being very aware of the attendance today and how many of the people in that on that list have been involved in this design team um what is your proposed favorite of these answers so okay I should be careful I could speak for myself here sorry Benjamin do you want to say something first uh no I'm uh no I'm just coming on to support the chairs okay so um I can speak I think like for myself personally and I think"
  },
  {
    "startTime": "00:42:00",
    "text": "to some extent for like my colleagues um at isrg we've been talking about this a bunch um my in my opinion what we should do is uh proposal zero which is to say don't do Heavy Hitters in dab but um Heavy Hitters is in the charter for PPM and I I strongly believe PPM should still do it uh but I don't think it should be done in the DAP draft so um I want to lay out a few Arguments for this the first is uh again as as as Chris leted us through the the prio stuff or you know the the stuff around like running VFS that have that shade which includes Pine and mastic in um attribute based collection mode um I think that stuff is much more mature we have a much better handle on how it works what the security properties are and how to run it um versus uh Heavy Hitters which regardless of whether we do that with mastic or poplet or something else there's some open questions and particularly the differential privacy stuff I think needs a lot more attention and thought so I think that the um so for that reason I think we should split those two things out like the best path that we have toward say getting an adap RFC that uh tells us how to run prio I think is to focus in on the stuff that we have like high confidence in how to run safely um yeah ultimately the other thing is uh We've I think the question of like how what do we do about Heavy Hitters at all regardless of in which document I lean towards proposal to uh because I think it has Better Properties in terms of minimal information leakage but here's the thing what we've learned from like thinking about this problem for a while is that um you really end up wanting a protocol that is aware of Heavy Hitters but is specific to Heavy Hitters because there are semantics uh that aren't just sort of generic across Al vaps they are particular type of Heavy Hitters problem so I think that the the way that we get to like documents that do a good job of solving like the prio case and The Heavy Hitters case is to is to write two documents in PPM the Each of which is specialized to those use cases um I"
  },
  {
    "startTime": "00:44:02",
    "text": "think we end up with two strongly focused and higher quality documents as opposed to one document that's trying to be this meta protocol and has like these awkward and overly generic affordances gotcha um I'm gonna go ahead and take folks from the cube and hear whatever viewpoints we hear and then then I'll have some questions for the for the rest of the group Chris yeah um I'm I'm supportive of proposal zero as well um I think um I think there's there's two things I want to warn people about based on my experience and what I know the first is that um um I don't think I don't think that we're going to do much better than idpf plus differential privacy so um I would not uh I would I would just not hold out hope for like some magical like black box that comes and solves our problem um so I think the the future draft I I think there yeah I think the future draft would um specify the you know whatever whatever VF we go with plus um the the differential privacy bits and I would really like to see like End to-end security analysis for that um the second point is I've implemented proposal one and um I want to just point out that there's not really a natural way of making a Dap implementation purely agnostic to the vdf so for efficiency's sake there's going to be information there's going to be details of the vdf that bleed into your implementation of dap if you're doing something like Heavy Hitters so um I think um yeah so like from that perspective I think proposal one is not really uh is not really that much there's not much"
  },
  {
    "startTime": "00:46:00",
    "text": "of an advantage of of one over two so I think you know if we do have this the separate draft I also think it should be number two thanks Chris um Shan hey uh I have a question regarding proposal two maybe related to proposal one as well um I apologize if the Google Doc already explained this um but why in proposal two we have to have these multiple RS between aggregators coming we not communicate the uh the levels we want to Traverse as a fixed parameter when we configure the task and when we collect only reveal the have eers U for those prophecies and above that threshold so the reason that you need multiple rounds so okay for one thing when you're doing Heavy Hitters you have to have multiple rounds regardless of whether we're doing proposal one or two um like that's because we have to Traverse every level of the tree uh and then we don't know at any given level of the tree we don't know which path we want to take down the tree until we have evaluated the prefix at some level sorry let me try to rephrase that more clearly we don't know what uh what nodes we're going to Traverse at level n plus one until we've evaluated the prefixed counts at level n um that's true yeah again so so somebody some actor in The protocol has to evaluate what are the prefix accounts we discovered at level n and then choose the next set of candidate prefixes um and you can't do that ahead of time because you don't know yet what the prefix tree is um Shan does that go to your question yeah I think so okay so again I think the key point there is whether or not you need multiple rounds is sort of orthogonal to whether it's uh whether it's proposal one or two but in two we have a law of message passing between the aggregators that the"
  },
  {
    "startTime": "00:48:01",
    "text": "collector never sees yeah that's correct although for what it's worth um I would my intuition is the proposal two should be strictly less message passing than proposal one consider that in proposal one you would have at each level of the tree The Collector would initiate a collection like send the request of a leader which in turn would cause the leader to like request an aggregate share from the helper so there's still leader helper communication there right um all we're doing really proposal to is taking that like collector to leader Edge out of the graph um arando hey hello um so I just want to make a comment about like the da uh draft uh it seems to me that this was like an good effort to generalize the protocol but actually the the the actual instantiations are like uh you know like uh few of them like I I don't think it's it's actually ER fulfilling the the the right abstraction in collaboration with with Babs so yeah like I I think I'm I'm think I'm um I agree with what Tim says about like going with a specific documents about one specific uh proposals let's say in this case heavy heaters or or one specific algorithm for doing for doing that and what else yeah and yeah I I I would be I would be more like leaning towards having like a dedicated document for for that and in in the future it is needed like some other better algorithm or better construction that can be"
  },
  {
    "startTime": "00:50:04",
    "text": "Revisited thank you amondo um Tim I saw you Q did you want to or shall I um I guess I just wanted to double down on Armando agreeing with me uh but sorry specifically that uh I I wanted to add that I think the vaa abstraction remains useful in that um I think there's been some interesting work that's been done like say proofs that have been written around vaps as an abstraction uh but yeah the path going forward I think would be to write documents in PPM Each of which Each of which maybe spells out like how to run a particular vaf or how to use some like category of vafs um to solve a particular problem yeah rather than a Dap level document sorry rather than a PPM level document that tries to be able to run any vaf conceivable but the point I want to make though is that like I think it was still useful to have this this sort of object of the vaf as distinct from the higher level protocols um that run them all right so the obvious question does anyone to call descent in this and think that these that we should not take Heavy Hitters out of the base adap draft Simon's in the queue go ahead Simon uh not sure if this working can you hear me um I'm not dissenting specifically but I think there is uh some issue that we should discuss about which parts of the current dep we are throwing away if we are throwing out heavy hits because some of the protocol parts will be unnecessary and depending on whichever"
  },
  {
    "startTime": "00:52:02",
    "text": "draft or new proposal this comes back in people might still want to do similar things in their implementations so I'm not an implementer so it doesn't specifically concern me but I think we should discuss a little bit if we keep options for doing multiple rounds or if that just doesn't matter um but I'm not descending with Z go ahead Chris we're we're small enough we can just have a discussion and then I can interrupt as we need to okay um so Simon I think um um so so just so everyone's on the same page by by multiple rounds he means rounds of during the aggregation subprotocol where you're actually doing the multi-party computation step to actually do the validation so right now we allow any number of steps um concretely popler requires two prio requires one um mastic and pine both require one so if we decide we're never going to implement popler then we can simplify the protocol there um so I think um as an editor of that draft I think it's as an editor of both you know pop and mastic it's um uh I I don't I'm not confident that uh mastic is is even going to get adopted um given uh CFR G's lack of interest in taking on new work in the space which gets to a broader like a broader you know conversation we can have about what things should PPM do versus cfrg but regardless I think that we have to um uh I think I I would not I would not I would not make that simplification yet um just because we don't know what the future of popler"
  },
  {
    "startTime": "00:54:02",
    "text": "is so the question Simon raay was what what would we delete out of dap um if we go proposal zero and it's not that much text really but rather I think I think we end up deleting like disproportionately complicated stuff so in particular we now get to State very clearly a batch gets collected once just like one time when you know when you're running something like prio or or um or pine um and yeah there's like a there's a good amount of messy language around um how do you know whether to allow more Collections and so on that just like goes away um uh I'd also note that we have more details in the Google Doc that is linked at the end of this slide deck about like how each proposal would resolve uh some number of open issues so there's there there's a more fleshed out picture of what this actually manifests at as protocol changes in there um David I saw you peek into the queue I was gonna say what you did that's good go ahead yeah uh can you hear me okay yeah I was just going to say what Tim did about eliminating multiple collections so that was it thanks Simon how is this doing it answering your questions yeah I think sounds good to me I mean if we if we don't take back too many things that that makes me even happier I think the minimal change is kind of spelled out we just we just disallow multiple collections there's that parameter in the task config that we just remove um and um and probably not mentioned popler in the in in the base spec unless we think there's a use case that it actually solves that we can support um assuming and we we'll come back to how we're going to Sol this that we choose the option zero um let's go to your second question"
  },
  {
    "startTime": "00:56:03",
    "text": "about path forward for Heavy Hitters I was hearing you proposing I was I was hearing both Tim and Chris being fans of option two where the aggregators compute the prefix Tree on their own um same question as before is there descent on that Choice Simon I think if we're treating this as a research problem and I think that's a pretty good stance to take then it makes sense to also consider doing this with multi- party computation maybe with more parties whatever but I think it's uh not standards writing work but something yeah to to be worked down first yeah I want to so I think Simon's perspective there is valuable in that a big part of the motivation to me for say um doing proposal zero or rather to focus dap on vaps that have the pr shape is that I think that that's our best path to getting uh an RFC that like lets us run prio so similarly I think there's a PR we might apply a certain pragmatic lens to what we want to do about Heavy Hitters in the sense that um yeah there's probably is like a few different exciting you know at least novel compositions of cryptography that we can do here right like um I think what Chris spelled out would be like an honest majority three-party uh computation of the prefix counts or like I have this very half big notion that you could do some kind of a shuffle so that the aggregators don't know like what the nodes are but they're working on a given level stuff like that um there's probably a few different interesting things we could do there the question is would it be worth our while to invest in like a less ambitious Heavy Hitters say one that uses like basically"
  },
  {
    "startTime": "00:58:00",
    "text": "say one that does proposal 2 and has limitations that we know about and then write a document based on that um so that again we can like produce something that is a stab towards Heavy Hitters that then further work can iterate on or would the working group want to like wait for some interesting new piece of research to to come about something concrete um maybe not as generalized INX yeah like a concrete solution to the Heavy Hitters problem that has you know but here are the caveats here are the problems with it um like is that worth the while of a of a of an ITF working group if we don't have like if we don't necessarily have confidence that people will go deploy it but is it useful as a point of reference against which like future research and future work um can compare itself I'm most interested if people want to actually use it yeah I was just going to agree with I was just going to say exactly what you just said I think it um a standard is not supposed to exist as a straw man it's it's supposed to be for something that people intend to use so that's that's the of it yeah there was a part where we were talking about taking things out and and how people might want to use them I was thinking we could have an appendix that says if you're thinking about modifying this here's directions we go but um I don't really want to be writing rfcs for things that people are unlikely to use um yeah just along those lines something a comment that Simon made in the chat like Star I mean like star star was and is an example of a protocol that uh a group of people wanted to use and were trying to get other people to to get behind and this working group kind of uh you know uh decided not to take it out so so do we have who wants to work on the Heavy Hitters"
  },
  {
    "startTime": "01:00:00",
    "text": "stuff actively um I think we do I'll say I do I'm pretty sure my employer does but I can say I do I think uh my employer would like this as well like we'd like we have definite use cases for this um I think the approach that we would take is prototyping and deploying for a real use case before we even brought it to ITF um this experience you know I think we've learned a lot from like what we can do um so um so but but yeah I think uh I think this is probably a while out for us shim yeah I was going to say the same thing I think we are interested in the use case we just not sure the exact method we're going to use great um yeah but I think we could make a lot of pro like experimenting with this and our respective organizations maybe even like throwing individual drafts up on data trackers as needed and then um it probably would be wise to like not adopt something until we have much more confidence about like yeah this is a viable solution that people would actually want to ship great so so given again the attendance today I think we need to take these the consensus call to the list which I'm taking as we are are pulling the heavy hitter support in particularly the multi-round communication support out of damp to simplify the draft we are entertaining work on Heavy Hitters knowing that that's going to be back at the ID phase and we want it to be deployed along the way um I'm not stating this well but I I I think that's what we're going to propose"
  },
  {
    "startTime": "01:02:02",
    "text": "um Ben that sound right to you sorry just ask what is ID fees sorry um we're going to be seeing some internet drafts including some individual drafts rather than working group drafts oh I see thanks uh yeah uh this has been a great discussion uh thanks everybody for um for being willing to entertain a big change here um I think uh one of the interesting questions that comes up uh to me is is Heavy Hitters really a uh an early research problem can you repeat that you cut out for a second Benjamin Ben I I think he is cutting out what I heard is is Heavy Hitters really an early stage research problem like is it ready for the group and then is completely Frozen so and my own intuition is that it's not that early stage and that is ready for this level of we we should be discussing individual drafts in this working group is my intuition but yeah so I I think um I think it has one dependency that we we have not resolved which is we need to spell out um uh like a lause mechanism for differential privacy that people want to use so um this there's the DP draft that was presented a couple of itfs in a row um and um I'm not sure why but there seems to be I mean generally speaking there seems to be people don't seem to want to adopt new work in this"
  },
  {
    "startTime": "01:04:01",
    "text": "working group um so but but I'm I'm just suggesting that like I I think all that's required to kind of standardize idpf for this problem uh is um is is a common understanding of how to apply differential privacy to this particular protocol um and I I people talk about the possibility of of of of using fancier or different crypto uh that hasn't panned out yet it's still a possibility but I personally don't think it's worth holding out for I think we know what the I think we know of a workable solution and um uh it's just about having the will to implement that solution great um Tim and Chris are you happy writing that proposed change up for the list given how much work you've done here or do you want us the chairs to do it so you're talking about summarizing like what we would do to DAP and what the shape of this new Heavy Hitters thing going to be yeah absolutely great um if you we put together a PR for the spec too um so I think we've resolved the main topic on our agenda um Jun asked for time to talk about Pine I propose that we all take a 5 minute break come back here at 10 minutes after the hour and do that any objections great I'll see yall in 5 minutes we'll be talking about Pine"
  },
  {
    "startTime": "01:06:24",
    "text": "e e e e"
  },
  {
    "startTime": "01:08:24",
    "text": "e e e e"
  },
  {
    "startTime": "01:10:06",
    "text": "welcome back I hope everyone enjoyed your break I love being able to take a break in the middle of our working group meetings um Jun wanted to talk about Pine um Jun do you know how to share slides uh sure yeah uh Sam is there is there any way you can present the cfrg slides or yes or or you can so if you at the bottom of your screen the fourth button to share slides okay and I uploaded those slides for you oh shoot try it again pull up the slides here let me put them up and see if I can give you control all right I have them up I can run through them okay uh let me try requesting control there you go oh cool okay that works okay uh hi everyone uh today I'm going to give a quick update of the new vaf called uh private inexpensive Norm enforcement uh or in Short Pine uh uh this is a new VAV to support uh Federated machine"
  },
  {
    "startTime": "01:12:01",
    "text": "learning use cases uh which is a news case for the PPM working group uh this was also presented in the cfrg working group uh but I want to bring the uh this VAV into this uh this uh audience as well to to make sure uh people people know this this is an option to basically uh support Federate machine learning use case On Da uh this is a joint with with quis patent uh I'm going to briefly talk about the use case first here I have a list of devices on the left each of them trains a machine learning model uh denot denoted as F here with this local data each client then generates a model update Vector which we also called gradient and to improve the model we have this Central server on the right that collects the gradients from the client the server then agregates the gradients and applies the agregation to the model and we get an improved model called f-prime and then we send the new model back to the clients to repeat this process in the next iteration so it's great this process doesn't require us to collect the client data directly but the model gradient still leaks very very sensitive information about the client so we want to see if we can do better in terms of privacy here one solution is to leverage the vda framework to remove this trust on the central server which uh this working group is is heavily involved on uh VF is basically a secure multiparty computation protocol that can agregate client measurements with privacy and robustness and the base V address specifically provides a spec for PR 3 which uses the idea of a fully linear"
  },
  {
    "startTime": "01:14:01",
    "text": "proof or in short flp uh which is basically a distributed zero knowledge proof system that can verify certain properties in the client measurements this ensures we don't learn anything uh about each individual client measurement but still able to ensure the robustness of the system therefore have proposed this new VF called Pine which is based on a recent paper with guy rothblum Iran omy and kuna Tower this new VF is used to compute an arrogation of the client gradients expressed as vectors of real numbers and to ensure the robustness of the agregation we require each client gradient to have a bounded al2 Norm which is defined as taking the square root of the sum of squares of all gradient entries it uses a similar idea of a flp like Pro 3 and if we were to simulate this in the in the vaf framework um here I basically replace the central server with two arrogators one leader and one Helper and The Collector collecting the final arrogation from the two arrogators what client does is it encodes its gradient into a vector of field element and generates a proof to prove its Alto Norm is bounded and then SEC shares it's encoded gradient and proof and sends them to the two arrogators the arrogators then compute some results over their respective secret shares and exchange the results to determine the validity of each client gradient basically to verify the the alto Norm is bounded each arator then produces a share of the gradient ARG uh gradient arrogation and sends it to the"
  },
  {
    "startTime": "01:16:01",
    "text": "collector and finally The Collector combines the two argate shares and applies that to the model to uh to obtain a new model f-prime and and we repeat this process by sending the new model back to the client so why do we need a new VAB to support this why can we use pro3 directly uh that's because because uh Computing a squared Alto Norm in a field element operations can overflow the field modulus uh here I have given a simple example uh suppose your Alon Norm bound is 10 the field modulus is 23 and and I have a clearly invalid client gradient that's uh 990 and7 and taking the square out to Norm of this grer module Q is is only six which can cause um the arator to incorrectly accept this client gradient as a result we'll corrupt our machine learning model and we certainly do not want that so the challenge here is how to uh prevent or detect this wraparound Effect one way to address this problem uh is to make sure each gradient entry is sufficiently small for example we can represent each entry with bits uh and use a VD like Pro 3 but a solution like this will incur an extremely high communication cost that's infeasible for high dimension and the cost is approximately in the order of Dimension times the number of fractional bits you want to keep in your U gradient so Pine proposes this Innovative idea of checking through this wraparound effect the hollow idea is we'll generate a random Vector of negative one one and zero and compute a doc product of the random vector and the gradient and if the norm computation"
  },
  {
    "startTime": "01:18:01",
    "text": "overflows the field modulus this St product is likely to be large and in our paper we Pro this check can correctly detect this wraparound effect with probability at half so an invalid grading may still be accepted half of the time and in in order to achieve the desired soundness error we basically repeat this check many times and because of this the need to sample this uh random Vector this is incompatible with Pro 3 because we need both the clients and aggregators to independently derive this random Vector on their own uh here are some quick performance comparisons it turns out with a dimension of your gradient to be 100K Pine achieve Pine can achieve a 15 times smaller communic cost between the client and aggregators compared to a solution with Pro through uh finally some next steps for this draft uh the current status is we've complete the core design work uh we have uh implemented some reference code and generated test vectors for it uh the core component will be supported by the security proofs in the paper uh uh we plan to complete the draft text next and finalize some parameters to optimize for communication cost and we in the meantime incorporate feedback from implementers uh there wasn't much interest adopting this draft in cfrg but I think people in the PPM working group will probably be more interested in this draft because this basically enables a new use case of Federal learning on on app and VF uh and shouldn't require any changes to the core protocol and it can"
  },
  {
    "startTime": "01:20:01",
    "text": "and it can improve privacy and robustness for training machine learning models and um uh I think that's it from my presentation uh and I I want to see if people in this uh working group is interest interested to working on this um so w i welcome questions and feedback uh thank you what was the reception to this work in cfrg I admit that I didn't watch it uh I think I think people just wasn't very familiar of uh the the the VF in wasn't familiar about the feda draft so they they W sure about um like the the scope of this and the impact of this I think that's generally my my feeling after presenting this draft thanks okay so first off my impression from having attended the cfrg session in question sorry cat um yes so the issue I think at cfrg is that it's not clear that the members of cfrg have like the the necessary expertise or appetite to look at I think MPC and to some extent there's some there's some measure reservations around machine learning um one of the things that we did hear at cfrg clearly was that before they would take on yeah it was Pine in particular um they wanted a clear statement from an ETF working group PPM specifically um you know an affirmation that we're interested in Stu in this stuff like sufficiently interested that uh cfrg should like take on um this work um that being said my read on this is that I it's not clear to me that there is it's not clear to me that cfrg is a great venue to do NPC work um it doesn't seem"
  },
  {
    "startTime": "01:22:01",
    "text": "like what they've wound up specializing on doing um so so sorry coming back to the question like what to do with pine so first off I support this work I think this is interesting stuff um so far as I know it fit it should fit neatly into dap um because it has like the same sort of shape as pr3 in the ways that we've been discussing and I think that the machine learning use case like is quite compelling there is a a fair amount of Industry interest in uh in doing this stuff privately so I think this work should be done and then past that I just I'm not aware of a better place at the IE or IR TF to do this work um I like I say I'm not optimistic about cfrg doing it and then I don't know of a better home for it than PPM mind you that comes with some caveats like I'm a little concerned about PPM becoming the deao home for certain topics um like say differential privacy or certainly machine learning uh but as far as like looking at this as a NPC aggregation problem this does seem like a reasonable venue in which toh take on this work hey J uh could you please uh talk more about like uh the choice of the modulus Prime so you mentioned that that that operation can wrap outs but what if you actually choose a larger Prime for that for doing competitions you mean the prime number in the uh field element yeah so I think the choice of field element uh field field size impacts robustness uh we definitely want to uh it also affects the communication cost so I think we want to strike a balance between between achieving a uh reasonable robustness is also uh make sure the communication cost between the"
  },
  {
    "startTime": "01:24:01",
    "text": "client and and the aggregators to not uh just blow up so um I think it it shouldn't impact uh your model accuracy the agregation of the model gradients should be exactly the same if as as if you were to you know directly compute uh uh a summation of the client gradients directly uh outside of VA for example yeah it would be good to have like an an overview of how does the parameters are involve it but for or at least for communication because Robert N I think is uh it's easier to to show uh yeah I think I I support like this this work and um yeah I think it's good to have it here at least BB sure I I think uh one of the few uh remaining work is to try to finalize parameters to uh minimize communication cost also to and the security analysis to achieve the desired uh robust you want to achieve so I I wanted to just add to jun's original answer to armondo's question so picking a larger modulus doesn't really help with wraparound uh effects because you can just if you want to pick a gradient that uh wraps around the field modulus you just need to pick bigger entries if you have a bigger field so the the the key thing the key thing that's needed is either you need to encode your inputs in a way such that wraparounds just don't happen and in fact there's like a way we can do this with prio uh but but um does uh Pine just does a different thing and and and and lets you do so in a much more communication efficient way"
  },
  {
    "startTime": "01:26:11",
    "text": "I'm guessing that this should get more air on the list Ben Ben you have words uh yeah so I I it's clear that there's a lot of interest in this topic um I have a question for for all these folks you know we've heard a lot of questions about um where work gets done and I think there's a little bit of a football here where the ETF tries to stay away from Cutting Edge computer science research and cfrg tries to stay away from uh topics that are outside of its focus of expertise and uh and a lot of these uh recent PPM items seem to be falling through the cracks uh I wonder if as Tim maybe was hinting at earlier uh whether it's time for a a multi-party computation irtf group that could be a a clear official partner for PPM and a place for for these kinds of discussions that is not something that this we can decide in this group I wonder what people think about about that or about other ways to structure this work or you know do you think that we should really just try to do it all in BPM I mean so on the one hand like I feel like creating a working group to to sort of generalize what we're doing uh it only really works if the the experts are showing up and I think the foot I"
  },
  {
    "startTime": "01:28:02",
    "text": "you identify the right problem we have a football problem but like the we don't have the community really developed around this protocol like this this this space of cryptography at least within the ietf um something that we've tried to tried to do over the over the over the last couple years with mix success I'd say um that said I mean um some folks have heard Rumblings about what uh meta wants to do with IPA and sort of the generalization of that framework and there was a there was a side meeting at the last ITF about um like kind of the future of that um it it's sort of like you know like PPM is a is is a small group we've we have like at various points like not necessarily like explicitly decided to not do new things but there hasn't been the energy here to take on new things and we're really only taking a bite out of a very complicated problem so there's NPC but the other part of this the other very important part of this is differential privacy um differential privacy is going to be needed in any application of D pretty much because it's what you mean when you say is the protocol secure you're asking is it different private so um so I would say like I'm I'm I'm strongly against considering like I I don't think I don't think a bu for uh for MPC stuff is I think I think it's premature um I just don't think uh there is a critical mass yet um um yeah anyway that's my that's my my my view this work in particular like I think this is really really this is the exactly the kind of thing that dap we want to be able to enable with dap"
  },
  {
    "startTime": "01:30:00",
    "text": "there's there someone some folks write this paper it fits fairly neatly into our framework with a few changes uh and we have with dap SL Pine I think we have like a really nice solution to this problem maybe you can do better uh the the the the solution is maybe not complete there's there's definitely still work to do um but like this is it's already really close to engineering um if you believe the proofs that exist uh you know we're we've you know we're also on this slide we're pointing out to some some some gaps that need to be filled but um yeah anyway that's that's kind of a rant but I think I think Pine's really cool and I I think uh I think uh we should all uh want to to support this kind of this this kind of design work yeah so I have to I agree with everything Chris said in particular you know one of the questions before we would say go start a research group whose focus is let's say MPC is you know who would be participating in that research group and I think what we would find in practice is just what we've observed with um the work on vaf at cfrg and that it's exactly the same people who come to PPM so um so I think yeah if we were to start today this new research group whose job is NPC then we would find it's exactly him contributors would come to PPM and I think we would just incur like the overhead of management of an extra of an extra group I think to little gain um because the work that we're talking about here say with pine and even other stuff to do with NPC I think all of it like is perfectly well motivated by this working groups Charter because there's a clear line to like enabling you know the measurement measurement work that we are signed up to do that said like I'm broadly very bullish on NPC I think there's a ton of exciting stuff that could be done to make the internet you know safer and more private um using NPC and I'm hopeful that in the coming years like perhaps many years there's going to be a"
  },
  {
    "startTime": "01:32:00",
    "text": "lot of like very exciting NPC systems that'll get uh drafted up through ITF now if we find in the future that like new ITF working groups are coming to PPM and saying like hey we want to site this document that you're writing or hey we want we would like you to take on this piece of work because it'll help us then that's a point at which I think it starts to make sense to try to like factor out some of these NPC Primitives into something like a new research group but for the time being what we have is work um that directly supports PPM and would be done by people who are already at PPM so it's it's it makes sense to me to continue to do this work here uh yeah yeah I have another question about Pine like uh what is something that you cannot do with pine like like in in in in this area of uh Federated learning what are the limitations uh I think I think any arations that you want to enforce uh some Al to Norm on your on your client measurement is is suitable for Pine we we sort of use uh Federate learning to motivate Pine in in this particular presentation but uh I think uh anything basically uh you want is basically it's specialized for Al toor verification so so um there there uh there are limits that we impose on not not not just not because of the Paradigm necessarily just because of the"
  },
  {
    "startTime": "01:34:01",
    "text": "overhead so you can't scale this to like arbitrary Dimension at least without some like protocol level like work um so like you're not going to use this for an llm in the next 10 years um at least not you know chat GPT size you you mean is not possible or that the scale uh scale it's not going to scale to that application because we're you know we're at like like like tens of billions of parameters I don't you know I I'm not an ml person but that's my understanding is that like the dimension you're targeting is like tens of billions of of parameters in here um yeah I mean not know not knowing much about just having read the paper like my imagination is that we get to like you know I don't I don't even want to put a number to it but I don't think this is for chat GPT yeah roughly you can imagine the communication cost between client and aggregator is roughly you know Dimension times the the number of bytes that you choose for your field size uh the proof is uh is relatively a a small percentage of it so uh but yeah ultimately it's it's bounded by the dimens you choose for your application uh yeah so so there so there is some limitation but I think like Dimension is something I think every uh pretty much every application every RAV will have to deal with so yeah okay thank you"
  },
  {
    "startTime": "01:36:01",
    "text": "is this in Charter for us to take on I think arguably it is I would argue that it is okay uh it the charter doesn't say machine learning the charter says is more generic but um I think the it is is to increase privacy when in in these in these data collection applications and I think ml is absolutely that I'm I'm not worried about the ml part I'm wondering about the having the vda be here CG so the draft says we will like we will use cfrg to work out uh crypto details as needed all right that suggests that this is a candidate for an adoption call um junior thank you very much thank you for letting me present absolutely not to not to create chaos here with with the the multiple chairs chiming in but the I so so I have the charter here and it says um the PPM protocols will use cryptographic algorithms defined by cfrg so uh I think you know in the charter as written you know we wouldn't be able to move forward here with without certainly you know a substantial cfrg involvement in the protocol definition or you know some some very uh some some substantial reinterpretation of the charter but P itself is compatible with vde interface is that not considered algorithm from cfrg I VF I don't think can be considered an algorithm"
  },
  {
    "startTime": "01:38:03",
    "text": "uh this uh the charter says um cfrg takes care of crypto and this is a crypto bit so uh I think I think we would want to change the charter um our new ad Deb was not able to join us today um but Ben and I can have that conversation with her um going back to the original presentation I want to thank um Tim Chris and everybody who was on that design team um for the good write up um and thank you all for the good discussions today um do we have other topics for today yes you want look your logo well now you spoil the surprise but yes um what do we got here there we go okay so yeah Martin Thompson actually I don't know Chris if you want to speak to this I think you spoke to Martin about it no no no is literally just everything that's in the slack thread is is what I know fair enough um oh there's a slack thread sorry uh yeah so Martin Thompson apparently I learned today uh whipped up this PP this uh pitch for PPM logo for us there it is um uh so yeah it's you know a plus split in two uh to represent the two aggregators so I don't know if we need an adoption call for Logos I think it's neat I've been reserving a little bit of space on the back of my laptop for a PPM sticker now I'm thinking about ITF IPR rules and owning logos and blah blah blah but that's not really what you're asking is"
  },
  {
    "startTime": "01:40:04",
    "text": "it I mean I'm not not asking about that I don't remember the details it's not you know when I first saw I'm like that makes some sense but it's not quite intuitive I don't care so Mar the way Martin described it is it's it's simultaneously representative of uh the kind of the overall Paradigm of secret sharing but also the limitations of dap so this basically is uh dap is just for aggregation you're just adding things up so that's why it's a plus doesn't sound like you're going to get more of a uh cons is here today yeah let's let's uh let's move on all right um so I'll not thatf uh logos do not require working group consensus so uh the chairs will consult on logo questions if you have comments please bring them to the chat all right so to summarize from today Chris and Tim are going to write up the proposal um for the Heavy Hitters path which includes removing simplifying the that protocol removing the multi-round stuff and heavy hitter support and breaking out heavy hitter support into new drafts um and Ben and I need to have a conversation with Deb about whether we can take on Pine and how to recharter to do that yes Chris so um so that write up exists it's in the Google Doc um we could say more about what a what the what the separate document would look like but it would basically be proposal too yes what we can is like create a like a short a couple of sentences on each and"
  },
  {
    "startTime": "01:42:02",
    "text": "just just so you you have something to do a consensus call with is that what you like to see what I would like to see is based on the discussion at this interim the editors are proposing this comment and where this this is no more than 20 lines got it and can include the link out to that and should include the link out to that document that explains it in more depth thank you yep no and thanks everybody today uh appreciate the attention in the discussion I've uh I've tried to take notes on all the interesting conversations here if you spoke today please double check my transcriptions and summaries uh in the hedg do and then we'll put out some minutes once everybody's had a day or two to make any corrections there and to get to that document um in Meet Echo at the top of the page there's the note taking tool icon that will get you to that document"
  },
  {
    "startTime": "01:44:17",
    "text": "s w is Shan wing and Sam W is me right feel free to correct any uh names that I maybe have wrong or have rendered Ambiguously I don't think me I didn't ask a question what do you think we should do I'm sorry I missed that I'm going to end the meeting or oh I guess I can't in we just have to leave the room for"
  },
  {
    "startTime": "01:46:07",
    "text": "[Music] [Music] [Music] for"
  }
]
