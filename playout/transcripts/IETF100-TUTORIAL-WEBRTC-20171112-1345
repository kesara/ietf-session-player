[
  {
    "startTime": "00:00:04",
    "text": "part of the education team and part of the education work we have the tutorials at the beginning of the week just before this we have the TLS tutorial and after the excitement of TLS the excitement of web RTC we found the co-chairs of the web RTC working group ready to guide us through the work and also show us some of the implementations that are going on and without further ado because we already lost some time over them there you go thanks so much oh and very important if you like this or if you do not like this please fill out the survey so we can learn from you thank you oh well alright yes so I\u0027m actually not one of the chairs one of at least one of the chairs is sitting in this room and I don\u0027t want them to get any ideas about me trying to take their their spot but my name is Dan Burnett I am the first and longest running editor of the WebRTC specification in w3c and I\u0027m also co-author of the most opulent web RTC with Alan Johnston so what you\u0027re seeing here is actually condensed down from an eight-hour live training class that that we do so the slides actually have a lot of information on them and I\u0027m going to do what I always hate when other people do I\u0027m not going to tell you everything that\u0027s on the slide but I am going to tell you the important thing the important points on each slide and you can read the details later yourself if you\u0027re interested so these are not these are not the updated slides oh boy it\u0027s just today okay here we go real-time change yeah these are the old slides sorry okay so okay so the the main thing here is is okay so WebRTC is designed to do peer-to-peer communication and this diagram this WebRTC triangle is the canonical architecture diagram for WebRTC we have a we have two browsers a mobile browser on the Left a laptop browser on the right and a web server up at the top now just like with any web application your browser is going to fetch the application from the web server and then execute it what\u0027s different here with WebRTC is that the browser\u0027s are able to set up a media communication channel directly between the two browsers now this is client to client okay so this is actually a breaking of the web model with what "
  },
  {
    "startTime": "00:03:04",
    "text": "WebRTC is letting us do now in order to do this to set it up there\u0027s actually there needs to be some sort of communication between the two browsers before the media is set up and for that we use any kind of signaling approach you want and I\u0027ll talk more about that later in this particular example I\u0027m showing this one web server being used not just to serve up the HTML pages but also to act as a relay for information between the two web browsers until they get the media connection set up now just as an example many of you may be familiar with sip and may be familiar with the SIP trapezoid which is kind of the canonical architecture there you can do that with WebRTC as well there\u0027s no requirement that you you actually use the same application in both cases there\u0027s no requirement that that you\u0027re signaling goes through the same servers and this is just showing that you can you can use multiple servers if that\u0027s something that that\u0027s important and works for you so what\u0027s really being added this is very complex picture here okay but the main piece to look at is on the lower left corner where it says browser our TC function it\u0027s that lighter blue there there are new controls that have been built into web browsers now there are new audio and video codecs the ability to negotiate these peer-to-peer connections and also echo cancellation and packet loss concealment that\u0027s pretty cool actually that this is now in web browsers support for this is actually in all the major browsers today except for Internet Explorer and we\u0027ll talk more about that later so whoever you see is cool because it uses the the web platform so you can write in HTML with JavaScript and the standards are defined in terms of JavaScript API s now we\u0027re going to talk more later about some that you don\u0027t have to use those api\u0027s particularly weber DC also provides natural versal and i will be talking about that quite a bit in a moment and of course the codecs that I mentioned one of the nicest things is that it includes ice and that\u0027s one of the first things I\u0027m going to be talking about so okay for many of you who know networking you\u0027re gonna be just incredibly bored by this okay this is very high-level let\u0027s say we have our mobile browser and our laptop browsers and they want to communicate peer-to-peer over the Internet right media can go directly between those two it doesn\u0027t have to be relayed to you know if if if I\u0027m here and I\u0027m talking with Alex who\u0027s here in the front row the media doesn\u0027t have to go "
  },
  {
    "startTime": "00:06:04",
    "text": "all the way to the US and back for example okay you can just go directly between the two of us but as we all know this is not really what the internet looks like the Internet actually looks like this right so the mobile browser on the left and the laptop browser down in the center each of these is behind an access point a Wi-Fi access point of some sort a home coffee shop whatever which has its own network address translation so the problem with that is that so I\u0027m showing here the home case and with the mobile browser we\u0027ve got two devices here with different IP addresses but those are their pride internal IP addresses as far as anyone else on the internet is concerned their IP address is the one you see at the top there the 203 0.11 3.4 and not the 192 168 addresses so stun is a protocol defined here in the IETF for basically being able to find out what your public IP addresses you you send a query out to a stun server and the response that comes back tells you what IP address it sees for you again that\u0027s useful because these brought these browsers are going to have to communicate with each other so they need an address that they can route to now there\u0027s another protocol that\u0027s related when you\u0027re talking about ice and that\u0027s turn this is essentially a media relay so this is the case where you\u0027ve decided oops we actually do need to relay the media somewhere hopefully not all the way to the US from here but sometimes things just don\u0027t work peer-to-peer the Nats and firewalls are just too restrictive for that to work so the thing that\u0027s important here is that WebRTC includes support for this okay it includes support for Ice which knows how to use stun and turn to find the optimal path if one exists so WebRTC and this is when I juggle the slides around actually put this one first because for this this crowd I think this is probably the most interesting slide WebRTC is a very highly successful joint standards effort between the internet internet Engineering Task Force US and the world wide web consortium or w3c so IETF is responsible for all the protocols of course and many of the protocols actually virtually all of them are they already exist okay the intention was not to have to create a bunch of new stuff but we have had to create extensions in the IETF and I\u0027ll talk about a few of those later on the RTC web working group in the IETF is the one that has the its main focus on WebRTC I\u0027ve listed some others here that are that have related activities as well so "
  },
  {
    "startTime": "00:09:04",
    "text": "anyway it\u0027s it\u0027s spread throughout the ietf or at least has been for the past few years the JavaScript API is are developed in w3c and there are two groups there the WebRTC working group and the media capture task force so if you hear RTC web that\u0027s IETF if you hear WebRTC that\u0027s in w3c and in general when you hear people refer to WebRTC in general they\u0027re referring to the JavaScript API is defined in w3c although what IETF does is rather important here as well just briefly on protocols I\u0027m not going to go through these in detail these are all the protocols that end up getting used at some point and in some way with WebRTC I do want to call out a couple of them let\u0027s say I wonder if this will give me a pointer yes but you can\u0027t agree you can\u0027t see it very well the main thing I want to point out is that media is sent using SRTP and that actually sits here basically uses using DTLS for UDP packets the data channel which I\u0027ll talk more about later uses SCTP and DTLS and UDP these are the ice related protocols like I mentioned before and STP we\u0027ll talk about it a little bit more this is used to describe exactly how the media is supposed to be encoded as it flows from one browser to another so what this is it\u0027s really interesting when I talk about WebRTC it turns out that in every crowd typically people either are communications people or they\u0027re web people and it\u0027s tricky to create a tutorial for both because you actually need to understand some of both so I\u0027m going to talk now about what\u0027s new for web developers there are two new API pieces again this is in w3c one is capturing of local media so without a plug-in you can get access to the camera and microphone now there are some extra specs that define how to get access to what\u0027s playing in a video element and in fact your screen and window as well the another specification defines the peer connection and that\u0027s a term you\u0027re going to hear a lot the peer connection is what provides the ability to transport both media and data between two client browsers you will often hear me say media peer-to-peer it\u0027s media and data so if you hear me say that I\u0027m in most cases referring to both okay so very briefly there are several steps here this is not everything to building "
  },
  {
    "startTime": "00:12:04",
    "text": "a web RTC application these are the pieces approximately in order that a JavaScript application would have to do in order to add in WebRTC functionality okay and the first one is getting access to local media that\u0027s done using the getusermedia call and I\u0027ll show a little bit more specifics later but one of the important things here is that this is where the user is asked for permission I\u0027m not going to cover the the details on that but I will show examples later on when you call getusermedia you\u0027re given a media stream which consists of one or more media stream tracks and I\u0027ll give more details on on these later but basically you need to have local media in order to send it you need to set up a peer connection and the rtcpeerconnection API is the one that does that I\u0027ve listed here all of the things they the pure connection actually dowser provides an API surface for so a lot of what we think of as WebRTC really lives here now once you have a peer connection that doesn\u0027t mean that any media is flowing yet okay the simplest way to have media flow is to call add track this is a peer connection call and what it does is it basically tells the browser I want you to negotiate how to send media it doesn\u0027t actually cause media to be sent yet you can also create a data channel as well at this point now in order for the media to begin flowing the two browsers have to agree on a lot of details including which codecs to use maybe some bandwidth information candidate addresses all of those kinds of things and that\u0027s handled through the exchange of an offer answer and I\u0027m going to talk about this and in a bit more detail but the main thing you need to know right now is that there are functions for creating this session description the offer and answer and then functions for telling your browser what it is that you\u0027re actually using which session description one thing that I didn\u0027t show in this diagram is the signaling channel and you\u0027ll hear me I talked about this at the very beginning the signaling channel is just however you send however you communicate your low-level session description information before you have a peer connection going so it\u0027s not standardized okay this is not officially part of WebRTC but you cannot build an application without setting up some means for relaying this information from one browser to the other okay so in a little bit more detail here navigator dot media devices die getusermedia is how you get access to cameras and microphones and there is the permissions "
  },
  {
    "startTime": "00:15:05",
    "text": "check now once you have that media there\u0027s a new method that was added to video and audio elements this isn\u0027t we actually had to add this ourselves in the WebRTC specifications initially but it then was moved into HTML itself so the point here is that instead of just setting the source of a video element to be a URL you can now call you can set the source object to be your media stream directly okay so it\u0027s not going to fetch a resource that\u0027s at the that\u0027s referenced by URL instead it\u0027s going to use a local local local media stream you can also create new media streams as well so just briefly in the in the model that we have of course you can you can load content from files or whatever but what\u0027s really being added here is the ability locally to get access to the camera and microphone now interestingly the peer connection is actually both a source and a sink for media because you can send media over a peer connection in which case when you\u0027re sending it it\u0027s obviously a sink when you\u0027re receiving it it\u0027s a source so here are a couple of examples of what what the permissions prompt looks like when you call getusermedia you\u0027re going to be asked whether you\u0027re giving permission to use the camera and microphone or in the case of firefox they tend to allow you to select the you know which device you want to use but in either case once it\u0027s set up both browsers allow you to change which device you\u0027re using if you have more than one the key here is that you cannot use getusermedia without a permissions check at least once there are conditions under which the browser is allowed to save that permission I\u0027m not going to go into it because there there are some subtleties in that but it\u0027s something that\u0027s important when you\u0027re building an application to keep in mind that at least the first time someone visits your site they\u0027re going to have to agree this permission prompt okay so a media stream track which will often just call a track is a handle to one flow of one kind of media okay so audio or video a media stream is a collection of tracks and the tracks do not all have to be of the same type so you can have you know an audio to video for example one of the other things that\u0027s interesting about a media stream is that the intent one of the intents of a media stream is that the collected "
  },
  {
    "startTime": "00:18:05",
    "text": "tracks are to be kept synchronized to the best ability of the browser and if you think about it if you have a camera on a person and a microphone you do want to make sure that the audio that\u0027s coming out matches the movement of their lips okay so the browser works pretty hard to actually maintain that as much as possible it\u0027s not possible to guarantee this in all cases guarantee synchronization because if your media came from let\u0027s say several different peer connections into your current browser they may all have different clocks and it\u0027s not clear exactly how you would synchronize them but again the browser will do what it can okay so there\u0027s a lot on the peerconnection API s as I said there\u0027s a some track items which we\u0027ve seen already and there are some offer answer controls as well which I had the other slides okay so I\u0027m gonna jump around just a little bit because I had reordered these so I\u0027ll I\u0027ll come back to this in a minute I want to talk about SDP offer answer so again this is how do you get the information about which codecs to use any bandwidth information perhaps candidate addresses at which one browser can find the other browser these things all live in something called SDP we used this session description protocol just doesn\u0027t sit or approximately as in sip to describe the media session at the protocol level STP is very widely used in sip systems today the use of STP in the in the browser for WebRTC is defined in the JavaScript session establishment protocol and I had added a link to that in the other slides basically if you search for that you\u0027ll find the the drafts that describe this so with respect to s with respect to WebRTC this protocol level configuration information is just treated as a blob okay the it\u0027s not expected that the application the JavaScript application will ever have to look at it so now let\u0027s just take a look at what the application does have to do though and yes I\u0027ll come back to that other slide so here we have our two browsers in the in the center down here and what we want to do is actually set up the de pere connection and and make it so that media can flow between the two so this browser "
  },
  {
    "startTime": "00:21:06",
    "text": "here the one on the Left calls create offer and is given an SDP blob so what this browser then does is it calls set local description with that offer so what is returned here from create offer is an offer it\u0027s this STP blob so the application needs to tell the browser yes this is what I\u0027m using locally but it has to do something else to it is the job of the application to send this offer over the signaling channel to the other browser and then the other browser takes the same thing so this remote offer and look they\u0027re the same okay but from this person from the perspective of this the remote offer so this browser called set remote description she\u0027s in a disco called set remote description to tell the browser here this is the description that the other guy is using now once that\u0027s happened this side can call create answer to generate the answer for for this offer and once the right size has right side has the answer which is right here it calls set local description just like this one did over here with the offer this one\u0027s doing it with the answer and then just like before the application is responsible for sending this answer over the signaling channel to the other browser so it can call set remote description with that answer and at that point both browsers have the low-level protocol information that they need for media to begin flowing okay so I just want to mention here that there are a number of extensions that were made to SDP in order for WebRTC to work one is bundle and actually this is this is particularly interesting because as we worked on this we found that basically everybody\u0027s been wanting to do this for a long time anyway so before bundle stp only allowed you to send essentially one of your media lines one of your audio or audio or video streams to be sent over a single port and bundle allows for these to be multiplex together so that you can have one or only a small number of ports used for all of your media there\u0027s an MS ID specification that key allows for the browser to signal what media stream the flows belong to and then there\u0027s also additional information available about sources "
  },
  {
    "startTime": "00:24:10",
    "text": "so I\u0027ve mentioned signaling a couple times and I just want to point out that again you can do anything you want for this this is not standardized in any way so if you want to use WebSockets to communicate with a server if you want to Pole to a server if you want to use a sip library whatever whatever you want to do is fine so I\u0027m not gonna read through all of these you\u0027re welcome to take a look if you\u0027re interested in in what some of the options are but it\u0027s it\u0027s not standardized but you have to do it here\u0027s one example right so if we\u0027re signaling using sip we\u0027ve got some sip library in in JavaScript here and so we have the web server as usual that that the HTML application comes from but then for the signaling the library that we have is set up to contact sip proxy or registrar server just as the just as you would normally for sip the only thing that\u0027s really different here is that the sip signaling information is sent using the WebSocket protocol and this is not a new RFC that\u0027s one of the other things in there in the update this has been around for a while now but this RFC defines sip Transport over WebSockets so this is what you would do if you have an existing sip infrastructure and you just want to use the WebRTC front-end essentially there are a variety of libraries that you can use to do that okay briefly for audio the browsers are required to implement both opus and g.711 opus was developed here in the IETF it\u0027s a great codec sounds wonderful g.711 works that\u0027s what I\u0027ll say about it the browser is also expected to support DTMF which are basically the you know the events you get when you press the buttons on it on a keypad for video the requirements are h.264 and vp8 of course you can use anything else right these are the mandatory to implement ones and in fact people are already using 265 and vp9 for video opus is still the state of the art for for audio so just briefly I want to talk about the the data channel again it uses SCTP basically the the data channel we wanted to make sure that we had congestion and flow control for an arbitrary data channel and so that\u0027s what we use SCTP on top of DTLS and UDP i understand that some of the design of quick or at least some of the motivation for how quick was designed came from experience with building the data channel in WebRTC so from a JavaScript "
  },
  {
    "startTime": "00:27:17",
    "text": "standpoint it\u0027s one of the peerconnection API is you call create data channel and what you get back is something that has a send method and an on message you can set an on-message handler by default data channels are bi-directional but you can change that and you can send any any string or any of the various array buffer types so very briefly about security there\u0027s a lot that could be said about this your media is always encrypted in WebRTC we actually mandate the use of SRTP this was a contentious point but the world is used to it now so just just expect that and this I actually got rid of these slides because they\u0027re just very confusing um there is a mechanism now in the specification an identity proxy mechanism and essentially the SRTP is great because you know that your media is protected end to end from browser to browser but how do you know that you\u0027re actually talking to the person you want to be talking with on the other end okay the way WebRTC deals with this is by allowing you to use third-party identity services like Facebook Connect for example to to log in to prove who you are and basically interaction with that proxy server is used to tie your identity in with the DTLS keys used for the communication and so I don\u0027t want to say more than that about it again I believe we\u0027ll have the newer slides up on there and I and it\u0027s probably a little clearer in that but just know that this is to help deal with that problem of wanting to make sure that the media is only usable by the person you want it to be used used by and not just by the other browser okay there are some new controls new low-level controls in WebRTC and going forward we\u0027re eventually going to only be recommending the use of those the reason I stuck it at the end here is because they\u0027re not implemented anywhere yet even though the specification that you know assumes that everyone is implementing them today so I\u0027m gonna walk through an example here we have one browser one and browser to browser one has two streams in it one that was defined within browser one and is being sent over a peer connection and one that was defined in browser - it\u0027s being sent over a peer connection and received by browser one okay now what I\u0027m showing here is effectively what you see in the SDP in SDP there is an EM "
  },
  {
    "startTime": "00:30:19",
    "text": "line or a media line for each one of these bi-directional SRTP streams so this shows media going to the right this is media going to the left now if you notice here on this first stream stream one I\u0027ve got audio audio and video and for stream - I have audio video and video right so these are not symmetric here so question then is since an EM line can only have audio in one or both directions or only video in one or both directions how do you decide you know which of these tracks goes into which of the M lines to make this work well the browser tries to do smart things but people a number of developers have decided they don\u0027t trust the browser to do what they want them to do that\u0027s why we\u0027re getting some new controls what\u0027s recently been added is RTC RTP senders and RTC RTP receivers which we just call senders and receivers and they are handles to the outbound and inbound RTP streams so for every track that\u0027s sent or received over a peer connection there\u0027s an Associated sender or receiver and they that gives you direct access to those particular streams so in the picture here these are senders and these are senders these your receivers and these are your receivers okay so it lets you find out for example fine you can get statistics on what\u0027s happening here you can also perhaps control specifics if you have I\u0027m just going to say specifics about how that particular flow is realized over your over your peer connection now it turns out that wasn\u0027t enough and that\u0027s because at the STP level every sender is paired with a receiver right so when you look here there\u0027s something being sent and something being received now of course it\u0027s possible that you know it\u0027s only flowing in one direction and not in both well the trend there for with transceivers now for each browser there is now precisely one transceiver for each M line and there\u0027s a sender and receiver for each of the transceivers so and I modified the diagram a little bit it\u0027s actually each browser has a transceiver for each M line so there would be one two three four transceivers that this browser has and for that this browser has and this transceiver has pointers to this sender and well you "
  },
  {
    "startTime": "00:33:22",
    "text": "know this receiver which doesn\u0027t have anything in it okay just briefly the status of the I want to talk about the status of the API so again the JavaScript API czar being standardized in w3c there are two main specifications the WebRTC spec and the media capture and streams spec both of them are now a candidate recommendation stage the WebRTC spec just went to candidate recommendation stage a couple weeks ago so WebRTC unfortunately I\u0027ve had this bullet up for a while when I give this presentation core is stable just cleaning up edge cases now so we\u0027re still cleaning up edge cases even with it being at candidate recommendation but we\u0027re getting close what we really need is implementations for the media capture and stream spec yeah it\u0027s it\u0027s been closed for a long time again we need some stuff implemented for the IETF protocols I\u0027m not going to try to list all of them : Jennings maintains a dependencies draft which which is always interesting to look at to see just how many specifications there are that WebRTC depends on and then of course for the individual groups you can follow their progress like you would normally just briefly there are tools that have popped up you know out in the world to help with WebRTC there are a variety of libraries I\u0027m not going to list them here I mentioned there sip signaling libraries there are several of those there are hosted signaling services so if you don\u0027t want to write your own signaling you could just use PubNub or firebase or something else cirrhosis and Twilio provide hosted stone and turn services so when you get to where you decide you need turn servers and you will for virtually any application you\u0027re going to build you need to figure out what you want what to do whether you want to put it up yourself or or use one of these others and then of course there\u0027s some interesting selective forwarding unit video video servers and and other you know sort of optimization players in the space there are also some higher-level api\u0027s if you just don\u0027t want to code all of the stuff that you\u0027ve seen you can use some of these simpler api\u0027s fun links you can look those up on your own if you\u0027re interested and I thought it\u0027d be good to talk about this I almost wanted to start with this one facebook chat Google Hangouts and duo and Amazon Mayday all use WebRTC virtually every unified communications platform today has WebRTC support and then there are a bunch of free or freemium communication services that work they really do work I use them regularly when I go to contact people in my in my work and that\u0027s my "
  },
  {
    "startTime": "00:36:28",
    "text": "last slide now I\u0027d like to have Alex schoolyard come up and he will tell you what\u0027s actually really implemented and working today assume even get the the slides to come up [Applause] mmm-maybe that means one is you my new version yes yeah the one you wish you had yeah right so we\u0027re seeing that there\u0027s a part 1 and part 2 and hopefully part 1 is my updated slides well thank you then I think that was shorter than usual because I\u0027ve seen this presentation during hate hours done before I think is still very interesting and it\u0027s very difficult to actually condense 6 years of work and multiple specification into into 45 minutes so you did a great job there I\u0027m going to try to do that in in in just one slide um so it\u0027s an even bigger challenge and then I go through everything you said in the next two slides right just to you to repeat everything but first my name is Alex Squire I\u0027m actually living and residing here in Singapore and I\u0027ll participate in both w3c Andy Andy IETF met me on web RTC but also on browser testing and tools web drivers and things like that those we went to GS confessor last year already so us I hope you going to enjoy having high ETF in town we know we very happy it\u0027s here so speaking about web RTC you have the two aspects you have the JavaScript API which most of the web developer interested in and then you have what you know is a change on the wire with of course is more of the scope of IETF and needs to work so if you get a you know satellite picture of to specification even though there are more than 20 the most important one are listed here and the good news is all of them went into the latest stage of recommendation and w3c or are already a specification in IETF I think that\u0027s the first IETF where we only need one session of RTC web and not to as we used to so that\u0027s great that means consensus is is is reached for everything that is the specification but that doesn\u0027t mean that the implementation actually is at the same "
  },
  {
    "startTime": "00:39:29",
    "text": "level so you went through the specification let\u0027s look a little bit about the implementation during the lifetime of web RTC there was really two pivots and for people that want to have all the gory details there is a fantastic block by the the WebRTC Mozilla team that\u0027s going to give you in detail the list of all to SBI so please go back to the slide later take a look at that that\u0027s really informative and I\u0027ll explain to you why today\u0027s some website use some kind of API right a set of API and other user different subsets so long story short you can see free main stage where at the beginning you were speaking about streams so you could get a stream from your camera with audio and video inside anywhere passing a string right but stream sorry and then we went into a track model where instead of saying you know I just want to stream actually I would like to do track so one use case that make us think like that was the webcam plus screen sharing you know I\u0027m presenting a PowerPoint I want to stream my PowerPoint and I want to stream my face in my voice then I will need two video tracks and one audio track and that was getting complicated with the stream API among other things that were complicated with the stream API another use case very simple I\u0027m using a mobile and I want to switch during a call between the front camera and the back camera right this this video is the same time might be not the same resolution but there is no reason why it will be complicated and so we came with a replace track API that was specifically dedicated to to that use case when you want to switch to two tracks of the same type in that case that the video type while keeping the rest without going through an entire cutting the call renegotiating the call and restarting the call eventually we brought in a more fine-grain API like that and we wanted to make the the glue with with the SDP and all the exchange real format we were using the signaling format at that time and that\u0027s where the trends ever gain and transceiver has a lot of very good application not lose that you\u0027re gonna see directly in in an application you\u0027re gonna write yourself there\u0027s not a lot of new API but he will alow for example for a call to successfully go through in a tenth of a time it took right so we call that click to earn to first media when you click accept the call button how long does it take before you actually in the call and start talking with someone with the original version that will be one to two seconds with full eyes then we bring trickle eyes we bring that down to 100 something milliseconds with the things that add transceiver and the new API will bring in you will be able to go close to the experience you get with a real phone call today which is ultimately what we want there is no difference between a call over the internet and a call with a real telephone line no difference between voice and audio right so if you look at this free generation not all the browser\u0027s are equally advanced and of "
  },
  {
    "startTime": "00:42:31",
    "text": "course the specification was still in flux so if we go to the next slide we can see the what was presented by Chrome last week so last week was the technical plenary meeting of w3c dan and I had the choice to participate there and flew in this morning and chrome presented that line which is as far as we know the latest state of the art status of way about his implementation in chrome so get stuck track constrain receiver so part of the second generation of the API are here but not all of it so the sender which is the bender of the receiver at track on track and so on is still to be expected in a few revision lastly a unified plan which is the standard way of doing simulcast a good way to actually adapt the resolution in case of bad bandwidth or a lot of people joining a call is still using Plan B or old version in Chrome and it\u0027s blocking a little bit the standardization effort where Firefox is already for example using the the real unified plan according to Chrome that should be completed by the end of this year or first quarter next year on Firefox is little bit simpler everything is there except transceiver and I think Neil\u0027s just told me last week or or two day that transceiver should be here within two weeks so in terms of JavaScript API you more complete and more compliant today with Firefox than you are with Chrome but it doesn\u0027t matter it\u0027s gonna converge in a very short period of time Safari is interesting Safari was a little bit late to the party but that gave us an advantage they didn\u0027t have any historical implementation of WebRTC based on the older spec so they directly went for the latest version of the spec so the directly went into the track and not the stream generation of the API unfortunately they based on the Google code so it was too complicated to switch to unify plan and still using Plan B there are a few other things that are still improve above because there are the latest implementation of off of WebRTC but one year ago Safari was not supporting WebRTC so today we have all the major browsers that support web RTC including Safari on iOS and that\u0027s really a big step forward right edge well it\u0027s difficult to answer for Microsoft they have different approach depending on the product and will you ask - there\u0027s one thing we know for sure Internet Explorer will never get web RTC they want people to move away from with all the old windows and get into Windows 10 so they\u0027re putting all the latest API on edge if they think that if they put the WebRTC API in Internet Explorer that "
  },
  {
    "startTime": "00:45:31",
    "text": "will give a they will not give enough incentive from people to move away from windows 7 edge as to implementation today one is based on a RTC with a polyfill on top and another one that is the web RTC 1.0 but first generation which is the ad stream and and removed stream implementation and finally they have a third implementation for native that supports web RTC and ease as far as we know we\u0027re not told me last week the the most advanced we support for vp9 and things like that for example so originally the biggest problem with the edge was that they were only supporting a very proprietary codec called h.264 you see that was the SVC flavor of 264 used by Skype now it\u0027s getting much better h.264 the vanilla h.264 is in vp8 as well vp9 with SVC is coming in some of the 3 different stacks they have at Microsoft so here again slowly by the end of the year things will be interoperable enough so you can initiate a call with edge and receive a call from any other browser with at least one codec that will be understood by each side so w3c unlike IETF require that test exists before you can become a standard so we had compliance test which is test of the JavaScript API that\u0027s very interesting for web developer because you can check if not only the API exists but she behaved according to the spec or not and so we were not close to consensus last year so not a lot of tests with written and they were written mainly by people that were volunteering their time Herald over there myself and someone called Dominic from the w3c where basically the free weekend contributors when you know our wife was not around now we put some more people Google but a big focus on testing this year and the result is we went we got 1,000 more tests but beyond that next slide what\u0027s interesting is not the number of tab themselves but the coverage only 10% of the spec was really tested one year ago and nowadays last week we did another a check and we went up to 70 percent so we went from 10 to 70 percent all thanks to we fought from people here in Singapore working on on the specs next slide again what does that mean if you web people well if I\u0027m a web developer I have two choices I can look at this back there beautiful I really want what the spec is selling but the implementation in a browser is not there so I need to make a practical choice and that\u0027s where things like jQuery came in and that\u0027s where some polyfill and some adaptors are also coming for WebRTC so if you had to "
  },
  {
    "startTime": "00:48:31",
    "text": "choose between the browser you look for you could use those test suite to see which one is the most compliant so in June 2017 Safari was the most compliant oh my god that was a shock for Firefox well they were not the most compliant by by a lot and still 50% of the tests were fairly right so those tales are very fine granularity and you have to take them into account I think the presentation we did before in terms of further or second generation and where they are is is giving you a better idea so Firefox is great at getting the latest thing in they have almost everything there we want to test the latest thing test test it against Firefox Chrome and Safari are not far away behind and you can expect everything to stabilize by quarter 1 2018 and I know you heard that before but before the specs were still in flux now they\u0027re in candidate recommendation which means there will no be any change in the API so in the object that you can use in the browser so the only difference now is whether they implemented or not right so that\u0027s that\u0027s the situation as of today to be able to test and to make the things better the browser vendors beyond the usual JavaScript testing also implementing the new new new test tools especially one kite that was developed here in Singapore for all the browser vendors to actually test specifically web RTC in the cases where having only one browser at a time is not enough nowaday the JavaScript API is pretty ok everybody know what they need to do to make it work but what doesn\u0027t work is when many people try to connect to each other edge try to connect to Safari in the same cola with someone on Chrome on different operating system that\u0027s not something we have as a standard committee to test before and we didn\u0027t have a lot of visibility in that and that\u0027s where we were getting most tickets from from the users so now we developed a specific tool for that that is going to be run every day to test into probability between a matrixes of different browser revision operating system and so on so we hope that this is going to reduce the number of problem you you people might have experience is using web RTC in big production environment and that\u0027s really a big push by all the browser vendors again specifically for IETF they love to have feedback on the on the tutorial did that show everything you wanted to see did that leave out something you would have seen like to see do not hesitate to participate whether you\u0027re happy or unhappy about the result and I\u0027ll say that we\u0027re officially at time we there were obviously some technical problems getting started so I think if I don\u0027t believe there\u0027s any other session that\u0027s right up against us right now so we could we could take one or two questions "
  },
  {
    "startTime": "00:51:34",
    "text": "any questions okay all right thank you thank you [Music] "
  }
]