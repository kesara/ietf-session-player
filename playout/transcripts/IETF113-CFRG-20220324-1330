[
  {
    "startTime": "00:00:26",
    "text": "uh good afternoon i'm i'm very happy to open this first hybrid cfrg meeting in vienna so we do actually have quite a back agenda so uh i'll try to keep on top of it so i'm alexi melnikov my co-chairs nick sullivan and stanislav are remote this time um let's get started so as you probably know the session is being recorded we have minutes in code dmd i need a note taker can somebody please volunteer to be a note taker"
  },
  {
    "startTime": "00:02:08",
    "text": "don't be shy don't stare at your laptops i'll help i'll help chris since i see he volunteered in the chat this is rich that would be perfect thank you if somebody else can [Music] just sort of be a backup and help out with uh notes taking um luckily we don't need to do blue sheets in the former paper form you logi logging into metacore whether it's metacore regular light will automatically add you to the blue sheets uh rich if you can mute yourself actually your keyboard is very loud yeah sorry about that um so uh irtf operates on the note well uh you should be familiar with this already if not this is the slides with relevant rfcs um but one of the main points is always be professional be nice to each other be respectful uh even if you disagree with the technical point and we are governed by good privacy and code of conduct as a quick reminder this is irtf research group so we do research here not standardization as such cool so uh and i will do a quick update on the document status we have quite a few documents in flight actually no um any agenda bashing first"
  },
  {
    "startTime": "00:04:09",
    "text": "yeah um this is chris wood um notice the the software topic on the pseudocode and sort of specification uh discussion is first um i'd like to recommend given the highly technical nature of the agenda that that be moved to the end so we reserve time for those more important things um i i'm happy to just you know if we need to in the interest of time i can send an update to the mailing list to kick off discussion but i don't want to take away time from the more technical topics that could use airtime okay uh that's fine so you will want to start with key blinding then yeah great okay fine that sounds good right so my apologies for a small font we're trying to get trying to fit in all the documents in uh with the life expired but the the gist of it is we have one rfc published which is hp key rfc 9180 we have one document in rfc rescue in misraff depend which is aspect 2 which depends on hash to curve we and hashtag is already an rsg review so hopefully it shouldn't be a long um till it's approved as well we have quite a few active documents pretty much everything was updated draw vrf is waiting for the document shepard um kangaroo 12 is also waiting for chairs to review result of second research group last call i will get very few responses to this so if you haven't reviewed the document or confirmed whether you"
  },
  {
    "startTime": "00:06:01",
    "text": "still think it's a good idea please send us a message or reply to the mailing list other not worthy thing is actually uh the cfrg um appearing friendly curves is while it's expired um there is another coeditor for the document and it's being worked on so hopefully it will be refreshed soon yes chris um what's the status of the uh restretto draft the the last time the state i do know it was updated to take into account the feedback from the crypto review panel review um so i'm wondering uh like what the next steps are um there are a number of documents that are building up dependencies on this and it'd be good to move that forward so i'm just curious what the next steps are uh yeah i suppose we we should review it with micro chairs but uh it sounds like you're you're suggesting that it should move to a research group last call soon yeah that's my understanding okay sounds good uh so the other thing is uh this is a our regular slide about crypto review panel that serves three purposes it helps cfrg itself security area independence stream edit stream to review crypto related documents we just we announced new"
  },
  {
    "startTime": "00:08:00",
    "text": "round of membership in this uh lost atf um with discussion about um our past performances as well as availability of different people uh we basically capped everybody and added a couple of names veranda kumar and ludwig pereira [Music] i think that's it the other thing i actually just realized and i forgot to have a slide zone in case you haven't noticed we now have secretary thank you chris wood for being a secretary in helping organize us and help us uh with a website and github and background and now let's get started yesterday good afternoon i have a question on the crypto review panel um i noticed that sometimes a request is being made and then it takes an extremely long time to get the review done even though people really promised to do it in about a month and these are all people in the crypto panel who volunteered for this position so it's not just a random person of the audience right so so a particular case was last year of a document that i had reviewed already three times and it took more than four months to uh essentially get any feedback so are you still waiting for a view or are you just i'm not very still waiting for review but i think uh it's it's really yeah it's it's really hard to accept uh a four months delay of people who promise"
  },
  {
    "startTime": "00:10:01",
    "text": "to do something in one month right right that's fair enough uh yeah i think uh lesson here is uh chairs really should be on top of this and make sure that if you cannot fulfill that we should should be able to find another reviewer chris okay christopher are you uh on crypto review panel or no i i i um i just wanted to like i had a follow-up question but there's someone in the queue before me and i don't want to jump um i'm curious how how many requests for reviews that um uh sorry about that i i i misattributed i looked at the name in the queue um i'll fix that in the notes um i uh i wonder how many requests for review the the the crypto review panel has at any given time i'm just curious uh it really varies sometimes there are a couple of months or three months in a row then there are none and then sometimes you get two or three in one month or even like twice as many i would say about two or three requests per month on average right okay yes renee um so sorry just one more question i i looked at some of the uh as we all know uh the crypto uh uh mailing list is only sporadically being used for"
  },
  {
    "startTime": "00:12:01",
    "text": "technical discussion and i noticed that uh some documents apparently were ready to be uh to to proceed based on one crypto review panel review and it was absolutely a kind of a dead silence on the mainly mailing list and i i always thought that crypto review panel was not a substitute for the normal operation of the sea of rg so i'm just wondering whether cfg is kind of dead and the crypto review panel is doing something in background right no it's absolutely not substitute that's they should they are basically working the same with the director uh we should take offline any specific cases when you have concerns right with this uh chris wood keep landing on signature schemes i'm going to okay can you uh accept the request oh are you driving uh no hold on i can pass it to you that'd probably be easier uh hold on let me cancel the request and then i'll i should be able to just pass slide control okay oh oh awesome i know you could do that all right uh yeah thanks everyone um this is a presentation of a new draft um on this subject we call key blinding for signature schemes um joint work with uh frank denis and eating it and myself so for some context um consider this the setting where you have what we call a single proverb verifier where the prover wants to convince um the verifier of you know uh"
  },
  {
    "startTime": "00:14:02",
    "text": "basically a signature over a message where the message is input on the left-hand side to the approver and the approver has a given key pair as input the approver runs some signing algorithm producing a signature and sends message public key and signature to the verifier the verifier checks this particular signature and outputs basically a bit as to whether or not it's valid in this particular setting uh we basically want that this uh this proof the signature is unforgivable meaning that if you're given a a a tuple of message public key and signature um it's the verifier will only conclude um uh that the signature is correct if it was produced by someone who actually has access to the corresponding secret key um with overwhelming probability um this is i mean this is just a basic digital signature scheme um but but there are scenarios wherein um you know the the you might not want the verifier um uh to learn information about sort of the the prover that produced the signature um or more specifically you might not want the verifier to learn um you know information about the the signers the proofer's long-term public key through either the the public he used to verify the message or the signature itself this particular property has a number of applications in practice right now for example tor's hidden service identity binding protocol uses this sort of construction um for like signing things in such a way that you can't link them back to the the original like service provider um that's also used in like cryptocurrencies for private airdrop um it's used in like bitcoin hierarchical um"
  },
  {
    "startTime": "00:16:00",
    "text": "uh wallets too to minimize state and um recently uh we've found an application in privacy pass for a different type of issuance protocol that we called rate limiting so it does seem to be a sort of common uh common functionality and the purpose of this draft is to try to uh basically you know uh standardize it um uh and specify it so that we can have interoperable implementations of it um and to to to sort of explain what i mean by this i guess not revealing anything about the prover let's move on to a sort of alternate setting where you have multiple approvers now so you have on the left-hand side two provers brewer zero prover one each have their own uh unique key pair um and each are given the same input message to sign you have some party in between proven verifier which i'm gonna call mediator just for lack of a better technical term and you have the verifier as before in the setting both groupers are going to sign the message with their respective secret keys send their their triples over to the the mediator who is going to choose a random bit and then decide to forward on the triple from uh either proof or zero approver one based on that particular bit to the verifier and the verifier will run the verification algorithm and try to guess the bit um uh so we asked two questions um you know is this unforgivable uh in the in the traditional sense like uh that i described earlier if you're given you know this triple message public key signature can you verify uh or can you try to cheat the the verifier um and the answer is yes uh in particular because this is just a basic digital signature um but then when you look at the unlinkability um uh sort of property uh that does no that does not hold in this"
  },
  {
    "startTime": "00:18:01",
    "text": "particular case um in particular because the verify you'll notice on the right hand side has as input the public keys of both river zero and proofer one so it's pretty trivial to check whether or not prover zero proofer one generated this particular signature by just looking at the public key or trying to verify the message signature under either one of the keys so what we want is for this the verifier to not be able to determine uh the bit b um with probability negligibly larger than half no better than a random guess and so this is where the sort of functional requirements come to play we want an unforgeable signature scheme with these sort of additional properties in particular that the the per message public keys that are used to sort of verify a message at the verifier are independently distributed from the long-term uh approver keys moreover that we want the signatures themselves that are produced to not leak any information about the long-term signing keys and sort of jointly this this captures this intuition notion uh or this intuitive notion of unlinkability that i was describing earlier and our proposed solution is a signature scheme with key blinding um functionally what this does is it extends an existing digital signature scheme with two additional functionalities one of which is a function for blinding and unblinding a public key so given a public key and a blinding key which in in the in the syntax of the object is just another private key um you can produce a blinded representation of that public key and then you can of course unblind it using the unblinded the original unblinded public key and we have this separate functionality which is called blank key sign we're open to name changes if it's helpful for clarity"
  },
  {
    "startTime": "00:20:01",
    "text": "but the gist is that if you run blind key sign with a long term uh signing key and a blind key the output signature will be valid under the correspondingly blinded public key using the same blind so in this in this relation at the bottom we have that verifying trying to run the verification algorithm with the blinded public key over the message um using a signature output from blankie sign succeeds or is equal to one and that's the sort of requirement or functionality i guess so if you were then to plug this particular scheme back into the multiple prover scenario before um you notice that the the provers on the left are slightly different now they have the same inputs as before they have the same long term signing key pairs in the same input message but in actually producing a signature they both generate a random blinding key referred to as bk0 bk1 in this particular case and they run blind key sign with that particular binder key producing a signature and their triple that they send now to the mediator consists of the message signature and the blinding key the mediator will choose a bit just as it would before um and it will uh use that bit to determine which of the blinded public keys uh to gen sent to the verifier um so if the bit zero it blinds prover zero's public key with proof of zeros blind producing bk and sends that over to the verifier along with the signature produced by proof of zero and likewise for approver one um"
  },
  {
    "startTime": "00:22:00",
    "text": "so if you look at the the two properties that we were uh targeting before unforgeability um uh has a sort of a signature scheme uh it still has this property that the the the signature itself um must have been produced by someone with the long-term private key um with overwhelming probability uh um and uh by virtue of satisfying the sort of functional requirements um it also now has this uh this unlinkability property that we want um in particular uh the the blinding key or the blinded public key bk is independent from both of the long-term public keys pk0 and pk1 are the provers and like similarly the the signature itself is independent from the long-term signing keys so that the triple itself reveals nothing um about either proof of zero or approver one so it intuitively the the verifier cannot guess the bit with uh it cannot do better than a random guess at bit uh b um which is the property that we want that's effectively it for the functionality it's very simple the eddsa variant is in the draft building off of rc 032 um uh it only covers the pure edsa none of the context or prehashing variants or whatever um that's very straightforward it's often uh used as an offhand demonstration of the the concept in academic literature and whatnot um there also is an ecdsa variant in there as well um this is a bit um uh different um different in the sense that it's not doing the obvious thing i guess uh that that you would do for a key blinding um uh i can i can elaborate if people would like but um it's different enough that"
  },
  {
    "startTime": "00:24:01",
    "text": "uh you know we're doing security analysis to determine whether or not it is indeed safe um but the the intuitive intuition is that it is it is um it is correct uh there are also a couple of implementations thanks to frank and myself um and other people are working on them as well it's fairly simple straightforward draft to implement and there are test factors available for those who would like to implement it and as i said the security analysis is underway we hope to make those results public as soon as they are done um to gain confidence in the the constructions um but uh um as i said the the eds eddsa one is um uh fairly straightforward intuitive um it's the ecdsa one that is the the more interesting variant um and with that um i guess uh i have two questions for the group um the first of which uh do folks find this to be a compelling use case um and problem worth uh working on uh and trying to solve with this particular this construction you know signature schemes we keep winding um and secondly are folks interested in adopting this as a an initial draft to to do exactly that and i will i will pause here for questions and i'll read the chat we have three minutes for quick questions if there are any um [Music] i think the question of adoption will probably discuss between chairs and might take it to the mailing list uh worked for me um if if folks um are interested in um you know more clarity on uh um anything um you know the the draft link the draft has a link to the repository where it's being developed um"
  },
  {
    "startTime": "00:26:00",
    "text": "and uh of course we welcome any and all of your feedback um and contributions christopher are you on this topic uh my question i would like to understand a little bit um more about uh what changes you made to ecdsa or hashtag right so um the ecdsa variant uh the the the intuitive thing that you might do is uh given a like a private ecd ecdsa key um and a blinding key which is also a private key um you might uh like just multiply the two together and similarly you might multiply the like the public key by the corresponding blinding key um uh and ergo have a a blinded representation of both the private key as well as the public key unfortunately there's this um there's a body of work on related key attacks and how they can be used to produce forgeries for schnorr-like signatures as well as ecdsa and in particular this sort of naive uh way of blinding uh an ecdsa public and private key uh does lend itself to forgeries um i can point people to the relevant references if they're interested so the tweak we made was to basically rather than um uh maintain sort of the algebraic relationship between the the blinding key and its impact on the the i'm using the word blind um okay um the the trick was to uh rather than maintain the algebraic relationship between the the input blinding key and the output blinded public key uh to hash the input blinding key to a scalar to sort of blow away"
  },
  {
    "startTime": "00:28:03",
    "text": "this this algebraic relationship and make it so that basically you need to produce a collision in this hash in order to produce a forgery um so the the construction does kind of depend on um uh security in the random oracle model uh intuitively um but that's effectively the gist i i i wish i had it written down in slides um but perhaps that that vocal description was not overly clear but chris i can i can send you pointers um uh to the the diff where it was included if that would be helpful okay so the unfortunately the queue is closed and we are a bit out of time on this topic so let's uh winning list okay right thank you uh steven you are next um do you want me to drive slides fine i'll do that somebody was very tall just hold on one second okay uh next slide hold on just one second okay great so um i see there was like a hundred people in the in the in the in the session so i hope to get some some feedback on this even if it's just kind of a sense of what people think so the lake working group is defining a protocol called ad hoc uh for authentication key establishment for small devices it's you know you can think of it kind of a bit like dls it does the same kind of thing but uh it's you know really tailored towards kind of small packet size few octets to"
  },
  {
    "startTime": "00:30:01",
    "text": "be emitted and if you care about the requirements you can see them there so again like tls there's kind of a concept of cypher suites and in those kind of cypher suites there's a signature algorithm represented for authenticating one of the parties there are suites to find that above ecdsa and eddsa and that lake working group is trying to figure out how to do you know which which signature algorithms should be part of a mandatory to implement set of cypher suites and as always there's lots of argument in itf working groups about that but that's not what we want to talk about here so next slide so in this particular context um there's kind of an additional argument to the usual mti cypher switch selection argument issues um essentially where an adversary controls a provision device and can mount you know fault injection attacks and extract a signing key and for the kind of applications the ad hoc protocol is designed for that seems much more likely than for you know tls as used in in data centers or whatever and uh thanks to renee for bringing this up and there's a link there to the thread from the lake working group and the context is you know small relative relatively inexpensive commercial devices that are provisioned private key is probably not that well protected so if you open the device it won't be zeroed but hopefully you know you can't just connect to a uart and read the private key from the file system and private key extraction could be significant because then perhaps you could pretend to be a controller to some actuator and cause damage so next slide uh so i i know i know almost nothing about fault injection but i did find a few papers that were interesting um so the first one i thought was was actually pretty good for me because it actually explains how you can do fault injection via kind of a low voltage attack it's a little bit old but uh it's it's a"
  },
  {
    "startTime": "00:32:00",
    "text": "really nice explanation of how these attacks can be kind of mounted in a realistic environment and it includes an example of it's not a novel attack on rsa signing but it's in that paper there it shows how such a such a fault injection attack on on on reading from memory can leak in rsa private key there's also another one uh about ecdsa from the similar time frame same kind of attack where you're you've got a fault injection on on kind of memory reads um although the second one is a simulation as opposed to a you know an actual demonstration but that's on ecdsa and then there's another one on eddsa which is in that case it's a power analysis attack rather than fault ejection so those are kind of useful papers and i think what they seem to show is that at some level all of the possible sensible options we have to choose from might be vulnerable to these kind of attacks in this kind of context for these kind of devices next slide so one of the points being raised that was raised in some of the discussion here is that you know eddisa is deterministic that might help the attacker as opposed to the randomness in ecdsa but of course you can have a bad random number generator as it's been seen and there have been some suggestions john matson has a suggestion for adding some noise or some more randomness into eddsa and again i i don't have a personal opinion on these just to to note that the these issues have come up so next slide so the ask is i think it boils down to two questions i mean we seem to have three kinds of signature algorithms we you know we may assuming a working group wants to pick a mandatory to implement cypher suite with a signature algorithm is there really much difference here between them in this specific kind of attack context so you know we could do it some help if that's the case or assertions that from from knowledgeable people that it's it's more or less the same or it's"
  },
  {
    "startTime": "00:34:01",
    "text": "different for different ones um or could cfrg develop something that's uh usefully better in this attack context uh so for context i don't the lake working group has kind of currently you know picking their mti choices has kind of oscillated over the last number of months i think the current they currently they've landed on just saying just three two five six um they probably won't want to wait for an answer so it's not a it's not a question where uh we need the answer right now and are kind of waiting on it and assisting cfrt doof but i think it's a generic problem for particularly for this class device and again there was there was a thread on cfrg list that didn't really reach any any anything i thought was a clear kind of answer um hence this presentation so that slides so again just to summarize the attacker here controls the provision device is able to mount the fault ejection attacks let's say an extract designing key i described the kind of device and the two questions there are which of those are if any of them are kind of good to recommend as a manager to implement cypher suite and in that context and is there anything better that you could recommend and i hope to get some input does anybody know about this topic remain right quick remark so indeed i raised this topic at the lake working group um i think is usually the deep determinism in the generation so as as long as one can fix that this edd is a eddjj may be fine if one cannot fix it ec dsa already doesn't have uh this determinism as long as on this just rfc6979 so i i think it's it can be easily"
  },
  {
    "startTime": "00:36:00",
    "text": "solved but it has to be addressed if this is to use edd and otherwise the ecb davis p256 would be a viable option in my opinion so that that the audio chopped a bit there but i i think you were basically saying ecdsa is better um which then leaves me wondering about those other two papers that i referenced so actually um steve i i didn't say ecd is a better ecd is only ecdsa is only better because the noise generation is non-deterministic if edd is a would be specified in a non-deterministic way then the problem goes away as i identified so again that makes it clear so that leaves me wondering about the paper that i referenced that has a fault injection attack on ecdsa well the the whole side channel attack uh arena is quite large i have more than a thousand papers on this topic so uh obviously there are attacks on uh on all different variants but uh uh there are attacks that can easily be prevented that take advantage of deterministic behavior sure yeah i need to know what you're doing you cannot that's in that right right but again just you know from the point of view of trying to pick something if there are lots of you know if if we're talking about a manager to implement cypher suite we're asking everybody to implement it and if there are known attacks against all relevant signature schemes they happen to be different attacks but they're known in this context then it's not clear to"
  },
  {
    "startTime": "00:38:00",
    "text": "me that one is better than the other i i i i think the problem of the problem is that lots of attacks are also recycled for example open ssl versions that are known to be broken and um you still need to be slightly competent in in terms of implementing signature schemes okay john thank you thanks steven for raising this uh discussion i think it's very helpful uh regarding two i think hillary summarized nicely on the list why we need three different alternatives purely random purely deterministic and a mixture of the two they all make sense in different settings uh i think we can do better and i think what maybe more a question to the chairs that should be discussed either here or on the list later but uh how do we progress with draft mats and was quite large uh support for adopting them during the adoption call there was a discussion about the potential alleged ipr on it do we have should we have a second uh call for adoption now when that is known stephen also said said on the mail that he might he could declare an ipr statement on the draft uh that would of course be used in court as stephen perel has analyzed this ipr and he he definitely thinks it applies so there's negative things but that also but it would raise to any implementers and users that they're there somebody has claimed or not ipr"
  },
  {
    "startTime": "00:40:01",
    "text": "yeah sorry i i'm actually the responsible chair who dropped the ball on this i think yes we need to rerun the adoption call with ipr disclosure and just to just to clarify if i made a third party ipr disclosure it just says you're on notice that something exists it doesn't say i claim it is relevant i think uh the lawyer in court would say that anyway i agree with you in principle yeah that sounds great for the second adoption call okay yeah let's let's record this as an action item uh philip yeah i was just going to point out that since we're going to be doing thresh we've got the threshold signature work we're going to have things that look like non-deterministic signatures as far as the verifier is concerned anyway uh in that i i never have never considered the idea of relying on a signature value to be a non-deterministic uh function of the input uh in a protocol to be a sound move um so i i think that we should definitely allow for some form of signature with a deterministic component i think it still makes sense to require people to blind the input blind the random number uh in some deterministic way and then apply the non-deterministic input so we get as much robustness as possible um but i think i think that we need that anyway uh what i'm a little bit um nonplussed about it i've never understood why people want to put signatures into key exchange"
  },
  {
    "startTime": "00:42:01",
    "text": "protocols if you're doing a key exchange protocol diffie-hellman does a key exchange and you can build uh robust systems on top of diffie-hellman as the signal folk approved and they've got a proof um proof of correctness of that i would just steer clear of anything to do with signature because if you sign something you create a non-repudiable proof that somebody was involved in that communication and that's dropping a piece of information that i simply don't want to drop unless there is a good reason to drop it just a thought all right thank you phillip part uh bart you need to hit the unmute button as well i guess or request audio permission there you go okay does it work okay yeah i heard steve asked some questions and maybe i can help by by simple clarification to this so in a differential power analysis attack you need some secret information and some variable data and by actually making so this if you can avoid that by the having using you can avoid a dpa attack but there is other attacks like simple power analysis or fault attacks that are kind of orthogonal to this and that are not helped or not help or helped by having variable data and so this may answer your question that i don't think there is any signature scheme that is by itself robust against any of these attacks you always have to look at your implementation but particularly having deterministic involvement the generation of the nulls in a way that is not randomized you will make dp attacks harder i hope that helps thanks yeah that does help all right"
  },
  {
    "startTime": "00:44:01",
    "text": "thank you thumb thumb these kinds of attacks fault attacks they don't they aren't just limited to the crypto right they allow you to sort of in my understanding anyway skip arbitrary instructions that your target is is doing so in the case of signature verification they might also just skip over the entire verify call altogether they could also skip over a random number generator so this is i think a type of question that you should consider wider than just what kind of crypto do we use and how vulnerable is that but also how do we structure our implementation in the first place because if the crypto is not getting called it will not really have mattered in the end sure i i think that's entirely true it's a little bit you know so i think the the concern here is an adversary gets control of a device and manages to extract a private key yeah and then can you do what he likes of it so so it's a little bit different from that i think but i i i absolutely agree what you say all right uh my apologies i probably i not not going to pronounce it correctly willing uh so actually i just have a simple question so what does mean you call it mti signature what is it mti oh sure apologies for not explaining it so the when ietf pro working groups are developing protocols typically they would like to specify a mandatory to implement that's what mti stands for uh set of options so that you we increase interoperability because we hope most all implementations will include them so the question here is which one do we which of these signature schemes would we like"
  },
  {
    "startTime": "00:46:00",
    "text": "everybody to implement okay yeah thank you very much all right thank you uh we have one action item and you got some useful feedback i think sure so i guess the action item is on john's draft i guess um [Music] you know i think this i suspect the same question might arise with ctls for if it was deployed in similar environments that lake isn't envisaging um so if the answer is as bart says that there's different attacks but so there's no real difference because you can mount one attack or the other that it would be great to kind of have a sense of that so that we could feed that into itf working groups okay i suggest uh interested party are welcome to continue on cfrg mailing list i can certainly try again okay well uh i think now that you kind of raise the awareness of this maybe there might be more uptake yeah hopefully your presentation may have helped so uh i'll i'll start a thread and and if you deal with john's draft i guess that's that's the two actions i suppose okay great thank you sounds good uh next is chris patton hold on just uh yeah let me okay only my second cfrg talk so i don't remember how to run the sides slides so you should have now control for driving slides okay let me test it out hi everybody uh it's fun to be on the big screen once"
  },
  {
    "startTime": "00:48:00",
    "text": "again okay um all right so this is i wanted to give a quick update on an individual draft that i've been working on with uh richard barnes and philip schiltmann this is called verifiable distributed aggregation functions and i'm going to give a quick overview of this so if you missed ietf112 don't worry about it you should have get all the contacts that you need speaking of context um so what this is really about uh we've recently formed a new ietf working group called ppm which stands for privacy preserving measurement that group is meeting um tomorrow by the way for anyone who's interested uh please attend um and the goal of this uh working group is to standardize uh uh cryptographic techniques for uh what we call privacy preserving measurement um and uh and and we're thinking right now we're mostly thinking in terms of like uh multi-party computation so uh uh what this what private what ppm kind of means is you have a bunch of users and you want to you're interested in um aggregate uh uh aggregate statistics about these users um but you don't want to see their individual measurements in the clear um and so you're going to run some sort you want to run some sort of multi-party computation to uh to make sure that you don't um so uh yeah so right now we're uh we're working on um a what we're calling our first protocol and we're hoping that it'll be adopted by uh by the group as the first uh document um and what this does is um specify the end to end verification and aggregation of measurements over https in this document the vdf document is kind of the core cryptographic component of the ppm protocol so my objective for this talk is to explain to you uh what this draft is"
  },
  {
    "startTime": "00:50:00",
    "text": "about and i also want to ask the cfrg if this is a ready for adoption by the working group okay uh a quick overview of uh vdf so um the the main cryptographic technique we're going to use is just simple secret sharing um so the the way our architecture works is you have a bunch of clients and they're going to be sending um their their uh secret shares of their measurements to two different aggregation servers or more aggregate one or more uh two or more aggregation servers i should say um and then uh aggregates are collected by another server another party called the collector and who who assembles the final result so in this first step uh the the starting step each client is going to split its measurement into uh input shares as we call them and sends one in one input share to each aggregator and then uh in the next step the aggregate for each set of input shares uh where the uh the aggregators are gonna uh engage in in uh a multi-party computation to basically uh which has basically two goals this one is uh to um one is to verify that the input shares that they're getting correspond to a valid measurement um and the other is to what uh basically prepare the input uh for aggregation um and i'll explain uh i'll uh yeah so i'll get into that into a little in a little bit and then the last step so the preparation step is kind of the core uh meat of the protocol and then um the the all they have to do after they've done this step is um is combine their output shares into um aggregate shares locally and then um send aggregate shares to the collector and uh the collector uh"
  },
  {
    "startTime": "00:52:00",
    "text": "combines these aggregate shares to get the final result so i'm going to give a couple example protocols so hopefully this this will be clear in a moment um the the spec currently has uh uh the goal of the spec right now is is to actually specify two instantiations of this of this uh of a vdf um and the first one is prio which many people might be familiar with uh uh it's it's kind of been in the ether for a while now um and in this protocol it's very simple a client is going to encode its uh its measurement as a um as a as a as a vector over some finite field and um uh it's going to it's going to split that vector into secret shares and send one uh one vector to each of the aggregators um and then in the to prepare these uh to prepare these uh uh input shares for aggregation um they're going to just make sure that uh the the vectors that they have uh sum up to a valid input without actually learning what the input is um and uh uh and then um all they have to do in the aggregation step is sum up their vectors um and then all the collector has to do is sum up the aggregate shares uh now there's another uh the other protocol that the other uh protocol that we want to specify is called it's called poplar so this is a this is a another paper from the from the same group of folks now the problem that this solves is uh is the heavy hitters problem and this is where you have each of the clients measurement is an nbit string and you want to know uh which of these strings occur at least some number of times so t time so in our example here we have three bit strings and uh only two of those strings occur in that set more than uh more than"
  },
  {
    "startTime": "00:54:01",
    "text": "at least two times and the solution for this problem i won't go too much into the detail um is is called an incremental distributed point function and um uh uh what this uh what this allows you to do is kind of query uh your so so clients are going to compute what are called idpf shares from their input and what this idpf share allows you to do is to uh kind of query your uh the the input on like a on a candidate prefix so you can ask um is so if your input for example is zero one one you can ask is zero prefix of the string which it is or is one a prefix of the string which is not etc um and so what uh after um kind of what the query the result of the query on your idpf share is a share of the of the this answer like yes this is this is a prefix no this is not a prefix um and what you can do with that is aggregate them into shares of hit counts so basically your aggregate share uh is uh is uh the is a secret share of the number of times a candidate prefix occurred in the set um yeah so this this turns out to kind of nicely fit the shape with a few minor tweaks so in the sharding step the client generates its idpf shares from its input string and now in the preparation phase we're going to do something a little bit more complicated we're going to we're going to evaluate our idps shares on a set of candidate prefixes but then we need to verify that the output is well formed basically we should only have the the each aggregation aggregator should only have a share of a vector where that is uh that is where you can only have one candidate prefix"
  },
  {
    "startTime": "00:56:01",
    "text": "um uh for about like a given input so um yeah so uh yeah i i i i think i'm running out of running a little bit low on time so i i won't get too much into the to the weeds here um okay so uh there's this uh idpf share evaluation and then verification that the output share that you get is is well formed and then um in the aggregate aggregation phase all you have to do is uh sum your output shares into a share of the hit counts and then you hand those to the collector um and so the way this is gonna help you solve like find the heavy hitters is you're gonna run this the collector is going to run this protocol basically prepare aggregate unshard uh several times uh with several different sets of candidate prefixes until it finds uh the set of heavy hitters okay um so very quickly uh we've made a lot of progress since iutf 112. uh what we have uh today is basically a complete special specification of prio um and a with including a reference implementation that generates test vectors and um we have there's at least one implementation of this that's that's uh pretty fast um um so the next things i think to do are is the next the next chunk of work is to complete the spec for poplar there are a couple of uh implementations of pieces of this um it's it's a little bit more complicated um none of them like interoperate yet so uh this is why we want to uh to to have a spec in the cfrg um especially because people want to implement this um uh the other things we need we were working on security analysis um and uh we want and we'll need to flesh out the security considerations um"
  },
  {
    "startTime": "00:58:00",
    "text": "and of course we're the hope is that this you know drives uh uh cryptograph cryptography researchers towards direct direct cryptography research towards um the design of vdfs that solve different private data aggregation problems um yep and so there's some a few other open issues but um i think uh i in my view this is i i've based on what i've seen i'm kind of new to this um this i think this document is mature enough to to start working on it in the working group i'm curious to know if people are interested and what would be the next step comments questions chris go ahead uh yeah thanks chris for the presentation um i'm i'm obviously supportive of this uh the group adopting this as a research group item well we're in the purview of cfrg to work on um bringing something like this to the rest of the industry by standardizing it i mean as chris said we have a a number of implementations already uh it's prio especially has a lot of experience in like running code so um this this seems like kind of a an obvious candidate for adoption and i would like to i would like to see that up stephen i see yeah i similarly like to see something that can support ppm adopted and produced by cfrg i think that's it's the right place to kind of work on these functions okay cheers we'll talk to you"
  },
  {
    "startTime": "01:00:00",
    "text": "uh christopher and uh i think tentatively we're happy to do adoption call but we'll just double check between ourselves thank you all right okay a asgcm exploit next hey can you hear me shall i share i'll share and pass to you uh controls again great thank you all right it's a new and exciting feature oh hold on what's happening oh um christopher you need to stop sharing okay right thank you um okay oh great i have the slight contrast great so um yes thank you um this is about a rather applied topic it's about an exploit of aes gcm authentication deck for hidden communications it's a work that has been co-authored with alexandre hartle who's on site and some other colleagues from academia and"
  },
  {
    "startTime": "01:02:00",
    "text": "from industry so next slide yeah first about hidden communication just uh very very briefly so so malware it's about malware communication hidden malware communication and critical infrastructures initially so malware needs network communication to unleash its destructive power and initially malware used explicit communications meaning communications that use dedicated messages on tcp udp etc and nowadays malware becomes increasingly stealthy meaning it tries to hide within existing legitimate benign communications in the example below you have alice at the left bob at the right they just exchanged some benign ip datagrams and and mallory somehow got access to an asen on the path and is communicating with eve just by exploiting some of the unused fields or or less used fields like ttl et cetera that are not that apparent and and so they misuse existing communications um can happen uh rather as a covert channel so ipptl flags options inter-arrival times modulations or you could use also some nonsense random numbers etc now if we have this critical infrastructure and we want to harden this uh infrastructure um we we need to assume that any system is vulnerable that is the question is not who discovers the the question is not when or if a zero day is discovered but who discovers this vulnerability and when what is typically done in industries now to protect the key materials ck using so-called ckmds crypto key"
  },
  {
    "startTime": "01:04:00",
    "text": "management devices so trusted platform medius modules or smart cards that are uncompromisable in so they protect the key material physically and offers some well known api so in the case of compromise shown as a red so even if an a iit device for instance a smart meter is compromised this key material is cannot be leaked because it is physically shielded meaning there are just some well-known apis to encrypt or decrypt using these keys and this can be either a hardware module or it can be networked as you see at the right and the question that we ask ourselves is can malware exploit cryptography for hidden communication and disheartened systems and the answer is obviously yes so aesgcm i'm not going into details i just want to outline so so we have some some input parameters to the algorithms we have an initialization vector we have some plain text uh plain text uh message blocks uh the key material that is protected so we don't believe we can change this or we can find any exploit related to it and we have output parameters shown in red so ciphertext blocks and the authentication tag now concerning the initialization vector there have been already some discussions on the lists during the definition of aes gcm and deployment so it was a discussion whether initialization vectors should be chosen randomly and the answer was no so a clear recommendation not to do so because in particular of this this ability for hidden communication so it is recommended to use deterministic counting initialization vectors when using such a ckmd the state keeping is difficult this segment these are typically stateless devices meaning they just map"
  },
  {
    "startTime": "01:06:01",
    "text": "a specific a specific id and a specific message to a specific key so deterministic initialization vectors should be likely managed by this requesting device if we now consider a legitimate device communication in this context that we have analyzed we have uh at the left a sender application that uses a kind of security proxy that is a center application a benign one just wants to send some plain text p to a secure to a receiver application at the right and on this behalf it uses a security process consisting of a sender and a sender ckmd so if the sender is compromised nevertheless in center ckmd protects the key material what it does so if the legitimate message arrives at the sender it computes a new initialization vector sensor encrypt request is a plain message initialization vectors to the ckmd which encrypts a message and gives back a cipher text and a authentication tag which is then transmitted to the receiver security proxy that decrypts all of this information so passes cipher initialization vector and authentication tag to the receiver ckmd however please note that this receiver ckmd typically will refuse the decryption if one of these three parameters is faulty meaning if we have a for the authentication that we will not decrypt and as a response we get a p plain text again and the receiver proxy then forwards this message to the receiver application what can we now do or two observations firstly so gcm works similar to stream ciphers that is we can decrypt by encrypting with the same initialization vector and"
  },
  {
    "startTime": "01:08:02",
    "text": "authenticate authenticity verification that's a problem common to other counter encryption modes mapping this generic sequence diagram to an iot infrastructure think of a smart metering infrastructure for instance you have iot devices that are potentially compromisable so you have physical access to these devices they protect the keying material but nevertheless firmware exploits zero days etc can lead to these devices to become compromised and we have not just one device but we have tens of thousands of such devices that work together these devices are concentrated so the traffic from this device is concentrated and forwarded to an edge server that is reachable from the internet and therefore might also be compromisable an idea is just supervised or supervises the communication to this edge server and it is worth noting that yeah passing any keys to this ids is not advisable we know about solar winds sunburst orion exploits so giving keys to to end and and administrative power to ids this is not that good design decision finally this edge server then uses a network ckmd server to pass the encrypted data to it and then forwards the information to an application server what we want to do is to implement some subliminal communication between a smart meter and an edge server without this information being without this information exchange being visible to the outside so we want to exploit and in particular the authentication tag has shown to be uh available for this and want to circumvent the ckmd authentic authenticity verification in the"
  },
  {
    "startTime": "01:10:01",
    "text": "receiver but please note that this communication is bidirectional so we can use exactly the same approach in both directions and the solution is quite straightforward so subliminal sender is compromised subliminal receiver so these security proxies have been compromised they cannot leak the key material but nevertheless they can act uh whenever a sender application or central legitimate benign message that is to be encrypted the subliminal sender chooses an initialization vector encrypts this information and what it does is simply replace part of the authentication tag with the subliminal message to be sent to the receiver and sends this message together with the modified so the subliminal authentication tag to the subliminal receiver the subliminal receiver uses this stream cipher property it simply takes a random string of length corresponding to the cipher text encrypts these data using the initialization vector and gets a cr so a cipher text of the random text and a tag that it doesn't use now unfortunately by xoring the random text and the ciphertext we obtain the cipher and by exploring the cipher with the ciphertext we get the plain text so subliminal receiver now using this encrypt operation has access to the plain text it has from the authentication text it obtains the subliminal message can take an action a destructive action and for what's the message the plain text to receive replication so from an outer world perspective that's absolutely legitimate"
  },
  {
    "startTime": "01:12:00",
    "text": "the only issue is that this subliminal information here needs to be somehow hidden by the subliminal sender can we do better yes of course we can change the entire process just by doing the entire exactly the same process and this hidden information as is exod into the into the authentication tag that is instead of replacing we xor it is the authentication tag and get and get encryption of our hidden information for free proceed process at the receiver is absolutely the same one we get the plain text but wait we do not have the authentication tag we need the authentication tag to extract the subliminal information what do we do we encrypt once more so we encrypt the plain text with the initialization vector get back the authentication tag and then by uh decoding and and also by by exploring the authentication tag and the subliminal tag we get the subliminal message that's it so from an outer perspective without having the key material it's almost impossible to identify that something has happened here this exploit is agnostic to protocol semantics can be used in any context and cannot be destroyed by intermediate nodes because otherwise the legitimate authentication tag is destroyed without leaking the secret key to some some observers it's impossible to identify that the authentication tag is faulty and the capacity is usually high because message authentication codes are typically transmitted frequently some such reports or messages moreover the location of the sender of this hidden information can be anywhere on the path where it can access the authentication text so it needs not necessarily be collocated with the device i'm getting short of time so yes mitigations thank you you have"
  },
  {
    "startTime": "01:14:00",
    "text": "already standardized gcm siv so synthetic initialization vector which solves exactly the problem so it is uh the issue is that we have initialization vectors that can be repeatedly used it's clear that this is this should not happen but it happens in this setup we can generate ivs on the ckmd but this can lead to ckmd originated subliminal channels and we have a deployed mass of of systems that need to be maintained that is we could use distant keys for each direction for what reverse or we could use the initialization vector a segment a segmented set of initialization vectors for each direction and i'm almost done so just uh combining the ckmds with the gcm encryption can show security shortcomings and yes obviously gcm siv is recommended or some some remedies but it's it's a general architectural problem and uh yes one should consider this opportunity uh you have a link over here that leads to the paper if you don't have access please drop me a note and i'm happy to send you a pre-published version on the last slide you have some some information on the project that we're doing and this was a result of this project so yeah happy to take questions and thank you for the opportunity to present okay we are slightly behind on uh i'm sorry now it's all right uh i i locked the queue so uh two questions jonathan first jonathan hoyland's cloud player uh one way you could potentially detect this is in use is surely by having some middleman munge the uh"
  },
  {
    "startTime": "01:16:01",
    "text": "authentication tag right they just mess it up and the the receiver should then reject the message but it won't yes sure there are many so for instance you could also intercept here the network communication yes uh by seeing that you have two encrypt requests instead of one you could identify that something's happening or by logging this receiver ckmd to see that you have a mass of encrypt request instead of decrypt so you should have a symmetry ordinary uh by by default but uh so so we have several counter measures they are they are mentioned in the um in the paper but thank you yes obviously this is there there are some mechanisms but by default so so today's system are rather bad at this it's very cool thank you thank you scott okay yes uh if i understand that you are assuming the ascender and receiver uh crypto engines are both compromised if you assume that can't they replace the the gcm algorithms with anything they find convenient and use that as an exploit why is it specific to gcm they they could in theory uh replace anything so so it was a uh the topic was derived from a rather applied project where we have an existing system and there we have aes gcm and yes in in theory you could replace also some some messages you could introduce some additional information but we we try to stay with the minimum of uh of exploit that is needed to transfer information and in this case just the authentication tag is sufficient and you have of obviously you have external observers too that that could intercept some some changes in communications all right all right thank you um can you please stop sharing slides yes uh"
  },
  {
    "startTime": "01:18:01",
    "text": "okay thank you right thank you very much [Music] next is dual prf construction nimrod can everyone hear me and see the slides oh okay fine hold on uh did you okay i'll just okay all right uh can everyone hear me and see the slides yes yeah all right so hi my name is nimrod aviram and i would like to present a dual prf construction this is joint work with uh benjamin dowling ilan komargotsky kenny patterson ayal hernandez so in modern protocols we usually have the client and server agree on a shared cryptographic secret along with other protocol parameters and then we feed the shared secret and the protocol transcript into a kdf a key derivation function to arrive at the pell session shared secret and from that shared secret we derive symmetric keys that key derivation function should be uh total random that is the output should be indistinguishable from random when the key is uniformly distributed even for an attacker that fully controls the transcript so we can use hmac which is uh probably a prf under very mild assumptions and it would appear everything is well and good [Music]"
  },
  {
    "startTime": "01:20:00",
    "text": "however in some cases we have more than one key and now we're asking uh what should we do here for example in uh tls 1.3 where we use both diffie-hellman key exchange and appreciate key and this happens a lot for example in resumption which is very widely used in hybrid key exchange uh well we have both a classical key exchange algorithm and a post quantum one and in the signal double ratchet protocol where we combine an existing shared secret state with new uh keying material that is output of difficult in all of these situations we use two keys at least two keys and the general approach for doing this is uh to combine the two keys into a single single unified key using a key combiner function and then compute hmac with that key of a say the protocol transcript and have that be the output of the entire kdf uh this is largely done both in existing constructions and in our proposal and when we say that key combiner function takes two keys uh we mean it should be a dual prf that is the output should be in this indistinguishable form random uh when one key when it one key is uniformly distributed and the other key might be controlled by the attacker and this scenario is actually realistic because say in protocols an attacker can replay a key share from a previous session and attempt to learn that r key while injecting new keying material from the uh to the other key and of course"
  },
  {
    "startTime": "01:22:00",
    "text": "we don't know which key the attacker controls and which key the attacker attempts to learn uh so it's then natural to ask uh can we use hmac as that key combiner function and the answer seems to be no because hmac is generally not a dual prf to be fair it was never claimed or designed to be a dual prf under any assumption and it's definitely not one if the underlying hash function is not collision resistant that's where our proposed construction comes in uh so we have a construction for a dual prf uh the construction uses an underlying hash function as a basic building block uh the hash function can be say shadow 56 or any other standard hash function it doesn't even have to be collision resistant for the um for the construction to be secure the whole construction is fully practical it only uses symmetric cryptography and is overall cheap to compute and we'll get to that in a minute uh it's especially cheap uh when we compare it to asymmetric cryptography so if we want to use it in protocols which we mostly do the relative cost for including the construction is minimal and we have a security proof in our paper on e-print how do we compute our construction we take the first key prepend some common reference string to it run that through the hash function and use the output as the key for hmac then we take the second key run it to a new fi through a new function which i will describe in the next slide and expanding injective one-way function we take the output uh prepend some common reference link to it and use that"
  },
  {
    "startTime": "01:24:01",
    "text": "as the data input for hmac then we do the same thing again with the key roles swapped we then take the two hmac outputs x of them run that through the hash function one last time and this is the output of the whole construction note how everything here is uh symmetric and uh standardized cryptography except for the f function which i will now describe um how do we compute uh the expanding injective one-way function f we have a message m and we split it into blocks with the same size as the hash function block size then for each message block we are running through the hash function several times each time while uh first uh processing um an input block uh of an appropriate index so we we first prepared a full input block of zeros right and then the message and we run that through the hash function and we get the start of the output then we uh do the same thing again but with the whole block of ones at the start run that to the hash function and get the second uh we get the continuation of the outputs we concatenate the hash function results uh of all of these computations and this is the expanding function f note that it is also symmetric and cheap to compute uh this in this diagram we we process each block twice but this is only for simplicity in practice we propose our pros processing each block three times"
  },
  {
    "startTime": "01:26:01",
    "text": "uh the amount of times we process each block is called the expansion factor and this is the parameter of the construction uh and what why do we choose an expansion factor of think so what this uh expansion factor does is it helps the function be injective the more we expand the input the longer the output and the higher the chance that the function is injected if we want to standardize and deploy a new kdf this has high potential forcification that is once we deploy something into the field it will be very hard to upgrade and we're still dealing with uh with these situations now so we want to be conservative here uh so we assumed that whatever hash function we use uh will eventually be as broken as md5 is broken today so even with nd5 and coolant cryptanalysis uh taking an expansion factor of two is unbreakable to our knowledge and taking an expansion factor of 3 seems like a good security margin we are also open to taking other values for the expansion factor uh we said the construction is fast um it's much cheaper than asymmetric cryptography again it takes roughly seven microseconds to combine two keys uh comparing that with say hkdf using concatenation uh hkdf takes roughly one microsecond so the overhead is roughly six microseconds and this is very cheap compared to asymmetric cryptography so even just doing diffie-hellman over x255 19 with two exponentiation still connection each exponentiation takes roughly 45 microseconds so just the fieldman is about 19 microseconds"
  },
  {
    "startTime": "01:28:01",
    "text": "uh during ecdsa takes either 80 or 25 uh microseconds and if we want to also use entro that would add some more so the largest overhead we came across uh is uh when doing only two different expansion exponentiations and a signature and even then the overhead is only five percent uh this is the largest overhead uh we know of and the overhead is lower for probably most other uh use cases uh if uh some people find the overhead unacceptable we can consider an expansion factor of two which would make the construction speedier uh what do people use in practice instead of dual prfs uh so we've spoken of tls 1.3 in dp hermann plus pre-shield key mode and of signal which combine keys using hkdf which is equivalent to hvac this would implied we think of hmac as a dual prf even though it is generally not one in hybrid key exchange where we combine our classical and post quantum uh key exchanges uh both in the itf work and in similar proposals from etsy and east we actually uh concatenate two keys and then feed them to the kdf as usual uh and even with only a single key uh in tls 1.3 where we only use diffie-hellman uh that dpman output is actually passed through the message input of hmac which would again imply we treat hmac as a dual prf so we think uh standardizing a dual prf would help make protocols more robust both when with multiple keys and also with a single key and we're asking uh we we would like to write an internet draft and we are asking all are people interested thank you i'm happy to take questions"
  },
  {
    "startTime": "01:30:03",
    "text": "yeah uh chris wood please go ahead yeah thanks i have a two well one question um and then more of a comment um uh the first of which is on the the the claim that hmac or hpdf extract is not a dual prf um uh i'm not a cryptographer so i defer to you experts to determine whether or not that is or is not true um but my concern is that um many of the proofs that i've seen for tls 1.3 do assume that hkdf extract is indeed a dual prf so i'm wondering if you can comment on what the impact of this particular claim is on existing security analyses for protocols like tls 1.3 and in particular the key schedule of tls 1.3 um a second comment is on the the the idea as to whether or not to write a draft um i think having something that uh has like rigorously proven dual prereq properties would be nice however for applications like mls um which require more than one key as input it might be nice if an nprf was the construction output on the other end um i know chris um forgetting his last name um [Music] proposed for mls specifically an nnprf construction i don't know i don't recall what came of that whether or not it turned into an internet draft or there was a paper published or not but i would like to see it perhaps generalized beyond just two key or two inputs um about the first question thanks uh so i will start with the second comment actually uh we can accommodate uh however many keys uh uh is needed uh and we can do so efficiently so we think we can help there as well"
  },
  {
    "startTime": "01:32:00",
    "text": "and uh as for existing analysis of the 7.3 i guess it's complicated [Music] a lot of analysis doesn't reduce to to any specific assumption on the hash function or on hmac but rather to something which is kind of a toy protocol like the prfodh assumption um well i i just uh i pulled up a paper from um who is it sorry scrolling about that actually uh i'm sorry sorry yeah i think we're just running out of time so uh i would say that we have a good an extensive discussion of this on our paper on eprint and uh and we can take it to the list if that's okay uh sure i just wanted to just comment briefly that like the they in various proofs they do use this assumption to bound um uh to make certain bounds on like adversarial advantage and they're in their their hops amongst various games um so uh it does seem it does seem relevant um and i'll i'll take a look at the paper we can try to find yeah and and let's discuss this on a list i'd be happy capitol okay quickly christopher um my question is um why uh i i was curious about like your benchmarks this seems comparable like is this is this comparable to what hkdf does um why compare it to what's the value of comparing it to like asymmetric crypto so we're trying to argue that if we uh add this to a protocol the overhead is minimum okay okay um because i okay okay so i think um"
  },
  {
    "startTime": "01:34:01",
    "text": "i guess i guess the the comparison the the one apples to apples comparison is is hkdf right so like yeah but probably fast hkdf we're in good shape yeah so it's it's slower than hkdf but we think it shouldn't matter that much because uh we only do it once say to process it yeah i totally agree i don't i don't think it's the overhead's insane but i'm just saying that's the benchmark right okay just just want to be clear thank you very very very cool construction by the way oh thank you thank you uh do we have time to from phillip yes uh i closed the queue after philip so philip quickly yeah i i like this a lot um there are some other things if we're going to revisit uh hkdf there were a few landmines that i came across uh when i was trying to use it in the mesh um some other thing some of the assumptions that are reasonable to have as a user um about what certain inputs will do are not actually true and you know that that that just hit me a few times uh in a way there was surprise that should be eliminated the other point anime is you you're looking at hmac i think that that's the right thing to use as far as timing is concerned but if i was going to move to a different key derivation function at this point i would not want to use hmac i'd want to use kmac because you know hmac is a hack trying to make a digest function that wasn't designed to be a mac do that kmac is a construction that is designed to be a mac in itself and i i think it's a lot more principled so i think if we go to standards on this we"
  },
  {
    "startTime": "01:36:02",
    "text": "should definitely have kmac in there as a recommended algorithm and i would like to see the favored algorithm all right uh thanks i'm not familiar enough with the kmac to comment on this and uh also a great hidf yeah it's just using shah 3 in a principled way to do a mac because it's yeah it's all right so and that is not always advantageous but uh i think we should discuss it uh further on the list if you'd like i'd be happy to all right i guess that's it for me uh thanks all right thank you uh if you can stop sharing slides that would be great um how exactly oh oh actually that might help all right fine um okay uh the next one is the aegis aegis family of authenticated encryption algorithms okay let's do that okay um shall i pass controls to you or do you want me to drive i can do it away i can do it myself okay okay let's okay okay so i'll uh talk introduce the ages fast authenticated encryption family [Music] there was an internet draft with first version written last september there is further support from google and also in the team of the two designers of hs hong kong and myself from university of louisville so it's a family authenticated"
  },
  {
    "startTime": "01:38:01",
    "text": "encryption algorithms is non-spaced it's twice faster than aes gcm so on the benchmarks we get to 0.287 cycles per byte family offers a high security level and there is already multiple implementations available including one in the linux kernel a few things about the design i'll be very brief and not go too much in boring details the design was actually inspired by a pelican mac which is a mac designed by the aes designers to be faster than aes because for a mac you can be faster and we then turn this into a stream cipher which is actually also a lot faster than the as so we have a 128-bit key and iv and all the words are here 128 bit but there is a large state of 8 times 128 bits 1024 bits in edges 128 l the scheme is very modular and easy to analyze so you take your key and your iv you do 10 steps of each operation and then for every next step you accept two message word you produce two key stream words which you add to the plain text and you output the cipher text at the end you add some linked information and then you do seven more steps and you get the 128 bit mac value out so it's a stream cipher with built in mac generation so inside the each is operation so there is eight blocks and they're updated they all use one round of aes and they update as you can see in parallel you see also the injection of two message words and then to compute the output we take for each of the outputs four message words and then two are added and two are end and then added to the rest so this is more or less how it works so i think it can be described in a very simple and compact way properties um we propose two variants the 128 l variant has 128 with security against confidence healthy attacks and"
  },
  {
    "startTime": "01:40:01",
    "text": "authentication there is also a 256-bit version that actually have has 268-bit security against key search and we decided to keep a 128 bit tags we have 218 against forgery for the smaller version we allow use of 248 messages per key which i want to point out is much larger than what you can get for the same security level um with for example gcm or ocb for the large version lipsticks there is no restrictions or number of messages in practice but we want to point out that if you would do to the 128 attempts you could do an online forgery attack so you need a huge number of interactions with the verifier and then you could shorten um key search but we decided this was not really a problem a problem for security security properties are that unlike gcm it's key committing and these actually stop search and key partitioning attacks i don't have time to go into detail gcm also has some issues when you're very nonsense you can may get some security problems so this is not a problem we recommend here to have maximum size nozzles but they can be shortened what we don't achieve is resistant to nones reuse we don't also achieve resistance to or you could be in trouble if you start releasing plaintext before you check the mac and we also didn't design the scheme that's compactly committing could be used for example for franking so these last three properties are by design we believe that if you want these properties you have to do at least one full block cipher encryption per 128 bit and you can never be faster than the aes itself and also the same thing the franking would require a larger attack which we also didn't think was necessary so there has been quite some independent security evaluation each cipher was designed nine years ago and was submitted to the open cesar competition there were close to 60 schemes and in the end a handful of those were kept for"
  },
  {
    "startTime": "01:42:00",
    "text": "the final portfolio including ages 128 there has been some independent analysis by other teams there has been some correlation analysis on his 256. we are not concerned about these attacks um of course theoretically it's below 256 but they require between 150 to 160 ciphertex blocks we're not concerned um and i spoke to the people who did this attack they don't see how to improve it they don't they have done some attempts in the in last year but they didn't get much further than a small factor of two or four i'm also very impressing very recently this this month actually and this week at fse two attacks have been published on aegis 128 is actually not the version in the draft not in the draft but because in the draft pf128l in 256 um and they break about five to of the ten rounds with a reasonable complexity this is interesting because it shows independent analysis so this is an attack in which you vary the iv with chosen iv attacks but we also want to point out that if you take four or five rounds out of ten of aes there is also quite efficient attacks so we think this is normal that in the as base scheme if you have only four or five rounds that there is an efficient attack this is not a reason at all for concern it will be very hard to scale up those attacks as for aes and finally performance the scheme is highly paralyzable online for encryption and can make optimal use of the aes instructions i'll just show you some benchmark numbers on the next slide um so intel skylake as you see on the bottom the speed goes up a lot this is cycles per byte so the smaller the faster uh once you go below one above 1k messages i zoomed in on the top and there you see that it's about twice as fast as gcm it goes down to the number i promise you point to something to five cycles per byte we have similar results um"
  },
  {
    "startTime": "01:44:01",
    "text": "for arm which i will skip and i conclude here now so saying that each is a very simple design that is a variant for 128 bit increasing bit security it's also easy to analyze easy to implement and it offers a very high security level the design is targeted for platform with aes support but would also be implemented efficiently if there is no such support and so there has been quite some scrutiny and so far all the attacks give us confidence is really highly secure cipher thank you very much all right have any quick questions comments christopher uh uh no no question really i just wanted to say i think aegis is a beautiful construction and i would love to see it standardized so i absolutely support the work uh cfrg working on a draft thank you okay armando hello so i i just want to know how does it compare with other lightweight ciphers well each is not exactly lightweight it's a high speed design there is a nist lightweight competition going on and we didn't submit it there because we didn't think it fitted there it's really more like a high performance design and nist has announced that they will complete the competition soon but there is no results there um so i think in general it's definitely one of the faster designs there have been some more recent papers trying to do further optimizations but i think those would definitely need another five to eight years of analysis before they can be adopted but there is there is extensive benchmarking available i only showed you"
  },
  {
    "startTime": "01:46:01",
    "text": "but it's clearly it's also faster than ocb and it's faster than any other cypher that has this kind of scrutiny okay thank you um uh thanks for the presentation and perhaps i missed it um but i'm wondering if you can comment on the um sort of non-story use properties of this particular variant so each is is vulnerable against noise release by design um it's better than gcm in gcm with one nonce you lose your authentication key and you have a big problem uh in aegis it's clear that there is a paper that was actually in our written design document the details were not worked out but the paper has shown is it needed 15 users of one month and then you can recover the state now this is a design decision we believe that if you want to be resistant against this you need two passes or you need lower performance we don't think you can achieve this performance for an on-to-use scheme so we warn inventors implementers for this is also clearly pointed out in the draft that you have to be careful just as for gcm that you don't reuse your nuts and luckily the consequences are not as bad as for gcm okay thank you but do you have any requests from for cfrg is it for our information for now or do you want this to be adopted i guess this is i guess we wanted to introduce it at least because i'm not sure it had been discussed before and i think it's we seem to get some good feedback we would like to get feedback and of course if there is consideration for adoption we definitely would support that all right okay thank you all right thank you very much let's move on to the oh"
  },
  {
    "startTime": "01:48:03",
    "text": "okay then okay hold on i will just drive right yeah please okay thank you uh we're just okay there we go rock and roll uh uh i'm dan harkin so this is a proposal for uh making some uh changes to hpk or additions to hpke so next slide please us it was recently published as rfc 9180 and i saw i think it's a good time to make this ask again uh the issue is that hpk is is a really nice construct and i like it a lot but it doesn't work for some use cases for instance for devices are operating in a constrained environment uh the serialization and deserialization for the uh nist keys is over twice as long as it needs to be and so i'm proposing that we could uh address that by using the rfc 6090 compact output serialization another issue that uh i have with hpk is that it is it doesn't work on lossy networks it assumes that there is guaranteed and order delivery of of all the packets that are being used and uh that's because the sequence counter that's being used with the the aead cipher is completely inside of the context then the the user has no ability to to know what the sequence number is or to have any control over it so if there's any uh packet loss or packer reordering uh the center and receiver get out of sync and everything just falls apart uh and since the internet does not provide guaranteed or delivery of packets i think that this is a problem we should we should address so for the first one it's pretty easy to we can solve that by just assigning some new chems that do a compact serialization uh but for the second problem next slide please i have a couple of proposals one of them is uh we can use a deterministic aead and not worry about the nonce if the the nonce is causing problems then let's just use"
  },
  {
    "startTime": "01:50:00",
    "text": "something that doesn't care about a nonce like uh siv uh now of course siv has problems uh uh regarding some of the security of of the deterministic mode uh but you can achieve semantic security with siv if the plaintext carries enough probabilism that the nonce normally would have provided in a a normal aed scheme so for certain situations where you can have that guarantee i think a deterministic ead mode of hpke would be acceptable if not uh what we could do is use a rolling replay window uh analogous to what was done with ipsec and rfc 2401 where you've got a bitmap of received uh received packets and that allows you to receive them uh out of order and uh some can get dropped uh packets that are way too old will get thrown on the floor and the the window advances as you you uh you verify receive packets of course this requires access to the sequence number the receiver has to know what the sequence number is to determine whether it's valid in the receive window or not so that would require us to export the uh four octet sequence number uh into the ciphertext uh this shouldn't be a problem because uh that information would already be down to any sort of attacker who can count and we're not going to be exporting the uh the secret knots we're just going to be exporting the four octet sequence number that gets xored onto that thing so next slide please so what i'm asking for is uh three things i want to add new chems for the nist curves that do compact serialization i'd like to add support for aes siv as a deterministic uh cipher mode in hpke and for the situations where aes siv would not be appropriate on velocity networks i'd like to uh have a defined way to use uh a rfc 2401 style replay window to deal with uh packet loss and reordering next slide please uh"
  },
  {
    "startTime": "01:52:00",
    "text": "i do have a draft it's in the 01 version uh please take a look uh and i also have running code if you want to take a look at my github uh there's a hpke wrap it's fully compliant with rsc2401 it handles all of their test vectors but i also added uh additional test vectors to do compact representation with uh new chems and uh authenticated and deterministic authenticated encryption with siv i just basically stole a couple of values from the the reserved number space uh but hopefully that won't be a problem uh so please take a look uh next slide please so we have running code and i hope uh we can get rough consensus to adopt this as a work item okay stephen all right see a couple of comments uh one is there's with 9180 when i combine all the cams and kdfs and so on i have 480 different combinations of algorithm so there's already a lot however these seem like reasonable changes um the other comment i have is maybe hpk should be thought of as moving from cfrg to an ietf context because the perhaps the part where we needed cfrg expertise is in the past and these kind of changes seem like more engineering changes than cfrg like changes i i'm not i wouldn't be too focused on it but just for you know things like you know changing the um the points representations that's like a yeah that shouldn't require you know so you're saying dispatch me to sex dispatch basically no i'm not i'm not recommending that it just occurs to me that that might start to make sense because maybe the you know maybe the cryptographic bits of this are are mostly done and it's more about kind of"
  },
  {
    "startTime": "01:54:00",
    "text": "engineering at this stage fair comment um and then i actually have one question is so does your code now include the kind of uh x2559 no okay so it's it's just the nitch curves for now still yeah okay cool uh well otherwise i i'd be supportive of this happening i i'd probably be a little bit happier if it didn't happen immediately uh i don't know i don't know i have to think about it happening is doing it sometime yes maybe here maybe elsewhere okay all right thank you okay chris uh yeah thanks dan um uh i also think that these are perfectly fine um additions um well the the the chem uh with the different uh compressed format is a fine addition i'm a little bit concerned about the deterministic aad and the implications it has on how you use it um so i prefer uh well i i'm less less supportive of that um but i guess as i met a comment this is our question this is not directly to you but more for the chairs i'm realizing now that we don't really have established any designated reviewers or experts to review these ayanna registry editions the the registry policy for editing anything any new chem aad or kdf is specific specification required which doesn't require this to be published or adopted or anything it just requires a document to exist and that approval from um one of the experts uh so do we need to establish experts to review these requests so that we can get dan's algorithms in the registry so yeah typically what happens is ayanna asks for experts on first request so i suppose that's kind of constitute the first request so yeah can you"
  },
  {
    "startTime": "01:56:00",
    "text": "email chair as a reminder to just double check and investigate this specific question okay sure sure um yeah for example as well yeah but this would be the specification that didn't would be required right i mean the specification required there has to be some published document yes yes but it doesn't have to be it can be like independent stream or itf stream or doesn't even have to be an rfc okay i think right richard i don't know which is slightly more controversial but you know some people tend to think that's okay i'm well well if there is enough support to do this it seems like it's a relatively simple and smallish document so we should just um already done like you know extra lms extensions for example i i think without consulting with micro chairs um i think the bar for uh reasonable extensions is a bit lower okay so if there is enough interest let's let's do that um christopher yeah i just wanted to say that was specifically why we went with specification required for the policy so that you don't have to go through the whole process of publishing an rc just to get something in the registry to make it easy um to add new things so less work overall for everyone but i will i will email the chairs um and see if we can get the designated experts set up okay thank you okay said it um i think that's we had another christopher in there didn't we uh he just took himself off all right um great we have three minutes left and"
  },
  {
    "startTime": "01:58:01",
    "text": "chris uh sacrificed one of his presentations who basically used up all of this time so i don't think there is a point unless you want to say a few words very quickly yeah thanks um uh yeah there's definitely not time um try to give it just a brief summary of what i was going to talk about um uh it was mostly going to be a reflection on you know the documents that the cfrg is producing right now um and has produced in the past and their sort of uh importance in the community um in particular the itf community and how they use cfrg specifications for specifying protocols conducting security analysis and whatnot um trying to point out that there are um there's probably a number of ways in which we can improve as a group um the output of the documents in terms of clarity consistency and uh and correctness um and it's kind of a plea uh for you know people to um for volunteers who are interested in like sort of the editorial production of these documents to um uh brainstorm things that can be done to improve document quality be it with respect to the pseudo code that's produced the terminology that's used in the documents you name it and up won't go in the slides or anything but i'll try to summarize uh and send a message to the list um because i think there's there's something we can be done that can be done here um and um that's it thank you thank you very much all right and with this we have one minute left which i give back to you thank you for coming and uh see you online and hopefully again next time"
  },
  {
    "startTime": "02:00:03",
    "text": "at least hybrid meeting maybe even fully fledged one thank you goodbye uh yes yes"
  }
]
