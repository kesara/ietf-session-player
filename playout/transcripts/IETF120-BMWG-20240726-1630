[
  {
    "startTime": "00:01:03",
    "text": "Made it there and back in time Not yet. I don't know if she is going to connect Rosanna, if you're ready me, shiatsam-jish"
  },
  {
    "startTime": "00:02:09",
    "text": "Thank you wait a couple of minutes and then we can start We tried and from every direction It's arriving from everywhere It's just for test for people that is remotely. Can you hear me? properly? Please reply on the chat Okay it looks like it's working. Maybe I was not too close to the mic"
  },
  {
    "startTime": "00:04:00",
    "text": "Just one more minute to see if my co-chair will join, otherwise I will just start Thank you Okay I think we can start. Welcome everyone We are in the benchmarking methodology working group session. If this is not the session you are looking for, you have to go in another room I am giuseppe fioccola, one of the chairman of this group and if you're not subscriber to the BNWG main list and would like to be, just visit the website on slides and please join the main list Okay, these are the not well page that I get you are familiar with this, so by participating in the IETF meetings you agree"
  },
  {
    "startTime": "00:06:01",
    "text": "to follow the IETF policies, proceed and processes so if you are aware of some IETF contributions covered by patents or patent applications, you must disclose it As a participant in an IETF, or any IETF activity, you acknowledge that written audio video material may be made public and as a participant or attendee you agree to work respectively with other participants as well We also have the note really well, so it means that IETF meetings virtual meetings, and many lists are intended for professional cooperation and networking The IETF want to maintain an environment in which people of many different backgrounds and identities are treated at the same level with dignity, the essence and respect and if you be believe that one of these conditions are not met, there is you can notice that you are encouraged to raise your concern Some meetings tips, so make sure to see into the session via the Datatracker or the QR code For in-person participants, join the mic queue if you want to participate and make comments and keep your audio and video off for the on-site version For remote, make sure that your radio and video are off unless you are presenting or sharing of course"
  },
  {
    "startTime": "00:08:01",
    "text": "And we need no take care so everyone is welcome so okay thank you Paolo to volunteer volunteer In any case, you can find the not taking tool on the client, on Datatracker, and you can help with the notes Okay, let's move to the agenda We have four working group drafts to update The first one on multiple loss ratio search. The second one on young data model for network tester management including the hackathon part the hackathon result presentation then we have the consideration for benchmarking networking, network performance in contained infrastructure from Ming-Gong and then the recently adopted document on benchmarking methodology for segmented audio then we will move to the proposal so the first document is the characterization and benchmarking for power in network devices The SRB6 service benchmarking guideline and then we moved with benchmarking methodology for source address validation Then we also have a presentation about benchmarking IoT devices from Anna This presentation for now is not related to a draft, but it may be a document in the future An update about the status of the document As usual, reviewers are needed, so your worker is adopted completely and published when the participant of the working group read the review and share the comments on the list"
  },
  {
    "startTime": "00:10:01",
    "text": "so it's a two-way street We are two dogs documents that can be considered in preparation for the working group last call are multiple loss ratio search and the young data model for network tester management that are quite stable. Maybe the author will give some updates about the camera status and what they think about We have ongoing proposals that have recently adopted about the consideration for benchmarking network performance in container infrastructure and the benchmarking for segment routing Some proposals are coming We already discussed the last IETF, the benchmarking for power in network devices DSRV6 service benchmarking guide and the benchmark, a new work that is the benchmarking methodology for source address validation Regarding the activity, later we have the benchmarking methodology for stateful not-ex-ipsed gateways in RFC Editor Q So it will be published soon, hopefully New working group adopted a new working group document adopted. So the benchmarking methodology for segment routing And I think that's all we plan to update the milestone according to what we will discuss today and after the meeting relating to the working group plus call of the of sound documents so I think we can start now with the first presenter Masheko Obratko. Yep"
  },
  {
    "startTime": "00:12:00",
    "text": "I'm here and I will be presented on behalf of both of us Do you want the control of the last? If you don't mind Giuseppe, could you drive the slides? Yeah, sure of course thank you go ahead I don't see anything sure shared Can you see the slide? I don't see the slides Anybody else sees the slides who is remote? In the room, we can see the slides. Thank you Okay, why don't I? see those slides? on my, oh, there we go, sorry. Okay guess the screen was, um see the slides on my, oh, there we go, sorry. Okay. I guess the screen was not adjusting. Thank you, Giuseppe And hi, everyone, welcome And so I will be presenting on behalf of Radco and myself on the specific updates for the 07 draft Next slide. Please So unfortunately, we missed the submission deadline. So we posted this past submission after the reopening on just on some Sunday morning at this UK time or midnight Saturday And that's because we took a bit of the ambitious task to address any and all comments, firstly, audio check. Can you hear me okay? Yeah, we can hear you. Yeah, a little bit if you can speak a little bit. Hi comments firstly audio check can you hear me okay yeah we can hear you yeah a little bit if you can speak a little bit high louder yeah slower okay i'll i'll try to do uh try to do"
  },
  {
    "startTime": "00:14:01",
    "text": "we can hear you. Yeah, a little bit, if you can speak a little bit. Hi. Louder? Yeah. It's slower. Okay. I'll try to do louder. So we, what? i'll i'll try to do uh try to do louder um so we uh we took an ambitious task to address um all the comments we received from the working group in meetings and on the mailing list going back to the very original version of the draft 01 from 2019 and focusing specifically on making the MLR search specification more approachable, readable and readable and clear and specifically focus on the being very clear with the terms and definition and the non-formal definitions and specific references to the associated past BMWG RFCs So namely RFC 1-242 to 2 non-formal definitions and specific references to the associated past BMWG RFCs. So namely RFC 1-242, 2285 and 2544 And we're also making references to the TSP 009 at C document focused on NFC So that's really what took us a bit a bit longer And we believe with it clearly us a bit a bit longer. And we believe with it, you know, the best we could. We believe the draft is now quite well written and clear in our view, so in author's view And we believe we address all the comments ever raised in the working group and we believe that it is ready for the last for the last call Next slide ever raised in the working group and we believe that it is ready for the for the last for the last call. Next slide. So what are would like to cover during this a few slides is just the highlights on the on the of the draft, starting with problems looking at the specification areas, and then coming back to the work status Next slide, peace So we have one of the big comments we got very early on is to clearly articulate what problem we are addressing and why this work is"
  },
  {
    "startTime": "00:16:01",
    "text": "required to amend the content of the previous RFCs including RFC-2544. So I identify those five problems and I think they came also from the discussion with in the room on the mailing list and also in the corridors or chat rooms. So we believe that all of those five are recognized by the about the working group and so going quickly for them, you know, make the search duration shorter by finding a sooner the interesting region and recognize the fact that focusing on the software networking we do have DUTs, so the software networking applications that share resources within the within the SUT. So instead of focus on a single throughput characteristics, or attribute or quantity, look at the spectrum, performance spectrum and then go after the, what we believe is the fact of best practices in the industry make sure that the software networking benchmarking results are repeatable and comparable between the labs and between different implementations of the test, test, test and recognize the fact that apart from this zero frame loss, people are also quite often measuring the throughput with non-zero loss And then because we are repeating trials at the same load, due to sometimes the need and also we're searching for multiple goals, we want to be sure that the method is consistently and deterministically handles the inconsistent trial results So these are the five problems we are after and we believe that the methodology proposed and specification does address all of them. Next So I'll quickly go through the specific"
  },
  {
    "startTime": "00:18:00",
    "text": "that basically consists of the number of term definitions and existing and new or amended and then finishing with your compliance, compliance aspects. So this compliance aspects so this three sUT the dut trial they've been defined already in RFC 285 and 25 specification compliance aspects. So this three SUT, DUT trial, they've been defined already in RFC 2285 and 2544. So they'll just call out that we're using the definitions without any deviations whatsoever Next slide Now the next two slides talk about trial terms and here um we're not necessarily redefining those terms but we are further, you know, we jared mauch more specific about what they mean in the context of a benchmarking software networking applications. So the trial duration trial load you know, these are intended duration duration intended load as configured by the operator We then deal with the number of components metrics or quantities in the case, you know, trial input is the composite of the trial duration trial load and traffic profile includes other aspects related to the load like frame sizes versus profiles, and such Now, because of the problem space, we are addressing, we want to be specific about, you know, things like ratios, rates and so on. And we want to have them for them to be formally defined as we are finding definitions in other documents not specific enough at least for the use cases we are we are after So here we have, you know, number of trial terms, trial forwarding criteria loss ratio, and forwarding grade"
  },
  {
    "startTime": "00:20:01",
    "text": "They're related basically to the amount of frames forwarded versus lost and versus, you know, expected to be forwarded. The trial effective duration is really by default equal to trial duration, but there are some discussion points that are covered in the draft when that is not the case. And then the trial output and trial results is basically a composite values of of the ones listed above. Next slide So in terms of goal, one of the aspects we find hearts to articulate clearly and using a formal language is, you know, as we are dealing with multiple goals like, for example, the, you know, the as we are dealing with the performance spectrum, we wanted to make sure that the attributes or the quantities that govern those are very well defined So, so hence, there is a number of different goal quantities that are now defined in the draft and we're using those goal quantities to actually illustrate, for example, full compatibility unquestioned unconditional compatibility with RFC-2544 and also with TST 009 and the V testing. And that will be covered in about two or three slides. So we have a the goal final duration duration, so equivalent of RFC-544 trial duration, which is as the recommended 60 seconds The goal duration sum when we are repeating trials at the same load, you know, we need to track the duration some, and there are some formulas governing that And then there's the loss ratio and exceed ratio, which gather in base load, you know, we need to track the duration some, and there are some formulas governing that. And then there's a loss ratio and exceed ratio, which gather basically what the goal what the attributes of the specific goal are if we are going after multiple goals"
  },
  {
    "startTime": "00:22:01",
    "text": "As the name of the draft and the specification says, it's a multiple risk ratio search. So we're searching for multiple things and go with governance precision Next slide so once we define those, we then actually can use much simpler language to describe the specification and the functionality required for the systems that are compatible with them in our search So here a search goal suddenly becomes a composite of multi- attributes that basically and some of them, a number of them are required some of them are optional and right now we only have one optional goal with, but we are looking for feedback from the group if there are other optional attributes that should be added to the specification. But the main ones that are required is the final duration duration, direction, some loss ratio, and exaggeration. You will see how we illustrate the use of them in the following slides, specifically referring to the existing standard And then we have the controller input parts also in the definitions that is that is basically a list of search goal instances. So this is what the operators provides as a configuration for the system And next slide So now let's look at a, I think, a territory that everybody in this group should be comfortable with. So RSC25 So how is MLR's search? unconditionally compliant with RSC-544? Well, it is enough to, by using this mandatory or required attributes to the search goal to configure them as follows. The goal final trial duration is 60 seconds. Goal duration sum is six seconds, which means basically a single trial at is the final trial at 60"
  },
  {
    "startTime": "00:24:01",
    "text": "seconds at the right load The zero frame loss is based at is the final trial at 60 seconds at the right load. The zero frame loss is basically expressed here as a goal loss ratio zero And because we have a zero tolerance to any repeated trials or loss then the goal exceeds ratio is zero. And with those parameters set the behavior of a similar search is the same as RFC-2-4 binary search. Next slide When we look at TST 009, focusing on NFV benchmarking, work done by late Al Morton, a number of the teams from our OPNV in NETC we, you know, the team there recognized that there must be that there is a need to repeat trials in case there is a loss in some trials and and to make those results more or measurements more repeatable and comparable So in our case, to really be fully compatible again, unconditionally with TST 009, and so using this example, R-equal 2, where R is basically a number of repeated trials, and in fact, there is a bit of the type of here should say max are equal to as per quote in the footnote With the goal final trial duration to 60 seconds, goal duration sum to 120 seconds and then goal ratio to zero and then to be complete with TST 009, accepting one lossy trial, goal exit ratio at 50% With that, we are compliant with this, MLR search is compliant with TSD 009 Next slide In terms of results, relevant upper bound, relevant lower bounds, really what people will care about is throughput. In our case, because we have set of conditions attached to this report"
  },
  {
    "startTime": "00:26:01",
    "text": "is a conditional throughput and it's basically a forwarding rate at the right lower bound for the search, for the specific search. Next slide so Yes, what are the results? So this is a goal result and the search results So basically it's a composite set of the results following the set of goals And at the controller output is basically a composite equity of quantity of of of two. Next slide so in terms of the architecture um really MLR search architecture consists of the three main components, manager control measure measure uh measure the trials, controller basically drives the measure and makes decisions about you know, the loads and such instructing the measure and the manager is responsible for conferring the components pre-configuring the environment calling the controller with the configuration, inputted by operator and of course creating the test report And the details are captured in the in the draft so the final slide of the specification next slide is compliance so when we're proposing that to make a networking measurement setup to be compliant with MLR at search specification, it needs to contain the logical components as listed in the spec, so measure, controller, manager and they have to be basically satisfying the record requirements as specified in the draft. How the specifics of the implementation are out of"
  },
  {
    "startTime": "00:28:01",
    "text": "scope for the draft, but the requirement on measure, controller, and manager are within the scope of the draft And next slide So with that, work done at 07, I think we are in a position to state that the draft in our review is ready for a thorough review by the benchmarking work group, including any nitpicking And yeah, we would like to propose to go ahead with the last calls subject to any concerns And I think it was the specification is, I think, the best we can produce And if there is a further hardening required, we would appreciate the contribution and collaboration with a working group One thing to say is that we developed this and we've been collaborating in the working group focusing on software networking workloads and applications but it equally applies to any network devices, of course And as highlighted in the previous slide, with a specific configuration is fully compliant unconditionally with RFC-25444 and TSD 009. And there's one more slide just for reference Giuseppe, if you could just one more, some links So we've been, MLR search has been in use in Linux foundation networking in FDIO system project for multiple years now and it's been over the last year or so, it's been stable And we are currently benchmarking VP DPDK applications, and also T-Rex as a test"
  },
  {
    "startTime": "00:30:00",
    "text": "generator itself And the other reference is really here to highlight that just one of the recent publications that is focusing on software networking benchmarking in the cloud, in this case and they are referring to FDIO, but they also or the the author of the GCP block runs the benchmarking Rick in the GCP Cloud And, you know, there is a recognition that there is a need for a benchmarking methodology suited for software networking. So with that, the final slide is a thank you slide And I think when done and open to questions. Thank you, Masha Yeah, any question? Maybe I can join the Q. Okay, I just want to say for thing as a chair, the next time please submit the draft in time with the submission cutoff of the IETF On the other hand, as I participant, I just want to ask regarding the controller because in the architecture there is a controller remind me, I didn't read the last the last version because you submit just a few days ago it was too close to the meeting for the controller you don't suggest any specific technology I mean um netcom young For the controller, you don't suggest any specific technology. I mean, NetCon, Fian. No. So you leave it open to the implementation. That's right. Okay That's right. It's a pure, pure separation of functionality to clearly delineate the responsibilities of different functional blocks over the system in terms of executing a trial, executing"
  },
  {
    "startTime": "00:32:01",
    "text": "a set of trial to hit a certain goal and making the load select logic being in the manager a certain goal and making the load selection logic being in the in the manager and sorry in the controller and so on so there is no specific you know the technologies itself and and specific implementations are not really in scope. It's focused on the, it's focused mainly on the logical functionality. Logical blocks with associated functionality as defined in the draft. And we're trying to make those definitions formal. So not subject to interpretation and as unambiguous as possible, if that makes sense Okay, thank you. Gene? Yeah, uh, Masick, actually, thank for presentation. Actually, very impressive you already implemented in the, you know, federal this kind of open source project. And I haven't read your detailed draft, actually, but my question is you know maybe related to the you know that you have his question is about controller is, what is the scope of this job is focused on you know benchmarking methodology, or it's or you want to define some main architecture, seems that you introduce some controller, so so whether this is a architecture should be in the scope this job? can you clarify a little bit? Yeah this is actually a very good very good very good very good question The real reason why we introduce that the the architecture section is to clearly delineate the logical components and their functions within the MLR"
  },
  {
    "startTime": "00:34:00",
    "text": "search methodology And that is mainly for explanation reasons. Now, the text actually goes into the you know suggesting that the implementations may choose to merge those those functions into you know under the difference split as dictated However, the way that we are specific the requirements is based on those logical functional blocks. And if an implementer decides to implement it into the one, functional block, but there is a clear delineation in terms of how those functions are implemented then it is, it is compliant But it is it is questions like that that we would like to really expose to the word group and make sure that the non ambiguity that we we're trying to reach is there and we basically need more eyeballs and and benchmark focus minds to make sure that this is this is this is this is this is also correct, so it's not confused to anybody else I don't know whether Bradco you want to add more to this because it is something I've been discussing for quite a bit. Yeah, yeah, thank you for clarification but you know for this amendment architecture i suggest maybe you can draw a figure to make it you know it's more uh give the people the whole picture and if they understand, you know, how benchmark methodology can be used in this kind of main architecture Okay, we'll tag the feedback. Thank you Okay, if there are no other comments I just want to give a reply about the last call, the working group last call. I will discuss with Sarah and maybe after the IETF"
  },
  {
    "startTime": "00:36:00",
    "text": "meetings we can follow up with you and with the group. So with the decision but I think that it's it's quite stable so maybe we can consider it okay Shepa thank you very much so be looking for but I think that it's it's quite stable so maybe we can consider it okay Shuseppe thank you very much so I'll be looking forward to to hear from the chairs on the mailing list, and we take it from there We are very eager as we've been working on it for a while we're very eager to progress this and also address any concerns and also collaborate with the work group to harden the specification Okay. Thank you. Thank you, Master Thank you. Maybe, Giuseppe, one more thing We would really like to get feedback from the users, the testers, and so on but also from the tester equipment vendors. I think that there was a comment in one of the previous IETFs from Carson, Rosen Howell. It would be really good to get reviews from the testing vendors who are participating in the MWG. So we would really appreciate that. Thank you. Okay, okay good Okay, let's move to the next presentation, Vladimir Hi, can you hear me? Yeah, very good. All right So, I'm ready for the next slide Yes, so starting with the hackathon results, we did manage to synchronize the implementation with the latest modification of the draft and the changes which we didn't manage to make the last hackton presentation were added and tested We did make a sandbox device that we announced four days before"
  },
  {
    "startTime": "00:38:01",
    "text": "today. So if anyone was seen interested to connect, it was possible to connect to the public IP others and with the credentials that we specified and we posted a notification about this on the mailing list so it anyone who has some interpretability orchestrator could connect and validate that the device can actually send packets and receive them and this can be done with their software I don't think there was many that we saw two connections that were trying to configure something from the presentation. So this is like the example one that contains the credentials and the IP others and that's what we send to the mailing list So if you go to the next slide can I go to the next swipe myself anyway? Yeah, I can give you the control if you want Yeah, so without me interrupt you. Yeah, nice. So this is our setup. This is what we managed to get going and we tried Just one thing, Vladimir, I see that you have 33 slides, so consider Now this is containing some slides that were presented already, so we know so consider... Now, this is containing some slides that were presented already, so we're not going to spend time on them. There were some links that have been updated. So it's consider that you have 10, 15 minutes. Yeah spend time on them. There were some links that have been updated, so it's... Yeah, consider that you have 10, 15 minutes, so... Yeah, it will be over in five minutes. Okay, so this is this is our setup, nothing dramatic, just the minor changes that we had to validate so I can go to the next slide and this is the slide that we presented on the ripe 88. We were allowed to present the work that is going in"
  },
  {
    "startTime": "00:40:00",
    "text": "attempt to find interested operators mostly because we know the equipment makers are a bit of reserved about new standards and new interface they all have their proprietary interfaces and we were hoping to get some interest in the Ripe conference, and we did get some interested and this is like what we had to get started there so which two of the five network testers below have standard based? measurement interfaces and protocols so this like a good introduction and that's what this was is all about So I'll go to the next slide Yes, so as you know, this is what we are focusing on, the network test management solution and there are examples of existing solutions. As you know, you can configure test generators with command line interface which are partially standardized, like the protocol is standardized there but the commands themselves are not. So you have this 30 years old GPIB interface, and that's pretty much the only standard interface Cisco has made some new new ways to write tests with CISCO T-Rex and implemented It's most of the people testing SDM are familiar with that Keyside is a big player, which has recently released this open traffic generator API based on Rest and some proprietary data models have been made open but they're not based on Yank, especially the configuration is not based on Yank And then you have a spirent, which"
  },
  {
    "startTime": "00:42:01",
    "text": "made a significant work on the HLT API, which is very complex very heavy API for writing almost anything. So there are also other interfaces which are existing So we are doing this in Young and we are doing much simpler if you compare with the the focus of HLT API, for example, we are only focusing on that this what is minimalistic and allows you to do level two, level two example, we are only focusing on that this what is minimalistic and allows you to do level two, level three tests and nothing more specific and flexible So I'll go to the next slide Yeah, I added these two slides a part of the Ripe conference, which contains a list of why I think this work is important and this is like a test and measurement instrument automation wish list. This is like something that we have been thinking about for a long time before we submitted the drafts we have an interchangeable interface, something that you can change the drafts, we have an interchangeable interface, something that you can like change two different test equipment manufacturers and they run the same code and get the same results This doesn't exist today in network It exists in instrument, though It's like ivy if you have been used power supplies osteoscopes and things like that, there is an IV interface which allows you to control two different instances with the same interface so we just use Young andy newton to do that, same thing. That's measurement equipment vendors have been standard 20 years ago without using programming language specific"
  },
  {
    "startTime": "00:44:01",
    "text": "model. They are using program language specific models, so they are standardizing C++ classes, for example. And we're using Young, which I believe is better and it will pick up even in the measurement equipment device vendors So the list is there. If anyone is interesting in this can check it and comment on the list if they don't agree with anything or they think something should be added So yes, this is the second slide And I added also two slides about the current wogy of the evolution in the network test equipment standardization and implementations. So this starts from the 1965 where HP developed the first interface for configuring test instruments. And go further into networking the internet port protocol is published and yeah, it goes through Cisco that requires TOF and young becoming industry standard in networking so yeah it's the detailed list of what I think is relevant to this work that we're doing now if someone is interested and wishes to comment on that can send them to the mailing list. So this slide has been already presented It just has updated links where needed and some information is updated as well. So I'm not going to spend your time on that, but you know it's there and you can check it out. If you have been using any of the repositories they have been changed. This is like probably the the most concise implementation of a complete RFC to 2544 benchmark. It's 407 lines of Python. So this can be a useful thing"
  },
  {
    "startTime": "00:46:01",
    "text": "if someone is working in RFC-25 RFC-2544 it can check this implementation because it is really a proof that it is an elegant benchmark specification if it can really be implemented in 40 lines of Python codes. And the code is based on the standard the draft that we are working on to make a standard RFC So it is probably the best thing to start if someone is considering reviewing the draft So this is like our report partial report hooks from this command line line It's added for reference This is the device that we are using. It is based on an open source FPGA and open source even the enclosure, is open-source and publish Here is a quick reminder of the young tree diagram which we have presented already just for reference for people who do haven't been on the WAS session. This is more detail view of the implementation in hardware to get that And yeah, I think these slides have been presented on the last IETF session. I just left them so that we have a complete presentation that can be useful for someone who hasn't been following with the work during the last four years So I'll just go quickly over that We did some work on stimulation and this draft is very useful for simulating network devices, because it is contrary to the other network programs interfaces for traffic generators"
  },
  {
    "startTime": "00:48:01",
    "text": "it is concentrating on like just specifying the the network programming interfaces for traffic generators, it is concentrating on, like, just specifying the train with packets and not involving any protocol semantics. So, firmware designer doesn't really care about the protocol He just wants a sequence of packets and valid that his transceivers are not generating any bit errors. And we're not the only one who is doing that. Spirant is doing a cooperation effort with Cadence So they have made a portion of their model, the proper proprietary interface they have integrated in cadence so it is interesting side to have a standard way to write tests because you can pre-Ciricom test the hardware that is being developed. And if it's in Yankee, it's even better, I think so these are some slides just illustrating how we simulate and how we use NetConf communicating with simulated hardware to configure transactions and value them. So that's all We are working for people that are interested in trying out this if they have if they are operators, it would be of interest We are also interested, of course, with harder vendors if they decide that they should introduce this new API as part of their products And everyone who is experienced in testing, can look through the draft and say if there is something missing so in a conclusion I just want to say that the draft has been stable There have been a lot of things removed and what is remaining seems to be"
  },
  {
    "startTime": "00:50:01",
    "text": "what is the base and we don't know if more is needed if no one else finds use cases that are not supported I guess this is ready for last call but it would be nice to wait a bit and find out more people that are actually using it and reviewing it, since there is no big hardware vendor that is behind the draft Thank you, Vladimir. Any comments? from the group? Yeah, please, Chin Hi, Vladimir Mia. I don't have noosey sick. Yeah. So I, uh have no, actually, you have been due this work for quite a long time. Actually, you mentioned Cisco you know, already support these kind of feature. I'm just so wondering who will be the main consumer for this worker. Is any other vendor willing to, you know, support this kind of worker or also? for like maybe in there's some multi-vender interoperative test they call the intact actually there's a lot of tester company like spring or key site so is any this kind of player to really interest in this kind of work? It did like this industry standard companies producing test equipment two years ago were for it was Teledyne it was Xena for Denmark, Kisite and Spires were the biggest and now they are there too because they bought each other. So now is Kisite"
  },
  {
    "startTime": "00:52:00",
    "text": "and Teledyne remaining and they have been working on this and they have very extensive models for example like this model that we are proposing is based on the Young configuration They have been working on other approaches and it is of their interest to keep their customers using their models, I guess So they have not been into the discussion, although they have all the competence to improve that work So if they join that work, it will be much better draft. But there is significant difference between what they have and what we are proposing We are obstructing away all the protocol details and allowing the users to use open source packet Posers. Like, there are open source packet comps that allow you to generate your pocket put it in that sequence that is going out, and not use any preparatory resources to do that. And on the opposite, these companies have like extensive support for for example, they have IP and IPV6 and all the details that fall out of that. So they have introduced that as integral part, even of the lowest level So in our model, you just specify the content of the packet and you have the freedom to specify which fields of the packet are going to change but you're not introducing any protocol semantics so it is a draft that is much simpler and it doesn't go obsolete if a new protocol is introduced. You just have to generate these templates yourself So it's very much minimalistic"
  },
  {
    "startTime": "00:54:01",
    "text": "and it's something that should be of interest But we haven't had any direct input from this companies. So I wonder also if KeySight and Teledyne is willing to join this discussion So your question, the answer is that they haven't said anything to us and they haven't commented anything on the IETF mailing list. So if you can generate interest and hear from them, that would be no nice. Thank you for kind of communication I think, yeah, this company probably takes a different approach. Yeah, but it seems your solution more companioning yeah thank you Just one thing I wanted to mention, they have in the worst two years, they have released the open interface key site has released it open API, and they have even included young model, which is only for reading status information. So I think they're they're working at what is happening with this standardization effort, and they try to position themselves so that they are closer They know the value of young models and they have introduced young models for the first time Last year happened that So I'm also following what they're doing in their repositories So if I see something useful that we don't have, I will definitely, if it's open, and they declare it's no problem, I will propose it to the working group as well Yeah, thank you. Thank you Beladimir. Okay, let's go continue the discussion on the list And yeah, as you mentioned, for the last call we can wait a bit to have some more inputs, but of course you are next after the previous drop"
  },
  {
    "startTime": "00:56:01",
    "text": "Thank you, Vladimir Thank you Yeah, let's move to the next presentation Mingong, okay Yeah, hello, go ahead Good morning, everyone. My name, Immingok, and I present the consideration benchmarking network performance. Speak closer to the mic Benmasking in coordination infrastructure trap Please go to the next slide. So quick introduction about the document again So we have previous NFPA band benchmarking I see that's a very much about an quick introduction about the document again. So we have previous NFP benchmarking IFC that's very much about NFC IFC. I've seen 8172 and 8204. The gap of these work as they haven't considered continuing right infrastructure So the scope of this document is to fill in the gaps of the previous work when applied into a continuum of the infrastructure and the specific consideration graphs are the different network models and topologies configured by Cotagon Network interface and research configuration for continuous that's show in the list below. Next slide, please So this is a the whole development of the trap so it starts from 2019 And the first measures update of the draft come in 2020 2023 so we have received review from the five and Get Project from Lineup Foundation and we agree on five continental NEPOS models and four resource cooperation in consideration And the next one is on IETF roni even we update on based on the command in 116 so it's a major"
  },
  {
    "startTime": "00:58:00",
    "text": "updates are the resource configuration benchmarking parameter are mentioned and we remove the benchmarking appendix. And then on 118 meeting, then we once again update beyond the command in the working group, we provide a clear scope section and we remove duplicate containerized infrastructure contents and uh the draft is a job at the working group draft on 199. So the update here as a the environment set up repeatability guidance for all content networking models and since then wen lin this in this version one it's just a minor editor change. Next slide, please so version one, it's just a minor editorial change. Next slide is. So, we, in this meeting, we would like to call that if anyone have any suggestions or additional change for our working group grab, because we have received anything from since the previous time and at this moment of the is dead thing that the drop is pretty quite stable and ready for the last goal so I would like to hear the comments from others Okay. Any questions? comments, feedback? Yeah, I understand your point and I try to push more reviews from the list in order to move this forward because you also recap about the history of this draft you made several hackathons and so, yeah but we need more review and comments. Did you try to contact the authors of the, the"
  },
  {
    "startTime": "01:00:01",
    "text": "NFV-related RFC? to to see if they are willing to give some feedback? Do you have enshr? Maybe, I mean the NFC 8172 and RFC 80204, maybe they can can be interested to review this is a first that seventy two and r fc eighty two zero four maybe they can can be interested to review this is a first attempt to do suggested yeah i think we should do that Since they are already working to related the NFM nfvvc so maybe they are willing to review this work as well That's just a suggestion as a first step But in any case, I will push the working group to review that Chin, yeah go ahead. I think in addition to use the working group, results, I think have you considered maybe propose some Hekisson project to maybe introduce your project to get them more, maybe some feedback or comments to identify some you know, issue, maybe operation issue or some other issue and so maybe that's a good approach here. Have you considered it? have a hexon project for that? I think we have done a lot of hackers from before. Yeah, they already did If you look at this, the draft development, it's been for four years of hackathon before we finally have a review from the lineup condition position okay and maybe you can I don't know if it can be valuable to organize another Akathon for the next IETF to just to summarize the tests and to maybe to familiarize with more people and get more reviews I don't know it's maybe an additional an additional"
  },
  {
    "startTime": "01:02:00",
    "text": "suggestion You should consider. Yeah, you can consider, but in any case, you can also contact the other NFB related authors of related RFCs to get some review Okay, thank you. Let's move to the next Paolo, yeah So, good morning, paolo volpato away. As usual, I'm presenting on the half of the other autos you see listed here. So basically this presentation is about the as a scope of providing the update on our draft which is for the benchmarking method for segment routing. So very quickly where we are right now Just to introduce the scope of the draft for the of you that maybe are not familiar, the scope deals with defining a standard method to benchmark in the segment routing packet for working capability of a network device In doing that, we, let's say, rely on and extends the work done on the list of the RFC listed here, so it's not just the scope of the benchmarking test, but also the benchmarking of specific capabilities such as IPV6 or MPLS said that probably you know that you're aware of the fact that the draft was adopted by benchmarking working group just after the past ATF So first of all, thanks to all who's supported the adoption, who received quite a good amount of comments and feedback So we are quite happy about that. And"
  },
  {
    "startTime": "01:04:01",
    "text": "you remember that the adoption took quite some time because there was a discussion with to merge the two previous drafts Maybe you remember there was one specific on segment routing MPLS and another one on SRV6. So after quite extensive discussion, we decided to merge the two of them. So basically we are dealing now with just one single draft deal with the benchmarking of both variants of a segment routing after adoption we uploaded version 00, of course that was back in June, and we also uploaded a new one, so we are discussing right now version 0 version 0-0, of course, that was back in June, and we also uploaded a new one, so we are discussing right now version 0-1 because we decided to deal with some remaining comments just after the previous ATF and in view of let's say ATF 120 so that, this is the status I will move to the next slide, please, Joseph what happened so far? Let's take a few minutes just to discuss the remaining comments because I believe this is part of the work that we will do in the next few months before IETF 121. First, there is a remaining comment from Minok and also echoed by other What about the let's say, the testing of the complaint? seed? So we are fully aware that this needs to be discussed in the draft. We have done a first analysis which is not exhaustive i would say, but just to, let's say, understand the amount of work to be done. We believe that it is not a complex task to introduce the discussion around compressed seed, but we need to touch every single test because we need to insert this specific description"
  },
  {
    "startTime": "01:06:01",
    "text": "on what to do in case we have a compressed seed As Edward one of the co-authors often says, it's like having kind of third data playing in addition to SR&PLS and SRV6 to deal with. So we need to again review all the list of the tests and see how and where we need to introduce the discussion as I said for sure this will be part of the next review, so for sure in Dublin at the ATF 121 we will discuss it. In any case, feel free to provide your view and your feedback on that then there is another comment which was raised by louise as you can imagine we have internal discussions sometimes among the authors Luis is also active with a draft in spring, which is about the security of SRV6 That is a personal draft So for the moment, I will say even if we will consider that as a source of input for us own, we have time to this with that. In any case, the question was, is there anything? concerning a service 6 security that? we should also consider in our draft? now again we have done a sort of preliminary analysis At the moment, our feeling is that the two drafts are sort of orthog So we don't believe the reason At the moment, our feeling is that the two drafts are sort of orthogonal. So we don't believe there is something that we should consider in the scope of the benchmarking of the forward capabilities of a network device but we'd like to have your feedback. Maybe there is something that could be considered. We are not completely sure at this moment, as I said, and apart from that, there is a question that may be a"
  },
  {
    "startTime": "01:08:01",
    "text": "like to anticipate to the chairs and to the AED, which is sitting here. So it's something that we can maybe start discussing and then move on the list In case we see there is something specific some features, some capabilities associated with security, a service six security So let's imagine that there is something. We are not sure, as I said Something that we could consider for benchmarking and we maybe identify the possibility of preparing a new draft new contribution. Is security something? that we can deal within benchmarking working group? So is it in scope? I'm just asking, there is no need to have the answer right now but you know, it's something that, you know, the discussion triggered the questions so I'm passing to you maybe it's something you can give us, let's say, recommendations or suggestions on how to move on on Warren kumari, Google. I mean, I don't know It depends on what exactly you're asking if it's doing benchmarking of the security part, like, you know, Canada on what exactly you're asking, if it's doing benchmarking of the security part, like, you know, can it do this number of packets per second when inspecting something, then that's probably an scope. But if it's like the security of does it actually do what it says it does, or, you know, the security of the benchmark itself, probably not. So right Okay. So I think we are aligned on that Okay. Next slide, please Okay, then there is a let's say the editorial review that we did in the past few weeks, so we had some refiner of the language, and while doing that"
  },
  {
    "startTime": "01:10:01",
    "text": "review, there was also a doubt, let's say, that we shared once again in the group of authors which is about fragmentation. Now we tried to refine the language here because there was a doubt mainly raised by Luis and Bruno Again, moving back this discussions from Spring. So it is clear that for SRV6 we don't have a fragmentation. So the transit devices cannot do that So that's clear. But the doubt triggered back in your discussion what about SR MPLS? And in turn, that was about the fact that maybe an IPA packet could be carried by SRMPLS frame so we we discussed that internally and at the end with specified that hopefully in a clear way that fragmented basically is not in scope of our draft in any case. So we're not considering fragmentation of BVC pv6 because that is forbidden by 80 but at the same time again, the noise of the three. Closer to the mic. Yeah, probably so we decided to specify that even for IPV4 packet carry on SRAPLS, that is not in scope of the benchmarking test So that maybe it's better I move to the conclusion of my presentation because for the people on the are connected remotely, there is a road works outside so there is the noise of the drill So next steps, as I said, for sure we will discuss the compressed seat cases That will be the subject of the next review then this is once again a question that probably we will also forward on the list We had private talks"
  },
  {
    "startTime": "01:12:01",
    "text": "basically with people that told us some tests are ongoing using your draft, but those are private tests, so we cannot reference those tests in the draft. So if anyone is doing or performing some let's say publicly available or let's say activities that could be referenced in the draft feel free to contact us. It's a reference we'd like to insert in the draft And then for sure, the remaining next step, probably for the next day IETF, we will try to cross-post the draft on the other two service related working groups, meaning spring for sure but also the new SRV-6 Ops working group because we believe that some of the experts in segment routing are there so we like to start receiving also their feedback so that thank you again for supporting the adoption and we are open to any questions Any comments? input? Then see you just want to say, yeah, it makes sense to involve segment routing experts to to review this draft especially if just want to say yeah it makes sense to involve segment routing experts to to review this draft especially for the remaining open issues that there are in the draft. Speaking as also echo out of that that, yeah. Okay, if there are no comments, we can move to the next presentation we have finished the working group documents Yeah, we can move to Chin Okay"
  },
  {
    "startTime": "01:14:00",
    "text": "Go ahead Morning everyone. Actually, Carlos is not here, so we have Carlos and other author to, I will present this worker And this is not a new job. We already, you know, it's a second time to present And the title is caricaturation and benching methodology for power in networking device Next. Okay, now I can change because I didn't plug in Yeah, a little bit of background capture actually and as we know, you know, we more focused on, you know, network operator network, you know, currently in this networker because though, you know, exposed growth over the network traffic and also also some new technology should be used as so the equipment energy consumption is, you know, a big challenge actually so so in addition actually you know some of these energy consumption energy you know to be consumed by equipment is on unnecessary is wasted so therefore more and mental operator, you know, more thinking for the automation tools and a solution to better access and control the energy consumption for the large network And so we see actually technology trend and efficiency management and this really, you know, allow that operator to offer optimize energy usage. And in addition, they can, you know, improve the overall utilization So the benchmarking methodology we proposed in this job actually can be seen as the important tools and can help vendor to test energy efficiency for their devices. So this can be made of"
  },
  {
    "startTime": "01:16:01",
    "text": "for example data traffic per unit and you can sum vendor to test energy efficiency for their devices. So this can be mayor for example, data traffic per unit, energy consumption across the network. So in this job, actually we set several objectives Actually, we more focused on objective one and also object two and you know, for objective one, actually usually know, to assess, you know, which part of the in the network which Thank you device is consume most energy or which system can perform the best. So we can target to a set of well-defined scenario, for example, you can set us on traffic load as you know with the different traffic load conditions condition and with some, you know, temperature or some power setting so you can test this kind of network device to see, you know which device can, you know, perform the best and which device consume more energy second the objective is trying to measure the contribution of sub-system this subsystem could be the component within the network device, for example, MPU, or the power module, or optical module. So we can measure this component to the overall system, the energy efficiency performance So in addition, actually, I think we also can cover the third objective to measure the contribution of the sub-system to the overall power consumption, in other words we can measure the power proportion So one of example, we can report a percentage of the power to be consumed by the power module or optical module. And so this give a sense of you know you know you know consume so much energy in the orbit be consumed by the power module or optical module and so this give a sense of you know you know where to consume so much energy in the operator network next so the document that update and this is, you know, already you know present for a second time actually previously we introduced this worker in IED environment workshop before the IET119 and"
  },
  {
    "startTime": "01:18:01",
    "text": "we see a bunch of the comments and all we present in last IETA media and thanks for all the you know commentator for the comments actually we think are very useful for example the Louis actually gave a suggestion He thinks we should consider more realistic scenario the current work away actually make some assumption, you know we just focus on, you know, to calculate the total installer capacity of oil interface so this may be not represented the the real deployment scenario, so we will you know, cover more deployment scenario cases in the next version and also during the environment impact workshop, you know, in last item meeting, actually several suggestions also useful for technology such as not only the magazine we propose can be working for the router or switch, it also can be worker for some other equipment, like, you know, power distribution unit or um uh or some power supply, actually. And Alex, raised a very interesting question, you know asked us to consider how long it takes for transition among different traffic loads and level. So this is something we haven't solved through actually whether we need to propose some new metric or we need to add some, you know consideration to set some constraint how we can do the benchmarking test. And, uh, uh, the latest version is zero actually, we make a minor update We know we add an section and we move a bunch of reference to the informative reference section. Also made us some editorate change and but the the comments raised in the last time meeting, we think could be the good input"
  },
  {
    "startTime": "01:20:01",
    "text": "and can be you know uh uh uh uh incorporated in the next version but the current version actually we also believe actually it's pretty much a good basis already, actually. Next cover a little bit of relation with the green bubble deliverable actually as we know, actually, this ITA week we already you know you know have a green bulb and it goes well. And so this proposed green working group is targeted to explore the information to be exposed using young data models so they cover two type of information one in a static capability parameter so one of the examples is we can you know to model like the power interface static parameters like you know nominate voltage or the power setting support by a power interface, but this is not in the scope for this benchmarking job. And in addition, actually this young model also tried to expose like a dynamic metric information So typical example is the maximum throughput traffic load energy efficiency ratio. This is already covered by the benchmarking draft and for some other use for metric way, actually discussing during the green bulb actually is about, you know, power usage effect some other useful metric, we actually discussing during the green bulb actually is about, you know, power usage effectiveness and a cool load factor power factor that can be used in the data center network and we think this more focus on network level not, you know, device level, so probably not in SCOWIN in this job, but for the, you know, for a third body actually we you know, explore like a paragon any proportion per component more focus on the power proportion actually the typical example, for example, power gain, actually this can be determined by you know by the actual power consumption"
  },
  {
    "startTime": "01:22:01",
    "text": "divided by the maximum power consumption. So it's kind of device level metric Another example about energy proportion is you can report the percentage of the optical model So we think this metric metric, you know, could be you know, is a new metric we can develop a benchmark method for this metric could be, you know, be the new metric we can develop a benchmark method. And so actually in the green working with China, you know, did describe some of you know collaboration effort this green will collaborate with like other work group that, you know, might have related workers expertise with the defined standardized metric, or management framework So benchmarking working, you know, has been listed one of the related working group could provide some of you know foundation or some kind of automation tool. This can be leveraged by green to develop a young technology some of you know foundation or some kind of automation tool this can be leveraged by green to develop a young data model but i i think for group working with the young model, they focus actually not tied with these, you know, benchmarking, what can they could, you know, report some real time you know, metrics using some sensor, you know, to be installed on this specific component. So this report some real-time, you know, metrics using some sensor, you know, to be installed on the specific component. So, so this, you know, the, some there's something, yeah, we think it related to this bench benchmarking work next for next time, we think of this really, you know, provide a, you know, use automation tools and foundation for the green work can be first automation tools and the foundation for the green work can be further, you know, build on top of it and develop a young data model And also I actually have some discussion with some you know expert from Yintech actually like, you know, actually they are very"
  },
  {
    "startTime": "01:24:01",
    "text": "interesting in this, you know, power benchmarking they want to, such as maybe to have some modern interoperative test in the future if we have this kind of work already, actually, I think we probably can such as maybe to have some modern-vander interoperative test in the future, if we have this kind of work ready, actually, I think we probably, we can encourage more vendor to, you know, to, you know, to together to perform this model vendor interoperative tests during the in tech in the future so we think it is a good basis already cover the basic metric related energy efficient ratio at a device level and but we do decide for some open question because we can consider some new dynamic explored by the green working group so we think of some of this metric at a device level could be used for, you know, so we may consider actually maybe, you know, to you know, consider the paragon energy proportion per component, these kind of metric at a device level. So we can develop a benchmarking methodology for this metric and probably this need to be rotating in the next version. So that's all from my side Yeah, happy to take comments this need to be rotated in the next version. So that's all from my side, yeah, happy to take, you know, comments. Thank you, Gene. Any comment? Yeah, good-chun for all Huawei. So I have a question here All this seems to be device level are related, but there's also some energy efficiency related to you infrastructure level, such as poor supply conversion units from AC to DC, DC to AC so on and so forth and also for the air conditioner related to the infrastructure, there are also energy efficiency related. So will we cover this kind of features in the future? Yeah, this is good question actually"
  },
  {
    "startTime": "01:26:01",
    "text": "Torres also, you know, have a similar comments. He's single with another just a limit to the router switch you know, in the operator network We may, you know, cover, you know, some other equipment type, could be the, you know, power this unit or or HVACA related, you know, equipment in the data center network And, but if we, you know, extend to explore at the device level, I'm not sure in the scope of this bench benchmarking working group probably the chair can give some guidance or because we more focused on device level metric. Now, services are also in scope so the is also in scope okay In terms of benchmarking, yeah yeah probably yeah I think we can consider yeah to these kind of network network metric service level match yeah thank you Yeah, I don't know if it can fit in one document, but let's let's think about yeah sure actually too many metric actually yeah you can see them how to know if it can fit in one document, but let's think about it. Yeah, sure. Actually, too many metrics. Yeah. You can see them. Any other comments? any other comment yeah i just want to thank you about the update also with relation with green boff that was very successful I also attended and maybe some we can in the future discussion it will be good to cross post also the green mailing list for this draft to have some feedback from the experts that I yeah this could be useful but do you think a green and working good we do have expert has some expertise, for example, IPPN? they may develop some new metric do you think they should also post to the green? for now in IPPM there is not related to draft on metric maybe"
  },
  {
    "startTime": "01:28:01",
    "text": "in the future there will be so in case of course yeah so agree agree the working crew review where we have four but then I think this will not a title with you know green this actually provides some useful tool. So the green working working can know build on top of it yeah yeah sure no it's just to i'm to give to receive some feedback and some more feedback if in BMWG there is no uh expertise on that so maybe there are some experts that are not aware of this work in BMW and it's good to be sure that they are aware and they can share their review Yeah, it's just to, to get more reviews Yeah, I agree, actually. Actually, talking with Carlos and other problems, actually, they believe this is already good basis for adoption yeah and so and i do agree actually we need to have more eye on this draft to have course review, maybe not limited to the green, maybe RBPN working guru, or maybe we can request some, like, a performance main director review, right? so this will be helpful thank you Thank you, Gene. So, yeah let's move to the next So yeah, let's continue the discussion on the main list for this dog 60 Hi, hi, Jesus. Can you hear me? well? Yeah, go ahead Yeah, so will you help? you know, to handle the slides? Do you want the control? Yeah, I can pass you the control Yeah, I can try"
  },
  {
    "startTime": "01:30:00",
    "text": "Please consider that we don't have so much time So yeah. Yeah, OK. I will make it very brave. Hi this is Shueson, and I will give a presentation about ISO6 service benchmarking guideline representing other orders Actually this one has already been presented in last night IETF119 and after the presentation we have received some feedback from the working group and the most important thing is that there is an opportunity that in R.C. 25 it has already defined a number of tests to describe the performance characteristics of a network device, which is widely used for a single device While when we look into the requirement from the service from the service provider, and the vendors, we notice that besides the single device benchmarking network level benchmarking is also very critical especially the service provider can use it as a valuable reference point for on net testing. And that is very essential for both service providers and a vendor. So we are wondering whether it can be considered that similar tests and performance characteristics as it has already been defined in ERC-2544 like throughput, latency, and the packet line read could be defined for network level rather than only single device. I think there will be a lot of differences if we want to extend the benchmarking to network level"
  },
  {
    "startTime": "01:32:01",
    "text": "The most obvious one will be the topology The typical topology for a single device benchmarking is like the figure in the left hand, there will be a tester, there will be one device under test But for the network level benchmarking, at least three devices should become considered under the test and it will be a network that includes PE nodes which connect to the tester directly and also P node which connecting to P-E nodes So if we take SR6 as an example, in the existing working group work, as Paul has just presented, there is a very good base about SR6 batchmarking for a single device. Although the doctor is not only for SR6, but it have already gave a very detailed definition about the specific report parameters for SSI including the number of segments considered in the seed list and also the behaviors and flavors used for XI S.R.6 tests So, yeah. Do you want to take a question now? Yeah, sure. Okay as you prefer, it's the same warren kumari, Google. I should probably have waited for the end for this but you might want to also discuss some of this in the SRV6 Ops working group I mean, I know it is specifically about benchmarking, but because it involves more than just, you know, a single device having a good view of what all"
  },
  {
    "startTime": "01:34:01",
    "text": "sort of environment should be considered might be useful or what all deployment scenarios Yes, I think this is a very valuable comment. We will try to bring this work also to SRA6 ops, working group next time, maybe. Maybe after this presentation we can bring it to the main list first Yeah, you can always cross post both mailing lists because of course also this also apply to the other document on segment routing so since the expertise is in another working group is 12 is always good to to involve the relevant expert that are supposed to be in that working group go ahead yeah okay so based on the existing work about the single device benchmark, we are trying to propose the benchmarking of SRV6 service campus capability, which means that different types of service could be transported through SR6 network If we take SR6 as an example, the PE1 here can be used as SR6 ingress node, and the PE2 here can be used as S-RV6 egress node and the P node here can be treated as SRV6 endpoint or transmitting Um, so I'm sorry, uh, so because the SRV6, services can be divided into two classes basically, one is for best effort services and another for traffic engineer services. And for different types of services can be transit through the SR6 network including the internet service, Layer 3 service over SR6"
  },
  {
    "startTime": "01:36:01",
    "text": "also EVPN service over SR6 So all these different scenarios can be considered in the topology which we have just shown And actually we have already bringing this discussion into the main list. We also have received some good feedback if a working group think this is the right direction in the next version, we plan to end this test topology with multiple nodes into our document and also bring the existing performance characteristics parameters have been defined in our ERFC-2544 try to adjust the single device parameters into the next level benchmarking in this document And also we are looking for more feedback from the working group Thank you. Thank you, Shretzang So yeah, any comments? Right. Yeah, again from Huawei, I have a quick question here. So my question is that one may makes it different considering the network level comparing with the device level test, what decide? if there's a multiple devices across what decides the final results beyond the device level capabilities? As I have mentioned, the most significant difference is about the test topology, because it will be much more complex to consider multiple nodes in one test. And the example I have read in the previous slides is just"
  },
  {
    "startTime": "01:38:01",
    "text": "includes three nodes but that is the most simple case especially for some other complex case For example, the protection more PE nodes will be considered in that scenario So for this more complex test scenario, then the parameters, the definition of the parameters, all the metric for the benchmarking, should also be, you know, redefined compared to the, you know, the parameters only for one device So considering all the aspects, we think, maybe for the service or next or network level benchmarking, we need something new rather than just reused existing mechanisms that have already been defined Okay, thank you. Let's continue the discussion on the list and on SRVC ops. Yeah, sure Thanks. Thank you Thanks Libin Yeah, I'm on Okay, me? Yeah, we can you just one point. Since you are the last one, but then we have also an invited speaker Can you please introduce your drafting? five minutes? So. Sure, sure I will make it a sample. Because I noticed that you are more than 10 slides. So just to make sure that you will focus on the important management sure sure thank you Can I contrast slides by myself? Yeah"
  },
  {
    "startTime": "01:40:01",
    "text": "I can pass the control to you Okay, sure. I can communicate Okay, good morning and I'm the opinion I'm Libyan from Zhongguyen Lab. Today, I will present the new draft benchmarking methodology for source or address validation So, I very interesting a draft in the following seven aspects Okay, so let's quickly review the background or the draft as we know that attacks based on source are addressed both things, such as reflars dedalves and farting attacks continue to present significant challenges to Internet security So mitigating these attacks in intradomal and interdomain networks requires effective social address validation And BCP, H, FOPP, proposes some solve solutions such as a ACL-BIC ingress filtering and URIPF UIPF-based cell mechanisms. And the operators are suggested to deploy these some mechanisms based on their network scenarios Also, the introdom and the interdomen soundnet mechanisms are proposed in their sound network rule to solve existing problems of the cell mechanisms in value validation accuracy and operational overhead So for this draft, the motivation is that proposing a standard benchmarking must methodology important for evaluating the performance of intradoma and the interdome and the interdome method proposing a standard benchmarking methodology is important for evaluating the performance of intradoma and the interdumments are a mechanism of very. For network operators, the benchmarking methodology can help them to get more accurate idea about the performance of the cell devices in their deployed network environments For device vendors, the benchmarking methodology can help vendors test the performance of their cell implementations. And also the benchmarking methods, can guide how to evaluate whether there are new intradom and the interdome mechanism"
  },
  {
    "startTime": "01:42:00",
    "text": "can satisfy their design requirements proposed in the intro domain and the internal on that problem statement draft draft Also, there are two objectives of this benchmark methodology, including assessing which some mechanism perform best and measurement the contribution of sub-systems of cell systems. For the cell device, under test the benchmark aims to test performance of individual cell devices including the hardware or software routers So for the test methodology, the DOT is connected to other network devices to construct a network topology. Since the location, where the DOT resides in the topology affects the cell accuracy The tester can be connected to their duty directly or by other devices And it can generate traffic to test us the sound accuracy. And the tester can be connected to their duty directly or by other devices. And it can generate traffic to test the cell accuracy, control plane, and data plane performance And also the network traffic generally by a tester should specify the traffic rate, the proportion of spoofing and the legion of traffic, and distribution of source addresses Also, the device in the network's power have various routing configurations and the generated solos of the device the DOT is also affected by the configuration of the other devices in the topology Also, we have proposed six cell performance in the case including the proportion or improper blocks proportion or improper meets, and the particle conversion time, and the protocol speaking age processing throughput, and the data plan are saltable refunction rate, and the data plan refraction rate rate and forwarding rate. And we put a processing throughput and the data plane, south table refunction rate, and the data plan forwarding rate. And this draft proposed the benchmarking"
  },
  {
    "startTime": "01:44:01",
    "text": "tests to test the introdomen and the interdomens cell mechanisms in terms of cell accurate control plane performance, and the data plan performance And for these benchmarking tests use different self performance indicators, as we pointed here And for the cell accuracy of intradomber and interdomen sound magnanim, the objective is to measure their sound accuracy of their DOT to process legion motor and spoofing traffic in various intradomal and interdomen scenario We have listed the scenarios in detail in the drought and for the intradomal style, it includes the scenarios of cell for customer or host network so for internet-facing network, and cell for aggregation router-facing network. And for the instrument, networks, the scenarios include the cell for customer-facing ads and the cell for product or peer peer-facing-S Here, we list two test cases for Introduce-Dermen-S cell to test their cell accuracy Test case 1 and test case 2 are for different scenarios Test case 1 shows the introdomen symmetry routing scenario and test case two shows their introdomen asymmetrical routing scenarios But for the detailed test cases, you can read a draft, we can discuss it in the mailing list as well. And we also list the comfort plane performance and the data plan performance uh, test cases and test set-ups of the interdome interdomen style. And we also define the objectives and how to test their data plane and the control plane performance and how to calculate their information indicators as well as our other"
  },
  {
    "startTime": "01:46:00",
    "text": "protocol convergence perform time and the protocol speaking agent processing throughput and for the data plan performance how to we also define how to calculate the data plane self-table refreshing performance and the deep plan forwarding performance as well so also for the security considerations we will like to test the DOT in a library environment The network is also the lab network is also isolated from the production network to guarantee their test traffic on the not be forwarded to the production network. Also, okay we would like to seek feedback and comments from our work group and also this is Washington 00, and the collaborations are welcome Okay, thank you very much Thank you. Thanks a lot for being quick and thank you for introducing and for proposing this new draft in BMW Any quick comments? So if not, please review the draft and comment on the list Thank you, Libin. Thank you very much So now we have a presentation from we have a presentation from Anna. This is not related to documents, but it may become a working group document, an IETF documents So Anna, please well hello hi Joseph thank you very much everyone Anna please well hello hi josepa thank you very much hello everyone it's should I introduce my myself? I'm Anna Maria Mandalay, Assistant Professor University College London. And since 2018, I worked in Internet of Things, privacy and security, and in particular benchmarking privacy and security for Internet of Things Today I'm going to present"
  },
  {
    "startTime": "01:48:01",
    "text": "a work that is going on since three years now And hopefully, like Giuseppe said, this can be become like mostly today I'm going to present a problem statement more than a solution and hopefully as Giuseppe said, this can become like a drafting future Do we have control on my slides? Yeah, I can pass you the control. Thank you Okay. So we know that any objects nowadays can be connected to the internet of from smart speakers, but also devices that we had in our seating And the main problem is that these devices have sent on them so they can sense the environment, and most of the time we saw that these devices are sending traffic across the internet sometimes and encrypted and for this reason leave the users vulnerable Not only this, but the are also concerns about privacy destinations contacted by these devices, third-party tracking and advertisement, and in fact, nowadays many regulators are like in many countries, are establishing regulations and framework for studying these devices and particularly like try to contain the privacy and security issues that can come from them At UCL, we have established one of the most advanced Internet of Things Tasper in the world and we have the same devices in also Imperial College and Imperial X. And there is a similar laboratory in Northeastern University in Boston, that are our long-term collaborators So this research started as a sense in 2018 and we bought hundreds of Internet of Things devices and we tried to understand what are the primary and security issues with them just a we bought hundreds of Internet of Things devices and we try to understand what are the privacy and security issues with them just by sniffing the natural traffic. We have any kind of devices from smart speakers to cameras, even freeges"
  },
  {
    "startTime": "01:50:01",
    "text": "And what we can do with this test is emulating the activity of the devices So we sniff all the traffic from these devices but we also test them in a way that we are able to trigger the activity through the companion apps of these devices in a completely automated way and we are also able in emulate privacy and security attacks in a control environment So we sniff all the traffic on the router well in this case is represented as a router, but in reality, it's a big server giving connectivity to all the devices It's mostly Wi-Fi, but we also have some medical Internet of Things devices that are connected to the insulin pump, for example or mobile phone through other communication technology, like for example Bluetooth. So we are also able to sniff the traffic from Bluetooth in our thousand. One of the works that we did lately, so this is one, just an example some examples of how we benchmark internet of things devices and some nice results that we had Here you could see how we tested smart televisions for example, for a particular technology that came out in November 2023 that is called automated content recognition for which smart televisions like Samsung or LG take screenshots of your screen, every X-me milliseconds. So we are able, we were able in this case to emulate the users of the television watching videos for some time, and in the same time logging in and opting out to our television account or various privacy policies, for example and then test what happened using the national traffic to the traffic of our televisions We notice that basically this kind of technology is not only used when you're watching some video, but also when you have the television"
  },
  {
    "startTime": "01:52:01",
    "text": "connected via an HDMI cable, for example that is a privacy concerns. And most of the time we notice that this is not written in the privacy policy so this also you have the television connected via an hdmi cable, for example, that is a privacy concerns. And most of the time we notice that this is not written in the privacy policy. So this is also another concerns. Another bunch of tools that we developed in our task plan is benchmarking Internet of Things security safeguards. So there are a bunch of commercial devices that you can buy nowadays that claim to protect your smart device in your home and they claim to do any kind of activity from dogs attack recognition to open scanning attacks recognition, PII, person identifiable information recognition etc. We studied the method so we implemented the methodology. This was a paper that has been upset in one of the top security conference and we were able to develop a methodology for which we could trigger the attacks from the IoT devices to the safeguards through this configuration that you see here in the figure. So we basically put the devices then an IOT bridge between the devices and the devices and then another IOT bridge gateway between the safeguards and the Internet We discovered that we can emulate in our test but various security tracts, network security tracts, like, for example, anomalous behavior, open port, with password, but of also spoofing, device quarantine, dose attacks, ports scanning, regarding privacy, DNS over HTTP, testing DNS over age also spoofing, device quarantine, dose attacks, port scanning, regarding privacy, DNS over HTTP, testing, DNS over HTTP implementation, and encrypted traffic of persons identifiable information exposure This is just an example of one of the many methodologies we developed for testing these devices. So we usually have various steps. We simulate a threat, wait 20 minutes, check if they safeguard to detect the threats, and then we do repeat this 30 times. If at least one"
  },
  {
    "startTime": "01:54:00",
    "text": "time the threat wasn't if one in one time the task, the was identified, then we see that the self-guards is able to detect the threat So we put like a lower bounder And this is just an example of the many emulations and methodologies that we use for testing. In this case, safeguards, but also for testing like the reaction about benchmarking Internet to things devices themselves and then behavior against security attacks Well, the results were interesting we found the majority of the safeguards are not totally due behavior against security attacks. Well, the results were interesting. We found that the majority of the safeguards are not actually doing what they're claiming in their web page, so they're and we're talking about big that company like for example Be Defender, FSec, Ur, Vaira, we discovered that the majority of them are not actually able to detect their attacks or if they do, it comes after like three minutes, sometimes, 30 seconds so when it's already too late. This is why we are in implementing a system that allows to do monitoring and benchmarking of Internet of Things devices in the home router And the motivation for the and the problem and the gap that we determine was that the majority of, so there are many nowadays cyber security guidelines, some requirements that are issued by the initial by the needs, regarding particularly internet things Devices, consumer in some cases, but we're talking also about Internet of Things Devices in critical national infrastructure like, for example, water control, etc So in any country nowadays, we have a sort of regulation These are like cybersecurity guidelines, but they are converted in regulation nowadays. In Europe, we will have in 2026 the cyber residents act and the NIST, the NIST, 2. So all these regulations need to be enforced in some way, but also the developer, the manual manufacturer, let's say, for example, a developer of an internet,"
  },
  {
    "startTime": "01:56:00",
    "text": "a manufacturer of an manufacturer of Internet of Things devices in Europe won't be able to sell the device in Europe if it's not certified against this cyber device Things devices in Europe won't be able to sell the device in Europe if it's not certified against these Suburacy Act requirements or any other GDP in the case of privacy or any other cybersecurity regulations for digital services So we developed a methodology that allow us to convert what are the requirements for the regulation and standards, but the main problem is that at the moment, regarding this regulation, particularly the Sabresians Act in Europe, there are no standards So this is the main gap that I wanted to hide like today with you and then hopefully find us solution with this draft. So the idea is that you take the requirements from these regulations and then using it, you can turn them to some metrics that using the natural draft will be able to understand that we're active and passive tests. I gave to you an example of the kind of tasks that you can do for safeguards, for example, and then the define experiments and tasks to extract the metrics on Internet to Thinks Devices, and in the end you will produce a sort of certification of a sort of like a sort of like attestation saying that you, the devices is actually required under the requirement of the Savaristence Act of the regulations that we have just an example we did like a super simple task in which we took some of the requirements under the Sabresidents Act like number of unused imports or any ports that is open needs to be used, or number of unrecognized protocols, and we saw that between all these devices that you can see here in this table, almost half of them are not compliance with the staff requirements that is just the one small"
  },
  {
    "startTime": "01:58:00",
    "text": "requirements of a big regulation that is that So we identify this, this, basically, and we hold have a methodology that allow us to understand if device is compliance with a requirement, just looking at the response So network traffic response from the active test that we have. Using machine learning, we were able to train a model that, looking at the response from the Internet of Things devices to reply it attacks will tell us if the attack was successful or not just to give you an example So we can use actually machine learning for benchmarking Internet of Things devices, just looking at the network traffic. This is an example of how many devices in our lab with our methodology using machine learning for a client attacks where basically tested against reply attacks and the attacks basically work. So in green, you can see for how many devices our reply attacks work and there are many of them over like 32 devices that we tested. So what's next? Hopefully a new draft on this continuously doing tests in live on the home router or you Gateway when we're talking about enterprise of us smart city is something that is needed is crucial nowadays because everybody know in this room that's when an IOT device is we are talking about enterprise of a smart city is something that is needed is crucial nowadays because everybody know in this room that's when an iot devices as an upgrade everything changed in the behavior of the naturopy from one day to another and hopefully we can have what we call privacy and security label certifications and private and security scores systems for which using like standards and hopefully requirements that will be established in this working group, its devices will be certified and scored from A to F, for example"
  },
  {
    "startTime": "02:00:00",
    "text": "against the requirements and the standard of privacy and security issues for this device Thank you very much. Happy to take it A to F, for example, against the requirements and the standard of privacy and security issues for these devices. Thank you very much. Happy to take any comments, questions This was just like a kickoff of Philly a discussion that can go on for many more months Thanks a lot for the presentation Anna. We have a comment from Chin. Any quick comment? Any quick comment? already have finished of the session. So go ahead quick yeah this is you from far already so i'm not so it's thanks for presentation actually very useful work actually i'm not sure there's a relevant job already actually, but I see, uh, one question I have in mind is, you know, this relate to the, you know, privacy security, you can provide a very useful tool you know for you know regulator or some vendor They can see, you know, how these can comply with privacy or security. So it's a very useful tool and fitting the BMWG and could be you know, have a job to discuss this but I I feel the topic you know bring actually maybe you know, very broad actually because in I tell me, we have some out here topic you bring actually maybe, you know, very broad actually, because in I tell me, we have some IoT office area, actually. I'm not sure, you know, is any part can be, you know, break it down actually also write some job to contribute to IoT ops. Also, I have a quick question, you know have you been aware, you know, is any other related? STO, do the seminary worker, maybe in Dutch consortium? One, in that consortium, I'm, having aware, maybe highly related what you are doing is IoT Security Foundation? and the IOT security founder in UK yes I'm part of the I'm part of the IOT security foundation part of the executive board oh so you Yes. I'm part of the I'm part of the IOT security foundation, I'm part of the executive board. Oh, so this is a part of this project?"
  },
  {
    "startTime": "02:02:00",
    "text": "no no no this is the part of UCL but I i mean this is just to say that i'm i know very well the I UT security foundation in fact we are going to have a meeting in September a UCL talk about this. So this is a topic that is going on also in the IOT Security Foundation but the IOT Security Foundation is very broad so we're talking a lot there also about basically certifying the hardware more than looking at the communications for these devices But just to tell you that I know very well the IOT security foundation I'm also part of their board member the ambassador board. Yeah I also see, you know, maybe ETSR, there's some IoT related, maybe working group or maybe relevant And if you can list several relevant standard body also I my suggestion is that will be the good topic, you know, also discussing in the IoT Ops. Yeah. Because if you have a connection with the IoT Security Foundation, yeah. Yeah definitely. Yeah, thank you, Chin Thank you. Yeah, since we are already out of time, thank you again Anna, for introducing a very interesting presentation thanks to you for an invitation yeah now regarding, of course, the benchmarking parts of the IOT, maybe this can be also in scope of BMW if the draft you have in mind have a broader scope, we can also involve IoT ops as well. This is just my comment because i don't know if Warren have some thoughts of that, but in any case uh please if you are interested, so reach out to Anna if you are interested to work on a new draft on more this work forward in IETF reach out to Anna And thank you again. Thank you. Thank you very much So, yeah, have a nice day for me"
  },
  {
    "startTime": "02:04:00",
    "text": "in my case, evening. Yeah, have a nice evening Okay, thank you. Bye-bye Okay, we reached the end of the session. So thank you everyone. And please continue the discussion of the drafts on the many list and have a good rest of week in Vancouver Bye-bye. See you in Dublin Thank you Thank you for something"
  }
]
