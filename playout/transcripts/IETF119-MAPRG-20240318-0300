[
  {
    "startTime": "00:00:11",
    "text": "Oh, that she Okay. There are still a few people joining the room. Maybe somebody can also close the door why you're coming in Perfect. And then I think we can. Start the session. Welcome to Mebrachy. My name is Mia Cullovant. Dave, do you want to introduce us up? Oh, hi, everyone. I, I'm Dave Plunka. Joining you from Boston, Boston, Okay. So this is Okay. And this is an IRTF session? But the IITF also has very similar, IPR rights than the IETF. So you should be familiar with that. Also be aware as every session at this meeting, This session is recorded. Both audio and video also, even so this is an IRTF meeting the code of conduct applies or actually the IRTF code of conduct applies, which is very similar, or relies on the IET of code of conduct. So, please make your make sure. You know what's written in there and you really follow them So we have a nice meeting a good discussion here today again, it's an IRTF meeting. The R stands for research, And that's what we're doing today. Basically, we have a couple of invited talks that talk about their very interesting research and we can have some discussion."
  },
  {
    "startTime": "00:02:04",
    "text": "This is also one of our admin slides. The important point here to notice is that you should please sign in to meet echo in some way because that's how we keep our blue sheets and our blue sheets are important in order to keep room that actually fits the size of the people. So please sign in And, also, you need to sign in if you wanna join the queue. So, remotely, that should be easy, but here in person, you can use the on-site tool and join the queue over there. Also, the chat, you can find in everything you need, I guess, So please join the meet echo And then if you this is you know, tiny letters, so you don't have to read it. This is more like go to the meeting material and you can look at this yourself but just last week, there was the PEM conference, passive, and active measurements, and We have a couple of, talks from the conference here today. But there's much more that is related to IETF work. There are a couple of really interesting paper I just encourage you to look at this. For example, there's stuff on congestion control, mail, IP addressing, who is, Bpf is there's also something on Bpf now that we have a Bpf working group So please check out the slides. I'm I'm pretty sure you find some interesting papers there. And, that's our agenda for today. So we are already ready for our first presentation. It's a very quick heads up talk. Nalini and Junaya is is coming up here. And just Give us the 5 minutes head heads up about their work. So so so this is work that's been done by, Chunmai and Amog, their final year students at NITK, Suresh call in India. And, we've been working together probably for at least 2 years on this project. So Chin Yeah. Hi, everyone. I'm Tim May."
  },
  {
    "startTime": "00:04:02",
    "text": "So we'll be talking about, an EBP of pace implementation of, an extension ipv6 extension header. Called, PDium. Yeah. So, the objective of our project, was mainly exploration of user EVPF in networking. And implementation of IPV 6 extension headers in EVPS. We also did performance analysis to see how well our implementation fat against, a traditional kernel implementation a Linux kernel implementation of PDM. We also try to analyze the results that we got, regarding the performance overhead of the VP of implementation, and we are in optimizing our implementation. So, these are the interested working groups and, the agenda slots that we managed to at atat Yeah. So, the work done till now is, we have successfully implemented section header in ABF. We've used, TCBPF. Which is a BPA footprint in the traffic control subsystem. Yeah. So the performance analysis, of the EVPF program, against a kernel implementation. Is, yeah, that that's given. And the test bed that we used, was mainly 2 VMs with 8 courses and 16 CB RAM. And the CPU cycles, the EVPF program performs worse as of now, but there's a lot of optimization to be done. And we plan on finding out how much exactly we can optimize and what that the main points of overhead in our program. Coming to network throughput, the kernel PDM, performs only slightly worse, because, of course, adding extension header work, what, what, what, what, what, would cause that. And DBB implementation also performs"
  },
  {
    "startTime": "00:06:00",
    "text": "slightly worse than kernel premium implementation. That is definitely because of the increased number of CPU cycles taken in our EVPF program. And the packet processing latency, the that is per packet latency. We can see that that not that much of a difference between the kernel implementation and the EVPF implementation. Yeah. So this is just a quick heads up. If you have any questions regarding this, we can meet later. Yeah. Perfect. Thank you so much. Any quick questions right now? Otherwise, talk to, talk to Jinmaya, thank you. Thanks. Okay. Next one is Ike. And he is also kind of he gave a presentation at the last meeting. Yes. So it's just an update from that one and, like, about his future plan work. And I should be able to hand this over to you. No. Are you have to request sharing, maybe? No. No. Right. Oh, here. Are you cool? Very good. Yeah. So hi, everyone. Hi, everyone. I'm Ike. And I'm a PhDs unit after university. And this is Smyr has already set, basically just follow-up work, based on our presentation that we gave last time. In Prague, And, yeah, that we, presented our work on, the spin bit, On our internet measurements, And, yeah, we've, follow-up on that in the last few months, So, as a starter so that everyone is on the same page,"
  },
  {
    "startTime": "00:08:02",
    "text": "the spin bit is an optional mechanism, in quick and it allows passive observer to measure the round trip time. And it's quite simple. So the client always spins a bit and the server always reflected. And then the observer can track the round trip time by just measuring the time between consecutive spin bit flips. In our work that I presented last time. We did a large scale web measurement, where we assess whether the spin bit is used in the wild And then we found that around 10% of the domains, that we could measure with our approach that they were indeed, using the spin bit around 50% of the underlying hosts, had a support for the spend. Then also looked into more spend bit specific features, So on the one hand, we looked at whether the Spin mute was always enabled. For the domains so we, we did weekly measurements and then compared the the results for each week. And here it's interesting to note that spin bit, even though if you would like to support it, you have to disable it sometimes. And what we found was that only very few domains always used to spin that. So that kind of seemed like, that the this disabling was followed but our methodology is somewhat limited because we always had, like, a week difference between the measurements. So there could a lot of other stuff be going on there. Second aspect that we looked at was the accuracy of the spin bit measurements that we found that it was quite accurate for around a third of the connections but that was also very inaccurate or around half of the connections. And here, we had kind of, If you thought why this might be the case, but didn't really look into it. In that paper. And these 2 observations. So on the one hand side, the limited methodology for the disabling of the spin bit. Well as these inaccuracies of the spin bit, are basically what we are currently looking into a bit more. And there are the the first part"
  },
  {
    "startTime": "00:10:02",
    "text": "is this, disabling compliance that I was talking about and there we are currently extending our measurement pipeline. Basically, when we see that the domain uses to spin bit. We then do a few consecutive measurements to then find out whether it's always spinning or whether they're also Yeah. Connections in between where spirit is disabled. We then are also looking into the, RTT overest estimations, is actually something that I've been working on here during the hackathon. And, in essence, we're just looking deeper into the, the key logs that we get out of measurements And then finally, we're also looking at the impact of different stacks and these configurations. Which is kind of something that came up during, the second aspect that I was talking about all the to to work during Decaton, and here we are now setting up a test bed where we are then trying to use different stacks and find out whether, Yeah. The, the, the, the stacks have an impact on the forms of spin bit. And today, I would like to focus on these last two aspects and regarding the the overestimation. Let's first have a look at how the measurement looks like in our settings. So we are, as I said, we are we're doing web measurements. That's all we are always first, setting up the quick connection and afterwards doing HTTP request and response, and now the yeah, this would be the the overall connection with acknowledgements on the quick layer in between And regarding the spin bit, this would la it would look like this. So the The client first sends out the request with a spin rate value of 0. And, like, this this, yellow dot supposed to be The measurements the time point for an upstream observer. So some that an observer that"
  },
  {
    "startTime": "00:12:00",
    "text": "can only see the direction from the client to the target can at this point basically start the timer for the measurement. Then the blue one, corresponds to the downstream observer. So someone who's monitoring the direction from the target to the client. And, they can start the timer when the, the acknowledgement for this HTP request comes back than some time passes. And, the HTTPs ponds comes back. In this case, we assume that it's, fits into a single congestion window. So all Arctions fitted with the same value, And then as soon as, the client acknowledges that we can see the first value of one transmitted so that the, the upstream observer can now to a to a measurement. And what can also see in this, this picture is that the downstream observer will not be able to do a suspended measurement in this case. However, this is only very simplified view. And in fact, this is not how it looks like in our measurements, because there are also other back, That quick control traffic frames going over the wire. And Oh, yeah. That's what I forgot to mention. So basically the downstream observer the Upstream absolute server. In this case, would be able to measure the the RTT, but there would be a lot of overestimation probably due to the application delay on the right hand side, so if the HTTP response is not readily available or needs to be generated, for example, that will take a while And then there are also also the acknowledgment delay. By the clients. Yeah. Know what I was already starting with, the, the quick control traffic patterns. And there at the start of the connection, we saw a different patterns for the new connection ID and retire connection ID frames. So this is now only one example. So the, in this case, the target server would send us a new connection ID and the client would then acknowledge that connection ID."
  },
  {
    "startTime": "00:14:01",
    "text": "And there's no changes a bit the the order of the the spin bit signals that we get. So in this case, the the one from the lowest acknowledgement would move up a bit and is already transmitted earlier before the HTTP response such that now the HTTP response actually carries the standard theory of 1, And what this means for our measurements is that now the upstream observer can do actually 2 measurements the downstream observers now also unable to do a measurement. However, there's already still like the, the application delays. In there. Well. Yeah, and during the hackathon, we then looked at different stacks. Because this was now only one pattern and and and and In fact, all the three stacks that I looked into the different patterns. So there were different numbers of measurements that we're able to do and there was also then differences in the accuracy of the measurements, but in general, it seemed like the use of this new ID frames and the entire connection ID frames, helped to help to make the split measurements more accurate. Then there's also, the pink frames, that are used differently by the different stacks. And these, force a response by the, by the other side. So if the client uses it, then the server will will send something even though the edge HTTP response might not be, available yet. And depending on the exact exact, sequence of the packets. This might also then trigger a new splintment measurement. But this measurement then kind of gives us The time that's not really the real RTT but might be a bit higher, but it might also not be the the actual application delay. Because it just triggered earlier. So it's really not At least for me, it's not yet clear how valuable this information will then really be"
  },
  {
    "startTime": "00:16:00",
    "text": "these measurements are triggered by piping frames. Yeah. And due to these different patterns that we've seen just by looking at the new ties and the tire connection IDs. As well as the pink frames, we are now looking into this, in more detail. What we plan to do. The test bed study that I already mentioned but we wanna test different stacks, and see how they work together and, impact the the spin bit measurements. Yeah. So, overall, the the spin bit is used in the wire, which already saw in our earlier. Work I'm and we still don't really understand the the rework behavior. And hence, we are currently working on this, by looking at this mandatory disabling, as well as the over estimation that we saw in our measurements and also looking at the stack specific behavior in the test bed study. And as there's still a lot of stuff to do here. Unless I'm currently going towards the end of my PhD. Maybe if you're interested, you can just reach out and contribute to you as well. Thanks. Go ahead, mad. Matt Torres, Meta. One quick thing you mentioned, Quick thing. Uh-huh. You mentioned pings. So one thing to note about Pink's frame once that familiar. It doesn't actually require an immediate response. It just serves as an acolystating thing. Now this has come up if you times in the quick working group. Some stacks will act them immediately, but some will not. So that'll probably be another thing that's that's different based on how a stack chooses to acknowledge ping frames. Yep. Thanks. Dalini, Hi, Delaney Elkins. I'm curious as to what stacks you tested. And if you found some any any kind of interoperability issues in your testing. So we looked into Quick Go."
  },
  {
    "startTime": "00:18:02",
    "text": "Nicole and Alice quick. And they were interoperable, interoperable, so that worked but it's just that they kind of, and use different patterns for the, for the spend for the measurements. Tommy, Tommy, Tommy Polyapple. You for the presentation. Early on when you're talking about the other, measurements that you previously talked about, you mentioned, like, 50% or, like, over 50% of the time, we had inaccurate measurements. Can you clarify, like, how did you how do you know that those are inaccurate? Like, is related to the overestimation we're talking about, or is that something else? Yeah. So, what we did was we use the, RTT estimation by, the quick secs As our ground truth, and then compare that to the measurement provided by the skin. And was the inaccuracy kind of always in the same direction? Or Was it just all of the overwhelming large number was on over estimation, but there were also a few cases where we had on the estimation. Okay. It was not that many. I think So, I'm not really sure at the top of my head, but I think the loss single digit percentage were on the estimations And I guess do do you have ideas of what causes the underestimation? I guess, is it also just patterns like this Yeah. But that's also something that we're looking into. Just the over estimation are the the, like, bigger problem and, map, DMV, And the restamations are also on the on the list. Brian? We cannot hear you. You're just unmuted. You gotta click both buttons. Course. Good morning, afternoon, evening."
  },
  {
    "startTime": "00:20:02",
    "text": "Wherever. Hi. Thanks for the work. There's a little bit teaser here, that I did wanna, ask a question on You're saying you wanna, dig deeper into mandatory disabling. Have you just anecdotally, have you seen That implemented in all of the stacks? And have you seen any divergence in how that's implemented? Yeah, we're still looking into that. And I've not looked into the the the text yet about that. We're just basically standard our measurement methodology and will run run out that in war. We will start the settlements, at the start of April, I think. Okay, cool. Thanks. Okay. Perfect. But the main takeaway here is, like, any questions you would ever have about the spin he's going to talk to Vic now because he's looking into this. And we're excited to see your final result next time, hopefully. Let's see. Thank you. Thanks. Okay. So our next speaker is Manas Vinny. I hear you here. Perfect. I will hand over the control to you. Give me one second. But you can test the audio already, I guess. Can you hear me? Yeah. We can. Okay. Great. Wait. There you are. No. You should have control. Awesome. Awesome. Thank Yeah. you. I could see the slight note at the bottom. Perfect. Just go ahead. Okay. Okay. Hi, everyone. Today, I'm gonna be talking about our work. Towards improving outage detection with multiple probing protocols, up here in, Pam, last week my name is Manasini, and I'm a PhD student at Georgia Tech. And this work was done in collaboration with Zachary and Alberto also at Georgia Tech."
  },
  {
    "startTime": "00:22:03",
    "text": "So I don't really need to tell anyone here, but the internet is a large distributed system and its critical infrastructure, and outages are not uncommon. The figure on the right shows the outages that happened from 1 week from February 26th to March 2nd. Sean here. So if you look at how outages are detected, ICMP probing is one popular method. And the idea here is to, pick a subset of addresses in a slash 24th block. And then drop them via ICMP and record the responses. And then based on some threshold and, some criteria, we decide if the block is up or not. So ICMP has many advantages So it is fast, and it has white coverage. But it also has certain drawbacks. For example, some networks block ICMP And there are also some flash 24 blocks with 2 few fonts of hosts, and these may require more than 1 probing cycle in order to determine what their state just just So in this work, we try to address some of these limitations And, specifically, if you look at how, some networks block ICMP probes, then, we can maybe ask, our transport layer crops allowed. And this is exactly what Internet bytes can do. And then that white scans use a variety of protocol handshakes to establish the liveness of hosts. Systems like sensors use, ZMapp to scan, preferred addresses, using a variety of protocols like HTTP, SSH, DNS, and so on. To check for open ports and for design and for seeing that services are running on which host. So in fact, the previous work showed that, combination of TCP, UDP, and ICP probes, discovers the largest set of host. And so we use this insight, and we ask in the context of outage detection,"
  },
  {
    "startTime": "00:24:00",
    "text": "What improvement can adding TCP UDP Probing bring in terms of block coverage? So before we get into what kind of improvements see, we first need to understand, how ICMP Programming works in practice. And one of the popular methodologies is called trinocular, which is this ICMP probes to estimate slash multiple block status that is if a block is up or down, using bayesian inference, this method is used in at least 2 operational systems one at Georgia Tech and 1 at and ISI. And here, the idea is to take the historical availability of slash 24th blocks, determine using ICMP probes and use it to see a basic inference model. Then in the live roaming system, we basically pick an IP address in a slash 24 o'clock and send an ICMP hope to it. And based on the response, we update the belief about the block status. And that is if a block is up or down, So ICMP Sorry. Kinocular guarantees that outages in a slash lasting longer than 11 minutes. Is the length of a probing cycle will always be detected as long as the block's availability is at least 30% and so using now the Problem here is that implementing the TCP European in a operational system is difficult because of the complexities in testing and, making changes to a live system, but we still want to know what improvements we can expect when we add, CTC PDP domain. So we instead focus on the historical block availability, which is estimated using ICMP we, try to determine what happens to these availabilities for the slash 24 works, then we add TCP repeat probing."
  },
  {
    "startTime": "00:26:02",
    "text": "So now we know how the system works. Let's go to what metrics we should use we use 2 metrics. The first is the availability of your the second is that the liability of a slash 24th block So so define host availability as the number of successful responses from a host divided by the total number of clubs to the host expressed as a percentage, And if you look at this block. We have an address 2.2.2.1, and it responds in 2 out of the three probes that I sent to it. And therefore, it's available to be 67%. Next, we'll look at, blockreliability So we define 1st block availability as the availability of an address in the block on average. So if you look at the block over here, we have 3 addresses and, we add up their availabilities and divide by the total number of addresses that were ever seen in this block. So 3, and therefore, we get an availability for this plot as 44.4%. Now going back to trinocular methodology, is is is is slash my 4 blocks with an availability of at least 30%. We can always do all this detection, as long as the outage last longer than 11 minutes. So that would be defined love to be a reliable block if its availability is at least 30%. Okay. So now we know what our metrics are. Let's look at the data set. So for ICMP views and ISI IP history dataset, So and conduct periodic surveys of the IPB for address space, using ICMP, and it reports the responses from the host. And it's always about 2 months apart and, for TCP UDP Groups. We use sensors, Internet bytes, scanned data, sensors performs protocol handshakes with a variety of, with, different protocols like HTTP, DNS, and so on. And it reports the responses."
  },
  {
    "startTime": "00:28:02",
    "text": "So we had access to weekly surveys from census And what we saw was that out of the total 14,000,000 usable slash 24 Blocks in ipv4. And covers about 5,800,000 of these. And census covers about 5.72000000 of these. Together, they cover 6.21000000 slash 24 blocks. And, we see that The for the data spanning 9 snapshots over 2 years, November 2020 to December 2022. There are about 800 and 820,000,000 hosts spanning 6.21000000 slash 24 blocks. So now let's look at some of the findings first, we find that the average response count that is the number of surveys on average to which a host response increases from, 5.24to6.02 when we add the TCP European probes And of I'm sorry. I think I mistakenly clicked a button that I wasn't supposed to Now you should have control back but this is not the right side. Org Yeah. That's fine. Let me go ahead. Okay. I think it was Yeah. This one. And we also found that, the host that appeared in all the snapshots increases from 22 to 28% when include, the GSP beauty patrol. And so in summary, we see that the availability of a host on average can higher than we use a combination of following methods."
  },
  {
    "startTime": "00:30:03",
    "text": "Next, let's look at the findings at the slash 24 block level. As a reminder, a reliable block is the one whose availability is at least 30% And from the Sankey diagram on the right, we first found that out of the total 6.21000000 slash 24th blocks, which across the two datasets. About 328,000 blocks were missing in the ICMP only data set. And out of these 328,000, blocks, about 106 1000 can be the library the liability code when we add the TCPA groups. And, also found that, TCP UDP folks can improve the availability of existing blocks So out of the 2.3000000 unreliable blocks in the additional ACMP data set, about 720,000 or 30% of the underlying blocks can be reliably probed when we add the TCP music codes, And so in somebody, adding PCP European folks can reduce the number of unreliable by about 826,000 Next, let's look at the findings at the AS level. First, we found that, when we add the TCP repeat proving, be found 1800 ASSs, which were previously not seen in the ICMP only dataset. And we also found that the percentage of reliable blocks within an AS on average increases from 66 to 83%. So in summary, the availability of a host on average higher when a combination of probing methods is used we find that we can reduce the number of unreliable blocks by about 800 and 6000 or 5% of all slash info blogs in happy before And, we also are able to discover 1800 new ASS, which were previously not seen in the ICMP only dataset. So these findings are interesting, but, we don't really consider the overhead from additional probing,"
  },
  {
    "startTime": "00:32:01",
    "text": "that TCP UDP can insert And in our methodology, we don't consider the IP address churn. But, given these findings, we're pretty excited about incorporating TCP repeat programming into outage detection And it remains to be seen, how much of the potential for improvement can translate into actual gains in outage detection. So that's about it. And thank you. Thank you. Actually, as a chair, I take the liberty and jump the queue and ask a question first. We already have people in the queue, but I'm, like, maybe I'm I missed it, or maybe I didn't understand it correctly, but why actually do you see because he combined both ICMP and TCP and UDP. Right? Why don't you why do you also see things with ICMP that you don't see TCP GDP probing. So ICP is not reliable, but Sorry. Can uh-uh let me go back to the slide. I think you're referring to the 24 block the sankey diagram that I had. Is that right, or I'm referring to this slide where you had the bows set set kind of overlapping. And so there's, like, also part that is not detected by the TCP proving. Right? Oh, Yeah. Right. Yeah. So, they're only able to observe these many because, these are blocks, which, are actively being used, but not short of the other blocks are not visible to either method because either they don't have any hosts are their reserve. So I'm not sure about that. But Yeah. If your question is whether there are, blocks where there is SIMP probing works, but TCPDP doesn't work, then, yes, there are and, this could just be like, edge"
  },
  {
    "startTime": "00:34:04",
    "text": "hosts which are just, like, not running any services So PC community probing, in that case, it's not going to give us too much of an advantage. Okay. I see. Okay. Then we start the queued now, actually Lorenzo. Okay. Yeah. So Lorenzo, I am So I I'm not sure I see the value in a in a metric that uses a block it it it 8888 there are lots of reasons for that. I mean, I think it sounds like you you picked or or the the previous paper picked 24 because it's sort of the smallest unit in the IP before Internet. That's really only true sort of on the global background. So just I've got a few examples. For example, you know, server deployments, they don't That's so first, all blocks are generally sized based on based on, how many hosts there might be in there. So they're typically variable size, right? And servers kind of like don't move around. So I don't know what you're measuring here. Maybe you're measuring residential users, and and measuring whether their gateways do or do not respond to ping on the LAN interface. But it's very hard to understand what this metric actually So, something that's containing clients like laptops, They're not 24. They could be 18. They could be 27. They could be whatever Right? And so what you're measuring, if you measure a slash 24 is you're measuring what percentage of hosts are responding to to paint. And so the availability depends on how many hosts are connected. So, you know, maybe at night, assuming there is pawn to ping, which you generally don't, at night, there are fewer of them and so servers, you know, I mean, consider the subnet 888 slash 24. Right? I can promise you that 8888 is not an actual machine And it and if it was, it would not be in the same block as a machine called 8889. Those things are basically, like, spread out across the internet. But I think the issue is that many server deployments are actually, like, So"
  },
  {
    "startTime": "00:36:03",
    "text": "and also, yeah. So client hosts typically don't respond ping to the generally behind firewalls, again, the only thing that I can really think that you're measuring is essentially users that have public ipv4 addresses, which means home gateways that it can let's say to cable modems, and maybe mobile phones, and because the address space is sparse, those things move around across 24 as But you're not really measuring anything at the 24 level. It's just whatever happens to be there at the moment. So, yeah, there's a So, anyway, I think, you know, focusing the in in the previous research long, long time ago when I was, you know, doing this stuff, I think I think there was this idea of the, of a list of ping targets where they would sort of be around all the time and you would ping them. And that's sort of, I think, While it's definitely biased, it feels like a better method because, yeah, 4 is just very there's just not enough face sharing to construct a reasonable metric Across 24 is in So I I think I understand what you're saying, but, been this is from the perspective of trying to figure out if there is an outage in a that's happening on the Internet. Right? So, yeah, maybe slash 24 o'clock is not the optimal way of doing it. But then, what we're trying to show over here is that, using things alone is not going to help detect outages. And, typically most outage detection systems also use multiple kinds of, signals for seeing which blocks it up. Right? So there's a mix of active and passive probing methods. And So while it may be true that, ICMP probes only discovered like shows, that are also, like, TCP overview probes, which are discovering services which are running on hosts. So"
  },
  {
    "startTime": "00:38:01",
    "text": "yeah, looping by slash 24 blocks may seem like an arbitrary choice, but I mean, that's, I mean, that's one way for us to compare against, like, what already is out there. And so, yeah, Okay. Nalini? Milaney Hawkins, So I was curious that I don't see any kind of references to ipv6 at all. In these, slides Yeah. That's a great question. It's also difficult because, we don't have a methodology for detecting and, like, So the way this technique works is like probing random addresses in within the slash 24 blocks. So it's a it's much easier in, like, the ipv4 address space than it is in ipv6 So if there are methods to do that, then it would be, like, any just an addition, but, yeah, they don't exist yet. Was this your question already, Eric? Okay. Mike, where are you? There. And and gonna just add a little bit to Lorenzo. In the data center space, it's quite common to for individual cabinets, if you like. To have a slash 28 or slash 29 even. Some of these, even a 32 plus a V Six maybe. For them. So if you're if you are randomly picking within 24s, And those 20 fours belong to Hosting providers, Okay? You're effectively going across 16 to 32 different customers so who's doing it. So you might actually want to go down finer granality, how to determine which slash 20 fours are hosting providers in which are not probably you can get some of that out of"
  },
  {
    "startTime": "00:40:04",
    "text": "the BGP announcements was some heuristic. And some other thinking and maybe some community strings and you know, other stuff, but I would also suggest that there's a strong bias to picking you know, the first host within the thing because that's where people put their they're their gateway router thingy Okay. And the other stuff or other kinds of stuff that they have. And so while you're randomly, maybe I don't know why you randomly pick, but I would say that's the thing. And and that would actually give you a a better measure of from ICMP, but also maybe other other protocols that you're things you're trying, that some things there that can respond to the other thing point that Lorenzo said, I don't know that Well, there are geolocation stuff for IP addresses. And if you could take that and turn it into a time zone, You could hit them during the day. And I think that speaks to Lorenzo's point that hosts turn off at night and are, and some of them turn off on the weekends, and we see all these, you know, diurnal things for malware. For instance, we know it's stronger during the day, during the East Coast of the United states and stuff like that. So there's definitely more hosts turned on so don't know what it whether you can do with that, but I think that's a useful thing. These six, your problem is that the space is so big. But you can go from the announcements And, again, you know, see a 48 slash 48 announced My bet is 0001 there's stuff in that that know, slash 64 because people tend to use that. Right? So you could go for something like, I don't know what else to do, but there's a lot of interesting research and and you know, people talk about internet telescopes and whatever. And I always think it's really interesting because it really is like JWST, you know, looking through the dark matter of the cold cold cold"
  },
  {
    "startTime": "00:42:00",
    "text": "the universe. Right? So so thank you for this. Yeah. Thank you. Those are great points. And, I think I think I think the the outage detection system, which I was talking about, does look at BGP prefixes and announcements, So, yeah, just not Okay. Just because we have so much time today in the session because we didn't fill up the whole do you just, as a last question, do you have any kind of high level takeaways about outages in general? Like, how often does it happen? Does it change over time? Like, is there any, anything exciting that you could share with us? So I can't speak to, how often they are good. I I'd I'd did not actually work on, like, the audition system yet. Yeah. I don't I just update. Yeah. I I do think there is some interesting aspect to geolocation. So there were some, blocks which feed geolocated to Say, initially, if you just use it if you just use ICMP, there were, like, some, say, 40 blocks in, I think Yemen and Ncedia and places like that. And then then we added the DCP UTP proving it sort of increased, like, 80 plus or something. So I thought that was interesting. So Okay. Perfect. So, please also check the chat. There's a lot of comment a lot of interest in your work. And then if you have more of the results, just come back and present them here again. Okay, we go to the next talk. Thank you. Thanks. Okay. Next present is JiaMo. Can you Please turn on your audio and video"
  },
  {
    "startTime": "00:44:23",
    "text": "Actually, I don't see him right now, so let skip this one for now And we go through the next one. So this is TJ. Yes. You're here already. I give you control. And you can go ahead. Alright, hello, everyone. My name is Tuesday. From Virginia Tech. I'm an assistant professor in the CS department. Today, I'm very excited to talk about our gross recent work, eradicator, pruning IR entries with RPK a valid PGP insight. So we are all familiar with BGP. So BGP is really important. Right? But a BGP was not designed to, designed with a security in mind, so there has been lots of, BGP security incidents, including Indonesia and ISP and also oops. Sorry. Yep. And also, there are recently many crypto currencies are stolen because of BGP hijacking attack. And also the hackers, emptied a through in wallet, not only for other crypto occurrences, you know, main Ethereum Wallet as well. And also sometimes I'll Google traffic where we route it through China and Russia. So because of this reason, there has been a"
  },
  {
    "startTime": "00:46:01",
    "text": "in large to approaches to improve his peer security, The first one is Internet routing registry IRR, which was invented in 99 5. It is widely used for sharing, global routing information. However, the main challenge is It Lexa, authentication mechanism. And, also, it's been their so many years. So there has been anecdotal evidences that has many outdated entries. Because of this issue, RPKI was proposed, So using, PKI, it provides a cryptographically variant verifiable method of binding IP prefixes To their respective or is in a s's. One challenge of this protocol is it it is a slightly narrower and a coverage than IRR. But, however, it has a certain guarantee of binding between IP prefixes to origin So it has been known that it, the, deployment is been increasing, but still about 40% So our work is focusing on so we are using RPKI to improve the quality of IRR entries by removing stale IRR entries using special learning techniques. So, for, this study, we use 2 different data The first one is RPKI dataset, thanks to ripe NCC collected all the RPKI objects since 2011. We collected a we also used our all RPKI data set, And, also, we use RadB, one of the most popular IR database, and also we collected all the IRRs, across different sources. Alright. So Let's take a look at how IRR and RPKI are used in practice."
  },
  {
    "startTime": "00:48:03",
    "text": "The top graph shows how many, BGP on Altima can be covered by RPKIorir. And, the bottom graph shows, alright, among them how many of them are actually valid when he validate BGP on Osmonds using RPKI or IRR. So As you can tell, 98% of them covered by, I'm sorry, Alan. Yep. 93% of PG on hospitals are covered by IRR However, RPKI covers 40% So, however, when we focus on the quality of this objects, RPKI provides you a strong, the strong authentication mechanism. So 90 98% of covered BGP on hospitals are valid However, when we are using Oh, you are only 90% of them are valid. So so so As you can tell, if it tells us that There must be some a gap between the percentages of validation Pianosmos, which means there are some inconsistency between RPKIN IRR objects. So, for the references registered in both IRR and RPKI. Let's first take all the that how many of them are, actually consistent and how many of them inconsistent. So we found that More than 60,000 IP prefixes in IRR are in constant done with RPKI. When you consider RPKIS Ground truce, it means that 60,000 IP prefixes registered in IAR are actually invalid. And when you take a look at that, a BGP announcement is verifiable through both RPKI and IRR. We found that 20,000 more than 20,000 BGP announcements are covered by inconsistent"
  },
  {
    "startTime": "00:50:01",
    "text": "Invalid IRR entries. This, the bad news is this kind of, you know, a covered BGP on Osmusesinconsistent BGP on Osmus are increasing over time. So Let's, if we are focusing on to removing such stale, invalid IRR entries, we can take a look at their different, characteristics and using some heuristic methodology, we can cut down some in invalid IRR entries. So for example, the we can take a look at IRRobjects ages. For example, very some stale, stale IRR objects tend to have a very, old IRR objects ages. So, for example, as inconsistent invalid objects, when you take a look at the 90% 90 percentile, 15 years are their ages. However, consist 1 is only 10 years. So one might think that, alright, using just, you know, this stretch you hold, we can remove if the object is just too old. For example, JP IRR, forces the IRR registrars to update their their IRR entries Every year. So that they can cert they can be sure that all these objects are very new so that they can just trust the all the IRR objects. So We can apply this object, but the thing is, If we are choosing this 16 years as a kind parameters, it only covers 10% of inconsistent IRR objects. Of So here is the challenge. If you're just applying just the heuristic, the problem is setting up good threshold is always hard. Right? And one alternative approach is for example, IR default for the or RIP NCC, they filter all the IRR objects if they are RPKI invalid."
  },
  {
    "startTime": "00:52:04",
    "text": "So using RPKI, they can only prune the IRR entries if they are covered by RPKI. This is really great. However, the affiliate side is it only covers 44% of IRR objects, which is we are missing, we are just assuming the 50, we we cannot determine the other 56% of IRR objects, So our idea here is We are using RP we are observing beach patterns of such our, inconsistent and consistent objects, and we can learn some pattern so that we can use this insight to filter the rest of uncovered IRR objects. So We can model BCP on Osmus using, various modeling techniques, but our, approach is simple. So, for example, we can focus on, let me go back, life span, which is the difference between, the first and the last derivation divided by your monitor window size. Monitoring window size is We can just monitor VGP, VGP announcements for the day, for a month, for a year. Okay? And uptime is the number of days the PG pronouncements have been observed which is divided by monitor window size, and a relative uptime is simply divided out time by a deal. Life span, So, when you see a, we are trying to compare this kind of consistent, the BGP system objects and also inconsistent objects. So as you can tell, The first graph shows that consistent IRR objects tend to be more recently used. Which is pretty obvious. And consistent IRR objects tend to be more frequently announced in BGP than inconsistent ones. However, this is really"
  },
  {
    "startTime": "00:54:00",
    "text": "interesting when you're focusing on inconsistent, visit pee on Osmons as we increase monitoring window, which means as we take a look at the as we consider more historical busy pean alphasmas, their, relative time increases, which means this is the Ocado from signal that This object used to be correct, But, for example, let's say one network operator moves this IP prefixes to someone else, they forgot to update their IRR entries. They are old stale, not correct anymore. So inconsistent IRR objects use to be honest, actively. So inconsistent one becomes no longer on our own thin pizza Alright. So using this inside, we are using a more than 300 features for each prefix origin pairing IRR. And we, applied multiple, much longer in techniques, and with the classification with rejection so that we can be determine the status of IRR entries So Oh, when we are evaluating our technique, the, most significant con significant challenge is we are missing ground truth data set. So we are using RPKI. 1st, we are using RPKI to set to ground through us. For example, we train our model you know, using, the current date. And then we are testing our model using, know, T +1 days of RPKI data set. So the performance is good. And the other, grant test data set is using transpolis from each IIR. IP prefixes can be transferred between the organization whenever it moves to happen. Our IR has your database that tracks all these kind of tracking transport history, that we can use that, And when we found that in the all your entry is not updated, you know, even though it was transferred, we can label it"
  },
  {
    "startTime": "00:56:01",
    "text": "a, invalid stale entries. So using the slopes, we all so tested our model. And the performance was, was great. So let's take a look at, the comparison with our techniques and with other other, techniques, for example, IRD4, which prunes all are you are entries using arctic air validation. So we, evaluate the performance using for, metrics, so let's focus on the first one. The first one is number of toll objects. So IRR has nearly 2,600,000 entries. And after removing still and trees using RPKI, which is, I we're IRD 4 r, currently deploys it. Then, nearly 20% ish objects are filtered. Using our technique, we remove 59% of IELR objects. So using our technique, for some reason, we, our model determined dead. 59% are stale. However, when you focus on the percentage these IR objects, remaining IR objects that actively used in BGP. We found that our, our model has, even though we removed number of, objects, we see that 60% of our IRR objects are still used used in practice. And also, we take a look at the BC Pianosmos, how many, remaining IRR objects are actually used in practice, how many of them are covered by BGP announcements? So this red bar shows the percentage of visa pronouncements that are covered by respective IRR dataset, And even we filter out 5th 9% of IRR objects, we still cover 88% of BZP on Osman."
  },
  {
    "startTime": "00:58:01",
    "text": "And the last one is how many of them are actually, turned out to be valid using Ira before or our technique. So we found that 94% of coverage on osmo's are valid against IRR data set after filtering all of them, a 59% of them using our approach. So, when you zoom in, we can find the even after we filter 59 percent of the objects, the percentage of Volley piece P and Ls must slightly increase, which means we actually we we are we want to argue that we accurately filter the stale IR of 6. So so so Our methodology is, is not perfect for sure. So the, the biggest challenge is reducing first Negative. Negative. Negative. Negative. So, for example, let's say we deployed our tech in for example, IRR, registry. Then after we, let's say we using, because of false negative, we removed a default objects Then we have to explain to the network operators that, hey, because of this in our model turned out to be dissolved, just to be invalid. So that you can, maybe you can honestly more, longer so that it can be finally determined as So it has to reliability in our, machine learning model. So because of, 2, with this techy, we, publicize all the source code data set, and we run our technical databases, and all the all the data sets are publicly available. On this URL. So quick conclusion is we we did a longitudinal study, and that we characterized the inconsistent stale IRR objects, And we propose that your email based attack. And, also, we pro we publicize all the data set and search code."
  },
  {
    "startTime": "01:00:03",
    "text": "And for more information, this work was published in NDSS this year in February. Thanks for your time, and I'm happy to take questions. Yeah. Thanks a lot. We don't have anybody in the queue. But we would have time for questions. But, I guess people can also contact you and chat or on the mailing list? Sure. A lot for your talk. Bye. And this means we move on to the next speaker. Alright. Thank you. Thanks. Thanks. Okay. Which we know is how controlling Yeah. Can you hear me? We can hear you. Go ahead. Okay. I can see the control From my end, you cannot you say? Yeah. Let me double check. Now maybe. Yeah. Yeah. I have it now. Perfect. Thank you. Yeah. Okay. Hello, everyone. This is Rishvi. I'm going to present our work titled any cast polarization in the wild this is a joint work between USCI and Akamai Technologies. So any customization, apps latency. Let's start with a specific scenario. You can see from the figure that a client from Europe is going to North America even though that client has a nearby site in Europe, We call this type of scenario as polarization On the other hand, you can see, a client in South America is also going to North America."
  },
  {
    "startTime": "01:02:01",
    "text": "But this time, the client doesn't have any nearby sight in South America. As a result, we don't call this type of scenario as polarization. Since the client doesn't have any option to go to the nearby location, in South America. So polarization adds latency, since it may cause Traffic to move from one continent, to another up. Next, we will, show what our contributions are, in this paper. Okay. So the time, polarization was first defined by, Jovanimo, right, in his, twenty twenty times a paper, but here, we make 3 contributions. First, we find out the key reasons behind polarization 2nd, we investigate all the known prefixes to see whether they have polarization problems or not. 3rd. We worked with the CDN provider to make real world changes to mitigate the impact of polarization. So what's our expectation from an any cost network. You can see an ideal anycast network in the, figure the client should go to that nearby location. Mean, that's our expectation from the figure, you can see that all the clients are going to the nearby location but this is not the case, in reality in reality, you can see, you will be you will see some clients will go to the distance site as you can see, with the red line. And we call, this type of scenario as polarization. So we already describe definition of polarization. Next, we will show The reasons behind this polarization problem"
  },
  {
    "startTime": "01:04:00",
    "text": "So we find, 2 top level reasons behind polarization, The first one is multiple back bone problem. And the second one them. Next, we will describe both of them. So we identify multiple backbone problem as one of the, key reasons behind polarization, this problem happens when, a tier 1 transit is not connected to the anycast pop in a white geographic area. For example, here you can see a tier 1 backbone is directly directly connected to the Pop in North America. Which is shown in the left left top side. This tier 1 provider has a broad coverage. And it's not connected directly connecting in in the in the pop in Europe. As a result, you can see an unhappy client, which is from Europe is going to that North America even though it has a site, but even though it has a pop in, Europe. So we call this type of scenarios, multi pop backbone problem since this backbone is not connected in a wide geographic area. This is the 2nd kind of polarization problem The cost is kind of similar like the previous one. But here instead of having, having a a direct connection to a tier 1 provider, the North American pro pop is indirectly connected to a tier 1 provider. Through a regional s. This regional is maybe, connected as a private peer and leak throughout, to the tier 1 provider. Or it may be connected as a transit provider and propagate routes to the tier 1 provider"
  },
  {
    "startTime": "01:06:00",
    "text": "for for getting a global coverage. Like the previous problem here, we can also see an unhappy client who is going to the pop in, North America. So far, we we showed the basic causes of pluralization. Next, we show the ways we find this dis polarization problems and their reasons, So finding polarization problem is straightforward. So we we, within a continent. We pick 2 different clients. And, if those clients are 2 clients, one of them goes to a nearby site and One of them is going to a distant site then we identify this type of scenarios as polarization. The next question is how do we identify the client that is going to a nearby site? Or a distance site. So we use a threshold of 50 milliseconds. To identify the clients, which are within the continent and a different threshold of 100 milliseconds to identify the clients that are going to a distance site which is out of the continent. And we did this type of test for all the known any cast prefixes, we got this list of known anycast prefixes from a prior work, by Rafael Somizin, his IMC 2020 paper. So after getting all the potential any case prefixes, which have polarization problem, We make trace routes to find the root causes of this polarization problem. So We use rye petless vintage points, and we make trace routes to this potential prefixes that might have polarization problem."
  },
  {
    "startTime": "01:08:01",
    "text": "At first, we identify the Pan ultimate desktop if the penultimate s hop is a tier 1 provider, then it's probably a multi one problem as shown, by the blue box, If it's a regional ace, not a tier 1 is, then Probably it's a regional routing problem as shown by the green box. These are the two reasons that we showed earlier. And these are the key reasons. And these two reasons have some sub classifications. The other details, to find out the sub classification is in the paper. If you are interested said, at the high level, we actually try to find out the real transit provider from other parts to identify this sub classification. You can check our paper to get the full algorithm. So we showed the reasons behind polarization. And we showed how we, find polarization problems and and their reasons. Next, we will show how common is this polarization problem. As you can see from the table that polarization problem is not uncommon. Around 28% of the known any case prefixes have this kind of polarization problem. And we could confirm 18 of them with the CDN operators or with the, operators. So a common, expectation is that when the number of sites is high shouldn't have any polarization problem. So here, we try to find out whether that's the real case or not. Here, you can see the number of any sites in the and in the y axis, you can see the percentage of any case prefixes, that show a polarization problem."
  },
  {
    "startTime": "01:10:03",
    "text": "As you can see, when the number of sites is below 30 5, we have more more polarization problem. So it's true that when the number of sites is less then then probably we mostly have this, polarization problem, but it's it's not it's also, common, with with the network, which has of 30 or maybe 25 sites. So, what's the impact of polarization? Here in the second, last column, we can see the significant difference between the median and 95th percentile latency. You can see in many cases, there there are over 100 milliseconds. This is because the of the cross country and traffic that we are observing due to this authorization problem. As some examples, you can see, some traffic from Poland is going to u the US even if it says it has a site in Europe, some traffic from the US is going to Australia. So lots of, cross continent traffic. And this polarization problems are common in many different types of, providers like CDN, clouds, or, DNS providers. So we show how we, next, we will show how we improve the situation by working with the, CDN k. So there are some common techniques to improve polarization problem. One is to, improve the transit connectivity that you have your transit should be well directly connected in multiple geolocations. 2nd, we should have more number of sites with proper connectivity. Up"
  },
  {
    "startTime": "01:12:00",
    "text": "I mean, not in complete connection, in multiple geolocations. 3rd, we, should measure the performance frequently and we should be vigilant about our, about drought leakage. And lastly, we we, use, we can use different routing, changes to make the situation better. With CDN, we, mostly work with the last solution. So we try to use different routing changes to improve the situation. So let's start with the, with the with our first case study where you can see the three sites, in Dallas, Virginia and Milan. We we can see 3 maps here. Each map indicates that particular, catchment. The first one is in Dallas This map shows the probes that are going to the Dallas location The second one, is for Virginia. This map shows the that are going to Virginia US. As you can see, there lot of red dots, in all these graphs, this reddish dots indicate over a 100 milliseconds of latency. So in all three cases, we can see a lot of cross continent traffic going from Europe and Asia. The US. Or vice versa and we observe, bad latencies overall. So we are, what's the reason behind this polarization problem. So We are showing here, one reason, behind this polarization problem. This is the first, one for the Dallas location that we just saw in the previous slide. So here, the tier 1 provider, which is 1299 is only connected in a Dallas location. It's directly connected in the in the pop of Dallas location."
  },
  {
    "startTime": "01:14:00",
    "text": "As a result, we can see many cross continuing traffic from Europe, to Dallas location. Because this CR1 provider. Is only connected in Dallas. And that's why, it's a it it creates the multiple backbone problem that we just discussed. The other two maps from the previous slides have a similar kind of issues. I'm not not going over the details, but, you can find them in the paper So what did we change to improve this situation? So we changed the routing announcements to that tier 1 providers, which are not completely connected. We found that this CDN has other tier 1 transit can providers. So they are really well connected. But they have some this kind of extra connection with the tier 1 providers, which is not directly well connected in other places. So we just block those announcements to this tier 1 provider. Which are, not well connected. As a result, you you can see in the right side there very few, red dots So the situation improved after our changes. Up. The improvement, world improvement is shown in the first right map. You can see that all the continents observed improvement and especially in Europe it was improved by 20, and in Asia. Continent observed 40% improvement. So that's a big change in terms of performance And this is, one scenario, and if you are interested we we show some other scenarios in our paper. And by the way, these are real changes and the real plans were benefited after these changes. Okay. So to conclude in this paper, we show polarization is not an unknown problem. We showed reasons behind polarization"
  },
  {
    "startTime": "01:16:02",
    "text": "And lastly, we showed, how a city and provider changed its routing policies to improve the situation, Thank you all. Please let me know if you have any questions. Thank you for your presentation, Michael. I wondered if you, Michael Richards the mic. I wondered if you had any or have developed any metrics for the degree of polarization for each of your sites, sites, we we just use the latency difference So if that that's to each each destination, But as a group, no. be really useful, but We didn't do that. That that that I think would I think for some operators because it would tell them which, which squeaky wheel to fix First, Yeah. I think that's a good idea. Yeah. Yeah. Yeah. Can be a possible future work. Jim. Go ahead. Yeah. Jim Reid. I really enjoyed your presentation. Fair interesting. What I wonder though if you've considered other possibilities for these kind of polarization problems. I remember in the very early days of doing any casting there were pitting issues with service providers at internet exchange points and so is sometimes the traffic wasn't going to the right node because the period wasn't set up the way that should have done. Have you given any thought to those kind of issues as well just the basic problems of people screwing up BGP. Yeah. I I think what we are showing is kind of similar to the issue that you are mentioning, like the they're not proper pairing connectivity in, many different locations, and that's why the traffic should go to a distant location. Yeah. Does that answer your question? I mean,"
  },
  {
    "startTime": "01:18:02",
    "text": "we are showing here kind of similar kind of, issues. We we show showed the transit connectivity was not complete. That means many in many geolocations, we didn't see a connectivity that's why traffic should go to the other, continent. Okay. Thanks. Mhmm. Hello. Hi. Benchmarks. Mehta, I just wanna clarify my understanding of your mitigation. It sounds like you essentially disconnected the any gas prefixes at the at the pop that was getting too much attention, from that that tier 1 provider So presumably, that would have had a negative impact on latency for at least some users who were being properly routed to that top via that provider. Etcetera? Yeah, your assumption is correct. But, in this case, this particular unicast network has some other tier 1 providers. Which are really well connected in all the contents. So even if, some some of the clients are not getting the service through the blocked tier 1 provider, they they can get the service to the other other transit providers. So, since the other transit is well it. We didn't see that much bad performance, but maybe some of the clients got some bad performance, but that's that was not significant. Compared to the a real problem. Thanks. That's helpful. Mhmm. Okay. Let me ask one more question, actually. So it sounds like the routing changes you did were kind of applied manually more what more more or less, basically, how hard would it be to detect"
  },
  {
    "startTime": "01:20:02",
    "text": "these things automatically and try to optimize. So, yeah, we we did it manually, but but it's a We we have a, we we made the changes manually. Yeah. That's true. But, there there are some automated systems which can, major the performance and show us, like, What at the that we need to change or let me ask the question the other way around. How reliable is your measurement and your metric to actually take this as input to do automatic changes. Yeah, for routing changes, I think automatic changes, people, I mean, the network operators don't really want to make out of automatic changes because they can be really risky. So yeah. I mean, we we they're both automatic and manual process, that are working together totally understood. So, an area for research, I guess. Yeah. Okay. Perfect. So we have nobody in the queue anymore, so we go on to the next stock, Thank you for joining Not that tall. Alright. Thank you. Okay. You want me to advance the slides. Right? Oh, well, I didn't plug it and you can use your phone Yes. Perfect. or you can say next. I'll tell you when. So, hi. My name is Jason Lookinggood. I'm here for Comcast today talking about dualqlowlatencyfueltrials. Next slide. And so this is really interesting when the TSB Working Group standardized L4S, which is low latency, low loss scalable throughput and is in the process of standardizing non Q building or hop behavior."
  },
  {
    "startTime": "01:22:03",
    "text": "The top ones l for us are informational or experimental, and the bottom one is standard track, but still obviously not on RFC. It's probably gonna get a few more, iterate before it, it goes to another, working group last call. And for those of you that are not too familiar, with these 2 standards. Basically, these are oriented towards latency sensitive traffic. It describes some mechanisms whereby applications can mark their packets to be, given low latency treatment in the network. And that is done by creating 2 separate network queues at what are bottleneck links. So traditionally, in end user CPEs, think of like a home router or other places where there's a a network bottleneck. And that location has 2 queues and it knows whether to shut the packets into the low latency queue or to the to keep it in the classic queue, based on the application marking, which you can see at the bottom. L4S uses the ECN header, either ECT 1 or CE NQB uses DSCP 45. And just to differentiate between the two l four s is intended for applications that need higher levels of throughput. Think of things like video conferencing and gaming as an example. NQBE is lower bit rate things. That could be DNS lookups, you know, other kinds of for signaling traffic. And so, this talk is about taking this stuff, you know, from early experimental phase out of the working group. Into the field and how we're measuring that as an operator that does weird stuff sometimes. So next slide, Yep. I'm back. There we go. So we announced the trial in June, lot of customer interest, which was really interesting And then, from July, we began the trials in earnest And we broke them up into really 2 major phases, the 1st phase being Upstream. So only doing dual queue in the upstream direction, which is the"
  },
  {
    "startTime": "01:24:02",
    "text": "generally the most constrained and asymmetric, networks. And then, more recently adding downstream. There's some ancillary things that we did, at the same time, which I'll mention little bit later. In terms of technical requirements, this is all on what we sort of refer to our next gen platform or virtual ah, CMTS platform and a wide range of, of cable modem technology, both ones that we control the software code on and some of their retail. In terms of the tests, we took a varied approach here. So Some of our tests automatically report. User goes and clicks a button It goes and runs the stats, collects them, And that's great. Because users just, you know, easy to click a button or have to test run automatically. We other also had other activities where we asked users to run tests. And while they could relate some of their qualitative experience, many of those tests would give some, quantitative output, and we ask them to enter that into a basically a web form. And that's good, but, you know, it doesn't work well after the first few users. And what users type into forms is very interesting and varied. So we learned a lot about form design and, you know, how how to do all that stuff. And then, also, really beneficial. We have application, partners that are participating in the trial with us. And, obviously, they're traffic is all encrypted. And so we can't really see even through imperfect inference? Do they have really good quality or do they have crummy quality? And so asking them to provide some stats from their server and clients has been useful. And then we also had a wide range of 3rd party test probes and I'll talk about why, in just a second. So next slide, So this is the first of 2 really, mind numbing diagrams, the the first one is to show on the left hand side, we have this thing called INP Internet measurement platform, very novel name. That's basically a software module"
  },
  {
    "startTime": "01:26:02",
    "text": "that runs in our, CPE, our home gateway devices. And it can run you know, ping tests, latency under low tests, speed tests, you name it. All these sort of quality metrics. And That is there and running, and we're running special tests on these subscribers. The first, really, thing of interest for us was does this perform as well as the existing subscribers? Basically, have we done no harm here by adding this new queue. So that's one of the key parts and helped us with. And then we also added testing to send packets through low low latency queue as well, which is good. And also that schedule that's not in control of the user. So it doesn't suffer for many of the sort of, messiness that goes with you know, people typing things in the forums. We also derive a lot of stats shortly out of the cable modems and the CMTSs. So you know, things around RTT and and other kinds of quality metrics. We have users that are both windows and, macOS users, or just sort of iOS in in general as well. And, you know, there are different applications that we ask them to run on a scheduled basis every week. They have sort of a list of things that they go through and that we ask them to do. And then on the right hand side, we have these 3rd party probes from 1 as university Chicago Net Microscope. Right Atlas, we're all familiar with them and Sam knows. And, again, those are in the background none of them yet have a way to test dual queue. So, for example, the Samnose probes aren't running an NQB test. In their latency under load or or something like this. But what they are there is again to provide an external validation of is everything performing as well as we expected. And all of that on both sides with our own platform and those third parties were meant to answer one of the questions that at the working group, which is is the classic queue going to be starved of bandwidth"
  },
  {
    "startTime": "01:28:00",
    "text": "in some scenarios when this slow latency queue gets used And so, you know, that was helpful for us. Next slide. And then as you would think with any other network, you know, there's a whole variety of things So we went out to our different parts of the network team said, hey, we're gonna be doing this trial. What, measurement sort of things are you doing and can you start giving a stats about this of course, everybody comes out of the woodwork and says, oh, I've got all these stats. And so we have a way to collect all that and and essentially dashboard it. To monitor things over time and, and make sure again, things are performing at least as well as the existing population. So next slide. So some selected measurement findings, the first problem made that that we found was probably a pretty elementary, elemental one. Which is We had a test that was unidirectional, and it should have been bidirectional. So we were testing to make sure that we were not bleaching ECN marks, you know, the ECN header and were like, oh, we're patting ourselves in the back. We're doing a really great job. We're not bleaching. And then we're like, Oh, wait. Yeah. In the other direction we are, And so, it was actually related to another DHCP related issue and It was on all of our cable modems, and we went out and rolled out a fix, so that was good. And, and and glad to have that fixed. We also confirmed that, initially, just via basic p caps and interface stats on machines that there wasn't classic q starvation, again, being a core concern, out of the working group and then doubly confirm that on a more automated basis with the probes that we have embedded into the cable modems and with 3rd party probes. And I will say related to that, we actually saw the inverse of what the working group was worried about, which was the working group said, oh, what if the classic you get starve. We actually found we were starving the elf rescue, because of miss a misconfiguration on our side. And operationally, what we found, which was really interesting, is unlike our implementation of"
  },
  {
    "startTime": "01:30:00",
    "text": "pie in this case, DOCSISPI as an AQM, which was real literally, like, on or off. It was a binary configuration for us, which was really wonderful. We found this long list of configuration parameters go on, and a lot not to from our, cable apps group, and some of the testing that they had done in the lab was like, very short, distances of copper. And so on. And so when we were actually in our live field, we found we had a lot more tweaking an experimentation to do. So we actually just recently did a couple of weeks ago, a very big update of our cable modem configuration, and, and And so we're now in the process next slide of essentially rerunning all of our tasks. So as we've started to do that, that one INP test what we have shown is, and again, these are synthetic tests. The green column is, you know, classicqlatencyunderload. You know, there, you're seeing about 51 milliseconds the non queue, which would be L4S in this example, 26 and unloaded is about 14 These are good, again, for providing a baseline. The green one, just making sure that this population looks similar to, to others in the broader customer population But, ultimately, it's still a synthetic test. It's not, you know, a real, representation of user traffic, which is a little bit harder to do. So it's a good data point, but it's not necessarily determined to have Next slide. Is better is getting, stats from an application provider. And so one of the partners, NVIDIA, which has GeForce Now as a gaming platform basically were able to pull stats from both their servers as well as their clients and and basically see, you know, what their cloud gaming, customers experience And, you know, they saw a a demonstrable difference between the kind of lag spikes that they saw on game."
  },
  {
    "startTime": "01:32:01",
    "text": "And importantly, for them, it it was really just you know, more consistent latency than anything else. And and I think that was one of the lessons that we've been taking away from the trial is that the consistency of that latency is just such a critical factor. Because the application conservative adapt to it and it wants to sort of stay at that level. And, you know, things come along really nicely. So that's been good. And, from their standpoint, I don't have them now, but, there's, I think, some screenshots. If you go to the next slide, might be there. We've actually got some videos that they gave us. And just to show this, what you actually show in a split screen of, you know, a little bit of normal load without L4S and then L4S on the other side is you know, playing a game, you're you're actually seeing moment to moment sort of like you'll jump ahead a few frames in the game as you're playing. Basically, you're missing, like, fractions of seconds of time. Whereas, you know, with L4S in that load, it's a more sort of smooth experience that you would normally expect for that kind of cloud game experience. Next slide, And then Valve, was one of the other partners. So Counter Strike, by the way, I should just to digress for a second. It's super fun to play games as part of your job. Be like, yes. I'm a really testing boss. Like, this is this is totally legit. But, you know, also notice there that their loaded latency was actually pretty close to what they observed for idle, just sort of normal. Traffic. So again, a nice result there. And, valuable than the synthetic testing more from our standpoint, point, point Next slide. So, we are continuing to test as I mentioned, we pushed a new boot file, a configuration file out to the modems. Just of weeks ago. So all of the tests a couple we're in the process of rerunning and that includes both L4S and NQB tests. And all the things behind the scenes from a monitoring standpoint, automation,"
  },
  {
    "startTime": "01:34:00",
    "text": "provisioning all these kind of things. We're getting ready to basically be able to scale it to millions of users. And that's just a shorthand way of saying make sure it works so that we're not waking up during the maintenance windows to do things And there's not anything manual that has to go on. Nobody has to look at logs that alarming thresholds are set and all the normal kind of things that you'd wanna do operationally. And, Nick, I think that's probably it. Next slide, just in case. Yeah. I don't think so. That's it. So happy to take any questions if there are any? Yeah. We have a long queue. Thank you. Oh, no. Basically, I'm just curious to see The, P99 latency was 50 milliseconds. That's probably, like, way better than actual sort of worst case latency, right, because it, you know, I guess p 99, it's like 99 is still too low. Right? If you measured 99 point 9, it'd be more like 300 or 500. So it's actually better than twice as fast. I is is what I'm saying, I guess. Right? Yeah. It's it's already good now. I should that's a really good point because that population are all DOCSIS 31 modems. Which by default means that they're going to support active queue management. So Docs' pie, our older modems don't have active queue management. You are gonna see 100 milliseconds t99. But, you know, AQM certainly helped a great extent. So I was gonna actually ask about the same slide. If it it would help if you put take slide please. 7 up, That bar chart yeah. So so that green 51 Millis seconds then. That's, Yep. That is with pine, isn't it? Is. And and I should mention one other thing. It's all on network for us. Right? So you always wanna know, like, where are you testing to and from? This is gonna be from an agent embedded in the cable modem. So no Wi Fi craft. You've got that to one of our regional network servers. So not over here in your connect, not over some distance after that. So sort of within the things we can control, yeah, go ahead. Yeah. So but in terms of The advantage"
  },
  {
    "startTime": "01:36:02",
    "text": "that would be perceived by users if they switch from a normal network to the blue bar there. Yep. Yeah. With gain be the the would more than twice, wouldn't it? It would be Yeah. And they're typically 100 in the Yeah. And and the other problem with this Again, synthetic measurement is that, like, then you have to go in behind this and say, okay. Like, what's the load profile? Like, what are you what's the load generation? And so We've toyed a lot with that. And we have other tests that run that that that that gather stats where a user runs it from in their desktop. And, you know, do you do a burst up to 90% of the the and then another one to 50%. Again, it it's really hard to come up with some sort of synthetic profile that's representative of kind of normal traffic user might generate. So that's why even when I see results like this, I still discount them a little bit because you have this notion of trying to created this synthetic load in the background. Right? Yeah. That's why I like much more looking at, you know, the slides where you know, the GeForce now stats were shown because know, there's nothing else going on in that user playing their game, normal stuff happening over their network that you would see in terms of normal working conditions. And that shows an even better benefit. So that to me says from a test design standpoint, that synthetic test in our gateway is still you know, a little bit primitive, if you will. No. It's all really good data. Thanks for sharing it. Yep. I will check the value from error. First of all, this is the thanks for your technical leadership to get this to so many users. This good data. I had a clarifying question on the slide before this which was, I think it was upstream jitter that you were showing. So is that just keystrokes? So if that's a game, is this 1 or the one before? The one before that. Yeah."
  },
  {
    "startTime": "01:38:03",
    "text": "So, again, this would be a cloud game. So there's no console in the home. That's running that game locally. Basically have a cloud server that's running your game you have a controller that's in your LAN, so you're gonna have control input generally you know, like, you know, you're gonna be moving around a board, shooting, you know, those kinds of things. So it's those control inputs that are going upstream And that's obviously very important from a gaming standpoint. Right. But but the but the this was not the streaming video hitting the dual queue, but it was more like the keystrokes or was hitting the dual queue. Because it was in the upstream. that In Is this case, from the upstream direction, yes. Now in the downstream where we've got, which was the next slide, and you know, the videos that we've subsequently know, had them produce and record. That's where downstream low latency is being used as well, where you are seeing what's showing up on the screen. Being smoother instead of that sort of jerky experience. Thank you. West Hurdaker ASI. Thanks. Excellent presentation. 2 really quick questions. 1, You said that you couldn't, you couldn't actually get notice that class acute starvation is happening. Did you try and force it at all? I mean, did you push the boundaries of, like, We tried all kinds of stuff. We couldn't pull it off. With with No. I mean, again, it's, Trying to to produce something that's realistic that you might see. And and we had, some of our security folks that even wanted to get involved because, like, someone who's gonna do something really in the I guess, and try to, know, choke everything you off. And and so the one thing I should say that that's in the the L4S standard is this notion of cue protection. And so There is a notion of, sort of a bleaker function that's looking at packets that get marked and go into that classifier to say, do I go"
  },
  {
    "startTime": "01:40:00",
    "text": "to the low latency queue or the classic queue, being able to sort of catch something that's you know, matching a pattern which, like, the malicious sewers, you know, harmful in some way because it's, incorrect. So Right. So that that tailors into my second question. You also said that you couldn't get like real representation of user traffic. Did you consider know, there's well known servers and ports that you could just, you know, mark you seeing for those and, for other ones to try and get realistic traffic. Yeah. that. I mean, we certainly, we can do It we tend to find after a lot of years to to you know, put this really directly It's really hard. To synthetically duplicate all the stuff in the lab. It's It's often better just to put it out in the field and see what happens, you know, in a controlled fashion. Right? Where you can roll things back and you have it the well instrumented. Yeah. I mean, that's why I was thinking, you know, just for for some of your users that you know they're playing this game. time. Right. Market and see if there's a difference between the latency of those users that are actually playing at the Yeah. I think we've In in that case, we've just found as much sort of futzing around we can do in the lab to try to reproduce a synthetic notion or recorded version of traffic. It's it's better just to put it out into all the massive permutations of stuff people have in their house. And then, have them tell you what their experience is and measure it independently, Alisono. Good evening. I sort of have a question about what your future thinking for this sort of functionality is, specifically as far as I understand, you partnered with specific application providers like Nvidia or, or Steam but would you see this model of partnering Specifically, wind. Providers versus like a more general model where someone might, you know, onboard onto this Alvarez Functionality more, like, generically Yeah. My hope is that it's"
  },
  {
    "startTime": "01:42:04",
    "text": "just totally generically available as a network function that people can do. Only we only have partners because we needed some willing people that could give us data. But ultimately, you know, as a say at layer 3, I don't wanna know what people are doing it layer 7 to use this. It's just sort of more like, hey. You can now mark for, you know, L4S or NQB. Enjoy. Like, we don't have to tell us about it. Unlike, maybe in contrast, you know, Network's licensing or something like that where there's an API and you've got sign an agreement with us and all that baloney, you know, anybody just be able to do it, and we would encourage you to go out and instrument. Some of your tests with it. And, know, like, is BGP safe? How about, you know, do I have an NQB or something like that? But so, so, like, my follow-up question is, Would you see this being you know, abused in terms of, like, maybe by mistake or, like, on purpose, like, oh, sure I would like low latency for all of traffic. my Sure. Well, So I I I get to ask ask that question a lot internally, and my answers are two things. Number 1. What, first, what a great problem it would be to have if we had a lot of people marking traffic this way, right? We're in the opposite, which is like trying to get people to implement, But secondarily, It should be a sort of a good self correcting kind of feedback loop in a way. So if you're an app developer, you want your app to perform really well. Right? And you're not gonna do something dumb that harms your app QOE. So if you're not know, if you wanted to be Q Building, and you weren't know, latency sentences sensitive, but you marked for know, NQB or something like that. Like, you know, your app experience is gonna be pretty terrible. Right? And you'd notice that and be like, oh, maybe I should go update my app. I shouldn't have done that. Right? Just in the same way that if you did something else, dumb and HTTP or whatever it might be, know, you should be able to go out and"
  },
  {
    "startTime": "01:44:00",
    "text": "notice that and go correct it and fix it and fix it and Okay. Makes sense. Thanks. Yeah. Yeah. Yeah. Yeah. Yep. Thanks. Thank you for your time. Yes. I guess you all have noticed that we skipped a talk and somehow we don't have speaker, unfortunately. So for that talk, you can check out the slides they are online. You can also read the paper And I was hoping to have, like, a few minutes time to do a power pawn car okay for this like, for the talk that we skipped, but we use all the time for questions. So this is our last talk now. Go ahead. Am I audible? You are. Okay. Thanks. Hi. My name is. Today, I'll be talking about implementation of mobile app filtering in India. This is a joint work in collaboration with Cartikay, Rashidin, and samodo from Delhi, As you can see from this particular map, almost all countries engage in some form of filter What is filtering? Is a control of what information can be accessed published or viewed over the internet. Acted by regulators, typically governments. Internet filtering is overall on rice. We have seen increasing number of internet shutdown, banning of apps, Even EU countries have started doing censorship, namely Greece and Spain. Russia is a country that legally passed the bill regarding internet censorship and then there are some usual suspects that have been extensively studied about the traffic filtering practices. In this work, we focus on India. Why? It is the 2nd largest internet user base. And is much less studied when it comes to traffic filtering practices. Previous studies, although very few in numbers, demonstrate that Indian ISPs follow a feudal model of censorship where different ISPs have disbanded block list and different filtering infrastructure. In 2020, we observed an emergence of a new phenomena called as mobile app filtering."
  },
  {
    "startTime": "01:46:02",
    "text": "Where India officially banned 220 Chinese apps. Thus, our objective in this research was To study this new form of blocking, and then studied mechanics in detail and also come up with some possible circumvention techniques. An initial observations were like this. After the ban, all the blocked apps were unavailable in the official Indian app store example, Play Store, the Google Play Store. This was expected. Thus, we obtained the APK files of these 160 band apps from 3rd party sources like AP Game Middle. And we want to observe was fortunately 136 of them were directly accessible from India. But something fishy was happening the remaining 24 apps that even after the installation, we were observing the network connection errors. If you are in India and you open a website, open an app called a TikTok, this kind of a message would be shown to you. There is no need for connection. And in some of the apps, there is an overt blocking, is it saying this app is not available in your region following the orders of the government of India. When we investigated the peak apps to study, like, there's a potential for ISB level filtering happening. What we observed was there was there was a legitimate IP address belong to Akamai for TikTok. This indicated that there is no DNS filtering happening. Otherwise, you would have got a incorrect or a Bogan IP. After that, we observed a successful TCP and TLS handshake happening which indicated no TCP IP filtering or meddling with the TLS client alerts and aid packets, and then we even observed a legitimate certificate of dotcom. Which indicated no certificate dimpling or a large scale possibility of man in the middle of the entire And then there were encrypted data exchange. And in the apps, when we observe the block's message, it was also a part of this data exchange. This indicator that the server is them is itself filtering the clients."
  },
  {
    "startTime": "01:48:02",
    "text": "This ruled out the involvement of Indian ISPs in our Filtry. And this marks the departure in how the traffic filtering happens in India. Before this particular point where ISB's were doing filtering they were blocking the websites at different locations. Now The app publishers are themselves restricting the client. This we ask this question. How can the app publishers basically deny access to the users? And we are we hypothesize that they may be using geoblocking either based on source IP or restricting content on sedans. Although this this audience, it is not a new term just but but just to set the context, If, for example, a TikTok has an origin server, its main application logic is running on some solvable in America. It's going to be replicated a across different replica servers called as front ends or edge servers and different users in different regions are going to be mapped to those locations, based on the geographic proximity, to reduce time and increase throughput. But why I'm even discussing sedans? Sir, what if I simply use a VPN? I should be able to bypass this kind of a filtering. But my my intention was to analyze which precise factor is responsible for causing this particular traffic filtering behavior. It is either the source IP or a CDNID server is restricting the content. But using the VPN would change both. How? If I'm in India and I establish a tunnel, VPN somewhere in US. It's not only going to change my IP address. From IP 1 to IP 2. But also, it's going to change the front end that I would be mapped to. For example, in India, if I wouldn't have not been using the VPN, my traffic would have originated with the source IP IP 1. And terminated into a CDN edge server, which is serving the Indian clients. But now it's going to originate from IP 2 and terminate at a CDN edge server."
  },
  {
    "startTime": "01:50:03",
    "text": "Which is basically serving the American clients in this particular example. So with this, We have changed both the variables if I use VPN. IP address as well as the CDN edge server. And I will not be in a position to basically disambiguate which particular variable is at play in order to identify the Indian client. Thus, we had 2 variables and 4 cases. The first and the 4th one are pretty simple. If I'm in India and I open the app in my phone, I have an Indian source IP and I would map to an Indian edge server by default. And the fourth one is also simple if I use a VPN or I or there is someone who is physically traveling outside, they will have a foreign foreign source IP in foreign itself. But an interesting point was, how can we obtain the second and the third scenario? That is the traffic originates from India, but eventually terminates in the CNN server that is hosted outside India. And vice versa. For that, we require the knowledge of sedans. We identified that these blocked apps were relying on DNS based sedans. Heard you all to refer to our paper for more details, but high level, the idea was that We tried to, you know, resolve the domains associated with the apps from 5 different vantage point across the globe. And when we obtain different IP addresses, basically that indicated it is DNS based Indians, and then we use Presort to confirm the traffic locality. The the interesting point with DNS by CDMs is if a client the client obtains the IP address of the front end which is closer to its DNS resolver. How I use this knowledge? If in the app, if in the phone, I configure my resolver not to be the Indian one but say somewhere in America, my traffic would eventually eventually lead to a front end that is going to be in America. And this is how we basically analyze the 4 cases. The first one was quite simple. Indian source IP Indian and server opened the phone in India, 24 apps for an accessible."
  },
  {
    "startTime": "01:52:00",
    "text": "Now the second point was I wanted to create scenario Indian source IP and then the traffic is going to foreign edge server. For that. I configured the phones in India to use open DNS resolvers. In uncensored countries. And then again, we observed that the 24 apps were inaccessible. The Here, we just changed the one variable from Indian edge server to a foreign This basically indicated that maybe source IPs at play. This is how they're identifying. Then to further confirm the hypothesis, We changed. We come to the 3rd scenario. We are aware we have a foreign source in an Indian headset. At that point of time, I was in Germany, and they configured my phone in Germany to use DNS was already in India. With that, the traffic eventually terminated to a resolver that is serving the clients in India. What we observed was here also 15 out of 24 apps were accessible. But something interesting would happen with 9 and 9 apps were still inaccessible. This basically indicated that there's something happening at the Indian servers also. Maybe they are not caring about the source IT. Whatever request is coming to these bearing this block domains, they are not going to basically respond. Then at the last case, I connected my phone in India to a VPN and we obtained the foreign source IP and edge server. And now ideally, all the apps should be accessible. But to a surprise out of, like, previous 9 apps, only one was accessible. That's the remaining 8 apps required for the investigation. And as a case study, we again examined the TikTok. What we observed was, like, our goal was to see the traffic in plane and we use a medium proxy. And we observed a myriad of parameters for having a country called as in. Even when we ensure that the location parameters are turned off that we could have, like, turning of the GPS, changing the local settings Germany, not in India, although the phone was in India. But still, there were parameters like Carrier region was set as India."
  },
  {
    "startTime": "01:54:00",
    "text": "And just to basically, as a last resort to identify, like, how the apps are identifying the region as India we reverse engineer the apps using a Jadex decompiler. And what we observed was something interesting, a piece of code, That is these apps are using this Android telephony manager package. Where they're actually accessing the location of the client from the SIM card. They're probing the SIM card. And accessing the ISO code of the country. And to further confirm that we are on the right track, What we did was We changed the region from I n to US on the fly using the and also we removed the SIM card. And we were able to basically bypass the particular, censorship that was happening Then to further, like, I'll give you a high level idea of what these apps were and, how different techniques were being at a blade. There were 15 apps, for which the application server was just looking at the client's source ID. The 16th one for which both the client source ID and the in that server was responsible. So even if the traffic is originating from a foreign country and it is terminating at the CDNS over the Apple And these eight apps sensor. were something like having the most sophisticated logic they were not relying on the source IP or the server. They were rather probing the SIM card to identify the locale of the client. But this is not the end. In 2021, Indian government imposed a permanent ban by saying that these apps are not going to be allow. I think the discussion basically halted between the government and the app providers. And after the ban, what we observed was this mechanics changed. Now the same apps, rather I would say 7 apps out of the 8, are not only looking at the client SIM card, but also identifying the client locale based on the source ID."
  },
  {
    "startTime": "01:56:03",
    "text": "On the circumvention side, this makes life somewhat complicated. Now, the users have to remove the SIM card, connect to a Wi Fi and now use a VPN to change their source IP address. At a high level, We can divide this app censorship or app filtering into 33 tiers. In tier 3, there were 160 apps that were just you know, blog at the app store level. Out of these 160, 23 apps with geoblock based on IP address. And these 7 apps not only were geoblocked and were removed from the app store, but they were also probing the SIM card in order to identify the location of the client. With this, I would like to conclude my talk by saying that we conducted a 1st systematic study about this ad filtering practices happening in India. Here, we make an important point that Indian ISBs are not at all involved. Instead app publishers are filtering the content likely on the orders of the government. Report this novel 3 tiered censorship and we were able to circumvent all the block With this, I end my talk, and I'm open for questions. Thank you. Yeah. Thank thanks a lot. Very interesting talk, and we have a couple of questions. Please. Alright. Tommy Poly Effold. First, yeah, thank you very much. This is super, super interesting, and Thank you for digging in. So the question I had came up kind of earlier in the talk. And I don't think it impacts of the results you got because you Doug very deeply. But I was curious when you were changing which resolver you were going to do you know if the resolver you you were using is one that takes the client IP, like, the the the client subnet into account. Because also depending on which public I you need any DNS? don't know if Exactly. The EDS 0 client subnet. Because sometimes that will cause you to still be in get the results for the location you were in. So is that something you controlled for? Yeah. Yeah. Yeah. We actually looked into this aspect and we we looked at, like,"
  },
  {
    "startTime": "01:58:01",
    "text": "EDNS is these resolvers are not looking at the EDNS ones. Correct? Like, open DNS resolver and the university resolvers. So we confirmed when we were doing these experiments. Okay. Great. Thank you. Dalini, Louisne Alkins. Yeah. Yeah. Really great work. And one question I have is Have you played around with Esims at all? No. In this work, we have not played with the e sims rather just an interesting observation there's a there are dual SIM phones. Correct? So whether this SIM card is present in the first slot or the second slot, still the censorship will happen, but I have not played with easins, this one. Yeah. Yeah. Yeah. No. Cause I think I must have, like, 4 e sims on my phone right now. Yeah. Yeah. Abhishek from Meta. Yeah. Great talk. Couple of questions. One is, for your man in the middle proxy Did you have to, mark around, with a certificate pinning at all. Yep. Yep. Second question is, was the study limited to Android, or did you also look at iOS. Yep. So for this man in the middle, we use the older version of the app because in the newer version, there's a problem with the certificate pending. But if you just downgrade the 2 older versions, we were able to see it in plain text. And in any case, eventually, when we reverse engineer the apps, that was the latest version. And then we confirm that is based on, you know, same based, blocking. And the final act was when we removed the SIM card, we were able to bypass it. But, yeah, certificate pending was an issue. The second question is, did we look into the iPhone? So the point is for iPhone, we were not able to, I didn't able to obtain, you know, these 3rd party the the the application, code from the 3rd party sources."
  },
  {
    "startTime": "02:00:03",
    "text": "So we didn't consider. All the something interesting was there. There were some pre installed apps in in one of our, co authors, iPhone. And when this app filtering started happening, those apps also were basically not working. And again, when we remove the SIM card from the phone, the same thing happened there. So we very strongly feel that they are also you know, probing the SIM card, but we have not on into the technical details of how it is happening, but just a cursory analysis we made. Okay, Ben. Last question. Hi. I wonder if you could help me understand why these presumably foreign Entities are complying. Or appear to be voluntarily complying with this order, maybe relatedly, Could you help me understand if these CDNs Our in house for these for these companies, or is Is the CDN potentially another another third party in this and yeah, network of then, information. Benjamin, How can I explain you even we are in the position to figuring out ourselves? Like, why this is happening? So the thing is, we strongly believe that when this particular, ban was imposed, there were a lot of media hype about this and These app publishers were in like constant discussion with the government that the threat to privacy that you are thinking is not that very serious and do not just impose a blanket ban. And there will hope that, you know, if they're going to abide to the initial ban, maybe, you know, there will be in the good books and eventually the ban will be removed. So that's the first point. But eventually, the discussions didn't went into the right direction and the band was basically made"
  },
  {
    "startTime": "02:02:04",
    "text": "permanent. Now why CDNs are doing this add. I don't know. Maybe this is between the application publisher and civilians, and that's that's a very important part of the future work to look into this aspect. I think This is just the tip of the iceberg that we've been able to identify. And I strongly feel that in other countries also, this might be happening, but although we don't know this to what extent the apps are, you know, being filtered. But I cannot say it for sure. We even send emails to some of the app publishers that I don't want to name, but we didn't but the response that why they are doing this and what would be the potential reason. That they're so strictly adhering to the bands. Okay. Thank you. Thank you. We are out of time, but please come back with your the results of your future work. Maybe in person next time. Thank you everybody. Presented everybody who contributed asking questions. Also, Tommy for taking notes. Dave, thank you as well for staying up late. Thanks so much, Muriel. See you next time. Bye. Have have a good week, everyone."
  }
]
