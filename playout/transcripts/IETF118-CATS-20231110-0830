[
  {
    "startTime": "00:00:13",
    "text": "It's time. Okay. We are going to start our meeting. Hello, everyone. It's a cast meeting in IDF118. And we are, Asian and Pam as echo shares, Our a day is Jim. Due charts and, family is our WG's Secretary. Yes, you can find every time we fight Cass? For you. And it's our 3rd 3rd time meeting. So, find a story cats as as a get for you. And, So, for the note Northwell, please members that you are agreed to follow IETF process and the policies. And if you are aware, any IETF contribution is covered by patents, patent applications you must disclose that fact. And, personal information that you provide to IT will be handled in accordance with I have for us a, statement. And, the list below are more informations as a BCPs document. So, we are asked asked by the ISG that please remember to show your respect to all of the, colleagues at our rooms. And, Well, we, I expected to have in personal discussions. And we are working on the diverse solutions for the global internet."
  },
  {
    "startTime": "00:02:05",
    "text": "And, meets the needs of diverse technical and operational environments. Yeah. So we are individual, individuals, and and contribute to the our WG. And for the, mid echo, we get a new version of the client, and we are using the Q control and the chat is also available. So, Tony will help to take the minutes, but we expect that anyone could help to check, their comments are recorded correctly. And here's, meeting, tips, we have both in person participants and remote participants So for the impose in person participants, please make sure to sign into the session using the meet and keep audio and the video off at all times, because you are in the room. And, you also need to it was a miracle to join the IQ before you stand behind Zurich. My my cue. And for the remote participants, please make sure your audio and video off until, it is your turn to speak. So here are the deliverables and the miles months. Oh, drops are welcome. Please post them in the data tracker and discuss, are the millennialists before every meeting. Please remember that is critical and, the solution draft, now we find some, solution draft aside our, WGs use case, but now we, just to have, WG document as a use case and a requirement by the way, don't have, say, frame framework, W. G. Draft until now. So, setting has as a motivation is premature."
  },
  {
    "startTime": "00:04:04",
    "text": "And, we have done, use case and requirement draft as a milestone. Yes. And here's, today's agenda we, the it is focused on the use case and requirement framework and architecture. GAAP analysis. We will have 4 use cases 2 frameworks draft. And, 1, gap analysis, presentation. And any comments on the agenda? Okay. So, we are going to 1st presentation, Okay. Thank you. Mark, could you share my screen? Yeah. We'll we'll just Okay. We'll just move on to the next slide set, and then we'll, we'll actually give you control of our slides rather than you doing share screen. Okay. I I mean, they might will be some animations that probably will miss, but but that should be okay. Yes. We passed the slide control. To you. Thank you. Okay. Right, let us get started. Well, well, 1st of all, thank you for the opportunity to talk about use cases, especially, to use case about AI for me. So I'm Rajeev Ramani. I'm a senior or an engineer, a BBC r and d, and I'm a co investigator in this in the AI field project. And I'll be giving an overview of the, yeah, I think project on behalf of of my colleagues and, and also of of Dan, Daniel King, who is actually in the room, and who will be, coming to a microphone, I think at some point, you you"
  },
  {
    "startTime": "00:06:03",
    "text": "to contribute and answer questions. So we want to actually explain the the network challenges that we have. We are experiencing the project around the delivery of of a personalized media. And how these how these challenges actually, seem to overlap with natural requirements, you know, for the cats working. So so first first, we'll actually, probably talk about, how the requirements for the, AI field project are coming from real business, need and requirements, from the BBC and, the BBC, is one of UK's largest public service broadcaster and probably one of the oldest, in the world And, It has been engaged in in continuous research, as you try to navigate this way in in actually pushing most of its content online and, delivered over IP and mobile networks. It has been, on a on a confused parts of of research innovation on every aspect of the broadcast change, so like audiences, production, and distribution, And that, that history of of innovation, you know, has a number of, notable my students to, to, too long track history me to kind of cover, but, the BBC was, like, the 1st Organization to do, a public TV broadcasters are in, up, like, in 1937, and it has been kind of going through, various innovations like, during the first HD broadcasting, creating the 1st, catch up TV service over IP. Trying to deliver content in a new novel forms, for example, VR immersive content binaural audio, delivering over new networks"
  },
  {
    "startTime": "00:08:02",
    "text": "delivering TV, AR VR360 over 5 g. Contributing to standards, looking at how to improve, the latency and the adaptation we can do over, media streaming, for example, over HTTP and quick And, we recently won an Emmy for the work we have done. On HTO. Okay. So, so the AFM project essentially is about delivering personalized and hyper personalized media experiences at scale anywhere. So it is a a gladry project, running about 5 years between the BBC of South Shore University, and we are mostly trying to, to focus on building capabilities using AI techniques and and tools that will allow us to to kind of create and deliver and we're focusing on our 2, 3 main areas. So, and these areas kind of reflect the different stages of preparation of content, production and, and and delivery. So for today, I will be talking a little bit about Object capture representation, object based production. And then on the delivery side, they compute our way into a gallery where we think it has, the most applicability to the cats working group. Terms of, a use case So, what underpins, personalization, and hyper personalization is a concept of object based media. And, what you're going to say is quite suitable for this purpose because It allows the content of programs to to change to adapt to the requirements of of the user. So a media object is is basically an asset. Any asset that you used to actually make make the content. It can be audio tracks, video tracks, graphics, captions, 3 d models, data streams, textures can even be code like shaders."
  },
  {
    "startTime": "00:10:02",
    "text": "So in in a traditional linear broadcast, for example, you would actually combine those, compose the different media objects in production. And then you would create probably 1 or 2 assemblies. And then broadcast the safe, the same assemblies to to to everyone. But with, with object based media, you you give opportunity of that assembly to happen at consumption at the consumption based on the user preference interaction. And you can't actually make those experiences become, and interactive. So in in the first work package, so the the the in the project that you were broken down into these different work packages, and, each kind of focusing on on those 3 different, areas I've mentioned. So in the first, work package, we'll we'll call it streams as well, So we're talking about creating really objects for utilization. And here we use AI based techniques to extract objects and and metadata from existing and new audio visual streams. So for example, what we would do is, Given a particular scene, we will try to do, active speaker protection and localization. So we might be able to crop you know, a particular, piece of video creation object, we would actually analyze video, scenes and, build a semantic understanding, generate natural language descriptions on the scenes, and we might, be able to delicate the boundaries of events or actions in the scene and create as well. And, sometimes because of the nature of capture, we are limited in the data set that we have. We we use, like, neural object rendering, for example, to build, some, neural objects. So there's some neural network, of"
  },
  {
    "startTime": "00:12:02",
    "text": "of objects in the scene and then generate from your a set of 2 d partial images. We generate novel views not for a 3d scene. And in in stream 3, what we are looking at is essentially, the productions stage. And, how do you combine those different objects together in all sorts of novel ways, but still making the flexibility for the for the user. So here we actually playing around with, AI powered tools and techniques to allow producers to create these personal injury experiences. For example, would actually use, an LLM or GPT kind of model to generate, a weather forecast secure these, the the script, the narrative, feed that to a Texas speech engine, generate the audio, animate a synthetic human, for example, to actually deliver regional weather forecast. What is kind of key to understand is the the, degree of flexibility, flexibility and adaptability that we need So when we create those, those experiences, we, we really don't know the conditions in which they are going to be consumed. It could be in your living room. In a fairly quiet living room, or it could be on the train on a mobile network with a fairly noisy environment. So, modeling quality, quality degradation, encoding, drawing attendees is something that's kind of we are looking at as well. Now, probably more interesting to Dan and and myself we kind of work on stream 4. So we are looking at, compute awareness delivery. And here, we want to actually deliver this person on your content cost effectively and efficiently to mass audiences. And in doing so, you know, we want you to compute, that we're we're server label. And, p."
  },
  {
    "startTime": "00:14:05",
    "text": "So we we are focused on, intelligently provisioning compute and orchestrating our our services on on this compute. We are very much, driven by, keeping costs down. So subbing your scaling is, another key area we are focusing on. And as we kind of, run these particular distributed media pipelines, we we know that things can change. You might have suboptimal pipelines. So we want, we we want you to look at a solution while our the real time adaptation optimization. So to to kind of add a little bit of motivation context to this, this, this kind of area of work as a driving stream for So we know that personal data application will actually need compute very often, rerendering content actually requires GPUs or compute, and personalization increases the chance of actually having to do rerendering or reprocessing, especially, you know, if you have interactive experiences. And for the BBC, we know that that compute might not be available in the end devices of of of, of the users because some like, we don't well, you don't see shaving fast enough in in those, end devices. So to actually achieve universal access, we we kind of looking at ways where we can offload computer to the cloud, but we we knew that offered in computer cloud is can be prohibitively expensive. I mean, if you consider the the case of cloud rendering where you where you you would dedicate a GPU per user, for example, delivering a 1 hour, program per day 20,000,000 people would outstrip"
  },
  {
    "startTime": "00:16:00",
    "text": "by four times the entire expenditure on distribution, for the BBC So, essentially, what we want to do to keep costs down is make, very efficient use of compute where it is available right from the call data centers, down to the mobile edge or edge, edge sites, and to maximize also what we can, use on the user devices as as as compute resources. So what, what the animation would have shown is, it it is not just kind of utilizing, compute resources, but they are available. But we would have migration of services from the call to the edge. Where it makes sense to actually, look at that computation For example, if you want to do, regionalization, able to continue to personalize per region, it would make sense to actually push that service further down on the edge, rather than actually push all your your backhaul traffic or or your processing load onto your core infrastructure. What you would also do is whenever you have this compute, fragments running on your, on in, on your device as your media applications. If the device is not capable, can migrate those green boxes, as you say, to your mobile edge. Further upstream to give a better, quality of experience to the user. Now this to to achieve this this presents us with a number of challenges. For example, Our applications, they need vehicle connectivity to combat resources, to actually offload compute fragments And we kind of realized that as you've seen, we need the flexibility of of migrating compute"
  },
  {
    "startTime": "00:18:02",
    "text": "up and down the delivery chain. This a contemporary approach of of statically assigning compute to centralize cloud locations or to the to the device. It doesn't work, for for AI for me. We we see that, compute and storage nodes should be able to move very freely along the entire the pipeline. And the time is is right for this because we see a lot of enablers that will make this possible. We see know, availability of of data centers at different parts of the network, visualization of hardware resources, cloud game technology, Softhetonetworking, traffic steering, for example. But we realized that This is not just enough. The decision to move compute you know, in a very large distribution system is is quite complex. And, and this needs to be done at in real time as well because deployment that will make can quickly become and, but it needs to be evaluated and, re adapted in in real time. So to kind of give probably, an idea of what we mean by those, deployments and, and, so let's let's consider a couple of permutations. So here we have a typical object based media deployment where we have a relatively powerful user said can do this, substantial amount of processing And then we have, like, 4 media objects. And, we can actually compose this from the objects on on on the device. So is gonna be fairly, a simple approach to scale this up. We just need to throw some more CDN cells, caching cells, for example, and we should be okay. But for more compute intensive,"
  },
  {
    "startTime": "00:20:02",
    "text": "type of media application. This becomes a little bit more challenging. Especially if we we have, low end devices. And, we want to actually have nearly the same level of functionality on those devices. Let's consider, a a use case where you have a a user navigating around your 3 d scene. So now because the the device is not key people, you might want to actually move that that rendering, to the edge. So here we have some lookup data, which is actually being streamed to, to the edge, and that mock up data is is is used to to drive a and animation, and and that, the other technicians use kind of then is is rendered, and then, we are we are composing that, other with a number of other other objects. So The the interactivity that we we might have in this in this use case, we're also kind of add some some more constraints on where we can actually offload this kind of of of compute. In terms of the use cases, we are actually, implementing and, in for me. I would mention, like, a couple of them. So we we we use we are we have weather forecasted as a, an object based where the forecast experience where the user can choose between different, personalization options. Simple and more explicit, Some of them are implicit because the content will instantly adapt to the the device. So at the bottom, of the of the screen, you will see, like, the number those are the options where the user can select, between a signer, or or or or the, the normal presenter, change subtitles, even, how sedarials are translation, or you can even change the presenter to it child version of you know, of of of a presenter or like a animated character."
  },
  {
    "startTime": "00:22:00",
    "text": "Or for visual, visual, usually impaired people you can switch, you can switched to the high contrast weather map, in the background. So we have implemented a number of deployment and, and and permutations of those deployments. And, what you will see here is, at the bottom, the list of personal options translate to a number of of services. And some of the services they govern in the presentation All those different objects. Some of them are, renderers or competitors. Synthetic present her kind of, generation, for example, some of them handle user input. So, The first device, as you see, that completes the client device here, He's a fairly capable So this is it's running rendering and condition, on on the device. And then in the sec as more and more devices arrive, we might end up with a device that is not capable at all. And and there, you will have to offload the rendering to the to the edge so we have here on the on the if I can read that, on the, not the call, but, in the middle of data center there, so you you would have, for example, a lot lot of the services, service instances are actually actually running and then the result, the rendering is done there, and the result is excel stream to, to the, current device. You might have a slightly capable device that can do most of the of the, computation, but you might offload the map rendering, for example, And, so for example, you see that at the mobile we have the map rendering, which is happening. But as you kind of evolve your your your your devry pipeline. You might take advantage of"
  },
  {
    "startTime": "00:24:00",
    "text": "of computations being done. So for example, these two boxes on the mobile edge can move. To the a middle data center where you can actually multitask many users on on, on on the same services. We are looking out to some more compute intensive, use cases. So for example, we have a radio 1 program at the BBC called live lounge where we have artists, to, performing. So we aim at putting them in 3 d environments, in volumetric captures, making, of, of, studio sets and rendering those even have AL kind of, type of experience where you can actually the artist in in your kitchen. In terms of of of of resources. So if you translate what, the services you in terms of things we really need that we are able to smoothly deliver these high quality kind of stream experiences We we think that, okay, we we have the services and services content servers that are actually process, like, media, generate media, stream media, We need a story, content storage. We need caches, for example, we need, CDN components. We need content encoders. We need, low latency pixels streaming, to client devices. And, we need GPUs, of course, whether it's through rendering or machine learning based, in inferencing. So these are examples of of some of these kind of, capabilities and resources that we need. And if you were to kind of organize into of architecture that it would look like this where, you know, we Right? We will run our services on complete resources available across the chain. And strategically making decisions on where to actually put her erandrea"
  },
  {
    "startTime": "00:26:02",
    "text": "where to put a compositor, when to actually move, There was some of the services, you know, from one node to the other. So, we think that, this architecture site, and the services There are 2 additional components, which is needed. So we we need, Carlos Strata and a network controller. These are going to govern, where we actually run things and how crowds are interconnected to them. But for for them to make, to make these we do need a number of of different metrics. So we do need to know where, where the the the concrete resources are available. Where they have been provisioned ahead of time. Or what is the provisioning delay, for example? And would you need to know about response time latency to that to that, computer resource. We do need to know real time, usage metrics? What's the memory pressure? What's the GPU pressure? What's the CPU pressure? We want to know how many users in a service instance can actually support and how many user actually being being, running on on that particular service, higher speed for storage, for example, maximum connection supported, network latency, a jitter, bandwidth, in terms of the caching, what's the caching performance, the, mystery, the, fetch latency, for example, the throughput of, that particular cache so all these are really important metrics that, we need in our decision engine. To to do a intelligent placement. And, we we talk about these 2 kind of possible components. We have a cloud orchestrator which actually, decides on the placement of the service instances"
  },
  {
    "startTime": "00:28:04",
    "text": "so the deployments. So as you kind of have jobs are streaming in, from, from your, client devices. The, the, the, the conditional demand screen we identified, and the calculated will assign users to particular, service instances based on on the metrics I mentioned before. And then a network controller will actually direct traffic coming from the user to that particular service instance ensuring that You know, we are we're reducing user latency and, we are able to optimize, content delivery So in terms of of of of our requirements, you know, for cats, essentially. We we haven't identified a number of compute metrics. Right, and, it would be kind of good to agree on standard set of metrics. Uh-uh and on standard ways of actually collecting and distributing them, we we would likely been tracking within more of compute providers, cloud providers, So what do those, cloud platforms report the standard set of metrics is important. We we now have a delivery chain that involved in just a network with with, with computer as well. So we, we, we need, a certain set of metrics inside of the way of actually collecting and reading them. So then can we agree on on on on certain kind of architectural models, can we agree on what metrics will will be able to get from network which metrics will be able to actually will have to provide us as part of a service instance and how those metrics will be collected anywhere they will be, stored, where we distribute it. Okay?"
  },
  {
    "startTime": "00:30:02",
    "text": "And, we we kind of showed a a a a high level architecture really, so we really want to know how our network architecture And, our AI AI human service kind of model overlap with existing, IETF caps objectives. So we see this day but we want to ask, people the audience. And, Oh, and the question is, do do we want to actually, what's the, what's the approach here? Do we, document, our use case? The eye for me isn't in draft. Or do you contribute to existing use cases? And, that would be all for me. So if you have any questions, I think, myself and and and Dan would be quite happy to to answer your questions. So thanks, Rajeev. That's It's always interesting and valuable to get a a sort of wider view of of of concrete use cases. We have a a nice line of people. First up is, Tom Hill. Hi, Rajeev. Tom Hill from British Telecom. The, the network, I think, on page number 9 looks a lot like the one I run. So I'm, I was interested to see lots of GPUs in there. I think that's interesting. To say that. However, I I firstly wanted to double check if, you know, had had you engaged with anyone at British BT about this at all. And and would you like to we have been in talk with, at least a person with Andy Gawa but mostly on, on the object based media, preparation, or country preparation side. But we would we would really love to talk to actually people who are look at virtual network, function, like the network network"
  },
  {
    "startTime": "00:32:00",
    "text": "you know, for commercialization, while I was actually, create those compute sites, DH. Okay. So, because that's that's quite a quite a large conversation. I think separate to that though, what I really wanted to ask in relation to cats was how do you see your, you know, metric sharing manifesting in a in a fashion? Do you think that this is something that would be built into a an over the top or an out of band channel sort of signaling on the side, or do, or do you think that there is some you know, change to internet protocols or or data path. That needs to be done to achieve what you're what you're hoping to build. In. So I'll I'll try to answer this. And if John is there, he can also, answer it. So I think, right now, think that the, the the answers we don't know because we are playing around with these different, deployments. And, and and try trying to come up with the architectures where we can actually collect metrics and and make those edge decisions about where to push compute. So We don't do it yet. So it could it could be actually, inbound. Or it could be, you know, some sort of as part part of some somewhat CDN kind of functionality, maybe? Okay. Are you done with answers with this question? Might It's Hi there, Dan at Lancaster. So as Rajeev kind of pointed out, we're still exploring options. Indeed, the architecture is kind of fluid itself, what we wanna get embroiled in is, you know, what the underlay looks like. What we're primarily focused on is really how we can get metrics both out of the network and from the cloud nodes, and then how they can be used ensure that the users are attached to points that will deliver the experience required. And in terms of Direct cat's applicability"
  },
  {
    "startTime": "00:34:04",
    "text": "maybe the job scheduler, certainly the metrics definition, how we exchange metrics between the provider of the connectivity, and even, the host that has the, compute resources. The BBC will operate Islands of DC, themselves, but it anticipated that several additional sites close to particular users where BC Web the BBC doesn't have physical infrastructure for the data center, we'll have sort of peering agreements with and we'd need to understand what the current utilization availability, and some of the scheduling as well. Would also be exposed Okay. Thank you for the clarification, and I will, I'll reach out to you, Reggie. Thank you. Danielle, the other one. Okay. Okay? At the, from yours, KEC, I I understand that there are 2 scenarios. The first one is about service instance to deployment where are you you want to optimize the deployment to basically on the matrix to from service and network. Bot. Under this scenario, actually, there's no traffic steering right here. Only about the deployment. And, the metrics of network and computing is quite, comparatively would be right, dynamic. And when it comes to, the service traffic steering. I believe the metrics you collect the from post to the computing side, the networks inside would be far more dynamic, So I I I'm not sure if whether or not we can have, common set of the metrics for the 2 series wrist. And, also, I'm not sure whether a lot of cats can do"
  },
  {
    "startTime": "00:36:01",
    "text": "to address the, requirements of the deployment, the service deployment, Thank you. Okay. Yeah. So so we we actually, of course, want to know the current conditions, for actually to be able to predict the quality experience of the user at the user. And so so for that, we need to know about network, they didn't see bandwidth jitter, and all that at least between a potential service instance and, and and and that particular user. And as kind of you say that there's things we actually evolve and change, what we also do is is, in real time, monitor what's what's going on with, the network connection between the user and the service instance and and adapt the content So we kind of have quality lattice where we adapt the content to be delivered to the user. A job. Okay. Yeah. We will actually charge kind of, differentiate between those different metrics, needed for those 2 different spaces. Chung. Tony from Huawei. So first of all, many thanks to your presentation. It's really interesting to me. So could I invite you to share some information in the mail release or let's discuss in the mail list to see where we can add this kind of scenarios into the use case draft because I think this is a very interesting use case that we might we might, implement and deploy it in the shot him. Thank you. Thank you. Yeah. I think we're we're quite happy to do that. That's perfect. So Wei Qing, I had closed the queue, but if you're really quick, Yep. Very good. So, one quick question. So from the architecture, it seems that and, the network node. Doesn't need,"
  },
  {
    "startTime": "00:38:00",
    "text": "a wire Opissa computer computing metrics, right? You just need a the orchestration knows, much Is that correct? Could you repeat that? So the network nodes But don't need, a while of the computing magic but the the Ultra Streeter neither, understand the the computing orchestrators. Right. Okay. Yes. So I think that the orchestrator is doing a matching between, your computer comments, and then asked, and then with the computer resources that you've got, and the network controller, so you might have, you know, 2 level decision So you might have additional machine which is done by the the orchestrator. To do your initial placement, but I think it's evolved. You might have your network would steering, the the the the the user to 1 or other service instances. So it's something that we are we haven't really explored that deeply. And we we won't actually understand those, demarcation of responsibilities, you know, for, for which and then what would be the metrics needed for them to make those decisions? 6, Okay. Thank you. Brilliant. Thank you, Rajeev. Hope you stay around for the rest of the meeting. And, we can push forward with this to try to understand better what the overlap are and, and where cats can help with your problem space. Yeah. Thank you. I'll I'll look forward to more conversations on the topic. Okay. So, next is a matrix with auto Jody, please."
  },
  {
    "startTime": "00:40:03",
    "text": "Thanks. Yeah. So I'm Jordi from Welcome. Yeah. And actually, the presentation from Rajeev provides, I think, a wonderful context for this the presentation as well. So thank you very much. This is about exposure for communication and compute information for infrastructure where service deployment and selection. There's a draft. We posted, the first one. Rational and computer metrics. But there's also that, actually, that's back from 2019 with was, submitted as part of the outer working group. Position in the area of exposing compute metrics some of the use cases actually that we're we're describing today, actually. So and then there's also side meeting that we had, couple of days ago that some of many of many of you actually attended. Where we focus on exposure of computing metrics. There is a GitHub repo, and I'll probably update, update the slides with that information. So if you wanna go next, thanks just to skip, we can go to the next one actually to I'm gonna try to debrief and just probably focus on 2, 3 slides only. So we can have few minutes for, getting some feedback. But the problem stays is as follows. And to position the problem here, we we're looking at a very simple model. But, which is the life cycle service, and keep it it simple, which is looking at 2 stages when you have a service, first, you need to deploy it. And after it's deployed, then you need to run it and instantiate it and connect to it. From the end user to that, service instances. So looking at this, this full life cycle, helps understand the the whole problem is stays. Service deployment stage 1. The action to take is basically to to set up the to place the service itself. So you need to know"
  },
  {
    "startTime": "00:42:00",
    "text": "Where is the compute, GPU CPU information? And, what is, how much available bandwidth and latency all these metrics that we need to extract in order to make informed decisions on how we place the service who needs this information? So typically, the service provider and this, any position here, we position here, some workers that are looking at this or have been looking at this historically. The out of work group, as I mentioned, out of application layer, it's active optimization. So it's about exposing information from the network. The application so that the application can make informed decisions. Second stage of the life cycle would be service selection. This involves usually a couple of things. One is selecting the service. So now the user wants to connect that service, the service is already deployed. So which one usually might have replication of services so you can decide where to connect to. And then also the past selection problem. Which path to get there, basically. This requires, again, computing communication information North make an informed decision, and who needs this information, could be at both levels here. The network provided itself which is what Katz is working on, and the ingress port, router, figuring out how to connect to the egress. Then also exposing, to the application because the the application itself whether it's a client or a proxy on behalf of the application, which is this information to also make, a better informed decision how to connect. So that's the bottom of the space. We can go to the next one, I think. And what we really like to get feedback on is, to be very, practical is on these 4 questions. Gonna stay then. Question not one is, is it likely viable that the network can expose communication and computer information. The service provider and application. Question 2, are there gaps in the entire service life cycle? Deployment is selection selection. Are not currently being addressed and that are relevant. Question 3, would it make sense to define a common set of communication and complete metrics? That as the various service life cycle stages,"
  },
  {
    "startTime": "00:44:03",
    "text": "Question 4, if so, where should this work be, being held a good, faction of this work has been held at Katz. Is this the place to to extend, to expand this work, or should there be other options? With these four questions in mind, maybe while while the obvious things are a little bit about this, just one more slide. I'm gonna skip through maybe you could go next, next on more things. Well, yeah, some use case go next. And next. Thanks. So, just to position a little bit more where we are in this conversation, ITF117, and, at the end, presented, the the potential, idea of, working together, cast Alto and defining these, metrics. Given the common ground. If we look at the chartered, a goal is to develop these metrics, part of part of the goal. So that, But and so that the, the network itself can make these, decisions, inform decisions Cuts are very clearly specifies that, the network and compute conditions, exposure to the applications is not in the scope of cuts. And here, as we saw in the life cycle of the service deployment, some of these metrics, would be useful to, to expose the service provided to make these informed decisions. So that could be a potential gap. With how, and the question is how do we corporate that. And so then this is, this, appointed to the, to the slide deck by Adrian, which I think connects pretty well to two sides. This sets sort of the context. There was a idea potentially having a side as, having an interim meeting to to, continue this conversation. So with this context, then maybe an the slide and then, These are, well, some principles pretty briefly, we're looking at"
  },
  {
    "startTime": "00:46:01",
    "text": "two principles, which is, on one hand, avoiding drain venting the wheel. So we can leverage work that has been done, or it's in the process of being done, don't replicate that. Second principle, which is kind of like the dual of this this principle is you know, avoiding gaps, making sure we take a comprehensive approach a comprehensive view of these metrics that we define while in some cases might be different, they have some commonalities and and making sure that the design takes into account things like the full life cycle of the service, And then next one. I think it's just, to include back to the questions and, don't know if there are any any comments or feedback I'm I'm gonna ask, a quick unscientific show of hands here. Who in the room is sufficiently interested in compute met tricks that they would review drafts Okay. Thanks. Her in the room is sufficiently interested in commute and compute metrics that they would actually do active work writing document. So Lovely. Thank you. And who in the room is so interested in compute metrics that they would persuade their employer to give up their time and travel budget to come to a meeting dedicated to discussing compute metrics. Okay. And for those of you who're not in the room, that produced no hands, and this is really helpful to me. Hank Shee, you're in the queue. Hong from Huawei. I have one question. You're talking about 2 phase. One phase is for the deployment phase. The other phase is kind of for the traffic steering phase. Do you think this 2 phase will use a wine unifying computing metric model or they are using separate computing Metro model. It seems to me that some medics"
  },
  {
    "startTime": "00:48:00",
    "text": "are gonna be different, but some are gonna be common And in fact, it seems to me that they need to be in agreement because the two stages are connected. So when you deploy the service, you deploy it so that it can be actually instantiated so at the time of instantiation, you wanna make sure that whatever you deployed was deployed in the South metrics, but then instantiation stage can also speak the same language. So It seems to me that, yeah, that there's a need to have a common language between the the very, the the two stages. Okay. Anyone else? Okay. Sorry. One point, I mentioned, common set of metrics, maybe it's the it's don't refer to the same metrics to different usage, right, but just to that people work together, to, make a set of metrics. And, because different usage may have use different metrics, but, it also needs more discussion. And more work being made bright. Yep. I think Right? That's part of the conversation. And, trying to get some, consensus on this Okay. Thanks for pushing this, Jordy. It's I was getting a bit depressed by by the cat metrics effort until very recently when we started to get some emails, it's changed exchanged on the list. Let's try and build on those and work out what really needs to be done and see how to take this forward transcript. So next up is,"
  },
  {
    "startTime": "00:50:02",
    "text": "Hi, everyone. I'm going to, update some, information about the working group document I'm sorry about there are some, problems in the displaying So next up, please. For the document, use cases and the requirement document. So here are some, major updates compared to the dropped in the last meeting, we rewrite part of the introduction to avoid some verbal speech encryption in the introduction part. And, also, we have, echoing some terminologies defined in the, ltbccastframeworkdocument on the service related document, it it it it the service related terminologies, which have been reached on consensus in the last meeting. And also, we will merge computing where AI large model use case provided by AliCloud after, this meeting. And also we the major part is on the requirements part. We here are some updates on the service and the instant instant selection and also some updates on the matrix definition. So I will go into details. So next step, please. So these are the service related terminology is echoed from the, architecture documents or not go into details because, most of them are already, reached a consensus in the last meeting And also you can see that, we have very detailed service definition, and also it's, on on also some, clarification on the document, on on the terms. So next, please. One more thing to mention about terminology is that, we, we received some comments on the, that, of in the previous documents, the, network, edge, and at computing in the document are not, well clarifies. So we have, provided some, clarification on attempts and also leader relations"
  },
  {
    "startTime": "00:52:01",
    "text": "the relations with the, the between the ASH Computing and the network ASH that the ad computing infrastructures connect to corporate networks network edge entry or exit point. Next next up, please. So here are the major updates in the requirements part. So the first, the first major, requirements about survey select survey selection. So, previously, there are only, 2 requirements So we, received the comments that, like, a quickly Slack think the in service instance is not a point viable and not sufficient for the survey selection. So we have, added 3 more requirements. So we yeah, we should provide a time out limitation for selecting service instance because this can provide, some, evidence that, how we can quickly select the services. And also, we must provide a method to determine the availability of a service instance because if service instance is not available, it cannot provide the capability to like, like, a special to traphics, to steering their services. And the last one is when my provide a mechanism for solving the conservative contention problem when multiple service incency with the same service ID are all available to provide computing services. Here are the basic, you know, we're scheduling them. You can use the ingress, PE to contact to the service contact instance and then sharing different service instances. We need to consider 3 different requirements at the same time. Next time, please. And then and, it goes to the metric stuff. So first one is metric, distribution. So we add, one more requirement on the metric collection, because it, even though the matrix collection is not within the cask scope, but,"
  },
  {
    "startTime": "00:54:00",
    "text": "we should we hear from, we received some comments that that we really need this requirement, providing, mechanism for a matrix collection. So next up, please. And also like the previous talk about the metric definition and the usage we, here are some, like, ongoing debate in the list. So we provide the temporary version on the requirement that, in addition to common metrics, that are agreed by cast components like, processing delay. There should be some other ways for matrix definition, which it's used for selection of service specific instance So we try to avoid, like the term slack school because it's also vague expression. So but we know that these requirement might not be, perfect at the moment, but we hear about different voices, voices on metric definition, the list. So, the the talk, discussion is still ongoing, and we will keep update the the requirement on the matrix definition part. And also, we must include a default action, for the interoperation of network nodes, which may or may not support the specific metrics and here we avoid also we avoid the, the discrete a description of the the term flexible The next slide, please. And about service instance, affinity that in the previous document, we there are lots of, like, this a similar description like, service continuity, service, session continuity and and a session, persistence So we think that, basically we, what we wanna do is that, we, we want to instance affinity. So we unify all of these terms to instance affinity in the requirement part"
  },
  {
    "startTime": "00:56:01",
    "text": "So if you guys have, think we we really need to like, we'll move back to previous, description, please provide some evidence in the least. Thanks. So next, please. So in terms of the use cases, update we will merge, a computing aware AI loss model use case, which has been, you know, or which has been discussed in the last meeting and, achieve the overall consensus though, so the the use case to cover about, 3 major scenarios that may be interest to cast working group, like, clock, edge, co inference, cloudash, device, coinsurance, and the ASH loan inference. Next up, please. For cloud edge co inference, it's about that that we need to deploy these models in the cloud and actually can coordinate it each other because the the models may be the a lot, relatively large model will be deployed in the cloud and, some customized model with the be deployed at the edge when scheduling when model is, you know, deployed or inference, they need some, like, AI specific computing, infrastructures to, you know, finish the task. So this case is about clot Clash, Cloud and Ashco inference. In the next app, please. And also, some in some cases that, when there are some AI, you know, AI, embedded AI compute, functions or AI compute, modules within the devices. So we can also split a very large model and push some pruned model inside the device. So In this case, then the cloud and ash and the device can coordinate to make the inference together. So next step, please."
  },
  {
    "startTime": "00:58:00",
    "text": "And in ASH inference, model, we only, you know, deploy the customized, models in the air. So in 3 different, scenarios we cover mostly cover the the AI model inference scenario, which require different pieces of AI, computing resources to, deploy it in different scenarios and compute this task together. So next time, please. Here are, very quick recap and the summarization of all outstanding comments received in the last meeting in IETF, 117 we have, made some point to point, you know, reflection on the comments. So our, you know, our updates has been mocked in blue. And, there are some also, some, ongoing work and the, some further updates need to be done in the next meeting. And we mock them in, in red. So I would just quickly go through, all of this. So for the first one, there's some we we see that, there are some, requirements, prescriptive. For example, ruling auto band methods with loadings in the justification which does not clearly state why auto band is incompatible with low latency So from our understanding that, the low latency characteristic of, cast mechanism like a which use inbound solution is very intuitive. And it does mean that, auto band method doesn't work well. So, just in our scope, we focused on inbound solution and also the mapping of a service ID to address, we think this is very required in necessary since the same Sales ID may be bound to different addresses, but it also depends on the, how the cast architecture is designed. So and also we have update some, you know, terms on the vague expression that quickly selecting the instance and also flex ability, about the metric definition. These are also mentioned in the previous"
  },
  {
    "startTime": "01:00:04",
    "text": "you know, when I, introduced about the requirements So, but, you know, yes, the most important thing is about, the metric definition stuff is still ongoing. And also, the metric collection and the service and the instant affinity we have updated. And also there are some further things we need to update in the next version of the, document is, First one is the use cases are too high level to derive specific requirements. Even though we have, we will merge a new use case by the correspondence between use cases and requirements really in the need further updates. And also the security considerations are relatively high level. So we will also update the security part in the next version. And the formation has been solved. So this is the overall requirements update in the, in this version. The document. So next slide, please. So, for the next step, we will update 3, make major stuff. So the first one is the corresponding between the use case and the requirement part. And, Second one is the metric, definition requirement and also some security related requirements. So we'll come for more comments and the questions. So and also one more thing to mention is that, we have opened, a repository on GitHub. So I know that, not all the people like to not all people like the way that, editing on the GitHub, But if you really prefer doing this pre, please feel free to raise full request on it. Thank you. Any questions? 3 people, 4 minutes, Jim. Thanks. Yeah. Jinky shirt, no hats on. I did notice one thing that that concerns me a little bit, and I don't know whether is something that we need to consider from a working group perspective, but we've got this kind of like fluffy, high level idea of"
  },
  {
    "startTime": "01:02:01",
    "text": "determining metric, compute metrics and deciding which particular service instance which to use based upon those metrics, but we don't have any discussion that I can see of of how you abstract a service instance or a service identifier to an actual service. In other words, service identifier as in and of itself is meaningless. We need to have some, is that something that we we're gonna just assume somebody else is gonna take care of that the operator, whatever. They're gonna know that service identifier. 1 means particular service that they have with these parameters or is that something that, gonna get bogged down in. The reason I mentioned this is that we had this discussion as Adrian he knows in SFC. Working group many times. We never really sold it. Thanks. Yeah. Thanks. Yeah. And I think it's more complicated for us than with SFC because I think for us, middle clusters of packets, need to go to a particular service instance with SFC, it was like just all packets. So it's a bigger problem Joe. Joel Halpern with Ericsson. Thank you for the presentation. You don't need to page back, but you had a slide in which you talked about a requirement that I will paraphrase as default interpretation of metric. I think there are two ways in which that doesn't quite match our agreements. And I would like you to consider when you go to revised this. First, we have said that the processing at the ingress edge from the client of how it uses the metrics to decide where to send subscriber traffic is a local matter. Therefore, saying how it will anything will use the metric by any given metric by default"
  },
  {
    "startTime": "01:04:01",
    "text": "other than saying what it means is out of scope. And conversely, If the metric is not understood by an edge node, The only possibly meaningful default behavior is ignore that metric. Because if you don't understand it, you don't know if you're supposed to add it, average it, turn it upside down. Who knows? It could mean all sorts. It's it's a Mac from minimum The only thing you can do with a default with an unknown metric is ignore it. If you wanna put in a requirement, the closest I can see we can come is to note It's not even a requirement. It's a note it is anybody defining a metric should understand that nodes which do not understand it will ignore it. Thank you. Thanks. Daniel. Okay. Denu Hansi, t e, and I have, 2 comments true for the requirements part. When it comes to the traffic steering. Actually, we we have to assume 2 criteria. One is what do we have? I believe that's how computing or where networking or where information is about. The second is watch the service re request. I mean, is it commons for the service? Such as video, video service, what, what, what, how much bandwidth do you need how much latency you you you you want you want to achieve from it and the end perspective. So, I do not say that part of the requirements in the documents. Second comment is is minor 1. And about the discovery of, surveys the requirements says, the cast leads to associated service identifier to,"
  },
  {
    "startTime": "01:06:01",
    "text": "to a address, IP address. I believe it will be it should be a little banter to, to say, associate service identified to service instance rather than on the IP address because, the exporter of the are in essence is not necessary. IP address. Thank you. Thanks. About the, first comment that I have one more thing to mention that we had, previous you know, talk, discussion on the metric that, you know, some like bandwidth related matrix maybe, you know, can be drift to the processing delay. Like, we can, also use a delay to representing the other matrix So but, we really, know, promote the work on the definition that we hope that people can, focus on the work and, to give different voices in the list, and I think it's a better way to So reach a a consensus, in the near future. Thanks. Brilliant. Thank you. Thank you. So we move on to Yuan. Okay. Can you hear me? We can hear you. Yes. And Phong will give you a controller for science. Yeah. I'll send the control to you. Flowships and synapse my slides. That's true. It still has a problem of silence. Okay. Sorry. But you can talk about these slides if you want to. Or do a little dance while you wait. Yep. Did well. There you go. Okay. Thanks. Yeah. Okay. Thanks. Well, hi, everyone. I'd like to share our related work."
  },
  {
    "startTime": "01:08:02",
    "text": "According to the feedback of the last presentation in 1 March 7th, accepted this Justin's got more focus on the use cases and problem statement. And updated our draft. Well, it seems stuck that I cannot control it to the next slide. Or is there any network thing? Well, it looks okay. Go ahead. Yeah. You wanna change it. Okay. Okay. Okay. It just it just takes a few seconds to up with you. Okay. Okay. About use case in proper statements has lists of several related use cases. 2nd, computing aware of AR or be our scenario mentioned here, for instance, a more specific condition is further discussed here. Moving cloud native applications are typically architected as connections to and more microservices. And was each microservice performing an independent pissed or functionality to compose this, application, which might be located across a number of service nodes to achieve scalable use of value sources and active condition with ubiquitous service instances might be predictable. If the premise holds Marco or even massive instances because that's behind, Castro. Within this duration of procator, we may discover corresponding potential problems and the rest. Firstly, it should be acknowledged that the computing related matrix could be diverse and quite dynamic. Especially wrong metadata. The computing delay, circulate, and bandwidth utilization, for instance, And with the relevant attributes or selected as representatives, or related items."
  },
  {
    "startTime": "01:10:02",
    "text": "It could be a catastrophe for health control playing which is required to record updates and maintain massive and frequent variance matrix. And regarding the premise we raised previously, massive instances which led to a worse circumstance. The number of catheters multiplied by average instances per catheter, interests are recorded as any customer here. Another problem also occurs that's the capability and capacity of a single site or a service instance is always limited And just achieving load balance among age size or servicing expenses could be an indispensable requirement Here, we present a simple condition in which a round robin strategy is applied A simple time slots of 10 seconds is allocated for is qualified and applicable service instance. And to simplify the position, identical weight is presumed here. And then we may place our attention to a remote have ingress catheter here at the bottom of the side To select the names or design it service instance, the function at the ingress catheter updates sporadically Well, 10 0 to 10 seconds for instance 1 and 10 to second 10 to 20 seconds for instance 2. It's paid the same 20 instances that's located at egress cut through to 1 year, Or there is a traffic issue with identical cars ingress in a comparable stable manner the selection with instance granularity would change very frequently, As awesome, mentioned into the worker use case drafts network pass causing the current routing system, you really did not change very a complaint,"
  },
  {
    "startTime": "01:12:01",
    "text": "ever matrix that are not oriented towards compute key abilities and resources, and general can be highly dynamic Therefore, an aggregation scheme might be required in cats. A public example is raised here. The suppose a solution instance with the lowest of computing and processing delay should be considered as to move to move. Selection. Other constraints and objects are temporarily ignored here. And for a cast region that has instance 1 to 4 located locally. It may not even be a necessity to expose and distribute all details It is quite simple because an instance, which is not capable of being selected as a fast locally There's undoubtedly no chance to raise optimal selection globally. Furthermore, if, a instance 1 always wins to be the locator or original representative. Other suboptimal performance among unselected instances. Is not required to be carried. Representative matrix may also seem much more stable. A scheme to calculate the representative performance could be diverse. And average value, the best value will be determined to expose on demand. They're back in to be generalized aggregation methods. About computing related major distribution. There are some requirements mentioned in our work groups use case and problem statement, which is requirements aid must provide mechanisms for major collection. If I meant nigh must provide mechanisms to distribute the matrix and recommend 10 misrealized means for a reconstruction for distributing of matrix But as analyzing the previous sections, a dancing message required in the common term may not be sufficient. The correspondingly, we proposed our incremental suggestion"
  },
  {
    "startTime": "01:14:02",
    "text": "that should provide a mechanisms to aggregate service matrix for distribution. This matrix aggregated So they're interest collected, stored, and maintaining the consupling is reduced. And a cat's rooster only reports the information of local instances and the reproductive interests for the other catheters instead of or instances. Nothing instances plus global cat's rooters versus the number of cat's rooters multiplied by average in per catheter. The results of comparison is quite evidence. And additionally, since the interest aggregated the frequency of entry update as declined. And the behavior of service reach for calculation use terms. So let's review the previous slide. Visit service instance guardian, our selection as our ingress rooster this every 10 seconds, I have a risk 20 instances to locate a synccast egress seeing as a senior representative of a catheter egress a fluctuation as an ingress router would head for 200 seconds, which directs to a same cast egress not lost, lost, And at the conclusion we here presents our incremental requirements for cats, for our next steps, we are looking forward to Annie Justice to refine our drafts and thanks a lot. Any comments? I've I've put myself in the queue. This is Adrian Farrell. Speaking without my chair hat or anything. I think you're making some very good points here But I would like you to go back and look at the most recent copy of the LDBC CAPT framework to see the changes that were discussed in San Francisco and made an update to that draft."
  },
  {
    "startTime": "01:16:00",
    "text": "Around introducing something they called the Service Contact Instant. Which I think, is providing the aggregation of server instances that you're talking about. I'd like you to look and see how well you fit within that context? Okay. Thanks. Jenny from Holly? A a comment. For me, I I read a draft and I see a lot of similar requirements proposed by this draft. And and if you know that we have an work item in the mail in in a milestone that we have to, finish requirements and gap analysis. Analysis, require, draft in the future. So I would like to I would like to, invite you to work with the orders of that draft to see what can we do together. Let's merge the drafted if possible. Thank you all. Okay. Thanks. But such a use case as how much add to our current WG use case and problem statements, and I'd like to have them operation, Thanks. Good afternoon. Moving on. Thank you. So, next, we have 2 frontitions about the architecture first one, it's, Xinxing, please. Good morning, everyone. I'm Xining from China Unicom. My present presentation talking is the hybrid"
  },
  {
    "startTime": "01:18:01",
    "text": "computing and network analysis and routing architecture for cats. This is my first presentation in please forgive my nervous. My co author is from Talian Unicom, and Shao from Huawei. Okay. We have we have no, the cat's group is conducting about the the company, information and, awareness and Routing. So I summarized the three points. The first step is how to ever their computing information. And then the second is to select the TMR service instead. Then, the set is how to calculate the optimal forwarding parts after the select selected the the optimal service instance. So, about their 3 parts, there are 2 type core architecture just namely the centralized model and the rebuilt in the model. The centralized model is shown in the left figure. In this architecture, the same instance reports the computing information to their cloud management platform. And then the platform, send it to the network controller. And then network on China has selected the TMR service instance, and, calculate the 4 optimal forwarding parts. And then send it to the redash to the English router. In this architecture, their centralized their system, performers to the routing decision and, which can provide lower perspective. The distributed model is shown in the right figure. In this architecture. The service instance reports the computing information to the accuracy rotor. And then"
  },
  {
    "startTime": "01:20:00",
    "text": "community information and moisture between the Egress router and the the network device. Then the ingress router can selector, the optimal service instance, and the optimal forwarding parts And then, it can perform as the traffic steering. In this architecture, there is no centralized assistance and everything is done by the network device. Both, synchronize model and the distribute model have advantage and the disadvantage. So, there are 2 programs. Next place, them. There, there is 2 problems as summarized. The the first problem is, in the distributed model a number of device will be upgrade and, the cost will be high. Because, the computing information needed it should be notified between the service instance and their egress road the second problem, is as their as a business narrows become more and more diverse cats need to be provide different network and the computing capabilities for different requirements. So we proposed the next proposed 3 considerations The first thing is, we think we should use our easier and the last coasterly way to aware of their computing information And the second is, we can we should to provide enhanced capabilities for demand scenarios there, sudden consideration is to provide diverse capabilities for rich business scenarios. Then I explain this in detail. Next, please the first consideration, we should use our easier and the less costly way to aware the computing information."
  },
  {
    "startTime": "01:22:01",
    "text": "We proposed to we recommend that to use the cloud management platform. In this way, their cons the service instance reports their computing information to the cloud management platform. And then the platform can, perform work information intelligently and then send it to their network controller. And then the controllers send their computing information to their English router. In this way, we think, on on one hand, on on one hand, It's There is no, there is, we, it is don't don't do not need to report to computer information between the service instance and the egress router. So their work is reduced. And, on the other hand, their cloud management platform can perform some work intelligently. Next, please. And the the second the second consideration is to provide enhanced capabilities for the demanding scenarios. For example, in the intelligence, transportation scenarios, the optimal service instance and the optimal for body parts needed to be recalculated. When their car moves, there is two ways to perform the recalculation. The first model is, their network controller recalculus, and then send the readout to their ingress wrote And the second mode is the ingress router to recact recalculate directly. In the second way, there is it it can provide a lower delay. Because there is no transmission time between the network controller and then their ingress router. So it can be better meet the requirements in the intelligence transportation scenario."
  },
  {
    "startTime": "01:24:01",
    "text": "So, next, please. The set consideration, to provide diverse, capabilities for rich business scenarios. According to the previous consideration, to make, make the routine decision by the ingress in order can provide a lower delay patched if all users adopted this way, their English rule talk, we are great pressure, So, we should provide this reviewed or centralized routing to make the to make to make the two ways for the different servers. For example, in the intelligence transportation, it will be needed to lower the the latency. But in the AROVR scenarios, in in it maybe need, global organization. So the two ways to synchronize the decision can, provide a global perspective, the distributed routine decision can provide a lower that latency so we propose to, provide a different, different mode. And then you can choose different model based on the different requirements. Next place So we proposed the, hybrid architecture to solve this. It's service flow, include the 4 steps The first step is there are service instance reports that our company computing information to the cloud management platform. And the the cloud management platform process computing information. And, send it to the network controller the, Amazon, the network controller creditor network information. And the process is along with computing information."
  },
  {
    "startTime": "01:26:01",
    "text": "There is two way to adapt. If we, adapt the distributed route in it remote, The network controller sends the computing information to the network ingress router. Then, it selects the optimal service instance and, calculus the, for volume parts, And, if we adopt the centralized rocking DC remote, the network controllers lacks the service instance and the calculus, the forwarding parts and send the results to their ingress router. Then the 4th step is their ingress in order to perform their traffic stereo. Next split. So, finally, our next work plan is to, promote their comments and the suggestion please feel free to contact us. If you have any comments suggestion and the contributions. Thank you. Thank you, Xin Jin. There was no need to be nervous. That was that that went okay. There's some discussion been going on in the chat for the meeting. So you can go and read that afterwards that the discussion is about producing multiple drafts in the working group that are raising similar points. And I think it's fine for you to have brought your ideas in a draft. I think that's great. But the end objective should not be that all the drafts are pushing forward and on their own, we should be trying to pull our ideas together and work together Okay. So Look, and this applies to other people posting drafts as well. Look at what you're doing, and say, yeah. Okay. I've expressed my ideas. Now how can I those ideas? And help put them into a consolidated merge draft."
  },
  {
    "startTime": "01:28:02",
    "text": "Emerging will not be taking a 100% of both documents. Because that's crazy. So look at the the the drafts that are out there like the existing, working group draft or like the framework draft and say, okay, how could I take some of my text put it in there to improve it and make those suggestions on the mailing list. And then we'll all be happy. Okay. Okay. Thank you, Ashley. Tanya, you're in the queue. Okay. Thanks, Asian. And thank you for your presentation. And, I have 2 comments. The first thing that I want to double check that The suggestions you might hear is that for may be delay sensitive applications and surfaces distributed framework is just adjusted. And for maybe a a location and, a centralized framework is suggested. Is that right? Us. Sorry. Can you repeat this slowly? Okay. I mean, that's, in your stuff with my understandings that's for surfaces that are the late sensitive. May require latency as a as a requirements, distributed framework is to access and for, resource utilization. Centralized framework is Jessett. Is that right? Yeah. Yeah. Yes. I Okay. And and another comment to that, for different service place, I'd like to see some, maybe a unified architecture that's, granted for maybe devices or officrators or insurers. Is there share similar functionality for different service And that might be next, steps for for work."
  },
  {
    "startTime": "01:30:02",
    "text": "Okay. The centralized system can include in that network controller or others. I think Okay. Thanks. Yes. If I move the discussion to the minute And for this draft, I think there might be some good points based on the existing architecture draft, but hope to work together to, move on Thank you. Okay. So so I've done this. Good. Yeah. Yeah. Yeah. Okay. Denis Wong, ZTE, and on behalf of Ostrace, I will present computing and the network information of WELU system, architecture for cats. Unfortunately, there's another architecture, but I have to efficacy. We are a lot, trying to push other independent architecture problem, actually, we're we're trying to approach some additional components for for the, framework. Next slide, please. The draft has been presented it in San Francisco and, what's updated is X-ray's controller component is and divide it into 2 parts, the control plane and the management. So the workflows of, the 3 working modes has ports still being an update. Next attack, please. It goes. Yes. We introduced, the loop in Poland and the corresponding interfaces as well as workflows. So we believe it's, it's going to be complimentary with the existing framework draft. So we will talk to the his authors and and see what we can integrate"
  },
  {
    "startTime": "01:32:00",
    "text": "That's it, please. As highlighted in the diagram, what's new compare compared to the existing framework drafted is we introduced, CASK And Control Center which includes a management plan, control plan, What's the, zeroes, the 2 interfaces cloud management platform and the data plane. Involved So when we're talking about, control center Poland. We designed it to to support the fine grain and dynamic to computing information well next next as a please. Which is the employment of the, cast control center. So zero is going to be, 3 working modes we're talking about a centralized centralized to Mordu. It's, designs to for the Cilaras to where the where the service is, sensitive to to the fine grained and the lame to computing network status. Particularly something like latency, And, so the network needs you to be aware of the detailed computer network information by the centralized model. When it comes to distributed model, the services are supposed to be not so sensitive to the fine grains computing network. Rather, it's supposed to be sensitive to course grained and, comparatively that, sorry, static in the computing network status. Hybrid expertise is just a coexistence of the 2 models, accessories. Yeah. As far as the centralized mode, concerned."
  },
  {
    "startTime": "01:34:02",
    "text": "As, we presented in for San Francisco, the CIB and the NIB have been employed to to maintain fine grand, did the cost grand, a fine grand, computing and a net network information respectively. And it's a PCE in control plane. It's responsible for calculating calculating as a service cost and deliver to the data point. So, Under the, centralized model, or what's z? What's, what's, different is in the data plane, it only has the traffic to classic file. With them. Next second, please. So the, accordingly, the workflow or so, has been updated Right here, I I just want to emphasize this, see, on a service service selection policy will be generated by the PCE and deliver to the cast router. Next slide, please. For the, distributed model, right here, we can see easy control centers has no control plane. Right here. Functionality right here, because, the, the data playing part is actually I believe, quite consistent with the that that that of the IDBL framework. So, the only different part is the, There's just a just a, cats, cell spawn, the interfaces, which we've sponsored for report. Network and computing information to the management on plane for the network and management purposes. Next slide, please."
  },
  {
    "startTime": "01:36:07",
    "text": "Accordingly. Yeah. Of course, the workflow of the distributed model we're still and updated right here, actually, as you can say in diagram is just only network metric and the surface metric information has been reported from detopeline to to to the control set, CAT control center. Next time, please. Access file. Okay. When we're talking about a hybrid model, actually, we assume there are 2 kind of cast forward The first one is with routing capability on its own. So it can performs the, collection to compute information and as well as the selection of parts, a second, class is the, task forward in which which, had no routing capabilities to so, the, when it comes to the forwarded without routing capabilities to see service policy. Will be delivered from the control from the control plane. The PCE. And, also, the forward with routing capabilities can perform precise, more precise to pass an action operations to based upon based from the instruction firms controller, which maintain fine grained computing and networking information. Next, please. Actually, as, I took in a group is Adrian, comments, we do not want to push forward in and multiple architecture graphs in the working groups. So next next next step, we would like to coordinate with to the existing, Draftston talking to officers to see on what we"
  },
  {
    "startTime": "01:38:03",
    "text": "can integrate together. We, as far as the, additional parts of this draft, I believe there's 2 points. The one is a cast centric controller as well as to the management and control plane. And second one is it's according, accordingly, interfaces from the central controller with cast routers. That's for. At comments, Thank you. We have no person in the queue. The same comments, with the previous draft, we expect the others of the architecture of the client work together. And, I see there are also some overlaps of the quarters in different draft. So maybe it is a good choice for the person to start the action to merge the draft. Upgrade the points to to the millionaires to start the conversation. Okay? That's good. So, next is a depth analysis, Hi. Oh, so, there is a a long history of the gap analysis of fire. So, I'll quickly go through the update on the gap analysis because, it provides some, basic knowledge for, podcasts is established and how cast are different from other mechanisms for solving the steering attracts steering problems And, previously that, the gap analysis file and, the requirement file is in the same file, but then we have split, split the requirement part to emerging of a working group document. And so, next slide please."
  },
  {
    "startTime": "01:40:00",
    "text": "So, there is a overall a picture of the gap analysis. So there are existing solutions, utilize endpoint resource information for selecting server instances like, DNS, auto, and, other layer 4 or layer 7 load balances, but, basically, that none of these existing solutions integrate the computing resource conditions with the network and, with the network conditions for designing optimum path So next time, please So, the first one is a DNS based solution that, for the DNS that the clients, were, first use the early binding resolve IP address first and then send traffic. So and the the resolver will, then check the health of different servers, deploy distribution, utility, to, make the the right decision, grab the, you know, right information and then it will, balance the load over the DNS. So, for this, solution. We have made some updates on the info that, previously that the the resolving, see of the team that's relatively about minutes level. So we have a there are some, uh-uh, update on the didn't ask pollution mechanism because it can offer, is defined in RC ASM 65. It offers a relative efficient to publish computing status information to clients And so it you basically uses the state for operations defined in RBCA's 49 0 to give long line low traffic, a connection, better longevity, it's about, like, the default keep a live session duration is about 15 seconds with which we think might be, sufficient for dynamic steering, and, it's acceptable for refreshing the compute information, but basically that, the kind this kind of DNS based solutions"
  },
  {
    "startTime": "01:42:02",
    "text": "you cannot grab the link connection information. That's integrated decision based on compute load and network status still cannot be derived. So I think this is a major problem dealing as based on mechanism and NasAP Lease And so and for other solution like load balances, basically, there are 2 different options for deploying this. The first one is a single point load balancer deploy at a a single site So we might, face a single point failure at the load balancers So it cannot steal the traffic to the optimal point. And the other one is, to deploy different load balances as different sites. So, the problem is still on you cannot select the best site, because the the, the decision making is very, is still a problem. And next time, please. And also the draft compared with auto. The auto work has been, you know, explained and also, uh-uh, describe as a majority has in previous talk So I will not go into details into this one because there are still some working, you know, on the metric, definition stuff between auto and the cuts is still ongoing. So, we will keep an eye on that. Thanks. And next slide, please. And also, there are some, other mechanism like, client based solutions. So that basically is like the to, you know, to expose, like, an network service providers exposed some, network path information, like, another, like, compute information to the clients, let them, matter and decide to choose which site they wanna deploy dates, their applications. But, there is, some issue in the deployment and also in the the the, and in operation level that, because the service providers might not be able to our is reluctant to expose their, you know,"
  },
  {
    "startTime": "01:44:02",
    "text": "information to clients due to some maybe security issues or other issues. So I think So but we also compare a CASM mechanism to to this solution. So basically there's, some, major, solutions that, can do the steering of the the traffic Thanks. And, Next time, please. So here's a basic summary of the existing gap analysis. So first one is dynamic. So one thing I want, one more thing I want to clarifying the, about the traffic steering that are in cast because, we yearly, we don't want that a fast steering like a a millisecond milliseconds level, you know, steering because that seems not a ideal. So but we need to level, Denimist, minute level steering might not be able to satisfy the requirements. So I think, probably, about tens, tens of seconds a dynamic scheduling can can be acceptable. So we introduce in NASH Poach mechanism, but we also analyze its limitation in grabbing the link information and also what what we need is to combine information, the link, network link. Connection and also the compute load to make a overall, and, integrate decision. And that, and the the other And thing we wanna consider in the gap analysis is efficiency and the complexity and accuracy and also the matrix and exposure and the usage. So this is the overall picture of the gap analysis. So Thanks. And any questions about that? That's people getting in the queue, but I got there first. Okay, hon. Thank you. I think that the the gap analysis, apart from NTO"
  },
  {
    "startTime": "01:46:00",
    "text": "something that our charter tells us to work on. I think it's important, but it's gonna be background work. Sure. Thing I'd like you to be really careful of is this slide so that the gap analysis should not be developing its own requirements Sure. If you can be checking that those requirements are properly reflected in the requirements draft. And then you can reference them by number or or whatever and and then we'll be, yeah, we won't have surprise requirements coming up in a later document. Sure. So for the gap analysis is provide some basic knowledge to view this requirements. So I've got, according to the charter. It's not, you know, maybe it's not, in the milestone of the cats, So that we wanna let, let people know about this. So next. Yeah. Julian or Julian. Chino, Yes. Julia from Nokia. So, yes, I wanted to make same comment about the requirements. So I've made already this comment last time. I think when we put up requirements, we need to motivate them in some form. For example, by linking them to specific aspects of use cases. And in in this case, we don't have anything. These requirements are falling from the sky. The other aspect I would like to address is that I think that some of the solutions that have been presented here are unfairly criticized I think that, for example, if you're talking about load balancers, I think hyperscalers are using load balancer quite a bit into the internet. And he seems to work pretty well. So you have to be a lot more, clear in the criticism you addressed to them, saying that there are single points of failures with load balancers. It's just not true. It it works every day on the internet. If you look at the presentation that you made of DNS. What you're presenting is not quite DNS."
  },
  {
    "startTime": "01:48:01",
    "text": "It is a system that you have based on some DNS aspects you're using the DNS protocol in some specific fashion. And then you describe the shortcomings of your solution. Not to discuss feelings of DNS. So, likewise, for Alto, you have devised a specific way to use Alto. What you described, the shortcomings are those of what the use you make available to. So I would encourage you to be extreme a lot more precise on what effectively you describe as problematic in these alternatives because I can device systems based on Alto I can do the system based on DNS. That do not have the the shortcomings that you point out. So please try to focus on the aspects that are effectively problematic. Thank you. Okay. Yes. Hello. This is Luis Contreras from Telefonica. Yep. In the same line that Julian was commenting and focusing specifically on all you command of the critical aspect is the signal latency. I think that this is a little bit darker. Probably, it's worth to work in more details, as Julian was saying, Thank you. Thanks. Alright. Thank you for that. And I think we're at the end of our agenda unless anybody's got anything else to say about the work and the working group And of course, I have 3 things. We've talked about the draft merging issues and the parallel drafts Please remember your chairs have big sticks. If you are having trouble discussing or merging with somebody else come to us for help, and we will facilitate There were a couple of presentations where the slide formatting got really messed up That's because the tools convert your PowerPoint into PDF. As a matter of habit praise,"
  },
  {
    "startTime": "01:50:00",
    "text": "Once slides have been uploaded, go and download them again and have a look. Pull the PDF down and have a look and see if it's all messed up. And if it is, we can work to sort it out. And last point, we've just, in the last 24 hours received 2 liaison communications from the ITUT, and the chairs have not yet had time to read and process them you're seeing them on the mailing list. And if you've got any comments then please do send them in Otherwise, the chairs will work with Scott who's sitting over there wishing he wasn't. And, we'll try to generate responses as necessary. Anyone else? Last chance, Was he in the queue. Please? Oh, yeah. Delivery. No. It's is that you're still in the queue or yeah. The previous. Okay. Yes, please. Yeah. I'm I'm still have a few difficulties with the way that the working group is working here. Because we've had a few debates on the list, in the past well, 2 weeks probably. But otherwise, there's very little happening on the list. And I think that this is not really the way that we should work. I heard the chair asked for people to do a lot of merging activity. I wish that all this was done in the open and that we see more traffic on the list. There's a lot of work. we have, I think, 2 tie or 2 milestones why do we have 23 drafts? So I think that there's a need to work together and this working together needs to be done on the list. Not behind close dots. Thank you."
  },
  {
    "startTime": "01:52:00",
    "text": "Alright. I think we're done. See you in Brisbane if your travel budget allows it. Okay. Oh, good. Thank you. Not Yeah. We'll discuss it close to the tone. I my cough. But, you know, now. Just"
  }
]
