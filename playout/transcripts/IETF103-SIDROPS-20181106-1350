[
  {
    "startTime": "00:00:06",
    "text": "[Music] [Music] [Music] [Music] good afternoon it\u0027s Thursday at IAT f103 Thursday said my mail it\u0027s Thursday I\u0027m Chris this is care this is IETF 103 if you\u0027re not for the IETF 103 meeting you\u0027re probably in the wrong place it\u0027s the side Rob\u0027s me again I at HF 103 if United right for cider ops then we could say it\u0027ll be fun or you could go on to where you\u0027re supposed to be for those presenting there\u0027s a little X down here on the floor please stand on it speak into the mic always speak into the mic okay I think we need a minutes taker and a jabber scribe before we can go forward volunteers jabber for pen Oh excellent there\u0027s a seat right here for you if you want reserved for the jabber scribe let\u0027s say you get the good seat note-taker yes thank you Tim are you presenting to okay I\u0027ll type while you present how about okay paper oh my god this is cider ops this is cider ops not cider boom okay back to serious things there\u0027s a note oh this is all of our oh this is the note well I think maybe yes this is the note well let\u0027s just say it is we have an agenda here\u0027s the slides we\u0027re almost in two or five minutes we have two hours we should have plenty of time for this but we\u0027re gonna try and keep people on time on schedule as best we can so note the time you have if you\u0027re presenting except for George who has 20 minutes let\u0027s take I think you there - George\u0027s "
  },
  {
    "startTime": "00:03:10",
    "text": "yeah no yeah yeah that\u0027s why there\u0027s an extra space just think of it like this quotes under there okay alright first up is Oliver unless there\u0027s anybody who has any questions no good Oliver you\u0027re doing the thingy what do you mean use a mic just so everybody knows we\u0027re doing rpki origin validation on the on the IETF Network and it seems to work okay so last year we did some work on testing route origin validation there\u0027s different implementations and we made some observations what I thought might be interesting for everybody of you and if not then just doesn\u0027t do at all so the first thing what we were looking and we want to see what is the impact on convergence time we also very interested in if a router is connected to one or more caches what happens if the connections break down if we have changes within the connections etc and how does this whole thing work we tested a little bit the implementation of Route Orange motivations and routers and if they implement the RFC 1897 which is on ibgp the transmission of the validation state and also what kind of impact this has on my BGP so the first and convergence and so what we did we did not use any excess of policy actually we didn\u0027t use any policies at all we just sent around 700,000 routes into the router we measure it how long does it take we made it this one peer these two peers and then we did the same there\u0027s a hundred percent who are coverage what this means is we had for every prefix origin full match prefix and we got around an average increase of between two to seven percent I think the loss even was one point eight so we rounded to South but this is all relative because now if you put policy processing on top of it the the time stays the same our percentage goes down so I think rendi at one point made some statement that it\u0027s neglect to Bill and I think we can pretty much confirm that then we looked into the validation caches which is kind of interesting so we we used a combination "
  },
  {
    "startTime": "00:06:12",
    "text": "between the ripe ncc in our own cache test harness there was a little bit easier to fill in later and we made these tests like where the routers connected to a cache and all of a sudden the connection goes down for example we just put the cable waited a little bit the cache crashed saw router crashed and all these kind of things and we interestingly we noticed that I mean every router acts differently some detect relatively fast the loss of the cash and others seem not to they did eventually but this is all like configuration based and so if the router detected the relatively fast that increase the turn-on ibgp because if only if you signal the validation state because no the validation state changes the attribute changes and you have to reanalysis selected it\u0027s not much but it\u0027s something to to look at so eventually what we figure out it might be good if the router is not really that sensitive to the loss of a connection to the RPI cache or even if you have maybe more than one cache connected to the router then we looked into the signalling of the validation state we notice that not every router implemented it 100% the way as the RFC specifies it we talked to the vendors we got fixes for that so maybe just you look into your own gear what you have and check and talk to the vendors there are fixes out there if you see something some implementations label internal routes as well it some allowed to have the validation state configured some take the validation state of it is signaled we are CRC 1897 then we looked into the a s set the a s ed is an interesting thing so some routers had issues with the ASM actually so when we receive your ebgp in a sh if the prefix we cannot determine an origin but if the prefix is not covered and it\u0027s not found from the algorithm if the prefix is covered then it should be invalid interestingly we found an awful lot of not founds which didn\u0027t switch to invalid we receive one fix from one router vendor so but it doesn\u0027t necessarily need to be a bug because if the vendor decided I don\u0027t perform origin validation on routers a asset then the RFC says if you choose to not validate it set label it is not found so I don\u0027t "
  },
  {
    "startTime": "00:09:14",
    "text": "know if the internals of the routers if they just chose to not validate it then it\u0027s fine if they chose to them they have a back maybe you should look at your gear how this one reacts to that right now so then we yeah I think I talked about this already so I immediately we have some routers basically as I said label labor the updates as well it if they don\u0027t see anything else here the ibgp we as a community string another router labeled these as unverified what I actually liked and they didn\u0027t do anything and there\u0027s also kind of way how you can configure it the interesting thing what we noticed there actually we didn\u0027t only notice it we we actually made use of that for some of the measurements and later on we thought wait a month it\u0027s actually kind of an interesting behavior number one is the prefix packing so if I receive here ebgp and update this let\u0027s say three prefixes and the validation says two of them are valid and one of them is not found then out of one update becomes two so later on we notice this because we were thinking we were counting the updates that go in and come out and something was off we were wondering why and then we figure here actually that it doesn\u0027t need to be something bad it\u0027s just something what you should consider if you turn on this attribute but becomes more interesting down here I have only one prefix and now my connection to the validation cache starts flickering it\u0027s up it\u0027s down it\u0027s up its down so if the same prefix stays stays selected I receive HC which is valid then I receive it again was not found this valid and so that\u0027s why and not that fast reaction to the lots of manipulation cache might be actually not that bad I always hated this work conflicting validation results but I had to actually educate myself a little bit because we can actually have conflicting validation results you can have the same prefix origin in the router that has all three validation states how can that happen I have an S that has that has three routers every router has their own validation cache they are not synchronized they pull at different times and one router might say that\u0027s this prefix origin is not found this one says it\u0027s invalid this one says it\u0027s valid and then if they add this one is extended community string then router one will have known "
  },
  {
    "startTime": "00:12:16",
    "text": "it\u0027s table prefix one origin one three times with three different conflicting validation results just something to think about it this may be local processing might make more sense even on the ibgp side or if every of these router maybe connects to a centralized validation cache that might fix some of the things so that at least see validation results might all be the same because what can happen now is if our one receives traffic for prefix 1 origin 1 and a is a basically it might be a origin 1 actually or it might just provide through the router it if the traffic comes in here I would prefer that it goes out right away to a say but if I now prefer well it this link here is not found this one is invalid and this one is valid so now our one carries the traffic through its own network to the west coast and then sends it out to a say just something to think about same some router limitation of our three doesn\u0027t do any art from router invalidation and router one then by default declares this one is valid just if you deploy this just think about how you have to create your policies to maybe avoid certain things or think about that these things can happen I don\u0027t want to say it\u0027s good or bad I just say that\u0027s an attribute of the beast so in general what I figured out the number one documentation would really be better throughout all the routing implementation I have to include ours as well more more examples could be out there maybe they are I just didn\u0027t find them but if someone starts really fresh with that they\u0027re really on lost lost grounds because we even saw that in some router implementations we saw some labeling that would imply certain settings and I didn\u0027t find any setting that could activate something like I saw some using this primary and secondary cache I did not figure out how how can I say I wanna have primary secondary cache I figured out that the implementation all made a union what is actually a nice thing maybe better than primary secondary cache because now if I have two caches connecting one of them dies I don\u0027t lose everything right away other thing that Union again the detection or the detection of the loss to a cache I think it\u0027s better to work with stay daters and permanently have this flickering the other thing is if I turn on the rs3 eighty ninety seven I might see more traffic on ibgp "
  },
  {
    "startTime": "00:15:17",
    "text": "I lose a little bit of prefix packing I don\u0027t know if it is a concern I don\u0027t think it\u0027s a concern right now I don\u0027t know how this evolves over time and that we can have conflicting router States so the question here is is there an interesting to create a document where you start documenting these kind of experience things for people who come fresh and I think yesterday in the Grove working group they were talking about like a living document what can be always updated maybe something like this could be an interesting thing in this regards because things that might be an issue now might not be an issue in six months maybe things that we don\u0027t know yet are an issue might be an issue in six months and something that has value for especially newcomers I mean we are still in the deployment stage many people come in they hear this the first time or they may be heard at ten years ago and still aren\u0027t so mighty at set of ten years ago but they don\u0027t know what it is and maybe just can update things so for someone who gives a quick quick start something for the community there you can exchange a little bit and have some value not something what is two hundred pages long was something that is maybe I always hope for one but maybe a handful of pages maybe some it\u0027s ideas on how you can configure not that you have to do it like that but just what you have to consider something like this so that is one thing and we also think that we will we plan on doing some more tests if there is something that some of you really would like to see drop me an e-mail or Shriram or doc montgomery whoever and then we can see if we can put this into this test what we want to do or maybe that is something where we say hey that\u0027s really cool we want to do that I mean we don\u0027t have limitless resources but if there\u0027s some interest in testing certain things what you think is valuable also for the community let us know questions I got two questions here Patel on top of list the questions you have can you go back to your slide six yeah yeah so I know you made an observation be interesting to get your feedback on a set because in a said typically the Aces are not sorted that means very hard to find the region√≠s know but a s-set says if you find any a said we don\u0027t even have an originator exactly I mean I personally don\u0027t think that\u0027s a good idea because I think the aggregator could be the orginal but in the document it says you don\u0027t have knowledge at once you don\u0027t have knowledge inator you can\u0027t you can\u0027t so it is not found at this point if there is no row that "
  },
  {
    "startTime": "00:18:17",
    "text": "matches the prefix so it but if there\u0027s a row that matches the prefix and it should be invalid well if you have a bunch of aces in a set you can\u0027t authoritive ly tell if it was originated by the same guy no but then the prefix is invalid exactly yes or not found or not found all right so maybe you got maybe might at least my suggestion is you document something and say here are the rules and then I have a follow-up a question on your last slide yeah actually in summary you say that if the RFC 1897 is enable you see multiple states yes ideally you should see same state across within and yes right no the point is this what let me go here so slide number 11 so if I have r1 r2 and r3 they can have different states if they point at different times so now I have a prefix in this prefix let\u0027s say for example I see a rose left I got it okay hello cute ellipse so my thank you for the report my question is it made me anxious that if we are losing connection with the cache the state of perceived roots maybe change from valid to unknown and with this wonderful extended community these roots will be renounced all routes will be renounced inside all routes that change the validates of state yes yes do we need to update these RFC about extended communities and remove share information about the wedded state because it\u0027s quite enough to share information about Internet and unknown but in at the moment we achieved by changing the state and we are getting so so if you drop invalids and you never will send out that\u0027s yes sure and if you now just take the unknown out or the valid out then you still have invalid unknown so I don\u0027t believe in anything no it\u0027s different because the number of valid roots will be increasing and finally we are hoping that it will be nearly 100 percent of ballot roots the number of invalids roots is not that high it will not be you know and I I would I would prefer instead of changing the RFC and making it more complicated if you have more than one cashier connected to and if one goes then the impact would be less maybe that would be a better solution you suggestion to have every government to have at least two "
  },
  {
    "startTime": "00:21:19",
    "text": "applications connected to each route do you think it\u0027s reasonable or sensible I don\u0027t think it\u0027s very expensive now I\u0027m not sure it will happen in real networks and for I don\u0027t know I mean the thing the thing is is too in our day and age so if you take I don\u0027t wanna endorse any product but if you take for example the ripe NCC Valley data and you just power up three virtual machines and different physical machines and you connect to all of them you have it anyway so from my perspective that the situation when we are losing in connection to a cache and we are getting a ray announced of all prefixes that have a valid state those big I\u0027m strange to be to be also going one step further the best thing would be you don\u0027t even use this RFC 1897 and you make local politician even on ibgp because the thing is this the validation is not expensive validation is not expensive so if I also if the problem is this if I have a system there I have distributed caches this alone gives me already an interesting thing that I can have different validation states how do I deal with that what do I want to do about that so if I say I validate within my router every particular update I mean there are many many ways to roam I don\u0027t think by saying oh we just don\u0027t signal one state which solves a problem it may be reduces by a factor X but if you took a look at the implementation of bird routing daemon and the way they implemented application so it\u0027s it\u0027s just a piece of software with no separate cache just inside as far as I know there is no at the moment there is no way to connect into another one no but if the cache is part of the router itself so if you lose connection to this and the only thing how it could be that\u0027s the whole thing crashed so I mean then you have a different problem you still have a connection with your transitive anchors I mean the thing is the only thing what we want to do is here think about it think about the implication think what is the best for your organization you always can have policies of what you want to send out what you don\u0027t want to send out I think the more important thing is that everybody is aware that there are some things that can happen you might be able to see something in your outing and I just want to bring you a pointer to say hey maybe it might be that it might be even something completely different so more of you look into more we test the more interesting things we see the more questions come up "
  },
  {
    "startTime": "00:24:19",
    "text": "sometimes you do something wrong sometimes you say hey wait a moment that is a that is just an attribute of rpki and it\u0027s fine as long as they know about it as long as they can do something I could think about if if it flickers too much that I just turn off my validation care or I turn off the r Cena 1897 so I only sent you if I make a change in my in my selection process until I have a stable connection so and there\u0027s so many different ways and I don\u0027t want to tell you what you have to do and I don\u0027t think we would shouldn\u0027t stand here and say what you have to do the important thing is that you know there is something and you have to make your own decision what you want to do with this information okay so the AF set as far as I remember was considered when we D actually designed the whole thing as something that should go away and I\u0027m not completely sure whoever we were completely wrong about it and it\u0027s not that design decision is now biting us on the other hand one if one really wants to look into it one should recognize a assets\u0027 essentially happen because some AAS is creating an aggregate root and that essentially means that well okay you you probably are going to see the origin of the aggregate root as the last AAS before the a s set kind of kind of yes things can be more screwed up but kind of if we are talking about 1.0 0.0 one percent of roots that we are seeing and 1% of those goods are the special cases well okay yeah I mean if if you wanna open this this box that is actually something but I always like to open because all the identification identifies to identify an AI said they\u0027re all optional I can make a cure got a guess pass aggregation if we don\u0027t put my asset in my path if we don\u0027t set the aggregator you don\u0027t know it\u0027s aggregated and all of a sudden my AS IS the auger is the originator so why is it not the originator if I put C is it in I puts your asset in because I cannot guarantee that I create a route group so we can open this discussion I\u0027m really okay since years won\u0027t have that but yeah well okay the my highest priority comment actually is to your question should we do a document and yes that "
  },
  {
    "startTime": "00:27:21",
    "text": "might be nice maybe more documents than one but actually for getting started I think we really ought to identify some repository where we just dump the stuff that we find then what we get along and that may be later on be refined into a document that actually has consensus and while okay high quality and whatever as far as I can tell so far we really have not yet installed or recognized such a repository that we agree to actually use I may be wrong I may have forgotten about it the only one I know if it\u0027s email list and but but I mean that\u0027s a good idea as well I mean the question is just is there is there interest you know if it is a document if it is a github thing if it is I don\u0027t know what what other people will come fresh into the whole field can read and educate and get some ideas if not hey be my guest I\u0027m fine with that as well it is just an idea I throw in the room yeah well okay I think actually such a recognized repository is actually very very much needed because pointing people to a mailing list it\u0027s a little hideous yeah but works as well job Snider\u0027s entity too short remarks in terms of SS the implementation on an open Beach PD is that we ignore the asset when doing the original edition procedure so we just look at the last sequence of the s path the implementations I\u0027ve done internet exchanges for route surfers using birth birth has an option to look at the last not aggregated ASM in the SPF so we could just shortcut this discussion ASL does not exist I mean that maybe it would be nice updated RFC 6811 to basically the indication of my asset but I think that\u0027s a different discussion I think the thing is is if the only thing what we identified is that not every implementation Isis seems to validate a asset there are some implementations that actually allow you to configure the heck out of your router and you can do whatever you want with that you can even roll the dice so I think if the vendor provides enough document enough configuration so that you as user can decide what you want to do that\u0027s fine I mean it\u0027s a little bit of problem if other everybody uses its differently it is just something that I say maybe you "
  },
  {
    "startTime": "00:30:21",
    "text": "should look into what does your out or do with that and just know about how it works and final comments when speaking to operators about deploying origin validation a question that often arises is what will be the performance impacts and then I sketch out well it\u0027s patricia cheese we do the lookup in this table the table only has this specific types of tuples so we can do it in microseconds having a document where the title is performance impacts of origin fala Dacian or something more that just focuses on that one question what is the impact and that we can prove that there\u0027s virtually no impact would be very helpful in promoting original validation so if you end up writing documents one document specifying that there is no performance impact would be very useful yeah if the document says use something produced after 2014 sure [Music] thank you okay so we propose a validation state unverified and I propose that for two things for rpki origin validation as well as for T to be sick pass validation the I believe it is important to distinguish between validated and non-validated routes just label non-validated routes as not found waters down completely the validation state of not found we have yeah so there are situations where for example if I go and be to be sick for a second if I receive routes I might want to do lazy evaluation so I just want to take them in I want to do my route selection or whatever and then maybe I want to do the validation later on it would be interesting to see there if this route was validated or not not just to sign some label in the same I think is also true with Origin validation the algorithms in RFC 68 11 and 8205 specify values that are the result of a very defined algorithm not validating something if I just assign blindly the validation state I watered on them all as programmer for me personally if I\u0027ve write something and I have pointers I initialize them I either set them to nada or I have them to some objects I don\u0027t know what they\u0027re just leaving them hanging in the in the blue doesn\u0027t help me and sometimes I need to know if "
  },
  {
    "startTime": "00:33:21",
    "text": "they are not and I think the same thing we have here as well the for me basically an unverified and and a verified route are not the same and unverified and what sound is not the same if this is the same then what can I say if I see a valid route my router was it validated was it not validated there are some routers as I said before that on ibgp label everything is valid so normally well it tells me that this raw that gave me the result that this route is valid now I just labeled it it we found two word usages and I learned the meantime there also a couple of more but one for example juniper has the unverified and I don\u0027t I don\u0027t want as I\u0027m not saying that I want to prefer this vendor to any other I just say they for example have that I liked it very much our implementation isn\u0027t undefined what was a little bit more semantics but I\u0027m very fine with unverified as well going to RC 68:11 so we we wanted to have unverified specifies the state of a route prefix which one which no which no evolution has been performed and to update this statement a validation is not the fountain or au its implementation should initially they initialize the validation state of such route to not found to the sentence if no evaluation over out prefix is performed in any form the implementation must initialize the validation state of such route to unverified if I would have that then I could figure out if for example the routers that returned on the a s set they\u0027re not found in they chose to not validated or if they have a bug because if they chose to not validated they should be unverified if they validated I know you know invalid if I have a covering raw to this also RFC 1897 should should have the unverified look up in there as well I don\u0027t believe that the absence of the community string tells me that my peer validated the prefix or didn\u0027t maybe it just does not sent me the extended community string and the whole thing becomes really important when you start writing policies for not found once you start writing policies for every validation state you might be very interested in the fact if this one was validated or not because then you might say hey if I receive something VI BGP that was not Murray dated maybe I start my local validation or maybe a low prep it or maybe whatever it could be a good I "
  },
  {
    "startTime": "00:36:21",
    "text": "didn\u0027t good good thing if you look into your logs and you see okay why is this unverified why did some validation start not start for that particular thing maybe I have impacted my configuration same as RFC 8205 they be only have the state valid and not valid and these one are the outcomes of the body days an algorithm if I don\u0027t validate it then sure it\u0027s not valid but they\u0027re not valid in this case means more okay it didn\u0027t pass validation so if I did not validate it if it didn\u0027t even attempt it to evaluate it then I should say it\u0027s unverified so and then we would just say BGP routes feature music routes must be initialized using the BGP SEC validation state unverified until proper evaluation of the BGP SEC route has been performed how you evaluate is different this is completely up to you but if you don\u0027t do anything to the route it should be unverified why should I set it to not valid why should I said it - well it okay last time I don\u0027t strongly object to this but I think this is a little confused both the origin validation and BGP second round and round and round on how many output states are the from the validation okay an origin validation as the three states because they\u0027re very simple but good and I can prove it it\u0027s bad and I can prove it and everything else what the flavor of everything else is outside the scope of the validation I I can understand an operational reason why you might care what flavor of on of not able to prove anything it is but that\u0027s what you\u0027re talking about is different you know the point is this you made you made something very important you said the outcome of the validation algorithm if I don\u0027t even start the validation algorithm then I need to initialize it because otherwise I only have three States I can attach to this particular prefix RC 6811 says if you choose to not validate something you should have a case should set it to not for sure but look at all the weight look at all the different reasons why you could have failed to run the validation out for them you didn\u0027t want to you weren\u0027t able to connect to something you\u0027re having a bad day and you\u0027re running really slowly there are a whole bunch of different states there the point was just to skip over all of that in terms of the specification as I said in your implementation if you want to keep track of a whole bunch of different we didn\u0027t quite get there States whatever it\u0027s harmless but the algorithm description is trying to distinguish just the security state from the security standpoint either you can prove "
  },
  {
    "startTime": "00:39:21",
    "text": "or you can\u0027t similarly with DGP SEC it\u0027s but if you say you can\u0027t then why don\u0027t you say invalid why did you say not found that\u0027s my mind you know you go you know you\u0027re going you take a perfectly valid validation state no invalid is it\u0027s wrong I can prove it yes and not found is I didn\u0027t find any raw information no no you just said what I had yes not not found is mislabeled not found is everything else that\u0027s really all I\u0027ve ever been but if you listen to the algorithms there is basically says you look up and if you do not find a covering route prefix mmm that\u0027s one reason why you might not be able to do it doesn\u0027t say anything about anything else it just says this and then it is not found if I if I don\u0027t even look into it how should I know that it\u0027s not found how should I know that I did not find a security state when I was looking it up that\u0027s all it is similarly with BGP sec there\u0027s I can prove that it\u0027s good and there\u0027s everything else that\u0027s the algorithm description who UFO touch Telecom I think the BGP SEC argument is somewhat different than the origin validation thing in BGP SEC kind of yeah well actually actually no I disagree with my with the first statement no actually actually marking that you have analyzed analyzed the analyzed the state of stuff or you have not really really is significant information were not found as it is labeled in original validation by now by naive persons like myself is interpreted like we know that with our current set of rowers it is not covered and so we cannot tell we cannot tell with envy rpki whether it is valid or invalid because the authorization scheme of rpki doesn\u0027t say anything about it having having not done and started the test really is really is a different thing and it makes a lot of sense to have different policy decisions "
  },
  {
    "startTime": "00:42:23",
    "text": "based on that and with the BGP SEC yes kind of the Fate we have done for checks and we found it is invalid kind of is a different thing than in origin validation were the unknown really really is a distinct state from invalid but anyway undecided in logic in logic you sometimes run into the strange situation that the chatsy unknown dot or does not apply yes or no true or fails in some cases really has to be amended by we do not know we may not be able to decide state then at the end it\u0027s all up to the operator or to the user you can say for me in found and undefined I treat them both the same it\u0027s not that we say you have to treat them different [Music] you can treat them the same you but they also could say you know what I want to hold back on them maybe I\u0027ll oppress them or I don\u0027t know whatever you like reading I said it is it might be of interest and I think it is of interest to determine if a route was validated or not and just assigning they\u0027re not found I think it\u0027s not the right thing and not distinguishing they\u0027re quite certainly creates the what was your turn in the previous talk in our conflicting conflicting state if you have announcements coming in from a Rooter that has for whatever reason lost the capability of classifying and another one that actually does the classification you are getting a conflict knowing knowing that the one has lost its mind and cannot really tell allows you to say well okay if I have a "
  },
  {
    "startTime": "00:45:23",
    "text": "better classification I can use that I suppose are you proposing to write a document it\u0027s discusses this problem no I mean you\u0027re you\u0027re proposing to either write to biz documents or write something we can chat about in on list at the next meeting or whatever about should the states should there be another state or you just bringing up a problem saying this is interesting and we should fix this but not sorry oh sorry yeah actually two drafts uploaded yep okay great thank you I think Tim this is you I hope this is the right one it says something different in the the schedule says key role anyway that\u0027s probably my fault yeah okay but it is it just started life as a but I\u0027ll get there so I talked about this a few times already we currently have no proper way of rolling route keys in the RPI so goals of this whole exercise are that well we want to be able to do it in a reliable way you want to learn from the in a sec what can be learned there but we also want to look at what we have in the RTI and make sure that this has a soft landing into the existing standards that we may be able to leverage things that we have here a another goal that I think we can reach as well is that we can leave a trail for clients for a relying party software that is out of date that has an old sauce hanger configured with it that it may be able to find a new toilet trust anchor so the changes in the o2 version I there was not a lot of discussion at the mic last time I presented this but I had a long discussion with Austin afterwards and actually Rob came on as co-author as well we discussed well extensively and basically the bottom line of it is that we want to allow multiple keys in parallel in theory you can have 20 but the same thing is probably to only used to at the same time it supports both planned and unplanned rolls you don\u0027t "
  },
  {
    "startTime": "00:48:24",
    "text": "really know if you\u0027re going to need an unplanned roll so there is at the moment no indication of when you plan to do a roll because that might be misleading you might say I\u0027m going to roll to this next key and then it turns out you lost some X key so we can discuss whether that\u0027s actually useful to include because you know this should not happen all the time but for the moment it\u0027s not in there then the next steps well okay don\u0027t take it literally but I would say we\u0027re going forward with this what we want to do we want to actually make some running code and anybody interested in that I would say let\u0027s talk and work together somehow an option might be to do that at an ITF hackathon or get to get our but you know there\u0027s other ways as well so if you\u0027re interested in doing work in in this space please let me know this was actually the executive summary let\u0027s say so reminders all what we do is for you know a cast but if this time I do have more slides on how the process actually works so is it time to go through that then I might do that so okay now the current situation we have trust anchor locators that are either shipped with a relying party software or configured the software will then fetch a certificate verify it and then that certificate has a publication point we manifest serial and other stuff like a certificate or a child then that has its own repository and so on um so that\u0027s where we are today and if you want to reconfigure the chancre you have to put a new towel into your relying party so far so to avoid confusion between science tell and tell that was actually in the previous document to tell is a plain text file with a bunch of your eyes and a a subject of the key info fingerprint but this object that we are now creating is a bit more than that so I sneakily renamed it to sauce anchor keys instead of tell to avoid that confusion but I still have to get used to that name myself so I might say sign tell why excellent have you known them so but in any case what you can do just using one key like today one root key you can opt into to having this you can create a an object a tal tag object that basically refers back to your one and only current key associate this is a current key so invalidation software "
  },
  {
    "startTime": "00:51:24",
    "text": "will be configured with the trust anchor it will find the tag file there and discover okay that the crossing car used is still current so I\u0027m good to go and there\u0027s nothing else to look at importantly but of course just doing this won\u0027t get you very far or give you many you know benefits so if you have two keys well what you can do is lots of drawings here in this state both keys are current both are fine if you configured with its anchor for the first key what you do is you find a the blue tea a tag object you validate it and it refers to two crossed anchor certificates there are curls so you will then need to look at both well one you already looked at but you don\u0027t need to look at the green on you\u0027ll discover there are a new tack file that also lists these two keys that\u0027s good so you don\u0027t there\u0027s nothing else to look at as documented now the relying party tool can then just continue validation with the key that they started with so they can just take the thumb pointer here well the top see Acer certificate right this one to continue here right and I left that in manifest in crl here or because it would get very close as well um so another approach might be that you you would say you have to take the union of everything that\u0027s good and undo something like that but I figured that you know this is the easiest seems the easiest to me now to continue if you roll to a new key and this is a plan roll what you can do is you can well first of all you introduce a new key three it has the same content as s2 but for number one you actually stop publishing shell certificates on all that you only population manifest in this URL and a new long lift attack file that says I am actually revoked and these are currently the new keys yeah I guess in theory that could also be a pointer to ta three there at this point to here but yeah it\u0027s not strictly necessary actually the this file this one still refers back this one needs to be revoked letter out here because essentially the relying party only discovers this when you "
  },
  {
    "startTime": "00:54:24",
    "text": "already learned that this is going to be revoked and if you would keep the list of all the old keys around all the time then this list grows in road so I don\u0027t think it\u0027s necessary and that\u0027s it really on plants though I didn\u0027t make a slide of that but I\u0027m plan that essentially is you have no ability to update this so it will stay like this right so the viewpoint from ta1 will will be ok 1 \u0026 2 are current but then 2 will say one has actually revoked so you still learn about it and this is also what I meant with there\u0027s no timing indication in there you could include it and if you know if you do a planned role that might be useful but in theory there\u0027s no way to be sure that you don\u0027t you may have to do it on that role so that\u0027s why I left it out I don\u0027t think it\u0027s strictly necessary to have it there but you know if other people feel it\u0027s important we could include it and I think that\u0027s it yeah so I\u0027ve been even more concerned cat here I would say was hurt occur I say and I\u0027ve looked a lot lately into rolling top-level keys of various things and well the short and sweet the TLDR is it sucks it\u0027s it\u0027s not easy to do right a couple of takeaways from what and I there\u0027s a document I need to actually publish at some point that after looking at all of not just you in a sec but looking at x.509 and peak Hicks and the web and that kind of stuff - there\u0027s a lot of takeaways that that I\u0027ve you know gotten out of it that I can transcribe into a couple of bullets of wisdom one you need to plan on doing longer key roles lifetimes and you were probably originally thinking you\u0027d come back to that in a second and two you really ought to be prepared with you know multiple keys in the future and part of it becomes from that that need for a long hero and and the reality is that in all of these systems there are really three places three ways that Keys get distributed toward you know running systems first off you have software updates where keys or basically you know encoded weather whether it\u0027s a you know a flash rom in an in a router or whether it\u0027s actually you know bird being installed somewhere they often come with pre distributed keys and that update process especially in critical infrastructure takes a very long time so the other option is doing something like fifty eleven and DNS SEC where you know there\u0027s this automated update mechanism not everybody turns that kind of thing on so you can\u0027t assume that the that is always going to be the case and "
  },
  {
    "startTime": "00:57:25",
    "text": "as you know warren found things running docker don\u0027t really sort of have that longevity of measurement and then three is the manual configuration where you know users wait until the very last second before they finally update their config you can\u0027t really you know worry about them too much but but my takeaway from all of this is that it has it takes a lot longer to do those those key roles and and therefore especially if you\u0027re planning on accommodating the software pushes the the you know the the pre packaged software pushes that\u0027s on the order of two or three years so it might actually be a beneficial to have you know two to three key staging in advance not just one or two and being prepared for that and so the good news is yeah you\u0027ve your happiest stateful yes and that doesn\u0027t I can be in a second sada but the document also says something about that you you may actually wish start shipping this and if this is your intended new key what you might want to do is update all the nine party software then there\u0027s out there to get them to include a new one instead but it yeah you probably shouldn\u0027t do this you know every week and and I\u0027d like to go less everybody supports it fully automated etc but I think we\u0027re a long way from that I like the fact that you\u0027re you\u0027re considering using double signatures which we can\u0027t we couldn\u0027t do Indiana Secretary Pat concise constraints and things like that which you don\u0027t really suffer from so that\u0027s the positive side but at some point you still have to turn off the old key and you\u0027re still going to break every that failed to irrigate hi job Snyder\u0027s just relaying a message from IRC beta from Dec from Power dienes would like to sit down with you to share the good lessons and the bad lessons from DNS SEC and sync up to see the some mistakes made in DNS SEC can be avoided in this rolling mechanism so that\u0027s astounding offer will you fuck I\u0027d like to point out that the rpki Act use actually has some significant difference from our DNS SEC in our PK in rpki we actually we actually need to expect that relying party implementation has some heuristics that raise alarms if things start to fail in big ways or while okay in suspicious ways I\u0027m not sure that all of the existing relying party implementations do this but one cannot repeat that often enough until it is "
  },
  {
    "startTime": "01:00:26",
    "text": "this way and of course they also need some heuristics how to deal with it and that needs that needs to be documented in some ways so now for DNS SEC one would expect that yes operators of resolvers get some feedback but kind of the expectation in rpki deployment I think is that relying parties really need to need to have those expose alarms and the workarounds and yes that\u0027s not really helping helping with this design at this point right so you have a relying party so far that doesn\u0027t support any of this it\u0027s configured with this truss anchor locator and it comes here and it finds an object that doesn\u0027t understand and on otherwise anti repository yeah and yeah I don\u0027t know if all the validators alert about that I think the right-hand Susi validated two series at some point raised a Luis if there was a change larger than 10% of the repository or something but yeah that may need some veteran you know that\u0027s that\u0027s that\u0027s really that\u0027s really important that\u0027s hurt occur I say again because I forgot my second bullet point which is that the other thing that was horribly broken with the you know sec key role in particular and is also broken in the x.509 world but nobody\u0027s thought about it as much there is that the signaling mechanism so the publisher of a key can trust that all of the clock all of the relying parties are actually using the new key is basically non-existent or broken or incomplete and so I can had you know an absolutely horrible heartburn up until you know a month ago where they finally just had to go we\u0027re gonna do it anyway without knowing if things were gonna break and you guys might have the opportunity to kind of fix that but knowing how files and think are synced I\u0027d have to go it\u0027s I\u0027m two years out of having real associations so they\u0027re not in my mind at the moment so you need to do revalidation top-down all the time anyway so I see Russ is in Lima you suggest that a document to me where basically inside a certificate you can signal this is going to be the next key I looked at that but I figured that we actually have a way here to to do this as well within the up yeah because of how its published you need to Bro validate this whole repository anyway so you can have an object there a signed object that [Music] these things that we want to communicate and that\u0027s essentially why I went for "
  },
  {
    "startTime": "01:03:28",
    "text": "that but I think in in terms of semantics it\u0027s not that difference but so to take away it\u0027s essentially saying these keys our next public key fingerprint essentially panel location where you might find them and it\u0027s communicated within let\u0027s say in banner of the RPI repository so you can find these things so as you\u0027re doing validation all the time anyway I think you\u0027re missing my point which is that maybe maybe that I\u0027m actually talking about the relying party signaling to to the people that it\u0027s pulling data from looking at the routing objects saying these are the keys I trust so that so that the the publisher of the objects can say oh it\u0027s now safe to switch to using a new or revoking an old key because all of the relying parties have now you know gotten to like 99% of accepting the new key and without that signaling measurement you just burn heartburn interred right well yeah okay just shooting from the hip what you may be able to do is monitor the access to these things are published in different places right so you can look at your access logs and figure out if people still try to fetch the the old certificate for example keep in mind but no yeah okay it needs so it needs some more thought because you know you\u0027re right that\u0027s incomplete so this is Russ I think there\u0027s a big difference between what Wes was just talking about in this because our certificates have an expiration date if we get to that point it doesn\u0027t matter that is not the case in the environment wes is just talking about the other thing is I do think the document I pointed you to creates a situation where people get used to building the next at the head of time and so you\u0027re always in a situation where in an emergency you have the key which are let\u0027s go Randy Bush IJ Marcus so unlike the locally world of DNS SEC a failure to validate within the rpki tree is a soft failure it will only produce not founds invalidation relax so Warren Kumari the other other thing we learnt from the DNS SEC key role is that yeah or something or maybe the only thing we learn is that emergency key rules are really really really different to regular key roles and actually the process that we have for normal key roles in the DNS SEC don\u0027t work at all for KS I mean for emergency roles and I think that there\u0027s some correlation here where you know I just my key is no longer trust able needs a "
  },
  {
    "startTime": "01:06:29",
    "text": "better signaling mechanism maybe okay yeah just coming back actually currently the document says these keys are equivalent and we have no way of telling you which is going to do the next role so you can\u0027t really monitor what people do right maybe it\u0027s better to change it to really say this is the Intendant X key regardless of that maybe you do need to do an on plan thing because then you can monitor whether people have moved over good afternoon my name is Alexander Smurf from Goethe lips and today I\u0027m going to provide a short overview of about all works that are related to rotis so as far as I know there are three active drafts in different working groups first one is BGP open policy and it is related to preventing networks from leaking the sector one is the oldest one and now it\u0027s about detection of flicked prefixes using communities and the last one is using your application object here and is capable to detect both mistake and malicious activity so looking at this comparison one might ask why do we need this thing with communities because we already have something more powerful more advanced that is capable also to detect both malicious and mistakes and the problem is time and time does matter taking a look at the raw deployment process it will take years before we\u0027ll get a stay a verification in the world and that\u0027s why we decided to use communities to detect root leaks we will not be able to detect malicious activity but still it will be helpful to detect accidental leaks and so protects our networks and from these kind of threats so this is how see the strategic planning so at first the first will be deleted community based leak detection and I hope it will be delivered during this year or not this not until these at the end of this year but using one year period I hope to take less than this the we\u0027ll be a lick prevention this will also automate the way leak detection is "
  },
  {
    "startTime": "01:09:33",
    "text": "working and the last one will will be a a spay which together with raw records will finally bring us through the paradise but so you know there was a update of a spade raft and authors work worked really hard as a result we have a new abstract and some small restructure of the document but there was an important clarification in the version zero one ace be is not trying to provide a full ice bath check instead of its predecessors it\u0027s just provides a tool to detect real operational problems hijacks and root leaks that\u0027s all it\u0027s not a general solution for the ice bath it just a solution for real problems so this is how I see it in a near future I hope you will be able to see this progress in half in half year so be open policy seems to be ready nearly ready to up to the last call we already have two implementations and so after a clean up I sure I hope there will be a logical and it will be successful second root leak detection is also near a radio so after it\u0027s the growth session it seems that we finally decided to go on with large communities and specified format for these large communities there should be no location anymore and ASP is be a verification it seems to that we are ready for adoption code so I requested option call now thank you any questions please be sure to send your request enlist sure Julio Volk stupid question what well ok you are saying for open policy there is still cleanup necessary cleanup in the text just there was life it hopes that were included if it\u0027s some it\u0027s some time ago and we just need to make a cleanup be quick ok it was because time matters can that be done really quickly no I hope so I need some input from my co-authors and I hope I will have it that you in this meeting after that we\u0027re done ok and as you are asking for adoption "
  },
  {
    "startTime": "01:12:33",
    "text": "yes the ASPA is fine yeah thank you so it\u0027s good to this we have a consensus I hope so and I hope that both working groups of both all working groups that are related to this topic grow idea insider ops will soon see the progress in all these threats thank you that\u0027s totally not your side it\u0027s got two rolls in it maybe three it\u0027s coming [Music] thank you so we discussed this in back in London and desert policy and here the idea is to drop invalids if it is covered if there is another prefix route in in the router that is for the less specific that is either valid or not found in that case the most specific invalid can be dropped so this is a joint work between NIST folks and job so just to recapitulate what desert policy is basically it desert stands for drop invalid if still routable so the idea as I said is drop invalid if a valid or not found less specific route exists and why why do you why do we want to do this or policy so if a rover for a subsuming less specific prefix exists but there is no Rover for the prefix that you want you want you announce then the desert policy working elsewhere in other a SS it ensures that the traffic for the more specific still reaches you you are the "
  },
  {
    "startTime": "01:15:33",
    "text": "legitimate destination for it and but it reaches you possibly in a suboptimal or non traffic engineered path invalid announcement of your proof of your more specific by you or others are rejected so that\u0027s the basic idea of the desert policy we updated the draft to we got pretty good feedback and discussion when we presented it in London including comments on the list so there are two or three main discussion points questions that came up that that we have addressed and updated that the draft in the o2 version these are included so the s0 question s 0 Toa question came from Tim and the wording in the draft which did not exist before it says the existence of an S 0 Rover for a prefix means that the prefix or any more specific prefix subsumed in it are forbidden from routing except when there exists a different Rover with a normal ASN for the prefix or the more specific so given that understanding of a s0 rower the desert policy must apply the following exception if a route is invalid due to na s 0 Rover then always drop that out so hopefully that takes care of Tim\u0027s question so normally you would if if a route is invalid you would look for a less specific that is either valid or not found but if na s0 Rover is the one that makes it invalid then don\u0027t even look for it just drop it once should we can take questions on the fly if you know ok let\u0027s a couple of more minutes and we can finish through this and go to the questions and kay you raised this other question about default routes 0.0 v4 or v6 in the routing table so those must be excluded from consideration in the desert policy this what I think I would wanted to see so that that wording is also in the new version of the draft John Scudder pointed something about multihoming so jeff has has had some multihoming question so John took a cue from that and he posed this other question so in this scenario there are 2/20 force at the bottom those are aggregated by as2 into a slash 23 the and that slash 23 gets to a s4 and why another path through a s3 the slash 24 "
  },
  {
    "startTime": "01:18:35",
    "text": "on the right also goes to a s4 and there may be also a hijack also happening from a s5 so John\u0027s point was that the in the case of multihoming the slash 24 on the right it is really multi-home so that it can it can have reach ability in the event of a limit link failure so in this example the link between the stash 24 on the right and the s2 fails and now the question is for some reason John third thought that a s2 might still be aggregating and forwarding a slash 23 to AS four and John\u0027s concern was that a s4 now he drops all the invalids and routes the traffic for the slash 24 on the right to a s2 and s2 has no way to deliver it so that was is concerned then he\u0027s saying that sort of the purpose of multihoming is lost but the point is this is happening when you have a combination of things there is multihoming Noro as are being created by this slash 24 and there is also a link failure so apart from the from the combination of all those things having to happen if you look at a s2 one might ask if the link is down and it is no longer has announcement for the slash 24 on the right why is it still aggregating it should just D a grenade and send only 1/24 up so that itself probably answers the John\u0027s question in in addition of course any multi-home customer in in any scenario should be all should be always crew fool about rpki if they don\u0027t create ro as for any of the multiple announcements they make to their upstreams that and not have I mean not having rowers not a good thing so in terms of conceptually implementation Jeff had this was not in the original draft Jeff before the London meeting I spoke with Jeff Haas and he offered some interesting insights into what is feasible in implementation so so that aspect is now captured in the updated draft so the next two slides which talk about conceptual implementation they are now included in the in the draft yeah yeah they are fully described I mean they are described at the level of conceptual implement a implementation so Jeff\u0027s idea was that you don\u0027t do the desire policy as part of the path selection decision process you do the path selection decision process including origin validation and then you put your valid invalid not found routes in the local drip and after that you do "
  },
  {
    "startTime": "01:21:35",
    "text": "that is their policy and before you you\u0027d send the routes into the PHA but the ad ribbons from the local rib that is where they in between those two is where you do that is their policy and that we seem seem to make perfect sense for many people that that looked at this in London when we presented it so this included in the new draft and another point is that you can have these three situations that arise a valid or not found route that was used to route to drop invalids that when a new one can be can be added or an order existing valid not invalid route may be withdrawn or you may have a PKI state changes and they\u0027re out there is informed about it in all of these three situations we need to we need to do something that is a policy needs to deep refresh and those are described as well thank you questions and you would also like to request adoption call on this draft okay job Snyder\u0027s entity this may come as a bit of a weird statement given that my name is on this draft but I I am not so sure at this point whether pursuing this draft is worthwhile because I feel we\u0027ve been somewhat overtaken by events if I look at origin validation deployments and I\u0027ve had some handsome experience by now the mode of operation simply is reject invalid and deal with the fallout the DSD is our mechanism we came up with that as a sort of dampening mechanism to ease into original validation give him that there\u0027s between five and six thousand envelop HP announcements in the default free zone but I think attacking the five six thousand invalid announcements should perhaps not be done through a router change where we you know upgrade codes look at the deployment timeline of three to four years and then have a degree of tolerance for miss configurations I think it will be easier for everybody if we just reach out to the people that have misconfigured their robust and asked them to fix it or delete it new Sanu has shared some research on the right routing working group mailing list and on the nanak mailing list about who the entities are that have done miss configurations and it seems that the majority of miss configurations comes from only a handful of companies so I think this is a I\u0027ve come to realize that this may be a disproportionately big hammer for a "
  },
  {
    "startTime": "01:24:37",
    "text": "problem that can be attacked through different means and conceptually I like this but I\u0027m really not looking forward to getting to running code because that will require me far more work than just fighting the bullets and dropping in ballots and calling it a day so I\u0027m not sure we I personally I\u0027m not sure if we should pursue this idea so for the I experienced Eames like it makes sense to drop the invalid even now because like you explained yesterday or in the apg you you I mean it will just go through the transit provider rather than go through the IXP right so like I can lay in other cases that may not be true so if you drop invalid there will be so it\u0027s so the assumption that there won\u0027t be any invalid after some time if you are diligent and make calls I don\u0027t know if that assumption would hold it hold good in the long run so what I didn\u0027t mention that\u0027s I apt I did mention that I\u0027ve space should drop envelops right now but in the Dutch networker community we are deploying original edition in is piece where we do the same philosophy and drop in fella announcements and there\u0027s a significant number of commercial internet providers that are dropping in ballots and they report that it\u0027s not harming their business and that it\u0027s a viable model so you are able to drop invalid without affecting reach ability for the for this situation so it affects reach ability but it doesn\u0027t affect business another number I can share from a it doesn\u0027t it doesn\u0027t create user tickets well it creates a few per week but it\u0027s manageable that\u0027s the that\u0027s the message the the the discovery of the networker community is hey we can drop in ballots and it\u0027s not her too bad can it quickly since you both are the co-authors in the document I suggest at least to take this offline and then once you have a consensus amongst yourself make a request again if you want to still adopt this as a working group documents thank you I guess so so Warren Kumari I\u0027m was a part of the NOC team so on this network we have two peers ebgp peers who are taking full routes from both and we\u0027re dropping in ballads on both so it\u0027s 6100 prefixes or so or routes that we\u0027re dropping nobody\u0027s noticed nobody\u0027s complained no ticket no tickets I mean he noticed and was like why are you still passing these on and then I realized because the filter wasn\u0027t actually applied but it seems to just work presumably some of them are following covering routes maybe "
  },
  {
    "startTime": "01:27:39",
    "text": "who do folk I guess the timing argument is fairly important but first thing is I hate it when someone explains that a zero rowers are anything special the only special in the a s0 roars is that we know that routes with origin a s0 cannot occur hopefully the route of vendors actually enforce that or VI ISPs in their routing policy there is no doubt with s0 rower there is only a door with a s0 there\u0027s no doubt yeah well okay but explaining anything special about a s0 rowers is kind of misleading people if I if I if I if I in create if I create a rua for an a s that I own and I know will not show up in the in the wild it is effectively the same as an AS zero raw yes well okay kind of kind of creating special semantics for it are something that I would question very seriously but K you\u0027re actually if you found that also one has to protect against using the default routes of course that is something where you figure out one okay you do not you do not only want to protect against zero colon colon slash zero you also want to protect against zero colon colon slash one and where does that end now consider that as an invitation to people to announce / eights again that they do not own in the public v4 and all that stuff I I just wanted to comment quick Tim evangels quickly on the ASE or thing so personally I\u0027m not a fan of a s 0 robust but I brought it up because people to see these things as meaning block this and if that is intended then you would need to do something extra for that to work that that\u0027s what my point is not that I actually like them or want them or anything I think that\u0027s a completely separate discussion but people people "
  },
  {
    "startTime": "01:30:40",
    "text": "who think that there is actually special semantics don\u0027t get the clue that when they do their a as one two three Aurora it essentially excludes everything below regardless of what a s number and unless unless like in VA s 0 as more specific roar is issued for different yes okay I think probably any more needs to go to the list George\u0027s oh okay he\u0027s gonna so hopefully this is a shorter version of what we\u0027ve all seen once before so the problem statement is reasonably straightforward we have to have a coordinated state between three slightly different communities of actors we need code to both sign and validate and we need an agreement to move to using that code by both relying parties and signers and we have to decide how we\u0027re going to move this of course is predicated on the assumption we want to move in any class of technology in this space which is materially going to alter the behavior of validation or demands change in the existing software suite to take respect to something like an OID change or a structural change in the certificate so these conditions are in some senses a general statement would apply when we make non-compatible changes to the system but they are specifically focused in this conversation on validation reconsidered so I did a review of what I felt with the code changes we had conversation last time there are some people who would disagree with my summation of the ease of doing this I left the slide in for sheer cheek and outrage but essentially there\u0027s a conversation about how hard it\u0027s gonna be we have not had a real conversation about a timeline as RIR as the engine is in the RIR community we feel a very strong concern inside our own space about our exposure to risk it and we would wish to propose a future date which has not been too find in this document because that is presumptuous but we think that we need to define a future date to have a flag day and give somewhere around a year of notice the complete migration to a new OID and a new model of validation and our intent is to actually go into the nog community and talk with operators to "
  },
  {
    "startTime": "01:33:40",
    "text": "get some sense of convergence and consensus from that community of what they want to do but when I presented this idea last time I got very strong pushback on an expectation that a draft should exist that is a formalism that can be talked to in that context and having discussed this in the ECG we agreed to submit a draft so there is now a draft document which was lodged midway through last month which I am effectively seeking adoption on and it discusses some technology questions around what do we actually want to do it we want to make a complete radical shift in one hit to a new OID space or do we want to have some mixed mode of operation where the introduction of a new ID provides signaling options of a rigid validation model and a more lacks validation model and the questions that would arise from having provision of both in the same information space I feel very strongly personally that it is simply not possible to move without some sense of tripartite convergence on this it\u0027s a mutual deadlock situation and that does implicitly mean anybody in that sense of community who says I will not move has oppositional qualities that really are a roadblock we can\u0027t move without code we can\u0027t move without consent and agreement by relying parties and Cas to adopt this new technology we can\u0027t deploy towels specifically in our rir role without a sense the community is ready for them because deployment of something with a radical new OID instantly breaks the validation model for everybody I still believe that we are in that golden moment my sense of the number of active party who are engaged in this is that it is less than 500 now we can quibble about what the exact numbers are I am involved in a body of work with a collection of people to try and explore the space and understand how big is the field of people who are using this system so we already have a problem it\u0027s a distributed problem but it\u0027s not internet scale in the sense of tens or hundreds or millions of people it\u0027s 500 engaged entities and it feels to me like a Flag Day is logistically simple and actually can be done we can do this so I\u0027m seeking working group adoption for a zero state craft that has been authored by myself and Tim Tim was in the RIR system and wrote the initial state of this draft in his previous state but he has since moved to a different role he now works in NL net labs developing rpki software router Nader ha route inator 3000 sounds like something that should be on sale on a TV shopping channel yeah it\u0027s brilliant I mean I think I think "
  },
  {
    "startTime": "01:36:40",
    "text": "it\u0027s brilliant yeah these apps perhaps later did this for me so he\u0027s moved role and will probably consider an author shift because it\u0027s not fair he should have to take the eggs and tomatoes because he\u0027s not actually in the RIR space but nonetheless he remains on this ground state craft and will discuss what to do next but this is assuming we get adoption and clearly there are many things in the conversation one of them is that we probably have some flaws in the model we might have put forward timing constraints or behavior constraints that are wrong so there are conversations of merit around how does it work but there is also the more important conversation for me which is what would be a to be defined date if we were going to do this what are we prepared to put in the ground as a stake and say we\u0027re aiming to move in 2019 in 2020 because to me that\u0027s the more important quality what we do when we move I understand in technological sense is more relevant but the decision to move and when and how much notice in a process sense is what actually motor baits and drives me because that drives the conversation in the community to an emerging consensus that\u0027s it Radhika hi George my memory is leaking so I may have well ok my ears are someone sometimes dumped so I may have missed something do we actually have a roadmap were also into operability reports show up having running code kind of yes kind of having the kinks actually worked out probably would be good to have have been achieved and proven before of the flag day comes that\u0027s that\u0027s probably a fair point if we were going to actually do this and make an assessment of dependency and interdependence then having an understanding of which platforms do implement support for this would or could implement support have some interrupts that\u0027s reasonable um I believe the right validator has code for this model I\u0027m not convinced that it\u0027s live but I believe it was at one point implemented in their codebase but there\u0027s a reluctance to undertake a body of work that doesn\u0027t have broad consensus we\u0027re left with a problem and we are very aware of our exposure of risk in this problem and this is a path out for our problem but we have the other side of the balance book we have budgets in time and in labor so if there\u0027s no community intend to move then asking people as validation software developers or see a signing party to implement is a burden they don\u0027t need we "
  },
  {
    "startTime": "01:39:41",
    "text": "need the signal but no it\u0027s a fair point we would need a matrix we would have to understand interoperability I can stand here for another 15 minutes if you really want 20 minutes of me but I don\u0027t think you want the date what I think help students just make a date say 18 months from when the RFC is published okay yeah although otherwise we put 2019 in the original dog but X hey it feels a little unrealistic I don\u0027t know Jeff Jeff Houston um really his points really quite valid if there\u0027s no code I like setting a dates bullshit yeah you know don\u0027t know it\u0027s fair right you need code you need to actually have version and write your code before you can figure out when a dates valid that\u0027s all I\u0027m saying so we probably fantasy horses so we probably do have to lay the hole in the ground and write the code and test it to see if it\u0027s viable that would make an awful lot of sense in an Operations area George the long-haired friend behind you has comment at the mic well Rob last night so it should not be news that my gut reaction is two days after hell freezes over however I completely agree with verdigris comment if he wanted to consider this seriously I would be more comfortable with deciding whether or not to do it after we know whether or not it works so write the code first test if you can prove that it\u0027s harmless we can have the conversation about whether it\u0027s a good idea okay this is actually very good input because we have a coordination meeting this week and we can actually discuss investing the time to do the code base to provide a basis to make the move but Rob you are an author of validation code and you have a time budget for work in this space and it\u0027s pretty small so are you saying you will implement I don\u0027t recall having said that very not okay I\u0027m thank you thank you very much um do I actually have to formally stand on one leg inside requesting adoption mommy as with everyone else who\u0027s done this today [Music] [Music] "
  },
  {
    "startTime": "01:42:51",
    "text": "[Music] [Music] [Music] [Music] [Applause] because "
  }
]