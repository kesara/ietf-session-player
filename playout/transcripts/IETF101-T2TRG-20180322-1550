[
  {
    "startTime": "00:00:28",
    "text": "f91 resolution by that work things this thing is but it\u0027s not just okay good afternoon this is the thing to thing research group I\u0027m Kirsten : and it\u0027s Thursday afternoon so if you get the impression that that weird behavior little bit strangely it\u0027s just because we are completely exhausted and are leaving this hotel at four o\u0027clock tomorrow morning to go on to the next meeting there is a much better note wedge slide which we failed to faith in here so you can just go to a ITF of IPR for details but I think by this time of the meeting you you have seen that so you the pink sheets we have no takers do we have a Jeb honest nights look Tina Hannes so can somebody show honest the end of it okay so well yeah there is a mailing list and most importantly we put our github repository for each of our meetings and the one for this meeting is t2 t IG / 2018 dish IETF 101 so we have a nice assembled for today so the chance works quickly speak about the research status then Matt Acosta will report from the hackathon that we had on the engine from the Russia approach that was underlining that then John will talk about something that sooner or later we all will have to do for some machine learning and while he will not talk so much about how that is going to influence our work I think we all can start imagining that then "
  },
  {
    "startTime": "00:03:28",
    "text": "Sonia will talk about the project that extends the range of iterability testing to semantic interoperability so that will fit nicely drove to Michael\u0027s talk and finally we will have a talk and I\u0027m not going to try to say his name microphone from someone with a bothers me about secure computation should be centralized and violence that earlier version of which was at the MDS s this workshop and we thought was interesting enough first target and at the end don\u0027t know the way before the end we will have a quick look at meeting planning in this case mostly look at the meeting will have to with w3c and ocf okay so just to remind people what the research group is about who here is new to the research group has not been to a previous meeting okay so let me quickly say why we are here we care about a true Internet of Things which means our planet we are things including low resource nodes nodes with very little computational ability and power can communicate among themselves and with the wider internet so we are looking at issues in this area in particular when there are opportunities for IETF standardization or I should add for cooperation with other SDSU\u0027s for others with other standards developing organizations to make them happen so we don\u0027t usually do radios here we sometimes have to look at the radios if they behave strangely here will slide on that in a minute but we not only started the IP annotation layer and end in the hearts and minds of end-users so this is about the range we are trying to cover here then huge security because nothing India can work without security so recently we have had a big focus on semantic and hyper media interoperability and we have had a couple of meetings under the name wishy one physical and several over the Internet we also have a number of draft "
  },
  {
    "startTime": "00:06:30",
    "text": "the IOT security considerations draft which entered iesg IRS G last call we had a workshop and mbss on decentralized duty security at standards i quickly reported in nature we had an interesting side meeting so essentially we you know all these sounds about how many people fit into a phone box of course these don\u0027t work anywhere because they are for marcus any law so we just usually use the meeting room for the same thing and had a side meeting with 29 people in the room Waterloo which is about the size of the school and finally we worked with OCF we are having something like every six months a physical meeting and we had meetings in between in particular as this time to talk about security and the ACE activity okay let me quickly say something about the NDIS this workshop of course those are a few who work in security know that n VSS is one of the leading security conferences they usually have a couple of workshops associated with them and this time we managed to land a workshop on decentralized I hope you security and standards and this was about combining two views one that you actually have to have standards to get out early security and the other that IT security often will not be based on centralized approaches but will require decentralized or non-centralized approaches so we had 12 papers which are still in the process of being published so it will take until about mid April until you can download them these ranged from the security analysis of current ITF work to much more speculative ideas and yes we disgusting and yeah one of the trucks we actually have asked the author to come here because he\u0027s based in London and tell us about so that would be later in this meeting so what are we going to do next tomorrow we will be in Prague with OC f and w3c where are things so OC f is the open connectivity foundation the organisation that resulted from the merger of oh I see you PNP at the ancien Alliance and they are doing a very "
  },
  {
    "startTime": "00:09:31",
    "text": "interesting standardization work at their heavy user of IDF technologies so it\u0027s a good thing to talk to them and we will talk you particular about requirements they have our standards so that\u0027s that is really important for us now we have about in about a monthly Kevin\u0027s wishi cause where we try to move forward on the work that this workshop which was in Prague Prague where he started we as I said we make meetings with och maybe people to have further face-to-face meetings with ocf so I personally will be at one of the next plug test events of ocf so that\u0027s one organization we try to keep contact and what we should also do is plain what we will do in Montreal what we understand interesting and we are they are also interesting local people there that we may want to contact we suddenly will do more we share related things there there has been some discussions about distributed this country which won\u0027t follow a little bit the theme that we had this workshop but this the documents of the research would we have been working on a document about state of the other challenges for the IOT security better known as the security consideration is photo from Nokia so in terms of these joint meetings it may be interesting also to have a joint call with a joint sessions with we may we have a July we have a cos fest in Co which is done by TTA it\u0027s an organization standardized a test standardization organization in Korea it may be interesting to showcase some of this and also participate in the light a table to amicus fest there that maybe I think it is 17 to 23 if I\u0027m not to my mind is not know that\u0027s a previous week the previous week to IETF is where we are meeting in C own ok so that\u0027s like Monday to Thursday so that we are breaking on Friday so that people like Anna\u0027s can travel and hopefully but yeah definitely is one of the organizations that we have been cooperating with an IETF so for instance one of the collab flood tests was hosted by IO a night which was really useful because in the process we learn much more about the Iranian trip specification which was "
  },
  {
    "startTime": "00:12:32",
    "text": "pretty young at the time so we certainly make sense to deepen that ok so back to the documents we got very nice reviews particular from Jeff Shawn and Steve oh sure we got lots of discussion on the list and we are not going into IRS G review and this is always a good time for people to actually look at the document and yeah and maybe so the other document that is nearing completion is called restful design for IOT so this essentially takes the when your architectural style that is underlying the HTTP protocol and much of the web work and explaining how to use this by Internet of Things applications because there are some things that that may just not be very intuitive so that is a document that is pretty far advanced that there is a per request in your review right now we develop our documents on based on technologies or we use something like pull requests or mesh requests process larger changes and there is also an issue right now that we have to have okay okay so as Carsten said we we tried to fit 30 some people in a room to mentioned for about 10 or 12 other than that the meeting went very well there is a draft about some of these issues but the main idea is that when we put administrative Lea independent IOT applications so something I put my apartment something you deploy in your apartment no administrative relationship between them with these packets because of radio interference and that\u0027s not the IETF problem but what some recent research results suggest is that the protocols are inducing timing behaviors sensitivity to loss of certain kinds of it\u0027s that ours asked that exacerbate these problems and we can get kind of severe performance degradation at the protocol level and potentially that is "
  },
  {
    "startTime": "00:15:33",
    "text": "something that that we need to think about it this layer right now there don\u0027t seem to be good tools use simulators test beds that help us evaluate these sort of multi Network scenarios or if independent networks sharing the channel just some of the things we discussed miss Ted just potentially a number of IETF protocols anything that\u0027s touching these Mac layers six five six th some other things that came at the discussion this is obviously somewhere on the IETF I Triple E border clearly the Mac has to do most of the work here but it administrative ly independent networks the Mac only has limited abilities to isolate isolate the networks in the link sense so I think there are some rules for for IETF protocols to to try to understand the performance in this environment and potentially try to make some improvements some folks also raise various possibilities for active or explicit coordination via high-level protocols so there is a draft and I encourage people to be interested in this go get Montenegro a yeah I was wondering if you could elaborate a bit more on this research because for example in Europe the ICM bans require listen before talk not that\u0027s not required in US and many other in some other places but most that\u0027s why 3gpp with court and coexistence problems in in bands that are typically used by Wi-Fi assumed LBT and built on top of that so my question is is this research that you\u0027re pointing at doesn\u0027t it US does it assume LBT is in place or not or or is it that even with LBT you still have all these all these effects so basically what we are trying to do here is point out the problem so there is some initial research but the main conclusion from that is we need more research so it\u0027s not yet trying to answer other questions so essentially we are really this this a transportation system for cars that work on an empty Highway but in reality we have hundreds of SSIDs and apartment buildings and all kinds of gardening systems and whatever that see each other in the wired which never have you tasted from and yeah some things are tested but in a lot of situations that are just very hard to taste test in today\u0027s dispensed and so on so the idea is to just look at the situation "
  },
  {
    "startTime": "00:18:33",
    "text": "and find out what research can we get started based on the fact that we are no longer in their face where we have a single thing that has a cover of nose around it but we have independently and I only suggest would be to look at it in a from a practical point of view and many regulatory environments it won\u0027t be completely blank there are some rules already a place would be good to know in those places in the draft I think maybe since this wasn\u0027t a main topic for the meeting Carstens up that I\u0027m happy to take discussion question if you want to move on to the main program I\u0027m happy to refer people to the draft and ok thanks Juan Carlos when you guys see cooks so yeah just responding to to Gabrielle because I guess discussion but you\u0027re right all those points where we\u0027re definitely part of the discussion there\u0027s multiple regulatory domains if we focus let\u0027s say you\u0027re nice and bans there\u0027s different rules on there country on how you operate on these IES and bands not all these protocols follow the same Mack for instance so here we talk about the Triple E that\u0027s just part of it some some protocols may not be following the same as long as they comply to the regulation they are legal to operate in this country that they may not be legal to operate in this other country however I think at least a take that I that I got from from the discussion is there\u0027s still some potential to share some knowledge at higher layers and and that\u0027s why with there there\u0027s a pointer there to their white spaces protocol that was the past protocol that was a fine before where yeah I mean through the internet you can get some knowledge that could be useful for your area and maybe this bans the the sorry this operators are using or either private networks public networks standardized protocols or from different entities but still there could be some interesting information like I don\u0027t know to me it sounds like level of noise in this region something that doesn\u0027t touch on privacy necessarily but some some useful information that could help me operate better you know their problem you know you can do something in an IP at the IP level and communicate networks that may or may not be connected to the Internet maybe administrative level not at the device level but I think the solutions today but yeah I think the first question is where and how do you want to continue a conversation on the research reading list okay and the second question is within that draft is there a data collection a methodology that\u0027s recommended to try and characterize the problem a little bit more so the document really kind of "
  },
  {
    "startTime": "00:21:34",
    "text": "forces on focuses that kind of laying out the challenges systematically it points to a couple of research results both mine and other people\u0027s the main point in the documents is that I recommend and that the TTT RG or some other mechanism we try to understand what we would like to have for some practices for evaluating how the ietf protocols you know six low 60s role perform in this environment we need to have a better understanding that\u0027s really what the document says apologize for following up even before we make recommendations about protocols right the I think the first step is we might make sense I agree with you only understanding as part of getting the understanding of avista useful to have some sort of data collection sauce analysis methodology so that we can better characterize and this is a good I think we\u0027re not even there yet I\u0027m not I\u0027m not sure as a community we even know what we want to know and there has an eighty it\u0027s like you know the things that are getting that we\u0027re discussing there that\u0027s a really place to do and there are a lot of interesting topics there so on terms for this one this touches a lot of groups right now it\u0027s at t2 TRG there may be some discussion where we\u0027re or whether that needs to go somewhere else but I think not now because Hersh team has a program Thank You Laura so yeah you can expect to hear more about that future so let\u0027s dive in into the general after this first world so next is Michael with a report from the Vichy and hackathon activities right we have a as well I\u0027m gonna report on the the wishing part of the thing to thing research group which he is the workshop on IOT and semantic hyper milk media interoperability and we\u0027ve actually had eight calls since the ATF 101 because we\u0027ve had a lot of activity in planning this our participation in the hackathon and we\u0027ve really covered a diverse set of topics around the semantics and I per media I\u0027m probably not going to try to read through everything here but essentially we looked at sort of top to bottom so what is what is the nature of abstract semantic time we want to describe something you couldn\u0027t wait list protocol neutral how do we do semantic annotation how do we connect with RDF ontology x\u0027 and other data datasets that "
  },
  {
    "startTime": "00:24:36",
    "text": "are better around like these third-party vocabularies yeah so you know the qu DT as though I say SSM that are fairly hard to use and how do we make them easier to use how do we how do we integrate symmetric metadata into the system where does it go and you know who uses it how do they use it this idea of a layer semantic stack there may be different semantics at different layers and the stack at different protocol levels you looked at basically how to how to work with other stos around the semantic interoperability and also you know some of the more mundane standing some of these stuff like data types of engineering units which is still you know really part of the picture so from from top to bottom so the goals for this this one and we\u0027re probably going to have a series of these but the goals for this is the second hackathon but this is the first hands-on where we bought hardware and tried to make it do things bringing they diverse things together to interoperate and we wanted to start focusing on you know where you would start in an application workflow of doing discovery and synaptic based discovery and software adaptation to different data models and different protocols and that\u0027s that\u0027s generally the theme for this one we had HTTP co-op and mqtt protocols involved I\u0027m not sure anyone used the mqtt but we used coop and HTTP we started with really simple application scenarios like can a signal input like a motion sensor turn the light on and off just for very very basic orchestration and the implementations that people brought were w3c web of things which we\u0027ll talk more about there was a yang coma there was a coli based implementation tried to interoperate there was a om a lightweight m2m client and server and there was some various ad hoc device api\u0027s protocols just basic rest api like you might see from various connected devices that don\u0027t follow any particular standard but they still use HTTP and they still have a a rest ish sort of API so we looked at connected home and automotive domains and the idealist you know how do we make them work together what\u0027s the technology that we use and we started using some of the stuff from the w3c web of Things group because a lot of the goals and objectives are very similar so we brought that that technology and it will talk more about that but what we ended up with were beginning to generate thin descriptions which is a sort of w3c web of things metadata format from lightweight m2m "
  },
  {
    "startTime": "00:27:36",
    "text": "instances that we could find on a lightweight m2m server we we we started integrating komai with the descriptions so to show that you don\u0027t really have to have any particular kind of protocol of what what\u0027s the range of protocols that can be connected to we we brought a thin directory and we stored thing descriptions in the thing directory I\u0027ll show more about what that that whole thing looked like and then we had a at the end we had the web things implementation communicating with these different device implementations and different protocols so the thing description was was what we used as the metadata common metadata format here it\u0027s basically a media type with RDF it describes the abstract and interactions with things so you know read the temperature or lock the door this sort of thing and it also binds those abstract interactions to concrete instances of things that implement those interactions so you know things like the data shape payload structure data types and transfer layer instructions like use this protocol scheme use this method use these options and so the applications use abstract interactions to interact with things so they\u0027re decoupled from the underlying implementation so the idea is you have one application talking with many different protocols and really doesn\u0027t need to know what the underlying protocol is so here\u0027s a little picture that illustrates this description has both semantic annotation high-level information models and capabilities like what do you want to do in an abstract sense and protocol bindings sort of from the device specific problem how do you do it you know OCF lightweight.if so dot these are all based on coop but they used collab quite a bit differently there\u0027s not an easy way to to use them all in a single application without doing some adaptation and so that\u0027s what we do with thing description is that\u0027s up for me a format for adaptation between these different protocols so thing directory is what we use in the and hackathon as a central registry of these thing descriptions so basically each each device or connected thing registers its thing description in the thing directory and then applications discover them and find them using semantic queries and thing directory uses the same protocol as a core resource directory so it\u0027s the same sort of handshake protocol with uploading the thing and having to refresh it and being able to go in and do URI queries on it "
  },
  {
    "startTime": "00:30:36",
    "text": "to find out what you want to look for so there was one or more we basically used I think one thing directory but there could be one or more of it these are the well-known entry points to the system you go to the thing directory find your theme descriptions and that tells you everything you need to know about what that device does what that thing does and how to interact with it so look here we go so this is sort of a schematic of what what sort of roles and services are available and connected devices register with the directory and then applications small applications discover them after that the application might interact directly with the connected thing or it might be interacting through some intermediary like a proxy or a pub sub broker and if it was a pub sub broker there there\u0027s some coordination here to register in thing directory the address of the pub sub broker and how to communicate with it as opposed to the thing itself and some of these like my entries for example and some others registered both if an application is unmanned it can find and interact with the device directly if the application is on the Internet at large can find a proxy interact there and the thing description just had more than one entry point local land and lunch fora reachable over the Internet so here\u0027s some examples sort of showing that the same format how the how the gangue implementation worked it that it\u0027s stuff to a thing directory and then the Serbian which is where the application runs can then access the device same thing with an HTTP device that might have some ad hoc protocol buttressed fish registers the TD and then the application discover from the TV and interact with the device exact go back one slide and explain a little bit but the entities are so so the HTTP the green box is that the IOT device there the green box is that the IUD device that could be the device so that could be an angel on behalf of the device okay but with which one so I\u0027m trying to make sense out of this time I understand that that the thing directory is the directory that you previously explained which one corresponds now to the application sort of like and to the connected thing and the intermediaries on the next slide so like here so so I see the directory finger rectory which "
  },
  {
    "startTime": "00:33:37",
    "text": "one is now the client that wants to access some data from the IOT device is the comma I device is that the IOT device and and the application is the w3c wrap of things Serbian is that true yes yes embrace the proxy here sorry and the proxy it doesn\u0027t exist here so sorry this is acting as a proxy for the device the Serbian is essentially an application proxy for the device so the Serbian interprets the the transfer layer instructions from the thing description that it got from the thing directory and it uses those to access the device so in the w3c Serbian software we already have the adaptation or the protocol for the for the transfer layer so we use that as as the application proxy I\u0027m sorry I think you have to update the slides to get them in line with the architecture picture because there\u0027s one entity than apparently missing and some errors may be missing as well because in a previous life the IOT device registered itself at the directory directly in this case it doesn\u0027t and there\u0027s no application so that\u0027s what only one clarification here in the case of the hackathon the creme box was for example just a file so it was not necessary even created or by the device yes that\u0027s different gonna be helpful but they\u0027re these lots are more showing their architecture on the hackathon which is not fully the same as today let\u0027s say ideal architecture but this is how the hackathon setup was was made and the green box is in general are the new ones that were built for tacit on purpose here\u0027s a better picture to show a lot of these things so for example here the lightweight m2m device going through the lightweight m2m server and this discovery adapter will discover things on the lightweight m2m server and register them with the thing directory so that shows the whole loop that wasn\u0027t shown on the other side so unfortunately so like I take it a good point well we should fix those up to make maybe put something like this a little more upfront so in the same way for the HT so here it shows the device isn\u0027t registering this the device is doing lightweight m2m to the lightweight MTM server it only needs to have these tiny short URIs and numeric identifiers and all of that then this this piece goes in and expands them and knows how to create all the semantic annotation based on its knowledge of you know lightweight m2m number codes and if so smart objects and what have you and then and then the Serbian can can say oh I\u0027m looking for a temperature sensor and can find this and then from the transfer layer instructions knows how they interact with the device and the same thing for the HTTP and comai devices but they didn\u0027t show the whole loop of huh so the HTTP device was a Raspberry Pi it could just it can it can basically go "
  },
  {
    "startTime": "00:36:37",
    "text": "directly and register it\u0027s it\u0027s things with the thing directory also we had some cloud services that had we\u0027re running on a digital ocean instance that had enough you know CPU and enough resources to be able to go and both expose the devices in and create the semantic descriptors and register them so next steps we\u0027re here we go want to go look at semantic annotation and discovery like the thing directory case but using core RT and link format so how can we semantically annotate web links with the same information to enable the same sort of discovery on a general web linking format we want to look at different end device protocols and data models so you know please like back that even are part of this right more automation of the semantic queries because we\u0027re mapping this are : point to you our eyes how do we how do we make it easy to generate those mentally when you did the mapping from let\u0027s say like a team term two-finger scription from the w3c or other like kamae to that how was was that possible without losing some of the semantics or like because that has always been a challenge like we have all these different activities and we are piling on more and more standards activities on different descriptions in then it becomes very difficult to map them to each other without losing some of the semantics of it so how was that what was the experience obviously you used to started before simpler scenario that presumably everyone has but well when we\u0027re starting we\u0027re reusing these devices that are biomes and temperature sensors and stuff there\u0027s not much semantics there\u0027s not any synaptic box in fact the the semantic annotations pretty much have there have a way of describing all of the what you want to do sort of sort of things and that\u0027s all extensible so if you need more affordances like what\u0027s the timing of changing the brightness of a line or things like that those those are all part of that and can be added in the abstract because we don\u0027t have to describe specific protocol features it\u0027s sort of easier to to cover the broad field semantically but a lot of the differences are in the protocol yeah but one of the challenge or one of the observation that we learned from the IOC workshop was you can\u0027t just look at the data model but you also have to look at the interaction model because they are closely covered together somehow and so it would be interesting to go from the basic example to something a little bit more sophisticated actuators may be other more complicated things and see how well that works I would be curious in what the result is matthias Kovich siemens talked about Michael a bit here so they\u0027re different tracks going on so in "
  },
  {
    "startTime": "00:39:39",
    "text": "in the debrief you see level things we look exactly at these interaction models that have partly describing what protocols are used what are basically the abstract yeah operations that you perform there we found kind of this narrow waste of properties actions and events that cover from a programming model perspective basically what you can do and so we can read out some states we can write to some state if this is allowed in this particular ecosystem we can call DC\u0027s actions that often correspond to something that goes on for certain time some others might be RPC based so there\u0027s also this coverage and we can cover whenever a device can as synchronously send something to the client so there we have this disco model in the finger description so there\u0027s a formal model behind the description document itself and this is then extended a bit by the binding templates that Michael was mentioning so in this example here the common protocol was co-opted so we are able to describe the methods that can be used by coop we can describe if there some specific options that I use for the protocol and this is basically a noted down uniformly in the finger scription so we can describe the differences there what I\u0027m most interested in is reading up on some of that experience on when you did that I am like there are always some new standards efforts I don\u0027t I don\u0027t necessarily care about them I want to learn like did those things that you try to accomplish actually work out like where are the problems and maybe maybe you\u0027ve traded those descriptions already yes so for the descriptions a big part what I could explain until now is talk in the w3c document so there are the the drafts out there I\u0027m sure the w3c document will not explain your experience with the hackathon on mapping like way the end to him to the fing description right okay this part explicitly about that would be probably a report from this hackathon which was just the last weekend and I don\u0027t know how much time you had but definitely this is something we also working on so there is another clock fest happening at the debrief you see face to face at the upcoming weekend we also again people from different organizations come there will be some reporting on that and the last thing I just wanted to add because you asked about a semantic level so this is something that goes on top and that is something that we\u0027re currently collecting so it\u0027s looking into ok but in kind of semantic interactions do we have in the different ecosystems and actually listing this down and creating vocabulary but there we need basically yeah I could overview of what is out there maybe activity to point to is the IOT schema.org we\u0027re kind of the the ones that are easy to identify because they occur across all these ecosystems are being collected again in some kind of uniform format that can apply over "
  },
  {
    "startTime": "00:42:41",
    "text": "these different ecosystems are kind of the floor so quickly comment on there like with enzyme experience it was very simple thing that they did so far so for sure we\u0027re gonna be learning more on that how well does it my pics map so right now with the prototype did is that it curious from the management server what kind devices are registered there one of the you are eyes that are exposed and it maps them to think thing that thing description well the next step is to do semantic mapping and then figure out like how much tools how\u0027s vocabulary do we have to get how close we can get types of semantics but it\u0027s a journey so we\u0027ll keep you posted so these are like really good next steps I really appreciate you know where you\u0027re going but before you get to the more complex devices I think it might be useful also to expand out your with a more complex model of the existing devices that is to say Hennis has worked with ace and other things so one of the challenges that when I talked to my partner\u0027s you know this by the way I\u0027m Elliott I work at Cisco the moment we add in an authorization model it gets even with the simplest devices it gets pretty complex and I would I think that\u0027s a great area for discovery I realize I\u0027m saying this without saying I\u0027m gonna do it okay but it\u0027s just we\u0027re going through this now for very simple cases and they\u0027re turning very complex you know you\u0027ve been trying to get something to market it\u0027s very hard so that\u0027s that\u0027s a really good point and what we\u0027re working on a w3c and probably need to look at here also we\u0027re looking at how do we create security models for individual endpoints so you can authorize and learn how to securely communicate with them and we sort of seeing that as a next step but not far behind so yeah you\u0027re using access control entries how do you generate the correct token subject role for our back all of those things we\u0027re trying to work that into the next next phase of design for you for that my summer dive University of Essex I\u0027m wondering how big is the resource directory in addressing directory in here are you looking at the state what sort of scale are we talking in here is it like a local network of things are you talking about what\u0027s the size in here alright I think a thing directory would be like a scalable web service or a local thing that you would have on your router on your land just like a resource directory so is that going to remotely is that going to interact remotely with other thing directories what sort of scale are we looking at here yeah Federation of thing directories we haven\u0027t haven\u0027t really tried to work with but there should be a way to create all services that provide this through micro services or through some Federation of multiple instances of "
  },
  {
    "startTime": "00:45:42",
    "text": "directories so sonna on a more honor so what\u0027s the sort of discovery protocol are we looking it is innovative or is it are you looking at some of existing protocol Sparkle in point okay thank you okay so basically that\u0027s these all really been covered the one thing I wanted to say that we find we\u0027re looking at more diverse models we\u0027re looking at an automotive model and the automotive model is very dependent on being able to understand the feature of interest that the thing is bound to so if I have a door switch the automotive model really requires us to say which door that is and that it\u0027s a door and and that\u0027s something that we don\u0027t have in the IMT schema on top definition model yet but that\u0027s again the next thing we want to add is features of interest in that that basically creates a whole discovery system what do I want to do what which thing do I want to affect and then and then how do I do it I think I\u0027m out of time yeah thank you so as I said this activity is going on so this was just an intermediate Ellis report and not for something completely different turn your screen with us we can see that hello yeah so I need something completely different going to talk about a research project that is being developed in my team right now that allows us to do deep learning on microcontrollers which i think is one of the most exciting products that we can be doing right now my name is Yann oomba I am developer evangelist at arm and one of the first times that I was running into micro or like deep learning in the field was about a year ago and it was here in the University of Technology in Arusha Tanzania and in there I was teaching at a summer school where a bunch of daya scientists mostly mathematics master students and PhDs got together to look at the problems that they had within their society and how they could apply deep learning or machine learning to these problems and I went there as the IOT guy so actually we went to do fields sitacles the conference or six days so during that time with its IT training like how do you build devices how do you where do you store the data what do you do it is and after we actually went into the fields so we went to dairy farms or with the chicken farms actually capturing data that was published in open data sets that a variety of universities then used so a university in Kenya was actually using the samples that we generated during that during that workshop and returning in May very happy about it full day of science Africa we\u0027re gonna "
  },
  {
    "startTime": "00:48:43",
    "text": "do eight days a hundred students three field project three fieldwork projects and again just actually building something and getting this technology in hand with students yes very cool now if I think about machine learning I want to think about is large sets large data centers full of clusters of GPUs doing my training and crunching Facebook posts to resell them to Cambridge analytics guys but if we\u0027re taking one step back to actually a lot of the value of machine learning is added if we can run all these algorithms not in a big data center but rather on very small edge notes so one of the cool projects that we\u0027ve seen in the last year is a sensor fusion where you combine a number of relatively cheap sensors and get data points from all the sensors and see if these these points are not worth that much you can\u0027t infer much from it but if you combine a lot of cheap sensors together you create like this was the guys at gee rats called a super sensor so there\u0027s a there\u0027s a really good video on that link the slides were published after is where they all kinds of stuff that happens in the house like stuff like a faucet turns on they can detect from this very very small group of sensors and that\u0027s all just because of training on a variety of different sensors at the same time in the school another thing is what happens what what Google is doing with their keyboards on androids a technique called federated learning now there\u0027s a there\u0027s a local model running on the Android phones that gets locally trained as well but what you don\u0027t want to do is send raw training data is preferably if I\u0027m if I\u0027m a keyboard manufacturer I want to get data from all my users and get that back and refine my global model then push out to all the Android phones but don\u0027t want to push any raw data into a cloud because there might be all kinds of privacy aspects in there and Plus you know from living on how often I type it costs quite a bit of down plates or just stream it upwards so people is doing is they have a local model they keep refining that and they\u0027re sending in the changes in the weights in the model back to cloud and they use it from all the people to refine their global model so it\u0027s a hybrid model way to do some training locally and I take the learnings that you do in that model and push it upwards and uptake your global model that runs somewhere on the cluster of GPUs another thing is for a bunch of file formats were really good compression if I draw a number I draw a letter then I don\u0027t need to send the whole image I can just say okay well that\u0027s an eight and I just send one byte over line but if we\u0027re going to look at larger data sets or interesting stuff that happens all around us then I don\u0027t have a model that can compress the data straight away so just think we\u0027ll order encoding where "
  },
  {
    "startTime": "00:51:45",
    "text": "you basically have a deep learning model that\u0027s going to keep producing features up until the moment that it has its own compression for Matheny had the model on the outer sides or it can decompress that and basically give you a learned representation of what you try to send it this is a model where the deep learning model itself is going to figure out an encoding scheme for this training very very interesting compression schemes that we didn\u0027t device before for spare very localized sets of data it is very important if you look at low-power wide area networks like sick folks like Laura where your uplink capacity is very limited and sending a compressed version of whatever you trying to send even though it\u0027s very complex problem it\u0027s worth a lot and then the last thing here so this is just this is a photo from from the work week that we did last year in in Tanzania it\u0027s like a flying self-contained systems what if you want to do something in an area where there is no internet if I\u0027m going to deploy a machine learning model on a dairy farm so where it comes in Nia where the average income is under $100 a month to me there\u0027s no internet I\u0027m not going to put down a cluster of GPUs extra do some deep learning some deep learning models there but I am very interested what machine learning can bring to me so one of the things that we wanted to detect is can we see if a cow is in heat at any points based on data that we get straight from the Scout skin and it\u0027s important for the farmers because a semen sample is about two to four dollars and to the four dollars is a lot of money so you only want to put that in if you have the highest chance the cow is actually getting pregnant and there\u0027s a little blog post about that they\u0027re pretty interesting and the other thing is there\u0027s some research and it depends a little bit of what you tried to kind of inferencing you want to do we\u0027re doing it on the edge is actually a lot faster because you don\u0027t need to hold a round trip to the clouds and especially think about LP Wentz might be even harder or actually the energy per inference is lower metric well suitable non once he was doing her PhD internship at arm and she was looking at some inferencing numbers and this was for a very simple environmental monitoring system or shakes he found that the energy per inference for doing it on the edge if no thix relatively longer and more cycles was over ten times more efficient for other stuff like image or image recognition or video or object recognition in videos might be the other way around there are use cases where do you than the edge just more efficient now if we have anything on the edge it\u0027s not going to be a computer it\u0027s not going to be raspberry pi\u0027s gonna be my controller so i assume everyone knows that my controller is okay figured out well i say so just the small they\u0027re cheap and they\u0027re efficient i mean if they don\u0027t do anything they just basically don\u0027t "
  },
  {
    "startTime": "00:54:45",
    "text": "consume any memory either but the downside slow a very limited memory so that prompted us winning an arm were a couple of guys with in arm in our research not a complete research team in or the practical end to come up with a library to do deep learning on these type of devices deep learning on microcontrollers and that\u0027s the the micro transfer projects so micro tensor is an open-source library it\u0027s a budget to licensed specifically made to run tension flow models so what does it you normally train on your GPUs that you can run directly on my controller so it only does it is points classification so no training so what you do is you push the trains model that you train in tensorflow down to the mic controller and after that you can do classification with a couple of really smart tricks we are able to do classification of a lot of machine learning models and under two minutes it\u0027s 6km so that includes stuff like Android and digit recognition will just show in a video but even object classification in and it\u0027s a really good step forward think it\u0027s really cool it dis enables all kinds of incredibly interesting use cases without having to either upstream all your raw data to clouds or do your inference you know a really expensive machine so it\u0027s developed it\u0027s not an arm project per se it\u0027s originated from arm and these two four core contributors as you can see I\u0027m not there Neil Neil was actually in my team he\u0027s the developer evangelist infinite things for a pack I helped it seem a little bit so I wrote the simulator for this who makes a little bit easier to develop it I\u0027m not part the core team also not a machine learning it\u0027s machine data scientist anything that\u0027s very fun it after it so he like hey shit I want to develop on that feel free you know for people in the core team of which none of them is their full-time jobs actually develop this we we definitely work on contributions so just listen get up so here\u0027s a little demonstration and what we\u0027re going to do here is draw something on touchscreen banana for scale and after that it will do it will basically tell you what you\u0027re just drawn on it so I hope it\u0027s physical from the back because I was expecting a little bit larger screen so this is my controller this 200 megahertz MCU and that\u0027s three zoom in and there\u0027s the three and this has three hundred something km and I think ludes drive for touchscreen and there\u0027s a nice so this is a basic handwritten digit recognition running on a microcontroller in less than two minutes 56k of ram we\u0027re seeing school so what are we seeing here is essentially the the hello "
  },
  {
    "startTime": "00:57:47",
    "text": "world of machine learning and this is the Emma Stata set which consists of 60,000 images of handwritten digits all these drawings are downscaled 28 by 28 pixels and then thrown through the through a deep learning model where the supervised learning because with backpropagation and idea there is in supervised learning you try to map a set of inputs to their correct outputs and because we already have the correct output for all those numbers because they\u0027re tagged with the actual number that we expect use back propagation to fix models in the model by actually working backwards so this is trained in tensorflow just on a normal computer and the script is listed there so you can run it yourself and when we do the classification because that is what we doing in the microcontroller not the training we take whatever we just put in on the touchscreen we rotate it so it\u0027s upwards we remove the white space and then we scale it down to 28 - 28 pixels just as we have in the training sets and 28 by 28 pixels gives us 784 in neurons so every pixel is in neuron so it\u0027s downscaled black-and-whites 28 - 28 because it\u0027s a vector of 784 neurons in the end we get an output so it\u0027s a single neuron so we need to work backwards we need to scale 784 neurons down to one just before we have the output we have an output layer which is ten potential outputs because there are 10 numbers 0 to 9 and then we run softmax on it so we only picked one that has the highest probability of actually matching in there then we run so what\u0027s kind of reported is that what we do we jump from the input layer to the hidden layer so all of these neurons are connected on both the input layer and the hidden layer and just before we do a metrics multiplication with nones ratings we don\u0027t set theta to be known for the training set with a little bit bias which also no because we trained this model and they were in an activation function we did it twice we can talk about like the actual details of this later now why is this important um because how are we going to run this on a mic controller this looks hard this game looks cool and so what\u0027s important this is matrix multiplication table because that is what does kind of drive the memory usage of this model so we have on the left side 784 input neurons and you have an hidden layer behind the 28 and an hidden layer of 64 and an output well actually a layer of 10 and an out layer of one holding at all in memory that\u0027s what we want to reduce because for a lot of machine learning models during classification RAM usage is the thing that just explodes so the biggest trick that we\u0027re "
  },
  {
    "startTime": "01:00:48",
    "text": "doing is that we do quantization and quantize it typically what you hold to weights of the neurons as health in a 32 bits floats and that\u0027s completely necessary during training because it\u0027s the only way can probably hold everything have like proper accuracy we figure out and the guys intense flow figured it out as well based on research by Pete worden is that if instead of 32-bit flows use 8 bits integers you have a little bit of loss in accuracy so test against the cypher 10 data set which is a object classification in video feeds about 0.4 percent accuracy was but that drastically reduces the amount of memory that we need with leave four times so tense flow requires deep floating-point D quantization so if you quantify if you quantize you holds you map a floating-point to tune into eight so you hold the minimum value and the maximum value that you might have and then you met with in between tensorflow requires to put it back to a float whenever you\u0027re done with an arrow we worked around that because this is very slow on my controllers that doesn\u0027t have a floating-point units of them this way around this and now we see the current memory usage we\u0027re very aggressive with birching layers so only the if we\u0027re going from the input layer to hidden layer one we don\u0027t let anything else in memory so and then you see that the the metric multiplication table he needs in the first hidden layer is dominating the RAM usage so for this model we see a big memory usage of 98 kilobytes and that is a lot less than 256 K and we can push this even down further with a couple a lot of tricks that we\u0027re doing first of all we have Beijing of memory for larger models so if we see that we can\u0027t fit everything in memory because we have too many neurons at some points we have the message multiplication table doesn\u0027t fit we can page through memory of course sacrifice speed so your kolento Gatien will go slower but this is a way of even running these algorithms on micro tools that have less than on the 28 k memory we put the complete graph in ROM so if you have a tensor flow model there\u0027s just a graph which basically describes all the layers and what to do these layers and where you want to read the data from we pre compile this and put it in a read-only memory so that saves a bunch of wrong basics a smidge of RAM in our case 26 K so it\u0027s great actually and something we\u0027re looking into right now is that we want to take a function whose sparsity of data so it\u0027s going to cost a little bit of accuracy but we might win a little in memory with that not all operated for there what\u0027s the most important ones that are missing right now are convolution and pooling which are necessary for image recognition or object recognition and video files and these operate somewhat work these are working process and now the operators are important because these disable predators in tensorflow so the whole idea of micro tensors that we want to keep parity with what "
  },
  {
    "startTime": "01:03:48",
    "text": "tensorflow is doing so we don\u0027t want to have a new more new way of training your data set because all that happens currently intensive loan that\u0027s that\u0027s really cool there\u0027s a lot of innovation there like there\u0027s startups now that doing JavaScript\u0027s machine learning and I pack everything by tensorflow models so we\u0027ve been a lot by doing that so it\u0027s going pretty soon convolution and pooling and then we can also do image recognition the tensors that we have right now so in tensorflow everything is REM based because I\u0027ll see you renders on a machine that has enough REM a couple of ways is tricking that is a real set flash tensors where the commutes from flash of course at the sacrifice of speeds and the sparse tensors and there\u0027s also Network tensors which I figured why the interesting here people in IETF and so one of the things we can do there is say well we\u0027re going to split part of her data set between devices and those devices might be in a mesh and they can all work together to actually solve this do this classification you think it\u0027s cool it\u0027s something else it\u0027s floating ideas nothing else will implement this but it might be cool that like a hundred very small devices all work together to do a classification of course only if your computation is costs more than actually overhead the against assembled data runs and so the workflow my credenza consists of two tools the CLI and the library so you start with a great set of data you throw it to tensorflow it gives you a protocol file they run the you tensor CLI tools that does two things one it creates the metric multiplication tables from the trend model so that\u0027s your your training your trains model essentially and then it generates a C++ and a PHP file they contain the tensor graph you combine with the microtones lip and then you have something running on a micro solar and they you can put data and get plus quite a tells if you look at and if you guys ever if anyone at U stands fly before but you have tens report which can basically show you your current graph visually represented there and we just put it in codes here on the right and that\u0027s that\u0027s how we put the graph into wrong looking at time developing can be a lot easier we have something called the simulators developed in the same team in also my team so we have the same mes data set here and we don\u0027t want to run it on the device because it\u0027s pretty slow and if you want to refine the way to keep your classification it\u0027s bit annoying as well so we have an all my simulator just runs in the browser and two three and what it does is actually cross compiles the complete model into JavaScript and then runs the complete in inferencing in JavaScript it\u0027s really what very nice way of doing your development something that\u0027s new that has just come out is a bunch of kernel extensions for cortex-m schools the CMS "
  },
  {
    "startTime": "01:06:49",
    "text": "snn developer arm and the whole idea is that it lets the DSP and Cynde functions in your silicon if you\u0027re using cortex m4 cortex m7 or cortex I\u0027m 33 and used it to speed up a bunch of operations that you normally do like for example convolution or raloo so we see you speed up about four to five times which is pretty awesome because stuff that you might think is kind of impossible like object classification in an image becomes viable this way and because it\u0027s hardware acceleration Micro tensor doesn\u0027t use it right now but it will do that\u0027s very very soon so here\u0027s a video of the cortex m7 device running at I think 300 megahertz more or less doing an object recognition so here in the left corner you\u0027ll see the what is currently classifies and in the right top corner really see the data feeds from a webcam and then it gets down sample - I think 32 by 32 pixels and you color image so sees a frog now okay I think is pretty cool so there\u0027s actually running in the cortex m7 on a mic controller doing live up to classification in an inner video feeds that\u0027s it\u0027s pretty amazing so the speed of this it is measured on the cortex m7 running in 260 megahertz with the hundred 32 kilobytes of RAM used still running on the 236 K and at this point because we\u0027re at this point we have enough RAM to give this in so that the speed of the mic controller is actually the limiting parts having a cortex m4 core exam seven is definitely going to help you with use cases that require lots of computation so here we have three convolution layers so before CMS SMN this took about half seconds and now with shims design and introduced we can do this in 100 milliseconds so which is 10 frames 2 seconds analyzing a video feeds on a microcontroller that is 2 and off dollars this is absolutely crazy so recap I\u0027m holding a little bit short because we\u0027re also already over time and how to get started well on why development boards those tokens to do any promotional activity because people in ITF don\u0027t like that but we\u0027ve punched development boards that are compatible here they\u0027re link 20 you tensor mmm profit in the end I think this is really cool like for me when I ran into this my other interests are LP wins you know and sending large sets of data over the line is impossible and this allows me to do actual actual intelligence on the edge or actually intelligence in an automation system where device to talk to each other but where I don\u0027t actually have to push everything up to the cloud I think this is going to there\u0027s going "
  },
  {
    "startTime": "01:09:50",
    "text": "to be shipped in a wide variety of products and it\u0027s definitely has a chance to push the internet of things into the hands of a lot more people so with that forgive me thank you so we have time for about -1 questions but somebody feels a strong urge one that is coming up are you just leaving us no yeah well maybe it\u0027s more Edgar Ramos from Ericsson it\u0027s more like comment that a question the problem I see with this kind of architecture is the same problem that we can see for example with mobile phones and having this kind of many let\u0027s say micro controllers or or equipment that have their own things and then how you can actually address all of them so let\u0027s say you have one algorithm that you want to put in any device and then how you actually do it in such a way that you can port it to many many places so that\u0027s something we have to overcome somehow and it\u0027s in a standardization maybe one way to do it Baltimore state plan a question all right up next I guess yeah I\u0027ll be around so the slides for reading online young young one was gone if you want a toy around with micro tender or with simulator labs Emma does call your browser and go ahead thank you remote presentation so we take control how can I do that yes but he should be able to do that so so we a I am connected I\u0027m trying to see presentation mode No some of the medical team can change that do you know how can I do it because so far I don\u0027t see okay so if the remote this mode doesn\u0027t work we can also saw the slides from here I just have to say "
  },
  {
    "startTime": "01:12:51",
    "text": "when you want on the next slide yeah I think that would work okay go ahead okay good afternoon everyone first of all my upload is because I couldn\u0027t be there for the presentation server have to do it remotely my name is Samir Khan the data I\u0027m a research engineer in eurocom located in Sochi Antipolis in France and in this presentation I\u0027m going to talk about testing semantic interoperability it\u0027s a small industrial extension of European in horizon 2020 project called F and drop and for those who don\u0027t know about your account just in one slide you already see it so we are a graduate school and Research Centre in digital science and we have many academic industrial as well as institutional partners whose of whose logos you see in the screen already next please okay so my presentation would be very brief because we have just started our work and basically reporting what we have identified as gap in semantic interoperability testing what do we propose the two ways and I\u0027m conducting a survey and finally conclude the presentation X okay so as you know the Internet of Things not only requires a smart infrastructure but also deployment of new services capable of supporting multiple skill level and cross-domain applications it is often identified and said that interoperability is the key to achieve the full potential of the IOT market and due to the highly dynamic nature of the IOT this is a strong need of interoperability at the data level and in that case it would be easier for aggregation processing managing and storing data coming from all different IOT devices so recently the many European as well as industrial projects they have identified semantic interoperability as a way to address this problem next so we need basically a way to test a way to test that will validate the semantic compliance and interoperability among different IOT systems and this in order would boost acceptance and adoption of semantic technologies by IOT market and this is particularly important because when I talk to industry a lot of people they tell me that they understand the benefits semantics is bring to IOT but they don\u0027t have a way to test and quantify those things so that\u0027s why in this industrial extension projects we "
  },
  {
    "startTime": "01:15:52",
    "text": "identify distinct semantic interoperability as a quite a gap in the current IOT research and industry initiatives and as mentioned so within the scope of F interrupt projects this sentence incentives we are actually looking into how we can provide some tools requirements and guidelines for testing semantic interoperability next okay so in sentence we propose two types of testing the first one being confirmed semantic conformance testing here we want to test if a piece of semantic data confirm I have to reference unloading any reference ontology but it has to be a reference ontology and in the second type it\u0027s about interoperability testing so it is to check if two parties or two system under tests understand correctly the exchange semantic data next initially when we started working on it we started gathering requirements for both the conformance test as well as the intra public test so for the conformance test we basically clustered everything around three main checks so we should first perform lexical check then go into a lot of syntactical checks and finally doing a semantic checks next so this is a very basic test scenario that you see so we have one system on our test and a tester so the sut basically sends piece of semantic data to the tester and it would in in sequence would perform all the three checks generate a validation report and send the validation report back into SUT next then we came into semantic interoperability testing so here we feel that again three levels of validation is necessary the first one being communication level check so here we basically want to see that the message sent from the first system should be received completely and correctly by a second system then we have a lexical or format level check here at the message issued from the first system should be incorrect and understandable format for the second system for death further data processing and the third level is the data processing level check next said I tried to represent a very basic scenario using two system under tests here and in in this particular test our objective is to test the semantics crossing results in two I mean from the two systems so consider a suti "
  },
  {
    "startTime": "01:18:53",
    "text": "one it sends a piece of somatic data let\u0027s call it D 1 2 SOT - and the sot 2 runs a query q1 aspartyl query Q 1 on top of D 1 and then s UT 2 sends back the result R 1 as well as the query Q 1 - a 31 and the system under test one it would execute the same query Q 1 on the same data I can send before and let\u0027s say the result this time is our own complement so su t1 would actually verify if both r1 and r1 complement are the same if they are same or equivalent we would say we have 100 percent probability otherwise would say it these two system under tests are not semantically interoperable next now since you come and our partner in the project is a global market so we both work as well as follow the 1 M 2 M standards we want to map the interactions and the system under test in the previous slide with one and two entities so in this presentation you see that we have here Lisa re I\u0027m getting some echo okay so in this slide you see that we have basically identified the application entity as well as common service entity of the 1m tomb architecture and wish for the same set of operations and we divide them with respect to the application in the D and the common service entity so it\u0027s basically the same representation of the slide then with respect to Valen to them in and dislike that next now I think starts to get a little bit more interesting when we introduce additional computational components between the system under tests so let\u0027s say we have a very server which is generating the query q1 and it is sending the same query both to in 51 and two and then the system under test they execute the query 1 on their semantic data and return the results to the query server and the query server is going to compare our run and r2 and check if they\u0027re same it\u0027s the same then we have achieved interoperability otherwise we haven\u0027t next so the next test we are doing is to see how to achieve interoperability at the data level in the in this particular case so we extend what you saw in the last slide into this side so we have two "
  },
  {
    "startTime": "01:21:55",
    "text": "system alertness and the third party estra so a suit so here our objective is to take the data check if they share the same vocabulary so when a suti one sends the semantic data do you want to the tester so the tester would retrieve the vocabulary v1 from the semantic data it receives and it would do the same thing for D 2 coming from AC to do so it would retrieve the book ability to and then compare v1 and v2 and the comparison result would be sent both to both of the system under test and if we see that V 1 and V 2 are identical then they are said to be interoperable a completely otherwise they are not integral next then we also can think of a little bit more complicated scenario for data level interoperability testing and the main motivation of imagining are considering this type of scenario is that we have to since we are building an industrial extension of the Avenger of platform so we would like to use what it if enter of offer offers and extend it to additional components the comparison server that you see in the center of the slide we can think of bringing this to both of them next okay so that was almost resonance because basically the first steps er that we have took so far to define the different tests and the scenarios architectures different data that\u0027s going originating from one system under test 1 to 2 if we have additional components in between now we are also doing one service so I would request all of you to take note 5 minutes and complete this survey re has obvious already done it and so I request all of you to do it "
  },
  {
    "startTime": "01:24:55",
    "text": "it\u0027s gonna help this project a lot and if you see I have missed any requirements or did not consider any requirement feel free to drop a note there as well next so in a nutshell my presentation is about how can we test mantel interoperability because it\u0027s highly necessary for different LED systems and in this project we have proposed two types of stresses and we proposed some scenarios to implement the semantic performance and interpreted yes so I think I have a pipe of there and as of future work so we are actually implementing this test within the a pin drop platforms we are currently seeing how we can align the system under test with respect to what a pin drop have has as of now and how we can integrate our the system under test as well as the tester and the software components into the center of pressure and will soon be reporting our results pretty soon next so this is our acknowledgments in synthesis is an industrial extension of the F internal project which I received funding from European Union H turn 20 project and some tests is a joint collaborative project between easy global market and York um so we both are located very close to HC if you have visited so beautifully so you definitely know that\u0027s all if you have any question you can ask me or read if you are coming to the the opt-ins meeting so we can meet and discuss personally and please take the service it it\u0027s going to help a lot so with that I would say thank you very much for the opportunity to speak here and thanks for your listening thank you sue Muller for doing this so quickly so I was you for the hackathon over the weekend but had to go back and presentation everywhere so thank you do we have any questions again we have time for about not none but okay Thank You Silvia and we\u0027ll go on to the next presentation okay thank you a cursor bye-bye bring the dog yes okay thank you um and some input and payment that is "
  },
  {
    "startTime": "01:28:11",
    "text": "willing to pay for the computation and then the executing note enter input and then provide without selecting enter verify that the result is correct and then in the entrance perform the payment item right picture so right now this is done in the cloud so there\u0027s Microsoft Amazon at the computations pallets and resources and get back the result however we have I think anyone running it at the end has several benefits in terms of like cost and also privacy so in our efficient we want to have a mesh at nodes so even if I have my laptop I have some spicy PU I can just accept some tasks run locally and device for the execution however it changes kind of the the trust management because Google or Amazon will just keep them our data and in fact they\u0027re not here it\u0027s like untrusted knowledge with the security and privacy must be deal in the system design so we cannot just trust that the other part is not going to misbehave we have to be sure that it\u0027s not gonna happen also we want to have this open system when everyone can join so no betting system if you divide since you\u0027re just drawing and execute stuff we want to have incentives so the notes are rewarded for the work and acting times so cosine the rewards of course if I have my laptop and I\u0027m running from I\u0027m using my CPU to run completion for others I wanted to reward for that so it should be also the main driving point for joining the system because in my gt-r system that was the problem there was no incentive so if you consider torrents everyone else to get files but then why would I share my like to use my bandwidth to give to other people however it is for paying I would like to verify the result so be sure that I didn\u0027t just random link of bytes and there\u0027s also this problem when if the payment should be done because if I am requesting now I don\u0027t want to pay in advance first and execute a stroke and just take the money and walk away and the same applies from the execution now so you don\u0027t just walk away without pain so the result cache is also very crucial part of the system so we can have different types of tasks so we can proof of work "
  },
  {
    "startTime": "01:31:12",
    "text": "types of tasks but it\u0027s very easy to to check if the task buzzer was correct or not so it just compared to get hash however usually means we have to recompute the whole task and then compare it the result is the same we can generate prospects your knowledge across to posted correct however usually it applies very high cost sometimes it\u0027s even much higher than the computation itself and also it\u0027s not available for every single computation another solution would be to send the task to many people that we get back the result we compare them they\u0027re the same there\u0027s a high term that it was the result is correct however it\u0027s highly inefficient you have to fight for every computation and also have to prevent colluding because if to execute from collude they can just send send me the same random result and then I will no idea that it happened so then we also submit some input and we get some result and we want to make it private but not only private from them for the network but also from the execution platform and this is difficult because the execution platform us to actually compute something on top of our data so in the cards usually now we use encryption however again and it reduces quite a lot of overhead and it\u0027s not always possible the other solution would be to use trusted execution environments which basically allows us to write to have trusted Enclave on untrusted nodes however they require dedicated Hardware so now I just filled in our system so fast is Intel SGX so it\u0027s an example of this trusted execution environment so basically what it does it allows us to create protected part of memory that is an animal running on top of that so then only this process can read read and write to this part of memory it\u0027s invisible and encrypted from the operating system world hypervisor I can check first of all this on the remote host I have the valid internet yes but for then I can verify that the function running on the remote host is the function I requested and in the end I call so secure communication channel so I can communicate directly with the function kind of bypassing the the execution node and we also leverage hariom block time so we built on smart contracts that allowed to add some logic on top of a blockchain in our case it\u0027s Italian with solidity Turing complete language "
  },
  {
    "startTime": "01:34:14",
    "text": "however everything that we submit to the doctrine is publicly visible so it would be very helpful with these are a question yeah just a request to speak Alicia shows it to the microphone okay okay is it better okay sorry the problem with smart contracts is that first of all the execute and second of all there is quite a long delay because if we submit the transaction then we need to wait till it\u0027s compact so problem also use payment channels so Batemans channels allows us to run transactions off chain which is that much it is like almost no cost and no delay in processing so there are secured by deposits run on top of blockchain however all the communication is of blockchain and the last component are Oracle\u0027s so smart contracts by themselves they have no la lgion or they cannot communicate with the external world so we have like virtual machine and Oracle\u0027s can act as trusted data feeds so if I want to query a trusted HTTP HTTP server I can do it with an Oracle and then they die god is valid so then the ovary of the system just the assumptions we have requesting now than executing node they both distrust one another however they trust the function so if I call this function I know that it\u0027s gonna do image processing or DD rendering however both you know they trust the blockchain and also we assumed that execute enough has complete control over its system operating system hyper hyper visor and so on there\u0027s going to be a lot of so here requesting now some input on the top on the bottom we have the execution that and on the right web block change running with smart contracts so first of all and the execution part some brands tank light enter now the the requesting node can establish this secure communication with the with their function enter and identity to generate the proof enter and send it back to the to the requesting now and now we know that it\u0027s secure to send our input to the Sudan cliff and again this input that we sent is now secured from the from the executing platform so it\u0027s only visible for the function and so now after some time the language will will compute a result and at this point the "
  },
  {
    "startTime": "01:37:17",
    "text": "execution path some generates the secret sorry actually so this key isn\u0027t transferred to the Enclave and the Enclave will use this key to to encrypt the result and so done in the same credit result is sent back to the food executioner so now we have the result however it\u0027s encrypted enter we also send a hash of the scheme that we use encrypt the result enter and both the result and hash have at the station from the Envoy so we\u0027re sure that they are correct and so now the execution note is send again a transaction with the payment photo executing node in platform however it can be unlocked only if we submit this the secret key and can verified it using this hash enter so now if the executing node submits the secrets to the blockchain it will unlock the payment enter and at the same time the key the key becomes visible so the so the client gets the secret and use it to unlock for the result and so here it works however we have two problems so first of all it can be pretty slow and expensive to do to everything on them on the blockchain and second of all I can always send some computation to the executing node wait until it executes my stuff and then just walk away so this kind of denial of service attack answer so to deal with this problem we here introduce payment channels so now sending this money and unlocking is she cheap has almost no cost and it\u0027s also very fast so we can so can divide our ties into very very very well into a lot of small tasks into everything chunk by chunk enter so here this whole process is the same however we don\u0027t require any cost for the transaction do it very quickly so now even if I submit a chunk of work I don\u0027t know 10 seconds of CPU and just walk away the execution platform uses only 10 seconds of its time and not more and and to make it perfectly secure the problem is in this communication so in this channel of data between the repeating note and the Enclave because it\u0027s secured and no one knows what\u0027s happened so if the communication fails the smart contract or no one else is not able to say if it\u0027s because of the network failure or because the execution doesn\u0027t didn\u0027t want to compute anything almighty just the requester walked away and so basically we can either sum "
  },
  {
    "startTime": "01:40:20",
    "text": "everything to the blockchain and then everyone will know but of course it applies big cost and in our solution we decided to go with ipfs so the result if there is a problem so the the requesting on just walked away the ability to put the result on the on the ipfs then the smart contract can use Oracle to query it and they purify the file and unlock the payment for the execution possible so to conclude automatic payments it is fully automated and all the lower overhead and system the state-of-the-art with no third parties involved however have those limitations because so it must be revised and currently the limits of the application is up to 100 megabytes is the time for short question um I was wondering whether you did a comparison with some of the others of computing solutions that are out there did you did you do a sort of a comparison between your approach and di some other edge computing platforms out there like this as you know this work on open fork like Microsoft Amazon we and others have sort of an edge computing solution and so I was wondering whether you have done a comparison no it is a [Music] but the result is correct but obviously they have different characteristics but their pros and cons are so that would be that would be interesting yeah the workshop papers it was yeah to be published but that\u0027s but you mean the comparison in terms of in general the solution which you have worked on an edge computing solution as you explained and once with one specific set of characteristics in but there are other edge computing solutions out there that people worked on and they have different characteristics I would be curious on how they compare to each other yeah so we were before writing the paper we\u0027re looking for systems that have like the same characteristic you didn\u0027t find any okay then maybe broaden the scope yeah comparison like the other solutions don\u0027t use don\u0027t have blockchain that may or may not be a good thing sure I have my own opinion about that by them but they have other interesting characteristics that your solution may not have so maybe the merge that would be chopper mail to the list I would be interested to read that thank you "
  },
  {
    "startTime": "01:43:21",
    "text": "so I have one more slide just to have some open questions that we still have so if we assume that Network and we see it as a global computer we will probably need something more efficient to dispatch tasks because now kind of up to the nodes to negotiate the price and stuff like that however if you want to have model that will just dispatch tasks automatically between nodes and I think it\u0027s difficult because then if the payment is involved how do we define kindness in this case especially if we have like different priceless loads and not capacities also how to estimate the cost of the computations in terms of CPU bandwidth memory quality of service knowing again that we are very heterogeneous environment and how to provide full privacy because in this case we will protect input and the result however we still know who involved which function so this is also preferable to hide it as well so I think that\u0027s all thank you more questions okay thank you so I think this was interesting also for people who thought that TLS was complicated I think we\u0027re going to see some some really interesting security constructions to make things work in an environment where we can trust this and this I think it\u0027s just an interesting example for that and when you want to be centralized you of course the nonono always know who you are talking so the last item on the agenda is a quick peek at the meetings can you I have to yes why does that hey Dave do you want to come to like Dave Eric so mine is not about the agenda per se - about the logistics of getting there the meeting is tomorrow "
  },
  {
    "startTime": "01:46:21",
    "text": "afternoon in Prague and I know several people are on the 7:20 a.m. flights out of he throw so my question is as far as carpooling since this is before like Heathrow Express runs and so on I\u0027m organizing a car can I get a count of how many people would want to travel together we would be leaving the hotel lobby at about 4:20 a.m. tomorrow okay if you\u0027d like to carpool and come together in a shared car please raise your hand so I can count one two three anybody else here ok you should need to give the the the concierge account for how big of a car to get so so far I see three in this room if you know of anybody else let me know because after this meeting I will go and get the car booked ok good Chris just short comment I think the first teacher Express leaves at 5:10 but there\u0027s the Heathrow connected which actually leaves earlier so but I guess it depends on when you want to be at the airport yeah the concierge recommend it for travel to Prague that you be at the airport like two hours in advance and it\u0027s a one-hour trip so that\u0027s why you recommended for a 24 car departure or 7:10 and 4:10 for the car departure or so yeah ok so what we wanted to do is give you just a quick peek into the kind of things that we are doing at these coordination meetings so this is the first time that we actually no it\u0027s not actually not the first time it\u0027s another time that we actually having 33cm and LCIF in one room together with a research group and of course we\u0027re going to do status updates between each other we will also talk about Vichy again but there are a few issues that really aren\u0027t standardized yet and so for instance one issue that comes up repeatedly is that when you actually request some action from an IOT device how do you know what data you actually need to send with that action so that the energy device can do the right thing so this is a slightly different problem from simply requesting data you can request those data that think about them and so on but if you actually want the light to be green the loudspeaker to be loud or something like that how do you actually specify this in a way that allows interoperability so that\u0027s one thing we\u0027re going to talk about more generally we will talk about the model interoperability some things we tested in the hackathon and yeah "
  },
  {
    "startTime": "01:49:22",
    "text": "what\u0027s the next step to make sure the various models that are popular right now in to operate the episode or that way the model is relatively simple and straightforward pretty easy to translate three descriptions very powerful ocf has this Remlinger based modeling schemes maybe you put these things together another thing again the protocol level is how do we model the various forms of pushing data so we know publish/subscribe is a pretty popular way to model the edge but also his problem problems and would be interesting to see whether yeah additional experience of rest beyond what we already having card which is the observed function would be used for a tool to better handle these push models so there\u0027s actually a research group draft for non-traditional responses and we want to discuss how use what that would be we want to talk about the area of data that you will actually install or obtained from an IOT device about its relationships to other devices so how can you actually find the light switch to a light how do you describe the interfaces how do you build the links and how do the resource representations look like and then of course we have housekeeping on the base document like new response codes talking about the resource directory there are different ideas on how to use that and so on and finally we are going to talk about ace in the security authorization working group within the ITF and how that could be useful for the ocf which have their own security our model already in place and they also have interesting authorization formats at Salomon but talked about but one interesting question is how do we get multi-point security in so somebody sent me a link to the fair hair white paper the fails security white paper which nicely describes how they are going to do all the security and then there is this section seven there which and where is security I think that\u0027s really something that we need to do some work so this is the sanitation and finally of course we all benefit from the availability of "
  },
  {
    "startTime": "01:52:22",
    "text": "reference implementations and from test cases so we want to discuss how we get to a better situation with color--the particular property CPC well and other standards that we are using so this and however we want to go in one afternoon that will be here as background program but yeah we can always affect our things to telephone part later on so this is so next meeting I already said a little bit about two other meetings going on and I\u0027m very certainly we\u0027re going to have a summary meeting too much car so if you don\u0027t follow this working very closely you can still come to the summary rating and find out what thank you for coming and thank you and if you haven\u0027t yet signed the bullshit please sign them before you leave Thanks [Music] "
  }
]