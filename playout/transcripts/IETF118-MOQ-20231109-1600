[
  {
    "startTime": "00:00:01",
    "text": "Let it go over to my games. O like like like like could you read the transcript Check. It fits on. This is muffin It's pretty crowded, folks. So why don't you squeeze in and make room for the other folks? Can't get you all in one photo. What working group is going against us right now? Cause this has never been this late blah. Dinner. I wanna chair working. I believe the flying a home working group was happening very vibrant meeting right now. With us a minute. This this MOU session will feature a hot live demo. Have you ever really looked at your hand what's in the copy?"
  },
  {
    "startTime": "00:02:00",
    "text": "You're getting them. You're you're you're going rogue. Alright. Now the question, how many video codecs can we use in 3 laptops The answer so far is 17. Okay. Do you want me to share the slides and go. Okay. Will that kick them up? Yes. Alright. Hello, everyone, and welcome session 2 of media over quick on Thursday evening. Thank you for staying. We promise it'll be worth it. So here, we have the IETF note well. It being Thursday, Has anyone attended a session in every block all week? And has seen this sixteen times. Alright. Cratchers along the back. This is important. The standards by which we agree to do business here with covering important things like patents, code of conduct, etcetera. Please read and abide by them. Meeting tips. We think you've mostly figured it out. You can, you can, join the chat, join the queue, make sure you you speak into the mic. Etcetera. We did day 1 already. We have a scribe. Thank you, Ali. You got the notes open okay? Okay. Great. If anybody needs, something relayed in chat somebody volunteer to relay things at the mic. Okay. Ted will do that. Agenda, bash 28. Rebash. So, day 2. So we're gonna do we're gonna walk through"
  },
  {
    "startTime": "00:04:03",
    "text": "with the results of this week's, interop event WER and give, implementers a chance to talk a little bit about their implementations. Etcetera. And there's a you may have already gotten a preview of the demo that you're gonna see of mock Technology in action, I think, we're gonna maybe split from what's on the slide here, and we wanna cover issues. We're gonna cover some announced issues first, then let Will talk about catalog and work in CMF Packaging, and then circle back to parameter reform And then it's time permits. Did I get that Right? Okay. Does anyone wish to bash the agenda. You that's what I said. And that so, yeah, it'll be Interop, announce, catalog, Seatwarp and CMAF, parameters as temperamental. And we have at least one as time permits. It's actually Okay. Slides but Okay. Will says that it's mostly catalog, but he will use all 30 minutes. For ish Okay. Anybody any further bashing? Okay. Okay. Interoperate now. I will see if I can stop those and share. And if you are an implementer and you put slides in the want to come talk about your implementation, come close to your life. Or Where did it go? And I'll start Can we see it? We're good. Okay. So, we got together. I think there were 6 different implementations you're gonna hear more about them later. What we've done is set up an, Google Sheet spreadsheet that shows all the clients and all the servers, which you'll see on the next page, And then if if you manage to achieve a certain level of interop, one of these test cases, you"
  },
  {
    "startTime": "00:06:02",
    "text": "add that number to yourself, or maybe we'll go some people wanna read bikeshed and use letters instead of numbers, right now, it's numbers, the more interrupt cases you get, the darker your cell turns and then you know, if we're doing really well on the current draft, it'll it'll change to a very dark hue. So covering setup, subscribe, subscribe, okay, receiving objects, announce, announce, okay, Hints unsubscribe, go away and then also a signal of whether you can do do raw quick or not since that is not a common thing yet. Okay. So this is the this is where we got to this week. So, those are the 6 implementations. And, if you go down the Agonal, that is the implementation talking to itself. So, fairly good level of interop. People are able to, like, write applications. That's good. And we did get some implementation stalking to each other. And, several people got at least as far as 4, which is exchanging objects. Or maybe that was 3. One. 1. 3. Sorry. At least 3, which is exchanging objects. Which is exciting. When you get your first object, just to cover a couple of, notable results. So between some folks may have already seen Jordy's, browser based live streaming, and publisher and viewer, so we're able to use that, but through my relay, using totally different code. And we were able to get a a nice news video stream. He's probably talking about that. Luke has, for a long time in streaming things that quit that video, but he's up updated that to, draft01. And then, the Cisco team, has essentially gotten their VC call infrastructure work, and they're able to do a VC call using mock graph 1, which was published 2 weeks ago. So I just wanna, like, highlight that we've got lots of stuff good stuff going on. So I'll just talk briefly about my implementation,"
  },
  {
    "startTime": "00:08:02",
    "text": "which I'm calling Meta Mach for now. It's written in C plus plus. In only sports web transport, at the moment, it's based on, our proxy and HTTP library and the move fast quick library. I've got clients. I've got servers. I've got relays. I've got it all. The relay, is just generic. You just announced and and subscribe, and it just matches announces and announces and subscribers. Together and deduplicates, update ups upstream, subscribers. I implemented, an app location, called, which we came up called the mock clock. So it just publishes the time once a second and using groups and objects. So you can do deltas in the group, like it publishes the current time, at the beginning of every minute, and then every second, it just publishes the second. It'll support subscribe hints going back to 1970 so you can historically asked for what what time it was back then. I also have a chat, which is client server only. It doesn't work through relays yet. And I have what a sort of the curl equivalent of mock, which is like a client. So you can just point that at any track. You just give it a server name and a track name, and it will pull it down and just subscribe to So what I have. Jordy, you wanna just say, we're come to the mic and talk about your input at all or it's alphabetical. So I forget who's next. Okay. So thank you. So my implementation is just, publish that it's a lithium color, that runs in a browser. It's a pure JavaScript implementation that uses web codex to encode the video and audio. And users were transported to, basically talk to, to quick and and the MOU library is just the JavaScript code. And then the player is just the same, but the opposite side uses web codex to decode the video. It uses web, web transfer API. Get the messages from the and it's a quite it does a quite complex algorithm be the alignment, algorithm. That's it."
  },
  {
    "startTime": "00:10:03",
    "text": "Cool. Thanks, Rudy. Luke. Next alphabetically, Yeah. I wrote something that's in rust. It, it's been up for a while. This has been, I've been working on some as side for maybe 6 months now. So there's a my main problem was getting rid of a lot of my forks to try and get interrupt going. But got mostly there. But, yeah, it's it's streaming's video. You can watch quick that video. And there's also a TypeScript implementation I think that's the next slide. And, Although, I think I'm gonna writing that in Rust just because it's gonna be more fun. But, same as Joury. It's WebCodix. Works pretty well. Cool. Thanks. Oh, Martin? Wait. Math oh, q r s t. Yeah. Martin before math is. That did it right. I wouldn't. I shall say Victor written a bunch of code that's relevant to this right. It's a little less developed, but we had a lead for, like, 6 hours, then we're blown away on on the cheap chart, but, just web transport right now, just a client just doing, Alan's chat. And and I just wanna actually make a pitch for Alan's chat. What it's Mark Janet. Alright. Whatever. It's it's, it it yeah. Like, it's cheesy, you know, I'm doing it because I'm lazy and can do it on a command line, but also like, it's also like a simple version of a, like, a VTC model, which is like, in our side discussions, it's already come up with a lot of issues the way it subscribes and announces work that are not come from, like, regular video consumption. So would encourage you to, like, work on that. Yeah. So we have chat. We're like, we're like, I'm implementing messages 1 by 1 so I can do, like, an ounce and and subscribe and set up and, we'll get objects done very shortly. Actually, a server should roll out soon, you know, Victor's"
  },
  {
    "startTime": "00:12:00",
    "text": "push some code that we need to merge in So, yay. Cool. I see Lucas here in the queue. Did you have quick question for someone who wants to go through the rest, the rest Lucas. Maintain on cloudflies, Kish. This a commitment, Martin? About that. So, Ian, Ian has stated in the asked an intent to fix this. And so, like, a promise with a with an unbounded like like like like, satisfaction date, is well, I actually wanna Yeah. It someday. Cool. Alright. Thanks, Martin. I I just I did what originally added everyone to the slide averaged their first name instead of an in in name implementation. So I assume Martin was referring to. The maybe the name of the mock library, not the quick library. Anyway, go ahead, Nathas. Yeah. So my interpretation is, written in Go. It supports quick and web transport on top of quick go and web transport go, go I have a client and a server. You can find the code in the pitch up repository that's linked link up there. I have a chat application client and server. And just before this session, I tried connect to Alan's chat, and I could at least get it set up and the first object and then we had some catalog issues, but I think we'll figure out that one too soon too. And I made a little clock application that works. I think similar to Addens, which just publishes objects every second, And, yeah, then there's a gstream or video streaming application, which is not completely up to date to the latest version of the, library of the transport library, that just integrates go and, publishes a video stream it's very simple at the moment, and I will extend that in the future. Alright. Cool. Thanks. Alright. Sue us."
  },
  {
    "startTime": "00:14:01",
    "text": "Please. Do I need I need to stop? To need to go this a screen share and not have any administration, to present I didn't see yet. Should be wedding comments Okay. I get Oh, there we go. Okay. Make sure you grab a mic. Okay. So can people see colon there? So this is a live, a two way video call happening. If you want to re the real latency of 150 milliseconds you should see on my laptop. Metacode is just adding more latency. Have to go to mock as well. So that's the thing. So this is kind of hands. Start next yeah, the black, you know, the Yeah. The Mead echo and the projector system are not helping our cause year, but yes. Why don't you go ahead and talk a little bit about I gotta you out of this, even Okay. You stop sharing your screen? And status. Lost track of where we were. Cool. That's the video call, we showed going from here to London and back on Akamai server. That Will was, happy to share with us. And we got, 60 milliseconds is the latency that we measured. Next slide, please. So, Fikar, we've been working within Cisco. There's a small team of,"
  },
  {
    "startTime": "00:16:03",
    "text": "enthusiastic developers who believe in this technology, and we've been working on for a while. So as part of this week's hackathon, we wanted to kind of adopt it to kind of use mock. So this is Z plus plus based client, and relay. It uses raw quick, based on Christian's, pick up quick. And as endpoints, what we have is the MAC client, which can also build an iOS client, but Mac client, which is sending H 264 or 1080p, and 720p@2different, similar cash resolutions. We have 48 kilohertz scopus. And for transport modes, we provide 2 today. 1 we wanted to play with extremes they hear, one is the streamer track, which is audio and video on the two videos, you'll get 3 tracks totally. And also 3 streams and datagram is using the fake datagrams there. And on relay implementation, we do have a object caching. And we also mark objects. It's futile for exploration if the object is cannot be served within the real time constraints of what we define the objects gets dropped. That's how the holes get created, and we also have priorities. That way we know, HD is probably slightly less important than the 720p for, for example. As a catalog server, we have conference error that us basic catalog, which is not the catalog that, we're working in the IETF yet, but it easily convert to that one. And in terms of what features from mock 01 that we support are announced flavors have announced, all flavors of subscribe, and set up. And what we are missing or deviating is we don't it support, mock catalog and go up a message and subscribe hints. And one deviating thing that we deviate the standard is that right now, most of the video calls, we don't need the originals, publisher to wait for a subscribe to come through because the publishers are coming across the world they don't even I don't, having a suspect come to them to publish acts up join latency. We want to get to the point of 0 join latency. That's that's the hope. And that's something we would like to bring it to working at some point because it solves a reduced case. That's it from Cisco. Great."
  },
  {
    "startTime": "00:18:03",
    "text": "I just wanna say, we had a lot of fun all the implementers. I think we had a great time hacking this week. We so much progress was made. And, it's just super exciting. And so thanks everybody for your hard work. And, you know, let this keep us energized through the months ahead where we have to just talk about issues. Okay. Next, we wanted to do announce. Yeah. Okay. Announce issue. Yeah, that's fine. Okay. Still not shared. It's is it where you go and you meet it? Oh, you want me to say, share it or you should? Sorry. I start, you can Okay. Next slide. There's only 4 issues, right now that are tagged announced. So just put together some slides trying to summarize the discussion and the problems that are in. And if there's solutions there, try to present them, So, there are 2 related issues about conflicting announces. There's one about conflicting announces across producers. And there's another one about reconnecting with the same name. And if you if you read through the issues, the pictures are trying to show the difference. So when they say across producers, I think they're talking about 2 different clients that are both talking to a relay and they want to announce the same name space And they both, but they are different, they have different authentication credentials. They are representing different entities, and they both want to do that. And if you send a subscribe to that relay, it will not know what to do."
  },
  {
    "startTime": "00:20:00",
    "text": "It does not know where it's wrapped. We need to decide how we wanna handle that case. The other one is talking more about the same client. So, the the which could, in this case, crash and reconnect, or there's also not pictured the make before break scenario, where for some reason, I don't know, ran out of quick stream and hit them max quick stream ID limit or something, and it wants to reconnect and keep things live. So for it, it may want to announce a second time while there's still an active announced there. So any questions on the scenarios that we're trying to deal with? Okay. Next slide. Oh, yeah. Go back, maybe. Sorry. I didn't get in the queue correctly, but the I think on the the on this first scenario there too. I think it's important to include that client 1 and client 2 are not publishing the same object. They're publishing different groups are on object object names. Oh, interesting. Yeah. Okay. Now I'm not sure if that was captured in the, issue or not. And I'm sorry if I Or maybe we you know, you can discuss both cases where, but I think that that's an important distinction as we dig into this little problem. I think the way that I would design that I I recognize that we've created a lot of new constructs in name spaces and names and objects and groups that that application design using those constructs is still evolving. I think I'm what I might do there is extend to the namespace. So if client 1 wanted to publish track 1, and and clients who wanted to publish track 2, you could make their name spaces be different. In some way, but, like, add another differentiator in the namespace and then you wouldn't have this problem. If that makes sense. Because our current announced message does allow you to announce anything name, only the name space name. And if you look at the mock chat design, where I've done some namespace design, and I've sort of thought about and how the how I can get the analysis rather the way I want or the subscribes, and you"
  },
  {
    "startTime": "00:22:01",
    "text": "you you need to think a little bit about what your name spaces are Well, Yeah. I just we we might brush aside this use case where client 1 inclined 2 are publishing identical objects, and he say, well, way would that happen. But that's a that's what I spent my afternoons pulling me away from my IETF today is redundant publishers for high priority events, like the Super Bowl You don't, you're not gonna trust one client, right? You're gonna have 2 encoders that are time synced producing content. We need to build in to the standard. Some ability with agreement that they can both publish binary equivalent versions of content and somewhere you can announce it twice. And a relay says, well, if I can't get it from the 1 or something's weird or it's timed out, can go to the other. I think that's important if we want this to be used in production. That is a great point that I had not thought of, but I think before you leave, well, can I ask you a clarifying question? When you say binary equivalent, I assume the codex and everything are the same, but if they're different encoders is it always gonna be the case that you're expecting input streams going into the encoders that to be exactly the same. Yeah. Input's probably STI with study markers or something like that. By, instinct. We don't need to worry about Symantec equivalents where they've got 2 different feeds. That's completely different. We're just talking about There's nuances here. It's a higher requirement, but if they're binary equivalent, that's one case. If they're kind of close to each other and we could switch over in a failure situation and not fail completely, with a much lower requirement, for synchronization between them. It's also a practical use case that we should consider. Because clearly the second one would be the same name space, but not a binding. Same space, but probably, like, backup primary, but at some point in the network, saying these are alternate one another Okay. Thank you. Okay. I I see the the queue growing, so we'll we'll go through it. But I think that it's a"
  },
  {
    "startTime": "00:24:03",
    "text": "an option, I think, I'm not sure this was discussed on the issue, but I think it should be a valid option to have relays that allow more than one announcement, keep a priority list. The question is is that something that needs to be standardized, or is it something that we can have it be, you know, implementation configuration, So think about that. Inspect Google. I just want to support that that's actually an important use case. And, I mean, we've done that multiple times for multiple events. And I had as many as 4, like, feeds. And they are sometimes, like, 2 of them are that was identical to each other and the other 2 are different. Like, there's a whole number use cases, but, like, at least preparing the bit wise identical, but 2 publishers for redundancy is is actually an important use case. Think it should be down at the transport layer. I think doing it at a higher layer. Adds complexity because it's not so dissimilar from the crash case. So I have locked the queue, but that's just because we haven't gotten off the the question slide. I'll unlock it when we get to the proposed answer slide. Okay. I, Jordan, made a I also supported the use case. I don't know. So I agree with Alan. I don't know if they in the transport layer or above, But definitely, having a a feature that can switch between two streams. I think it's a it's very important. Also, I would be extremely careful in, when I say binary that it's not always true. Because 2 same fits, the they can be equivalent chunks, but not binary equal. Some just just just okay. Thanks. Suhas He's after the law. Oh, you see us as after the lock. Okay. Great. Okay. Move to the next slide. Here are some options we about it and you can reopen the so, these options don't have to be mutually These are some choices we can make. So one, you can say something like the last announce always wins. We can just say, tell people if you don't want 2 clients to show up with an advertise the same namespace. You can do that via how you issue authorization tokens. And if you if you"
  },
  {
    "startTime": "00:26:02",
    "text": "run into problems because two people are showing up saying they both are authoritative for the same content. You didn't want that, then you shouldn't have done that. We can also just make it an error and or added an error that when two people announced the same you try to announce something, someone else shows up for us to announce the same thing, you to say, I'm sorry. There's a conflict, and you can't your announce is gonna fail. And And that may have implications for how make before break works. But I think I definitely think there's an option for here, which is you just allow a list and of some kind and this relay just gets to pick. Among them, either in some proprietary way, or we need to define a way to give the relay information about Hannah select among them. So Talk amongst yourselves. I'm Come on. Is So I would propose a a 4th option here, which is allow it. So assume that they're both author assuming they're both have ballot authorizations. And by the way, I'm not necessarily tied to it. There's a one to one that every the idea that every namespace has a different credential. I'm not fully body but it has a valid credential for that namespace. How are the credentials work? Well, that's a different set of issues. But we have a both if both clients have a valid credential for the name spaces, you know, there, you assume the application knows what it's doing and if one client publishes object 1, and the other client publishes object 2, people who are subscribed Get object 1 and get object 2. And if both clients publish object 3, this is where I get sideways on this. Then I'm in the all bets are off. Who knows what happens? Some everybody will get 1 of those 2 of 3. And if they're the same, that's fine. If they're not the same, all bets are off. I ask a clarifying question about before any objects come anywhere, the relay would have to issue subscribes."
  },
  {
    "startTime": "00:28:02",
    "text": "I actually don't know. Okay. So that See us told me that's not how it works. Let's just say they do. Not say they do would are you saying the relay would my clarifying question. Would it transcribes to both? Would it transcribes to both? I mean, I think if it really wants The question is how much of do we need to have normative language and standards that defines a way for all relays to handle this. No. We've gotta be very clear on which one of these coming down. Like, if somebody building an application has to know what the relays choose between any of these options we have to do. We have to say what the relay network as a whole. Does. We don't have to define how relays are built, but we have to say what its characteristics are. Right? And, I mean, you, you know, maybe you could have some way of negotiating what type of relay it was or something like that. I'm not arguing, but I'm just, I'm throwing out there that you know, allow it is actually a fairly valid option. We certainly could design that if we want Okay. Thanks. I mean, obviously, our current implementation works that way, but I'm not I'm not it's a different question of, like, which one we should have, Alright. Great. Will. Yeah. I would actually have option 4, which is first authorized announced wins. Right? So the first one in 2. Right? Yeah. I am millisecond. Says we're authorized. The second one says, gets an error. That's that's option 3. Is Announce error namespace con okay. So I would go with that I like Cullen's idea for because that would give primary and backup. They're both valid, and they could both say, hey, I'm here. But I really worry about what you do with the subscription because then both clients should then start producing, and now I go to dedupe, but the relay and I think that's confusing. So if we can figure out Colin wants to write that relay, I guess it's He can't. He's been here. D duping at the relay turns out to be insanely trivial. Okay, because you already have a cash on the relay. And you're like, I already have this object? Drop it on the floor or when when the when when 2 objects arrive, right, And that's why I said very clearly if both people publish opt both sides publish objects, that are different. Okay. The dedooping algorithm is going to drop 1 of them on the floor."
  },
  {
    "startTime": "00:30:01",
    "text": "But if they were publishing up to different relays, different relays due to timing conventions and how the data flow through the who through the relay network may results in different things winning. So what what I'm saying, I'm I've always viewed this as a name data networking problem, right? So I'm saying that for a given object, If 2 clients publish the same object with different data, there's no guarantees about which any of the subscribers will get other than each subscriber will only get one copy I think we're screwed if they do that. Yeah. That's not like you're screwed if you do that. That's nuts. That's crazy talk. Right? But the idea of deduping is trivial because it's just like, do I have it in my cache? If so, I already have it. I qualify that? It's trivial if they're the same relay. But if I want redundancy, I'm gonna have one relay in New York City and another one in Boston, Boston, And now my Dadoop has to work across those 2. And I don't think that's trivial. That becomes time and distance. Okay. That's Do you wanna go back to the queue or you have a quick response? Yeah, quick response. Totally. That's what I was trying to say. As long as they publish the same data, that's both of them are you're going to go to 2 different relays. Definition, you're trying to do redundancies, right, And those are those are going to flow through some way that your relay mesh work. Which is out of spec of this document. Right? And different relays are going to end up let let me call these 2 copies primary and back Right? Different relays are going to end up with primary or backup, but any given relay that ever gets copy copy can trivially deduce to whichever one it got first for whatever timing mechanism that used for getting one first. If it never gets 1, it will get filled in from the other relay network. Everybody's sort of doing that cash and the insubscribers will do that. So The in the clients that that relate they got a copy from the primary or backup, they'll never really know, and it won't really matter, and it might change for different clients different things. But as long as the same you have the same data on the primary backup, or at least you don't care which copy you got because it's so close and the video codec stuff that might not be bit wise exact, but the whole GOP sequence is fine no matter which one you"
  },
  {
    "startTime": "00:32:00",
    "text": "like, this is a pretty easy I I think when you if you go if people implementing relays go back think about how their relay implementations work. You've come to like, oh, actually, that doesn't really change my relay implementation at all. Okay. So I will point out that we have now chewed through the first half hour we've gotten through less than one issue. So, we gotta speed it up. There are a lot of people in queue. Got Ian Suhas, Victor, Luke, Luke, please make it crisp And, we're gonna have to figure out how to get to the second issue possibly with no Okay. Yeah. I think we should just allow, duplicates and, was gonna suggest you throw a number in there and that, like, like, a tie breaker, and it's, like, GGP or something. Right? Like if they're equal, then you can choose or whatever, but like want a preference order, let those application choose how to do that and just throw a bar into there and let but I don't really care. But, I think we should need to allow What was the last thing you said? Hello? We need to allow. Allow more than 1. Yes. Okay. Sorry. I mean, speak to that. Okay. I I agree. We need to allow analysis duplicate analysis, and that, for example, from the implementation we do video in our relay. There's, like, 3 lines of code change. And also in our relay network that we that we have a missed fear. We do get duplicates from different parts. The same filings of code will apply there as well. And this provides the use Will was talking about and also the make before break use case. Okay. So it's fine. I think I'm just hearing I mean, I think I heard Will say probably good to define this error. And if if some really wants to throw it, it can, but also we should totally not forget or rec we don't require this error to be thrown. And if you're really handle than one announced, then you can handle more than one announced, and it's up to figure out how to make it work. Right. Right. If if the if the authorization allows, definitely allow it. If the Relay policy allows that, you know, you cannot allow 2 name spaces, allowing us in a totally application and relay decision, but at the more transport level, It just allows it. Okay. Who's next? Victor."
  },
  {
    "startTime": "00:34:08",
    "text": "So saying that I was going to point out this by where it wrote the chat. It's it's their art two aspects here. One is when you're a publisher and you announce what kind of behavior can we expect. And the other is how do you as really, approach the problem of the back fur of billing your cash. And we should do some. I I think both option an option for our valid behaviors for the release. And option 4 can be more complex, but can also give you better reliability properties. But from perspective, for clients, the most important thing is One is for similar in the sense that when you reconnect, you just re announce an works, but one of free are dissimilar in sense that you can get the name space conflict when you reconnect and now you're you know, like, you don't know what to do. You reconnect and you have one application level to just replace your the name space. So, I I think option 1 or for our preferable to option frames at regard and, that's what matters for the public shows. Okay. Thanks. Luke. Last word. Last word. Yeah. I I think it's really important that they're the 2 announces are bit wise identical. There's another case here that I don't think we're talking about, which is when, the publisher crashes, and starts a new instance that has not been wise in in a call and re announces the same name space. That shouldn't be treated as you know, like transferable as in the previous announced needs to go away. I I almost I think I think, Victor proposes at one point, but I'd like to see a token you announce you send a token and if you send that same token,"
  },
  {
    "startTime": "00:36:00",
    "text": "the second analysis treated as binary equivalent, and then the relay can choose which path it wants to use. But if you want to make a new toast, a new session, a new broadcast, and it's really, it's fine to error or or pick whichever one then. Okay. So I need to thank you in the interest of time. We will just something from the chatter, do you want to relay to him? So in the interest of, what would we do for the next thing? It sounded like there was some interest in seeing at least this is permitted at the mock layer. And if it's permitted at the mock layer, but the application wants to disallow it what a relay wants to disallow it. For policy reasons, then we need facilities for that. And it sounds like the 2 facilities we would need is error code to say, sorry, you lose and a mechanism to decide what you lose. How it determines who loses. What's currently on there for options is last announced wins. Several people have suggested replacing that with a token because that would then also allow you to distinguish between the case of, a newer Sorry. A replacement from the original publisher, even though it's on a different instance. So my proposal is that we don't disallow this at the Mach layer. For the next draft, right, 1. Not permanently bid. For the next draft, we don't disallow this as a mock layer. We do add the announce error code and a token. And then, the token can be either used as a tie breaker in the same way BGP uses tokens or, for this replacement functionality Is there anybody who's going to fall on a sword if that's the next draft? Seeing no sharp objects? Let's move on. Yeah. Okay. Sounds good. And, yeah, we can continue discussing an issue in PR. Okay. Oh my gosh, sweet. Is this one gonna go quickly? Maybe it'll go quickly. Does relay matching behavior need to be negotiated band. Maybe this has been overtaken by events. I think I wrote these slides before Monday. So, draft 1 is a little bit vague about how a relay is supposed to match, subscribe, and announce"
  },
  {
    "startTime": "00:38:03",
    "text": "And, the question, the options I thought were we can continue to get big, can mandate a behavior, which is that you match the, which I think is maybe where we ended up Monday. We matched the exact bits in the namespace. And that's how you know we're going. We can define something like longest prefix or concatenations off the table now. Can do something else, or we could make multiple behaviors and negotiate between them, but maybe this is OBE. Ian? It's about Google. Yeah. I suggest we go with 2a to align our with our decision on Monday and I I would go with hits statement unless someone's gonna die on that sort, Let's just do it and see how it works. And they wanna die on a sword right now? Seeing them. For option 2a, Make. Colin, go back. 10 I'm not I I so we agreed on Monday we were gonna do bit wise compare Right. Right? Theme the current behavior then for the next one will be bid wise compare, which is 2a. And that we're not revisiting that to add renegotiation because of this issue. That's the conclusion. We treated as OBE by the previous decision to do that was compared. Right. Right. So we don't need this. Yes. It will basically then that's I I agree with the conclusion. I got confused on what you guys were proposing. So proposal is 2a. It was the draft will say, a relay will take the namespace from subscribe, match it against table with bitwise compare from this, you know, the answers and find the one with a bit wise name space that matches and route the subscribes there. Or if there's more than 1, see previous issue. It it it figures out of its own. Way to figure out Okay. Great. Okay. Last issue, and then we can move on to something else. Subscribe, rejecting, announce. I think this one loop filed, but listed a few different use cases. So right now when you announce, You gotta announce okay. An announce error. But if you get an announce, okay, and then sometime later, the, relay decides that doesn't, it doesn't, it doesn't, it doesn't, it doesn't It's gonna drop your announced out of the table."
  },
  {
    "startTime": "00:40:03",
    "text": "Do you we need to add another message to do this? So there are some use cases here, like, off has changed and you want them to reconnect and revalidate with Nuance, another publisher came in, so you a couple issues ago. Or like, for moderation or something. Okay. So should we add some other message, to let the publisher know that its analysis dropped out of the table. Look And I'm just gonna say this is mostly for, like, telling OBS Yeah. that it's not publishing anything anymore. I don't think there's any way of doing that right now. Using the connection What about unsubscribe? Nice to sell OPS as an error your your announce is no longer valid. Luke, can I clarify it? That's in a case where wherever OBS is pointed has never subscribed. Because otherwise, unsubscribe would tell it Well, unsubscribe doesn't have an error code. True. Anymore. So an unsubscribe could be you know, through no fault of the the publisher, This is telling the publisher, like, you thought you were authorized to send this and you thought you were publishing twitch. Tv/kixrelated. But you're not anymore. You know, do something about it. Okay. And, and you and closing the session is too drastic. Well, the idea is that you could be publishing multiple broadcast. That's the whole point of an Right? And one of them might be closed, but the other one's not. Okay. Alright. So you'd think we need this message. gonna assume yes. Okay. I I'm I'm gonna plus 1. Yeah. I mean, they're probably too. I think we need the best best. Okay. I I I think we should just have a sort of general design principle here, which is when we have a message that creates state on the other side, The other side has some way of saying, I'm blowing that state away, and here's may and here's some hints of what you could do about it. If there's anything that's useful to recover from it,"
  },
  {
    "startTime": "00:42:03",
    "text": "And I prefer not to use closed session as the result for every single error because you just don't get enough information about what's going on. It's not really recoverable. So I think that should just be a sort of general level design principle, for dealing with subscribes, and announces because they both create state assignment. Sorted. Giving them. Alright. Sounds good. Okay. More people in the queue. If anybody wants to get in and say, we don't need this message. Wilson. So to comment on your point about the tying announced and subscribe together, there are decoupled announces I'm I'm a source for this, but I'm not gonna do anything till I get a subscribe. So your point about, well, if you get a subscriber, it really doesn't matter it's like you've said I'm a source. The other sides agree, and now the other side's unagreeing from that. So you don't have to stand by to publish. Yeah. I see. Okay. Anybody else? No. Okay. So, yes, we need this needs PR. I think that's it. Yes. I think then, Will, you might be up. Read up this is catalog format, right? Yeah. But I uploaded 3 of them. It's the last one. Did you put V3 on any of them? No. Go to the next slide, and I can tell you if it's the latest this is not the latest. No. It's got V3 in the actual file name. Okay. So Is it they all be going to admin, upload manage slides. It it is a go up I saw it. Up up on this list here. There's a B3."
  },
  {
    "startTime": "00:44:03",
    "text": "Well, while we're getting the slide, so we're switching subject over to the common catalog format now. Which is the next level of interop up for those who choose to use it. It's not required at all for mock transport. But we're trying to make it a common basis for the streaming formats that will sit on top of mock transport Here we go. Are you It's not a Okay. That's good. Thank you. So what I wanna cover in the time today, there's 2 PRs. I wanna go over them. Explain what they are. We and debate whether we should have Yep. Yep. Yep. Accept them or not, and then followed by 3 different proposals. Let's get a next slide. So the first one here is something that came in from Mike Misha proposal to adopt a Jason patch or merge as an alternate. So in the initial version of catalog update, we described our, a custom ability to update the JSON state of the file. And Mike pointed out that there's 2 RFCs we could use for this, Jason Patch, Jason Merge. They listed there. Patch, is basically a very simple list of instructions, operations that perform against the file. It does reference arrays by indices, which by think is a weakness, but I'll I'll I'll, like, I'll show you it on a different slide. There's multiple libraries available for this. So we have a deterministic way of it behavior we can expect Jason merge patch kind of similar. It basically creates a dev file."
  },
  {
    "startTime": "00:46:00",
    "text": "You can't change keys to null, but more important for me is the array elements cannot be manipulated. To resupply the entire And in our catalog, the tracks are described in an array what we might wanna do is have many tracks and only change one of them. So I really don't like the Jason merge weakness on that part. So I wrote a PR implementing Jason Patch. Next slide. So what would it look like? On the left is what it looks like currently. This is a delta update. You got a big fat catalog. You just wanna change a little part of it. In this case, we wanna add a slide track to a video conference. So on the left hand side is how we do it today. The right hand side is what it would look like with Jason Packs. There's an operation that says, air, and there's a the tracks. We're putting it in as the last element. That's what the dash means. And then we give it the objects that we wanna place in there. So two things to note. First is that there's no more sequence or parents number. This was another parallel request. We can rely on the object sequence number that's there, and we rely on reliable delivery of a catalogs It does mean that we with the prior system, we could have a a tree based relationship. We didn't have to. Have a delta from the immediate prior object. I think in most cases, that's probably what we want anyway. So that it, by removing sequence and parent numbers, we make our updates slightly smaller. But we, we lose the ability to fork the the patching. But that's what it would look like, over As a client would receive it, Next slide. And here's what it looks like removing 3 tracks. So on the left is what we currently, we actually reference the tracks by name, audio, video, and slides. On the right hand side, you reference it by indices so you can remove the the third one, the second one the first one. It's 0th based. But if you read the spec, it also says that when you remove an item, from the beginning, it should bump the other 2 up. So in fact, the same operation, you can just go track 0, track 0, track 0, you'll get to the same state."
  },
  {
    "startTime": "00:48:02",
    "text": "I I don't like that as much as explicit referencing But there there isn't in a a name based match for an element inside Jason Patch. So so It's it doesn't mean it doesn't work. The same producer of the original cata dogs making a patch, so it just keeps track of the indices a software problem. It's not anything other than that. Next slide. Woah. Okay. Let's go back. So before we get a next issue, just, Chris, are there any immediate objections to this or improvements? Should we use JSON Patch, basically? Yes or no? To us. I'm not saying yes or no, but I think, without an experiment, experience in implementing even the catalog. It's very hard to save just some patch makes big difference that if if it's it's improving something I I can, I can see why we should not pick it up? But for a couple of libraries that I we used they don't have this support either. A few libraries, yes, but not the common libraries that were used in our code base. So I'm I I would like, if, my proposal would be should wait on this before we decide. Come with an experience for an exchange drop and see if it makes sense. But not against a good idea or bad idea. It's just But when you say we I I don't think we can defer this. We do you are you saying that we go with the current patch update mechanism described in the catalog. For next for implementation. That that would what I would type as stuff. Okay? Inserted myself in the queues and individual, but to save time, I'm not gonna run up there, which is just to say I I tend to have preference to reuse stuff. And so I kinda like the other one better, but I'm also not super opinionated. Betsy."
  },
  {
    "startTime": "00:50:00",
    "text": "Were just talking about a case where 2 different publishers could be like updating to the server. Right? So is there a case where they might like, both of them are trying to update the the catalog because they're trying to add different tracks and they might lose track of what index a track is, and they accidentally removed the wrong wrong track from the catalog later on. Yeah. I think you raised a very good issue. We would we would just just we always build these things, assuming I'm on a a single source producer, but we just raised the case if I've got redundant producers and what happened to my catalog updates. I would want, in that case, I would want a fresh catalog. So catalog has a as a group boundary, I would want the new as as soon as the producer realizes the primary, it should update a a xx independent catalog, right, stop doing delta updates because it doesn't really know or trust the prior state So then we'll have to have a mechanism for the producers to notify it. Yeah. I think that's an a consequence of that. The producer might want to know that it's now primary And that's difficult too. So maybe we should also think about a mechanism of what are if there are 2 updates? Is it just a race first one wins and you get into some weird condition with your indices, I think that's very possible. So, I'm gonna lock your queue in a moment. So if you're planning on having an opinion on this, please, Luke. Luke. Yeah, just for use existing standards. Like, I I do like the the what we have more, but at the same time, it's, like, not invented here. Thing IRC. Get Mike? Like? Yeah. So I've opened this, issue. So I am in favor of reusing the existing RFC. I do have a question though. Which would be"
  },
  {
    "startTime": "00:52:02",
    "text": "how much do we lose in omitting the sequence number and do we maybe want to nest the patch like, within some other structure. So the actual operations are defined by this existing standard, but we could have, you know, additional metadata on top of that. So I I originally was a fan of number. That's why I put it in. There were some detractors, though, so whoever ask for sequence number to be removed if they wanna speak up it's a warrant. It's not a big thing. And it gives flexibility. There's a queue that's over there. Oh, there is. Oh, there is. That was Mike. Colin and Lucer in the care. So, hold on. I mean, I I'm I'm sort of okay. I I fall I'm I'm in this thing is Luke a little bit of not invented here, like, there's some stuff to do this. We can use it. I do know with Jason patching that, you know, one of the great things about is every time everyone's tried it, it hasn't really worked. And that's why we keep getting a new way of doing it each time we try to do which does give me a little bit pause for thought. But I'm sort of thing that's really bothering me is like, Do we need this at all? Like, a patch is a compression technology because something is too big. In this sort of case. And it's not a complexity gain here. So I we just and I thought that we had these hierarchical different track so that you could arrange your catalog so that you didn't have to write out a huge, huge catalog each time you updated a part of So I think I'm not not knowing enough about how complicated catalogs are in the traditional streamed media world. Okay. I take this with a huge grain of salt. I would just ask the question, do we do we need this? Does really help us. And if there's a case where we really need it, then, yeah, I'm I'm I'm in. It doesn't matter that my case doesn't need it. I'm in for it, but"
  },
  {
    "startTime": "00:54:01",
    "text": "do wonder whether we really need it, or we could just send the full catalog every time we update So can I answer that quickly? Because I think, yes, you don't have to use Delta Uplink. So if your catalog is of a size, where you would just happily reissue it every single time there's a change. Then you do that. But there are conceivable use cases where I'm describing a 1000 tracks, and I'm changing to I really don't wanna repeat the 998 every single time. And that's a use case. I could be updating a, 3 d world and I have to describe vertices all over the place or different tracks. It's it's I think we need to create we need to allow for it and allow an efficient mechanism to do But if your use case doesn't need it, then by all means, Don't use it at all. Issued discreet independent. Catalog updates, I see. I was thinking that I was gonna have to implement it even if I didn't use it. But I guess your point at my application never uses. I Your application knows your producer will never produce it, then yes. But if in a streaming format, if if if we say it should be supported for streaming format, you're gonna have to let me just back up. I said I didn't understand the use cases that drove this, that there was good ones that people thought like we needed compression, then I support it. That's where I am. I working, doing it. Okay. Okay. As a conclude, I was asking not to use martial metaphors anymore, so I'm gonna ask it this way. Is there anybody who, will have an allergic reaction if Jason gets folded into the mocks to Oh, yeah. Definitely. I'm always allergic to cases, but I can live with Okay. So, allergic, but not anaphylactic. Okay. Good to know. So, at this point, it's your description because we'll merge it. Move on. Next slide. Okay. Number 2, again, this is Colin's not gonna like this one, but You don't need to use it. This is for those who want to use common encryption and c which is a lot of there's a big industry out there today. Currently, we have no encryption DRM textion, description. So the original post to be 28 says, how do we provide this?"
  },
  {
    "startTime": "00:56:02",
    "text": "To provide this, I'm proposing the following fields be added to the catalog. We don't need to read through all the text right now, but these are the same fields that we use in HLS and dash today. To supply encryption parameters or at least what I believe we do. So I'm what I'm looking for is people who actually use the on a daily basis for DRM to say, No. This is insufficient or unnecessary. Victor, do you wanna comment on on this slide before I go to the next one? Oh, is this a slide for the same issue? Yeah. I think I've got a few slides for this issue. Okay. So go back one slide. To the picture. Yeah. So what I'm adding is a new top level root array. Right now, we just have track array or an array of catalogs. We're adding a new one, which is a content protection array. It holds a bunch of content protection elements. These are all optional. Doesn't have to be there, but if it's there, this is what it looks like. Next slide. And we also to make it efficient because one thing I hate about dash is this very verbose repeating of of PSC's H Blobs. So on the left hand side, we've got a textion, array, red, green, and blue. And we got tracks that use some combination of red green and blue. Like, maybe the blue's white wine and one of them, 2 of them don't use white wine. So on the right hand side, you can declare your content protection, elements the root, and then they're inherited by all the tracks. And if the track read finds its content protection, then it overwrites that inheritance. So we can be clean and efficient about information, some of which can be verbose. For the PIGs and things like that. And here's an example actually implemented. The text might be a bit small, but this is a catalog with common encryption DRM info. You can see on the left"
  },
  {
    "startTime": "00:58:02",
    "text": "side, I got the new content protection array, and it's got a bunch of content protection entries. And then on the right hand side, In the purple right at the top, it's it that's the root. Right? It it's saying, hey. Every common protection entry has an ID, and it's just saying that ID is 1, 2, and 3. Apply to all the treks. And then the very first treks says, no, I disagree with that I'm only under prediction 12, but XD does and protection, 1, 2, and 3, all the other tracks inherited. So I think this is a pretty concise way of applying content protection that is common encryption compatible, to our catalog spec. We have a comment from the chat, from James think the key system specific fields should be replaced with some more open fields that are agnostic Also, is this using the dash list of UUIDs for key system IDENT should should that be in a n an an INA registry. So I think this is specific to C Map, though. Right? So Yeah. I would so clearly, my example, like, cut and paste out of a dash file that I happen to have but but but but but but but but but but but but I don't I'm not sure that the, scheme I the scheme ID is generic. It doesn't Bjash's sister field called scheme ID. So if these fields, I I agree we should have generic fields. Right? The streaming format can map explicit fields over to these generic ones. So the goal should be that these fields are as generic as possible. And I think key ID scheme ID may be certainly PRO for the for the Microsoft 1. That's very play ready specific. So we should just have a extended field DRM field number 1, and in the spec says if it's play ready, you put your probe blob in there. So I I would support that, actually. Okay. Let's go to queue. Wait. Let's go to final slide because it's got explicit questions that maybe people can answer. So, yeah, all these sufficient to describe it"
  },
  {
    "startTime": "01:00:02",
    "text": "should they be added to the common catalog or defined separately in a streaming format spec? What does lock need? We wanna we would ideally have elements so so generic that other container formats can apply their own content protection. And is there a cleaner way to do this? Those are my questions. So is for Kyiv. Victor. Yeah. I guess my My comment would be mostly answer question 2 and the answer is specifically for if specifically for DRAM, don't believe it should be in the common catalog since they're it's a very specific use case in like instead of all media of our quick use cases, and most of them don't require a constant protection. That said, we shoot our like, inheritance model for tracks and fields should be sufficiently generics at something like this. Would be able to be embedded and just work without the, like, in the additional effort and like what they mean by that says which you have enough extensibility as it should be put in and we should make sure that Now, there might be some rule, some room in the common catalog for, general purpose encryption So you can define how you encrypt and that would be on top of whatever can paintery truth since that is entirely plausible. But one, I am not sure that there's actually would meet requirements of CRM, awful team around to send to It it is unclear to me that this is the correct layer compared to the container. It's Okay. Look, yeah, just one of the second everything Victor says. This is a really good, use case for actually just testing how extensible, the common catalog,"
  },
  {
    "startTime": "01:02:00",
    "text": "format is I it should be totally feasible to define this in like a CMF specific draft. Without really making a super all DRM schemes must use this in the future. In the common catalog. Thing. So, yeah, we should do it. I just I just that's what draft this goes into. Okay. Think I shared my concerns with this earlier with you, Will. The the things that are very specific to DRM, I would assume that would be either a catalog extension draft for CMF. If if that's what it would be or it could be in the container or the streaming format draft, But if you want to define something very specific, a generic way to say, this is the encryption scheme that we're using. And and if you want to learn more about this in encryption scheme, go to the extension spec should be okay. Speak to a one particular use case. Is no more no more common. Good. Good. I know the queues close, but I'm going to add a comment like the chairs I might care about for a second on this one. But most before you. So, Moe, I agree with Victor that this probably doesn't belong in a common catalog. And I think it's a good exercise to figure how we do the streaming format specific extensions and how those are separated from the common common catalog don't see this being used outside of CNAF. Whatever we do, let's make sure we don't need a normative reference to the ISG can't get because that always causes approval problems. So I think We've heard pretty consistent feedback that this should not be part of catalog draft that we should make sure there's an extensibility and kind of rough graph that a separate streaming format, can create can it can normatively create elements that the streaming format should use within common catalog So I'm actually fine with that. And what I'll do is I'll I'll drop this PR and I'll go back and I'll make up a a"
  },
  {
    "startTime": "01:04:03",
    "text": "sample use case with extensibility to see if we are actually extensible enough to carry this next slide. So PR34, the one got a lot more approval. This is allowing relative track names and inherited namespace. So The current catalog as it stands says you must put the namespace in there must be declared. Because it's pretty central to our discovery mechanism and our routing. However, this introduces the notion that it's optional and that if it's absent, the tracks in the catalog inherit the nay the namespace of the cat flow, because the catalog is itself a trick. And this provides some very nice encapsulation features, I believe. And it's something I strongly advocate we support. So I gave an example they're, you just video and you inherit the namespace of catalog, which is visible to the client and also to the relay. Next slide. Okay. I didn't have a question slide. So I'll just go back to that slide. Any any objections to align relative track names. I'm sorry. I I couldn't hear. You actually don't have a match there. And several people have asked if that's intentional or mistake. It's almost certainly a tie a typo Sorry. Yeah. This was late at night. It's meant to be a match. No. Should be premier gaming Just a clarification question. So, we do the client subscribe, the we should say that the last catalog piece should be removed when you construct in the track name. The string catalog from that? Yes. That has to be specified in the graph. If not, yep, they take the entire thing and slash audio. So the catalog draft says that the reserve name of the catalog track is catalog"
  },
  {
    "startTime": "01:06:02",
    "text": "So it knows what to remove. Streaming formats, I think free to overwrite that. With their own reserve name, but either way, It should be clear what is the track name of the catalog. Because it might be gamer 34 slash catalog in in in that should be different. Sorry. Clarifying question then. So up from the full track name, I mean, I I took this as when you wherever the catalog was, It's in some namespace. I don't really know what that name space is. And let's not make any assumptions about the slashes or anything else. We know what the names are. Those are. That's tuples or tuples. Right. Yeah. Okay. There's some to pull there. That you have some knowledge of via some mechanism. You just inherit it. Yeah. Okay. We got thumbs up. Next one. So I'll merge that. Bitrate definition. This question came in from Ali. So right now, we say bitrate, right? There's It's a murky swamp. When every we have a number called the trait. If you look at what I put in what HLS defines, put in what dash defines and put in what web codex defines. So they're all sort of ambiguous What I think we should do is have 2 placeholders, average betrayed, and peak bitrate. Those would at least give you some idea, but is already shaking his head. Some maybe we we just have bitrate And then it's application defined, but there was, you know, We've had problems with this in the past if you've got highly variable betrayed. And do we, at this point, want to take the time to describe or at least communicate the variability that's inherent in the track. I just wanna say we've we've got delta updates. We can actually do better than HLS and dash, and we can up the bit rate periodically with the with the catalog update"
  },
  {
    "startTime": "01:08:03",
    "text": "because that's one of the issues with HLS and dash is you have to specify up front, I will never send higher than this. Because I don't know what it's gonna be. But I think we can actually avoid this and make it a lot easier. And actually support VBR because HLS and dash don't support VBR as a result. It would mean that your Delta update period would need to be smaller than the rate at your your bit rates varying. Which would be a pretty fast update. But you're right. It's we can do that. Hero. Carol Miller. Just a kind of from experience. Right? So on metal side, we have bunch of different bit rates. Right? So there's max betrays. There's also average bet rate and also or what window that ever is betrayed is. So there's I I don't know if you want, like, we won't list of those victories. Well, that's the question. You know, based on your experience, what would you like to do here to minimize few Japan. I don't want to I want to be flexible. Right? So because I don't want to restrict okay, just publish average the trade. Right? So I won't play tomorrow, we will find, like, okay, there is a new we or that there is better way to compute the betray, right, so, of course, type of So maybe using something else. Right? So, like, I don't want I I don't want to specify what it what it is necessarily. Okay. So to be clear, are you calling you're calling for more fields, but not the fields proposed here? I I'm not sure if we Need to specify the Can it be like just can we can we not sound like custom field? Like, a Yes. So the catalog allows custom field. Maybe we do, like, you have a bit rate that's it. Then you can have a what are you com.meta.com.customcustomvbr measure and"
  },
  {
    "startTime": "01:10:01",
    "text": "but then why would you have been, like, just betrayed field. At all at all. I guess you could. Well, we gotta draw a line. The whole thing can be custom. Right? Because we want a some point, people wanna write a parser that generally works for stuff. So we should we should pick the subset of fields that most application is gonna use and those are part of the catalog standard. They're they're optional. Right? You don't have to use them. But if you do use them should be a defined definition of what what the value is and how it's calculated. So I'd be fine with single bitrate. And then let's do used custom fields until everyone's using the same custom field, and then we We folded in. Something like that. Colin. I think to have interoperability, which is the point here, you can't really have custom fields. You do need find this. Now you might define later, better fields with more detailed sort of measurement of various types of more advanced clients might use, but I think we do need to nail this. And what I was laughing, I mean, like, we do need some sort of that there's many systems have defined this many ways at different level precision. I think the most important and and spent incredible amount of time arguing about which about what this is all mean. I think we should point out one of the existing definitions Right? And and say what it is. I think that we do need the average and the max thing. I will point out the max bit rate of a server with a 400 gig card and it is always 400 gigs. Right? So it's like finding how you measure max is in is always has that element time to it and variability and, like, You can see how that gets really complicated fast. Right? So this is the encoded bit rate, not the throughput to be clear. Median coding bitrate. Yeah. I I understand what's the but, you know, the that you you can, you know, is that Anyway, per frame. You know what I mean? Like, the timeline that you're measuring on that important. So there's all kinds of definitions. We just need I think that we should this is Like, what matters is that we"
  },
  {
    "startTime": "01:12:01",
    "text": "all that our clients understand which one they're using. And so we should assign Ali to come back to us with what is the best one for us to He he knows both worlds. He's seen all of this. He's laughing at me already. He's like, there's no because I tried that and He's like, there's no right answer to this question. That's what we need. The guy who knows there's no right answer should bring us back the best worst answer. And we should go with it. Okay. Just, remind people that we're actually pretty tight on time. We've got Ali, Jonathan, and Moe, and I've locked the queue. So please be brief. So I'll just also, are we getting close to bike shed territory? This. Rat hole. This is a rat hole, none of it. Yeah. I think I'm getting a feedback that we we need more discussion that we, yeah, So there's no clear pretty short. I think this is one really exact use case for personally that they did track if you are really worried about your bitrate variation over time, you can certainly signal this in a separate metadata track, which actually what we are doing in Dutch now. So, You know, I mean, we hit certainly hit these values in the manifest. But those are not sufficient for Yeah. Enough in Yeah. Draftonics. Just in case things weren't complicated enough, in so far as we have objects within a track that are discardable by the relay. We wanna be able to specify the bandwidth both of the you know, without the discardable and with the discardable so you can know you know, you know, what you can successfully squeeze into the bet the bandwidth you have available. That's a good point. I think we should keep this catalog very simple. First of all, the relocating this. So this is not something that would be relevant for really processing This is really for the endpoints and for the endpoints. Like I said, there's a lot more metadata that's needed than just this. So I think keeping the catalog simple with a simple max bitrate. It's well defined. You know, sliding window over one second. Never exceeds the network bandwidth never exceeds this bitrate. Over a sliding window of one second."
  },
  {
    "startTime": "01:14:00",
    "text": "A traditional definition, Adding anything else is not useful for the person consuming this. It's not gonna be sufficient for the ones that really need very tight control over per frame level bit allocations Sounds good. Okay. So, one thing, I did hear several people suggest that there be a metadata track for updates to, things like this. Somebody wants to produce a PR about that, that would be separate from this bid rate definition. Please, find out. If you got a next slide, we'll see what I think a metadata track looks like, sort of, this is different. I don't know. We have time for how much time. We just have a few minutes. Yeah, we're Okay. I'm gonna whose issue was this So this was a suggestion that we put catalog fields in an INA registry and not Let's skip this one. Yeah. Okay. Okay. Let's go to this next one. This is how does a client learn about group numbers? Right? We're all focused on the live edge where you you sort of don't care. You care you get the latest thing. But as we go into sports media and other objects, We have a timeline, a 5 hour window, and I wanna go back and watch part of it. I need to discover the relationship between group number and object number and media time, and So I'm proposing that there's a, a notion of a timeline track And it's Well, firstly, There's the use cases of why we need this information. And I list them there. There's also no ways to achieve this. We can use the catalog and update it But this list could have 3000, 4000,5000 we we don't want it. I don't think baked into every catalog. We can use a template mechanism so the client can predict a deterministic relationship between these. And that's you can do it, but it's very inflexible for other formats. And then the 3rd option is adding a timeline track that clients have described to if and when,"
  },
  {
    "startTime": "01:16:02",
    "text": "they want to understand the history of group numbers and time. If you go to next slide, it would be no more complex than I'm showing it in Jason. It wouldn't be it would be a binary compacted file that is just listing group number, o'clock time and media PTS So a client could start playback by subscribing just to the last object of this life age. This is itself, a track that has Delta updates. But if it needed to build a a scrub line or need to build its UI or needed to go back, it would go and grab the whole timeline track. The moment it didn't need it, it would stop subscribing to it. So I think the notion of a timeline track is interesting. I'm just looking for feedback on that. Okay. Cuseful this is for warp streaming And format, by the way, mock transport doesn't care. It's just a track. It delivers it like any other. So we're talking out key attributes of a streaming format now. Yeah. Perhaps you you will need also DDS. Depending because on on the wire, the frames are loaded at, DPS. Depending on the group, if the group is a go, probably you won't need it. If the group is a frame, you probably will need just saying that Okay. Probably adding that DTS. Do you need TTS and DTS? Sex, it it doesn't seem like, probably necessary to specify something like this because I think different, applications they wanna implement there their timelines in different ways. So I don't really see a need to have a standard for this. Because applications may choose to encode their timelines and Demid choose not to need it. They may be parsing the object headers anyway and understanding the actual PTS and seeing the object group mappings from that. Is no way an application can know about older group numbers that it hasn't seen. I agree, but I I I don't think that I don't think there needs to be one simple one single way to do it because other may decide to encode that information in different ways."
  },
  {
    "startTime": "01:18:04",
    "text": "Understood, but what we're talking about is warps, which is an attempt to create a interoperable standard that players and producers can work on independently. In that case, there has to be a consistent way to discover this history of group objects. So that's what this is about. Work is all CMAP applications, right, that's a big space. And there's many people already CMF has already delivered today in many different ways, right, HLS dash and whatnot. So there's already different ways to encode that information in in those CMAP applications. So so, Mo, we we were really running low in time. Let me ask you question. Would would adding this for testing in the next iteration of the the draft cause you any heartburn. Wouldn't cause me heartburn. I'm just thinking maybe unnecessary to specify. Okay. Then let's go on. I I think this is, you know, need to specify this for DVR. You need this, you say application specific, but I don't wanna implement a DBO and per app. I I want to have you know, a player that supports Stevia, regardless of which website I use. Oh, and, this would be a good place to put the max bit rate. In fact, we could put any objects that's changing any attributes change over time, Okay. That was it. That's the end of this one. What do you want next? Do you have a Just, general call, I would we we brought the catalog repo in from a private repo into mock WG. The so the call would be, can we adopt it and then work on it. The threshold for adoption is that it's a reasonable place to start, not it's that that it's the best or it's in even looking like the end, but that's a that's a chair based. So we have, I mean, at the interim and the catalog was resented, there was, very strong support among the group that was there, that catalog is a reasonable starting point. I'm not sure. There's there anybody here who was not at the interim who has read the draft and does not think it's a reasonable starting point."
  },
  {
    "startTime": "01:20:02",
    "text": "It's too complicated for the end of the week. I I won't put it in a different way. I think if you're going to merge your recent set of updates, feedback from this meeting, publish another version, ask the chairs to call for adoption, and we will. And if you do not support adoption, we'll see you on the list. Some parameter reforms. Martin Duke parameter reform. How 10 minutes. 4 slides. 10 minutes from one man. Week, I'm the absolutely stunned, Martin Duke. I'm actually getting this, next. So they're just a note from Spencer in the chat. He wants to make sure that at least we discuss how many term meetings we need the next day. So the the the little thing underneath that was a sarcasm marker in case you didn't see that. Okay. There are 3 kind of So we fixed a lot of the primary stuff. There are 3 kind of lurking issues that, frankly, they can live with anything on any of these, but I think we just need to decide. What is the length of mismatch we've we've thankfully gotten rid of, like, 39 byte integers and and now integers are variants. But of course there's a link field and there's a link field in the Verint. And so like they could be different one solution is something Victor froze a while ago, which you just have like a like a a bit in the type field that tells you, like, if it's a Verint or not, and if it is a Verint, the link field is eliminated, therefore, there's no possibility of mismatch lot of people didn't like that. It's not in the draft today. What the draft says today that is it if you detect a mismatch that that you ignore the parameter and keep going. Using the link field. That's the link field, not the variant length. And the reason for that, which is like kind of aesthetic is that it is annoying to have"
  },
  {
    "startTime": "01:22:04",
    "text": "So if you don't understand primary, that is what you're going to do. And if if you do understand the parameter to have, like, different behavior is is like sort of, It's, like, harder to write and and a little harder to reason about, as I would think. I Luca Luke made a counterargument to this that you might have some weird behavior unexpected behavior if this happened But, I don't know. What do people think? Ants wet Google. I actually think this is a bigger footprint than I realized because this is not so similar from, like, HTTP on content, lengthen coatings, sort of like, masking issues where, like, I can throw down an invalid length and get you to skip over stuff and maybe you handle it differently. If you understand parameters, versus don't. Like, I'm not sure I can actually instruct an attack immediately, but, like, gives me the ABG piece. So the rather year of it. K. Thank you. Victor. Yeah. One thing I do want to say is I am really unhappy with any format where we can, like, add random data to the fields that we do not have any use or how to interpret with systematically avoided that and quick. As as we could not allow that to end in my queue. I'm sorry. What? Oh, because by having a length greater than the variant length, you can put more can, like, smoke all data in somehow. Okay. You're arguing to go to the number 2 to the 6, which is to, like, remove the link field. So it's impossible to have this error. Oh, either removes the lamp failed or error. Out if you or if your lands mismatch. Okay. Alright. Well, that that would okay. Thank you for clarifying. Colin, be really quick, it seems like you're thinking there's a bunch of stuff To agreed on here. I don't think it is, and I might be getting the wrong context of where these could be happening. So there's places where we have to pass on where we have to parse"
  },
  {
    "startTime": "01:24:02",
    "text": "unknown parameters for extensibility reasons, right? Yeah. And then there's all the other places where you're in field that you know the type of already. Like, we're not gonna have double lengths for Okay. For things where we know what the type is, we're not going to be doing this, right, am I No. That's not correct. So Okay. So I I think I think we should schedule a bunch of let's run a design call, spend an hour on this because that that makes no sense at all. I I'm sure we're talking past each other so a Verint has a length in it, obviously. Yes. Right. Yeah. There was a proposal to, like, have a a, like, a of some voodoo in the tight bit. So that if it was a verint, there would be no link field. So But I mean, if I if if if I have a type if if I'm parsing a structure that has 10 variance in it, and I know the type of the the structure. Yes. Not gonna have a type between each one of before each one of those variants. That's, I think, where we're talking past each I mean, that's No. No. No. This is for setup. And for track request parameters. For only for the parameters in the setup. That, Well, I mean, there's also a tracker press parameter right now, but that's that's a different thing. Okay. But if primers can be unknown, so there has to be an overall link field to skip them. There's also a link encoded in the actual value in the case of So they could be different, and we have to properly handle that case. As it stands. Okay. I agree we needed it in that case, and I was Luke. Luke. Luke. Yeah. This is this is a shotgun. So if client thinks the parameters of our end the server thinks it's a string. This text will basically say that the server must ignore it and just pretend like it never happened because there'll be a length mismatch. So I I think that if if you have two implementations that know about a parameter but implement them wrong it just causes you to silently ignore errors is what this text says. Okay. That's another vote. And Christian is sat down. So,"
  },
  {
    "startTime": "01:26:01",
    "text": "Great. There's a clear signal that we just wanna make it be an error if you identify the parameters I will do the linguistics to make that work. Next slide. Track request parameters. So, So set up parameters, I think there's a very solid case for those now with the VNTech we have on why you need them. So no notes on that. At least on this slide. We have one parameter in all the rest of the protocol. Which is authorization info for subscribe and announce. Like, we could just get rid of that with optional field in those messages and just not have parameters and all this parameter stuff in the rest of the protocol. Like some people have stated that actually they do see a use for message parameters On the other hand, we have like, we can negotiate capabilities and set up. We have 2 of the to message code points. So, like, I mean, I would favor just get rid of these, and I think it just cleans up document editorially a lot. And, what do you guys think? Again, I could live with either. Yes, as an individual, I like the flexibility. They look like HTB headers, and I think that we should have them even though we only have one defined right now. I'm probably going the other way. I'm saying because we wanted to put the hint parameters, and we put it in the subscribe message now. This one left authorization, I'm okay to put it in the subscribe message. I'm sorry. You want to leave parameters in there or not? It as a field as an optional field. Take them out. Got it. Okay. And, that's it. Alright. That's We we actually don't have time for your last one because it's 6:27 and we wanna talk about, whether or not we're Okay. Thank you. I'm gonna flash it up so people solve one thing What what what do you resolve on these what Alright. It sounds like this one, currently, no one seem to agree what role is for. There will be ongoing about attributed, send something to list. So, Alan, talk to us about interim interim. So we, our thinking that another, certainly, an"
  },
  {
    "startTime": "01:28:01",
    "text": "like, interop has been very valuable, and we should continue to do that. An interim, I think, would also be valuable before Brisbane, We had Ted and I had also discussed that maybe because of travel budgets being drained for travel to Australia that people would perhaps prefer the interim to be virtual only sometime in the wintertime, January, late January or early February. Do people have thoughts that they want to share on this plan? The interims actually give us 2 solid days of we get a lot accomplished in them. So if I've got scarce travel budget, I'd rather allocate it to an interim and then do a virtual IETF where we only get, like, 90 minutes or something. So that's my Ali, then Cullen then I'll have a comment on that. just wanted to say that, some of us, bunch of us will be in Denver. I Early February for my high video, and then, Comcast again, we'll be happy to host an meeting. If there's an code. I my vote would be for an in person. We just spend so much more time when we so much more Denver would be a great location. I'm sure we could host in Seattle as well. But those 2 locations come to my Okay. The, I will point out that if we do do an interim, this does not mean that we will not have a meeting at Brisbane. There's actually some rules around how how you can use replacements, interims to avoid travel, and they're not they're there to make sure that people are not disadvantaged if they come from parts of the world, in the 1111 cycle. So in particular, we've not met in Asian time zones for a while, and"
  },
  {
    "startTime": "01:30:00",
    "text": "can't really skip Brisbane. So there will be Brisbane meetings. If you are not planning on coming in person, of course, welcome to attend them, remotely. That's why they're hybrid. But we'll be getting up at that that time of day, just just plan for for we'll talk offline about, what dates look good Ali, thank you for suggesting, Denver. Thank you, Cullen, for volunteering sing, Seattle. I've outside Singapore, and that would have probably caused a lot of people problems but we'll we'll try and figure out some dates and, get back to you. I think that brings us to the and listen, you just want to Okay. At 6:30. We are you're all set if you have a plane to catch. Go grab it. Thanks for a great IETF everybody. And, yeah, Well, see you at the interim and or Australia. Okay? It's gonna be in the basement."
  }
]
