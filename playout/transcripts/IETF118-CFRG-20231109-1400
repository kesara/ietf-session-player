[
  {
    "startTime": "00:00:20",
    "text": "I need my neighbors Aaron you are pain I just encourage him to actually turn it on. Oh, yeah. Good. Good. You're right. Okay. Excellent."
  },
  {
    "startTime": "00:02:13",
    "text": "Alright. Good afternoon. This is cfrg@itf 118 in Prague. So I hope you're on the right room. We have reach a packed agenda. So let's get started. So I'm Alexi Milnickoff, an accelerant sitting next to me. Stanley slash slash life is remote. K. So as usual, this session is being recorded. If you on-site Either use on-site tool or full client, Because this is how we know how many people attended the session. So we'll have minutes. Thank you, Russ. Housed it for taking some notes for us. I hope by this time, in the IETF meeting, you've seen the note well. So this covers, IPR rules as well as, behavior policy. In our RTF. Okay. Going So, We do, fairly big number of documents and flights, but actually there were quite a few changes So let's just quickly review them."
  },
  {
    "startTime": "00:04:05",
    "text": "Oh, sorry. 1st agenda, any agenda bashing? Okay. So we actually have by my account, 4 RFCs published since last, IETF meeting. Which is probably, our CFRG record. And thank you, especially, Stanislav, for shepherding at least 3, maybe even 4 of these documents. So that was quite quite an effort. We have a couple of documents in Alex Edithruscu, actually, in all 38, So just about to be published. We have one document frost in ISJW. At the moment, we are not waiting for IRTF chair for anything. But I'm sure this will change. So Here's the list of Fredis. Active documents. As you can see, a lot of them would update it. So this is all pretty active. We got one document by Nick on guidelines for writing cryptography specification. That was posted shortly after last 8th. And We do have a few expired documents, the one involved are the documents actually, which are supposed to be an active work, but"
  },
  {
    "startTime": "00:06:01",
    "text": "would expire recently. So I just want to highlight that. And with that, I think Okay. I'm going to k. Chris, please come on. Yeah. Up here and then tell me when to, move slides. Good afternoon, everybody, slash good morning slash evening. So, this is not a lot of, new things recently. But this is a document that's been steadily moving along. The editors think it's pretty close to being ready for research group last call. And, so I wanna give everyone just to kind of, a narrative of the development of this draft and, talk briefly about what is left to do. Next slide. Okay. So, VDAF, is is a verifiable distributed aggregation function. So this is a particular class of of party computation protocols that are based on secret sharing this is a primitive that's of interest to the PPM working group. Basically, at a high level, our goal is We have a bunch of measurements generated by clients. And we want to compute some function over over those measurements. Without revealing to any of the parties involved in the computation, what the measurements are. So, each which is shown on the the right side of the screen here. Each client takes its measurement. It secret shares its measurement into input shares,"
  },
  {
    "startTime": "00:08:00",
    "text": "sends one of these to each of, multiple aggregators generally just two. These are these are the servers doing kind of the bulk of the computation. The aggregators interact within the with, with 1 another in order to basically, you wanna know that the secret shared data is data you can safely consume. So, this requires interaction and what the kind of arrow between the two aggregators shows. Finally, each aggregator kind of adds up supputes this thing called the aggregate share sends this to a data collector who then just, kind of adds the aggregate chairs together to get the final results of the protocol. So, and want privacy, basically, in the presence of a malicious collector and all but one aggregator we wanna make sure that no honest client's measurement is revealed. And all they learn in fact is the aggregate result. Then we also want robustness, which is basically in the presence of malicious clients who might be trying to, trip to upload junk data we can detect any malformed inputs and and remove them from the input set. Okay. So that's a high level overview of kind of VDS and our security goals, as what we set out to do with this draft is to not only specify the this abstraction and the goal, but also to specify 2 constructions. We've done so. The first is called 303. This is like, the this is kind of the basic tool that we have in our toolbox. It gets input validity from this thing called a fully linear proof. Basically, the is a 0 knowledge proof system that you can evaluate on secret shared data instead of like committed There's lots of variants of 303 that are specified in the draft."
  },
  {
    "startTime": "00:10:01",
    "text": "Each of them is kind of suitable for a different use case. And we have built this to be very flexible. So there's, lots of different variants can imagine for this. And in fact, there are unspecified variants that are not in the draft but, we're using for different use cases. The other construction is called Poppler. This is, an, a VDF that's intended to solve the, heavy hitters problem. There you have a set of, like, bit strings uploaded by clients, imagine like URLs, and you wanna count the the ones that are most frequently occurring. Say, for example, you wanna know how many people visited like, but, like, what what is the set of of websites visited by at least, t users, in a in a day for some threshold t. This is based on a a function secret sharing scheme called an IDPF, incremental distributed point function. that's basically our 2 core kind of primitives So that we use in this space IDPFs and F LPs other MPC techniques and we've sort of intentionally left room for schemes requiring multiple rounds. Generally speaking, like we are constraining the architecture of MPC in a way that doesn't give us all of but we think gives us a sufficiently large set of possibilities to address, the use cases that are coming up in the PPM working group. Okay. So our first draft, we adopted this. I think I getting now. It's like 2022. Our first feature complete draft, the one that spelled out 303 and popular in full detail, was draft 2. These are based on papers from the literature. So, Prio is is, I think, well known amongst a lot of people. It's based on the original paper, but we included some optimizations that appeared in a, in a later paper."
  },
  {
    "startTime": "00:12:03",
    "text": "And then, Poppler is this thing that appeared later at p 2021. And you can see here it's it's it's a it's a lot of the same set of authors. So people are probably familiar with, Henry Corgan Gibbs, and friends, and they're sort of the ones who are making all this happen. Next slide. Yeah. So, Henry took a look at Draft to and provided some feedback. One thing we found at this time was a bug, that would have amounted to an attack against robustness This is, before we had formal analysis of this part of the protocol, and also at this stage, we introduced code points for to distinguish different VDAS, which is helpful for, proving stronger balance. Next slide. You're, you're on top of it, Nick. I love it. Draft 3, we actually wrote a paper to do some, to kind of formalize our security goals, and try to prove security. So we found This led to lots of different robustness improvements also kind of additional considerations around, the kind of functional part of function of sharing, we also adopted, Cshake 128. I will not tell you what we were doing before. It was pretty ugly. But what we found in this analysis is we needed a random Oracle model needed the random oracle model, and this is how we chose to instantiate the random note that we didn't for popular one, we were looking at kind of a related scheme that turned out to not be very but we learned a lot, in the attempt to, study it. And then we also had an implementation of Poplar 1 from David Cook, who, then, you know, it was great. And, he joined us as a co editor of the draft. And from that, we got like some additional implementation guidance, And also, you know, we thought through how we want to make it constant time. I'm not a 100% confident about this part of"
  },
  {
    "startTime": "00:14:01",
    "text": "construction. And then the next draft, Philip, one of the co editors, reached out to a colleague named Shell Wing. Provided some feedback on, how we might optimize the IDPF and the central kind of, observation is that we don't need a random oracle for this for for for the privacy of this construction. So, we dumped C shake and, replaced it with a fixed key mode of AES based on the paper cited here. The next draft, we got some feedback from actual, like, users, so that was great. In this, we ended up streamlining kind of the inter interface between VDAF and DAP. DAP is the, protocol that PPM is standardizing. This is sort of the deployment pipeline we're envisioning right now for this class of protocols. And we also did a lot of editorial work, the draft. And also changed one of our, pre03 variants to just be more useful for the use cases that we had. X. Yep. And then, we got some more feedback from implementers about, like, well, C shakes a weird choice, so we move to shake. This makes it easier to implement because this is more way more common among, libraries that implement Shaw Three. We also defined a new VDAF. This was based on an cermation about that David made about how to construct the validity circuit for the the proof system, in a way that was much more efficient, and then we applied the same optimization to 303 histogram. And then we also, handled, addressed more editorial things from from wood. And then, after we implemented sort of this interface between DAP and VDAF, we found some bugs that we fixed, in this draft as well. Cool. So we're at draft 7. We are going to take some additional features draft. 1 is 1 more optimization to IDPF."
  },
  {
    "startTime": "00:16:01",
    "text": "For which we are sacrificing a little bit of security. So this is something that we need to I think study a little in a little bit more detail to make sure is is safe. We've also added a feature to 303, which is pretty safe. Basically, what this is meant to do is allow one to trade off communication costs for CPU time. And the idea is that you kind of generate multiple proofs And, by checking all of the checking each of the proofs separately, independently, you can go with a much smaller field size, and this, this lets you kind of shrink how much, data you put on the wire. Yeah. What kind of the open question here that we're looking at right now is, you know, we need, we need to provide some guidance on, like, how how to to do this trade off in a way that actually is is secure. And I think we're still thinking through this right now. But I think we're confident that we want this feature. At least this was requested from a particular group of people. And, Henry had suggested it early on, so we thought, yeah, do it we're considering a few more breaking changes. The one we're definitely gonna do issue 299. We are replacing shake with turbo shake. So this is in a in a draft going on through the CFRT right now. The, the motivation here is, CPU cutting down CPU time. Is significantly faster for 303. At least for certain variants, let's see. And then, yeah, we're we're kind of nitpicking to work to make them a little bit more flexible. The kind of the big open the the one that I don't understand given my lack experience with, IRTF and ICTF stuff is the IN of considerations. Basically, we have, code points. We need to write them down somewhere. We gotta figure out how this works. And then finally, we have lots all most of the open issues right now are editorial. We have tons of works to do here."
  },
  {
    "startTime": "00:18:01",
    "text": "Before I think we're ready for, the the a consensus call and also I'm, you know, we want to reach out to individual people, to to do reviews. We still have one implementation If someone is, like, interested in this work are interested in contributing to CFRG in some way and you wanna do an implementation that's not rust. Yeah, like, Let let's do it. I think that's it. Anybody questions in the room? Cube online, Alright. Simon. Simon. Hey, Simon Fee Program, Mozilla. I'm just wondering there's not a lot of mention of popla here, and it doesn't seem as well baked as Frio, And, yeah, it's still in the V dev craft. Right? So does it stay? Does it Get removed? Does it get replaced by a mastic? Yeah. I we'll hear about Mastic in a little in a little while. I don't have a strong opinion. I think Poplar is pretty well banked. It's not I don't think it's as well baked as prio. You're right. But, it could, it could potentially stay. And general question, there was talk at some point about things needing multiple implementations for standardization. Is that Not a requirement? I wouldn't I don't know if that's a I would doubt that's a requirement, but, we it's one way to make sure It's it's one way to review something and make sure you understand it and make sure we get the feedback that we need So, independent implementations will make the spec better. Okay. Thanks. Chris, thank you. Alright, Vasiles."
  },
  {
    "startTime": "00:20:04",
    "text": "Yes. Hello. Can you hear me? Yes. We can hear you. Great. Hello, everyone. Vasilis on the talk about the PPA signal sheet. And, Next. So, just like we could go but PPS has to decreasing of, let's just say, did have signature on queries, suppose the deterministic cost of size digital signatures over that sort of mutable messages, they will support what we call and look at the rules. That are essentially 0 knowledge groups of knowledge of a signature And those resources support select discloses of messages which essentially means that a approver, improved generation on the can choose to disclose some of the signed messages for these others. And next, Yes. So, will you going to go through what we've changed in the latest draft many made editorial updates, some operated changes, in this version, that would likely be at least 200 and 6 in the next best to cover the talk about it. It's a little bit But in this version, the main things done is to factor, the the draft to, 1st of all, separate the proof operations to different subgroups. And to separate the main operations into high level API, oppression and a corporation And next So, what we've change to the proof generation, the different subroutines, well, This is just make combining the PPS proof with other 0 knowledge protocols, easier."
  },
  {
    "startTime": "00:22:00",
    "text": "So for example, if you want to use something like pseudonyms, if you want to combine the previous proof with, a membership proof of, or a non membership proof saying if a case or maybe it's accumulated or stuff like that, this stage makes it way more easier to do so. What was essentially done is instead of having one proof generation proceed to no cost, 3 subrutines that are on sequence, proof visualization, it sounds calculation, we're gonna do randomization. Depth, result of the proof analysis is from the let's The other change we've done is we've, that it is concept of an interface operation, which we call it cooperates so essentially, what we previously had is that the the operations and main BBS production. So sign, verify proof generation of suggestion they need it, a way to calculate a set of points we call the generators in the draft, also a way to process a message is input message is to map them to tell our bugs Now for flexibility, those procedures were defined as part of the cipher seat. However, we found that a lot of applications required more flexibility. Had some of the case. For example, I want to use predicate Pros and you know, for those 2 work, they have to define even multiple saucer sheets. As part of just one product. So what we've done is we've removed those operations from the Sysha suite We created what we call the core operations, like cosign, for example, that expects the set of generators the message is already marked to tell us as simple. And then the interface operation assessing the greatest generator will process the messages and then pass them to the corporates. So this has a necessary flexibility. Next, Okay. So what we are working on right now and what would likely be in the next trust"
  },
  {
    "startTime": "00:24:02",
    "text": "is a new proof generation procedure. Next, we give a bit of context of why we think a new proof to arrest you to resnick it, would be the breaking change essentially what we originally used in the draft, was this work from route 2016 2017, there was a series of 4 that presented of knowledge protocols, efficient proof of knowledge protocols for DBS signatures, that had this, the format of AES. So a point with 2 scalars. The interesting part there was that it was really not apparent to anyone, what this last scalar, the value s of the signal to who was providing both when it comes to security and when it comes to speech, where the main assumption was that the customer did not afford to work that the software have done that kinda led to the DPS work. And to cut up back up these hypotheses in this year's eurocrypt there was a paper that got presented that proved the strong efforts that will be proper for PPS signatures that don't have the So the PP signatures of the form a and T, so that's a point and a scalar. Now this paper did also present a proof of knowledge protocol, that became essentially what we used in the drugs. A slight alteration as well as a proposal the protocol, which has some practical ring, the the issue now is that is that in, almost a month ago. Some photographers that were reviewing not trust roturantations that it's not really that straightforward to prove the soundness property that's, you know, of that not as good. Essentially, what you have to do is to reduce, the solvingist property to the 2 strong."
  },
  {
    "startTime": "00:26:03",
    "text": "The issue with that is, 1st of all, it's that document not that straightforward, and it's not really the startup thing that most you know, of knowledge protocols do So it become harder or let's say, unclear how to combine PPS with other of knowledge protocols, Or how to use VPS as a blog post as part of other protocols and stuff So what we do, what we're going to do for the next version is to use this proof of knowledge protocol that this small iteration from the SDL 16 1 long projected, in that work, that's a very small iteration of that protocol to work with signatures in the form, presented in the 20 district. So we're going to use those signatures of just according to the scalar, but the proof notes protocol from 2016. We have another PR. Now we set protocol for the draft we ask a couple of photographers to review it as they have done. We've got positive feedback by all of them. We have also talked with the authors of the 2023 paper. And they would be updating their record of that protocol. And I am also aware of at least 2 implementations, for that you know, protocol that has some data conditioning protocol And, just yesterday, I think we've got feedback that the test records we have in the APR could validate it by of those limitations. So we would be likely merging this very soon. And after that, we think we would be very close to asking for review for critical transcript review by the working next. Thank you. So another thing that we are working on are blind biggest, signatures. Mix."
  },
  {
    "startTime": "00:28:00",
    "text": "So DT signatures can also support, the blind is not functional. So this works as we can imagine, the approver, 1st of all, commits to a vector of messages that they want to get the signatures on, they create a proof of correctness of this commitment the sharing of the commitment and the proof today's share, Yes. So we verify the proof think if it verifies they will sign, they will create a signature using this commitment maybe also adding messages that they want to include to the proof and send that signal to that to, the approver the proof now this does a valid signal to some messages that proper committed and messages of the issue committee. So you can just use it to create we can regular PPS tools, as normal. So some characteristics of this, of this protocol is first of all, one thing is that you don't really need to have any unblitings step as in many, you know, blind signals protocol, essentially, the randomness that the broker used to blind the commitment it just becomes fun to assign the messages. That will never be a bit of it. No. We have heard by a lot of people that they have used cases for staff like user button, for example, where the user will commit to a security but they then need to know in order to give generate proof stuff like if you use pseudo names, you can just splicing out the they should kind of trouble. Stuff like that. For that reason, we started working on the draft. For, an an exceptional document, that about we'll extend the core because, for example, this is additional, blind signal, structurality. And and and"
  },
  {
    "startTime": "00:30:02",
    "text": "the question we're going to ask by the group is that Should this be a separate document? Or should we have the functionality to the main a. Our opinion right now is for that to be a separate document, mainly because the main document is not that's more on the 80s. So we don't want to add you know, more styles and make it bigger and harder for people to read. But we have heard arguments for the other way around of on documents for consistency. So if people have an opinion on that, that would very much appreciated. And next, Yep. Never necessary references, and that's it. You very much. We both have to take any questions. Any questions or opinions about the blind signature part of this draft. Alright, Christopher. Thanks for the update, and, thanks for presenting this. Do we have a specific use case in mind for blind BBS signatures. In the IETF or outside? In the IETF, there has been some discussions on using this for privacy bus. So, essentially, we the user will get silence by some realignment party or knowing as it's good for that group. And then we want to get the signal so that it's bound to the standards without the revealing the standards with the issue. So you can use splicing as a sort of. Now this is just discussion at this point. Outside the the ITS As I said, you have the risk there."
  },
  {
    "startTime": "00:32:02",
    "text": "The major cases for user binding, binding the the proof to a secret key, not only by the user. In a way that If you don't know that's cyclically, you cannot generative. So, For for for privacy pass in particular, what's the difference between this and just RSA Blinds signatures. Like, what's the thing you get? The main difference is that you can generate multiple data sprues that are on lookup. To my knowledge. And if from part of this facet here. Let me have more information about that. But from perspective such a pictograph I think that's mainly difference. You don't need to took a nice, let's say, we have multiple, the ability to of them. Okay. Thanks. are you still? Alright. Or is deal? So use cases within the IETF there's an application of this in the adjacent web proof work in the hosey working group. Or Some application of it to privacy pass as, as was just mentioned, you know, potentially some you know, better performance or smaller proof sizes, but for a similar sort of, building block, and in general, this is useful for building anonymous credential systems, and there the spice off, you know, earlier this week. So I think it's a generically generic building block. It's very useful. To the question of, whether or not it should be 2 separate documents of the the existing document without the blinding is already long, and stable to the point where you're you can do a clean-cut, and it really is gonna be safe to, ship it and not touch it and the blinding part won't need to come back and make changes to it. Split them up. And and ship the regular version. And if not, if you still need to be able to touch both sides, keep them together. I don't know what state it's actually in. Thanks. Thank you."
  },
  {
    "startTime": "00:34:05",
    "text": "Kalia. Kalia, just to compliment what What Ori was saying, the winded secret. From the holder to the issuer and being able to present to the verifier is really essential for a lot of the credencilling work, including the EI desk, work in the EU where right now, the only option is to reissue credentials many, many times so that you have different signatures you're sharing with each verifier. And this is a pathway forward to support issuers being issue able to issue 1 credential and that the holder can Make sure their presentations have different signatures across the range of verifiers they share it with. Thank you. Okay. We're already over and we've closed the Q form. I don't know what we're gonna do it. Sounds fine. Okay. We can keep, questions, follow ups on the list. Okay, Andre. Can you Hello, everyone. My name is Sandeepashkom, and, I will keep an update on AD proxy draft. I will try to do very brief since the Henick Now, a really exciting, easy talk, right, after me. So next slide, please. Of public health news,"
  },
  {
    "startTime": "00:36:02",
    "text": "any person was published, like, 2 weeks at Coworkas, there, there are some new practice edits, some new examples of functional applications, how, as usual, of the most interesting things is, probably aspiration of commitment property into 2 properties, into full commitment, and key commitment. They, surprisingly to me, had different applications. It is a rather interested topic, a rather interesting topic. I wanted to thank, Samuel Lucas for pointing for noticing that, the draft only covered key commitment before and help for providing me a road map, prune that, Robert, interesting topic of commitment. Well, next slide, please. Yes. Speaking about approaches, And the most requested one is probably in differentiability. I'm ask to to edit, like, after every, ETAF talk and Sappell, I remember about it. I'm, working on it. We've had, very small problem had, individuality is not actually, like, an additional security property or like, an additional implementation property. It is, entirely different, and you approach to defining AED security. It is Well, I love integration ability, and I really want to add, I didn't have a draft, but, I can't, find, approach yet, and I will try to do it, the next discussion. Yes. Next slide is and I would think I wanted to,"
  },
  {
    "startTime": "00:38:00",
    "text": "talk about is, needs to workshop on the box hyper modes of operation. Has, I need topics for discussions, for discussions there and, better some really great talks. I wasn't very personally, but, all all recordings of both days, available Anthony's website, there are some really interesting talks, some really interesting papers. And, how, one of the topics, which was discussed for box shop was additional security features. Next slide, please. And, well, at least 7 of fourteen accepted papers, for out, in some way, third, how, additional additional properties, or even, like, a redefining, the standard properties, and extending. And next slide, and free of this, for related to commitments, like, to to it's probably the most demanded property at the moment. And the next slide, Yes. And at least one, paper sites for draft I couldn't help, but notice it. It was, are really motivating. Oh, oh, oh, And, finally, we think, the think, which is high I give this talk every itself meeting. It is advertisement. Please contact me. Feel free to contact me if there is a property which you want to see in the draft, if there is some application or protocol or which requires, a deal with additional properties, or you just want to have a chat about AAT and, you have questions about traffic. I will be really happy to answer you. And if"
  },
  {
    "startTime": "00:40:01",
    "text": "proved to be very helpful for the development of the draft. I hope that's all. I can answer your questions. Hi, Jonathan Lennox. We, back in April, there was we had a brief change on the CFRG list about the property that sometimes it's useful to have just verification without decryption. Because, you know, the use case I had was you can tell from the AAD that I don't care about this encrypted data, you mentioned you'd put it in the draft, but you haven't. So I just wanted to remind Yes. Thank you. It is also not going to do list and, I just, couldn't figure it out yet. Yes. I remember about it. Chris. Yeah. They I think this yeah. I think I said this before, like, this is I think gonna turn out to be really important work because, like, I've had a lot of conversations at IETF like, still about, like, confusion about what AAD is supposed to do. I think I was also one of the people who was like, hey, check out in different a I don't think there's a need to put it in the draft if you don't think fits because you're right. It's a totally different paradigm, I just thought, yeah, it might be interesting. One question about the draft. Do you think, so there's I was noticing that there's, like, lots of different properties that might, like, there might be, like, implications between various properties. So for example, for commitment implies key commitment Yeah. Mhmm. Do you think it's worth First of all, describing the relations in the draft, Second question, Does it make sense to just talk about full commitment and not key commitment? Like, like, is it better to provide less choices,"
  },
  {
    "startTime": "00:42:06",
    "text": "I think, well, to the first question, I think, it might be useful because, might be useful to note, different correlations between properties because about, sometimes Oh, I think more importantly is to notice, like, that some property is can be achieved together with our property. I think this like, more useful, of course, just leave a full commitment and key commitment was very straightforward or that one place each other. So I put it in the draft. And, as to the second question, I don't know, to be honest, because, of how it is likely, nonce misuse. Like, you can have, non specious resilience, a non is resistance. Sometimes you need only 1 of these 2 probably it is something like that when you, if you want to as far as I understand, full commitment, is a little bit more difficult to achieve and keep a commitment So probably there are some applications of it might be important. At like, for a key, he, you know, you only need commitment and, fulcommunity is just more difficult. Yeah. So I I I think it might be useful to think about who the users are Yes. It's probably better to give non cryptographers less choices. And that's sort of one of the purposes of CFRG to my mind is providing like, the useful, useful things for for, for, for implementers. As an implementer, I think I like to not have to think about which of these properties I need and just if there's an AED that's fast enough, for"
  },
  {
    "startTime": "00:44:03",
    "text": "and that achieves full commitment, then that's the one I would reach for. So something to think about. Yeah. Thank you. I'll that's fair in the Thank you. Alright, Alex. So hello, everyone. I'm Alex, and I'm gonna talk to you about some research I did with my co authors earlier this year. The impact of subtle, AAD differences in, protocol security and especially in analyzing protocols. And before I go into, like, to talk about AADs. Because I think some of you might be unfamiliar with it. I will start with some slides on, automated security It's not Yeah. So a very high level, we wanna do, wanna analyze protocols to prove them secure while also discovering like, attacks on protocols, which then helps in the process to actually get secure communication. And traditionally Like, people when when also in the beginning, when analyzing protocols, you would, use, like, a manual effort during cryptographic stational proofs, after protocol, trying to figure out what properties to prove and building frameworks to prove them. While this is still used today, for some, like, especially bigger classes of protocols, this can be like a really big challenge, as like, having, a proof, a manual proof in in a scale of like TLS, and then there's this can be very hard."
  },
  {
    "startTime": "00:46:00",
    "text": "So our research focuses mostly on the automated analysis of protocols which started in the nineties and had a like like big technical progress in the recent years. So we are we're able to actually analyze large protocols Also, mostly automatic like, like, helping in TLS or EMV, analysis. And in our group, we mainly use the so called tamarind prove which is one of of many tools to do so. Next slide. So I will give you a short overview about the timber improver, which is, a theory improver. Taking a system, which is a model of the protocol, and the actual properties we want to prove about this protocol. Next slide. It's internally where it transformed the properties in the system, into constraints, and together with some attacker model And an internal dedicated constraints over, it will give us Next slide. Either, like, a solution, which is an attack on the protocol, on the properties we defined before, and it will visually show us, like, how to how this attack can happen in our model. Or we will get a proof or in the worst case, run out of memory, the problem is very hard to solve. In that case, Tameron, allows us to check the interactive modes with which we can inspect like, the proof and then like, assisted tool and with, for example, in variance, to then again, improve the system and find the text or get approved. So This, the mother, It's also not perfect. Otherwise, we would only use these kind of tools to to prove our protocol secure So one like, pick a limitation we had for years was the the attacker model, we used assumes cryptographic primitives"
  },
  {
    "startTime": "00:48:01",
    "text": "should be perfect. So like having symmetric encryption hash functions, signatures without any flaws. Which in practice, we know this does not work. So, for example, like having collisions of hash functions or like reusing nonsense and AADs problems, we face but that we're not covered in in these kind of analysis. And our research focused focused, focused today on like, actually, finding a better way to model AADs, such that we can capture nons we use the text, but also more. Thanks. Right. So, yeah, we want better models for Automated But why was this a problem? Why was this a problem? What you see here are just a little bunch of papers which define properties for AADs. And those are only a few said only like academic papers, there are way more definitions and properties out there. And some even, like, competing and defining really, like, similar things. But So that also opens up for, like, protocol designers, a lot of ways to misuse and misunderstand AADs when actually using them in practice. What our our approach to this was now to 1st Collect these definitions and this roperties. Knows the constructions that were used in practice, and then find also all the, or, like, known a text, that's used AADs to attack like, properties on a protocol level. And when we gather them, we next try to find the relations between the properties we gathered and also proved missing once if possible. We then classify it known attack vectors we found into several, groups. Before then modeling them in in, symbolic way and our temer improver,"
  },
  {
    "startTime": "00:50:03",
    "text": "to conduct some case studies automatically Exlet. So what we identified, where that most practical allow, attacks We could capture with 3 classes of properties, properties on integrity and privacy, collision resistance, which we defined, here as also, like, encompassing, like, a text like key commitment, and a non a nonsmuse. We then, checked, actual concrete AAD implementation or schemes whether the fulfill these properties or if there are some attacks on them. Yep. When we model these classes, we also Like, so, okay, they are not these three classes are enough in theory, but in practice, people also use AADs on a weird way. For example, splitting description and, verification. Of an AAD can lead to potential attacks. Or using text, the text of of certain AAD schemes and weird ways. So we actually modeled them as well. For all of these, we then modeled a lot of variants such that we can capture also more fine grain attacks, in our analysis. It's like then had, started to analyze our case studies in key, 2 approaches, a targeted approach, which we use to find a text and actual schemes. And the preemptive approach, which we then like, kind of, these properties we actually want to have for this certain protocol. Next slide. Targeted approach, we can then, for example, use, like, the table I showed you before, to figure out"
  },
  {
    "startTime": "00:52:02",
    "text": "what kind of scheme does the protocol use in its, design. From this, we then choose the right models in in the camera improver. Model the protocol, and we can then either show that the protocol, is not vulnerable. We find, the text on the protocol. In the preemptive approach, we want to run all the different AAD models we built, In all variations, against, protocol model And Check for the minimal, like the the weakest threat models that lead to potential attacks and the strongest attack models, under which the, protocol remains secure, to then using these 2, lines to figure out what properties does the protocol really rely on. Next slide. To test our, to test our approach then checked for, like, protocols that were actually vulnerable vulnerable to such, a text based on e a AADs, like an older version of the UVHS HSMS frame or this Facebook message ranking a tick. Like using our, like, building them intemorin or modeling them intemorin, we could automatically find find the same attacks that were reported before automatically in a matter like a seconds. Excite. From, we then tested our other approach and also established protocols. To just check, what properties do these protocols actually rely on? So the the things you see here are not actually The text. Most of them. But like, like, behavior, which maybe wanted or not wanted in them they're based on using"
  },
  {
    "startTime": "00:54:04",
    "text": "AADs. So what we could see here is that, the assigned class in in this table It's, the collision. So all of these protocols rely on some version of collision resistance which with, which I said before, also entails, properties like full commitment or key commitment in our setting, So we reported, all of these, maybe unwanted behavior, so I'll text, And, could could could could perform our complete analysis also, in a reasonable time and 2 hours against all different threat models. So what do I want you to take away from from this presentation? First, I think former methods can be used during protocol design and can actually help to figure out what schemes you might want to use and what, what schemes could, could be needed to actually achieve the properties you want for your protocol. We what's what could help the, IRTF is that we had new insights on the actual AAD properties and, how they relate with, to each other And, Our framework, like, it's also completely automatic and extendable. So if if there is an error or if there are future text, we can easily incorporate them in all analysis. So thank you, and feel free to ask any questions. Andre? Yeah. Hi, Alex. Thank you for your talk. It's very interesting and, really important. I wanted to, well, I know I asked it in the big mail, but I want to ask it here too. I'm planning to"
  },
  {
    "startTime": "00:56:04",
    "text": "what are your future plans, in relation to this study. And the, the second question it is not really question, but, I will be really happy if you out as share, a user experience too will help me building the draft. I know you told me that, but it's it'll be very great. Yep. So, to to to answer the first question. I, well, Yeah. We plan to to focus right now also on other primitives other than AAD. But right now, our analysis also approach also has still, limitations we want to address in the future. For example, we, right now, do not have a way to to model, all in all the text, perfectly, intemarine. So there are certain, Oracle style attacks, by which, for example, was defragmentation attack on SSH, which used like, this, that decryption misuse in in AADs which we currently cannot, model perfectly But, yeah, so I'm I'm I'm very happy to like, have a chat and to to give input on the on the draft. For sure. Thanks. Any more questions or comments? Okay. Thanks. Demetras. Alright. Hey. Alright,"
  },
  {
    "startTime": "00:58:04",
    "text": "And Dimitris. And today, I will talk about MasTec a new verifiable distributed aggregation function or with that. I would like to thank my co authors, Hannah Davies, Christopher, Patton, Practixar Carr, system. Next? Okay. So Chris already explained, what the visa is, but let me go over it quickly. So in the bit of setting, we have multiple clients. And its client has its own private, their own private measurement. And on the right hand side, we have 2 or more servers which we will call aggregators. And the idea in the video is that, we want to compute an aggregation function in a secure way without revealing anything about the client measurements. Next. And when we talk about aggregation functions. We care about counts. We care about histograms. Which are, like, account by a different category. All heat maps with which are, like, multidimensional histograms. Or even more reliable statistics such as heavy heaters. And again, Chris mentioned this before, but, in the problem of heavy hitters, essentially, we can have, 8 clients, submitting a very big string, and we want to find the most popular or the heavy, strings among all the client measurements. Next slide. And many deducts rely on what called the distributed point function or some version of, DPF and a VPN is a very cool tool, or primitive that allows a client to secret share, a binary tree next slide. Without send without secret setting every node of the binary tree. So in this case, we care about, a binary tree that's 0 everywhere. Except for a specific path from the root down to a leaf. And you can see, like, in the client, we have a path that's all ones. This can be any non zero values, not just one. So in the DPFs allow the client to seek a set of keys,"
  },
  {
    "startTime": "01:00:05",
    "text": "and then expand, and then then the aggregators can expand on those case. And retrieve the secret sales. And the the nice part here, the nice property we get is that the keys are significantly smaller than secret whole binary tree. Next And now the idea is that if we have multiple clients, client can secret share, the keys that represent their measurement and the aggregators can expand their keys, retrieve the secret sales, and then aggregate, all the shares together and compute the Veda. Next, now this happens when Everyone is honest. And, Let's see what can happen if star some parties to start acting my and, here, we will focus on clients. So the first thing that client can do is double vote. And by double voting, we mean that the, client we'll try to create, a binary tree that has a nonzero value or more than a a non zero value in its level. You can say, for example, level 2 has 2 non zero values. Again, this is problematic now for the servers or for the aggregators because they will only see secret shares that represent this binary tree, and there's no easy way for them to detect if what they have, the secret service that they have are valid or not. Next slide. So, we call this property 1 hot verifiability and we, inherit this this property from the previous work of plasma, which introduced a very 5 incremental DPF primitive Next slide. So in this work, the 2 aggregators can evaluate a specific prefix, and the prefix here can be, the string Central that I mentioned earlier. On the key, so server 0 or aggregate 0 will evaluate a prefix on q 0 and server 1, the prefix on q 1. And what they will get back"
  },
  {
    "startTime": "01:02:00",
    "text": "are, vectors of secret sales for a specific level. So aggregate dot 0 will get the vector y, which are which has secret shares for this level, and then aggregator 1 will get the vector red has also the secret share for that limit. Next slide. Additionally, they will get these 2 pi 0 and pi 1, these 2 proofs. Now the one hot verifiability guarantees us that if the 2 proofs are equal, then, adding back or reconstructing the vector for that level and the y and the zed. If we add this back, then this vector is one hot. Next slide. Okay. So by using this property to verify the incremental DPF, we can, detect the double voting, and we can guarantee guarantee this 1 hot verifiability property. Next slide. There's another problem. No. So my licensed client can also, use different non zero values. These are the better values that I'm showing. Different non zero values in different levels. And also the non zero values cannot be maybe, like, on different paths. So if you see here, this these are 2 problems. Although each level has 1 on 0 value, The nonzero values are different between the levels, and they are also not consistent if they are not in the same path. Next slide. So, we defend against this with what do we call, path verify built in mistake. And the way we do that is at, next slide. The way we do that is that we, use a fully linear proof to to verify that the root that the secret sales of the root are correct are valid. Now a full linear proof is, like, a form of zero knowledge that, operates directly on secret sales. And I said valid. Okay. What what is valid? So the the valid thing, depends on the application that we're we're talking about. For example, we might find valid in specific application as having secret shares of 1 or 0."
  },
  {
    "startTime": "01:04:03",
    "text": "Or we can have, a different application that defines valid as having, secret shares of a value beta that is in range 0 to 100. This can be defined easily by an arithmetic circuit. And by using a fully radar proof, the 2 servers, can take that they have valid shares of a better value without, without revealing the the better value itself. So after we do that, now have verified that the the root levels are valid. So how can the next step or step 2 that I'm showing here is would say that these better values propagated down the tree correctly. The way we can do that is by taking that they reconstructed value at the parent or the the parent note equals the addition of the reconstructed, children values. So, as I'm showing here, the y at prefix p, the y represents the reconstructed value So the wired prefix space will be called to the reconstructed value a prefix concatenated with a 0, which is the left child. And added to the y con of the prefix concatenated with 1, which is the right side. Now this property alone is not sufficient. But in conjunction with, 1 hot verifiability, we get that if we have better at, at the parent, then only one of the 2 children can be better. Not they cannot have, like, some shares of it. And then we can, recursively around step 2 to verify the whole binary tree. And pro and verify that the beta value is correctly propagate down the So, This gives us a nice property, this is unique to mistake because by doing the FLP at the root level, we can very different properties. For example, we can do, histograms. We can do heat maps. But we can also do heavy hitters."
  },
  {
    "startTime": "01:06:01",
    "text": "And not just that, we can also do, what we call a weighted heavy heater. Where in this level, we have, like, a table with a better value and also, secret sets of 100 to to still do a heavy hitters. Next slide. Okay. By combining these 2 properties, the 1 hot verify bill in the path verifiability, we can defend against malicious clients. And we can guarantee that each client can submit, a binary tree that is 0 everywhere and has only one path from the the route down to to a leaf that has, better values, and the these better values are the same. Next slide. We have some preliminary results. So here, I'm so I'm only showing the heavy hitters example. Have not yet implemented the, aggregation by labels or this weighted heavy hitters that I So this is the plain heavy hitters, but this is using an FLPR to the root. But the FLP is, only taken if it's 0 or 1. So, you can see we have, 3 plots. So in the first one, so in this in all three plots, we use different thresholds. And this thresholds represent what we consider heavy heater. So, essentially, the higher the threshold you have, the more the more pruning that you do and the less heavy heated that you that you will, consider in the in your final results. And we can see that, the threshold does not really affect Popular, but we have seen, like, a a very interesting speed up in mistake when we increase the threshold. Next slide. So we will work on more evaluations and the full security analysis in a paper that we will publish soon. And, Again, like, The the interesting part that we see here is that we can both heavy heaters with the same protocol and also histograms or weighted heavy heaters or some aggregations by different labels and all this, like, with the same protocol."
  },
  {
    "startTime": "01:08:00",
    "text": "Okay. Next slide. Alright. All I had. I'd be happy to answer any questions. Chris Wood? Thanks for the presentation. I'm wondering if there are any advantages that Popular offers that Mastic does not. So Yeah. That's a great question. I cannot fully answer this right now as we have not fully fledged the implementation, So, I want to do an apples to apples comparisons at some point. So this, numbers that I showed you in MasTIC we do, a proof aggregation that definitely saves us some time. So we want to see how this will be without this professor proof aggregation, and how we the problem or what's the overhead when we have multiple malicious clients. I know this is like an application that's very interested, interesting for Chris. And, we we need to to to see, like, how how Masstic, how how performant, master case in this case. The nice thing about Popular is that, the their run time would not be affected by the number of malicious clients that do you have But in our case, because we do, we do American tree to to take the malicious clients if we have multiple malicious clients, then the the run time might increase. Not sure how much yet. So we need to to finalize the implementation run some more experiments, but all these would be included in in the paper. Yeah. Just to kind of flushed that out a little bit. We don't yet have an Apple"
  },
  {
    "startTime": "01:10:02",
    "text": "apples comparison in terms of, like, CPU time or communication costs or the things that we would care about. I suspect it's going to be like slightly slower if it when used in the same mode of operation, But the trade off here is, like, a much more flexible tool. I was mostly interested not in performance. But in, like, what is actual functionality that we're gaining opposed to popular Yeah. Poplar is a subset of what MasTec can do. Fantastic. I'm very interested to see what the performance numbers are for the, the dealing with malicious clients or whatever. If it turns out that, like, this thing is like vastly more general or whatever, more flexible offers more features and is, like, in your a modest performance regression. Would suggest just like punting poppler out of the speck. Just going with this. Yeah. I think, I'd be down to do that too. I think we would need to figure out if it's it's separate document or the same document, I think either as possible. Before we make too many rash decisions, I, I want us to do the security analysis and make sure we understand the performance trade offs better. Pressure. Yeah. And, keep in mind the comment at the end of the bidaf section. About, the differing levels of details with regard to poplar versus Prio in that document and introducing a new one. You should make them consistent Makes sense. Thanks. Alright, Norman. Okay. Hello, everybody. My name is and I'll be talking about, RSA guidance, document. So, what we'll talk about is in general, the blanket duplication of"
  },
  {
    "startTime": "01:12:01",
    "text": "PCS 1 version 1.5. New improvements to timing site channel epics, recommendation about, most common leakage sources, and about implicit rejection for the KCS 1 version 15. So, basically, I think everybody knows that, PKCS 1 version 1.5, first shown by Blaisenbacher up to be problematic. That was 25 years ago. And, we've been getting new problems every few years, and newest protocols like TLS 1.3 or webcrypt already duplicated it. So I think it's controversial. Now, as far as improvements to and timing search channels is that most of the research that's published most of the literature is, using box test by Crosby, and that test require assumes, independence in measurements That's not this, property that I truly happens in practice. And, like, even if we consider stuff like turbo boost in CPU, that, assumption is fundamentally incorrect. Given that it was incorrect, conclusions from it are also incorrect. That includes the widely repeated conclusion that, site channels smaller than about 100 nanoseconds are, not detectable over the network. Okay. Next. So here you can see, heat them up for detection of the side channel, on the vertical axis, we have the delta in for, for loop, like the the server was just counting from given value to 0 in a very tight follow-up, like just to instruct"
  },
  {
    "startTime": "01:14:03",
    "text": "for x8664. And as you can see here, like, when the difference between the measured and, site channel version is like 1000, cycles, then this is fairly detectable even with just few 100 probes. But if we go down to single cycle, which represents I think it was, like, 6 or 7 clock cycles of of the CPU. 1,000,000 probes, you can detect it over the network no problem. This is over real ethernetgigabitcopper. So kind of measurements are, as, as possible as you can imagine. So The solution is to use pairwise tests like the scientist, Wilcox and scientists, and, those do not require the in the, the, the same environment for each pair being compared. So, like, even if we have sample of a 1,000,000 measurements, half of that million can come from one host. The other can come from another host you can you can you can you can combine those results and, have a higher confidence in the result like smaller, confidence interval, smaller confidence intervals in the in the measurement. There is also a freedman test, which allows you us to send multiple probes and then I the whole, shebang together. And that gives us even more, confidence in the interval. So general conclusion is that there is no such thing as, timing side channel attack, that's not detectable over the network. Okay. So now leakage sources multiple libraries assumed that a small, staff"
  },
  {
    "startTime": "01:16:01",
    "text": "is not detect, detectable. And then, and as such, if you use base blinding for RSA operations, you're fine. The problem is that if the numerical library assumes those automatic memory management. So small, small value means small memory allocation, that will leak and that will be very detectable, since I offer the rebates are exactly what we are after inflation buffer. That means that you are vulnerable because you're doing blinding. Then, again, like, if you need to remove, blinding, that the the code itself needs to be site channel 3, but, that's that's also something that we are very familiar with. Now mean, generally, if we are talking about 64, but, architecture and typical, key sizes like 2048. You're mostly fine because, the chance of hitting that 64 bits of zeros at the very beginning are fairly low. The problem is that if you have an implementation that uses smaller values, like four beds because, like, you want to have easier handling of carry bits or something like that or just you're planning on 32 bit architecture. Then you are, again, in the very easy to, attack same for, non standard key sizes. So 2 1049 bit. Very much easy to attack. Then another problem is bad API design. If you use case 1 version 1.5, and you have an exception, in case of padding failure, you have a problem. Now with OAP, that stuff is generally fine, but with PCS 1 version 15, You're not. So m 2 crypto, Python cryptography, Python has a Java, in general, Ruby, in general, dotnet All of that stuff, guys, and exception,"
  },
  {
    "startTime": "01:18:00",
    "text": "All of them are vulnerable. Same with smart cards, if the message is exchanged between the smart card and the host are different sizes, then automatically, they will also leak that's, that's situation that, yeah, I've decrypted 200 bytes this will take more if if there is an error because that error might take, like, 64 bytes or something like that. And that difference will also be detectable. Next slide, please. So basic idea, to fix this stuff, is that we do implicit rejection. So we combined the implicit rejection as implemented in TLS, with the deterministic nongeneration as we do for, ECDSA. So, in TLS, we generate a random value. Use it in case of adding error and do the do computation with that value for the the description of the, final finished message and so on. With ECDSA, we use the private key end the message to generate a random source of entropy, to be able to get the nuns, with the, for the signature. Yeah. Next slide, please. So, in chosen ciphertext attacks like the Blaition buffer, damn, attractive doesn't actually choose the that's the the whole, ciphertext. They just multiplied by selected number and then pass it to the Oracle. Now the article gives that, the attacker information if the, ciphertext starts with 0s or not, and, Then based on that, the attacker knows if yeah, this is a good guess or or a bad guess for the attack. In fact, the for the PKCS 1.5, the attacker is not interested in be or, in that adding, if in that, decryption being valid or not, they are only using it as a proxy information 4"
  },
  {
    "startTime": "01:20:04",
    "text": "is the is the value starting with 0 or not? So, with implicit rejection, if we are always returning, a message, then that whole, proxy information channel basically disappears. Because, All messages look like they are correct. So they are attack can only assume that all they start with 0s, even the ones that don't. And then the attack doesn't work. Next slide, please. Yeah. So that's basically it. Questions piece. Alright. One from Christopher Patent? Thanks very much. So I'm sort of trying to I'm sort of trying to, like, connect this to the this is about your draft, right, Yes. Yeah. So What do you see as, like, kind of the function of this draft? Twofold. One is, stop using PKCS 1.5. Great. But, We've been telling people to stop using PKS 1.5 for the past 20 years, and today he didn't listen. So I think we really need to provide them with, option that they can impact this use. And I think the implicit rejection is that thing. Okay. Cool. How much overlap is there with this, like, the in the guidance in this draft and implementation guidance we would give to other RSA signing algorithms. Is it all about biking biker, or is there something more we can more general, we that might be It is general. It is general because the thing is, numerical library, site channel, comes from before you look at the padding."
  },
  {
    "startTime": "01:22:02",
    "text": "So it, like, if you have a leak from the numerical library, from the, deep binding that that will affect the OAD. And if affect the role RSA. So RSA, a secret value encryption as Okay. And and is is your you wanna is is is the goal to do you think this is ready for an adoption call? Not a lot of the, yeah, for adoption call. Yeah. I would say so. Okay. Bob Muscovitz. Bob Mosquitz. I find this very sing. I don't know what in the place they're gonna do with it. I got I'm dealing with people with avionic twenty five, thirty years old, Still doing MD 4. Forget about the rest of this. And and and how I'm going to wedge this since the discussion there. And and because it's like, how do we kill the use of RS say in the rest of it. I I I I thank you for this because this may help me, in some of these discussions. That that we're having but but but This is literally you say 25 years? Yes. Because the avionics are that old, and they're not going Well, I'm Until we get new planes? I'm the QE for, Red Hat Enterprise Liner for crypto stuff. So, yeah, I'm well aware of problems so replicating stuff. But again, thank you. This does give me, like, another discussion point in these discussions. Okay. Thanks. And, please, contact the chairs in the next couple of weeks, and we'll discuss bringing this to an adoption call. Okay, taking a look at the queue of upcoming work. Thank you. Alright. This is the last presentation. David, Hello. Hello. I'm David Joseph, I'm from Sandbox AQ, and we don't have very much time. So"
  },
  {
    "startTime": "01:24:04",
    "text": "Just wanted to make you all aware of this document that we're working on. So it's called bat signatures, Next slide please the the concept is in the sort of the era of moving towards PQC, Rick expecting generally worse performance. We're expecting, you know, as time goes on, generally more traffic. There's always more traffic, more signing requests. Can we do anything to improve this situation? Next slide, please. Well, the concept is you build a miracle tree, and the leaves are basically your, your transcripts, And at the top, you get a Merkel tree roots, you use your, you know, favorite, DSA, your favorite signature algorithm of choice, sign the root and each signature then becomes the root signature plus the sibling path. So what do we expect from that? Generally, there's gonna be a small increase in latency because you are not only signing, but you're also spending a little bit of time to build the Merkel tree there's gonna be a very small increase in latency this is amortized over, you know, being able to sign many, many more signatures so many, many more transcripts for the same signature. And so the actual throughput that you're you're going to be able to handle is going to be much higher, which pathological scenarios could be quite nice. And, yeah, it's not a new idea. There have been many similar constructions, in, you know, different working groups there was something very, very similar to this originally in, TLS working group from that expired about 3 or 4 years ago. There are sort of Merkel tree constructions, which are being considered maybe in in DNS. There's Merkel Tree certificates, So there's lots of similar things that are kind of you know, around the place and,"
  },
  {
    "startTime": "01:26:01",
    "text": "our hypothesis is that it makes sense to gather this together and, and and and and you know, bring together some kind of a document in this forum, And so This is what it is. If you have any ideas or you'd like to, collaborate with us, please get in touch. At the moment, it's, a bunch of people from SandBox. There's also Andreas whole seeing involved in, and Martin Albrecht. So, that's that kind of roughly the team Yep. That's it. Okay. Thanks, David. Questions? Yeah. Thanks for this. Just to clarify, this is like generic and not specific to any particular type of signature algorithm. Right? Not specific to any particular signature algorithm. Correct. Great. Yeah, we have precedent in this group for you know, specifying, like, tricks and techniques like this for helping, you know, various deployment, problems. So I'd be supportive of the group working on something like this, especially if it might later find its way CTOS or whatever other use cases, might be necessary. So Thanks for doing it. Happy to offer reviews. Thank you. Sorry. Alright. Forced steel. Awesome, and the Skit, working group, for during supply chain integrity and transparency, we use a construction you know, basically exactly like this for receipts, for transparency log. And that log is based on the binary Merkel tree construction in the certificate transparency documents. So just a sort of word of caution around the construction of the Merkel tree proofs that you're going along with it. There are many different ways to construct them and there's a you know, you probably know this, but, you know, CT has their their way of doing it."
  },
  {
    "startTime": "01:28:01",
    "text": "If you want other proof properties beyond consistency and inclusion, You you may need different tree structures. Love to talk with you after Absolutely. That'd be fantastic. I mean, at the moment, we, there's some delta on some previous constructions we moved from my collision resistance to target collision resistance. We get some nice properties there. But yes, I would like to talk with you offline. I'm also, very supportive of pursuing this. I think before we adopted, though, like, NCFRG as a work as a research group item, I would like to see, like, the engineering work done to prove that for some use case, this is actually better than, non batch signing. I suggested to you the other day, we can look at rough time is one thing because rough time already builds this in. What was the other thing? And then also, Let's get a sense of whether this actually makes a difference for post quantum signatures. So I I took I I I talked to some random people at my company, they seem to think that we're comfortable with like, the the signatures that we have for the handshake, like, they're not that much slower. They're just really big. That's the main problem. And I don't know if this really solves that problem. Absolutely. If, if anybody has any a juicy, use cases, then also, yeah, you know, please, we want use cases as well to to motivate this work. Great. Thank you very much. And thanks everyone for attending. I think just very quickly one thing we forgot to mention is One small comment, present. Grants for a for the nearest future. We've got 2 drafts. So the the OP draft and, the restructure draft. That are going to be"
  },
  {
    "startTime": "01:30:01",
    "text": "purposes, in the nearest future. And we are going to start resource group plus calls for opaque and for CPACE really soon. You've got a lot of great reviews from the crypto panel So we hope to start the side rule plus calls really soon. Please try to be involved in this because we really need your commands. That's all. Thank you. Thanks, Jonas Love, and thanks everyone for attending. See you next time. Session. It's"
  }
]
