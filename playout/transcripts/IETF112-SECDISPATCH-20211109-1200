[
  {
    "startTime": "00:00:36",
    "text": "all right i think we're at the top of the hour and why don't we get started uh good morning good afternoon good evening uh kind of everyone and welcome to the security area advisory group meeting uh this is uh session one that we're holding now my name is roman de nilo i'm one of the sekidi sec and my cosec id is ben kadek then if you want to turn on your video all right uh if we can go next slide please so two things we want to talk about before we really get started but are the slides flipping or is this an artifact ah perfect thanks so there's two things we wanted to talk about of course we have first the notewell which covers the various policies and procedures related to how we do work here at the ietf if we could flip to the next slide but in particular what we want to do is call out uh one of those things from the notewell which is our code of conduct and so this is really a reminder as documented in rfc 7154 that we extend courtesy and respect to our colleagues that when we talk about things we focus on the technical bits of"
  },
  {
    "startTime": "00:02:00",
    "text": "it not necessarily the messenger and the particular individuals involved so let's not uh kind of focus on the person let's focus on the tech itself and just a reminder that we're all coming actually from different contexts different kind of perspectives so remember that assumptions about what operational environments or the technical environments may actually be different than what you consider that particular kind of environment so please do give the benefit of the doubt to different participants next slide please so you'll notice uh in the agenda here and on the on the 112 schedule that we're doing something different than what we usually do we like to have sex dispatch early in the week we like to have sag a little bit later in this week but to accommodate different kind of presenters we're doing a we're doing a hybrid here so a split session so the first hour of this meeting is going to be focused on sag the second hour is going to be sec dispatch and on thursday we're also going to split the time uh sag will take the first hour and if sec dispatch needs it we're gonna we're gonna have them in the in the second uh in the second hour so the agenda roughly kind of splits like this where uh today in continuing largely on the conversation we had in the last ietf meeting about what post-quantum cartography agility will look like last time we talked a little bit about what we would be doing in the ietf uh this time around uh graciously nist uh specifically dustin moody who's the post quantum cryptography project lead at nist has agreed to come here to tell us about where that kind of project stands and we have some time from some open mic and then on thursday we're going to do all the different administrative kind of activities talk about working group reports the ad reports and other topics but thank you of course to the working group chairs who have set updates uh preparing for that this morning uh we're not gonna do any agenda bashing here we're gonna save that flexibility for thursday you know today we just really are gonna focus on our invited uh on our invited talk so uh kind of with"
  },
  {
    "startTime": "00:04:00",
    "text": "no further ado ben uh unless you would like to add something let's kind of move over to dustin okay thanks so much uh the floor is yours dustin we're gonna flip to your slides now okay thank you very much i appreciate the invitation to come and speak here to the itf um hopefully you're seeing and hearing me not seen my slides yet hopefully you'll get them in a moment so i plan to talk for about 30 minutes or so and leave plenty of time for questions at the end so i know that there's lots of things you might want to ask so again yeah i'm dustin moody from nist and nist as you probably are very well aware is leading a post quantum cryptography standardization project which we're nearing the end of and that's what i will be speaking about today next slide so the motivation is probably pretty clear um it's well known that there are quantum computers that are actively being researched and built that if they are constructed to a large size where they can do pretty good computations that these quantum computers would have a noticeable impact on cryptography and we've seen in the press over the past few years progress being made there are certainly still challenges to be overcome it's not an easy problem but quantum computers if they are built they will be sufficiently powerful to give us computational power for certain problems it won't solve every problem that we want to tackle but they would give us a lot of very good applications in science okay let's go to the next slide so at nist the the threat to cryptography is what we're focused on it's well known that shore's algorithm it's been known for a couple of decades would break the public key crypto algorithms that we use today commonly that includes algorithms like rsa ecdsa"
  },
  {
    "startTime": "00:06:02",
    "text": "the elliptic curve diffie-hellman finite field dsa and these are standardized in three of our documents at nist s p 800 56 a and b which deal with key establishment and public key encryption and then fips 186 which is our digital signature standard so if a quantum computer comes online it would completely break those standards we need to completely just remove them and get new algorithms so the threat is to public key cryptography the most and that's where the focus of post-quantum cryptography is a quantum computers would also be able to impact symmetric key crypto so we have various symmetric algorithms like aes and sha-2 and sha-3 but the effect would be a lot less dramatic grover's algorithm would give you a polynomial speed up not an exponential speed up but we could handle that by increasing key size using longer output from the hash functions so we wouldn't need to completely scrap the algorithms we just need to change how we use them and that's that's easier to handle than coming out with new algorithms so next slide so there are many names given to this area sometimes it's post quantum cryptography it's also called quantum resistant cryptography quantum safe cryptography but the idea is that we're looking for cryptosystems which will be implemented on our current classical computers because that will still be what everyone is mostly using but they need to provide security against attacks from both classical and quantum computers now quantum computers there is progress being made but they're not yet here so we may ask the question you know why do we need to worry about this now and there's a a simple explanation from dr michele mosca that explains why there's a threat now even"
  },
  {
    "startTime": "00:08:00",
    "text": "if we do not yet have a large-scale quantum computer and the idea is kind of there's a there's a harvest now decrypt later threat that your your adversary could be copying down your your data and it's encrypted and you can't read it right now but he will wait until there's a quantum computer that comes along and then he'll be able to get access to it and this little equation if x plus y is greater than z helps you measure that risk um if x is how long you need your your data to be protected and y is how long until we have uh post-quantum algorithms standardized and widely implemented and adopted and if z is how long until there's a quantum computer well if x plus y is greater than z they will decrypt your data and you will not be providing the x years of information security that you're hoping for so you can put in what x and y and z are for your particular organization your application to see if you could be at risk already being in standards we know that y is not as small as we might hope you know y is more likely going to be 10 15 years to get good standardization adopted we've seen that in the past with crypto transitions x will depend on your organization the application could be due to regulations where you've got to keep health information private or you're protecting financial data if you're worried about national security x could be really long like 30 years so it's important to have an idea of kind of what what z might be then so if we go to the next slide there are various experts around the world who were surveyed in this quantum threat timeline report that was done by the global risk institute that tried to give give an idea of when quantum computers that would be big enough to break public key cryptography"
  },
  {
    "startTime": "00:10:01",
    "text": "could be around of course it's an open research question so nobody knows for sure but they they surveyed 44 experts in the field and asked them what is the likelihood in five years in 10 years and 15 years that we would have a big enough quantum computer that would break the public key crypto that we use this chart is kind of it summarizes their findings although you need to go to the the full report to see all the details you can see in five years a majority of the those surveyed thought there was a pretty little risk less than one percent or at the most maybe five percent if you start going down further at 10 years 15 years at 15 years we start to see that there's almost half of the people surveyed we're starting to give a fairly significant probability that we could have a quantum computer so in terms of managing risk we can see that maybe 10 years 15 years and in their opinion certainly by 20 years this is something we need to be working on today and seriously considering next slide so to just be clear uh sometimes when we're talking about cryptography and quantum there's another side of the coin that that is mentioned and i want to make sure that you're clear that that's not what i'm talking about today um quantum cryptography uh also the term qkd or quantum key distribution is using quantum technology to build cryptosystems and there's some pretty interesting things that can be done here you can actually get theoretically unconditional security that's guaranteed by the laws of physics there are some important applications where this might be the right solution but this is unlikely to to be what is widely used on the internet the way it is right now it's not very scalable it's very expensive you need"
  },
  {
    "startTime": "00:12:01",
    "text": "special hardware you can do encryption but not authentication so this is not what our focus is is on right now in this project we do have people at nist looking at this and other researchers around the world are as well but our post quantum cryptography project is on looking for classical cryptosystems to replace the ones that would be broken go ahead next slide so nist has been looking at this for over a decade but it was around 2016 that we began to take some concrete steps um in this project to get standardization happening so we first published a report and announced that we would be doing a large international competition-like process similar to what we've done in the past with aes and with shaw three so we put out a call for proposals we put out our evaluation criteria and then we gave submitters plenty of time to design their algorithms we received 82 different algorithms in which 69 met our requirements and were accepted into the first round and since then we proceeded through a number of rounds where during the round that lasted uh about a year a year and a half we would evaluate the candidates internally we would look at them as well as the cryptographic community around the world as well looking at their security their performance and so on and so forth at the end of each round we would select the most promising ones to to move on for further study so we went to a second round and then we went to a third round which is where we are now at the end of each round we issued a report explaining our decision and in each round we also held a workshop where submitters could come and talk about their algorithms and other research that's relevant could be presented so i'll talk about this in more detail as we go on but this is kind of the overall timeline of our project in the past few years"
  },
  {
    "startTime": "00:14:01",
    "text": "so the criteria that we announced we would be using to judge these algorithms number one is security we need these to be secure against both classical and quantum attacks and security is the most important criteria um that said it's not always easy to measure security in a concrete way especially against quantum attacks quantum computers are not yet built we don't know how expensive they'll be to run we do not know how fast they will be to run so we have to make some guesses and some extrapolations and there's there's some uncertainty built into that we designed five security categories so that submitters could give us parameters to tell us what how much security their algorithms would be providing with certain parameter sets and we related this back to the the amount of resources it would take to break some of our symmetric key algorithms um kind of an unusual way to define it but this is kind of the what we came up with at the time level one meant that your algorithm was providing as much security it would take as many resources to break it as it would take resources to break aes doing an exhaustive key search and then we went on from there levels two three four and five we think levels one two and three should provide enough security for the next several decades and we have levels four and five for very high secure needs besides security uh performance is obviously very important if things are too slow or too large we won't be able to use them so we would measure performance in a variety of different platforms both on software hardware small devices and so we could see the comparisons between the different algorithms as we'll see there's not one silver bullet that is just magically the best in in all different categories"
  },
  {
    "startTime": "00:16:02",
    "text": "we also looked at or we also asked that submitters talk about other properties and that we would evaluate the algorithms on these as well as much as possible we hope for a drop in replacement we want perfect forward secrecy resistance to side channel attacks the simpler the better it's easier to analyze and less likely to make mistakes when implementing things like that okay next slide so we had submissions from around the world there was several hundred submitters that were involved with the the algorithms and if you put them on a map they were from all six continents and over 25 different countries around the globe so that was nice to see that we have a worldwide effort going on here next slide and summarizing the first round so we had a lot of algorithms that we we published on our website we put their specs up we had the code there so you could download and implement them there was a lot of schemes that were very quickly attacked cryptographers love trying to break new things and of the 69 i would say in the first month or two probably about 15 to 20 of them had attacks that severely weakened or broke a lot of those schemes about 15 to 20 of them we also noticed there was a lot of very similar ideas in what was submitted to us particularly in looking at lattice chems so we asked for public key digital signatures and the other category we asked for was public key encryption or equivalently key encapsulation mechanisms and in that table down in the bottom right you can see uh kind of the categories of the type of algorithm as well as the mathematical primitive that was based on and we had the most that were based on lattices and codes for chems"
  },
  {
    "startTime": "00:18:00",
    "text": "we held a workshop where the submission teams were invited to come and present their algorithms had a lot of discussion there a lot of good attendance there that was held jointly with the pq crypto workshop we established an online message forum called the pqc forum where we could keep track of comments about the algorithms people could ask questions discuss research results that forum continues today and is an active area where there's a lot of discussion uh going on there and in the first round we started to to get research and the first performance numbers as well so we could we could start evaluating these schemes after a year we made some hard decisions and we selected 26 of the schemes to move on into the second round so next slide also going on into the second round we encourage some submit some submissions to merge that were very similar and we saw some ford four uh different mergers continued on and they all advanced into the second round the idea with the second round was that we wanted to maintain the diversity of algorithms but select the ones that were most promising so we still had a number of lattice-based code based multivariate signatures and so on and so forth the second round was a lot like the first cryptanalysis continued some algorithms continued to get broken we held a workshop more research after 18 months we had 15 submissions that we picked to move on into the third round and we let we can go ahead and yeah next yeah we can go to the next slide so there are some tough decisions here uh if you've been following the process you can see some of the submissions that were on the left now these were the round two algorithms and we maintain the diversity going on into round three showing our our categories for encryption in chems and for"
  },
  {
    "startTime": "00:20:00",
    "text": "signatures they're kind of color coded according to the mathematical primitive they're based on so you can see that diversity was preserved and in looking to see how we're going to make our selections this could kind of help try and help provide some understanding as to how we'll do that where we try and again keep algorithm diversity but select only the the ones that appear to be the best of each type okay let's move on to the next slide so currently we are in the third round we've selected seven finalists um and eight alternates the finalists are the algorithms that we feel are most likely to be ready for standardization at the end of the third round and would fit the most uh general purpose applications the alternates we are still interested in but they would most likely need a fourth round of study some of these were selected as kind of backup candidates if there were research results or for certain performance applications some of these algorithms were suited for uh the performance was tailored that they might be appropriate for one application but maybe not as is good for general purpose use uh the chem finals that we had were kyber and true saber classic mcleese and the signature finalists were dilithium falcon and rainbow and then the alternates are listed there as well so we go to the next slide so if we look at the lattice-based chems three of them were selected as finalists uh kyber saber and entrue all three are good all-around candidates their key sizes are pretty good their efficiency and implementation is quite good kyber and saber a little bit more similar entries based on a slightly different problem but all three we selected as finalists uh we had n true prime as an alternate and frodo cam was also an alternate frodo cam made a a"
  },
  {
    "startTime": "00:22:00",
    "text": "security performance trade-off where they were a little bit more security they did not use structured lattices so the key sizes are a little bit bigger performance is a little bit smaller but potentially it provides a more conservative security option okay next slide in looking at the remainder of the chems three of them were based on codes classic mclease which if you've been in cryptography you've heard that before it's been around for a long time it was selected as a finalist very large public keys very small ciphertext very efficient for some applications that would work well a bike in hqc introduced some structure to get the public keys down a little bit smaller we're very interested in having another general purpose cam based on codes bike and htc look like good candidates so we will likely go to the fourth round to see if if either of them will be standardized and then we have psych which is based on isogenies of elliptic curves very small keys but it's a little bit slower than all of its competitors uh next slide so here's a couple of charts just to show you kind of uh the different key sizes that we're we're looking at and to some extent this is informed by the the mathematical primitive that they're based on i do note that on the axis these are logarithmically scaled you can see that the orange one psych has the smallest key sizes and then the lattice-based algorithms and the code-based algorithms and then frodo that blue dot up in kind of the top is up there and the gray dot in the far right is classic mclease with very large public key very small cipher ciphertext okay next slide if we want to look at the efficiency this is just to give you an idea of the order of magnitude performance will"
  },
  {
    "startTime": "00:24:00",
    "text": "be based on you know what what platform you're implementing it on again note that the cycle counts on the left here are in a logarithmic scale and you can see that some of the best performance numbers we're getting from our our lattice cams kyber entry saber entry prime and then you can see the code base ones a little bit slower and then psych has slower key gen and caps and d caps than a lot of the the other algorithms it's competing against okay next slide so if we look at the signatures we have two lattice-based signatures dilithium and falcon again both are very balanced they're very efficient they were selected as finalists we had two signatures that were based on symmetric key primitives we have sphinx plus which is very stable its security is very well understood because it's based on the security of hash functions it's a little bit larger and slower than dilithium and falcon picnic a very novel design so we're still looking at that and then we had rainbow and gems that were multivariate signatures they both tend to have large public keys very small signatures pretty efficient at verifying in particular however during the third round there's been some crypt analysis results that have attacked both rainbow and gems and call into question their security for both those algorithms if we go to the next slide we can look at a couple charts to see kind of the sizes that we're looking at for them again logarithmic scale for the axes you can see that picnic and sphinx plus have very small public keys very large signatures the gray multivariate very large public keys small signatures and the lattices kind of middle ground where they have key sizes and signature sizes on the"
  },
  {
    "startTime": "00:26:00",
    "text": "order of a thousand bytes roughly next slide if we look at a chart to look at performance again a logarithmic scale in the cycle count we can see that each of these algorithms has different kind of performance areas where they tend to little do a little bit better some have better key gen some have a slower key gen some verify very much faster so we can see kind of different comparisons there okay next slide so our timeline we have been in the third la third round for over a year and it will end roughly the end of this year start of next year uh right now it's looking probably more likely it will be the beginning of 2022 and that is when we will announce which finalist algorithms that we will standardize we've also previously mentioned we may potentially standardize sphinx plus even though it's an alternate um because we want to make sure we have more than one signature algorithm and we had dilithium in falcon we expect to choose one of those algorithms the multivariate signatures we had some attacks on them so that is why that might be something we do again we we want these algorithms to include uh are these these algorithms that we announced to include solutions that can be used by most applications so that's what we're targeting at the very first especially we'll also issue a report to explain our decisions and we will announce any candidates that we want to advance to a fourth round to continue to study in a fourth round for potential standardization these algorithms that we announce at the end of the third round we will work with the submission teams to create draft standards that we will put out within a probably ought to take us a year or so to get public comment"
  },
  {
    "startTime": "00:28:01",
    "text": "we will address those comments and hope to get the first set finalize published by 2024. okay next slide so i talked about the signatures um we have a lot more chems and different types of chems in signatures we have a smaller number so we're going to introduce at the end of the third round what we're calling an on-ramp for new signatures we will issue a new call for signatures that we want designers to design for the future uh we'll we'll announce a submission deadline we'll give designers probably a six months to a year this will be much smaller scoped project than what we announced five years ago we're not looking to get 60 70 80 new algorithms we want a very small number the main reason is to diversify our signature portfolio we're happy with the general purpose digital signature schemes that are based on structured lattices that would be uh dilithium and falcon but we also want another general purpose algorithm that's not based on structured lattices the other ones that we are still in the third round they're a little bit bigger or slower in some areas so that's our primary target that we're looking for you might be convinced to be interested in some signature schemes that target certain applications for me for example if there's a scheme that has very short signatures that could be something we're interested in of course the more mature the scheme the better and we'll decide when we see what gets sent in which if any of the schemes that we want attention focused on so next slide how are we making our decisions well right now we're already meeting to begin our process to select the algorithms we're meeting frequently we're comparing the algorithms we're taking a detailed look again at their specifications what's happened during the third round new research"
  },
  {
    "startTime": "00:30:00",
    "text": "results new performance benchmarks but we're trying to just follow the evaluation criteria that we announced we we said security was number one then performance and then some of these other algorithm and implementation characteristics for the lattice cams our main decision will be selecting an algorithm from kyber and true and saber all three are based on structured lattices have similar performance there are some differences and that's what's making this a hard decision for lattice signatures our main decision will be looking at dilithium and falcon um our other choices uh they'll mostly be made on an individual basis um if we standardize classic mcleese it's kind of competing against itself and so on with the other algorithms next slide one issue that's proven to be very important is patent and ipr issues and this is a very complicated area our team is mostly made up of mathematicians computer scientists we are not lawyers so we don't understand all the ip issues but we acknowledge that the impact of ip has can be very very important on adoption of these algorithms each of the algorithms their submission teams gave us sign statements indicating any patents that they had on the algorithms but there's an issue that's arisen with patents from third parties that are not the designers of the algorithms nist is actively engaging to resolve the known issues that we're aware of when we have something concrete we will share it we want to make very clear though it may not be possible for us to resolve all the ip issues we can't control what outside parties do completely and ip is a legal issue and it's not to be decided based on uh you know what what i think is a mathematician so in light of the above um"
  },
  {
    "startTime": "00:32:00",
    "text": "we think that this the discussion is best focused on the impact of ip how will that affect adoption and how should we factor that into our decision making process we very much want to select algorithms that will be widely implemented and adopted and we know that having any patent or ip significantly affects that we want to have you know standards that are freely available to everyone to use next slide so there has been some work done already on stateful hash based signatures the ietf has two rfcs that standardized xmss and lms based on those nist also issued a standard sp 800 208 that has stateful hash based signatures want to caution these are not general purpose digital signatures you have to carefully manage the state otherwise you can completely blow your security so there's both all of these documents include guidance on how to safely do this and which applications this is appropriate for but wanted to make sure you're aware of hash based signatures okay next slide um as we look towards the transition one thing that's being discussed is a a hybrid mode and that's where you use a classical algorithm and a post quantum algorithm in some combination where you're relying on the security of both algorithms and this is a potential approach for migration that many people seem to be interested in a nist allows this in one of our documents you can combine a shared secret derived from a current nist standardized algorithm with a shared secret derived from some other way i.e a post quantum algorithm you can catenate them and run them through a kdf and you can still get phipps approval on on that process as explained in that document so we know that there's going to be can probably continue discussion on the best way to implement um"
  },
  {
    "startTime": "00:34:02",
    "text": "any mode where you're combining algorithms this hasn't really taken a position on uh the best way of how to do this uh we think it it certainly is a an option to consider you take a performance hit but it's a it's a perhaps a good way to transition into the future and getting used to these post-quantum crypto algorithms so let's go to the next slide nist will issue guidance on the transition to post quantum algorithms we don't yet have algorithms announced so we cannot give that guidance yet um but we we've previously given crypto transition advice and we will do so here um so let's go to the next slide uh in working with the nccoe the national cyber security of excellence we also have a project that's focusing on the transition and migration to pqc so this product is being run by uh some of our colleagues in the nccoe um but we're participating with them in this process they've held a workshop they finalized their pro projects you can go to their website most recently they announced a or they published a federal register notice where you can join the the project and participate in this process so i'd encourage you to go to the project webpage if you're interested in learning they also have white papers on how an organization can start preparing for this transition and this is an important effort to help us get ready what we can do now before the standards are announced and before the final standard is published okay next slide these are some of the things that organizations can do now it mostly involves taking a look at the cryptography you're using now seeing what public key algorithms you have looking at how you can transition out of them how you can swap in new algorithms"
  },
  {
    "startTime": "00:36:01",
    "text": "that's often called crypto agility and then just making sure that you're you're making your staff is aware of this you've got someone tasked with having a plan for making the transition to post quantum algorithms and tracking developments in quantum computers in standardization projects you could test out some of these algorithms to see how will they meet how they fit in your applications but we'd encourage you the sooner you plan ahead and prepare the better it will be for you you'll you'll make less mistakes it'll be less disruptive less expensive as you do that so next slide so in conclusion we're getting near to the end of the the third round we can start to see the light at the end of the tunnel nist is grateful for everybody's efforts the cryptographic community researchers industry who's tested out these algorithms other standards organizations who are working as well to to help create strong post-quantum crypto standards and we'd encourage you to check out our web page sign up for the forum if you haven't yet and you can always contact us directly if you need to so with that i'd be happy to take any questions that you have about our project john please um how many changes or how little changes can we expect from now until the final standards already hopefully in 2024 which is like two three years from now um okay yeah so at the end of each round uh we allowed each of the algorithms that advanced on to make minor tweaks not major redesign so we expected at the end of the third round you know if we select an algorithm we would work with the team and allow them to make any minor tweaks from things they learned during the third rounds but nothing major no no big redesigns so i"
  },
  {
    "startTime": "00:38:00",
    "text": "would say that the type of changes would be similar to what you saw at the end of each round when teams made tweaks watson i think you're up next mad cover um so one of the things that happened with elliptic curve photography is there was a very long gap between new standardization and adoption and that was partially due to ipr concerns and and some operating systems really didn't have any support for quite a long time because of these ipr concerns as well as performance issues and then when further developments resolve those performance issues this didn't respond and it's not until two years really that nist has said that oh yeah we can do that we are going to standardize faster with the curves and my concern is has this is with post quantum where we have possibilities especially in signatures of faster and better algorithms coming out is this going to be more responsive or is it going to be a situation where we're going to be stuck for a while with things that maybe we can't do everywhere and really need to because nist isn't isn't paying attention anymore to what's going on they say okay we've got the got the spectrum we're done we expect our post quantum work to continue on into the future at the end of the third round we're announcing these algorithms we will get a first standard out but the project will continue on there's going to be a fourth round there's going to be the on-ramp for signatures um so to the best of my knowledge yes we will be we will be looking for"
  },
  {
    "startTime": "00:40:01",
    "text": "the future research and being proactive and if we need to you know make changes or standardize new things that we would be responsive i feel good yeah great talk i've got to ask about threshold um can any of these be adapted to a threshold version um it's possible that some of them could as they currently are in the specs right now i'm not aware that any of them have a threshold implementation um and that's not our our primary focus we first want to just get an algorithm standardized so that people will have it begin to get using and and that could be future work where there could be threshold versions of these done are any of them particularly suited where they be able to they look like they could allow that some of the lattice-based ones potentially i think uh if i'm remembering lima was a first-round ladder space candidate and it talked about how it could be done in a threshold way potentially so that would be where i would guess is a lot of space ones might have the easiest way to do that but i'm not currently aware that anyone has has done it yet okay ecker you're at the top of the queue yeah thanks thanks for talking appreciated um so um i noticed that i noticed that uh you're standardizing chems and um but a lot of our protocols really want nike instead um so um i know there's sort of a dearth of algorithms there but is this planning to do anything to try to like foster that work well back at the beginning when we were"
  },
  {
    "startTime": "00:42:00",
    "text": "first starting this project that's what we were hoping for was kind of a direct analog of the diffie-hellman um looking at the submissions that came in there was none that really provided that at the time i think there have been some ideas that are out there right now that could potentially do that but i think our feeling is we think that you can use the chems and you can modify them to to get what you need done still if we do see a really good one coming in in the future we could work on that but we don't have any current plans at the moment to focus on that oh okay it is true you can use the chemist for that but it is not convenient so it would much more convenient doesn't it that looks like it looked much more like to filming yep we understand yeah hi uh i i guess partly repeating what said earlier i mean encumbered outcomes would be a really bad outcome i think for this um i know you can't control that also though i think the thing you can control are the number of options um so i know that you're you're not liable to pick one single algorithm to declare a winner but i hope for each algorithm you don't declare 75 different options and then that would cause a lot of work in an organization like the itf where people then have to decide which of the 15 options that they winnow down to 10 and then to one and so on so i'd encourage you to extend possible to reduce the number of outcomes to the absolute minimum you can thank you for that suggestion yeah we've we've heard that feedback from industry as well and that's something we we plan on doing we will standardize more than one but we want to keep it as small as possible yes hi uh ben schwartz of google i uh i wanted to ask if you uh and and the the whole group at nist had given much thought"
  },
  {
    "startTime": "00:44:00",
    "text": "to combine schemes that that combined one of these uh one of these signature algorithms with a with a merkle algorithm to uh to amortize the signing costs there's a draft in the tls working group right now adopted draft on batch signing using merkle trees to to share the cost of a signature across a large number of handshakes i wonder if that influences any of your thinking yeah that that would fall into our third evaluation criteria kind of the algorithm and implementation specifics yeah we are aware of kind of research being done in that way and if an algorithm allows for you know batch signing in a particularly efficient way that would be a plus for it so we do try and be aware of that and factor it in when we're we're going to select something yes okay yeah just because in general it seems like slow signature schemes with small signatures um could easily be combined but sort of trivially with the with the old-fashioned merkle signature trees for in that situation yep and that would be a good way to get some algorithms that might be a little more cumbersome to be able to be used yes okay great thanks hi uh ben kadek so there was a topic that came up in the chat a couple times about uh sort of key sizes and um i know you had i think a little bit of of data in the charts about key sizes but like is there some minimum expansion factor that we're going to hit when in key size when we go to the post quantum algorithms well the main ones the finalists that we're looking at right now the structured lattice ones for chems they're roughly about for category one a thousand bytes"
  },
  {
    "startTime": "00:46:02",
    "text": "that's the smallest we're seeing for good general purpose algorithms psyc does have smaller keys that are a couple hundred bytes but then its performance is an order of magnitude slower i'm looking at the signatures falcon is smaller than dilithium for sure it's less than a thousand bytes die lithium's more like 1500 2 000 bytes for category one so based on what we have that's the best we can kind of do coming out of round three at this point sure are we going to have to take what we can get at these tips thanks and i guess mike you are next to the cube i remember at the florida iteration of this conference there was a proposal around hybrid schemes it was sort of flippant but the proposal was we were able to achieve a security level by combining two smaller faster things into a single primitive do you remember that idea floating has there been any any traction like did that idea die i know it was a joke but it sort of seemed maybe worth pursuing um i don't remember that from the florida workshop um [Music] so i'm not remembering it which makes me think there i mean maybe there's been some efforts in that direction but i'm not aware of them so i don't know okay so that's the that's the end of the queue is are there any other questions for dustin"
  },
  {
    "startTime": "00:48:02",
    "text": "i am not seeing anyone join so justin a profound kind of thank you for for joining us and giving us an update for what nist is doing this is kind of such an important kind of activity for our future roadmap so uh so again we if we have any other questions we may kind of follow up with you but again kind of thanks thanks so much and have a good rest of your day okay thank you all right so we have ended early we have about 10 minutes officially uh kind of on the schedule before we pivot to sec dispatch so opening the mic uh kind of open mic for sag topics and if we after we drain that cue we'll just gonna pivot on to starting with the sec dispatch meeting would anyone like to join the queue for open mic sag florence please hi um flo from uk ncsc um i was wondering kind of picking up on something we discussed at the last sag meeting um there was some chat around um kind of coordination of post quantum work in the itf um and it seems on the mailing list that that's kind of been turned into the discussion around the pq maintenance group which sounds great um but i was wondering if there's any plan for coordinating the post quantum work between different groups um so the stuff that's going on in yeah different working groups uh i i would say from at least kind of the ad perspective we are just informally doing that formal class of kind of coordination if we end up with a working group to do that we would hope that that entity would roughly kind of take the lead so that class of coordination is kind of on you know in the kind of thinking but right now this is roughly just informally done the three places at least in sec that we have actively chartered items are lamps tls and ipsec i'm sure more work is needed"
  },
  {
    "startTime": "00:50:02",
    "text": "so do you think some of that coordination could end up taking place in the kind of pq agility pq maintenance group or is that not on the cards for that group thank you i i think that's up to what the community actually wants uh the way that post quantum that that particular charter that particular thing we're having on the mailing list kind of talking about uh the that agility work group is largely to catch orphans so it's focused on what are protocols we may need to touch that don't have existing working groups uh so if there is existing work for example kind of in tls or kind of ipsec it's probably going to be handled out of that working group which is not to say you know the topic can't be coordinated but that work is likely going to be in that working group at least as the text is currently written the other thing that that working group that that kind of proposed kind of template working group that we're talking about might have is the ability to kind of generally talk about things before that we know some organic coordination baked in there yes thank you great thanks any anything else related to open mic for sag okay so i'm not hearing anything so we're gonna close the open mic line and metaphorically close the sag portion of this meeting and turn it over to our sec dispatch chairs"
  },
  {
    "startTime": "00:52:00",
    "text": "mohan i'm jumping in here i think you're talking but i at least can't hear you apologies for that hopefully you can hear me now uh so i was just saying it's afternoon here in helsinki so good afternoon uh evening or morning depending on your time zone uh i guess uh roman ben we can just start with sex dispatch we don't have to wait for another eight minutes to uh like meet the hour uh hopefully that's fine yeah absolutely please continue yeah yeah so welcome to sec dispatch at idf 112 and uh we have our ads and my co-chairs richard and kathleen uh will be helping me uh today so uh welcome uh i'm trying to arrange the slides from here so you have already seen the note well once and it still applies it hasn't changed you're still in an iedf meeting hopefully you're familiar with most of the bcps but if not you can have a look at them they are listed here on the slide please yeah go go and have a look at them is it's anyways good to refresh your memory once in a while uh perhaps we would like to emphasize the idf code of conduct still applies so you just saw this when roman was presenting the slides earlier but again to re-emphasize have a look at rfc 7154 please respect your colleagues and try to be as impersonal as possible in discussions and yeah we are trying to make the internet better so let's be good to each other while we do that uh so this session is being recorded uh if you were not aware of it so far you"
  },
  {
    "startTime": "00:54:00",
    "text": "have you were being recorded in the last one hour and you will continue to be uh recorded for about uh a little over the next hour as well and headsets are recommended state your name before you speak in the mic queue all right here's some links so very importantly we'll be using the notes dot idf.org for notes and we'll be using the sec dispatch link not the sag link i think my co-chairs will manage the jabber but we will need note takers so i wonder if we have someone helping with the notes already oh i see at least for suck someone was taking notes probably roman but it would be nice if someone can take notes for sex dispatch as well do we have any volunteers i don't see anyone on on the chat i would love to but i think i'll be switching slides from here and managing other things so okay uh thanks uh jonathan so we'll have note takers with that sorted out before we get to the actual agenda here's a quick reminder of uh what is sex dispatch and what do we do so sec dispatch is mostly for recommending the next steps for new work it does not adopt crafts we could direct the new work to an existing working group we could suggest"
  },
  {
    "startTime": "00:56:02",
    "text": "through a buff or not depends on the ads we could also ask our ads to sponsor some documents or then come to the conclusion that ietf should not work on a particular topic uh yeah hopefully you're familiar with these buttons by now i don't think we need to spend much time on on this slide uh fi so before we get to the agenda for sex dispatch there was a draft presented on secure credential transfer in the dispatch uh session earlier this week so yesterday there was a draft on secure credential transfer there was presented and the outcome is on the dispatch uh mailing list kirsty has thankfully given us a heads up so that we can also inform the security community in in this meeting in insect dispatch uh the full outcome is on the mailing list but i anu is uh wanted to provide a heads up that there will probably be a buff at least there was consensus in the dispatch working group to to initiate a buff on this topic and i'm sure like this is of interest to to many of those who are in attending the sect dispatch so do have a look follow what's going on and we might have a buff before or during the next ietf in 2022. with that uh we are now at the agenda for this session uh thankfully we managed to finish the administration before the hour so we are in in good time uh we'll have a presentation from tommy on private access tokens uh another presentation on security and privacy considerations for multicast transports from jake and then we'll one of the chairs will summarize the"
  },
  {
    "startTime": "00:58:00",
    "text": "outcomes at the end of the session uh we already have jonathan again thanks for helping us with the notes we have scribes in place is there anyone who would like to bash the agenda if not in that case i'll share your slides to me yeah actually if i could share my own slides i can just ask to do that that would be great yeah yeah that's fine that's fine and then as a minor agenda bash you know i know that there also is time on thursday so i guess if you know if we're having discussion i assume we have you know probably a little bit more than 25 minutes for each of these talks that we could probably go into sure all right i'm asking to share slides if you could approve that thank you all right good morning good afternoon everyone um so i'm tommy pauley and i'm going to be presenting on behalf of uh several co-authors i have here um i'm from apple i'm also working on this document with chris wood who's at cloudflare johnny yangar from fastly and stephen valdez and scott hendrickson who are at google and this is a document that we are calling private access tokens i sent it out to the list a little bit ago as we publish on the day of the deadline but we want to share that with you here and see what the next step for this type of work is all right so just to give you an idea of what we want to go through first we're going to talk about the motivation for why we think this is important work to do we're going to briefly go over what the protocol architecture is without diving into all of the details if you want to see that read the document"
  },
  {
    "startTime": "01:00:00",
    "text": "and then lastly talk about some deployment considerations before we get on to the dispatching questions all right let's go what problem are we trying to solve here okay so let's tell a little story so servers as we know often use client ip addresses as an identification mechanism and this is because clients generally just tell them hey when i'm connecting to you here's my ip address you can know who i am and this is useful and amongst other things servers can recognize these addresses over time and some of these are useful features like they can use it to rate limit access to some particular resource when that's appropriate and the origin can say oh yeah this is the fifth time i've seen you today or this week um sometimes that's a good thing for example um there are services that want to be able to have oh here's a metered paywall of you get seven free articles and if i can see this you get the free articles i'd like to point out that it's kind of hilarious that wikipedia for their metered paywall site had to come up with an entirely fake newspaper here now there are problems many problems with using ips for tracking like this even if you think this is a kind of a valuable thing to do you have issues where multiple clients can be behind the same ip address quite often and you essentially are sharing fate with everyone who's at your house or at the cafe you're at um you can hit limits but worse than that being able to track the ip address over time is a privacy violation and it's a tracking vector often so there are a lot of great privacy services vpns tor and at apple recently we've released our private relay service which has two proxies in between your client traffic and it's all about trying to improve client privacy by having your client identification be something that's"
  },
  {
    "startTime": "01:02:00",
    "text": "explicit and not just passively gathered up by looking your ip addresses but as you'll note this does take away some of the capabilities of origins to do correct rate limiting um or you know any type of inference of client state because they don't know the ip address anymore they don't know how to do this and this is what the entire ecosystem was built upon for a lot of things so private access tokens is about you know figuring out how do we let things like rate limits work regardless of ip address but we don't want to do this by introducing a new stable identifier that's going to hurt privacy we want something that is privacy preserving but lets you prove something about your access patterns and so where is this useful it's certainly not for everything um this is about cases where you have anonymous access that you want to be able to prove some limited client state to do things like per origin rate limiting and this is not for any case where you're going to log in anyway that's a much stronger identity and that should be solved elsewhere and help us think about this i want to split kind of the world into three categories here all you on one hand in this green box on the left we have you know things are truly anonymous access that don't need any state they don't it's inappropriate for them to have any client um identifiers i just want to be able to read a wikipedia article i just want to use the search engine public resources no need to track me on that on the right side in the orange box we have things that legitimately should have authenticated access if i'm going to upload something to my own social media account you should probably know who i am and i should probably log in um so the question is you know there's this category in the middle which is something where i'm coming in with an anonymous access i haven't logged into anything but you do want still some minimal per client states such as rate"
  },
  {
    "startTime": "01:04:00",
    "text": "limiting um as we mentioned before this can be accessing some being a bit more privileged resource like being able to read a newspaper article that otherwise would require you to pay but they're trying to let you see some things but also this account this applies for account creation where something wants to make sure that you're not having a thousand different accesses from a bot trying to create a thousand new accounts or even um if i'm doing log into a site where i'm anonymous before because i haven't logged in yet um a failed login is something that you probably want to rate limit so that someone's not just trying to brute force a password in and so these are cases that have been built up around doing ip address rate limiting today they exist and they're going to need to do something as connections get more and more private and the concern is that if there is not a good privacy preserving solution use cases that are in that middle box are going to have to move to one side or the other either they're going to say great we're just going to go totally open and that's fine but we're concerned that they're going to be more things also that would move more towards the right to say okay if you want to read this now you need to sign in with this major social media network to be able to get into something or hey if you want to create an account with us or log in you need to bootstrap it with some other account of some major service that we trust and that's going to be worse and worse for privacy so we think that there is a need to solve this and private access tokens is about solving this case essentially it allows the client to prove to an origin that is performed fewer than n accesses in a time window and that's all it's going to prove and the key thing that we have as a requirement in here is we want to make sure that no entity in the system can correlate user identity with their browsing history because of this all right so how does the protocol work"
  },
  {
    "startTime": "01:06:00",
    "text": "there are two parts to it there's a token challenge request and then there is an issuance protocol so the challenging request quite simple um what we're proposing here is just have a new http authentication method you get a challenge um is actually asking the client hey you know you're trying to do some privileged operation you're trying to create a new account um you're trying to read the newspaper article you need a token to get this or i'll otherwise you know you'll fall back to something like getting the captcha or having to do some other type of proof so the oregon origins can challenge for this um and then the clients will asynchronously fetch an unlinkable token that's specific to that origin and then present it and then the origin can validate that without having to do any more work this is a public verifiable thing so how do we actually get to the tokens so in order to go through this i want to kind of build up why we have the architecture we do based on showing you kind of process of elimination that you can't do some other architectures you know clearly we're not going to be able to just fetch the token from the origin itself because kind of by definition in the assumptions here we either don't want to log into this origin provided a real identity so it can track us or we can't because we don't have an account yet so that's not a good option so we could bootstrap off of something that you could call like a token issuer and you know you could imagine i could have some issuer that can validate me can know that i'm a real device or have an account that i have and then on my behalf get a token for that origin that i can then present to the origin i want to go to and this would you know technically work however um you know this doesn't meet the goals we have for privacy because this issuer would be in too privileged of a position because it would be able to identify the clients and track their state"
  },
  {
    "startTime": "01:08:00",
    "text": "potentially as well as knowing all the origins that they're going to and this would allow them to learn the browsing history and be a bad privacy story all right so we end up with a bit more split model um if you've gone to oblivious http this type of split is not the first time you've seen this so the model that we're proposing here is that you have a separated mediator and an issuer and a mediator is something that's chosen by a client that is able to authenticate the client and validate who the client is but is not able to see what they're trying to access but it helps just kind of mediate this entire transaction and represent their validity it will forward blinded requests for tokens to an issuer issuers are chosen by origins as entities that can vend out tokens on their behalf and enforce the policies that they want such as you get no more than 100 tokens per hour or even five tokens per week so overall the the protocol looks like you have this end-to-end challenge and response between the client and the origin and in between you can have these little loops between the client mediator and issuer to actually grab new tokens now the clients never tell the mediator what the actual origin name is they only tell some pseudonym an anonymous origin the actual origin name is encrypted from the client to the issuers the mediators are responsible for keeping a count of tokens that are issued to each client for a specific anonymous origin within a window and the window is something that the issuer can define to essentially say hey i care about rate limiting things within an hour like you could only try to create an account five times in an hour or you can only have five failed logins in an hour"
  },
  {
    "startTime": "01:10:00",
    "text": "and that policy window is given back um it's told to the meteor to help the whole system work like this there are um i just want to briefly mention some of the cryptographic dependencies that we have for the protocol for the origins it's very very simple they just have to do a verification of an rsa blind signature token they don't have to do any double spend prevention they can just validate that the token looks good on the issuance side the system of the client media and issuer do need to do a bit more work they use rsa bind signatures for the tokens themselves the client uses hpke to encrypt the actual origin name to the issuer and then the system also uses a blind it diffie-hellman with a schnorr proof of knowledge to help the mediator prove that the client is not lying about the mapping between its anonymous origin and the actual origin name chris will be talking about that in cfrg for the crypto details if you're interested all right so we've talked about why we think this is important to solve we've talked about what the basic architecture is but um the last thing i want to hit on which is i think one of the most interesting parts is the kind of deployment model so i mentioned some of this already but the clients are responsible for choosing their trusted mediators these are people who are they're willing to um log in with or who have some notion of client identity anyway that they know isn't going to learn about their browsing history but can help them as they want to get tokens and this could be based on a number of different things that's not specified in the document it could be based on a device level built-in hardware assert it could be some verified account that you have with a service some two-factor thing origins are responsible for choosing"
  },
  {
    "startTime": "01:12:00",
    "text": "trusted issuers and one of the assumptions of the architecture is that each issuer should be serving multiple origins and that's required so that the mediator doesn't learn based on going into an issue or exactly what website you're going to and this would work really well for issuers to be existing cvns or web hosting services or any kind of security capture service that you have it could be a number of different things but different entities that would be representing origins and then the system does rely on the fact that there needs to be some level of mutual trust between the entities running mediators and entities running issuers for privacy reasons they should be different entities and the client can essentially you know do some level of knowledge because they are choosing what media they go through that it's not the same thing as the origin that they're going to or the issuer that they're using but um if trust does break down if meteors start lying or doing letting clients get away with too much then you could imagine that there wouldn't um be good support between mediators and issuers so they do need to live up to a certain standard so one way to look at the architecture is that you have many different clients behind single mediators many different origins behind single issuers and that is kind of what preserves and protects the identities of clients and origins from the other parties in the system so that there's no tracking here um briefly talking about client identity um you know this is something that is determined by the mediator really um the private access tokens architecture does not require just one mechanism for this this is something that we can imagine could evolve but really just needs to be something that the ecosystem agrees is hard to forge um you know it shouldn't just be oh you have this ip address it needs to be"
  },
  {
    "startTime": "01:14:00",
    "text": "something that's relatively um relatively hard to get so one client couldn't just get a thousand different identities very easily but users could definitely legitimately have a few identities um you know i may have different devices that if i'm using device level authentication those would look up like locally different things you could also imagine that different applications or different browsers on a single device could use different accounts as their root of trust but effectively the amount i'd be able to multiply my effect on the system would be maybe one two or three times not thousands of times to be able to fool a system and then lastly the thing i want to touch upon for deployment is you know how we avoid centralization in the system um it's definitely something that we are conscious of and want to avoid um so the mediators and the issuers kind of by definition are entities that are helping represent many different clients and origins so there are fewer mediators and issuers than clients and origins um that that is important but we should also be avoiding letting um the overall ecosystem just have a handful of these that are just kind of the big players that it's impossible to get in it needs to be easy for new mediators and issuers to enter the overall ecosystem and you particularly want to avoid situations where issuers who are kind of working on behalf of origins only allow a specific mediator to essentially force clients to have a particular type of trust in order to be able to use different origins so yeah this is not an easy problem to solve it's the type of thing that is difficult to solve at a protocol level but we believe that you know private access tokens may actually be able to have less centralization problems than other alternatives when you think about the overall ecosystem"
  },
  {
    "startTime": "01:16:01",
    "text": "so looking at some of the kind of comparisons as i mentioned before if we don't have a good solution in this space we'll see and we already do see a lot of things moving towards oh a sign in with blah where blah is some large company and these are things that origins choose to prefer and choose to partner with and they can essentially embed um some relationship with another site um that will be more likely to have a user account already and they can be sharing data on the back end you really don't know what they're doing and this is potentially a big privacy problem and so without an alternative for clients to use if we're trying to move away from captchas and if we have more uh you know private ips and stuff we could see things moving towards this and this being a per origin choice is something that would be really hard to move and we're not going to end up with a situation we're going to have sign in with 50 different services and every little guy is going to get in there another comparison to make is the existing proposals uh like privacy pass um so i'll mention a bit more later but privacy pass is something that allows a client to present a token to an origin that it got from another origin that he was able to get some bag of tokens for that's not origin specific and in this case the origins that are redeeming this can choose to discriminate based on where the tokens came from and so they could also be in a situation where individual origins could choose to prefer major services or choose a small set of token vendors that they trust with private access tokens you know overall yes there is that general risk but it's important to note that the origins don't get to see the mediators they're only doing this through the issuers that they work with um so they can origins cannot individually discriminate based on how"
  },
  {
    "startTime": "01:18:01",
    "text": "the client authenticated they have actually no way of knowing how the client authenticated or what type of device or what type of account they had so it's actually a better privacy thing here now you could have a situation where issuers start rejecting new mediators um since you know meteors and issuers are all kind of all talking to each other they're not really paired in any way they are going to be a kind of a globally known list and this is something that could be publicly reported and publicly audited if there starts to be blockages between any of them now sometimes this could be legitimate like you could have a meteor that starts cheating and lying on behalf of its clients in that case yeah the ecosystem should decide to reject them in the same way that you can stop trusting a given certificate authority if it stops uh behaving correctly around how it signs certificates so this we think is the type of architecture you would end up with and we think with they could have the right incentives to move this forward and i do see someone in the cube but i'll just quickly finish my slides i only have two more so anyway we have the question about where this work should be done i do want to point out that there is a lot of related work within privacy pass so private access tokens is very similar in many ways to privacy pass but it has some key differences from what privacy pass is currently working on first it does involve some per client and per origin state it's not allowing just unlimited access and we believe this is actually something that makes it much more attractive and more useful for cases that want to be able to replace ip address as a rate limiting mechanism if you don't have that ability you still have to fall back on something like ipaddress to allow you to do the right limping these are also tokens that get scoped to a specific origin so you don't have any problems about cross-origin spending or a case where one origin can end up"
  },
  {
    "startTime": "01:20:03",
    "text": "essentially consuming your entire bucket of tokens and you have no more tokens left for other origins that you prefer to use it with it is using a private access token to use a challenge so it's an online protocol and so this doesn't allow you to do the same type of token hoarding that you could with privacy pass in which you could actually get passes from many different clients and just hold them on hold on to them and then spend them and make it look like you're legitimate when you're not and lastly we are defining this as a publicly verifiable token so it doesn't require the origin to do any work of talking to the backend or the original token issuer to prove that it's a valid token so in some ways we think this could be a more generic form and potentially more useful form of privacy privacy pass all right so our question essentially is you know first what do people think but then if this work should be done should it be done as something like extending the work that privacy pass does should it be something short-lived um in a very specific working group or something else and that is all i have php yeah um i like this a lot uh there's just one slight problem i proposed something very similar back in 2003 and verisign has a patent on it so it's not a very problematic patent it was filed march 25th 2003 uh but that is something to bear in mind uh the other thing is that what i was doing at the time was i got very upset about proof of work which i believe is absolutely appalling uh proposal i thought then i think it now"
  },
  {
    "startTime": "01:22:00",
    "text": "and the claims are very broad it does it doesn't it's not just for trusted computing implementations but the main idea was put this in a trusted module on the device and then you can get rid of all your other um requirements you know all your media mediating parties go into the device uh it might be that in the current circumstances um yeah probably the only thing that could verify could do to get some value out of that patent at this point would be to make some agreement with google if you could get google and apple involved they control silicon and they could get it pushed into the hardware yeah and certainly you can imagine the mediator could end up in the hardware of the client itself yeah it would be very very easy thing to add to anything that's got a secure enclave already i mean the reason it didn't happen 10 years ago was that there was another pattern prior to mine that stopped it happening that we could never get access to so yeah sorry well thank you for the heads up on that yeah i i i'll file the ipr or tell uh bert or whatever to do it okay thank you all right ben hi bench work i am so i happen to be one of the chairs of privacy pass yes um i think that this work uh is appropriate for the privacy pass working group okay i would be happy to see it come there"
  },
  {
    "startTime": "01:24:00",
    "text": "i i don't think that there's uh you know any presumption that we would essentially take this work uh as is as a and add it as a new working group item work item uh i think that these use cases are important and that we should think about uh making sure that we satisfy the the use cases that are most prominent here um and that may mean um that may mean changing what the privacy past working group is working on i don't think that we should essentially forge ahead with both this and something that looks like the current privacy pass protocol in parallel i think we should essentially um look at the elements of these look at the requirements and come up with one solution to pursue first at least and let it roll out and get some experience with it before we attempt to do you know another permutation of the same ingredients for a similar purpose right yeah and just speaking to that briefly and you know we have you know both chris wood and steven valdez who are privacy path past authors working on this and i think there is a appetite to um even if long-term some of the use cases could involve different flavors of tokens to make uh a lot of the stuff like the token challenging requesting an issuance compatible and maybe just have oh i want this type of token or i want i have that type of token yeah one thing i think we need to consider is that there are going to be mixed ecosystems if we have if we have multiple of these things no matter whether in parallel or sequentially we're going to end up with mixed ecosystems which means we need a negotiation system a capability negotiation to identify which kinds of"
  },
  {
    "startTime": "01:26:00",
    "text": "tokens both parties possess which means we need to think about the the the entropy leakage the privacy reduction that is revealed by which of these kinds of tokens you actually support or possess at the moment which means we need a unified privacy analysis um i think those are all reasons to take this to the privacy password great that makes sense as an individual i have to say i don't find this as compelling as privacy pass today in particular it seems to me that this architecture has much more complicated uh trust relationships required between the various parties whereas privacy pass manages to achieve it seems to me very similar uh security and performance results with much less reliance on on tricky partial trust relationships between the parties so uh i think that we should try to you know essentially lean toward computational privacy and away from information theoretic privacy to the extent that we're able and privacy pass as currently formulated i think gets us a little closer also i think there's more similarity here than we let on because both of these protocols have certain assumptions baked into them that we are not particularly talking about for example the uh a huge having a huge number of issuers here um would would get you back into the i think the privacy pass problem of uh of essentially leaking one bit of information for every issuer that is or is not um present on the on the client and so that means that they share the they share the same privacy consideration there then uh as much as i hate to disrupt the technical conversation let's end it here and uh for all others who are on the queue"
  },
  {
    "startTime": "01:28:00",
    "text": "so the queue is closed and yeah please keep it brief and we we would most importantly like to hear your feedback on how to dispatch this okay um this is just one thing to reply briefly um you know i i agree that is you know desirable to have something simple with you know simple trust relationships i think the main issue we see with the current privacy passes that it is simple but it also offers um sufficiently uh little information low value to the origins that it's actually not useful for replacing a lot of the cases that would have captchas or would have other types of rate limiting today so we want to just increase the utility all right siobhan hey um [Music] oh don't try talking now is that better yes that's much better okay great turns up my headphones don't work um so yeah we're just wondering it it really is like um the the presentation kind of read like privacy preserving rate limiting but the draft mentioned geofencing as well um and i saw that i thought there was a client hints proposal as well in the github repository um and just wondering like yeah like is there was a reason for that and i guess like just quickly but the other thing was um it seems like there's a centralization concern around especially the mediators so um they might be worth having some discussion"
  },
  {
    "startTime": "01:30:00",
    "text": "about that in the draft yeah um so yes there are in the draft mentions and we believe you know the overall architecture could allow you to have other bits of information other than rate limiting like you essentially you could use this or you really use kind of any privacy pass type ecosystem to sign or verify that oh yeah you came from this country or whatever and you know we know something about the client that way um so there are other bits of information we don't want to get into the complexity here in this talk um what was the second part of your uh just uh comment that we should probably talk about the centralization concerns uh but mediators especially i think yeah yeah and yeah we should discuss that more in the draft um as i mentioned here i think you know largely it does degenerate to a lot of other cases where you know essentially if an origin can know where where your um kind of sign into where your root of trust is coming from then it could similarly end up favoring just a few large actors and you know hopefully we can actually find a way to make this better than those thank you hecker anchor go ahead hi tommy uh thanks for presenting this hello um so i think i mean i'll i'll hit this best question first but i do want to talk about technology for a moment um so i think this isn't quite ready to go um honestly there's a bunch of questions about what the actual problem which you're trying to solve is um i do agree with ben that having this and privacy passed together is like bad news so we should try to figure out you know what the itf is going to do in this spaces um so i think we really first agree get agreement on the use cases um do this at a number of them but quite a complicated system and the complexity all derives from the desire to have a rate limit on a specific client for a specific origin and all the other cases you have are largely about"
  },
  {
    "startTime": "01:32:01",
    "text": "like making assertions about the client in particular but not about the origin specifically and so all those can be solved with the privacy pass solution do not need any of this complicated you know stop having to do with the media and issuer so i think it's really important to determine whether we think that's an important enough of an important use case to motivate um an effort in this space and i i guess you're probably getting from like what i'm saying that i'm not persuaded um can you explain why uh they are not origin related that essentially having a global pool of rate limiting for everything i do would satisfy these cases well no what i'm saying is geo um does anything to do with the server um uh is is it human to something to do with the server um okay but you do list them so like i think you know um um and but we have captures right the point of a capture is the person a human and um and so um uh and and so when you sort of say oh the alternative is capture this well they turn out this is not captured they'll turn their privacy passes captures um so um we often see that captures will come once we have hit rate limits or once people can prove or once they do not know if you have been below some rate limit and so avoiding the capture can also be well no i don't care yeah sorry we are getting into the next presentation's time so keep it brief i see echo your recommendation is not to dispatch it to privacy pass yet or or should it be presented there and it it can be discussed how to iron it out is that the recommendation well what i would like what i would like to see is um a more concrete analysis of the use cases and which ones can be addressed with is it with required technology that's fancy and which ones do not and um and then we can decide those use cases are sufficiently important to solve um to pay the price of the complexity and the potential um potential or choke point effect that this creates"
  },
  {
    "startTime": "01:34:00",
    "text": "um i i guess i guess i do i think it's important to mention this chokepoint effect because i think was mentioned previously um you know that uh we do have a worked example of this in the identity system and we have basically two identity providers so um and it's i don't think it's all impossible that the origins will insist that i see that the issue only trusts google on facebook and apple that would not be desirable outcome i agree okay andrew keep it keep it brief yep i'll i'll be uh um very brief um but just two two quick technical points to echo the discussion from yesterday um in a different working group um i i think this app really does need to address uh before it does go forward uh how to avoid the problem of colluding um uh mediators and issuers because if that's not addressed um the the benefit it's providing is completely illusory and in fact probably unhelpful uh because it uh potentially implies it's giving more privacy when it's actually giving less i think that from an end user point of view that's dangerous um uh if that happens um or also i completely agree with philip about the absolute appalling use of proof of work and just as a general principle um that absolutely should be avoided uh in terms of answering the spanish question uh privacy pass seems like the obvious place and i think as somebody just suggested in the chat maybe have it looked at in more depth in privacy past and then if need be come back to dispatch uh after a deeper look um with more time would be my suggestion thanks so that's good okay uh question to ads and coaches what's the dispatch decision here i think you have been following the chat more more than i have but is is dispatched to privacy pass the"
  },
  {
    "startTime": "01:36:01",
    "text": "correct decision here yeah there's there's been some um some proposals of the mic here and some discussion in the jabra channel so it looks like that there's a fairly good energy behind dispatching to privacy paths um having some detailed discussion there about how to kind of mesh this uh with the the use cases and technologies that privacy test has and if if anything like that new comes out of that um i think we can we can take it up here again but i think to first order uh dispatching the privacy passes the outcome here sounds good uh thank you yeah yeah so just to follow up on what kind of richard said it really does look like the expertise to have this conversation isn't privacy pass so i think if we're gonna decide uh decide is this privacy pass plus this work privacy pass becomes this work use cases from this informed kind of privacy pass they all nexus is point all the arrows kind of point back to the nexus of privacy pass so let's let's continue the conversation there i would remind the community kind of we still have two potential angles out of that you know if we need to change privacy pass that's a recharger opportunity so that's another place for us to kind of talk about whether we're comfortable with what that what the scope is uh if there is a scope change and then if it turns out that the kind of discussion from privacy pass says this is different you know again i'm kind of purely speculating here as others have suggested that that speaks to another turn through sex dispatch then thanks okay uh dispatch to privacy pass in that case uh uh thanks tommy uh jake i guess you're presenting i shared your slides already hope uh that's fine should we start thank you yep okay uh okay great thanks for the time here um i'll be talking about uh multicast security and privacy considerations uh next slide please so the problem we're trying to solve the"
  },
  {
    "startTime": "01:38:01",
    "text": "scalable delivery multicast of course um the uh the thing that we don't know another way to solve and the reason we're focusing on multicast is the uh the congestion we observe at isp access layers this is essentially a broadcast medium and so sending a single packet across it for all the duplicated traffic that drives the peak loads that are observed at isps is something that could be avoided if we could make use of multicast and we see a fairly large gap i discussed this there's a there's a link to the barb off at itf 111 uh where where i went into a bit more detail about this um later in the deck uh there are other impacts to end users that we believe can be addressed in the same way the the capital costs are driven by the peak load this is both for the sort of content owner content distribution network and for the isps and these costs pass all the way through to end users and have a disproportionate effect on those who can afford it less and the the peaks are generally driven by the popular content you know these these big game downloads the big sporting events that everybody wants to watch at the same time uh and and that's the main thing we're trying to address um and the problem keeps getting worse uh you know we can go into more detail on that but but the motivations i think are clear um there's a few things that uh that people often react to like do we really have to do something as hard as multicast we have looked at peer-to-peer for this and of course that doesn't address the isp access congestion problem that in fact makes it worse and the isps are firmly against it as opposed to multicast where as you'll see in a few moments there"
  },
  {
    "startTime": "01:40:02",
    "text": "tend to agree it makes sense application level multicast also doesn't address that access layer problem although it does help address some of the other problems this is essentially what uh cdns do already and we're finding that there's a substantial gap that we would really love to to see addressed so the next slide is uh we've been working on this at ietf already primarily in the mbd working group this uh the reason that we did it in this order that we haven't come to security really before is about uh sort of figuring out how viable it's going to be are we really going to be able to get this stuff delivered um but we started with dozens of informal conversations bouncing the idea around over the course of several itfs with uh particularly with a number of browser implementers just in in private discussions seeing if there's any obvious stoppers that would make it clearly impossible uh everybody agreed there are challenges but that uh with sort of uh sufficient buy-in this this could be reasonable um so we went ahead and we made we made the pieces that we think would be necessary we had some proof of concept implementations done hackathons have been reporting on it at mbod there's all these adopted drafts in mundi and uh at the last ietf we ran a barb off and got you know what feedback we could from that and now we're trying to sort of follow up on the next steps here uh next slide please in addition to our ietf work we've been doing a bunch of outreach outside the itf we've had a bunch of isp conversations we consider that kind of the critical feedback because there's no point if this stuff won't get delivered"
  },
  {
    "startTime": "01:42:00",
    "text": "but the answer we've gotten has been largely positive we've likewise been talking to a bunch of the content owner customers we have about whether this is feasible from their side and likewise the answer is is mostly yes we did several lab trials um with isps where the isp gear was being used to do the multicast forwarding based on based on ingesting according to the architecture we're proposing using amt and dryad um references there so the the feedback we got from the isps was that in some cases there were problems all the problems we encountered were were possible to work around by um by changing the addressing so we made a draft about how to do that this also has been adopted by mbo and d and uh and so with that we think we've covered the sort of practical problems to uh getting this potentially delivered and we'd like to move forward with with the the problems of how to do this in a useful way at the application layers um we're looking into both web and non-web traffic primarily because the peaks that we have are driven by uh by both of these cases the the peak traffic loads but the the web traffic is is one of the places where you know we're going to be blocked if we can't get into browsers and in order to get into browsers we need to have a decent security model and there are differences between the way multicast security would work and the way uh unicast security has worked in uh up until now so this is why we tried to to cover the uh these considerations in the in the uh draft i hope people got a chance to read um uh there's some other implementation work here uh we've got a w3c community"
  },
  {
    "startTime": "01:44:02",
    "text": "group um that's uh chartered to try to incubate the work i'm sure that there's a demo api just sort of proving the viability of playing video using and this is this is by the way a port of a receiver that that we have as part of a multicast product that's a wild garden multicast tv delivery product um but we've got that running under webassembly and playing video in a uh you know our own build of a browser um we gave an intent to experiment into uh chromium and we got some feedback from them the feedback is kind of what drove the uh the the writing of this draft because we you know the the point that was made is that we have to have confidentiality as part of the design which it was not up until uh up until that feedback came in um we get into some of the reasons why that is the case in the draft but uh basically it was a convincing uh convincing position and so we've we've taken it on board and we're trying to grapple with the complexities that opens that's the sort of background of the the genesis of this draft um we we are starting in the community group with aiming to to integrate to web transport and that's so that we can the idea there is that you can take existing multicast applications if you can solve the fundamental security problems uh that we do outline and draft and sort of get an acceptable use case then uh then web transport would be a great fit and would allow in the way that we've already done sort of adapting existing multicast applications into a browser context and deploying them at scale and then would open up experimentation for for other apis that would also be useful uh yeah next slide please so uh the problems that we"
  },
  {
    "startTime": "01:46:02",
    "text": "want to address here we want to separate the integrity and authenticity from the confidentiality concerns because uh these are kind of driven by different threat models it's very important to prevent someone from inserting a packet of death that's accepted and you know uses the rendering bug to take over you know all the all the devices of people watching the super bowl or whatever it is likewise very important to protect confidentiality but this is kind of a different set of problems and there's some trade-offs here that we try to get into in the draft that uh that come from the fact that with multicast you have the same exact packets that have to be decoded by many different receivers this inherently creates a sort of you know content discoverability problem of some sort but that comes with the removal of the destination ip address which it increases anonymity at all the network locations that are just you know increasingly distant from the end from the end user that's receiving it um we tried to look into like what are the threat models about here and for confidentiality most of what's written down covers like personalized information things like your medical records or your bank account info and uh it's you know on top of that the traffic analysis discovery is pretty good at picking out the you know especially popular things so we think this doesn't open up particularly new stuff but that is a point for for discussion if this really does create a an unsolvable hold then we need to know about it and stop working on it but uh our position is basically that the alternatives are just as bad perhaps worse um for the the scalable traffic that that we're actually seeing and so we think this is still worth pursuing in spite of the sort of differences in the security model we're trying to get that all articulated and this is a"
  },
  {
    "startTime": "01:48:00",
    "text": "big part of the work that we anticipate doing next slide please uh so the existing work is starts with the the multicast security draft you saw uh we have an adopted draft in mbondi that would benefit from security from a better security expertise this was originally argued as being reasonable to include in mbd based on like it doesn't do any novel uh crypto anything and i would argue that if there were a multicast focused security place it would have a better home there because it really does need some security expertise looking at it um there are other pieces of this that are likely to also uh require documents going forward uh when i've listed the ones here um as i said we're trying to target web transport so documenting exactly how that's going to work these experiments are just beginning but going forward we would anticipate writing the appropriate documents to include it and incorporating all the great feedback and and useful uh insights that we get from reviewing the actual security requirements and considerations uh that we've tried to put forth in the in the multicast security draft and there there may be uh going forward into the other apis some other related work that we'd want to address as well uh including specifically which protocols would be uh supported by browsers and how it would all work for uh for handling apis like fetch or download so in terms of getting into browsers this is uh you know we're basically blocked on on progress until we can get a firmer consensus answer or at least a you know a firmer tentative consensus answer on"
  },
  {
    "startTime": "01:50:00",
    "text": "what needs to happen in order for multicast to be used safely on the internet with the sort of modern threat model for the internet and uh i think we're really missing a lot of security expert participation in this space um at this time so that's basically why i'm here today uh next slide please so my first question is whether this is suitable for ietf work and if so what should we do with it precisely um you know i have a few ideas here one of which is reopen msac that was a possibly with a retarder um that that seemed like a pretty good fit for this kind of thing uh maybe a separate thing for broadcast msec but i would welcome suggestions and uh feedback on what we're doing here and uh whether anybody can usefully comment so with that i will open the floor to questions comments so before we let others weigh in just we have like nine minutes left and we have the option of overflowing on thursday uh of course i would personally prefer to get a dispatch decision already today but go ahead the queue is open ben so i guess the question i have is to try and get clarity on what you what questions you want to answer as the next steps so like i heard a fair bit about um like the question being is there a security model or security framework that we can use that would make multicast traffic compatible with like traffic in the web to the browser is that sort of what you see is the key question to be answered here uh"
  },
  {
    "startTime": "01:52:03",
    "text": "i would say with with delivering it safely to end users i think there's a lot of multicast traffic today that goes to end users that is not a part of the web security model and we can start there certainly like we can do some multicast based delivery uh on some of our use cases like some of the uh game delivery uh pieces or or software updates and uh as well as as potentially like um there's there's some interested parties with uh that have kind of large footprints of the those like smart tv on a stick kinds of things you can plug into the hdmi cable on your tv or whatever and these can these can go ahead and start deploying multicast so we can address some of our use cases today but we do think that web security is an important part of this and a big part of the security question we think that these same security questions are applicable to these other domains as well as well as the currently widely used multicast delivery for tv that's operated by by msps so we'd like to address all of these questions and land on a sort of as safe as it can be for end users under specifically what considerations that are uh properly written down and to apply that to all of our solutions um if the answer is well you have to get some deployment first then i mean we are intending to do that to the best of our ability but browsers are going to be a big part of solving the ultimate problem of scalability that we need yeah okay so still like looking at a security model but maybe i i was too narrow in the initial statement yeah i mean web doesn't have to be the starting point but i think it's one of the more complex spaces and so it's one of the ones we want to get the conversation started on today because we think we're going to land there yeah"
  },
  {
    "startTime": "01:54:03",
    "text": "hecker hi oh and thank you ecker for your feedback on the list really appreciate the conversation there thank you um yeah i mean i think it is a dispatch question um i think i don't know where to take this um um i guess i'm not sure your problem is quite the problem you think it is um um uh richard vegas like like i i i i didn't read that message with chris barroscu and i guess what i would say is if chrome were really enthusiastic about this then you would get a different message um and it wouldn't have said like probably problem for me and i'll be interested um i think we're like i think you're probably not very about security um um but i think probably like trying um i guess i would say like there really are two up security models um one is um you know uh the one that's certainly implied by web transport whatever you see and there was the one that's applied by fetch um and as they fetch as the origin model and web transfer but we can see something quite different um i i i do not think there's like any like any any near future chance of like having this somehow mapped into the the true web origin security model it's says they're just too different and we spent so many years trying to reason about that and only barely understand it now that i just like like think that's extremely unlikely um that people want to put the effort into it um the um uh maybe your chrome version that mapped into like the web transfer over to the model which is like not really tied to origins um because that's a much more simpler kind of case um but i i guess you know um i guess i would say like the itf is probably not the place to think about that at some level because the security properties um the people are not the actual security properties technically right here are not that complicated what's complicated is mapping into the web so um i'm not saying again i'm not quite sure what to do about that um um question whether we have a question yeah well thanks we are certainly engaging with w3c as well um we've opened the discussion with web transport in particular and uh and web networks uh interest group has has taken some of this on uh maps well with some of the sustainability work that's starting there but um we do want to get the conversation underway i mean you're kind of right"
  },
  {
    "startTime": "01:56:00",
    "text": "about chromium they're they're not interested but my read is that they're not interested because they're skeptical it will end up working i think a lot of people have that skepticism and multicast has a history that certainly justifies it but um you know we do think this problem needs solving this is a really good way to do it and that the problems do not look fundamentally unsolvable so to the extent that we can get continue gaining traction and get some forward progress on this i don't think it's really as out of reach as people think it is um but i i do acknowledge that that remains to be seen but thanks for the feedback there watson labs cloudflare um it's not clear from my presentation whether or not you need a protocol to meet security properties have a good idea about or don't really under don't really have an idea of what the security properties are you need to meet and i agree with echo that if you you don't know what security properties you need to meet that sort of w3c versus coming up with a protocol to meet the networks in multicast which might be more of an iepf thing so uh we have some idea we think that the the draft we put forth about security considerations presents a model that we invite uh criticism on and we'd like to sort of hash out to everyone's satisfaction um the the feedback we see from w3c is that well obviously this has to have the underpinnings of of uh protocols that satisfy the web security model needs and so this is the the sort of um middle ground we're stuck in at the moment and if we can get some forward progress on agreeing with the security model get some feedback on on what that's doing then as we as we go ahead with trying to build the protocols that meet that security model then we'll have some confidence that"
  },
  {
    "startTime": "01:58:00",
    "text": "it's actually driving in the right direction uh that's kind of the way we're looking at this now it's it's got a lot of moving pieces and a lot of people involved absolutely but i you know if you've got a better idea what we should do please we have like two minutes left so ben keep it very short hi uh i i think that this piece of work is too big to do all at once uh i think that that the first step would be to solve the protocol problem and leave the browsers out of it basically and i think there is a logical way to do that my suggestion would be to dispatch to cdni and treat this as a cdn to cdn multicast use case where you're moving content where you're essentially standardizing a component of application layer multicast so yeah that i believe is using these two client is a cache box and and then we can come back to the question of how to move that cache closer and closer to the client wow uh so we believe that problem is already solved and irrelevant to this case so um at least that's my belief uh so thanks for your feedback uh i think uh we have to stop here and we need to come to some sort of uh consensus on what's the best way to dispatch this what i saw on the list and i received some chats people seem to be saying we are not sure at this moment but perhaps discussing this on a mailing list seems sensible would it make sense to dispatch this to a mailing list say msec or starting a new mailing list that depends on the aeds and and the authors who are driving this so how about we we continue the discussion on a list once there is enough traction and we have"
  },
  {
    "startTime": "02:00:00",
    "text": "more clarity whether this is cdn to cdn whether uh browser should be included in the first attempt or not uh will result from the discussion on on that mailing list and it's up to the ads and you which mailing list this discussion happens on we'll obviously inform on sect dispatch for those who are interested in continuing this discussion how does that sound uh it is is that okay sounds like that sounds like a great plan so you know jake if you want to reach out we can talk about whether a new mailing list makes sense or and sec okay uh that is a start thank you very much so we are right at the hour i think i'll just summarize the two dispatch uh decisions uh for multicast security the discussions will continue on a mailing list for private access tokens we have tentatively dispatched it to privacy pass and the working group needs to make a decision on whether it needs to recharter update new use cases or send it back to sec dispatch and and we'll see about that so i guess with that we have managed to cover both the drafts in in this session itself and we won't need the extra time on thursday uh thank you for joining i guess we are done for today am i forgetting something looks like not so okay have a good rest of the day ahead thanks man you"
  }
]
