[
  {
    "startTime": "00:00:01",
    "text": "About it's saving I don't know. 9:30. I think I got all the slides up Is it next to them like this time? Just the problem. A computer rather than those kind of like you know? Hello, everyone. So it is 931. So we're gonna Hello. start And so if you could make your weight here seats Welcome to The quick working group at IATF 119 Most people are probably familiar at this point in the week with the note well, but, please do note well no. Well, This is This basically gives us Our guidance for all of our conduct, and you agree to the note well by participating in this in this working group, he has also notes of particular now, the code of conduct and the anti harassment procedures. And if any concerns about anything of that nature, please reach out to the chairs or AD or someone like that. So very briefly, meeting tips, usual type stuff, make sure you use the on-site tool. So we get credit for you being here. It's to have to pump our numbers up. And, please use meet echo to join the mic queue rather than just standing there. And if you're going to use the full version, just keep your audio and video off."
  },
  {
    "startTime": "00:02:03",
    "text": "And remote participants, yeah, that, you know, the typical keep your audio and video off unless you are presenting and We will try to give you control of the slides if you prefer it. Otherwise, we can also, control it from here. Do we have a good Oh, Okay. So everyone's favorite part of a working group meeting is that we need someone to take notes, for the minutes. So do we have any volunteers? For note taking, It's a very noble thing to do. And we can't really thank you, David. So let's see what else is Oh, yeah. And we're gonna run the we'll run the queue. So, don't worry about, like, removing yourself. We can we'll dump you when you're when you think we're, you're done. Okay. Agenda, we have a A lot of things on the agenda. So as usual, we start with our working group items and are starting with Q log should be presented by Robin, remotely. Then we're gonna move on to multipath in 50 minutes. It's a long time, but It's a big thing. So then finally, we're going to finish off with act frequency for the are all working group items. And then we're gonna move on to other items that we think are interesting. We have a a presentation on the some security considerations and recent experience with that. Then, click on streams. The BDP frame, some things about FEC, and finally, at ECN, and we hope to get to all these things, but as usual, we may not get to some of them. And finally, update since the last meeting, Not a ton, but there's some. So we have a We had finished a working group last call for act frequency. But we need to run another one because there was a bunch of issues, and that's that's why we run working group last calls. So That's okay. But we expect to run another one, and we'll hear more about that later."
  },
  {
    "startTime": "00:04:03",
    "text": "And then, reliable resets is much to the relief of certain chairs is finally way getting along and is awaiting shepherd right up. So, hopefully, that will that'll be helpful for our web transport in particular. And, with that oh, I I guess I should have said, does anyone want to as agenda. We did a bunch of pre agenda Fashing. So expect not, but anyone has any bashing, Please say so now. It looks like no. Okay. With that, we're going to, move on to our first presentation, which is from Robin Yes. Can you hear me? Can I get the slides? Yeah. I'm trying to do that. Hold on. That he wants from Robin, Robin, can you request permission to present. From the meet echo kill. I can ask slides if that's what I need to do. Yeah. I did that now again. I guess I'm gonna run the slides for you, Robin, because I can't actually give you access for some reason, another time. There we go. So for today's updates, I wanted to go for the June team. Because this is turns out Australia is about 40%"
  },
  {
    "startTime": "00:06:03",
    "text": "quite cool. dunes. So we're all basically in arrakis right now, Set I couldn't be there. But so next slide, please. Since last time, we actually had a ton of progress we made a lot of changes that we discussed last time, like removing QPAC, which is now done We had a bunch of editorial updates, and then we also had a lot of additional clarifications and nuances that were fixed. A lot of that was due to a new character, appearing on the scene, much like characters in the dune movie. In our case, this was, Hugo Landau, who is the open SSL quick implementer, and they also added Q log support. And through that, he found quite a few, subtle nuances and issues that we had tackle, which we're able to do. So a big thank you to Hugo HUGO, for that. Next slide, please. Other big thing that we merged since last time was, support for, pot migration and the basis for multi path. You might remember, this is a relatively simple proposal where we have, string part IDs that can then be linked to, specific path information using separate, defense. We think this is a good basis for multi parts. The goal was not to have full multi bot support yet, but we think this is enough for there would ever be a quick multi path keelog draft to build upon this which is why we also merged it. However, we're not a 100% sure that that's correct. So If there are still people from the multi parts, work that want to experiment with this or want to give it another review or try to implement this, we would definitely welcome, additional feedback Next slide, please. The main topic of today will be stensibility. So, one of the main goals of Q log is to allow later to define additional q lock schemas for other protocols"
  },
  {
    "startTime": "00:08:01",
    "text": "or for new documents to extend to add new events or new, protocol mechanisms for QuickenHcp3. And there are various ways that we need to add in accessibility. 1 of the ways is if you have a q log file, how would you know which protocols or which type of events are actually possibly represented inside of that file. And so we've gone to various iterations on this I've been presenting this for a while. This is the current proposal that we are quite sure will work. So, basically, you have 2 options. You explicitly list The the events, Event groups that you want to use in a specific file either through a pre registered UN So that goes through an IANA registry, or you can also use a fully qualified URL For things that are, for example, not yet an RFC or or not going through the IETF. System. This is already iterated on from previous feedback and is also something that is in previous RFCs, RFC 8 285, for example, also uses this. So we're quite sure that this is a good approach one of the main questions we have is how explicit we need to be enlisting all groups of events. So, for example, our quick document has various categories of events. Have your general transport based events, but you also have what we call connectivity events. Or recovery events having to do with congestion control, for example. And In the example on the slide, what we're basically doing is explicitly listing only 2 of those 3 categories for quick. Meaning that the other ones, the recovery 1, wouldn't be present in this 5555 is different from HTTP 3. Where we don't explicitly list which categories we're using. In our current proposal means we're using them all. So anything that is defined on the HTTP 3 document goes,"
  },
  {
    "startTime": "00:10:02",
    "text": "So that's kind of the approach that we're going for. If you don't have the hashtag with a specific category, you would just import everything in the file. If you have implicit, hashtags, you onboard only a part of the file. And the question basically is Do does anyone have any problems with that approach with implicitly including the full set of them, or if people or feel very strongly you should always explicitly list all of the categories that you are, using. So if people have a very strong opinion on that, Let us know. Next slide, please. So You want to add new thefts new stuff? But it's not always just completely new call. Sometimes you also want to add new things like new, frame types in this case too, for example, quick. So a naive approach to build the q lock, documents right now would be to say, okay. We have the existing quick frames. Let's say, for example, the max data frame, And then in a packet sent event, you would have a list of all the different frames that you could use as far as we know existing quick. That's, of course, very inflexible because you can't add new types of frames. To to this definition, you would have to redefine the packet sent event to add a new type of frame, which is for example, one of the problems we have with the act frame, extending that for things like multiboltac. So we want to prevent that for for Q log. Using the next slide. Using something called a CDDL type socket. So that's the dollar sign quick frame thing there. In the CDDL language, this that basically means it's like, I call it a type enem. So a list of types that are represented by this one type name, and you can later extend that. So the slash equals means you add a new"
  },
  {
    "startTime": "00:12:01",
    "text": "type to that type enum of quick frame. And so later documents For example, let's say the act frequency draft, they if they would add a q log support, They can just extend that existing quick frame type that is defined in the other document. And so if you combine the 2 combine the CDDL definitions of both documents, will it will automatically work, and you can then list act frequency frames in your packet set events as you would expect. Right? This was actually already in q log for a while. For some things, we now demanded to cover many more things. That's basically what the new PR is doing extending this approach. To, to other stuff. Next slide, please. So that was for if you have new things, new frames, that you wanna support. Sometimes you also also want to extend existing thinks. A good example here is quick transport parameters. Which we log in a, an event called parameter set as you can see there. It's a long list of transport parameters that we have there. And, of course, there will be new transport parameters in extensions defined. And the way we used to deal with that in qlock was to just say Oh, this event can contain any field of any type. That you can you can basically expect anything in there. Is what it was saying, which is flexible enough to support later extensions, of course. It's also completely useless from a schema validation standpoint because it's just too generic. Anything goes. So, you can't really differentiate if it's if it's a valid extension or not. And this is really something that we've only fixed very recently. Next slide. With a a different type of CDDL socket. In this case, a group socket. Which is different from the just look at from before because it has $2 signs instead of $1 sign."
  },
  {
    "startTime": "00:14:00",
    "text": "So this is not like a type enum. This more acts like a placeholder. Kind of field inside of the definition. So, basically, it's saying This is a placeholder and whatever gets assigned to the placeholder later can then show up at this specific, location in the in the event. So, for example, here, we would have the quick parameter set event in the main quick document that we have now. And then again, if you would have frequency, extension that would have q lock support you can then say, oh, there is a new transport parameter defined. It's this field, the minac delay. It will just be, slotted into that place in the existing event. There as well. So, basically, what we're proposing is to add 1 of those extension slots, to every single q log event. So that every queue log event can be extended with new fields if needed by, future, extensions. We think this is a nice, balance between verbosity and complexity in all so allowing for extensibility of everything, down the line because it's always just a single line for every event that is extra. Even though some of those are who will never be exercised. Next slide. With this, we are focusing on making q lock extensible for the main RFC extension points. So mainly the things that are defined as extension points in the existing RFCs, which mostly means they have an INA registry we also have a few things that do not have that. Let's say, quick packet types, or, fields in the quick packet header? Let's see if there was ever a loss bits extension that was accepted you would also want to extend what's in the packet header. So we also have extension points for those. For those things to try and cover our bases as much as possible."
  },
  {
    "startTime": "00:16:00",
    "text": "Right? So we think we got this right. Finally. And so the question is or the ask is if people would be interested in actually trying to exercise. These extension points for new work. Concretely things like multi belt maybe, or I know there's been some discussions in media over quick. And there's actually already an existing draft for care will resume. That is doing this partially, date on support Most of it because they only, add a fully new event. Which is great. But we would need, to exercise all the, older, extension points or at least to a to an So people are interested in this. It doesn't have to be full fledged or fully defined just to have a look at it and try some of it out. Would be very happy to help you along there and to Look over what you're trying to do and see if it fits with, what we're trying to do here. Ted, do you have a question you wanna enter down now? If you could send a note to the quick chairs, Elias, with what you think is the extension points we ought to be exercising a little bit more explicitly, that would be very helpful. Alright. We'll do that. Thank you. Yeah. Yeah. Excellent. So next slide, please. So accessibility is the main thing. The only other big question we have for the working group right now. Apparently, I said quick chairs, and I meant mock chairs. Sorry. Thank you. Mark Pierce. Okay. That makes a little bit more sense. Excellent. So if you have signals like Stream was finished or stream was reset or you get to stop sending. Those are usually received at the, let's say, packet level, and logged at the packet level, sometimes quite a long time 4, they're actually communicated up to the application level implementation Right? So some implementation, for example, only give you that signal"
  },
  {
    "startTime": "00:18:00",
    "text": "when you try to read from a specific stream, not when it was actually received And so The question is how how to log that in q log when that was actually passed to the application. Implementation in that way. And this actually has a lot of parallels, but how we communicate how data is actually read how data is passed between the layers. We have something called data moved events. For that. And so it seemed like a natural fit to also have these kind of signal in those data moved events. We had a lot of iterations on what exactly that should look like. But these things are so different, and they they all have their own nuances and everything in the end, we just decided or or hoping to decide, to say not to be too explicit in this event, and just have, like, a string that says In this instance at this time stamp, It was communicated to the application that there was a firmware set for this stream, or there was a stream reset received for this stream at some point in the past. If you want more information, look at other queue log events that do log the full frame or the do log more information about what that might have been We think that stretches the needle between being useful for debugging and not replicating too much of that, information across different events. But it is a little bit clunky, which is why, we're we're asking people if they have strong opinions on on whether or not we should do this. And it's basically the end of the presentation last slide. Please We think we are nearing the end. With this here, we think with extensibility done That was the loss of the major design issues. We currently have quite a few issues and PRs open, but we should be limiting dose very soon because a lot of sensibility stuff is spread over a lot of different issues and PRs."
  },
  {
    "startTime": "00:20:00",
    "text": "That those are hopefully going away soon. What I'm trying to say is if any of you were waiting for Q log to settle down a little bit. To start again looking into it and and seeing, and and helping us bring this to the finish line. This might be a good time to start looking into it again. As we said as well, looking into exercising the extension points it's also, starting to be a good time for that. And so we are very open to new q log issues, new reviews, Again, the big thanks to people like Hugo Landau who did this if more people are willing to do that, that would be very, very helpful today. It for me. If there are any questions, Let's have them. I, I inserted myself as a individual and author on the Q logs back, not his chair, but kind of in the lead up to this meeting and, during this week, we've here like, some chats and the slack, etcetera, from mock implementers and and co. So to to Tad earlier point, These extension points exist. I don't know enough about the mock ups, that that that to know what is useful to log. So we've had some discussion, if if you just have a log of stuff that is useful for seeing what's going on with the mock application from a performance or debugging perspective. We're happy to do some reverse engineering because we wanna prove that the extension mechanisms are working. Similarly, we had in DSVW yesterday, that there's some folks, extending to support the capital resume mechanism. All of these are really good. I don't think we need to block. The progress of this draft on doing those things. So this is suggesting in the chat, we do something like a toy application profile, like side deck. Personally, I don't think that provides my trolley. I'd rather invest our efforts in things are actually benefiting real quick work, even if that not quick related work, even if that work is not"
  },
  {
    "startTime": "00:22:00",
    "text": "financial done in the IT act. Yet. Fully agree. Any further questions or, comments. Seems like maybe no. Thank you, Robin. So our next presentation is going to be on multi path quick. This is working. Good. Yep. Should we stop? Okay. Hello, Ron. I'm EMA. Today, Mia, and I will introduce the explicit pass ID proposal for the multi pass extension. That size, please? Okay. First, I will introduce a little bit about the background, about the key that this proposal want to serve and how does this explicit pass ID works and then we'll compare this proposal with the 0601 we list these all these pros and cons and do some comparison. And then about this interim reports for the new proposal, And for the last part, we were introduced this all these open issues and make some discussion. Aside, please. Yeah. At first, I'd I'd like to introduce a little bit about this key issue in case people has for forgotten this this is from the last, IETF the key problem is that the last 0 6 approach is using an identifier"
  },
  {
    "startTime": "00:24:04",
    "text": "for the past, which doesn't have the same lifetime as the network pass. And earn Martin has the first point out that we should use a separate pass ID from, from, from the connection IDs. And, picks us for his great idea. So what we are trying to introduce here is to introduce an explicit pass ID that stays constant even if this idea on the past changes. So next size piece, So for this part, I I tried to introduce more details for how does this explicit pass ID works. I'd like to introduce it in 3 key parts The first part is for the past management is much more straightforward We just use this explicit pass identifier for each pass in a connection at each it it we we also use use it in all the multipass control frames such as for the past available and past abandoned. To address a pass. And, also, we need some more CID management cultural frames, We add this MP new CID frames. Which ties the CID to a pass ID at The CID sequence number is increasing under per pass. And we also add this MP retired connection ID frame as a pair, and also the re retired priority field is underperpass. And that's a difference, from the, I've seen 9000. And the the surf part is for the packing number space. Packet number space now is bounded to the pass ID. And it will remain"
  },
  {
    "startTime": "00:26:02",
    "text": "stable when the CID rotation happens, And we don't need to we don't need to force, for the trade off between the performance and and the the packet number changes. c and the Packet number space changes. The next side, please. For for this for this part, I'd like to try to compare with the old version and then list all these pros and cons here for the first part for past management, as please pass ID, the new proposal has a prompt here. Because it's quite straightforward, it introduced a link between incoming packet and the pass. Is ambitious. Always know you always know which which path does this packet belong to? But the overdraft and need you need to treat the situations when the CID rotation and the natural biting happens, and it will have ambitions situations for the second part for the CRD management, you really need to do more things to maintain these DIDs per pass. And to manage the CID and the past you need to retire all the associated CIDs when the pass ID is a pendant And the old one is just the same as the I I've seen nice 1000. I want to explain more here the new new proposal is fully compatible with the FC 9000 because you can you always use the EOC ID frame and the retire CRD frame for the initial pass. The initial pass has the pass identifier 0."
  },
  {
    "startTime": "00:28:00",
    "text": "The third part is for the packing packing number state for the lost recovery and congested control. For this part, explicit pass pass ID has a problem here. For the lost recovery and can just come through can can rely on single sequence number space for the duration of the past, it remained the same for the past whole lifetime. And the old draft has a Chrome here because CID rotation will trigger the change of this packet number space, and you need to face a trade off between the performance or more complexity in your court. So, basically, it seems we have 2 point to one point here. And I like to introduce a little bit more about hardware or floating. I'm not a, expert for hardware offloading, but I've I have discussed this question with someone who is And, Eric has told me that for all this, two proposals. From the hardware perspective, they just share the same cars from multi pass hardware of loading because they all need to use the PAS identifier to mixed into the lungs when when you use hardware offloading with SmartPass. So they are just exactly the same cost Next slide, please. Okay. For this part, we had already have some internal reports from the hackathon I'm very glad to see that even if this per request has not been merged several implementations has already tested implemented in in their codes. We have tennis, nice, interrupted. We have eggs, quick, pickle, quick, rust, and the creature."
  },
  {
    "startTime": "00:30:02",
    "text": "And, we'll we'll we're glad to find out that express it as ID management works quite well. And you in the new version, we have clearer logic, and we reduce the code complexity. Yeah. Next slide, please. So the next big question would be, do we want few birds this new To a request, Yeah. We're gonna do some discussion first before we do any sort of, asking a question. So Kazuko, I appreciate the efforts. Spent by the authors and the people, who did the interrupt. I thank the for requesting produce it the cost of I mean, introduces the cost of originating multiple CID number spaces, and it's a pain for me. That said, I understand that this is a trade off that I can live with. I think we have come to a point that, we should just merge this PR and solve the remaining issues that might exist in the new approach. Alessandra Giddini, Cloudflare. Let's just merge it. Janai and Garth, Firstly, I haven't read the PR, so I'm not gonna say merge it. But generally speaking, this direction seems to make sense to me. I just had one observation. When you are separating the packet number spaces, you don't actually need to separate it by path ID. Because all you really care about is that the packet numbers are monotonically increasing. From the sender's perspective. Because ultimately, the packet numbers just have to maintain, send order. Are you proposing 1 packet numbers everything? No. No."
  },
  {
    "startTime": "00:32:04",
    "text": "meetings ago. Right? That's the proposal we had, like No. I type I'm not I'm not saying that you should not use. I'm saying you don't have to uh-uh separate of the packet number spaces as well because you don't need to. You can use the power 30 for everything that you do. For condition control, context for various things. But, we don't have to spend a lot of time here talking about that. It's a very minor issue. It just seems to me that you don't have to actually separate out the packet numbers spaces. I like the explicit path, I think it's the right direction to go. Magnus Westen, Ericsson, yeah, also supporting explicit path ID. Let's go forward with that, etcetera. So 66 Kristen. Here. Yes. I mean, uh-uh, obviously I I had mixed feeling at the beginning. Yeah. I would really like to have just a single number's best, actually, I was mentioning. But, the truth is that you cannot do that with the the new proposal. And the reason you cannot do that is that, You cannot make that an implementation option because the, when you receive a new pass, you have a new like it that is arriving and you have to predict the 64 bit number of the ticket number. And if the number was shared with different parts, then you would get, you know, mess of, guessing wrong and and not think of end up recruiting correctly. So, Janice, we're as simple. As Mirya said, we add actually that proposal back. 2 years ago, I was the problem of that proposal. I lacked that proposal but it requires If you go down that threshold, it requires a lot more code the compete then the comp than the current proposal."
  },
  {
    "startTime": "00:34:01",
    "text": "So I would say let's merge it and and go for and and simplify. It's one of the wood not taken. Janeth? I feel like I, I'm I'm I'm triggering people here by saying this. So I'll I'll I'm not gonna walk it back, but I'm happy to take it offline. My point here is that you're talking about single packet number spaces only, What you're using is the packet number as the key to figure out what path you're going on. I I I am I understand. I understand, Jenna. As here Actually have that implemented in my code. No. No. No. No. I Kristen, hang on. Let me finish. Let me finish. No. What I'm saying is use the path ID as the key. As you're doing, And an optimization is you don't need to use packet number spaces that are separate for condition control or for loss recovery purposes. Because really that code is gonna work should work, one way or the other. Again, let's not write at all on this because Yeah. I mean, let let let if you want, we can discuss that. I have actually a full implementation of It seems simple at the beginning. It's get very, very complex when you get into the details. definitely Okay. I We had this on the table about 2 years ago, and we made it session that we go from out of pocket numbers. I know what was on the table 2 years ago. What I'm suggesting is different than that. But like I said, That's the last one I'm gonna say on this. We can talk about this offline. Thanks. So we're we're hearing, some support in the room. We wanna, as chairs take a, show of hands just so we can get a feel in case people don't wanna be more vocal. So we're gonna run that poll Now if you're not yet, Sign in, please do so. In that in that the 5 seconds, I'm gonna take the pusher button. And Get prepared. Get prepared. So the question we're asking here is should the authors merge this pr292. To go forward with the explicit path IDs design."
  },
  {
    "startTime": "00:36:00",
    "text": "Can see the options there. Yes. No. No opinion. We have I high number of participants are not many people voting. So Okay. We have some hands on the stage as well from the presenters who aren't currently logged in. K. I think we can I think we can stop there? Hello. Stop. There we go. Just so for the record there, we had, 23 Yes. Hands. 3, no, and 14 We have 23 yes, hands, 3, no, and 14, no opinions. So I'm taking that as a strong signal in support met going forward with merging this PR, we will take that to the list to confirm consensus But otherwise, Yeah. Yeah. So thank you for the the strong indicators here. That's giving us a great path forward, and means we can actually move on to the next discussion topics with our slides Perfect. So we were hoping for this, and that's why we prepared some open issues to discuss. We currently have 40 six of issues. Some of them will go away because they might not apply to this new PR anymore. And there's also a little bit redundancy in the issues, so we need to clean that up. But we brought a couple of issues for today that, are connected to the new exclusive pass ID proposal, that we thought might be worth discussing. So this slide is the easy one. That's the proposals where we believe we have already a way forward. So, there was a discussion that a pass that we should not be reused and there seems to be agreement. So we have a PR for that. And, yeah, I that Kasu has a different opinion circle to the mate. Right. So the problem state statement of 292 was the"
  },
  {
    "startTime": "00:38:04",
    "text": "not having a continuity of packet numbers, cross multiple connection IDs is a pain. And we agreed that we solved that. So that Now that we have facilities indicating with to which packet never space, each pocket belongs to the question becomes If we need a way of issuing and repairing new pass IDs, And to me, it seems like an unnecessary complication. I mean, if you have the opportunity to to open 4 parts, you can just use 4. Yeah. You you can just have 4 pocket number spaces across the entire lifetime of the connection And then not rebinding would look exactly like an explicit migration, there'll be no difference. And I think that will be a win. To use just to reuse facilities across the migrations. I mean, yeah, you you opened this issue yesterday, so I didn't think about So I think we just need more discussion on it. Okay. So, yeah, please comment on this issue. Either this one, or Kasu's new issue, I think they are connected and as I said, we will clean up the issues after this meeting somehow. Okay. Then also hopefully easy one, is 3 17. So, if a preferred pass is indicated during the handshake. Then this prepared pass should get a new pass ID, the pass so the the original part is pass 0 and the preferred pass would be pass 81. Is this just something we omitted in the current PR and straightforward. Hopefully, hopefully, Okay. Kassel, thank you. And then there was a point about capacity is still not fully clear, we already I think we also have a PR in this one, but there's still discussion going on it seems like people think it is valuable to have to use the same pass ID on both sides. So the client and"
  },
  {
    "startTime": "00:40:00",
    "text": "server, both know it's, you know, it's past number, whatever, 3 but also that needs some coordination. So, that when you open a new pass, you actually pick a pass. I'd be that it wasn't taken yet and you don't have conflicts. So I think we have agreement that we want to have the same pass ID, on both side but then there are some additional stuff we need to discuss, which is on the next slide. Okay. And, this is exactly as soon, as you have if if you if you if you have, like, 1 pass ID space, basically, the easiest solution is if we only allow a client to open new passes, you don't need to have any coordination. But if you want to also allow the service to new person. This is a very old issue, as you can see. We never decided on this one. Then this means you actually have to handle it separately. And the proposal here is to split the pass ID space and as similar as we did with streams and the client uses the even numbers and the server uses the odd numbers. But then that also means that all the kind of settings you provide about how many, passes you allow to open and whatever you have to do basically on a server and client side. So, yeah, discussion of this one is is ongoing. I don't think have a decision on this one yet, we Any thoughts, comments, ideas? When it's something On clarification question, is there anybody furnace this feature for their use cases, or is it something, you know, that's sorting Like a nice half? I think people believe if you same path ID on both sides. It's really kind of it's less error prone in your implementation. We have seen a couple of cases where the implement that did get it wrong. Right? Because you're always at each frame, you have indicate which pass you're talking about. And then it's always not clear. Are you using your pass ID? Are using capacity of the other site. And so if you could just just choose use the same number, then all of that you know, complexity goes away Yeah. Right. So I I think my question is around if we want to"
  },
  {
    "startTime": "00:42:03",
    "text": "if we need the server services. Okay. I think so this one, RC 9000 doesn't allow the server to open, connections. But it felt like and this is back in the old issue. It felt like the restrictions we, we had to make this decision are actually going away with So there would be an opportunity to just do it. On the other hand, there's not probably a big use case for because the server can always tell the client, you know, Peter S and tell trying to open the past. Right? So that is, like, if you really need this, you can, like, handle it this way. So Yeah. Right. Thank you. So I I think my comment would be that, honestly, there's a strong use case Right. I prefer just not doing it or punt the decision to a later moment. I mean, I I see a point, but it's also it's it's it's it's And why? So, I mean, it's not that much complexity. Right? It's just like if you can have it, Yes. I mean, To to answer the the same question, I I did study it and, The point is that if we do not, do the or even split Now It will be impossible to introduce it as an extension letter. all kinds of duplication of frames and things like that. So is possible, there there is a potential compromise, and the potential compromise say, okay. We are not going to authorize server initiated pass now but we are only going to use the even numbers for now with the exception of 1 for the server prefer the US. And if we do that, then we do not increase the complexity of the current implementation But we'll leave the door open for the server initiated connection at the at a later stage."
  },
  {
    "startTime": "00:44:01",
    "text": "Yeah. I really like this question. You can write a PR. I I did I did write a text in a in another issue with the Android Pious to Magnus? Yes. Magnus West Club. I think the use case I have for server initiated is the fact that you're going to try to use quick for replacing STP and, which actually has, You will actually can use, multiple paths concurrently for, and in both day. And it's actually in a scenario, like, in a mobile network where you actually client server role is not clear. Or saying it's someone needs to do what's it's suitable for them now based on what information they have. So in that scenario, it makes sense for opening up a path potentially from one that currently has a server role because they're basically equal. That I think also applies to peer to peer application to a certain degree. So and you may need to be open a new path. Otherwise, you would not get the right response based on mailboxes. So I totally see that it's useful in the scenario, but just double checking. So you think the solution server would be able to tell the client with where to open the past, like, given IP address the client and then the client opens the path anyway. That is not doable in the mobile network scenario. In the mobile network scenario, it's I would guess it's doable. It would potentially introduce the latency, and especially if you actually have a hard failure case. If you drop both paths, or is it both paths if you have 2 paths, both fails, entry injury cover up, it might be delaying things. I think it's more important in the, like, peer to peer quick and multipath quick scenarios. Where where it's a single path, your single interface might be nested and you need to be able, and while you have a listening port somewhere else, so"
  },
  {
    "startTime": "00:46:06",
    "text": "I'll I'll speak as an individual. I I haven't look at this issue closely, but here in Kazuko speak about need some use cases. The thing that sprung to my mind is to allow allow this as an extension and effectively in my head was what Kristen said, said So, I think maybe allowing for that. And if people are really interested and think they have those user cases to go and work on it as a separate that may be you know, maybe if we get to it, before multi path is done and we convince ourselves to merge it back in, that could be a way progress these things. But that's just as an individual I think this is a good way forward because I don't think this is blocking Yeah. anything else. So we can also we should decide before we publish the RC, but we can decide at the very end And and just to say, in in the chat, there seems to be some support for Christine's suggestion too. that this would be potentially interesting Just wanted to point out for my net reversal, extension that I presented last time but I have to admit I have to think about this issue a little bit more before forming an opinion. You said it's interesting for it, or what was the work What was the where it get you used interesting. So it would make it easier or harder? Easier. If you don't Maybe I need to think about this more. And so what? I I mean, I would definitely like this. F we're gonna implement multibath. I mean, this is close to half the value of a lot of the cases that I would actually wanna use multi path, right, because, like, Quick connection migration works pretty darn well on the public internet and solves an awful lot of use cases. So that the, like, order to do the work of doing multi path. I would definitely prefer. For other peer to be able to change. Change paths, change paths, like fairly strong. I mean, I,"
  },
  {
    "startTime": "00:48:01",
    "text": "it took me, like, 10 seconds to come up with, like, 3 use cases. So I'm, like, I'm sure I can come with more. Perfect. Yeah. I have another another, recent to do it. And the other reason to do it is that the alternative, which is to have a a new frame by which the Server test the client. Hey. You can join me at IKEA West so and so. I said slight issue that it becomes very easy to build a request for show attack. With that. Because, I mean, a a male phone server can send the attack of a target by that mechanism, And the client just on to target after that. So, the having the server send its own packet. That's an after some issue. I mean, if there is an attack, server can always attack someone, but it will be attacked from the server. It won't be attacked from the client. So I think, The more I think of it, the more I say, we we we should just Put that. Maybe we have a instead saying don't use it now until we have, I know it's like extension, but, I I think we should go for the even odds blatant and put it in the draft. Okay. Perfect. Sounds very good to me. Okay. With with chat on, I think what I'm what I'm hearing is Someone should go and make a PR at least so that the can give a bit more consideration into the the scope of impact on the spec itself. And that, yeah, there's maybe a little bit more here to see manifest rather than just talk about it. Yep. I think we have something like half a PR or something, but it's a little bit We will clean it Okay. If one was light, if we have time, we do have time. up. Okay. So this is about retirement. This is still under discussion, and there was question, how you actually retire pass"
  },
  {
    "startTime": "00:50:02",
    "text": "we already have to pass a random frame, but then the question is, what do you do with all the connection IDs. And the proposal at the table is at the moment that we don't don't introduce a new, like, pass retire frame, but we actually just use the pass add on in frame. And then after you have a bond in the past, you should also retire all the connection IDs, of that past. Or did I miss anything? Yes? Okay. Yes, Neha. Yeah. Yeah. The the question is whether this is implicit or explicit. Exactly. So any thoughts, opinions, feelings, Alright. By by experience, I would say, explicit. In place it is better in that case. That is if you do if you abandon the past, you have abandoned all the CIDs for that pass. So, yeah, there is a lot of caution on this issue already. And I think that's, direction that people probably prefer. But this was one of the bigger issues, so I wanted to bring it up here. But you can also comment on the issue if you have any additional stuff. And Arne, Yeah. I'm in favor of doing it implicitly. Not sending the frames is nice. And then you also don't have to deal with, with the weird condition where in the past it's abandoned, but you never receive the retire frames. So what do you do then? It's just easier. What else goes, at the same time as the connection IDs. Are there other thing? Are there that are being cleaned up at this point. No. Nothing. I mean, the packet numbers space handling. Yeah. So there's a bunch of stuff that you drop when the path goes away. Just drop it all. And and that's just neater and cleaner. Yeah. Yeah."
  },
  {
    "startTime": "00:52:01",
    "text": "As a whole, we just wanted to point out that this is one of the complications due to us having the concept of introducing new pass IDs and returning them. If you move to then we don't have this issue. At all. Yep. Another interconnected issue. So, Magnus I think it's fine with implicit where I've I've worked. I think it of the things I stumble on this during the discussion around this we had in the hackathon, etcetera, has to do with more with the fact of is this symmetric, etcetera, with the path abandoned, or you're forcing the other point to also close the reverse path So that's It's more connected to that. I think it's some cleanup send it there. But Yes. So there's definitely some clarifications needed. Is I believe that was my last slide. Right? Yeah. I think so. Yeah. Yeah. Thank you for the Good discussion, everyone. This is really helpful for moving forward the multi path work. And, yeah, thank you to the the office Thank you. Yes. And our next topic is, act frequency. So, Mia, you can't go very far. I'm sorry. Hi. We saved you from getting all the way off the stage. That Ian is doing some power punmingle here, but I guess not. Whatever so the egg frequency draft, we had a last call We, got a lot of feedback. Thank you. And we have a new version next slide. Should just stay here, I guess. Thank you. Yeah, the working group last call concluded in November or"
  },
  {
    "startTime": "00:54:01",
    "text": "ID. But we only submitted the next version recently. We got quite a bit of, feedback we closed all open issues. Thank you to for everybody who provided feedback. I forgot to update the acknowledgement but we would do this in the next version. Next slide. So, there was a bunch of editorial stuff and I'm not talking about this year. And then there were a few stuff a few things which were clarifications, which I want to quickly name here, especially the first two there was still a little bit discussion about how often you should send these frames And, we provide a little bit of non normative guidance about it we updated the text to make it more clear what we mean. But I think there's still different opinion in the working group about how often you should send things and how on what's kind of the minimum amount of act you should require, which is, like, kind of we are a little bit recommending at least 1 egg per round of time, but, like, for example, Gori sent another email to the mailing list. So we believe what we have in the draft right now has rough consensus, but just to name that that there was a little bit more discussion. Yeah. I don't think anything else that needs to be mentioned here next slide. I also want to point out some of the issues that we closed without any actions especially I want to point out the one about, how Cs are handled. So, or ECN is handled. So what we're trying to follow here is basically what is kind of done in the TCP, accurate accurate easy and draft. But there was recently an email on the main from Ingram actually proposing that we could do smarter than that. We kind of basically say, we leave this to, like, future versions or whatever and didn't address it any further here. But, you know, just to make you aware. The other thing that we also didn't further address"
  },
  {
    "startTime": "00:56:00",
    "text": "is what happens after an either time, we have, like, we edit only a few version ago, we edited send and saying, after an idle time, you should soon send an egg. But we don't specify this nomatively or say anything further. What soon means or whatever because we feel this is not an issue for this draft. I mean, of course, impacted by this, but it's more general issue. So people want to work in that space probably and and you know, should do it in future. That's on this slide. And then the last one is actually where I just wanted to sure we are really doing the right thing because we had 2 issues that did impact the normative. Parts of the draft. So one was a clarification about max act delay. And we we changed to a shoot here, and that is just, basically I would say that was an oversight because we did change a little bit the language and the whole draft saying This is not something we can enforce. This is just like a guidance we give and then the sender, you know, can still do whatever they wanna do because that's the truth. And so we changed this mask to a shoot. And then the other normative party was also that we changed the error type, when you when you receive an invalid value because I think that was just the wrong error type. The frame encoding error also used an ROC 9000 for a similar case. So we just, you know, picture, fully write type, no. So that's the first issue, and the second issue there was a comment saying that, or like the was a sentence saying in the draft that basically this this extension only changes something if after you received the first Frame of any type, I guess. And that wasn't fully true because we actually changed the ECN handling. And so the solution I went here is to actually keep this statement and say, actually nothing changes as long you don't send an xfrequency update draft. Which also means that as long as you're electing threshold is 1, you do the same as is written in RC 9000. And only if you send a new value, you you actually"
  },
  {
    "startTime": "00:58:01",
    "text": "change your ECN handling. And, you know, this is, like, it's kind of a little bit random. You can do one or the other. I just felt We need to clarify it, and I just felt this is kind of a little bit the nicest solution because it actually doesn't change anything as long as you don't send an occupancy draft. And that's actually I hope Ian, read this PR, but I don't know if you have if this is the best solution for you. Yeah. It's fine. Okay. Set, Yep. Sorry. Jonathan, I just had a question. Do Just wanna switch in CE and nonce or any change in the ECN because if your path change from bleaching to non bleaching, for instance, you probably wanna know that too. Yes. Sorry. Can you saying I was Do you wanna just have the change from CE to not CE, or do you wanna have any change in the ECN bits. Because, like, if you if you had a pass change from bleaching to non bleaching, presumably, you'd wanna know that. As well. Right? So if you if you if the previous packet So 1. Next packet is 00 because but something on the path has started leaking is the The question only when do you wanna know it? Right? Do you wanna know it is immediately, or you wanna know it when you, anyway, send your next act. And so because this is a congestion signal, we wanna know it as soon as possible. Everything else doesn't matter that much. Okay. I guess I mean, again, this is just we're following just what's kind of specified in the Accutian draft at this point. Lucas here, speaking as an error code enthusiast. I can't remember if I raised this issue or not. You did. did. You Yes. Yeah, I mean, I think error codes are important from an operational perspective, but also they don't really match So from a, I don't think it's a normative change really really anything cares about. The the muster shed is is as discussed, further supportive, the, I just wanted to make that yeah, clear. Thanks."
  },
  {
    "startTime": "01:00:00",
    "text": "not? I mean, who knows what actually is normative and what's But, I mean, this is just what I wanted to flag because I think these are really the only things that kind of impact anything that close to normative? If everybody likes it, then we working with last call. Yep. Ian, in the queue. I just wanted to give a huge thanks to Mary for, like, pounding through a ton of changes to get this over the line. You know, I I I enormously appreciate it. And so is the quick working group. And yeah, I think I mean, I've looked through a lot of text after effect, and and it all looked really good. And in many cases, I'm like, oh, I thought that's what it opes, maybe not. So I I think a lot of the changes were of the form In my mind, that's what I, you know, that's kind of what the authors have been content he did a wonderful job. So anyway. Thank you. Yes. Thank you, Maria. This is our longest living document, and the in the working group at at this point. So, happy to see some progress on it. And, hopefully, we'll We'll go forward with running another working loop last call and that will be an actual working group last call rather than a 1st last call. What do we have next The last last call so now we are going to move on to our next topic. Which is, Martin and talking about we're calling this quick security considerations, I think. Is what I make talking about Arnare you okay? Come up. Yeah. In the last last couple of months, we dealt with Closer to the mic. Okay. Why is that still loud? Mary's not very tall."
  },
  {
    "startTime": "01:02:02",
    "text": "So we we we we dealt with, 2 resource ex exhaustion attacks. Against against quick protocol itself, if you're not because if you follow the RFC 9000, By the letter, then implementation would be vulnerable to these attacks. And a lot of implementations were affected. And, we've we've now made these, Made these attacks public? And implementations have been fixed. So now is a good time to talk about it and what we can learn from this. For the design of new protocols, next slot. So, this is actually the 2nd attack That we discovered but I'd like to discuss it first because it is very instructive. So let's take a look at how quick manages connection IDs. During the handshake. One peer, let's say the server is setting a limit for how many connection IDs can be It is willing to store at the same time end, end, say the limit is 3 So now the client is allowed to send 3 connection IDs, right, and it does that since connection ID 0, connection 81, connection 82. However, This is not where the story ends. The client can send new connection IDs now. As long as it retires connection IDs that it should, previously, So Here, it sends connection ID number 3, and it says retire all connection IDs before too. Which means connection IDs 12 are now not active anymore. This is very useful if you're running in a load balancer setup. And you load balance it as, like, a, a config rotation and you can you know that The first two connection IDs"
  },
  {
    "startTime": "01:04:02",
    "text": "will soon not be, be routed anymore. So you can tell your peer, like, yeah, that's like, small period of time where I'm still able to receive packets with connection ID 0 and 1. But Soon, soon they will go away. So please return this connection at ease. And the server is then, required by the, by the protocol. To, send retire connection ID frames for connection at D01. So why is this a problem? Next slide. And it's a problem because of congestion control, basically, So quick reminder here, when a packet loss happens, supposed to reduce your con congestion window. Usually by a factor of, like, a half something in on that order of magnitude. And if, repeated packet loss occurs, Then your congestion window is collapsed again and again to a minimum value of two two packets. Around your time. So what an attacker can do now, the attacker can just receive your packets. But claimed that, let's say every other packet was actually lost. So now if client does that, the server will very soon arrive at a condition window. Of just two packets. Can basically not send anything anymore. It gets even worse. RTT measurement is also controlled by the client. A malicious client could say, like, yeah, I'm I'm acknowledging packets I'm not acknowledging any, any packets that are received in the last five I'm only acknowledging packets they are re received 5 seconds ago. This will make the server think that the RTT is 5 seconds plus the actual RTT So now the server is in a situation where it can send 2 packets, 2 packets, every 5 seconds. And now the client just keeps on sending"
  },
  {
    "startTime": "01:06:00",
    "text": "new connection ID frames that require old connection IDs. So, the server is now building a queue of, like, I need to send this retryer connection. Frame. I need to send this connection, a retired connection ID frame. And the client can just keep on going, sending new connection ID frames. Until the server runs out of memory. Next slide. So the question here is, like, why, why did the flow control mechanism that we try to build here, not work. And How is it different from other flow control mechanisms that we build in quick that work and that are not vulnerable to this kind of attack Let's look at how quick streamflow control works. The server declares on a new stream, I'm willing to receive Let's say a 100 bytes. This allows the, the client to send stream frames that carry data up to this, to this up offset. So let's say clients, sends 22 stream frames that go go up to a 100. And now it's blocked at a 100. It can't send any more new data. No matter what the client does, it can send more. Until the server sends some extreme data frame, granting new granting new credit here, up to 150. Now, the, the client can send 50 more bytes. So we can see that the main difference is that it, that it, requires Request action from the server. Unblock the client. The client has has there's nothing the client can do on its own. To free up, this Flow Control Credit. Next slide. So the attack that we've, that we've seen on the, on the connection ID mechanism is kinda similar to, The one that a lot of us had to deal with, last year, which is the HTTP 2 rapid reset attack."
  },
  {
    "startTime": "01:08:04",
    "text": "And, a quick reminder, in the rapid reset attack, in in HTTP 2, and it does not apply too quick. The client the the server would tell the client you're allowed to open a hundred streams at the same time and the client could open these streams, but could could then reset the streams immediately freeing up the limit, again, and then open another 100 streams reset them immediately and do that over and over again. No interaction from the from the server required. So why is, why is quick, not, not viral, vulnerable to the attack, It's because back in 2017, we actually fixed that. We fixed that for a completely different reason. We didn't know about, rapid reset at this point. We fixed that because when you're dealing with a transport that's not that's not ordered, but where packet reordering can occur. The client and the server might not have the same might not have in picture of which stream is open at any given time. So it is basically impossible to, to do a, to do accurate accounting and to, to close a connection when a client violates that limit, because this violation of the limit could just be have been caused by packet reordering on the wire, and you don't want that to kill your connection That's why we we switched Quix, the the way that we grant stream stream limits in quick to advertising, A street and maximum stream ID. Now the server says like, you're allowed to open all streams up to stream ID 100. And there's nothing the client can do to increase this limit, until the server says, like, okay, now you're allowed to open, open all streams. Up to stream ID, a 120. Next slide. So looking back at,"
  },
  {
    "startTime": "01:10:03",
    "text": "at our specification process. Like, let let's say Let's say our RFC 9000 was not yet a thing. How how would a good solution for this? And what we could have done is we could have introduced a Max connection ID for him, The server could declare you you can send me or connect IDs up to sequence number 10. And then at some later point, 1 connection IDs are retired. So if I could say, Okay. Now you can send me connection IDs up to sequence number 15. Would have been nice. But, That's not the world we're living in. RFC 9 900,001,000 was shipped. And is widely deployed. So, This will be this will probably be a hard be be a hard fixed to make at at at this point. The the way that most implementations have actually fixed fix this vulnerability is that they now implement, an explicit explicit check. For, like, how many, how many retired connection IDs do I have queued at any point? And the good thing is that Retire connection ID frames, are really, really small. Adjust a few bytes. And if you queue, let's say, a 1000 of them, for a a connection. You're not wasting a lot of memory. The the the problem only occurs if you're queuing like a 1,000,000 or a 1,000,000,000 of that, then you run out of memory. On the other hand, in on on on a connection where you're not under attack, you should never be, you should never be in a situation where you have a queue of, like, 1000 to tire connection ID frames. You know, So it seems like if you just, if you just implement a check where you say like, okay, if I reach if I reached this absurdly a high number, can just kill the connection. And that works. Next slide. So now we're coming to to the the second attack"
  },
  {
    "startTime": "01:12:00",
    "text": "which is actually the one that we found first. And, very similar a very similar thing applies for quick's path validation mechanism. Because the client can send a path challenge, with with some some random data. And then the server is is required by, by RSV 9000 to send a path response. And RFC 9000 says, like, you can't you can't drop this. If you receive the past challenge, you must send the path response. Obviously, yeah, the client can do a can do the same tricks with a a collapse condition window inflate the RTT. And, the server starts building a queue of path responses. The fix, in this case, is pretty easy because there's not really anything that breaks in the protocol. If you, if you drop a path challenge under those conditions, I mean, the packet contained the past challenge could have been lost So that's that's a way to to mitigate this attack. Technically, you're not you're not RFC 9000 compliant anymore. So maybe maybe we should do something about this and, use the Oreata process. To, to change RC 9000. Martin Duke Google. This kind of gave me deja vu because I I thought we argued this out, when we're doing 9000. What's the new wrinkle is this congestion control bit and I'm not real I I guess I don't follow why that creates the problem. But, this is issue 3509 in base drafts and the end result of that after much argument and, like, It's a very long thread, but we put in some some text at the end of section 512 of 9000 about, limiting the amount of IDs that is that a endpoint has to track, and you can it that you should you you you'd"
  },
  {
    "startTime": "01:14:01",
    "text": "you need not put the store more than twice that could ID limit in terms of, like, pending, retire connection IDs. And, throw an error called connection ID limit error. If if somebody somebody's doing something pathological about retiring connection IDs. But, again, like, I I mean, you know, Sitting here at the mic, I I I There is this this, pedestrian wrinkle, but I I don't really see why that supersedes what was in the text. Yeah. So it's not clear if this text applies to connection IDs that get retired because the the, retired prior to field I'm I'm I'm I'm not I don't recall if we discussed this issue before we in produce retire prior to or or afterwards. It was very late. The the fact jump in. Let's notate it real quickly. Can you repeat number? the issue Yes. It is, 3509 in the base drafts repo. Thank you. Thank you. The the fact that out of the 18 stacks that we, that we surveyed, 11, if I recall correctly. Were vulnerable to this attack shows that apparently what we have in the RFC was not sufficient. To prevent this vulnerability from occurring in the wild. I mean, it is advisory. They're, like, Shud's and and maze. Mean, you know, So, so, like, this this has been used full because, people have not taken, some advice in security considerations and in the section. So thank you for doing that. I'm not convinced that 9000 is broken in that respect. I mean, personally, if I remember my position is who would fight, I would prefer stronger language in it. But I think there are the tools here for for service to protect themselves. Thanks. Yeah. There's there's also the the catchall, that At any point during the connection, you can kill the connection. Alessandro Giddini Clout there."
  },
  {
    "startTime": "01:16:04",
    "text": "I don't know that this is, like, an arata situation where you change that RFC, but like, we we we basically had the same problem in HCP 2, right? The past, like, 5 ish years, we've been fixing a bunch of vulnerabilities and new implementations of HTTP 2 that didn't go through that kind of were surprised about all of the additional changes that they were supposed to do that they didn't make because all of those all of that work was in really collected in any particular place. I wonder if this could be a case of either having a separate draft that says These are all the gutches that you need to, you know, be aware of. Or even like a sort of RFC 9000 biz. With more clear language about all the But then the the other problem is we might, you know, find new issues in the future. So, like, we might need to do a business and business, business, Eric Kaniere. I think this is a good thing to dig into a little bit more on the note about somebody manipulating their RTT or congestion control, etcetera. Like, you can also just be in a place where somebody has an asymmetric path latency. And you can also be in a place where somebody has asymmetric loss. So the the facts that I can send you more than you can send to me is not necessarily something that an attacker has to do. But it it would be worth digging a little bit more deeply. Like, for example, with past validation, it's always expected that some of those may be actually legitimately lost. So, Yeah. I don't I don't know that we're in a, like, o m g, the sky is falling place. It'd be worth making sure that we've at least to Martin's point, like, maybe we should make"
  },
  {
    "startTime": "01:18:01",
    "text": "language a little bit stronger or at least go talk to those implementations and say, Hey, did you read this part? David Skinazi, infinite buffering enthusiast. We actually weren't vulnerable to the particular attack in our spec. And when we were to some of the other ones, And the reason for that is the way you we implemented this is when our stack needs to send a control frame like this. We have a thing called the control frame manager, because That was a good name. And you just tell it, oh, send this thing, and it'll send it right away if it can. Otherwise, it'll buffer it. And I don't know who the smart engineer was who built that thing, but when they did, was probably Ryan Hamilton. You went, oh, you can't just keep adding to a buffer. That could be bad. well, So if buffer dot size greater than a 1000, close connection. And this is the kind of guidance that, you know, maybe it's a quick document. Maybe it's even a more generic thing, but we should provide that kinda ground shoes because the we keep shooting our toes off, and most of the time, the same attack is buffer dot append that can be called in a loop. Okay. I think that's good discussion there. Thank you, Martin. And, as a quick community enthusiast, I wanna thank Martin for his work on this, looking into things and and the wider community for handling responsible disclosure. Mentioned, this affected a few different implementations and could have could have, had nasty side effects. Say. So Yeah. This is good. I think it's a a good example of of how we can identify these issues. In implementations, and get them addressed. So,"
  },
  {
    "startTime": "01:20:03",
    "text": "kazero, we don't wanna come on up. Thank you, my name. I'm I'm Kasulhoka, and I'm going to talk about quick on streams. We did have a side meeting this morning. We hope that we'd reflect the comments that we gather there, in the HTTP walking group, that we'd have session on Friday. So if you have time, please come to that as well. So next slide, please. So Quick. We know, quick is a huge success. It's better than CPLS in so many ways. And We now have broad adoption. Like, major browsers support it or going to support it. And it's already, like, 30% of all websites are supporting HTTP 3. Next please. But TCP continues to be used At least as a fallback. Next please. So now we have this substate of application protocols, Where we have to develop and maintain 2 different set of stacks. For HGB3, we have a 4 quick, we have a GB 3. And for TCP, we have HTTP 2. Head the compression. Different between HTTP to an HTTP 3 Priorities. Semantically the same, but because HEV 2 and HTTPS starts, different the priority logic has to be applied differently or, I mean, connected differently to each stack. MASK has 2 ways of sending UDP Data Rooms, because htp3 has datagrams but HTTP doesn't have data run phrases. Weptransport it's, like, a completely different base for HV 2 and HV 3."
  },
  {
    "startTime": "01:22:00",
    "text": "And this This having 2 different stacks I would continue for any new protocol that we develop in the future. Assuming that we want to support both quake and TCP as a backstop. Next space. So what can we do here? Is it look at what? IPV 4 and ipv6 dash with UDP and TCP. I mean, the European TCP work on top of both IPV and basics. The only difference is how the addresses are being represent. Represented. So if you write code for UDP or TCP, it works on both ipv5andphysics. So The question is, we do the same for quick? Next space. That's how we came out the idea of, pick on streams. It's essentially a back port. Of the quick API contract, on top of TCP. So you now have quick streams that can be used on TCP. And tealice. So once you write the code that uses the quick API, provided by quick stacks. Then you can run that code either on quick. Running on top of UDP or running on top of click on streams that runs on top of Sanjay CP Next please. So Our goal has been to eliminate a need to develop new things on top of 2 protocols, like HV 2 and HV 3 are different protocols. Protocol, protocols. And the other goal has been to eliminate the need to deploy 2 different ostax when you control both sides. So regardless of if the network passes or blocks UDP, we can use The same application protocol stack to run the protocol. And the nongols has been to not spend time optimizing TCP, like, solving the"
  },
  {
    "startTime": "01:24:03",
    "text": "the line blocking issue or improving the quick frames design because so so Our belief is that quick works in most cases and performs greater. So We saw that quick call streams can just be a fallback. With, minimal effort to design and implement. David, do you? Okay. Alright. Next, please. So the design of draft 0 is that We just say that Saint Creek Vian Frams on top of TRCP and TLS? And there's no app frames. All frames are implicitly act. And we also prohibit the use of, frames are related to stream operation, like, connection ID because we don't need connection IDs when we send quick frames on top of TCP And transport permit as I exchange using the first frame called transport parameters And for what's left, we've said that minimum maximum of frame size is 16 kilobytes because That's the size of t s circle. And it provides optimal performance when you are sending stuff on TLS. And for what's left, I succeeded in implementing a walking, proof of concept code in quickly in half a day. And it only took a half day to use that to implement each 3 of our quick constraints. Inside H2. So I'd say that it's fairly easy to implement Next please. And this is a slide that I presented, created, and I'm not gonna present it. We're short on time. We have 20 minutes in the HB agenda to discuss this stuff. I think we can move some what I was gonna talk about there. I'd like to I'd like to I'd like to ask people in the queue to come on up and speak. Bearing in mind, we do have a lot more time in HP's agenda to discuss stuff. So if you could keep it on track to quick specific matters that would really help us. David, please"
  },
  {
    "startTime": "01:26:03",
    "text": "Yep. Zara? Lucas, yeah, there. Can I use my Eddie kind of thing? Actually say something before you start the discussion. So I think I think this this I have talked with you and a couple of others. This this seems like we're trying to solve something of your state. So my, any kind of question is, like, and also, like, those who will be discussing this today is What I'm trying what's important for me to understand is, like, how this kind of work, helps deploying quick in the Internet. It's it's we're really doing good job getting it adopted. And the one of the dream for for us, perhaps, was to replace TCP altogether. Now we are trying to run quick over TCP. I'd like to just understand how this impacts the adoption of quick and where things should be fixed. So think about it when you are discussing this thing David's Ganazi, quick enthusiast. Mask enthusiast. Transport enthusiasts. Also co author of every single RC on this slide. So Thanks for this. Unfortunately, I think this proposal is actively harmful to the adoption of quick. The Fundamental concept of letting folks write applications over quick and saying, don't worry. We'll just hide it under the hood. Is super dangerous, because Quick and TCP work very, very differently. Right? And especially when it comes to HTTP Datograms and things like that, and I think one of the goals and to answer is I had question that we should keep in mind is the overall adoption of quick. And not not the overall adoption of quick over things like TCP, because that's gonna be a broken"
  },
  {
    "startTime": "01:28:02",
    "text": "quick that performs incredibly poorly. So in particular, let's say, you know, You have an application. Like, there are very few applications running directly over quick. And we don't wanna necessarily tell folks to build them directly if they don't know what they're doing. But let's take an example of a concrete one DNS over quick. If you start running that over quick over you're gonna have a really bad day instead of saying, just use DNS over TLS or you know, DNS or HTTPS or DNS or whatever your favorite thing is. So I don't think the idea of making it easier to just deploy something over quick. With this abstraction layer is the right answer personally. So I'm not sure if I agree with the assumption that this an internal panel This this has negative impact on our I have auto deploy quick everywhere. I mean, if we think that way, we shouldn't be developing mask for achievement to we shouldn't have created web transport http2. But the fact is that we've accepted the reality that we need to support TCP. And, the the sad effect that we have had is that that effort of accepting the reality happen at each extension that we develop. And that have created duplicated efforts. And I think that's harming us both in terms of the writing standards as well as maintaining implementations. And I would like to have a fix that applies to multiple approaches. At the chloroquelay. Eric, Kanir, to try to keep things reasonably quick. I don't know that I'm deeply into the actively harmful place, but I do very much want to solve the duplication of effort and the problem here. If you look at web transport over H2, that started as an effort to provide similar things as you could get"
  },
  {
    "startTime": "01:30:01",
    "text": "from just the multiplexing layer, which we decided was quick rather than H3. For people over TCP. So it would be interesting to have a little bit more discussion about how these things are going to layer and why we can't choose some of those existing solutions. A brief note that might be useful. You mentioned that about 30% of servers are using quick. We're also seeing traffic from Apple Platforms finding that about 30% of resources on the web are also loaded via quick right now. So that is that number continues to go up The number that isn't going down is the number of networks where we think quick doesn't work. So Do we push on that? Do we let this be the release valve and not need to push on that anymore? But either way, I think this is very much a a problem that would be good to solve. I'm not totally sure if this is the solution versus any of the other possibilities that we have. But someday, it would be nice to not have to be in in this weird dual world. Ian Thanks for bringing this up. I'm not sure if I Sweat, Google. support doing this or not. I need to think about it a lot more. Although I will note that we kind of already did it, technically, If you look at the web transport over HP 2 spec, there is the web transport reset stream capsule the web transcript stops at Nincastle, webcenter transport stream capsule. And that I mean, if you remove the w t underscore, from each of these, you basically end up with Quick, overhcb2 over TLS over to speak. Yeah. Yeah. I don't know. There's a few more turtles maybe there. Which, but, very much not an accident. And so I I guess to some extent, we sort of already did this. Which is not to say we shouldn't do it, but, like, I'm just I'm just For for those, you know, and, you know, David, I realize your your name was not an author on that are the chair, so probably you noticed it happened."
  },
  {
    "startTime": "01:32:01",
    "text": "I don't know. So it seems like we're kind of already doing it. So But but whether it needs to be separate effort and quick, I don't know. Yep. I think that's area a good point. And The problem is that The abstraction. Is not provided as a Cork Place. Well, I mean, So if you want to use, We we transfer over H2 at the abstraction of quick. On TCP whereas using quick directions. On UDP, then you'd have you essentially have 2 different tax that you have to maneuver with. That's not a good place compared to where you just have one quick stat that does both TCP and UDP. That that was actually, taken into account when designing it. So let's let's go more offline for that just in the issues of a reminder, please be brief and comments focus. We have a lot in the queue. We have still some agenda time to get through, so please In in my previous job, I I was I was layering protocols on top of quick that are not http, and I always had to deal with the TCP fallback. And it's just, it's just painful. Just getting streams on top of TCP is painful. These three multiplexes around there. Are are are a big pain. They are terrible. So this, this would have been very much appreciated if we had had this back then. Also as somebody who is implementing web transport over over quick. I really don't wanna wanna implement web transport, over HTTP 2. Sorry. It would be so much nicer if we had this. And, my web price, what implementation could just switch to switch to quick on streams. Krishna. Christine. Yes. I'm here. Yeah. I already said that in the in the email discussion."
  },
  {
    "startTime": "01:34:02",
    "text": "I I understand why Caso wants to do something like that. But I'm not sure that we solved the problem. I mean, the reason why UDP is blocked in various network. It's not because their Wi Fi water cannot do UDP. It's because some decision was made that they wanted to inspect traffic or protect against whatever And, It's very unclear. That those same network that want to protect against traffic, etcetera, will let you pass quicker TCP. That's very unclear. If I if I was designing a solution like that, saying Layers on top of layers. I would want to do the solution probably easier on top of web transport because they have already solved the problem. Or on top of web socket, sending the quick frames on top of website. So the I I think our response would be that if there there are those mailboxes that look deep into the pocket. Then they are also likely to block web transport of H2 or those new protocol that doesn't fall existing HTTP 2 does. Full. Yeah. But what what is is a a wide body of experience of doing stuff like, say, running Skype over HTTPS. By doing a connect for a proxy. And so That's the kind of stuff that does walk. Well, Just trying to do your own protocol on top of TCP, typically does not work. Well, so you're saying that there's assumption that some endpoints. Look at the traffic pattern, and they only are the ones that they want to allow. Allow. They know that they"
  },
  {
    "startTime": "01:36:02",
    "text": "he exist. So Any new attempt Yeah. being developed on top of TCP would be blocked regardless of it being protocol x or protocol y. That's my point So but we our our our precedence is that we expect some protocols to go through TCP NTS. That's why we continue to develop new protocols. And so then I think the question is, why do you think that click on trends of DB blogs while the other protocol being developed on top of TLS or TCP. I should be watching it, but, it would work. I don't think there's a good discussion to In the interest of time, I'd like to to move on to to it's a good discussion. Please carry it on, Martin. Martin Duke Google. Three things quickly. First of all, as a pro engineer, this is super duper cool, and, like, it was fun to look at. Number 2, I'm concerned about an x kcd927 problem. Just creating another permutation of things to support, like, I mean, I I think there's different working group that could give me an answer would be very satisfying. Which is that know, the the path here is that eventually we'll deprecate HB2, and I can delete that code from my stack. Because this is so awesome. If it's not if that's not gonna be the answer, it's some point in, you know, 5 to 10 years. This is, again, just another thing I have to test and another thing I have to support. And that's bad. And, of course, the other question, which I think it's we we can't answer yet, which would be interesting to to to answer is what is the performance impact of this versus running over HTTP 2? If it's similar or better, that is more appealing than if it's just worse. Of extend why would I do this? If I have to keep HP 2 as well. Thanks. Ted. Saturday, if you could go back to slide and while he's going backslide, I'll say, I think you have an interesting idea but that the current design is very fundamentally problematic because you you would like to have a common API, but, functions beneath the common API are significantly different"
  },
  {
    "startTime": "01:38:03",
    "text": "in ways that impact the utilization of the API. And I think the The result of that is if you have a common API that you're using natively for quick and appropriately, and you're treating the things underneath it when you fall back to TCP as if they were behaving as if they were the UDP structures, under quick, you will find corner case, after corner case where you have to work around it. Where you you can say, okay, we're not acknowledging it, or we're gonna TCP in ways that make it look UDP like I just, I, I think that you're actually driving complexity into a different part of this stack, that you'll still have to deal with at the end of the day because functionally TCP and UDP just aren't the same, and you're not going to be able to get the same functions with TCP over streams as you're gonna be able to get sorry, with quick over streams as you're gonna be able to get, for for quick over the, the current stack just because of those differences. So I think if you wanna tackle this, you actually have to tackle it by looking at what you need CCP to change to enable, and that's a much bigger problem, but it also has a much bigger payoff for potentially other parts of the protocol stack. Thanks. Thanks. Hello, Slab? Here are slot for some acquisition, Scalar. I think there are 2 main reasons why UGP for 3 or Whatever your port, favorite port is is, blocked. One is because people don't know error, just enable TCP HTTP443. And another reason is because, some enterprises want to inspect their traffic firewalls, proxies, and so on. And if they try to inspect quick embedded in TLS and TCP. Things will go bad anyway, and that will be a troubleshooting nightmares. I think we've been here before. With WebSockets circa 10 years ago, when it was introduced it was also blocked by default by all those fancy proxies. And the solution was"
  },
  {
    "startTime": "01:40:02",
    "text": "us to deliver some cool enterprise, critical employee, business applications that relied on WebSockets. And now 10 years later, I think most firewalls and proxies do allow WebSockets simply because things will break for enterprises. So I think the solution to increase quick adoption would be to deliver more, great applications. With multi path, with web transport, and all other great benefits that quick provides, and I'm afraid this solution actually goes against, that, it actually encourages not to enable UDP 443 to potentially gain those benefits. I'm Final brief comments, Colin and Victor. Colin James. I'm a David Schnazi enthusiast. The the point I wanted to make here is I'm very much interested in the the media over quick type cases, and they are very impacted by latency, the the running over TCC doesn't doesn't work very well. And the VoIP guys have tried all kinds of things. I mean, We have TCP stacks where you act stuff that you haven't received to reduce problems and stuff, which might help this. But I think that we're trading off short term problem versus the long term. Long term, we definitely agree. We don't want quick to be blocked. That would be the best thing. And we go and ask people why are you blocking UDP today? There are some people, as people said, inspection, but I think that is almost gone at this point. Because all the VOIP major VoIP applications are doing strong end to end encryption not interceptable on these types of proxies. I think that the proxy argument has mostly gone away, and we have a real similar data to support that. What they do say though is these these bizarre conversations about, like, Well, there was some malware virus that used UDP as a control channel some point, and we block all of that. And I think that those arguments are gone when you have a UDP can And I think what we need to do that the best long term thing for us to use this great application called quick and all the things are built on top of it. To go argue, look, quick is not like"
  },
  {
    "startTime": "01:42:01",
    "text": "UDP from long ago, and you can allow you to quit. Through your firewall running over UDP, and you don't need to block it. And If we if we have a fallback for that that just instantly disappears, there's no incentive for them to ever do that. So I don't I I I really feel like the long term best solution for us is let's do greasy like we do it everywhere else. And let's say that, you know, we're going to insist that the the UDP version of quick works across this. And I I think that that will be the best long term game. I definitely get the short term pain of everyone who's like, it's just such a hassle to implement fallback from my app, and everybody has to do it in 3 different ways. But I wanna get out of that long term you know, of that that short term pain. So I sort of lean the other direct I think it's a very complicated trade off that we need to think about carefully and that we should think about the long term path because the future's bigger than the past, not the short term path. Thanks. Pick the I wanted to comment on differences between quick and web transport, web transport aims to provide the same tractions that quick provides. And in fact, it's a Google MOC implementation. Works both over Rockwig and over web transports using the same toads because it was, it was, is intentional and designed to do that. So, I don't think that tractions would be the issue. And other than that, from my current understanding of the draft, it is That's mostly the same thing. That's what transport over h two does. Cool. Thank you very much all for the discussion there. Thank you. then move on to the next agenda item, which is the quick And BDP frame. Okay, Corey?"
  },
  {
    "startTime": "01:44:00",
    "text": "Hi. Have you Yeah. Okay. What's second? There we go. Yeah, I've got quite a long delay So, I will try and work this with the delay. This is quick BDP frame extension draft. The BDP frame is the mechanism we use to transport the data. It's in the 9 It will have to be the way we do this. Let's take next slide and go through what we plan. To do with the thing. Are you doing next or am I? Your day. Good. Okay. So the Use case for This is it's transport what we call congestion control parameters. CC PRAMS, careful resume uses these for rapid sender side rate increase So it can do a fast congestion control startup primary target of this is quick. Oh, Given this a center side change, could we propose a receiver's side change? Can the receiver participate in deciding whether a change in the congestion control parameters is a, useful thing. Take a look at time. Are there trade offs? Are there interactions that could be done with the applications? So two different possibilities obvious ways of using this first one is for the client tells, sir, that please don't use careful resume. Because I'm not sending much data because I don't want to on this particular connection, I'll use it on a later one. Or because I know that something about my path has changed it really isn't a great idea. It's not going to look out well."
  },
  {
    "startTime": "01:46:02",
    "text": "The second use of this is to allow a the CC parameters to provide a hint to the upper layers to basically you know what to do. The first thing not much above the congestion controller. Is it allows you to modify the quick floor credits, let you take advantage. Of a big change in congestion control. Those other applications also possible and other parameters you can send. So next slide. Basically, the whole thing starts by the client saying, please can't you tell me, the congestion patrol property information from the server. Next slide. The server then sends a set of getting a toll parameter to the clients, and these authenticated so you can't change them at the client, but you can read them And the set should be extensible because what you need now for congestion controller at the moment. Depends on what control that you're using, and there might well be good things you want to add into Basically, once you get the information of the client, you save it, next slide. You can use but you also return it back to request the server to take advantage of this information. You've given it. To told the server not it it it Oh, this is hitched the server So we had questions last time, on the list, but what happens to this request. Well, it goes to the server, the 70 use it like anything else it does with congestion control the server is in control. It could decide whether to use it or not. It gets the s you've seen the information from the client, knowing that the client can give its perspective, So next slide, please."
  },
  {
    "startTime": "01:48:02",
    "text": "Well, This draft has changed its words significantly since the last time, I tried to present it. We changed about 350 lines of text. Which is most of the draft to make a complete rewrite, which says the that is in a more abstract way to try and bring this into something which could be extended and used for of the congestive control parameters We are not particularly worried about what that set of parameters are. And we also don't really mind if the working as a strong preference to change the methods even the draft name can change What we want to do is try and allow the receiver to take part And negotiating significant changes and the way in which the path is used So, Do you think this is something that we might, be able to work on in quick. Is this something that the working group has appetite to help us Make Are people And this Here's the question. Mine? What? I'm a big fan of doing careful with the resume. I'm also a big fan of starring stuff with the token. And using that on subsequent con connections These are all very useful things to do. In your quick sec. However, I'm not sure if it's useful to communicate these things the wire. We have the token, and we have the session ticket and the server is able to Could it state?"
  },
  {
    "startTime": "01:50:02",
    "text": "Into the token or into the session ticket, wherever it likes encrypted, and send it to the client when the connection is restored, resumed. It can restore the parameters from the token and resume its congestion control. Same on the client side when the client saves the token or saves the session ticket, it can attach some, some data to it and then restore these parameters. When it resumes the connection. This is all possible. Right now, we don't need any protocol for this. This is purely an implementation decision What I'm worried about when we communicate things on the wire is that The congestion controller in use is purely an implementation decision. Client and server can use vastly different, congestion controllers. They can switch congestion control between, between connections, they can do all kinds of stuff. And I'm concerned that The values we communicate from the server to the client or in the opposite direction, have might have very different meanings, to the congestion controllers used on both sides. And if we do something like this there's, there's the implicit, assumption that congestion controllers will work in the way that they do right now. And I hope that there's more, more evolution going on in the congestion controller field, and we will have congestion controller that look very different. From what we have now in just a couple of years. And I don't see how this fits into this this extension. Well, I don't see any of the problems that you that Martin brought a player. I'm not sure that the details that's going to be revealed. In this case, it's going to be the amount of"
  },
  {
    "startTime": "01:52:03",
    "text": "the amount of RTT, some private data which you throw back the server and the server will decide what to use. I'm not sure that the giving this token or frame to the client is going to stop the evolution of congestion control. I think it would help, though, if the client then have hint has hints that a particular interface is not available or the interface has changed its rate. They couldn't tell the server about this. So Not quite sure I'd buy that, but anyway, go go ahead, Ian. Yeah. Ian spoke Google. Yeah, I've been I mean, for this has been talked about for a while. I've been trying to think for a long time about what use cases I would have of this I I haven't come up with anything really solid. I think I think The the the cases I'm currently most concerned with are actually better addressed by Skone Pro. Gosh. That's a terrible name. To be completely honest. So, I I guess I would say I'm not interested because I can't find a use case in the different situations I've looked at where this is more compelling than, say, putting something in the token or doing something else that I think works equally well. So not interested personally. That's why So I'm going to jump in here both as, an individual. So the I I think, Corey, would it be accurate to say that, not just the you're not just interested in potentially the The idea of sending congestion control parameters on the wire, but the general idea of doing this kind of optimization work in a quick working group or or are you because I think"
  },
  {
    "startTime": "01:54:01",
    "text": "What we're hearing is pushback on the particular solution, And I'm not sure that that's really what you were trying to get more clarity on and what we were trying to get more clarity on. Yeah. I wasn't I mean, I'm not I don't care about the particular solution I hugely think it matches which quick mechanisms we use to do this. But I know that we have implemented it using tokens and always implemented it. Thing. BDP friends, I was more interested with the client gets to say whether it's a really bad idea to do a congestion control change and give that hint back to the server So the sender side has some clue that something's changed at the client. Hey, Corey. Lars. Two two points. One, we we tried something similar for TCP a long time ago. It was much simpler in the sense that We had this idea that when it client knew it was gonna be disconnected Right? If if the sender doesn't know that, then it's just exponentially doubling the retry time when the client is back, right, it would be nice if the client could tell the server. Hey. I'm back, and I will keep sending stuff. And that sounds great on paper except what it does is it synchronizes all the clients that are, for example, on the train going into a tunnel and you have this massive restart event, because all the clients say I'm back. Right? So there's some subtle side effects here when when trying to sort of exchange this sort of stuff for the best intentions. Right? The the other thing I was gonna say is that This is not specific to this presentation, but maybe the the theme of some of the presentations. I said, we've designed quick with a lot of extensibility, and you can just scrap code points and and, like, do stuff and there's no need to, like, with TCP to standardize ahead of time. So so I would encourage, like, to experiment and do stuff and and then come here and say, you know, we tried this. It seems to work pretty well."
  },
  {
    "startTime": "01:56:01",
    "text": "Here's the ID and here's how we wanna do this. Right? So I I don't think we need a front load of standardization in in quick so much as we needed to do in TCP, and we should take advantage of that. So, just speaking as chair, this is a some work that's been presented enough over the last few years. There has been discussion on the list. There's been discussion here. That's great. Kind of hearing different signals, but depending on who's in the room and those kinds of things. So I think would help from our perspective is to do a show of hands just to put, like, some numbers into things I'm to to to augment, discussion that's happened here. So I'm gonna gonna ask the question about whether that the working group would be interested in adopting work to solve this probably more this kind of use case. Keeping this as as fuzzy. Right? Which is annoying, but that's what I wanna do. So live with it. I'm gonna start the poll now. We're gonna keep this very short. So we've got time for the Clarifying question. This being explicitly sending information like, as described, or this being the general congestion control it's changing information, but not specifically the proposals technically on on precisely how to do that. Sending them information, which was something that people objected to. That's part of the the Great. Thanks. Thank you for for clarifying. So, so Ian? Please please finish the poll. I just had one quick comment after the bowl. Okay. For time, I'm gonna I'm gonna close it there. Kind of seeing, the balance of yes, no, and no opinions there."
  },
  {
    "startTime": "01:58:03",
    "text": "So we'll take that, and we'll we'll follow-up with some offline discussion. Thank you. My quick requirement is just gonna be, now that there's a congestion control working group, I don't think it's unreasonable to consider discussing this there. Okay. Thank you. And Francoise, we have a couple of minutes. If you would like to jump on to discuss FEC, we can give you as much time as we'll have until we, Start leaving the room. Yeah, I can talk a few minutes for sure. Okay. Can we get the sides up map, Yeah. So there's, like, 2 minutes remaining. So I'll try to be, like, refust. Okay. So next slide. So you already know that when you lose some packet, you have to wait one RTT to recover it. Next slide. So with FEC, you can compute reversing those so that you can send it with your packets so you can recover the lost packets without having to wait for the this RT. So it's helpful for our videos. Or, short request response use case, for instance. Next slide So we made a draft for quick and fec. Actually, 1990s on that. We have an implementation or for, last prototype, based on closed verification implementation. So here, the the tool case, especially to make you aware that it exists, that we had interesting results that that that we may want to do some stuff with MRQT given the the recent discussions, on MOUT, recently about FDC. Next slide. So some of our experiments were about, tunneling, and now"
  },
  {
    "startTime": "02:00:03",
    "text": "stream and sending a PC to protect the RTP stream and see how it works with with Greek. So we have not to be streamed, then it's true quick with FDC and look at the, of the video. Next slide. Yeah. So it's a videos of a drone drone live recording. Next slide. You too. So our results So to be quite quick. So, on the y, I access, you have the average SEAM. So the average fidelity of the video And on the exact list, you have the playback buffer, so the latency that you add in the stream. Our implementation is the is the red box the green box is the classical quick, and you have RTP and RTP with FEC, in blue and yellow. And what what we could see is that we had good average of video fidelity results, with different playback buffers. When you increase the playback buffer, so if we increase the latency Then retransmissions will work well, and so classical quick works well, but below buffers, our quick FEC obtained is good, bit of data results. Compared to to the classical quick and even compared to some RTP FECD implementation. I think you can go to the last slide Yep. So here are some links So there's a link to my thesis. I just wanted to make you aware that it exists let me know if she want to collaborate. Keith is not totally compatible to MRQT. Right now, but it's doable because, we've pushed some generic traits to make it work. We might want to to do that instead of tunneling RTP streams. So, yeah, thank you for listening. Thank you for being so brief. Francois. We we had Abby in the queue. I'm afraid we're over time, so I'm gonna"
  },
  {
    "startTime": "02:02:02",
    "text": "I'm gonna have to, like, you sit back down. Sorry. Have questions with friends, I'll take them offline, go on the list. There's some good stuff there. Yes. I just wanna thank everyone for their time here, especially our note takers and any other people that were some great discussion in the in the Java I'll be sure to try and follow-up on some of that too. Yes. And and and and and Thank you all. Safe travels. Bye for now. Oh, Oh, gosh. Right? Yeah. Yeah. I like the like the the splicing it in the chat. That's good. Like, that the coil of freezing in is to get this exactly that."
  },
  {
    "startTime": "02:04:05",
    "text": "It's the same thing. On on the HDTP 3 is the delivered with provocative. I I I want when I thought we might finish early. Yeah. No. But, that's it on street things. I suspect what happened in the HV session is a lot of what people wanna possible It's it's So, like, we come back and go that's my record now and should have done this fight is"
  }
]
