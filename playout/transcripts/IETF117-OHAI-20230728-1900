[
  {
    "startTime": "00:01:05",
    "text": "Alright, folks. We're gonna go ahead and get started here. Could someone close the doors in the back, please? Thank you. hear you hear you. Oh, hi working group now starting. the oblivious HTTP application in our mediation group. folks who like to care about the privacy of their IP addresses. So, welcome, This is our meeting at IT of 17. I think the next probably the note well please note the station here. I have certain obligations as regards things like IPR and our code conduct. We take this very seriously, and please please know your obligations and abide by them. Next, In case this is the first session at IETF, your participating in You missed a lot of good stuff."
  },
  {
    "startTime": "00:02:01",
    "text": "But there's still some tips to pay attention to. please keep your mic off if you're remote unless you're gonna be speaking. And headsets are strongly recommended. on-site folks. Please check-in to scan the QR code. It's not as good as four square, but it will get us a better room ensure that we get us similarly capacious room at the next IT So much appreciated to check-in there, and and we'll use that for queue management as well. Alright. Here's our agenda. We have already welcomed you. We need we'll need a note taker for this session Could I have a volunteer for that, please? Thank you, Ben Shorts. Blue sheets are are no longer blue. We were just using the QR code. We already covered the note well. So thank you to Ben for covering that aspect. I'm gonna pass it over to Siobhan to do an update on the working group drafts and a potential working group draft. And then the main content we have today that for new stuff is a new proposal from Tommy on doing our HTTP, but streaming That is our proposed agenda. Do we have any modifications, batches to that agenda. before we launch into it. Alright, Shivan. Would you like to review the status? Thanks, Richard. Welcome all. Yeah. So our main protocol draft was in the RFC editor queue. but there was a late breaking change that the authors found. So we just to make sure that we are Yep. crossing our teeth and do gutting our eyes, we it back for a second working group last call that the past. We got some or some plus ones on a couple of different email threads. and some discussion on the PR. So we've merged that in. and publish a new version. and send it on again. second time's the charm, I think. I hope."
  },
  {
    "startTime": "00:04:03",
    "text": "And then, yeah, and then for this for the SVCD config draft, We submitted for publication. So both this 1 and the main protocol draft are being handled by Murray, Thanks for 80. Yeah. And then there's the 3rd draft. is an individual draft. the OHAI the feedback to proxy 1, and we talked about that at the previous meeting, folks generally seemed satisfied with the the content of the draft. There was some apprehension about whether there's enough interest 2, to justify taking that item on We have the chairs have an have a task to do a call for adoption. to solicit that interest. be looking for strong, like, interest and folks picking up. Does that plan still sound okay to folks? Or if If you don't agree with that plan of action, please speak up now. Alright. So we'll be doing that call for adoption very soon. Yeah. I think that's everything. Tummy. Tony. Yeah. pacing this course. wife on a with my own We did Alright. Hello, everybody. you've had a good idea of week. We're almost done. I'm Tommy Poly from Apple. and This is a discussion"
  },
  {
    "startTime": "00:06:03",
    "text": "on adding streaming capabilities to the oblivious HTTP, apologize for not having had a draft out before the deadlines but expect to have something written up soon The purpose here is to talk about the use cases, see how we feel about that and then go into the proposed details of what would be in this 0 draft. Alright. So first to review what our status quo is for OHP. we currently have a single message in each direction for request and response. that must be encrypted and decrypted. In a single chunk, each and that fit very, very well from many of the use cases that we have, and it's it's still a very, very good thing. I'm glad we are a publishing this document, soon. So things like DNS, etcetera, are small messages, and really there's essentially nothing you can meaningfully do with the data in the request or response without having the complete thing. You can't just be like, alright. I read the you know, first half of your DNS query. that's enough for me to start responding, like, not useful. Not useful. And OECD is communicating binary HTTP, as it's content by default. and very helpfully. Binary HP does support effectively streaming. It has this mode for indeterminate messages. So either you have a message where you know all of the like, the length of the fields, and how many fields you'll have and the body up front. or you don't know upfront and you do it instead of length value you have a delimiter at the end of these areas. binary HTTP is capable of doing more. than what we have an OHP today."
  },
  {
    "startTime": "00:08:04",
    "text": "And, actually, also, HPKE that OHK is based on also allows you to have multiple chunks. We don't exercise that So there's it's definitely possible to do StreamDOWHTP. And why would we want to do this? and I I I was a skeptic when Martin originally brought this up. And, you know, for use cases like DNS side, still am. But there are definitely use usage patterns that can benefit from streaming. we've talked about this on the list a bit. You can have very long messages or messages that take a long time. to generate, that can be processed in multiple parts. you have some database lookup or some other thing that's being interactively generated that you can start displaying something to the user or doing local processing on oh, while you're waiting for more content to be generated. And you can also have cases where you have a more interactive workflow, in which someone makes the initial request, and sends, you know, nobody or some of the body and then the server sends to the response. But then you can keep sending more body data in both directions and actually have a little interactive session over HTTP. you could do that if you had chunks and streaming. So assuming we actually want this, and we think that these are good use cases. What would we need to change to make streaming work. Here, I've identified 3 primary things. happy to hear if they're more. First, there is just kind of the cryptographic mechanisms about how we're doing chunk encapsulation, how we do the HPKE for the requests and how we do the AADs for the responses to make them chunkable, We have the requesting response format for how we"
  },
  {
    "startTime": "00:10:00",
    "text": "actually you know, send these chunks and delimit the chunks and everything else like that. And then as we talked about when this was originally proposed, it makes sense to have new media type for this since if we are changing our request response format, We need to know that we're doing that. Jonathan Horman, chunk enthusiasts. Apparently. Holland, Karlflare. Chunk from Just this not change the security properties at all? Like, I haven't had a chance to think about this at all, I'm surprised it's not on the list. as well that that Oh, that that that's why you're here. No. No. It's certainly I I think it does. I mean, the these I was trying to say, like, you know, what are the technical, like, on the wire changes that need to happen I think we need to make sure that we do correctly analyze the security properties, any implications it has. for things like HPK that do support chunks, like, Yeah. That that that that it's fine. It's the believe that analysis was done. doing it in multiple shots, does that change the obliviousness? For the the privacy aspects of it. Yeah. Yes. Do a security analysis standing on one foot. It's fine. Yes. I mean, certainly, like, it's not going as far as saying that you are stateful between multiple HTTP requests and responses. You know, as So I think there are there are 2 cases here. Right? There's a case where you just have, like, I'm just really slow at sending the response I would have done anyway. In this case, I think it's purely a performance thing. If we are going back and forth, then that does allow you to be stateful with in that one session, and I think it's then to some degree, a use case application analysis decision about how are we using this? And are we okay with that whatever state we are building and put in that one session. Like, if you had a extremely long lived session, you tunneled"
  },
  {
    "startTime": "00:12:03",
    "text": "all of your web browsing over this, then, like, yes. That's worse for privacy. Yeah. Pretty. Pretty Got it. Alright. There's more of a queue. I can't see who's next. So -- Okay. Ted. Saturday Friday enthusiast. So I I believe the way the charter is currently written, it focuses a good bit on the use cases for this protocol being those which do not require correlation. where the The requests and responses do not require any form of correlation And so you're when when we went out with the charter, it was like, hey. You're not gonna use this to Gmail Right? Because you're not gonna do that. And and and and specifically here, like, we're talking about correlation across different requests. Yes. Yeah. Totally. If you went to OHAI server 1 for request 1 and OHAIS SERVR 2 for request to, and they talked to 2 different back ends -- They back -- -- because of ECMP. It's all good. Yep. So when I first saw this and I thought, Yep. so I was streaming. I there was, like, a little moment of panic and, like, what? as speaking as mock chair, I was like, am I gonna have to have an o i from This is gonna be really bad. Don't do this to me. Please, no. So thank you for the the actual know that you calm down. It's Friday. You don't have to panic now. Yeah. Pieces of this. But I do think it's going to be very hard for you to describe in simple terms, and I encourage you in the 00 to do it. in very simple terms. which kinds of flows, flows will need this sort of chunked in determinant behavior but will retain the lack of correlation property. Yeah. That's a really good point. And so Please ditch the the phrase streaming at some point along the way. If I may suggest and -- Sure. -- pick something else that highlights"
  },
  {
    "startTime": "00:14:01",
    "text": "that particular pairing because I think that's the core of what you're proposing. and it's a little bit different from what leaks to mind. Thanks. Yeah. And and the here streaming is inherited from the PR that was abandoned previously. I would I would love to hear people's suggestions Maybe it's just chunked I don't know what it is. Yeah. Okay. Yeah. Send me your bad ideas. Dennis Jackson, and Cielo. just to echo the 2 previous comments and questions and and understand better. Yeah. The intent here is that the server's gonna start processing this before it reaches the end of the stream. like like or just that the client is able to send this a bit at a time. The One of the main use cases I have in my head is bet, actually. You know, probably your request is small and simple, and then you get a large response that may take a while to look up or to generate And rather than saying, I need to wait for this you know, giant multi multi megabyte response to all come in to be able to decrypted at all, I can start using it in chunks and, like, processing it while I'm waiting for this very slow server or slow network to keep giving me the rest of the data. at least on one side, you're gonna start processing the data in the stream. before the end of the stream. Okay. Thanks. Yeah. That's right. David Scanazi, privacy enthusiast in this case. So in our world where we're, you know, working to improve privacy, Ohio is of my favorite tools. Me too. But it's not the only one, and you can guess which one is my really, really favorite one. So So mask if it wasn't obvious. No. But so, Sierra, I regularly answer the question at work of, wait, why are we implementing both"
  },
  {
    "startTime": "00:16:01",
    "text": "mask and, oh, hi. Also do this. You're right. That's I kinda wrote a draft explaining that, so I don't need to repeat it anymore. But Anyway, there's they're useful for different things. And my answer when people ask is when you have multiple requests that are decorrelated you reach for Ohio. Yes. And then if you have a case like a web browsing context, where multiple requests are correlated. you reach for mask. And so my question is, this almost like, my my naive impression is why don't you use mask, and I hate to be that guy. But can you explain what why that's not the right solution here. Yeah. and having also answered this question a multitude of times, I I think there are a couple other dimensions to why you would choose Ohio for mask. And it's certainly not the generic one. But so first of all, I think the big one is Oh hi does require modification or coordination by the servers among the system. I think that's a really big one. that you need to intentionally be using this and working with the server that trying to preserve your privacy. Whereas mask is fantastic for talking completely unmodified back ends. So that's, like, the first divergence point. then yes. the other property here is with Ojai we're able to do, you know, it's it's per message Decorrelation and separation as opposed to, you know, per TLS session or whatever else the and connection is. not virtual. Yes. Yes. Yes. Alright. Chunk is different, but, like, oblivious HTTP message, which could include binary HP or something else. but it's when you want to be able to so that the privacy guarantees are stronger With oblivious HTTP, unless for a mask session, you had to do kind of like the end to end TLS"
  },
  {
    "startTime": "00:18:00",
    "text": "send one message and then tear it down, which is much less efficient. So if you want to have the complete decorrelation properties are able to coordinate with the end server and you care about performance you really should use. Ohio. And streaming here just means I may have a large message, but it's still always one request and one response. I I see. So just to summarize, make sure I get what you're saying. You can't you could solve this with mask. However, you would need for each work like, streaming requests to do a full TLS handshake and that would be cryptographically computationally more expensive. than doing 1 HPT. So this is a performance optimization. That's a good justification. Yes. Yes. Right. Like, order to get the same privacy guarantees, as this, you would have to do you you would lose way more bytes. You would have way more extra in solution, other things. Like, it just would not be worth it. policy matures and everything. Okay. That makes perfect sense to me. That Thank you. Great. Also, when you are working with you know, back end servers that are are are trying to build out this privacy system forwarding along a post request works really nicely through various reverse proxies, and it's a lighter weight than them all having to open up TCP sockets and worry about allocating an IP address to forward your traffic. It works well in a server environment. Okay. So Oakmark Mark Nodigan, Mark Nodigan, Where is Mark? marked you up. What? you go? You're virtual now. Is that what you're getting? There we go. There we go. There we go. Yep. I had Okay. I'm not coming into the last day of meetings enthusiast. I think, Tommy, what what you were just talking about should probably go into the currents. ohigh draft or some abstract of it. That was a pretty good explanation of some of the differences between maskins,"
  },
  {
    "startTime": "00:20:05",
    "text": "and and what we're doing here. But I got in the queue to ask what your intentions are regarding the unfunnel responses in HTTP. I I I missed that. What do you wanna do about informational responses, non final responses? o, Binary HTTP already handles those. Right. So, actually, that that that makes this work like, the streaming makes it work pretty well. Like, if you want to do that, you kind of need this. Right. because they were excluded last time, but this one It should be okay. Yeah. Yeah. That that's actually another good reason for this. Yeah. Yeah. Yeah. We'll we'll mention that. Thank you for bringing that up. Alright. So I think next, we're gonna talk about the actual Yes. Okay. So for chunk encapsulation, The things that we need to additionally protect and they said, oh, Dennis, did you wanna I've got this 5. 7, just stand Okay. Great. these are what I'm positing is the things that we additionally want to protect. need to make sure that if we are chunking things, that those chunks cannot be reordered without the receiver knowing about it. we also need to make sure that chunks cannot be dropped either in the middle or at the end. We need to know that we got to the final chunk. within a single chunk my understanding of that that cryptographic properties we have from HPKE and the AAD on the way back. is that we don't have to worry about a single chunk being truncated that should be detected. So these are think are the 2 primary new requirements Tell me why I'm wrong, No. I think I think you're right. sorry. Dennis Jackson. in the cellar. But I also think it's not enough by itself, So if the client or the server gonna start processing this stream. earlier on, They don't know necessarily know what's in the next gen."
  },
  {
    "startTime": "00:22:00",
    "text": "So without some additional guidance, about where applications cut those chunks you're back to the same kind of truncation attack. an example might be in a situation where So You can introduce extra content, like, into that stream. to move the chunk boundaries which turns out to be quite common in a lot of applications. and then, potentially, you get some kind of confusion of the application layer. because you start processing the chunk early. without having seen what's in the next chunk, which is gonna change what you would have done. had you seen them both at the same time? does not -- So are you are you talking about this at the essentially the application layer, like, once I've I I've decapsulated, and I have the binary HTTP. is that fundamentally different from the fact that when I'm reading from an h two stream, I I don't know what's gonna come up. in the future because I'm downloading an enormous enormous file, and it's gonna take several minutes. Yeah. No. You're absolutely right there. But in this circumstance, because you've got that Middle Relay, which is trusted to know identities, but not content. You've got an attacker that's much more capable of observing those chunks and delaying chunks. or potentially know, mess fiddling around with what's going on. So you've got a stronger privacy property here. than what TLS ordinarily Yes. It's striving to -- I see. I see. I see. Is so absolutely something we should note. Do you believe there's anything beyond privacy security considerations to be done there. I mean, I can't think of anything if it's gonna be general. you can't do anything better. But getting that careful roading in the draft, I think, is is really important. Yeah. That's a good point. And and it does what I'm getting out of this overall is that, you know, we need to have clear text on what is suitable, content for these and what is unseen. Yeah. And maybe a very clear example of how it Go wrong. because that is often more compelling. Absolutely not. I guess. Richard. Richard Brian speaking logically from the floor, but too late. Did you actually get that? Yeah. Yeah."
  },
  {
    "startTime": "00:24:04",
    "text": "It seems like maybe one additional requirement you actually have here don't see on the slide is guaranteeing that all of the chunks arrive at the same server or at the same ultimate destination. Right? because if there's something processing this, And, you know, there there are multiple instances of the same as Ted was saying, CMP, right, with with the existing uncorrelated requests, different ohigh messages. land on different servers, and there's no problem. and that's not the case for different chunks here. True. there may I think the requirements I which I did not state clearly state what these who who these requirements are for? Yeah. I was thinking of these for the receiver in order to verify the integrity Correct. -- of the data. Yeah. Nothing about the setting. And I think there may not be anything to do in the protocol to assure that property I mentioned. That's a great point. But it may be something mentioned in deployment considerations, operation considerations, what what of the failure modes here is your chunks getting misrouted, and you have a very sad Yeah. Good. But on your I I had been using on your last slide as to whether you you had mentioned the server beginning to process the request. while it was still inbound. I I'd been using on the question of whether The server was allowed to begin responding to the request. while the request was still arriving. And it sounds like as a property of HTTP, it is. Yeah. No. I I realized that's legal. But if you're concerned about these properties, then maybe you wanna have some sort of geographic interlock between the request and the response that would prevent that. I don't I don't my initial response is I don't believe we need it, but that's something we should discuss more. K. Who's after Richard? Jonathan. I'm next. Jonathan Hoidland. Jonathan Hoidland. There's also the trivial one I you shouldn't allow in sessions. Thank you. Yeah. Like like like, I think that's already it's already handled."
  },
  {
    "startTime": "00:26:00",
    "text": "But, like, that's one of the requirements. Yes. That's fantastic. Thank you so much. Yeah. Yeah. Cool living. But sorry. One highlight of the question while I'm listening to the this discussion and maybe I don't understand the use case well enough, and maybe I don't understand the non solution of a bowling while now. But it sounds like -- -- here. Yes. It sounds like there are applications where we can, yeah, provide small and bigger chunks on their own. Right? So I'm wondering is that actually the right layer to provide disfunction in TS is something that the upper there is can do anyway in a lot of cases already. mean, I think the the fundamental problem with the existing Alright. HTTP is that it allows exactly one chunk entire So it means you must have the complete request or the complete responsible before you can process any one byte of it. Yeah. But if I -- As long as that's not a problem, we should continue to use the existing one. But if I can just, like, put my request and and replace it are really small chunks. I can process them 1 by 1. on the application there if I can -- Well but no. But with the layer going inside OHCDP? you could chunk them up very, very small, but then you the other side would have to wait for everything to arrive before it could even decrypt a single bite. Fight. Fight. Fight. Fight. Fight. No. I'm I'm talking about shantling them up in different requesting. separate requests, but the separate requests are completely noncorrelatable. Yeah. In Right? So I need I need some logic on the higher Yes. I mean, that's, like, a lot of advice. I need to download a change file. I could have, you know, what is Patrick. You know, like, say, like, you know, give me bites 1 through 10, then bytes 10 through 20 is, like, separate requests, but that seems a bit mean, the what I'm saying at the beginning is, like, I'm not sure I understand the use case well enough I to understand if that is the right solution or if there's a different solution. k. Who's next? Anchor. So there we scroll. So we have, like, some handshake, and then that we, like,"
  },
  {
    "startTime": "00:28:02",
    "text": "start with, and then, like, we start streaming this data, and that seems like we're tremendously reinventing TLS. which will look concerning, frankly. So, like, the re the rationale HSTP was that you wanted to have things we're always, like, too expensive. like, tee up the TLS handshake. and and and we want and you wanna correlate across things. so, like, a little trying to figure out, like, why this isn't called mask. Right. So you missed the earlier discussion, which we have for several minutes on this topic. Okay. Well, I'll be happy to pick up the list. Mhmm. If you just want okay. Sir, of TLDI. Yeah. The TLDR, I mean, essentially is this is a performance improvement on that. Like, yes. You could do a full TLS handshake and then do exactly like, and set up an http session and then do exactly one request over tear the whole thing down. order to get the same privacy properties as this, through a forward proxy, But that's far more back and forth than just saying, I I'm just going to use HPK's existing ability to you know, chunk up the data into 2 chunks if it's being slow in driving it. Yeah. 2 years. Now five 50, a1000000, like, I guess what I'm observing is you're, like, merging into recreating a TLS. in pieces. I mean, fundamentally, like, what we're doing with HPKE and OHP to begin with is just choosing to send one chunk. don't see why going just expanding that to use several more chunks means we would want to then entirely switch over a different protocol here. to where we need to now set up"
  },
  {
    "startTime": "00:30:03",
    "text": "This is the way you The stream And this is the way that you gradually reproduce. Every future us by by by one feature at a time. Like, I I guess I guess I I understand what you're saying, and, like, that marginal argument tends to make sense in the margins, and I'm trying to ask you to look at the general equilibrium part of the Right. But, I mean, also, like, you know, in in the case of Now, let's say, I have a forward proxy So so you know, that would require that forward proxy to you know, setup you know, some TCP or UDP forwarding state to the next hop. And if I'm, you know, bouncing this around, like, that is a lot of infrastructure and IP allocation. Whereas here, it's, you know, just more of, like, a reverse proxy. Like, I send the post request with my message, it just forwards it along through whatever chain it needs to and sends it back. It's a different layer here. and enjoy repeat myself. Yes. That sounds like a sense of arguing with the margin. And, endgame is you reproduce the Ethereum protocol. So Well, I guess, are we doing is is the is the endgame of this Okay. talked about No. No. The end game is talk about what should go into a 0 draft. production? Like I said, I just individual draft even to have that discussion further. Oh, okay. Yeah. Yeah. We wanna see if there's interest and and see if folks are interested in working on it. And then -- And see if they're is polls. guess one question I had, Tommy, was Do you have, like, a list of use cases that that need this right now? Or I would describe the properties of the use cases. Right. But did you have, like, So is there any, I guess, right now are there use cases that you think would benefit from this because we did have some chat about the a chatter about that in the chat. Yes. Okay. yeah, I'm not I can't get into the details of them. Absolutely. Yes. Jonathan? So is one property of this? Okay. And I really should have read the draft already. Is one property of this that the entire Okay. Excellent. The nice nice and nice thing."
  },
  {
    "startTime": "00:32:00",
    "text": "that the entire request has arrived before the first bite of the response has been sent. that is an artificial limitation you could add. There is no reason to necessarily do that unless we say that we want a particular security privacy property that otherwise would not be a but there's no requirement for that from the protocol nor from, like, binary HTTP within that. So if I'm being psychopath. I could just do, like, a full, like, this is TLS, and just, like, continuously send a bunch of different requests, get a bunch of different responses, send a bunch of different requests. Sure. I mean, if you wanted to essentially make this like, this is not TLS, you could just add the artificial requirement that you must send all of the requests and then get all of the response. Like, that is something you could artificially put on this if you want. It it we thought that was valuable. Or It seems that that is a property that is in the original HTTP. Yes. So if we wanted to preserve that property -- And -- We could. I think that does have an impact on the security property, actually. get it. sure that's necessarily the one you want, but it is saying it makes a difference. Yes. You're right. You you're absolutely correct. And if I'm thinking about the use cases that we would have for this I believe, yeah, a lot of the I think they'd be fine with that separation. or even just a commitment to the full request in the first message. Right? Like -- Do you think part of the thing is, like, when you think about indeterminate, binary HTTP, like, it does not necessarily know the entire length of a response what what I'm what I'm worried about is an adaptive So like, based on something I get in the response, change what I'm sending in my request."
  },
  {
    "startTime": "00:34:03",
    "text": "And that just, like Right. So we could prohibit that. Exactly. And that that does like, prohibiting that would make this, you know, true truly only a performance optimization and not change any other property. that would make it easier to reason about for analysis purposes. I agree with you, though. Thank you. Okay. David's Ganazi. to me again. So thinking about this more, this so this is an optimization. It is. But but it isn't a free optimization. OHDP has weaker security properties than layered TLS or, you know, mask in the multi a bowl of nested mask proxies? What are the weaker properties? I just not have forward secrecy. if the HPK private key leaks, then information can be decrypted as opposed to if you do an end to end and the session keys get thrown out. you have perfect forward secrecy. Mhmm. So There are times when it is worthwhile to reduce your security for improved performance, and that's what the original HTTP is for. when you have small many, many small requests that are correlated, it makes perfect sense to do that instead of the heavyweight approach of nested TLS. put in this case, it's not guaranteed. So one of your use cases is a giant file download? Mhmm. I think the correct answer there is to spend the slightly bigger performance head, which will be negligible in the end of the day. to get the improved security. I totally understand that you might not be at liberty to talk about the all your use cases. But I think before I consider wanting to work on this in adoption, I would need to know more why this use case needs this. It's still unclear to me."
  },
  {
    "startTime": "00:36:02",
    "text": "Oh, Eric? Yeah. Just based on all the feedback of, oh, it seems like t s in these cases, you see us in that case stuff like that. I think any draft on this needs a pretty good guidance section on you should use TLS if these things are true and talk about it. if you need back and forth, if you need more than so many chunks in your spots, you need more more than so much time in your spots. And also means we can't either. figure out where these borders are for when it's useful to use OHAI versus TLS before we have this draft. So I think that's stuff we need to figure out. I think stuff we need to write down before we could really have reasonable draft that can go far. Eric is cool. I mean, just sort of, like, follow-up on I was saying earlier when Eric and David, we're just saying. I mean, like, One of the advantages of using a transfer alert thing when you were downloading a big thing is that the rate control works. here the rate control, you have 2 decoupled transfer sessions, and I don't understand the rate control works in any meaningful And if there's rate control from, like, there's rate control from the sender to the rate control from, like, from, like, December the proxy and the proxy there, and it's like Correct. It's, like, extremely goofy. I will I mean, for I mean, essentially, the the relay or any relays or other servers in between are responsible for propagating back pressure, which would be the case for any reverse proxy. as well. Well, I mean, like, the the the way that certain that's that's certainly is the way that that is, in fact, one reason why when we just run quick over a mask rather than rather than running tcp termination terminate each side. because we're controlled end to end. But I think in I mean, I think in the case here. because because remember, like, a large OHP relay is going to have essentially, like, normally just one giant, you know,"
  },
  {
    "startTime": "00:38:01",
    "text": "H2 session with the gateway or, you know, a handful of those. which overall for it is a better performance. thing. And it is just having to deal with the rate for the streams there. not sure I'm falling, but maybe that's a separate I mean, what? Why? Like, why is that better? mean, So in the case of like mask. Right? Yeah. We are like, what we want ultimately is this end to end. session that we were doing lots of stuff. You know, we're doing H3 or H2 to the end. servers where we want that and and flow control, back pressure, etcetera. In this case, mere. you clients will have some session to the relay. That relay will have essentially, you know, one logical large session to the gateway. and they just independently deal with their own No. I didn't. What I'm saying is a defect. saying that, like, that, like, the rate control I'm Like, that, like, that the that, like, you have totally decoupled rate control. between the data east direction, and it's just weird. But I think the sentiment is is the the scenario you're envisioning, like, The related gateway link gets stopped up for some reason, but the imbalance of the relay from the client doesn't know about that. It feels no back pressure. so that really ends up with basically unbounded buffering. But it should just have back pressure on its whatever it is, like, h2orh3streams are. on either side. Yeah. I'm not saying one can't build that. What I'm saying is that you took a situation where you had, like, automatic backpressure on the gender protocol, and you're placing it by by back pressure at the application layer. Which is, I mean, extremely Right. is it is it is it is that how do we already But have to deal with? for OHP and essentially every reverse proxy case."
  },
  {
    "startTime": "00:40:00",
    "text": "But it it's it's also a pretty well established thing here. when we are just doing an end to end HP request response. Yes. But there's there's self clocking because the because it makes it because they they're 1 to 1. They're what? 1 to 1. The the part before that. They're self clocking. self clocking yes. Anyway, like -- Okay. Yeah. We can -- It may require offline discussion. I'm I'm just saying, creating a series of new problems, which I I think we have to I I think that, like, demotivate this. This is I mean, I think it's one thing to say, like, I want to send I want to send two chunks, but when you're like, I wanna sound like a like a fact of the HR stream is like a different story. So anything. at least, you know, the way I've been thinking about this is the alternative that you're starting with is you just have a very like, you are using a OHDP. and you have a very large response that is being slow to generate. And the question is just, are you waiting for that or not. And in that case, you know, whatever backpressure there is, Those same number of bytes are gonna go through that relay. Yeah. The the the the the I mean, but not the the Yeah. I mean, your data sort of indicate and data indicated, like, the the the price the price you paid for this was for for for secrecy. There's a bigger price you pay. is it doesn't work with unmodified servers. And so -- Well, no. abs absolutely. And does the the predicates for using OHG in the first place is that you have a server system where you they are cooperating to do this, that they are trying to do this, so that you can get better decoupling on a per message basis than you would if you had to use mask. Or if you have to do a TLS per every No. No. I I I I understand. You said performance a privacy trade off. No. I I understand. What I'm saying is but the but the when that that the re the reason that trade off made sense was you're sending one message. you're trying to get a crap ton of messages, a sharpener makes any sense. Right? well, there's also cases -- Okay. There are also of going into the parameters of the use case here. kind But I think you need to. I mean, I know, like -- We're we're"
  },
  {
    "startTime": "00:42:03",
    "text": "upon making requests, you do not necessarily know the size of the response, and it may be essentially an extremely short thing that's very appropriate. for which people it may have some slower chunks. and having to commit to every single request being a full and, you know, and forth TLS handshake to then send the request in your HP session when it may end up being something that comes back in a single RPT is also expensive. I I mean, I I suppose that might be the case, but I guess, like, I'm I'm waiting to hear a motivating use case or whatever the case. And and, like, we have some internal thing I'm not gonna talk about. Doesn't really do it for So, like, I guess, this isn't the web. Like, these are this isn't the web. These are core free is the endpoint the other point is better controlled by both sides. Right? Yes. And so, like, if like so, like, you wanna have a fair amount of intelligence about what's going on about what's what characters are most likely to be. So, anyway, I I guess, I think I'll I'll I'll step back from it. Okay. Alrighty. say, Alright. So going further on the chunk encapsulation, the proposals, essentially, what Martin had written in his PR, like, a year ago this this slight modification. So to preserve the integrity of chunks, HPKE or the the sequence of it, HPKE or the supports this. It has its own sequence numbers. that needs to be added You essentially need to add a counter to the AAD message notes for responses. that is essentially just cribbing what HPKA did. And then for the final chunk integrity, propose purpose purpose purpose proposal here is to, you know, modify the AAD to say yes, this is some sort of Sentinel marking that this is the final chunk And then with those 2 things, I believe that it should achieve the requirements on the previous slide. The request response format is also what Martin had proposed. where you can have just a a length of warrant"
  },
  {
    "startTime": "00:44:00",
    "text": "that precedes the different chunks and then the final chunk is indicated by a length of 0. to say that this chunk extends to the end of the stream in that one also would need to have the final Sentinel in its AAD. affects, It seems to be a very simple obvious thing to do. One note I'll make is that there doesn't strictly you don't strictly need to use this particular request response format. with the in internal crypto that the link fields here are not covered and they don't necessarily need to be in order to achieve those properties. And so it does allow you to potentially have other media types or other formats where you ways to to move the chunks around. that don't change the properties of the actual cryptographic strain here. So if you have concerns with that, talk about it, but I think we also talk about more of the overall use case going forward. And if we do this, we have different media types some We could call it this. If we don't wanna call it streamed, we call it chunked you know, this is a a bike shed to be had. And I think Oh, yes. Also Mark hunting him had brought up several good points about just like the the ways you hold this thing apart from the actual wire protocol, such as, you know, do you have to negotiate or indicate support? is generally, OHDP is done kind of with some prairie configuration that is out of band, that could indicate whether or not you want to do chunkedness how you do this for discovered cases, I'm not sure. I also not aware of use cases for that, particularly. then there's a question of, you know, if can you have asymmetrical cases where the"
  },
  {
    "startTime": "00:46:00",
    "text": "server could reply with chunks and the client did not request with chunks stuff to be discussed. Anyway, so the next steps are for me to use this discussion to actually right up the 00. I've been playing around with implementation of this. I know Chris Wood was also working on this. Martin Thompson had built a version of this a long time ago. So there's if people are interested, we can Yeah. Yeah. play with Intrep on that. And then we should have more discussion once we have a 00. So you for the input Yes. Thanks, Tommy. So, yeah, I think we are looking for use cases that folks might have, which they feel would benefit. from this work, as opposed to, like, masked, mask or something? So yeah. So folks have use cases right now that it that they think would benefit from this peace pickup. So And, also, yeah, is there Yeah. At least we can go to that person first. Yeah. Hi. I wanted to put in a question for Tommy. So we We heard some discussion about the lack of forward secrecy. basically, there's the client is constantly encrypting to a fixed public key. for the gateway? Right. it's somewhat orthogonal to this specific proposal, but I wonder if we could enable some amount of forward secrecy with oblivious HTTP. by example, providing the gateway away to rapidly, update"
  },
  {
    "startTime": "00:48:00",
    "text": "provide clients with fresh public in a you know, chunked world specifically? I I think it's orthogonal except that it would go to one of the objections that's been raised just in this discussion. feels like then you go down the route of your reading It thing TLS. Maybe otherwise. Oh oh, you get Right. So it might be simpler than that. For example, we could simply note that the Gateway has the ability to create short lived, gateway key configurations. and very frequently rotate the target p, and then that touches on our consistency discussion. So how fast could you rotate your key config while having a working consistency system. Given that generically for OHDP, the mechanisms for key distribution are left as an exercise to the reader I think stuff like that would need to really be specific to some As yet unspecified, Yep. protocol for distributing those because there aren't assumptions you can make generically on OTP about how quickly they're rotating anyway. it's very possible for existing deployments to rotate those keys every 15 minutes if they want. Sure. That that case, it could just be, for example, a BCP. to -- Sure. -- to OHTP implementers, and that might also help people feel more comfortable is likely to be used in a in a way that's not that that's not as weak as some of I'm concerned. That that makes sense. Jonathan. John with William Cloud there. You could put the wow. This mic is really loud. You could put the you could put prekeys, pre shared Diffioman keys in DNS. Right? in just change them"
  },
  {
    "startTime": "00:50:01",
    "text": "every single DNS request. and just mix that into your key. Right? Just do if the diffie home and handshake. I'm yeah. I'm a little worried about the targeting there, but you you yes. This This is also good. beyond -- Yeah. -- a little bit beyond stuff for us. I think reinventing MLS is probably out of the scope of the 2 guy. Hey. Hey. Hey. I wasn't gonna use Domino's. I was gonna use Odd. we go over to the INMLOS. David Skinazi. Since we're designing the crypto at the microphone, let's do the name too and call this oblivious man. But no. More seriously, David Benjamin pointed out to me that another security property that this lacks is replay protection. So for the same reasons or or should he pee does. And the OTP specs is all of this. Mhmm. But It's not marketed very well. Not everyone in the room knows this. or knew this maybe initially. It's easy to forget. And this sounds like it's building something that has these weaker properties that people might act accidentally used. So there's there's a risk there. And another point that David Ben made is the security properties of OHTP and this. look a lot like ZURRTT. And, actually, if it's exactly the same construct as Google quick as your RTD. And -- Yep. Ben's proposal of then you inject another key is exactly how g quick switches from ZRTT to 1 RTT. -- reinventing TLS 13, which again, this is -- -- TLS. is not the point. Yeah. But so because of all these things, it really feels like this is building a weaker version of TLS, which is TLS"
  },
  {
    "startTime": "00:52:01",
    "text": "that only sends early data kind of, and that's not a good thing. So may maybe let's chat flying about these use cases because I think that is the bottom line here. They they will have to be incredibly compelling in order to propose something like this weaker secured it that people might use and accidentally in cases where they shouldn't be. Cool. like like a big file transfer, for example. one other conclusion I'll come from this is that I am very glad that when this was originally just a PR to be like, oh, yeah. We're just gonna have all of oh, it should be maybe streamed. I was like, maybe we should not do that. So this conversation bears that out that that was the right call. because this does warrant much more discussion and thought I hit I do think it is still useful for use cases, so, like, you know, we'll work on that. But it is not yeah, nearly as obvious, I think, as the simple OSHA case. Alright. So tell me you're welcome to send in a 0 0 draft you've gotten to a bunch of good feedback here, and so we'll look forward to that and handle questions on the list. Alright. That brings us to the end of our planned agenda for the day. We have Oh, a little while left in the session in case anyone has any other business. but otherwise, we'll conclude. Is there any other business for the good of the order? Alright. Then I declare us concluded. Thank you. Thanks, Ben, for the notes. So"
  }
]
