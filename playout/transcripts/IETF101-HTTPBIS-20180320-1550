[
  {
    "startTime": "00:00:04",
    "text": "yeah that\u0027s pretty cool good question have to ask her okay this is the note well hopefully you\u0027re familiar with this by now at this point in the week this is a new one it covers not only the intellectual property terms of participating here but also things like working group procedures anti-harassment code of conduct and copyright which is IPR as well in the privacy policy so if you\u0027re not familiar with this the best thing to do is to go and look up IETF note well on your favorite internet search engine our agenda for today we have one session this week we\u0027ve done the blue sheets correct they\u0027re flying around yep we have scribes thank you so much on right now we\u0027re doing the agenda bash and so we\u0027re going to go over our active drafts as quickly as we can and then we\u0027re going to talk about a related piece of work signed HTTP exchanges and then some work that\u0027s been proposed for start sooner which is h-2b Turk we talked about a few times the S\u0026L service parameter and the alt service via DNS and their aunts do we have any agenda bashing anything else it\u0027s a pretty packed agenda so okay selling for two and a half hours of HTTP phone let\u0027s get going yeah so with that I think BCP 56 bits one of our active traps that marks the author of is first um so I don\u0027t have any slides for this draft BCP 56 bits for those who don\u0027t remember is the revision of the use of HTTP is a substrate draft and that was put together in the early part of this century based on current practice then current practice has moved on quite a bit and so we\u0027re trying to write that down again I don\u0027t have much to say about this except that we seem to be making pretty good progress I think we\u0027re not quite done yet more examples more details and certainly more review are required but there aren\u0027t too many open issues on it right now so if you have it already or if you haven\u0027t for awhile please have a look at the document make sure that it makes sense to you and that you\u0027re comfortable with it and raise any issues or or have discussions is necessary does does anybody have any feedback based on the document as it sits right now I\u0027ve heard I have had some people review it and the feedback has been pretty positive which I\u0027m really happy about but we can always improve it I I will make the comment that it has been actively used in a couple other scenarios outside of this working group which is actually it\u0027s you know designated audience right so I think even though it\u0027s still you know at the internet draft stage it\u0027s been a useful North Star in the work in doe and I was able to reference it in a TLS which is "
  },
  {
    "startTime": "00:03:05",
    "text": "really you don\u0027t have a DAB on stage but you know it\u0027s been a very useful sort of thing to point to to say you know how does your design you know flow against this so hopefully we could expect some comments from those constituencies as well as on its usefulness as well as you know from this room on its correctness right and as with anything we want to publish you know we\u0027re interested in hearing their diversity of voices that say you know they\u0027ve read this this works for me so some discussion would be necessary before we get to the end but I have read it and I\u0027m pretty happy and and and we\u0027re trying to walk a line with this document where it is not dokoni and it\u0027s not saying thou shalt not or thou shalt must do this or whatever but more you know as that the status intends its best practice it\u0027s this is why you should probably think about doing this this way and so forth and so on so if you see a place where you think it is still too draconian or or it\u0027s not on the market place bring it up we\u0027ll talk about I would say the best experience with it in in Indo has been that it solves the temptation to redefine redundant lay various parts of HTTP because it can be a reference you know their upon for just nice so any comments the microphone about that rafter between move on sorry no okay great next up is bootstrapping WebSockets Oh you would even just a moment I know which way those Rockets are going are you sure there\u0027s lime oh there it is too long man what a fetching background it\u0027s easily appropriate there we go spring in London no next one all right here\u0027s the wrap for where we are last time we met we discussed an individual draft of mine and we essentially reached consensus shortly thereafter on the mailing list and in the meeting on refactoring that draft to include a similar level of header information in the HT request that was going to bootstrap web sockets as the old style it should be one version used I had played around with a couple schemes the tunnel that at a different layer but the working group kind of late to the old way so that\u0027s what we did and we adopted that document as oo in the IETF internet draft working group adopted document stream in December the only changes since then have been editorial in nature Thank You Julian making it better and we\u0027ve had a successful browser implementations I\u0027m hoping to get Ben\u0027s to come up and say a couple words about when we get to come in time in chrome and several independent server implementations that "
  },
  {
    "startTime": "00:06:06",
    "text": "that has has worked against with oo so if it\u0027s kind of the state of things I essentially have two issues I am aware of that I\u0027m happy to talk about in my 10 minutes today you know my personal goal here is that if we can get through those that I feel the document would be ready for working Google glass come on but of course one here any other comments that might be out there next slide all right so if you don\u0027t know what I\u0027m talking about this is what it looks like in order to use WebSocket protocol which is RFC 64 or 55 in HTTP 1 you more or less use the the upgrade header which changes your whole TCP connection to start speaking 64 55 we left upgrade out of HTTP 2 because things that operate on the whole can or a bad idea in a multiplex protocol and so the corollary I have chosen here is to use a stream in place of the connection that was previously used so the way this mechanism works you get an opt-in from the server that says I understand the funky new semantics you\u0027re then going to use in the next flight on the left and the next point on the Left uses the connect method to build a tunnel very similar to the way a bi-directional byte stream has traditionally works but with a few slight differences the primary one being that there\u0027s a protocol pseudo header on here that defines how you route and terminate that tunnel instead of routing to a new IP address as as Connect might normally do through a proxy the other request headers there are typical web Asaka 64 fifty-five stuff in the same stuff that was on the get an HTV one the next flight says okay that\u0027s great it includes its own set of web sockets headers doing sub protocol negotiation and then you just use the H to stream back and forth as if it were a TCP session that have been upgraded next okay so we have one github open issue open here which says consider using a LPU and registry values for the protocol pseudo header the draft uses the upgrade registry currently which corresponds to the same tokens HTTP one uses fundamentally this means that you send a protocol pseudo header with a WebSocket about you which just obviously not exist in the a OPN registry which tends to have things like h1 h2 a hole connection in my might my rationale there is that we don\u0027t want a hole connection based protocol value what we\u0027re looking for is really the the semantics we had with the upgrade registry so my recommendation is close this without action but there was no further discussion so I want to raise it oh yes I am pausing for further discussion yeah mom Thompson I had to do this to you so someone suggested to me that there\u0027s an inherent problem with this proposal in that if I want to make "
  },
  {
    "startTime": "00:09:07",
    "text": "a WebSocket connection to a server that does HTTP - I don\u0027t get to know that the server supports HTTP - and I can\u0027t sort of force the situation in any way to use this design you sort of have to choose HTTP 1 1 and that way you know that it\u0027s going to work you\u0027re gonna be able to negotiate WebSockets in that case or you have a reasonable chance of knowing that they can support that version of WebSockets so what do you show you - you bubble up or than you described in the case where there\u0027s no connection at all I mean that\u0027s where you write so you you get given a WSS your eye and you you faithfully go off and connect to that thing over TCP which which LP ins do you put in that to have the greatest chance of success now obviously this is the best way to proceed because then you get to use the connection for other things as well but well I mean yeah I\u0027m not sure we need to prescribe that that could be an implementation implementation choice about how you want to do that you can happy eyeballs that you do a few different things you know my approach would probably start by thinking about whether I wanted to use the connection for different things right but they always want the connection for other things don\u0027t you right well then I\u0027m probably going to happy eyeballs all right but we can hear other like gee does that tie closely to this question is that just a more of a mess loosely loosely in the sense that if the answer to that question was a OPN and I think your response I\u0027m quite happy with suggests that it doesn\u0027t then no it doesn\u0027t read on this but there was a potential here for this to say well I need an LPN and then and then it sort of now we\u0027re down the rabbit hole and creating ourselves other issues but if your response there is no and the answer is work it out for yourselves people sorry and we just document that side of things I think it\u0027s probably okay I don\u0027t know what other browser implementers another WebSocket is this is largely a browser problem what other browser implements think about that Eric Nygren Akamai on and I think it that is somewhat decoupled from this because in some ways this is that this issues about the portal pin for the protocol version but I think there\u0027s that question for for with that whether or not for the h2 with connect with connection protocol it\u0027s worth having a h2 with connection protocol LPN token that you can put before HTTP one-one and along with H wait um on the list list when you\u0027re making a WebSockets can actually so we have resisted bundling options with protocol versions at negotiation time and I don\u0027t think this is a big enough use case to motivate that but it may mean that if you don\u0027t "
  },
  {
    "startTime": "00:12:07",
    "text": "do that you end up getting stuck with HTTP 1-1 in a lot of cases because you can I share what I meant Bakey from Google first as Patrick asked me to comment very briefly on the state of implementation chrome has an implementation it\u0027s rough around the edges but it seemed to work there\u0027s two server implementations I know of NGH ep2 and live WebSockets the respective developers set up publicly available servers for testing and confirmed that they were able to make it work with chrome I have not tried myself I\u0027m sorry there\u0027s a third implementer who I\u0027m corresponding it privately it doesn\u0027t work yet with them second to contribute to the discussion that Patrick just prompted about this particular github issue I support metrics recommendation I don\u0027t think that connection level a LPN registry is right for protocol values third to respond to Martin\u0027s concern about there is no way for the client to know in advance whether the server does support WebSocket over HTTP - or not and this is a performance issue because if I open up an h2 connection even if I have previous knowledge about the server having support exams occur in the past I need to wait a roundtrip to receive the settings from the server before I can send a request so I\u0027m kind of inclined to just send the WebSocket request according to this specification at the server before getting the setting because if the server doesn\u0027t support WebSockets over h2 is gonna fail anyway and then I can retry on h1 we also have Craig oh sorry I\u0027m just I\u0027m just in the queue for my 10 minute slot after after his sorry okay come back get back in when it\u0027s done - thanks okay so mom Tom\u0027s to the point performance point I think that\u0027s have and I\u0027m an eminently reasonable thing to do I will also point out that in until s13 the server consented settings first so you won\u0027t have to worry about that particularly work much in in the future so it\u0027s it\u0027s a temporary problem and there\u0027s certainly not a particularly dial one so yeah all right thank you one more slide one more issue okay so the other piece of feedback I\u0027ve gotten it doesn\u0027t have a good hope issue open I can see this primarily from mark and I think I know we talked about this briefly in Singapore as it was the same in my individual draft is that the this mechanism is built around the connect method and the argument for using the command act method is that you know Connect is a special snowflake which you know it\u0027s consistent with the background on the slide so it must be the right choice and then everything surprising "
  },
  {
    "startTime": "00:15:09",
    "text": "about you know Connect is exactly what we need in this case so it satisfies the principle of least surprise that there\u0027s only a small change in behavior the behavior around Connect being that you know terminates to a to a protocol version instead of terminating to a another host as it traditionally does but the sense there\u0027s a special snowflake and in every code base which is quite a few left at that implement HTTP connect is really handled in a different code path because it needs to bi-directionally process the byte stream which is considerably different than is somewhat store-and-forward oriented you know message semantics that go on with them some other with with other methods including unknowns I would summarize you know the argument against is that you know creating a link level modification or hop two hop level modification you know using a different of a modification of a method is itself quite surprising and you could use an alternative method and it quite totally new namespace like you know get with upgrade or something like that and that could be a bit cleaner I have concerns around that if that method is referenced at all outside of this hop hop semantics its meaning is is totally undefined and isn\u0027t like rooted in any kind of Asia so I comments I\u0027m sure Julian was in line but I want to come in could could I respond to that first actually thank you if that was the intent there um so it I would state it a little differently in that I\u0027m just I want to make sure that we consciously do this which is have a a protocol specific protocol version specific extension in HTTP to setting override this mimics of a generic HTTP method and and I think your point about it being a special snowflake is is very well taken it\u0027s always been weird and it could be that maybe the outcome is is we decide Connect is the most appropriate thing for this but it\u0027s something that we document document in HTTP tour that that\u0027s not a good practice generally speaking for the other methods seems pretty reasonable yeah Julian I\u0027ve got a another thing that probably is not tracked on the github but I think we discussed that before what\u0027s the story about defining new pseudo had us I think the spec set we can\u0027t but this one does yeah so minimally it would have to update the HTTP to spec right No so I think first maybe Marc\u0027s comment that you know talking about the scope of these kinds of things inter is an interesting thing to do but 7540 is pretty clear that the settings mechanism we use to redefine aspects of 7540 with sun which is what\u0027s going on here so Madden Thompson I\u0027m I just put two "
  },
  {
    "startTime": "00:18:09",
    "text": "and two together and Vince\u0027s comment about sending one of these requests to a server that may or may not support this I just realized that if you\u0027re gonna put this new pseudo header in a request and send it to a server that you\u0027re you haven\u0027t received settings from yet I predict based on the general posture that we took in HTTP to regarding stuff that we didn\u0027t allow explosions right so can we avoid that well we can by reading it because to yell it\u0027s one three sentence first and that\u0027s a problem actually not just with the proposed here it\u0027s true of any of the proposals that seem to be able to do this we don\u0027t have the semantics we need for this so we need an extension of some sort a method a new method that has this behavior is also not really well isn\u0027t that\u0027s not cool either yeah is that is there some way we can ensure that the connect would fail but that the request would otherwise work so you you want you wonder you want a signal from the server that you want to make sure that the server does not accept this request you want one of our must violate could apply to the stream not to the connection because an unrecognized pseudo header will blow up not just that stream but those are the whole connection that\u0027s what I\u0027m consider I want I want the connections to restrain to explode yeah connection with the okay exactly yeah it\u0027s true this design doesn\u0027t give you that yeah and none of the designs we really looked at can give you that without either you know this is essential opt-in which we hope is solved mostly by server sense first and saying that the primary use case is being able to use established HTTP to connections right for which this is known information right right but that that\u0027s easy maybe the suggestion is to change the the use of the existing pseudo headers rather than define anyone or is there behavior you\u0027re looking for explosions speak I do not seek to explode the internet I was thinking that it\u0027s gonna fail because it\u0027s missing an authority header which is required with connect am i right but then it\u0027s also got this protocol one which is gonna probably break it so that may break the whole connection but it has enough I\u0027m sorry Alain friend on Facebook so it sounds like we have a new issue potentially or at least something to think through and make sure we\u0027re comfortable with it well I mean I think we did so we can revisit the issue so the resolution of that issue was this is negotiated and negotiations take a round trip and maybe that\u0027s settled because it doesn\u0027t require bilateral negotiation so if service ends first it it\u0027s a half a round trip right and then this is the scope of the use case this is we have this discussion same for we can revisit it but they don\u0027t have us really a "
  },
  {
    "startTime": "00:21:10",
    "text": "solution space isn\u0027t it often okay I see the queue growing and I\u0027d see the clock ticking so I just run here I wanted to mention that we Facebook also has a nascent implementation and there\u0027s significant interest of people the developers internally to use it so can in and Alan from Dell Facebook if the folks in queue could clarify whether they think that this is something we need that needs more discussion or they\u0027re comfortable the current design that\u0027d be very helpful Mike Bishop Akamai I just want to point out that currently what the HTTP 2 spec says and that governs until you get the server setting is that if you have a different pseudo header the request is malformed and you must treat that as a stream error of type critical error so a stream error not a connection error not a connection here all right we\u0027re allowed to upgrade you\u0027re always allowed to make it a connection error but that\u0027s an implementation question another hard fact good to go thank you Mike yeah mom Thompson yeah we kind of screwed up there but that\u0027s okay that\u0027s okay I\u0027m surprised to learn that yeah okay so any back to the original issue which is on the slide is is everyone comfortable with with modifying the semantics Patel connect with a setting I I don\u0027t see any great discomfort where is anyone so uncomfortable they have to insist on right and we can talk in to her about whether or not that\u0027s a good practice more generically without affecting this discussion okay thank you thank you and if we can also talk about pseudo header pseudo header extensibility as well is that your is that your book how I\u0027ve abused the HTTP protocol I\u0027m looking for a section inter okay that takes us to random access and live contents Greg can you come back please come back Greg Greg I see you published a new draft well to be fair dar check I got it published last night so yeah so I guess I don\u0027t really have any slides today just wanted to draw attention to the new draft who just had editorial changes and one clarification on the shift buffer buffer representations so and says it just went out last night I can\u0027t really expect people to have reviewed it and oh hi everybody I\u0027m apologize for not being there again hey someday maybe I\u0027ll make it so more just trying to draw attention to the new draft it\u0027s not significantly different from Oh to mostly editorial thanks to Julian and Martin and also thanks for the feedback on shift buffers and but I\u0027m willing to entertain any questions right now that anyone has I\u0027m "
  },
  {
    "startTime": "00:24:11",
    "text": "sorry if my face is like 10 feet tall because I don\u0027t have any slides there now I didn\u0027t think about that let\u0027s see look good you look good oh if there has been a little discussion so when Mark and I were you know discussing this in our regular meeting what we were looking forward to was getting a sense from the working group of whether you know they thought this effort was substantive enough and applicable and opportune up different people that it was worth going through the publication process for you know and and since then I mean right feel free to weigh in but there have been you know a number of comments I think have been been very positive to see so I think you know my primary question is what do you think you have left the floor last call yeah and thanks for rattling the cages and and get everyone\u0027s attention I think that\u0027s what helped us get some feedback nothing really I mean that the one area I kind of expected there to be a few clarifications on is exactly where we\u0027ve had them which is in that last section of our draft regarding the shift buffers there\u0027s one question out there that I\u0027ll answer on the mailing list I think that has a pretty clear answer the and so no there\u0027s nothing nothing I can think of I mean this is I\u0027d rather I\u0027d like to get this draft to bed I don\u0027t want to add anything to it I mean if somebody found a fatal flaw in that in the shift buffers I was already just that section is autonomous really from the rest of it I don\u0027t know if that\u0027s the right word it\u0027s discrete so we could you know not even discuss it the core the core portion is really about the range on live non non shifting resources and I think we\u0027re pretty solid there but I\u0027m always looking for feedback so right Partin I have a suggestion I think we should work in group last call this document right we\u0027ve had a lot of people look at it and those people are now largely happy and people can pay extra attention to the shift buffers section in that working group last call that\u0027s not unusual for us not only usual for us to do that but otherwise I don\u0027t think we need to hold on to this document any longer very good and and if you\u0027ll recall we already had one working class call it was a fairly long one you know I think the next one can be relatively short say two weeks um so Craig unless you have any concerns I think we\u0027ll probably start that after the meeting that\u0027s great I\u0027d like to look at other things I hope would you call that last call bits yeah Thank You Craig thank you don\u0027t quit your day job Patrick we can move on to something up to quote something from Jabbar q for Craig 416 and Hillier modes absent C "
  },
  {
    "startTime": "00:27:11",
    "text": "this is Tom Peterson see my last email on mailing list any reason why so this is the question about resetting and shift buffers is that the request you\u0027re referring to oh whoops relaying oh my god I have no idea I\u0027m quoting from Tom Peters right we don\u0027t have a vowel thick gentle so so much so go ahead the so the the answer really to that is that while a server may may internally utilize a real circular well you call a circular buffer to implement a shift buffer there\u0027s not there shouldn\u0027t really be a concept in the representation of a resetting it should always move forward and if it\u0027s ever used for any other content it should be a different representation with a different URI that would be my answer to that I mean so that you can\u0027t eternally have this model of a circular buffer in my mind using HTTP if you want to be casual it always needs to move forward that\u0027s my answer I think to the question Mountain Thomson that\u0027s a great answer I don\u0027t think we need to write anything down for that yeah I think if there\u0027s any fault maybe maybe finding a better name other than shift buffer keep people keep thinking circular buffer but I don\u0027t know how to get that out of people\u0027s heads I guess but anyway okay Thank You Craig you\u0027re welcome so we have the secondary certificates draft next on the hit list Mike Bishop gonna have some things they all might come join us up here let everyone enjoy your big bucks Mike pink box well I wasn\u0027t sure I needed to walk all this way to make a short statement but sure so we adopted it last time around we published it we have merged the integration with exported authenticators which is making progress in ALS group we still have one real open issue which is handling cases where the client wants to offer a cert because they expect the server to want it and negotiating things around that and the bigger meta issues it crops up in that is how we\u0027re using frames on on streams with requests that may have already ended which in HCP - yeah it\u0027s all a TCP connection you can send the frame the reference is a closed stream and nobody cares and it should be over quick that doesn\u0027t work so well because the stream is actually gone so we need to find a different way to deal "
  },
  {
    "startTime": "00:30:11",
    "text": "with that which is probably sending on the control stream I just haven\u0027t done that yet thank you hey don\u0027t go away Martin so mutton Thompson the the the other thing to point out is that there\u0027s a lot of activity going on and on this work in TLS and until that resolves a bunch of this stuff won\u0027t really be able to be buttoned down anyway so we\u0027re working in parallel but we\u0027re primarily focusing on the working TLS right and then and that was understood when we took it on so we\u0027re happy don\u0027t let it bubble along for a while and it\u0027s good you know that relationship is going well you know you put your peanut butter my certificate vice versa I actually think this is going to be a pretty important you know draft the future uses of HTTP and I saw I wanted to draw people\u0027s attention to that to make sure it got you know fairly wide review so if this is not on your radar it\u0027s mostly just a largely a reframing of the crypto work that\u0027s being done in geo us but nonetheless make sure you take a look at what it can do and if you have you know opinions on that you know where to find everyone the capabilities are interesting yeah the implications for a cheapy are interesting the actual you know the work that has to be done this document is not a big deal but the implications are significant yeah next up is expect CTO we have a Kinect yeah actually so I\u0027ve looked through the drive as I Ryan sleevee Google so I\u0027ve looked through the draft and and in particular it seems and I think have you framed it it\u0027s it\u0027s a rethinking of how we think about the TLS crypto I mean quite quite simply and from talking to various server operators it seems like this one of the things that is not yet addressed on the draft I might be curious to see here how the author\u0027s plan to address is the it addresses client risk meaning you know the client may want to look at DNS in addition to the certificate frame if the server sends a certificate indicating it has an additional thing but one of the things from talking to server operators is this potentially allows wide scale attack that\u0027s undetectably if you can obtain a compromised certificate through this method this is you obtain a a certificate for and I\u0027ll use Google comm as an example and you have a connection and you can advertise that I am authoritative for Google comm in today\u0027s model you have things like EGP detection that you can use to detect this but this would no longer apply and so I\u0027m trying to understand and I did not see discussion on list on these security concerns yet I\u0027m just curious if there\u0027s a way forward with that so um I\u0027m not one of the signers of this but I err Chris Farlowe but I spent a lot about it just like to make sure we got our Terms straight um so in the existing design I\u0027m the only gonna compromise didn\u0027t forget for google comm then um and I\u0027m able to fuck with DNS then I can then I "
  },
  {
    "startTime": "00:33:11",
    "text": "can then I can mount serious attacks sorry gonna come out serious attacks um we agree about that right right so or v GP or BGP yes so your concern going to take this out I can don\u0027t like a dork so your concern is that this relaxes the DES requirement it\u0027s just compromise certificate right not only relaxes the DNS requirement or the on path you know like yeah yeah yeah yeah yeah yeah basically says no matter what the network topology is if I have a copy my certificate is now game over right correct and the detectability of that the the mechanisms based on detecting network path changes our DNS compromise no longer apply to mitigate these concerns so I think you obviously accurately summed up the situation and there was a bunch of discussion about this and I think that the working group kind of felt okay with that um but that could be um I think that\u0027s certainly something people are aware of um but that may have been the wrong decision and so if you want to relitigate that I\u0027m not gonna so it\u0027s not captured in the draft range and to be clear that the relaxation of DNS was done in the origin specification and that\u0027s in the RFC editor queue right the the relaxation of DNS was done in the origin my highlight here though is that the security considerations list on the client may be concerned with this right and stuff the client may want to take provide a sufficient level of assurance such as optionally checking DNS as the ordinary mentioned um what I\u0027m speaking though is the server\u0027s ability to detect or mitigate compromised to its own services right so in the case of say Google com you could look at BGP routing tables advertisements and see is someone say Pakistan claiming a YouTube I P in a case a certificate frame you no longer have these mitigate able or detectable things even if it\u0027s after the fact detection the mechanism disappears and so the security considerations highlight except the client may want to take but not steps for a server operator concerned about compromised do you think you can suggest text I I honestly don\u0027t know and that\u0027s so what you just said there was actually pretty helpful and pretty clear because to that point I was really very confused with origin so that was a helpful clarification yeah yeah and my Mike if you want to step up here you can have permanent access to the microphone as an auto there is that I guess I\u0027m just like tired today so um bear with me um I the what the different how is this different from the organ frame in the sense that so so if I can acquire a certificate that is for Google and myself it seems to me that the origin frame is the same security properties that this does what am I missing I think it is very similar okay yeah yeah yeah detection now is you at least have there is some day extent of detect ability of the comm and that certificate whether and this depends right prior to TLS 1 3 the unencrypted and the certificate things like that but the whole threat model itself doesn\u0027t seem to be explored as to different levels of mitigations right so I guess are you asking are you "
  },
  {
    "startTime": "00:36:12",
    "text": "saying this is we maybe shouldn\u0027t do this or you just saying which is like write a bunch of text that people we\u0027re not follow so we do have mechanisms for detecting a certificate legitimization that\u0027s right I mean the certificate transparency work is that is the goal defense pants work is to detect certificate in assurance so to say that this removes the ability to detect that there\u0027s a problem it seems like it\u0027s I mean we\u0027re not giving up on CT right and I believe CT is one of the recommended things for clients to do so key compromise right CT would totally detect there\u0027s legitimate certificate with legit Mickey but the service key has been compromised the attacker can use that key compromise to mount these attacks without the detectability while still having all the visibility of CP and I think really that is the difference in that with origin you have to get a Miss issued cert for your own domain with an extra sayin and this this would go a step further and let you expand that attack to also if I got the surviving and there your protection is revoking the key when you discover the compromised but you have to find the compromise and I think that\u0027s that is independent of this draft but you\u0027re right it is a real operational security concern Eric yeah Eric nygren yeah actually I think when we think we\u0027re thinking about for origin and I still have people ended up from is it the the ID how we looked at it was a you was it you have ct2 for detecting this issue in tuned OCSP stapling as the way to respond I think that corner the case of compromise is one that not really well covered there another related one that this does make worse is in the case where you have a a side loaded certificate from an enterprise into this CA list this also makes it worse because that means that a enterprise for example could capture traffic much more easily without being on path which is kind of it\u0027s not compromised and I know that\u0027s a case where kind of like yeah if that we\u0027re doing this once you have that CA and they\u0027re all bets are off but it does just make that these each of these us they can\u0027t combine together make things a little bit worse Kyle Noke Rhodes I\u0027m also very concerned with this issue on both how it makes it a lot easier to exploit a compromised certificate in mass and it also makes it a lot easier to targets exploitation but I\u0027m wondering if we were to expand on a security considerations into suicide what options do we even have honestly in terms of guidance for how you figure out somebody has compromised your key I don\u0027t know I mean so we could amend some mechanism where when you went to a site they told you what list of CAS they could use and then and then like later you wouldn\u0027t accept other CAS quite a bit right if you had no eyes but we the other one "
  },
  {
    "startTime": "00:39:14",
    "text": "right on with a base issuance right I was just yeah yeah I know you\u0027re being facetious on musicians it\u0027s actually key compromised that in terms of thinking you know what our threat model is for what the future the PKI ecosystem is because of CT key compromises actually because you have these certificates online on active server systems it seems much more likely at the set of attacks we\u0027re gonna see over the next five years are going to be relate to key compromising we already see this to some extent there was a certain say see a reseller who happened to keep a bunch of keys handy and local and so those those are the sorts of things that the ecosystem is still dealing with that I think this has a chance to amplify the risk so so I think it\u0027s oh um right I think that\u0027s only true um I mean I suppose so one thing I mean so one the whole things we could do you could imagine doing right you guys one thing I would say is it sub certs do how to help this a little bit the sense that you can have it you can have the subscribers for a lifetime and then and then you and then you rotate you rotate their work they\u0027re working keys of course create other problems the another thing we could do that I have just just reinvented now so probably doesn\u0027t work but is um have some sort of weak sauce painting kind of version that basically we\u0027re in there stiff acute it says like you know do basically not pin me but do not ever let me be a certificate don\u0027t ever let me be applied this mechanism right and if you\u0027re like like HSTs P if you remember that then then then that would be a pretty safe and thing to omit like nhts but it would also give you a fair amount of protection against this kind of against the kind of attack now of course we need to like standardize that shit but um I mean like I guys can you form a queue because you think you know I think we think that flux is obviously unfortunate that like we\u0027ve created a situation where we have a new risk for everybody um you know a new risk for everybody um that they can\u0027t really mitigate without doing taking active steps but that\u0027s one possibility there might be some where I didn\u0027t vote that have it be opt-in but I haven\u0027t bought that three lexemes order yeah and that\u0027s I mean and what you\u0027re saying is very similar we\u0027re thinking about this in the context of what packaging and that\u0027s where these the similarities between web packaging certificate frame are very strong and the the mitigation of opt and versus opt out etc I I do want at the risk of pivoting to sort of a second conversation point on the draft one of the things that that did seem missing is a it just describes a single connection the processing model for a single connection when you receive these secondary certificates and how you handle what I could not find really addressed is when you have multiple connection say the typical web browser and you already have say a connection to will use again google.com as an example and then you receive the secondary certificate on a separate connection for Tom how you handle the prioritization selection things like that and so that would be the other piece that I would suggest for consideration in the draft is figuring out what the D conflict "
  },
  {
    "startTime": "00:42:15",
    "text": "sort of scenario looks like when you have these multiple connections it\u0027s mentioned in security considerations as a possible confusion but not a guidance for implementers as to how to prioritize and select available connections that you have when you do have multiple connections the queue is closed occur you may choose to be in the key with this okay so responding to that one I would be open to it guys if you\u0027re gonna do a side conversation can you do outside thanks yeah and if I\u0027m responding to your comment I would welcome some text on that but my initial inclination is right now HCP to talks about you may use a connection for certain domains and if you wind up with multiple connections that are acceptable for that domain it already gives no guidance on what to do and browsers already deal with that a common case for that is you don\u0027t know whether its support HCP 1 or 2 you open two connections in parallel because that\u0027s what you would do in HQ p11 LPN kicks in they\u0027re both HCP 2 and in practice what winds up happening is one connection closes never having seen single request so following up on the opt-out suggestion I think opt-in would actually work pretty well as far as the key compromise face I was also going to second the idea that I sorry in sweat for all the reasons only he just mentioned opt in would make me feel more warm physique is I certainly know of Surtsey Google where you know we wouldn\u0027t it wouldn\u0027t actually be the end of the world if they got compromised and then there are others that would be be in the world so you know I mean we we do have different risk profiles so the suggestion being to require some property on the server to be acceptable I guess so I mean I don\u0027t I haven\u0027t thought this through enough to have a mechanism I mean previously when we talked about this we basically decided like this seems scary but we didn\u0027t know what to do about it I mean that\u0027s basically what we ended up with when we talked about this last week\u0027s all right I literally have no idea what to do about it yeah okay okay I\u0027m sorry for that I apologize I don\u0027t have if I had a solution I would suggest it okay ad kg of the cuse closed thank you it sounds like we still have more to talk about and this isn\u0027t going to working group blasts school anytime soon so thanks but thank you for the substantive discussion indeed next up is expect CT Emily can\u0027t be with us but she sent a summary of where she\u0027s at which I\u0027ve put on the screen so they\u0027re having meta-major changes to the draft in a while only some minor tweaks to reporting which i think is mirroring some of the other stuff happening in reporting in other browser HDPE extensions the open design issue is where the header should have include some domain support and she\u0027s not inclined to add this at the moment unless another implementer comes along "
  },
  {
    "startTime": "00:45:15",
    "text": "and wants it we\u0027re unlikely to implement in chrome because it\u0027s a moderately complex implementation the marginal value is somewhat small so that\u0027s a status report I think it\u0027ll sit out there for a bit longer before we think of that working group last call any comments on this draft okay next up is header common structure which is me and Paul Henning complic both head income can\u0027t be here oh stand up mark get in the paper I\u0027m just getting the slides ready for you all right I know these Macintoshes could be confusing oh cool spacebar what\u0027s him hello next slide so this is structured headers so as a reminder to folks the goals of this effort are we want to make it easier and more reliable for people to specify and two pars HTTP headers there\u0027s a long history of people specifying HTTP headers and messing it up sometimes in major ways for example cookies and we have recently made a somewhat bad habit of putting a tremendous workload on Julian of verifying the the different a B and F of the people trying to put in their specs so that\u0027s not fair to Julian so we want something better a secondary goal is to accommodate feature in code for efficiency so once we have a data model for structured HTTP headers then we can express that data model in different ways which means maybe a future version of HTTP could have a more efficient coding in the water especially for things like binary content the non goals we don\u0027t want to recess off\u0027 i the syntax or or the parsing of existing HTTP header fields that is too fraught with danger and we don\u0027t want to affect or or or handle any headers that don\u0027t explicitly opt into using the specification when they have to want to use it next so after some discussion in Singapore we agreed to rebase the draft on a draft that I put together and talk to Paul Henning about so that\u0027s otwo and then in draft oh three we did a lot of refining of the algorithms in the spec which addressed various issues we split numbers into integers and floats and I really hope that into the bike shedding on numbers but we\u0027ll see and lots of other stuff like we would throw an error on child trailing garbage settles and so on and then we didn\u0027t know for more recently lots more editorial work changed labels to identify hers and made some other adjustments next slide so one thing I wanted to highlight the folks is that the current design that we\u0027ve settled on you have a number of possible top-level types for use when you define a structured header so you say I\u0027m gonna define the Foo header the Foo header is a structured header and it\u0027s top-level type is one of these types you have to specify that and more importantly when someone goes to parse it they have to tell the parser in some fashion that "
  },
  {
    "startTime": "00:48:15",
    "text": "which type it is at the top level and that might be with the method name or any other number of programming language specific constructs but you have to say I went to par section area now the bits below that are hinted on the wire and it will parse those automatically you have to validate that that in structure you get is the data structure you thought you were going to get but beyond that it takes care of everything for you so we\u0027ve got a dictionary we\u0027ve got a list the list doesn\u0027t have to be homogeneous we\u0027ve got a parameterised list which is probably the most complex data structure we have note that these don\u0027t wreckers none of these are top level types wreckers we have an item a simple item so you know identify or a float in a chair a string or a binary content and and that\u0027s all that\u0027s available and in discussions with bunches of folks for new headers we think that this has enough descriptive power for most use cases it may not be a hundred percent of new HTTP headers on the planet but it\u0027s enough to add some value and that\u0027s what we\u0027re shooting for we think next slide a couple of open issues that would be good to get input on number 433 length limits right now the spec specifies length limits on all of the different structures so it says a string must be no more than I forget I think it\u0027s added a thousand 24 characters you must have no more than so many items in a list so forth and so on and that\u0027s to make sure that there are same limits so that parsers don\u0027t overflow or whatever integers are 64 bit sign for example this helps assure Interop and assists optimizations down the road and it also means that specifications don\u0027t have to specify limits they can rely on the built in limits even though they might be very large so the question is is that a good approach in in previous discussion some people that said no these limits that make sense everyone who specifies a structured header needs to specify their own limits if they want limits and we don\u0027t want to prematurely constrain how many things people use for example items in a list the counter argument I think is above which is is that it helps ensure interrupted so forth and so on so that\u0027s question one and question two if we do decide that limits are a good thing are the ones we\u0027ve chosen the right ones what Mountain Thomson right I like the limits I think it\u0027s it keeps things sensible for those people are implementing this if I had to implement a generic parser and had to do greater than 64 bit integers I would be sad because I hate doing that every time I\u0027ve had to do that it\u0027s been painful that doesn\u0027t mean that someone can\u0027t define go off the path and do their own thing and that\u0027s that\u0027s why I think that it doesn\u0027t actually matter too much what the right what the limits are that you choose in here and they seem sensible did you have a limit on the number of items in a list or the number of yeses you\u0027re just inputting on slide I didn\u0027t put them on the slide yeah there are there onus back a little huge right there means well and and for some definition of huge the intent is to make them quite generous so "
  },
  {
    "startTime": "00:51:15",
    "text": "that normal uses of headers will never hit them they\u0027re just to give some implementations from certainty personally my observation is is that as much as we encourage people to try and do the right thing people sometimes when they specify heb headers don\u0027t bother to specify all the details of all the things that could overflow or be big or whatever and so having some sort of sanity is a good thing I\u0027m happy to make them bigger if the current limits make people uncomfortable so I\u0027m comfortable I think you had 256 parameters or something crazy about 512 so is that yeah those are big numbers if anyone ever decides to use all the space they have available to them things will explode in other ways so I\u0027m not concerned about the numbers that you have and I\u0027m also perfectly content with letting people just go ahead and do that so I think this is this is about right truly unless you come I I think it\u0027s a bit confusing to put a limit on the number size on the same page as like limits on the number of identifiers because they are very different things than implementation so I\u0027m I think I can be convinced that\u0027s having limits on the integer sizes are good but I\u0027m very skeptical about saying 256 identifiers are okay about 257 are not okay so maybe we should discuss that as two different kinds of limitations so for the purpose that\u0027s if it makes it easier let\u0027s just consider the numbers right out because I agree with you I think that there\u0027s broad consensus there needs to be some limit on numbers so maybe let\u0027s let\u0027s just carve those out for this discussion thank you that\u0027s all good I consider this from a extension point of view and for numbers and strings you always have the option to extend the spec if you want to include a large number so I\u0027m pretty fine with that so I think that you\u0027re naughty Julian\u0027s comment if we could provide a way to two may add more than 256 um I want to push back on that a little bit because the way that it\u0027s currently specified it\u0027s you literally go through the algorithm and if you have let\u0027s say the limit for sense for purpose of argument is 256 if you get more than 256 you throw a hard error and so then if a header specifies no don\u0027t pay attention to that that means that you have to modify you know the whole point here is to have generic implementations of structured headers now you have to modify generic implementations and if someone else is parsing the header and they\u0027re using somebody else\u0027s implementation they have to have the ability to modify that so it seems like it\u0027s making things worse not better like I said I\u0027m I think it would be great if people are uncomfortable with the size of the limits making them bigger if that\u0027s the issue that people "
  },
  {
    "startTime": "00:54:15",
    "text": "are having but if if we want to have no limits whatsoever then I get into this you know the the other concern that comes up is that well implementation a you know decided that 256 or too many an implementation be decided at 512 or too many and then you get interoperability problems in that sense just like we do for for integers you know so having some sort of sanity line is I think a good thing yeah well it\u0027s all I wanted to point out that we could always extend a spec if for example we need to send a 100 we always have that just like you know adding it prefix saying to you like ll something well it\u0027s all true we can always add you know we could have structured headers - and add a new type for example right and yourself okay so yeah Julien is at the end of the queue for this discussion mr. Roy fielding I I have very limited faith in in the the notion that that this will actually do any good in the sense that there\u0027s always been a standard way to doing HTTP header fields parsing and nobody actually used it because I\u0027d rather invent their own but aside from that if we do this and it\u0027s in 10 or extension header fields in the in the future I think it\u0027s really important that it be self descriptive in the sense that if you have a specific type you if partial needs to know what the type is before it begins parsing then that should be visible in the text of that are you talking about the gravy\u0027s issue now talk about well yeah I love that the whole thing so I mean I for this particular issue if there are limits on how to do a specific element of the parsing is that going to be enforced by the parser or is it going to be enforced by the recipient of the header field right now it\u0027s enforced by the parser and the idea is is that if I\u0027m saying the foo header I can say it\u0027s it\u0027s a list of no more than five items and then the you know a value comes off on the wire I gave it to my generic parser if the value length is over what the inspect limits are so one or 24 for example then it will raise an error and I don\u0027t ever see it I just see the error if it\u0027s less than the limit I get the full list and then I have to do my own check to say oh I was only I only want five members in my foo header this is more than five and then do my area yeah I so that that clarifies what it is you would expect for the invitation what I don\u0027t understand is why would you ever want the implementation to end at that point and say look the parts were made a decision for me and what what is the "
  },
  {
    "startTime": "00:57:16",
    "text": "correct response after that do you and the connection terminate the stream no no it\u0027s just it so when I define again if I\u0027m to find the food header if I\u0027m defining the food header then I have to tell you know I have to specify how to handle the errors in person the header if it\u0027s if it\u0027s a header that affects the entire stream maybe I do terminate the stream that it\u0027s probably well ignore the header or take some sort of default action because in our card in in the respect of in respect to the semantics of that header uh John Lennox some of this guide answered in those discussion since I got in mind but so I mean I think that I mean clearly specifying the defaults you know so that somebody who\u0027s not thinking very hard missus use the algorithm it\u0027s great I mean whether the ability for a specific header to say oh hey you know we\u0027re looking at this this is something or if this is routinely going to be you know a fork a fork a string because for whatever reason you\u0027re gonna put a fork a string and a header I mean maybe that\u0027s a bad idea that\u0027s a separate issue so so what we need to override this one parameter I mean as I see it from an implementation point of view just as the implementation needs to know the the top-level type for any given header header type you all may identify of parameter overrides so basically the values which the the the limits I\u0027ll take default values but you can override any of the default values on the when you call the function just at the same time you if you need to and you know I would assume that most extensions would not need to but if something happened you would that wouldn\u0027t be a huge headache I I guess I\u0027m just concerned that that was gonna be complex in that you know potentially these headers are gonna have to survive multiple hops and and and other transformations in the line has been cut yes sorry the Q s guys sorry was that was that a scribe interrupt or no the line has been cut I guess clever lines cute stop that\u0027s good I guess sorry Oh your screen just left I think it needs my fingerprint I just leave your finger over there I see what you did there so I I\u0027d be concerned yeah about the complexity of that and and remember the whole goal here is not to to cover every possible future HTTP header if a header has if you know we decide one case the limit for example and had a real ones do 4k we\u0027ll go and define a normal h-e-b header just don\u0027t use this framework or define an extension of this framework as a new top-level tie means I guess the "
  },
  {
    "startTime": "01:00:16",
    "text": "question is if somebody\u0027s rotor spec has said this is just like the generic header except the limit is 4k then obviously the implementer can either parameterize their implementation or write a custom or fork the code but either way it\u0027s sorry okay so basically if somebody wrote a spec that says this is just like the generic header except the limited spark a long right then I guess an implementer could either parameterize their implementation of the generic or copy and paste the code and change that one field field and treated as a as a specific header sure III certainly suspect they could do that what I imagine what would happen would be that at least while they were beginning to deploy it none of the existing implementations would support that they\u0027d have interrupt problems that people would have to write header parsers from scratch if it was a popular use case then of course you know support would evolve maybe that\u0027s okay sorry Julian bershka um maybe it would make sense to think about length limits in a different way than currently I mean what we are really interested in is the complete size of the header of you tried lots how long an individual part of it is I mean if I want to send a string that is longer than 1024 characters I might be tempted to define a list of strings just to get it into the head of you and that\u0027s not that\u0027s not the intent of that right so maybe just specifying the total size of the header fields gets us to a better result that\u0027s interesting but I want to think about it to make sure that the assumption is gonna hold but yeah maybe maybe that\u0027s way to go so that he was been caught I\u0027m sorry about that but there\u0027ll be plenty more Mike time and I think you\u0027ve got some things to think about and if we keep this going on the list as well as in the room I think that would be a suggestion question actually when I think about what Julian just brought up yeah I think I think that\u0027s a interesting point so you\u0027ll start a thread about that on the list yeah oops sorry so five oh five strings and identifies this is a little more esoteric we noticed that when you\u0027re generating one of these headers you also have to know the top-level type so that\u0027s interesting because you know when I hand let\u0027s say I\u0027ve got a data structure and I hand it to my implementation to generate a header from that data structure right now there\u0027s ambiguity between identifiers and strings as to you know if I just represent that in the data structure as a string well do i sterilize that as an identifier or a string for example I could annotate that with metadata in the data structure for example I could wrap it in an object or something depending on the language I\u0027m using but you know the question is is is this something we want to try and and an address we could say the current design is okay you just have to make sure that when you\u0027re jittering strange you\u0027re not stringing sorry you\u0027re generating "
  },
  {
    "startTime": "01:03:16",
    "text": "identifiers and strings you\u0027re not ambiguous or we could do something like remove identifiers from the item so that that\u0027s not ambiguous anymore there\u0027s another aspect of this that we noticed that I forget but I think is important has anybody been following this issue I think it\u0027s been mostly discussed on github ok this is probably too esoteric for the room then maybe we\u0027ll take it the list I think that\u0027s your last like yep so that\u0027s it except we\u0027ve had some pretty good participation on github with this more of eyeballs are always appreciated I think we\u0027re about ready for prototype implementations to really prove this out we\u0027d love to get some more experience with it and and after that we\u0027re probably gonna be ready for working group last call we wanna get this one out fairly quickly so that folks could start start taking advantage of it this is jeffrey askin chrome has at least the beginning of a prototype implementation for signature header in the dress that i\u0027m going to be talking about soon yeah but the minutes reflect mark shrieked Trillian I\u0027ve got one issue that I brought up sort of brought up on the list but didn\u0027t think through until this morning the way it currently is specified is essentially profiling the HTTP header field list syntax you know in a way that\u0027s not really compatible with the base back because the base spec tells you for instance that you if you have multiple ahead of you two instances and you have an empty head of get it doesn\u0027t count and in the list and it also talks about when you are allowed to recombine header fields and as you are not using the list syntax here it doesn\u0027t really apply so I think that can be addressed with pros in the current document and and maybe we can make it clearer on the other side in HTTP turn a pretty common last comment back microphone grant Klein I was going to make my comment with reference to the discussion about string lengths but it\u0027s probably more generic comment anyway something I find is common use in HTTP headers is conveying you are eyes and wonder if that maybe could be a cock should be a top-level type discuss so far we\u0027ve thought or URLs would fit into strings but if their use cases were something more specific is necessary yeah let\u0027s talk about it or or raise an issue in which case I would just suggest that the length of this maximum length of a string take that into account ah yes okay fair enough thanks Graham great thanks bark thanks for taking this on yeah I think we\u0027re probably a little bit away from working group last call we have some feedback to work through there "
  },
  {
    "startTime": "01:06:16",
    "text": "you want to make sure that it really works and yeah we\u0027ve gone through it before we turned out there so next up is cache digest for HTTP to Zoo who\u0027s gonna present before he presents I want to let the the working grab you can come on up because who come join us we\u0027re going to know that there\u0027s gonna be some authorship changes to this document kazuo reign is primary author and because of the way we have changed the digest format we\u0027re going to remove mark from the list and we\u0027re going to add a gyro which reflects the types of filters that are used in their primary contributions in that space so thank you to mark for your efforts and thank you yo F for you know joining this crazy ship we\u0027ve got so I\u0027d like to talk about and to open issues in cache digest and there\u0027s actually one more editorial pointed out by Julian thank you for that but it\u0027s not covered in this presentation so we have we now have draft zero three and it has the switch to use Coco swish caca wash and the two open issues these two so please yes the first one is about how we should negotiate their use of caches next please so the current approach is to let the client send a indication and then the server if it sees an indication waits for the client is in the cache that is before it decides what to push and on the other hand the client wait for a service signal to which indicates the service willingness to accept the cache Dallas and interest one point three the signal can be sent in the 0.5 RTD fright so it doesn\u0027t cause any delays and for the resuming sessions that could be easier to do assumption a client can remember the signal along with the musician ticket message and use that as a signal so that this is the current wall next please so the issue is there so we have basically two issues well the service indication is per connection whereas in case of a service supporting multiple origins we might want to make that decision power G so is that a good idea or in that case should we the origin frame in some form to negate that thing and the next issue is dead should we require the client to cash the signal that the server sent so that it can always tell me how or whether if it\u0027s just send a justice so these are the two questions okay would you like to make a comment I would have you don\u0027t so "
  },
  {
    "startTime": "01:09:19",
    "text": "I would say that\u0027s a fairly significant deviation from our origin frame means I\u0027d be more inclined them into a new a new frame type leprosy Alessandro godina CloudFlare regarding the first question I don\u0027t think it should be per origin um the indication basically indicates that the server supports cache digests and that\u0027s a per server thing for origin I think but thank you so make bishop but I will make the comment that the main case where you want to spell it out is when you have wild card subdomains because I remember correctly origin never did pick up a wild card did it that\u0027s correct and so if you you\u0027re going to have to enumerate to the client all the domains on which you think it could possibly make a request and you\u0027d like to see the digest even even if your cert is start up whatever and you don\u0027t send an origin frame so the search stands you still have to tell it unless we allow a wild card or unless you expect it to crawl every domain that it might have crashed resources for well if you\u0027re not caching it between connections it seems like the origin frame is the right match but if you are caching things beyond the scope of a connection then it doesn\u0027t make sense I think caching which cashing this information okay yeah I mean origin does have extensibility I did not envision that extensibility be essentially a set of settings with every origin well that that\u0027s an interesting way to put it because I mean we do have my quests origin policy thing right which keeps on limping along and I\u0027ve been trying to product but it seems to want to limp it\u0027s just yet another case where we say oh if we only had that we could use it and we don\u0027t have it but when we go and talk about it we don\u0027t have any use case so that\u0027s about your question of is this per origin or is this scoped to the connection right because origin frame is clearly just by its name scoped to the connection right you can have two connections to the same server with different sets of origin frames and they\u0027re very good uses cases for doing so yes and I\u0027d do you suspect that there are gonna be cases where you do want to pare down the origins that you send out just for from gondwana I haven\u0027t read this back admin so I\u0027m just listening along and taking notes but I guess the question here is what does the client need to know to decide whether to send something back or not if the service "
  },
  {
    "startTime": "01:12:20",
    "text": "supports handling a request that says here\u0027s the digest and handles it correctly even if it can\u0027t use it for a particular origin that its handling is that a problem yeah so if the server just says I support this and in the client sends it and the server can\u0027t support it for that particular resource that\u0027s not a problem yeah where I\u0027m getting to this is it scoped whether the server can handle it or not rather than whether a particular resource can handle it or not it is my impression which wouldn\u0027t means it makes sense for a connection rather than a specific origin because it\u0027s the server capability that you\u0027re asking for rather than the resource capability it\u0027s it seems like maybe a new frame type would be a clean way to do it I guess I guess get concerned when you\u0027d cache it between sessions because then there are a lot of questions about how you scope that and it\u0027s yet another kind of cash right i\u0027m service might be a precedent there we should explore on the list yeah okay just to add to the complexity of this caching frame once we have secondary certs I can envision servers that want to that are 34 and want to receive cash digests for multiple hosts and they need to indicate which hosts they\u0027re interested in getting cash digests for because the browser doesn\u0027t necessarily want to send the digest for everything that server is authoritative for yeah I think there\u0027s agreement on that point the question is whether origin frame is the right mechanism for doing that so thanks is another slide or yes so there\u0027s actually another question and fortunately it\u0027s about removing something so quickly we have four types of digest which is a combination of how we create chocolate chasm and whether it deals with a freshly cached response or still just response but unfortunately the only one which is known to work well is for high for the for the first row which is a using a URL already to just to calculate the hash time as well as indicating it to show the list of freshly cached objects other all the other patterns have issues related to how hep-2 is specified or how it is implemented or how a service implemented next please so but the fortunate thing is the the first up the first row one is "
  },
  {
    "startTime": "01:15:20",
    "text": "known to work well and actually Europe has gather some data from HTTP archive and it shows that the it shows that the most of the long term charged resources actually the most critical ones so we only need to send a shuttle that includes the URL key and for the fresh case so his proposal is maybe we should remove the other three patterns because it simplifies the the draft so do you have any comments alessandro Katina Cutler I agree that removing would be better there\u0027s also a complication where if you have an HTTP to start separate from your cache and you don\u0027t really know the e-tag then and the client actually uses the the validator or say that it\u0027s the stuff and then you can\u0027t really you don\u0027t really know what to do with it you were probably just going to ignore itself so agree on removing thank you thank you for that update looks like we got thank you thank you thank you okay moving on next up is client hints and we have again Ilya couldn\u0027t make it so we have an update from him so most the reason act activity has been around except CH lifetime and the privacy implications of client instant general and so he\u0027s saying we converged on clients delivery is restricted to secure contexts which is a w3c term it means basically HTTP and similar things opt in his origin scoped and by default extends to one party resources only first party will have to explicitly delegate permission for third party origins via topped in via feature policy so there\u0027s a tie-in now to a feature policy which is a w3c spec No WWII sorry it\u0027s WI CG thank you very much so these are breaking with previous implementation hints are no longer available over HTTP and third parties will not receive hints by default however clients used to describe the potential breakage is low and out waited by long term security and privacy benefits so this is the spec has been going on for a while I think it\u0027s still baking but that\u0027s where we\u0027re at with it right now any feedback on clients to mark I\u0027ve heard through the grapevine it might be interested in the variance work is that a potential dependency on this roadmap I don\u0027t think I haven\u0027t looked at in a "
  },
  {
    "startTime": "01:18:22",
    "text": "little while they used to describe how to use it with key or at least in reference key I think that\u0027s been taken out I think they\u0027re waiting for variants to be adopted or at least for its future to be clarified before they take any further steps I have talked to Elliot extensively about client hints and variants to make sure it meets his use cases though ok back microphone Nick Dodi UC Berkeley and I agree with these changes I think I think as it\u0027s noted that there\u0027s potential increased benefits over the previous drafts for privacy and security so so great work I am still a little bit confused about just sort of a status and direction sometimes in the group it seems like this is just for DPR and client width and then other times it\u0027s like hey we could put anything into a client in and we\u0027ll start sending geolocation lat/long and these headers and um just there\u0027s there\u0027s a big difference in what kind of review we need to do or like how we can consider all the implementations based on what kind of data is gonna go through there so so the current draft contains all the hints that we\u0027re talking about now people do raise issues in the repo to say Oh we\u0027d like this one we\u0027ve so far shied away from adding more ones without a lot of process around that and I think while client hints is of considered to be a framework in other words it\u0027s something that you can add new ones to later I think the expectation is is that you\u0027d have similar security and privacy review before you actually adding them okay great and the other question is um the lifetime idea is a little bit unusual like compared to like in some ways where analogizing to JavaScript web browser permission interaction and and in that case like sites don\u0027t indicate how long they want permissions for yeah maybe they should or like so what was the comment maybe they should yes ah maybe they should okay yeah or maybe there should be a chance to revoke it it just seemed like we just kind of threw it in there and like there\u0027s just a couple of sheds in there and I wasn\u0027t sure like what is this how we\u0027re gonna do permissions no it seems like that should be more coordinated that\u0027s an interesting point Martin yeah Simon Thompson that\u0027s a great point I like that I actually don\u0027t like the lifetime thing but considered myself to be in the minority and so would I\u0027d be okay with removing it if that\u0027s where people want to go but that\u0027s not what I got uh people to say these changes that you see on the slide and not the changes that may be slightly more happy with the privacy properties of the protocol please don\u0027t actually change things materially at all the the properties that made me more happy was the more robust text around the the sort of considerations you might apply when you decide to start sending one of these things and under what conditions you might send them and so there\u0027s a ton of "
  },
  {
    "startTime": "01:21:22",
    "text": "that in the draft now and I won\u0027t say that I\u0027m 100% happy but it\u0027s much much better than it used to be how did the reason that text was added was in response to comments about geolocation I would be firmly opposed to anyone putting geolocation in this one with no matter how much text you put in there because we simply don\u0027t understand how to control that sort of information and it\u0027s very different to the sort of things that the current draft concentrates on so just setting some expectations there if someone tries to put something in here and then really what we\u0027re talking about is defining a new HTTP header field that looks private information about users on in at this layer then we assess that on its own merits and this just gives us a little bit more text that allows us to deal with with those sorts of things and maybe explain to people the sort of things that you need to consider when you go into that it\u0027s kind of odd because we haven\u0027t had that in the past but this is this is a valuable exercise I think and well they carry the last come out on this draft oh great good so it seems like this condition three cuts in two directions right so on the one hand um you know the if you can store cookies and run JavaScript then you can extract this information and just shove in a cookie somewhere and so it\u0027s not clear not clear what this does on the other hand um for privacy on the other hand if you can shut this this information and with with uh stories or in JavaScript then why do we need that arc um so I think you know and so I guess that\u0027s I\u0027m wearing a little puzzled it seems to be the only difference is for the first is for the first hip right um am I missing something important I don\u0027t know I mean I\u0027m not trying to like yeah they just dropped that leave but how I proceed here cuz I just like I figure it out q is close if you can directly answer the question without extending the thread go ahead otherwise mailing lists are available payload scanner preload scanner you don\u0027t get a crack at it in JavaScript the preload scanner executes fetches about content in the document well before JavaScript executes so potentially the requests may be fetched in a way that the server will want to accommodate without the document having had a chance to modify the outbound requests in the first place and with okay it sounds like this is an offline discussion yeah okay thanks we\u0027re not gonna take this to working "
  },
  {
    "startTime": "01:24:22",
    "text": "group a last call terribly soon so there\u0027s still opportunity to discuss it speaking of dress they\u0027ve been with us for a while and aren\u0027t going to working group last call very soon we have an update on the cookies work so um I don\u0027t have an update from Mike on the cookie spec but I did have another photo from London which I liked so we\u0027ve been doing cookies for a while I know that he\u0027s incorporated some of at least some of the drafts so we\u0027re hoping to close this down relatively soon but we understand that you know there are other things that are happening as well so we don\u0027t really have an update per se but hopefully we\u0027ll be able to wind that up in the near future you know we have discussed this and we think you know it\u0027s moving if slowly and it\u0027s worth you know the working group continuing to pay attention to so hmm so next up we have HTTP tray this is not only no yeah we\u0027re going to talk related work first absolutely right signed exchanges so Jeffrey so this is explicitly in related work that we think the working group will find interesting but this is not something we think it\u0027s at a stage where we\u0027re going to ask for a adoption based on this discussion right so this is mostly a heads up and a an introductory discussion we\u0027ve given it 20 minutes so let\u0027s try not to go too much better at I believe ten in ten is the plan so I\u0027m Jeffrey Gaskin I\u0027ve been working on origin signed exchanges this was introduced at dispatch in Prague as web packaging but they suggest they and Marc suggested that I split it in half and that HTTP would be interested in this F so let\u0027s go to the next slide there\u0027s a collection of use cases the the one that\u0027s currently justifying a lot of investment from Google is that is something we\u0027re calling privacy-preserving prefetch which along with some other changes that other people are working on let\u0027s Google search treat amp and non amp content the same which should help with a lot of the the hatred toward amp another use case is avoiding the Slashdot effect so if you\u0027ve got a popular website that links to a small websites content you might be able to use an Origin signed xchange2 to avoid knocking over that website I should step back an Origin signed exchange is a representation of an HTTP exchange so a request response pair that is signed as an Origin and then a browser could trust that it actually came from that origin so that has some interesting security properties that that we\u0027re going to talk about later they can also be potentially used to evade censorship since the signed exchange could be delivered from a server that is not the one that your your ISP or government is blocking this this has the other effect that it can be used to evade legitimate filtering so we "
  },
  {
    "startTime": "01:27:24",
    "text": "have to trade that off there\u0027s a potential use case that to to push content from other CDNs I don\u0027t know how to actually do this in a private privacy-preserving way but if people can figure it out the CDNs are very excited and you can do it if you don\u0027t care about caching and then the the original goal for all of this work which doesn\u0027t work yet with just storage unsigned exchanges is to be able to share whole websites peer-to-peer while you don\u0027t have any connection to the internet so this helps with developing countries or people with very expensive data plans next slide the the basic structure here is you have an HTTP exchange we define a new signature header which signs a collection of things it it identifies a certificate chain and gives the hash of the certificate you expect to find there that\u0027s a URL rather than just embedded because you might have the same certificates for several different signed exchanges it gives a validity range which is currently limited to seven days to match the OCSP validity window it allow gives you a URL that you can use to update the signature so that you can get a new signature at the end of that validity range without redownload in the entire exchange it identifies a header that guard guards the payloads integrity for the initial implementation that\u0027s Martin Thompson\u0027s mi header of the Merkel integrity encoding but it could be other ones in the future and then you actually sign using the certificates public key a concatenation of a subset of those things that we think are important to to make sure our authentic it defines we define three ways to transfer these signatures you can use a normal HTTP header in a normal response for a same origin signed exchange you can use hg push to push a request response pair that might be cross origin and we\u0027re defining a file format that wraps the envelope and can be downloaded in a streaming way which is what the the initial implementation that that amp is going to use we\u0027ll we\u0027ll use next slide we believe that privacy is just about the same as what you currently get over HDS this is surprising because a signature provides authenticity but not confidentiality but the reasoning is that someone who is who\u0027s trying to get a victim\u0027s private information has to give them the link to the signed exchange and so they they know the link that someone\u0027s following and link tracking is a thing they can follow a redirect series or just use JavaScript to know that someone clicked on an exchange if we were to add a feature to "
  },
  {
    "startTime": "01:30:26",
    "text": "discover exchanges through some side channel that would break this property so we shouldn\u0027t do that there is one change compared to HTTPS the link source can guarantee the targets content instead of just being pretty sure it\u0027s what the link source retrieved from the from the target server it\u0027s a question whether this matters I don\u0027t think it does next slide a bunch of architectural risks have been raised so unlike the something in a client\u0027s HTTP cache if a user can load a signed exchange without ever connecting to the origin server now the assigned exchange can then if it can execute JavaScript it can ping the origin server and say hey I\u0027ve been fetched if it\u0027s just an image it can\u0027t this is very similar to old HTTP non gos caching proxies people pay CD ends to do this but there\u0027s a question of whether someone whether it should be possible to opt in by signing an exchange to someone you didn\u0027t hire to to cache your your content it\u0027s a totally new proxy model so we\u0027ve had normal proxies that are configured by the browser configured kind of globally they\u0027re reverse proxies that are configured by the target server this one is configured by the source of a link is this an important change and then there\u0027s there\u0027s a worry that this privileges large sites like Google that can run their own caches now CD ends can provide caches for smaller sites and can promise to preserve privacy in in fetches but but there\u0027s a question of who who this helps on the Internet and who will be able to take advantage of it next slide there\u0027s a bunch of security risks so all the risks with certificate frame also applied to signed exchanges it is possible to replay messages to clients so 0 RT T in TLS 1 3 and and session resumption let you replay requests this allows you to replay responses there\u0027s some defenses against the attacks that opens up in the draft but there are also other things like JavaScript setting cookies that we can\u0027t just block so a server has to has to be aware of that when they\u0027re when they\u0027re deciding to sign an exchange it allows downgrade attacks within the signatures of validity window so if a server finds an XSS exploit in their JavaScript upgrades the JavaScript pushes out any website an attacker can notice the change in the website take their cache scientist and send it to victims so this was this was already "
  },
  {
    "startTime": "01:33:28",
    "text": "possible if the user caches vulnerable JavaScript but now the attacker can force the victim onto the vulnerable JavaScript and finally if you expose a signing Oracle so you don\u0027t lose your key but you give people the ability to sign things with it then they could sign future packages there\u0027s there\u0027s currently no no trusted way of establishing the date that a package was signed next slide so we mitigate these in a couple ways to prevent replay attacks from being a problem we block authentication headers we advise servers to if they decide to sign an exchange they shouldn\u0027t shouldn\u0027t use any authentication information when they\u0027re producing the response we we think that signing things that our cache control public is safe although we don\u0027t have clients enforce that yet if people think it\u0027s important we could downgrade attacks are limited to seven days the the validity window is a trade-off people could have opinions that we we should make it bigger or smaller and there\u0027s a way to refetch signatures so a client that wants to check if it\u0027s being downgraded could fetch that from the server directly from the origin server signing Oracle\u0027s are defended against by requiring a new x.509 extension so that you can\u0027t use a TLS signing key as the signing key of a pact for a package so you have to you have to at least claim that you don\u0027t have a signing Oracle in order to sign a package we could could protect the date but we currently don\u0027t I\u0027d like to hold questions all a clarification question that it\u0027s possible so you said that the signature validity is capped they also said that you have no idea when it was no level way of what saying when it was signed those right so the date range the like begin date to end date is seven days but it is possible to sign one that\u0027s valid like that starts being valid a year from now and is valid until a year and seven days from now okay next slide so chrome is currently implementing this we\u0027re doing a subset of the draft and I\u0027ve kind of copied and subset of the draft into a new draft that that we can use for versioning I am interested in opinions from from this room about whether that\u0027s the right way to do kind of snapshots for implementation from matching implementations we\u0027re working with Google search Baidu amp and a couple publishers to try to prefetch both an and non-scientists changes from the search results page so right now search Google search prefetches amp content from the amp cache when you get it in a search result this allows us to preserve the URL and "
  },
  {
    "startTime": "01:36:28",
    "text": "prefetch also nan amp content and so we\u0027re trying to demo that around May we\u0027re hoping to find another hub kind of website or or just produce a out of a demo something something like reddit or hacker news that could also prefetch their link targets either to to preserve privacy but still prefetch or to avoid the the slash out effect and then if if all of this goes well we\u0027re hoping to run an origin trial late this year and in origin trial allows any website topped into two producing signed exchanges and hosting them I think this is the last slide to go to the next one and it should say another slide yes sorry next slide wrong wrong window yep okay that\u0027s all and you did a good job of hitting the clock so we have ten minutes for discussion so very fielding the I think this is a great idea but I absolutely terribly hate the notion of seven-day validity window I mean just with a passion our nigga are smaller unconstrained entirely I see no reason for ability wouldn\u0027t do at all if you\u0027re concerned about changes in JavaScript might indicate a bug just required not be used in that case you know we require that the identifier be changed for that particular type of resource it\u0027s the value of being able to sign public documents that are not JavaScript far exceeds you know for for the sake of privacy it far exceeds the concern of that they described thanks Lucas pottery BBC R\u0026D so I\u0027ve had a skim through this spec and while you\u0027ve mentioned carriage signatures but you both define the signature header so that\u0027s a clash albeit on a document that\u0027s not a thing absolutely we will I I don\u0027t expect the IETF to standardize both of those topics it will pick one so there\u0027s there\u0027s not a real clash if you go to slides later I have a comparison of cabbage signatures with this proposal sorry go again there I\u0027m do we have an understanding of how widespread coverage signatures might be in in the real world I do not if anyone can tell me I\u0027m happy to to include that somewhere my impression is it\u0027s used mostly by some api\u0027s so you know back end or none browser api\u0027s that\u0027s my impression okay thank you Dara briscola um so um I mean first thanks for attempting to ameliorate some of the "
  },
  {
    "startTime": "01:39:28",
    "text": "horrifying security properties as results are somewhat less horrifying my two of the legs is the underlying concerns of this proposal one of which I suppose this technical one of which I suppose is it is more philosophical um the technical one is I spend a bunch of time trying to reason about the security properties of this and I still can\u0027t and um you know that term that makes me very uncomfortable um you know certainly lots of terrible things can happen if people are not careful the way they have minutes their headers um um so you know having this having something which is a which is a statically signed object appearing in the origin is quite concerning um as I say we could probably work through those issues in the case of a sry caching or basically same origin signatures um the order of origin substitution feature strikes me as something which um is unnecessary for a large number of use cases I think the ITF should care about and is toxic for these cases which I think the idea probably shouldn\u0027t be endorsing and um so if we were to adopt this I think we should really focus on the cases that are not do involve Ward substitution and see how that goes and then we can discuss origins of solution at some later time for clarity this working group is not considering adoption of this document oh okay and thank you for your contribution it\u0027s very interesting all the miles have a global sign you mentioning an seven-day window for the validity linking that to the OCSP status of the certificate I\u0027m not sure did you have in mind to make that a static window because responses seven days is a maximum defined by I cap forum actually most CAS make that window shorter and while we even get often requests to make it even shorter and you also have a cache period so a request could be valid for a couple of days maybe only yes the the seven days is just kind of a first guess at what the right the right maximum length is we had a request to make it longer we have a request a possible request to make it shorter for the cache headers that interact with the signature length we we think that we will stop trusting something at the four of the two lengths depending on which one expired we might refresh it in different ways that we\u0027re still kind of working out okay Ben Schwartz so you mentioned potentially having more aggressive verification or more aggressive checking I was wondering if "
  },
  {
    "startTime": "01:42:28",
    "text": "you could talk just a little bit about what possibilities you see there off the top of my head it seems like you could imagine something like static content essentially is valid immediately but either maybe JavaScript can\u0027t run until after the signature is verified online or JavaScript can run but the page doesn\u0027t have an origin until the signature is verified so it doesn\u0027t have access to the origin if you maybe there\u0027s some some variation in there that that would yeah those are those are possible we haven\u0027t figured out exactly when browsers should fetch the validity URL to double-check the the idea of giving it no origin or putting it in a unique origin until it\u0027s verified is difficult if it calls API is that access storage because then you kind of have to swap out its storage underneath it which can lose data it or possibly hang until until nikto UC Berkeley I like how careful you\u0027re being and describing the security properties and the list of headers that might indicate that this is stateful I\u0027m curious they\u0027re like what should a server do like assuming server screw-up which which I feel like is a safe assumption what should the server do after they screw up after they sign something that was stateful but but somehow got through those checks how would they on screw-up right so I haven\u0027t thought through that the whole way clearly they should stop signing new ones and the old ones will eventually expire if they think their users have been forced into an attackers for instance they can use the clear sight data header to clear everything they can probably also set different cookies like if they if they notice a particular hack attacker is taking advantage of it they could notice those cookies and reset them there may be other mitigations wait can they use the clear sight data like I thought the whole point was that they were like it\u0027s gonna be delivered cached like they can\u0027t update the JavaScript right so once once the signatures so that the people are broken for that seven days or for whatever signature validity they set after that that the clients have to go back to the server or get a new signed exchange okay thanks I also want to mention that we had a side meeting yesterday at which we talked about web packaging in general and the agreement there was that we\u0027re going to hold a workshop to try to get more publishers to weigh in on the ecosystem effects that I know Martin and and Eric are are worried about we it will be organized "
  },
  {
    "startTime": "01:45:29",
    "text": "via the tag and the IB I don\u0027t know exactly when it will be that was a notion there I\u0027m not I\u0027m not sure to that solid yet okay that\u0027s that was one of the things discussed yeah and and and so that\u0027s it for that\u0027s it for this so thank you for that that that\u0027s very good like Patrick said we\u0027re not adopt considering adopting this now that doesn\u0027t mean that that will not ever change so I\u0027d encourage you to continue talking to folks both here and on the list and if folks have concerns or they support this or whatever it\u0027d be great to hear about that so thank you very much Thanks however our next set of items are things that we think are right before the working group to consider adoption on the first one is may she beter we talked about this in Singapore and before this presentation starts I just want to sort of set the stage a little bit you know we I\u0027ve been instructed that I\u0027m supposed to pronounce that Terr there\u0027s a lot of disagreement here so I\u0027m gonna move on from that bike shed take we take that one to the list that ought to be a great place to work through pronunciations in any event you know the discussion we had in Singapore and a little bit on the list scopes this as a statement of essentially the HTTP semantic layer as being the most important revision of the 7230 series of documents that we all know and love and that 7540 refers to the semantic bits of even though no one\u0027s is actually sure where those are defined and so that\u0027s one possible very important outcome of this work there are others if we do choose to adopt this work I\u0027ve asked Julian Marc and Roy all who worked on the first set of while they were in the first set but the the last Restatement HTTP the best two to be editors on all the documents that may be output from this effort and the question of how many documents I\u0027m sure will come up but so that\u0027s my instruction and Julian you can drive this service marketing drivers are we gonna drivers come join us on the study the last one so somebody else does this one so we have 20 minutes be great if you left some room for you know some discussion so as as luck would have it the Apache members meetings started exactly the same time as this so I just okay so we\u0027ve got this this notion that we\u0027re going to revise HB 1.1 one more time and finished absolutely be finished so the next time Julian you want to describe the history yeah so that was just a reminder how we got here "
  },
  {
    "startTime": "01:48:33",
    "text": "between 26 16 and the revision we had a gap of eight years we finished the revision in 2014 so it\u0027s already four years but if we start now so it\u0027s not like it was yesterday that we finished next slide so it\u0027s people sometimes ask why do you want to update so we can\u0027t change the RFC\u0027s that are published we collect errata but people do not look at the errata when to update depending on how much we want to do it will be something like five years since the last revision which isn\u0027t that quick so I think it\u0027s it\u0027s the right time to start work on that and also we might actually hit the point of time where the RFC editor starts publishing documents and HTML formats so people can actually look at a readable spec x slide so we took the original subversion repository that contained the work on HTTP base and did a few minimal edits like rename renaming the documents updating the obsolete relations change updating the acknowledgments and so on and have published a first set of drafts as non working group drafts for people to actually look at how that would look like with these minimal edits and the last link in each line has the diffs from the published RFC and those divs essentially are totally editorial was the additional bits that as we changed xml to RFC versions there\u0027s a difference and hyphenation of terms so don\u0027t worry about that that\u0027s a artifact of the XML to RFC version yeah the only real changes were are where places where the the existing RFC\u0027s refer back to the prior RFC\u0027s 261 six we\u0027re changing those we that to refer to the RFC 7230 31 yet cetera and we took out the history sections that are referring back to previous RFC\u0027s as well so other than that it\u0027s all exactly the same the only mechanical differences really er that RFC to H XML to RFC some of the some of which are bugs which have already been fixed in XML to RFC there\u0027s one more thing we took out the term HTTP 1.1 out of almost all document titles realizing that only the first spec actually defines the 1.1 format while the other "
  },
  {
    "startTime": "01:51:35",
    "text": "specs are supposed to apply to all of HTTP next slide okay scope discussion so the obvious things to do is applying errata updating references resolving the or trying to resolve the issues that have been collected on github and the number of these issues went up from 30 to 50 between IDF 99 - until today which is a good sign and minimally we need to say that there\u0027s also HTTP - and where to find it so that\u0027s something that absolutely needs to be done so that\u0027s why I call it the obvious scope next slide less obvious is do we reorganize our specs once again like trying to come up with one document that is specific to the wire formats and having the right hooks for HTTP 2 and the other discussion is was the number of six specs the right number or should we try to recombine stuff again less obvious things are also satellite specs that were published later but where we\u0027ve had that in theory that should have been part of the specs already like the missing status code for permanent redirects without methods name rewriting - authentication I had us that we lost and we added and 70s 615 some extensions to content encoding negotiation I had a spec telling describing how a server would tell a client which continent Holdings it it would accepts and we had recently mother requests for service to be able to specify which media types in a request they would accept so that could also be go there I also mentioned random access because at some point we thought that this was just a clarification or slight extension of the range model I\u0027m not sure whether where we stand with respect to that right now and mechanically it it should be ago to actually move HTTP to for ITF standard but depending on how much other things we want to change that might not be possible so yeah I mean certainly my my intention goal would be to not make any technical changes that would cause it not to be the standard that gotough will "
  },
  {
    "startTime": "01:54:38",
    "text": "start just because there\u0027s there\u0027s really no reason after this to to revise it again and if we don\u0027t have full standard for HTTP it\u0027s kind of stupid because there are many times in which people have asked us that to move it full standard even with known problems whereas we can actually fix all the known problems and just move it at the point Martin yeah Madan Thomson you just hit one of my hot buttons sorry and I stood up anyway even realizing that so I actually think that this some of these at least sorry candidates for inclusion in the in the main in these documents that doesn\u0027t necessarily conflict with your goal of going to full standard I personally don\u0027t think the value of full standard merits special attention from us so if we decide that something in here is this needs to go in there it should just go in there and we proceed otherwise it is our responsibility to maintain documents not necessarily strive for perfection because we know how how well that works in practice I\u0027ve written a few things on this topic if people wouldn\u0027t like to read about that yeah I\u0027d agree with that a with a separate you know at slight extension that in we do have a responsibility to communities outside of just the ITF in that they rely on us to provide something that we consider to be a standard so that their we consider to be a standard can reference it at the same level I mean the ITF has that notion internally there\u0027s also other organizations like ISO that have that notion as well so it there is some desire outside of ourselves just to call it the standard even if it were revised five years from now yeah next slide so I put together a few points that we actually should decide upon try to decide upon today like do we want to do this do we start with what Roy and I submitted a few weeks ago think about how this effects the HTTP quick document all these can evolve together whether there\u0027s actually some dependency on in one direction and what that means timing was decide on what we are allowed to do to change document organization wise yeah and technical bits so thank you for that I would actually like to clarify mostly what you\u0027ve put up on these slides which is great so if people have comments specifically about time line relation to the quick work and "
  },
  {
    "startTime": "01:57:38",
    "text": "the quick working group and whether or not we think that would impact this document which is not obvious that it would and also can you go to the I do about Mike\u0027s comments there but can give the previous slide you know also the bullet here about split information specifically exclusively HDTV one into a separate document and re label everything else just issue HTTP I call this the mysterious semantic layer of you know of calling it out and I won\u0027t make a chair comment that I think our ecosystem badly needs that contribution so I would want that to be in scope personally I have individual contributor comments and some of the other things but I\u0027m having to open them the Mike\u0027s and these particular questions and from my perspective whatever head I\u0027m wearing I think that\u0027s the most important thing to do to be honest so Mike Bishop couple different hats in hand as the author of the semantic layer draft yes please I would love to see this in the spec it would make things a lot cleaner as the editor for it should be over quick yes please it would make referencing hcp separate from one one and separate from HTTP to so much easier and then there\u0027s the timeline that if we actually want the quick Doc\u0027s to be done by the end of this year this isn\u0027t going to be done by the end of this year and so we we\u0027re gonna have to sort out I assume we don\u0027t want to hold HTTP over quick for this but then that would mean you\u0027re going to immediately replace what I\u0027m referencing with something better right so that\u0027s not a decision of this working group the issue you ever quick but I would agree so justjust I\u0027ll respond to that I think first of all you know with my quick head on the intention is to have the technical work done at the end of the year there may still be some baking after that having said that I intend to put a fair amount of energy into this and I\u0027d really like to get it done quickly I do not want to spend five years doing this I\u0027d like to spend less than a year doing it I don\u0027t know how you guys feel right I agree with mark I mean the the it sounds like an open-ended problem but it really isn\u0027t in terms of in terms of specification work particularly once we decide how many drafts we have once we decide how many drafts we are working on then it\u0027s just a matter of squeezing them into the the right spaces what we have right now we already have the text done right new text from so it\u0027s it\u0027s squeezing them into the right space which I could do this week mm-hmm and then the going through this set of issues that have been refined already for the previous one which is again it\u0027s not that\u0027s not going to take the ought the editor as much time it may take the working group more time to figure out if we have working group consensus but I "
  },
  {
    "startTime": "02:00:39",
    "text": "would be utterly shocked if we did not finish before quick yeah even reach technically greater this took a long time because 26:16 was in a pretty bad state and we had to work through a lot of issues to clarify things and figure out where we laid on you know the interrupts was really bad that\u0027s not the case now we\u0027re working with a much more mature document set okay if we do say so ourselves yeah I was I was going off the bullet on will the previous slides that said this wouldn\u0027t be done before 2019 at the earliest sure that meeting said schedule risk is something you know everyone\u0027s going to assess for themselves and part of the schedule risk is gonna be the engagement of the working group as you you know mentioned whether you know they\u0027re gonna be all help progress these documents and get the reviews done so before we get to the adoption questions I\u0027m just interested maybe in a show of hands of people who feel that they would be willing to be you know involved in in reading these documents and helping push the process forward so this may be 15 for the minutes okay thank you yeah wave time yeah not not more so I just go back to this the we were actually talking about splitting up the the document and we\u0027ve we\u0027ve talked about this various times I mean the the history behind it we split at the beginning of the the previous revision process I split up the document to six one six and to seven parts mostly so that we did assign different editors to different parts and do things in parallel but it turned out that we didn\u0027t actually go that way we did all the documents in sequence together and there wasn\u0027t really that need to split them out it was useful however to isolate the issues in this case my personal feeling right now is that we could move all the semantics documents into one document move the HCP Architecture from what\u0027s currently part one of the messages into the semantics document and so have one HCP semantics document one HTTP cache document and one HTTP messaging document and that would be it so we\u0027d end up with three documents the other alternative would be to move everything into one document and just just for the sake of referencing that one document sorry Roy when you say messaging did you mean HTTP 1.1 right well no point was a one point one semantics and caching so there would be one document that was specific only to HP 1.1 and the other to cache and and semantics would be the the not one point one specific where where would validate end up validation conditional requests conditional request would be in the the semantics don\u0027t cache cache would be in "
  },
  {
    "startTime": "02:03:40",
    "text": "you know how the caches would use those mandates would be in the cache document so that would still be a reference yes okay yeah there\u0027s definitely be a reference from cache like it is nation right now so does anyone have any horror or alternative ideas or preference for one way or the other Brett Jordan please fewer documents so that your documents means more desire for fewer documents so but not an absolute requirement of one if we\u0027re to try and go do a full standard for this then yes one but fewer documents is better than trying to find everything because it is really a nightmare right now so / so I would just have a certain tension there between when you have a smaller granularity right it allows us to revise documents I know we don\u0027t want to do this again but you know it\u0027s not implausible that we would rather serve eyes them independently and so that\u0027s from a process point of view I think that\u0027s really the strong strongest argument the resonates with me for multiple documents you know as an engineer who\u0027s worked with these things for a number of years I\u0027m on board with the less please I would actually make an argument for two semantics and h1 you know no separate cached argument I don\u0027t think it stands out as especially different then I think the one thing about it is it\u0027s it\u0027s long ish and it is targeted at a separate implementer right that that\u0027s the only really right so that\u0027s one with me to to is is great one doesn\u0027t make sense to me - makes sense to me three makes sense to me so I does anyone else have a comment on that I\u0027m not sure it is targeted at a completely separate implementer a lot of the time all right okay if I were chairing this section I\u0027d say that ultimately it\u0027s an editorial decision that should be made by the editors in consultation with the working group so I don\u0027t know if that we need to get consensus on it but input as input is appreciated and I did actually were going to talk about you know having a hump to adopt it I wanted to get a sense of the scope of where we were going but I do agree that you know the details you know are what the consensus of working groups are made of and will make that happen you know and after adoption I think that\u0027s fine so are there any other comments that would like to be made before we I\u0027m hum we\u0027ve had a two-hour meeting about a hum so if you favor the working group taking on this work specifically to include the semantic layer represented on this slide by Bishop decomposing HTTP but not necessarily that text menu favor expressing that in a smaller number of doc it\u0027s that we currently have you will hum for the first option and the second option is not to adopt this work at this time and if we end up there and there is an alternative I guess we will explore "
  },
  {
    "startTime": "02:06:41",
    "text": "those paths okay so I\u0027m option one which is for adoption please some now and option two to seek another path um now oh that\u0027s great very clear alright we are headed for HC betray slash sir thank y\u0027all get on the trade to our train okay alright we have Oh God a new meme is begun UH one question I actually - do you want us to stay in the repo that I set up now which is just in my private repo space I was thinking we could shift it over and blow away the contents of the Biss rep oh and maybe rename it because then we preserve the issues list I know we could blow it away but oh we can blow it away I think we\u0027ll huddle and send the results sorted I think that\u0027s the best way to do that okay thank you next up is s and I in old services actually it\u0027s really an old services focused presentation if I can find myself here where did my folder go no we got 23 minutes we\u0027re not no we\u0027re doing okay my computer\u0027s very confused excuse me for a second here it turns out it is Oh it\u0027s gotten confused about where the bar is right okay sorry oh it goes to the lower one right okay no not not the bottom one you\u0027re not a Mac user there you go all right so we\u0027re talking about keeping the data leakage down from putting the hosts that you\u0027re trying to talk to into your TLS handshake I imagine if you come to an IETF before you\u0027ve probably heard a session on this because it\u0027s a topic that keeps coming up and it\u0027s a hard problem like slide yeah so in a lot of ways encrypted S\u0026I has been kind of the Holy Grail it\u0027s something that we\u0027d really like to do in TLS and there have been some interesting proposals of way to do ways to do that I think is it Thursday that we\u0027re going to talk about the tunneling TLS in TLS proposal I remember another talk where we could use the hint chickpeas for 0tt to instead put the s and I and the one RTT flight and there are lots of ways to approach "
  },
  {
    "startTime": "02:09:42",
    "text": "this and it\u0027s because whose names are interesting they disclose some information about the person who\u0027s trying to visit the site or about what their interests are or what their plans are and we would like ways to avoid leaking that data where we can and there are lots of places where that information leaks another one is DNS but sni is an obvious place that people look for and it\u0027s a hole that we can plug next slide so the reality of the holy grail is a little more complicated than that because the servers do need to have the sni data to figure out what cert to give you there\u0027s a lot of routing that goes on based on the SMI header and even if you were to encrypt the the S\u0026I prior to TLS 1.3 you could just see what certificate came back so TLS helps with that but an active attacker can get this urgent good anyway because they just connect to the same site and send the same thing you did if they can replay it and see what the response was that well that helps next slide so we\u0027re trying to plug a lot of holes here in in getting this to be more private DNS gives you the host name we have do to help with that S\u0026I leaks it in the client hello well we can use secondary search to help with some of that in that if you can open a connection and then once you have the encrypted context then say what host name you\u0027re actually interested in and get the cert undercover that improves the privacy properties there but that requires the client to have some kind of innocuous host name that it can present in the actual handshake that will still get at a valid cert so the piece that we\u0027re proposing to address that is to extend all service to allow you to specify here\u0027s the alternative I want you to use and when you connect ask for this host name in the TLS handshake and then you might get a might get back a certificate that covers both host names you might get back other certificates just valid for the one you asked for and you need to use secondary certs but either way it gives you a path to actually approach an alternative with a more innocuous name that is visible on the wire but as Martin pointed out when I sent that draft out there\u0027s a good strapping problem and that you still have to talk to that host before to get the alt service record and so this idea has been around a lot of times but I don\u0027t think actually been written up that you can put alt service in DNS and circling back to doe again just there\u0027s a way that you can get DNS records without leaking them on the wire next slide so we\u0027ve got a couple scenarios here that we own that we like to make "
  },
  {
    "startTime": "02:12:42",
    "text": "work one of the easiest ones to deal with is if the host has many domains only some of which are sensitive you\u0027re on the same server so you\u0027ve got all the search right there it\u0027s you more interesting if you have a wild card cert that if you have a certificate for a start example calm then you can effectively conceal every sub domain that you have just by telling clients that they should connect asking for www.example.com or even underscore wild card just just pick something that is not going to give away with your real destination is and it\u0027s important to note that you still need the clients to validate the first version of the straf t\u0027 said that the client could completely ignore what was in the TLS name shake and just ask for the real one but the failure with that is that you then you have an active attacker problem that an active attacker could catch you to in TLS handshake min in the middle and then see what secondary cert request you made so you have to check that there is a real real server owned search they are first so probably you still need need one RTT to achieve this if you know that the certificate before covered both domains then you can do zero TT in that case next slide just trying to work through that Mike if you don\u0027t mind um so can you scope out the various light so the setting here for the 0n trip case is I connect to as you say start like github that I up and on and then separately I and so I\u0027ve got like a Dean I\u0027ve got it I\u0027ve got PSG established and then like some time later I want us to I want to go to CIA tech of and I do alt service I do a name resolution that gives me health service and and then I go and it says congratulations mister article at the i/o and then I and then and then what I do is I say well I already have that PSK so I\u0027m gonna do your arm trip is that is that the reason I\u0027m not quite okay yeah please help so in the the case where you could do zero round-trip is I saw the cert before yes the cert coverage start at github that I have so I will do s and I for something not github do and then send a request for something else so it\u0027s sorry so so I sorry so I do on the starry canopy IO and I and I resolve for CI a at github I oh and then it says you could do it any 30 good and you go to innocuous docket have the i/o and then I already have all that crap and cache so I just I just stare on drip right okay thank you I\u0027m not entirely sure I love that but I think it\u0027s okay so the the SMI extension "
  },
  {
    "startTime": "02:15:46",
    "text": "is really simple it\u0027s just one one more parameter on the old service record that says what s and I you suggest the client should use and sends alt service extensions are optional to understand if a client doesn\u0027t understand this then they\u0027ll just go use the alternative like normal they don\u0027t get the privacy benefits but it should still be a valid alternative the one departure from being a host name is the draft currently defines double quote to indicate don\u0027t use s and I I just omit the extension some people like that some people don\u0027t up for discussion and then next slide bends draft takes an old service record drops it in DNS so that you can go do a lookup for it and then just have it without ever having spoken to the host before so that helps deal with the bootstrapping problem where you have to connect in the clear at least once it also gives you some nice collateral benefits like being able to do HTTP over quick or opportunistic encryption without having to do the clear text or TCP path first and there one more question so it\u0027s back there yes I taught for this morning so I have a question so um I I\u0027m generally sympathetic and I think this is this is good work alternate services has a couple things just say about sni and I\u0027m wondering if we we think this dovetails without or we need to update alternative services it says that a client must not use a TLS based alternative service unless the client supports s and I note that the SMI information provided in to us by the client will be that of the origin not the alternative as well the host HEV header field value I\u0027m not sure whether that requires an update or not yeah neither let me consider food for thought yeah Kyle nekritz is your intent with this to be something that could be widely deployed and used on a large amount of traffic or just something that specific clients that really care about privacy and are willing to take a performance hit for that should do I think the alt Indian sp said that could actually be a performance win and so I would expect that to be done very broadly because you have good potential to step up too quick or hgp to on an alternative or an "
  },
  {
    "startTime": "02:18:46",
    "text": "alternative that\u0027s closer to you hiding your s and I you may or may not be able to do zero roundtrip and so I think it\u0027s going to be a an implementation dependent choice whether you value more consistent 0tt usage versus sometimes paying the extra round-trip to address a knife so the issue with a sni value in DNS that you send didn\u0027t get the proper certificate is that S and I can be replayed and someone else gets the certificate that you that will expose CX well site you\u0027re connecting to there\u0027s ways around that if you can give Akey that you mix into the TLS connection as well but that\u0027s more more modification into the TLS layer mm-hmm yes so I think we cover we cover both cases if you if you can do this that if you can get the certificate you need without disclosing any information and without an additional round trip then that is what will happen and if that is not possible then you will get the wrong certificate which does not disclose what you\u0027re trying to hide and the client will will pay an extra round trip in order to get the certificate that it actually wants we\u0027re gonna have to cut the lines after the people who are in line are fine just just to the host header point it has been a while since I looked at these deployments but at least a few years back there were deployments where the s and I check was used to make sure that they were in the correct set of cryptographic context but the host header was still being used to steer to one or more servers which were behind a load balancer so I would imagine that we would have to look at that to make sure that it was okay for us to to make sure that they changed in lockstep which I think totally makes sense given what you\u0027re doing here but I think that the language of the current alt service draft might need a minor tweak to make that no longer forbidden bytes back so yeah so right now what happens is you you will use a hold service to discover an alternative for a1 and then based on the certificate you\u0027ll discover that I can also make requests for other things and on those subsequent requests the host header will be different from the SMI so it was actually thinking about it in in particular in the use like the alt service records in the DNS case where you haven\u0027t gone and gotten the old service from the thing directly yourself but I\u0027ll have to think through it a little more but I I do suspect that either modern practices already moved away from this back or we\u0027re about to mom told in a very brief "
  },
  {
    "startTime": "02:21:48",
    "text": "comment acne you\u0027re looking at using s and I for validating ownership of domains this would be a landmine for them so it had to be a little bit careful with that and the other one was what\u0027s the relationship between the expiration time that you have in the record here and the are set things to think about not to not to respond to just just consider those things well okay I\u0027m responding anyway to save the draft New York to constrain you have to save him they must match so just stay here what Ted said all right so it\u0027s already the case that host header and s and I can mismatch because that\u0027s what connecting coalescence requires and it\u0027s not in fact I\u0027ve read the spec in a while because I include to me the first request has to match the s and I like say I you know say I connect to you know I want to fetch - I\u0027m trying to fetch you know from a column be calm and I erase the connections and then I and I connect it but the Viacom is higher priority and so I connect to a comm discover those are the videos for be a calm be calm I think I can just shove the be not common connection right away I want to write so um so it\u0027s already the place that they don\u0027t have to match um I think would be really good on Christian we don\u0027t did a nice job of trying to like map out the entire like threat model space and I have to have more this drafts and maybe it\u0027s like totally go awesome about this but a really good to define what the assumptions are like in particular I assume you\u0027re not trying to conceal the fact that it did you know that the funding server is in fact a freaking server for these guys because of course that\u0027s hopeless it you\u0027re right um so be good if your map and that\u0027s it is I think this seems a good idea at first glance but like be important about exactly what you\u0027re trying to conceal we\u0027re not hurting itself right so that draft does talk about one possibility of having an HTTP fronting server and concludes by saying there are discoverability issues with how would a client know what fronting server to use this in these drafts set out to solve that particular case while Christians Draft goes on to outline a TLS generic solution that requires a little more knowledge and cooperation from the server side okay thank you so just to clarify authors you are asking us to adopt these drafts correct do you think they\u0027re reading anything add so yeah I I would appreciate adoption if only because we need you know we need the attention to make it better sure so let\u0027s keep it simple at least to "
  },
  {
    "startTime": "02:24:48",
    "text": "start will do - hums first will be if you believe that these documents as a pair are ready for adoption and if you do not the second hump and if we have the second hump prevail perhaps we\u0027ll explore a bit further so if you think that both of these documents are ready for adoption please hum now and if you think that for whatever reason they\u0027re not ready as a pair for adoption right now please um does any does anybody want to get up and explain why they hummed in the- briefly they\u0027re not ready okay I think we might suggest you know they can be ready other than more time I I think I\u0027d like to see some people discussing the drafts a little bit more on that on the list quick question show of hands who\u0027s read the drafts okay all right we\u0027ll take that on board okay thank you so finally I have a last presentation which we\u0027ve talked about before so I\u0027ll do it quickly do you mind pressing the spacebar so we\u0027ve talked about variants before next slide right now we\u0027re at draft r2 there are a couple of outstanding issues they\u0027re not significant I don\u0027t think they\u0027re there they\u0027re just stuff we need to work through we have a toy implementation to prove the inspect our gardens and make sure that they actually work and and I believe that it\u0027s suitable for prototype implementation next slide so this is an example just to refresh your memory uh the variance header explains or conveys the request headers that are helping to form the secondary cache key along with the possible values for each request header and the variant key says which of those values was used to select this particular response next slide the previous proposal that is in this space which the working group adopted and then parked after we kind of stalled on it was key he is is different in that you basically ship an algorithm in a response header that says how to determine a secondary cache key for a particular resource whereas variant that algorithm is in the spec of the headers themselves so it\u0027s fixed this has a number of trade-offs it\u0027s a little more rigid but it\u0027s it\u0027s a lot more reliable for the server to produce the right response header though the big bit of feedback we got on key was it seemed like a big old foot gun that people could shoot themselves with and also there was a lot of implementer reluctance to adopt key they were concerned about the unbound nature of it whereas variant is much more constrained and and from the implementers that I\u0027ve talked to there seems to be a lot more "
  },
  {
    "startTime": "02:27:48",
    "text": "excitement about it next so should we adopt it I think it\u0027s ready for adoption putting my fast lay head on fastly seems like it\u0027s interested in this I\u0027ve talked to a couple of cache implementers who seem like they\u0027re interested in prototyping it and playing around with it so the question is is this ready for adoption or do we want to wait a bit longer is life in the room he\u0027s alright his lady I don\u0027t know cliff is here this time all right he provides this I see an audit of analyst comments yeah late life is is very positive about it so is there many comments that we\u0027d like to make before we move to the last time the chance between you in and evening out some on Thompson this is not me this is Mike real a for someone who\u0027s in the lake my coop and I\u0027m relying something from the chavn room from Tom Peterson who says it appears the suspect either D normalizes or writes off the question will be used that quality value indicators which are common in accept playing he said the goal um that\u0027s the Pacific about the accept header correct yes and I imagine there are is it using another ones no it uses the core devalue okay believe yep okay yeah I\u0027m happy to talk about the the specific algorithm is an adjustment if we can come to consensus it\u0027s just whether this overall approaches the right one do we have enough people in the room that have read the draft can make that decision or we need to go to the list can I have a show of hands if you\u0027ve read the draft maybe 10 they went down almost asbestos they went up but on the order of 10 do you feel like that working set is enough to to help you make this decision so I\u0027m not making the decision well make the decisions you know in your graph going forward let\u0027s see what Ryan to say here reviewers I mean I may be confusing Jesper I thought you had already asked for this on the list and got a list of responses on the list no I think we did an FYI and asked people to review the document pointing to the new dress submission so they should be counted too yeah hmm yeah it seems to have had it seems to have had positive engaging on the list yeah that\u0027s a good point right so I think will take home and then we\u0027ll take confirmation of it to the list I think that\u0027s probably the right thing to do is a small sample size okay so those in favor of adopting this is a working group item please hum now and those who think this isn\u0027t ready please him now okay well that was you know a modest home for the first in an empty issue um for the second so I think will confirm that Liss but we have another document dad Dorsett maybe one question um we\u0027ve parked key should we abandon that or keep it parked that was the assumption in previous discussions of us I should "
  },
  {
    "startTime": "02:30:49",
    "text": "have brought that up but this would replace key does anybody have any heartburn with that okay okay Roy fielding I guess since I\u0027m a co-author I should say I\u0027m fine with with parking it or abandoning it we need to then parked right so so as long as it\u0027s not a milestone that\u0027s great yep okay that brings us to the end of our work thank you everyone Oh Chet\u0027s got one last comment and we got one minute left no it was actually a question to you you you spoke feelingly at the sec dispatch meeting earlier today about possibly having people in this room look at some of the work that have been brought before it for a pepper do you want to give pointers well well you have everybody with you so Ted is referring to the a TLS draft application level TLS which defines the mechanism of establishing a TLS tunnel using a series of HTTP messages as well as some other methods and you might really want to have a look at that coming to a boss I suspect in a future meeting thank you "
  }
]