[
  {
    "startTime": "00:00:04",
    "text": "foreign foreign you want to um we're going to give everybody about two more minutes and then we'll start okay"
  },
  {
    "startTime": "00:02:15",
    "text": "thank you okay so I'm I actually managed to get myself on another account so it should be fine so now I see where the slides are and I see all the participants the the fun part of having more than one screen I want to get rid of this thing okay"
  },
  {
    "startTime": "00:04:00",
    "text": "um I think we can start since we have a pretty [Music] um uh pretty um full agenda uh this is uh Computing in the network this is our interim meeting uh and uh Jeff is on Eve has been ailing I would say I hope she can join us later but she sends her her best uh so this is the note well which everybody knows very well uh this is the policies you know the essentially I think what's important is the anti-harassment I think for me and also for a lot of people it's the question of patents so I think this is also very uh very important if you have some information in the presentations that you do today that you think is is covered by patents please make sure that they're disclosed um I will not I'm sure I'm not going to um have any information here that's not there but I think it's important and I think it's been that also a an issue with this group that is I think very important that this is a research group we're not there to do standards and you will see today that we have a large amount of most of the agenda is on research issues and projects and new ideas and I think this is what we want to encourage and and continue encouraging and uh I know Colin likes when I say this but we also invited today a lot of people who presented in other conferences and we were happy that they agreed uh to to share their um their work with us so I think it always is good to expand um the you know the collaborations and the participation and I think in our"
  },
  {
    "startTime": "00:06:00",
    "text": "case we're very very lucky that we are very much in a rapidly expanding um essentially field and that we are very lucky to be there and uh you want to take over Jeff okay hello everyone so today we will start from two papers first one yes child and uh it creates proposing a new uh programmable Hardware like Tofino but it's just more similar like the cheapest in the loudest in the internet so that can be very interesting and the next the second paper is and distribute that coordination or in network computation actually that was uh introduced to work in asean kotlin's last week I believe and that's quite relevant to our group and then the following three presentations was proposed to work proposed to last ITF meeting but we didn't have enough time there at the same time so they will be presented today so Doug chosen or I they will give us an update for the use case jobs and uh early early will introduce a new app just about distributed and learning architecture and uh you think though will introduce their some use cases for data population in network and our last presentation we have from Stefano he presented the EI EIP in one of our previous meetings like here today he will give us some updates"
  },
  {
    "startTime": "00:08:00",
    "text": "EIP in context of also machine learning okay yeah we can go to the next one okay so if you hear uh it's because you know what that meat gecko is uh I asked for a um a um help on on the minutes I I may get on on the on the tool myself later when we're done with this obviously we have our our mailing list and um we have all the material today online uh we have again we have two uh documents that need updates but there are G documents and I'm sure that there's going to be maybe discussion of sending them forward in the approval process after they need their update we have a new draft today uh which is this distributed learning and then we have this other ton of other uh um drafts that I think we really need to address um in terms of of making them either you know a lot of them are expired anyway so either we keep them expired or we move forward with them and we can uh discuss that online is not necessary to was there a question I saw somebody maybe asking questions no okay um and then our Milestones uh actually we're we're late um on on two Milestones uh which is which was I think the the covert has hit us in the way I think I think the scope is is evolving so much that um I think the three of us agree that it's really hard to uh to pinpoint and our Milestone review uh we're late and we plan to probably we should do"
  },
  {
    "startTime": "00:10:03",
    "text": "that more online to have more people but that actually uh is something that upon my really to-do list yesterday when I get out of the start of the semester here and uh three proposals I'm involved with but uh it's really under to do and Jeff uh even I uh are going to um cuddle on this and propose something else and actually propose a new set of Milestone that really really reflect like I said the the dynamic nature of this of this field that we're in uh you want to add something Jeff no no no okay uh so without further introduction I'm going to end my show and I'm going to start not unsharing my screen and on chair is where I'll unshare my screen and we can start going through the uh the presentations and um please send your questions on on the chat and I'll monitor the chat also while I try to take some notes and the first presentation is Trio Mainland can you share your screen share my screen uh can you see my screen or is it it's coming okay so this is uh you can see my slides not yet okay well I don't see them okay let me try again yep um do we have to approve it"
  },
  {
    "startTime": "00:12:02",
    "text": "yes yes it's starting no oh yeah [Music] okay so I've approved it but obviously uh that's not working hmm try again please oh no feedback please don't but there seems she's offline it shows yeah she may have problems um we should be able to load them from the from the um the documentation okay so here she is yeah yeah I'm sharing again yeah I know but somehow what does it say here I gave you access okay yeah now I think you should go okay okay finally okay good okay so hello everyone my name is Miriam a PhD student at MIT with Professor Maya gobadi today is my great pleasure to be here to share our work using Trio Juniper Networks programmable chipset for emerging in-network applications this is a joint work between MIT and Juniper Networks data intensive applications such as"
  },
  {
    "startTime": "00:14:01",
    "text": "machine learning databases storage and data analytics are the foundation of today's online services with The Graduate slowdown of Moore's Law Hardware accelerators are struggling to meet the performance demands of emerging Cloud applications in addition the amount of computation and memory that can fit in a single server is quite Limited this results in our need for efficient distributed systems for data intensive applications the availability of programmable switches such as Intel's Tofino has created opportunities for in-network computing to accelerate this data intensive applications such as caching database query processing and machine learning training the key idea of in-network computing is to Leverage The switch's Unique vantage point to perform part of the computation directly inside the network thereby reducing latency and improving performance let's take a deeper look into one of the representative applications machine learning training in data parallel distributed machine learning the Deep neural network is replicated across multiple servers and each server will process a small subset of the entire training data set at upper iteration servers will synchronize their model parameters by exchanging and aggregating their gradients to ensure convergence this step is called or reduce one challenge with the all reduced step is it puts significant pressure on the network fabric because the entire set of modal gradients must be exchanged many times during the training process recent work such as switch ML and ATP has proposed the idea of in-network"
  },
  {
    "startTime": "00:16:02",
    "text": "aggregation to accelerate distributed machine learning training throughout the talk I'm going to use a network aggregation as a running example the fundamental idea of in-network aggregation is to add the modal gradients inside the network switches rather than at end Host this can accelerate training jobs with heavy communication overhead our results show that the presence of strugglers imposes a practical deployment challenge with in-network aggregation for distributed machine learning the strugglers are quite common in shared clusters hosting several jobs where different servers experience uncorrelated performance data due to causes like congestion load imbalance or garbage collection [Music] when this happens all other servers must wait for this struggling server the struggler problem has been scouted extensively in the system and networking Community but there hasn't been much attention to struggler mitigation for in-network computing applications so why is this the reason is enabling efficient in-network structure mitigation intoofino switches is very challenging to handle the struggler problem efficiently inside the network the switch needs to perform efficient timer-based operations to mitigate strugglers such as checking whether a struggling event has occurred or not periodically and sending notification packet this is quite challenging because it requires weapon interaction between the switch data plane and the control plane as a result today's tofino-based Solutions and all other servers must wait for the"
  },
  {
    "startTime": "00:18:02",
    "text": "struggler when struggling event happens in this talk I will describe qml services foreign so can this talk okay okay yeah yeah nice okay so in this talk I will describe true ml our proposed system for efficient in network struggler mitigation in particular we can achieve 1.8 times faster training time for machine learning jobs by leveraging Juniper Network's programmable chipset in the next part of my talk I will first give an overview of juniper Network's chipset then I will discuss true ml our proposed in network struggler mitigation for Distributing machine learning training and finally I will talk about our evaluation result into final switches the central packet processing element is called pipeline all packets must travel through the same set of pipeline stages independent of the application true is a programmable chipset used in Juniper Networks routers and switches for over a decade shows architecture is fundamentally different from that of Tofino true has threat-based architecture and the central packet"
  },
  {
    "startTime": "00:20:01",
    "text": "processing element is called packet forwarding engine for an incoming packet to trio-based switch after it enters Ingress packet forwarding engine it will be processed by one of the available threats then the packet will be sent to the egress packet forwarding engine and will be processed by another available thread after the process is completed the packet will be sent out when multiple packets arrive these packets are processed independently using thousands of parallel threads on Trio different packets do not necessarily flow through the same physical path on the chip as a result Trio gracefully handles non-homogeneous packet processing rate whereas intofino only line rate processing is supported with the true architecture in mind in the next part I will talk about Trio ml our proposed in network structural mitigation for Distributing machine learning training to build an efficient in-network struggler mitigation Technique we need a network structure detection which enables the switch to efficiently detect struggler events we also need a network struggler recovery which enables the switch to gracefully serve the job without waiting for strugglers in network struggler detection requires timer-based operations based on user-defined straggler timeout and in network struggle recovery requires a lightweight mechanism in the switch to proceed the computation our system qml addresses both challenges using trios threads let's first take a look on a network struggler detection"
  },
  {
    "startTime": "00:22:01",
    "text": "as a reminder back to our running example to perform in-network aggregation for machine learning models the servers will send the model gradients to the switch and the switch will aggregate the gradient result and send back the result qml creates a new gradient record when it receives a new packet from the servers in normal cases without any struggler once Trio ml receives packets from all the servers and completes the computation it will generate aggregated results and send back to all the servers to build efficient in-network structure detection our goal is to detect non-responsive servers within Delta milliseconds timeout where Delta is a configurable parameter in the case of circular event if server 4 is a struggler then the gradient record only contains gradients from server 1 2 and 3. because server 4 does not have a chance to send a gradient from itself the naive approach for a network structure detection is as soon as new packets arrive the switch launches a threat and starts a timer to keep track of the non-responsive servers in this case the switch will launch a thread and detect which service is a struggler however the problem with the naive approach is that it couples the start of timeout threats with arrival of new packets so the switch we need to create one thread per packet as a result this approach is not scalable for large machine learning models about the number of threads needed true ml's approach is to decouple the start of timeout threats from packet arrivals in our design of qml we divide the total"
  },
  {
    "startTime": "00:24:00",
    "text": "gradient records into n groups and each group is scanned by one thread we also add a specific recently referenced deck to each gradient record this flag is set to zero when the record is first created every Delta timeout interval qml launches threads to scan gradient records if the recently referenced flag is zero then the thread will set its flag to 1. after another Delta timeout interval qml launches another unthread to scan the gradient records again if recently referenced flag is one then the gradient records time out and the struggler is detected with this approach qml guarantees the stronger detection within Delta to two Delta timeout window once straggler is detected we need a way to perform a network structural recovery true ml achieves a network struggler recovery by sending partial aggregation results to all the servers prior research shows partial aggregation can achieve comparable convergence performance in this way non-structural servers continue to make progress without waiting for a struggler for a very long time and also show ml keeps updating the struggler to ensure the machine learning model is consistent among all the servers in the last part I will talk about our evaluation result we perform training tasks on three on real-world hasbat including a Tofino switch a Juniper Networks router and six servers We compare our solution to ml to Tofino"
  },
  {
    "startTime": "00:26:00",
    "text": "based in network aggregating solution called switch ml in an ideal environment where no strugglers exist to evaluate the training performance when struggler exists in the cluster we followed prior work on structural generation pattern namely we set three possible delay points per iteration at each delay point one Server slows down with a given structural probability p and the delay time will be uniformly randomly chosen between half to two times of the typical iteration time the first result I like to show is the impact of struggling probability on training iteration time we sweep through different probabilities and measure the corresponding training iteration time the red line shows the iteration time for switchml as the struggling probability increases switch ml's iteration time also increase because the switch needs to wait for the strugglers before generating the aggregation result the Blue Line shows the iteration time for qml and the green line shows the iteration time for the ideal case with no strugglers true ml is able to maintain the training iteration time close to the ideal case because we have the in-network structure mitigation to mitigate the effect of stragglers the struggling probability P equals to 16 percent true ml achieves 1.8 times speed up compared to 3 Channel the second result is time to accuracy improvement with struggler probability P equals to 16 percent in this figure x-axis is the training time and y-axis is the validation accuracy the red line shows the training time for switch ml and the blue line shows the"
  },
  {
    "startTime": "00:28:01",
    "text": "training type for our solution true ml with a moderate rate of strugglers shoe ml can reach the target accuracy 1.6 times faster than switch ml we also observe similar gains for resnet 50 and this net 161. to conclude strugglers impose a deployment challenge for a network computing Solutions in this case we propose true ml a noble in-network structural mitigation system using Juniper Network's threat-based programmable chipset ensure ml outperforms today's in-network aggregation Solution by up to 1.8 times that's off my presentation thank you so much for listening and I'm happy to take any questions are there any questions yes Jeffrey so Mina I have two questions maybe also first one is you mentioned the irritation time so it seems It's a several hundred milliseconds so is that is that a realistic assumption or because a typical Network delay is quite maybe when several micro second so why this kind of assumption is a realistic in the data center um okay yeah you can go ahead and I'll answer the second question is uh"
  },
  {
    "startTime": "00:30:08",
    "text": "so please go back to the pre maybe you can answer the first question first oh yeah sure maybe I can just go back to the evaluation where the first result shows so yeah so your first question is about the training iteration time uh like it's in hundreds of uh milliseconds um so to answer this question basically we perform this training experiment on real world has values in a real world model um this is the result we get from The Real World model and also we compare our results to the typical machine learning training work and we found that the result is quite similar so it's not an assumption but it's the result from the real experiments so can you go back to previous slide and I noticed you have a set in your setup yes so you introduce some uh staggers right so that way is the time scale of that stagger is uh it is iteration time iteration time yeah my question is is that realistic assumption yeah so the stronger generation gender oh yeah The struggler General patterns that we follow a prior work on on struggler we follow a prior work on struggler and we follow their approach to generate those Travelers um it is a realistic um it is a realistic assumption because the typical iteration time is in the next slide and basically we just choose the instructor account to be half to two times of the training iteration time"
  },
  {
    "startTime": "00:32:02",
    "text": "this is based on prior machine learning work and also so you think it's too large or too such a long a large but the Stagger in realistic maybe is not the same scale of the iteration time because well I think you are thinking the structure is mainly caused by Network delay is that your point yeah can be caused by many reasons it's not simply about Network there are also something can happen on the struggler operating system such as the strawberries busy doing other tasks or the driver is doing some garbage collection tasks on the server so these are all the reasons that can cause the struggler it's not simply about the network delay okay I see so my question second question thank you the second second question is about the background on the check so in your experiment are there any background check or it's just this single application um what do you mean by background you mean other tasks are happening at the same time oh yeah so in our experiment we are not having another task happening at the same time but in realistic setting it is possible that multiple tasks are running at the same time and this is when our structural mitigation effect can benefit the real system okay so I have no further question thank you and uh one country in the the chat channel so can you see it or I can delete it um"
  },
  {
    "startTime": "00:34:03",
    "text": "which improvements are related to the specific specificity of the chip and which are generic [Music] so for the Improvement we are getting from the struggler mitigation it is quite specific for our approach of mitigating the structural effect so as you can see in this evaluation result the red line is the in-network aggregation for using the Tofino switch and the blue line shows the iteration time based on our chipset our main benefit comes from our ability to mitigate the struggler inside the cluster whereas in other Solutions the tofino-based solutions it is very challenging to implement such mechanism so the improvements are basically coming from the struggling mitigation we have in the system and so uh thank you again very very much for for this presentation and we can move to the next one yeah thank you thank you and the next one is dicer which is another application of course in network computing and I will let uh the authors present uh who's presenting what's us Rose presenting so I will let us presented thank you hello am I Audible hello yeah we hear you please okay perfect thank you so uh good evening everyone uh"
  },
  {
    "startTime": "00:36:02",
    "text": "a PhD student at Robert Bush game we have with uh Technical University of Munich and my research topic is currently focused towards orchestration solutions for ICN based in network compute systems today I'm going to present Daiso which is a distributed coordination solution for orchestrating in-network computations uh the traditional internet was originally designed to enable communication between stationary nodes and therefore it adopted the host Centric communication model in order to communicate with a node and retrieve a piece of information one first needed to establish a channel to the node providing this information so information Centric networking architectures such as name data networking shifts this Focus From The Host providing the data to the data itself and hence the name information Centric networking this enabled end users to not only address and access data directly using name based access mechanism but also resulted in a loose coupling of data to the host other features of ICN include inherent multicasting capabilities by interest aggregation in network caching flexible hop routing and so on so information-centric networking uh therefore provides access to data instead of the host now name function networking further extends name data networking or ndn by enabling end users to request not just static data but dynamically computed results so consumers request for execution results with the help of defined workflows that help to structure the request name nfn uses this underlying NFL Indians forwarding principles along with resolution strategies which resolve the function request and help forwarding it to the suitable execution nodes the Default Resolution strategy in nfn is find or execute the nodes identify the function and data objects from the"
  },
  {
    "startTime": "00:38:00",
    "text": "requests and decide to forward the request if the node has neither data nor function so this can be Illustrated with the help of the figure here where nfn Node 1 um forwards the interest i1 because it has neither the function nor the data for executing uh if on the other hand if there are few missing components the node initiates a fetch operation to fetch these missing components for instance the NFL node 2 here initiates a data fetch operation because it has a function F1 but is missing the content of data D1 and therefore sensor data drag operation so if it is missing function then it sends a code Fetch and so on the Node decides to execute a function if and once all the required components for computing the result is available so in this case node nfn node 2 here executes F1 as soon as D1 reaches node two and the result of this execution is now again sent to the consumer compute consumer so although this decentralized significant resolution engine performs per packet decision which results in timely resolution of requests there are still some limitations for instance the structure of the workflow defines the path taken by the interest or the request so the consumer who defines these workflows is often unaware of the characteristics of data or function while creating these requests and this could result in a workflow taking a different path that is less efficient when compared to the path it would have taken had the same request been structured a little differently additionally in order to make prompt decisions the resolution engine restricts the amount of information it processes to the local knowledge of the node and therefore does not consider the topology or the neighboring node characteristics while making these forwarding decisions so such this could lead to some optimal forwarding decisions uh especially while uh considering decomposition of"
  },
  {
    "startTime": "00:40:02",
    "text": "functions so some such suboptimal decisions could be improved by enhancing the knowledge scope of the notes while making these decisions Daiso is a distributed coordination solution which Targets this enhancement of local knowledge scope used in decision making and improved the sub-optimal resolution of nfn so dicer retains the nfn folder at all compute nodes additionally dicer establishes synchronization groups where all the nodes in a group exchange their state information with each other this added knowledge of the surrounding notes would then help improving the resolution decisions taken by nfn the figure shown in this slide comprises of three nodes NFL nodes a b and c and the blue circle around the node shows the knowledge scope of the node using which the nodes which you can see that does not extend beyond the node the Q next to the nodes V and C refer to the execution queue that the node is presently handling when a compute request reaches node B the node decides to forward the interest Upstream towards node C due to lack of data and function even though it has a compute resources that can be used for execution node C on the other hand despite holding the necessary components does not have the resources to spare for the execution of this interest hence the interest is added to the execution queue resulting in further delay in the response with Daiso we now Group B and nodes B and C together and the nodes periodically exchange information about the estate with each other in this case node B is made aware of the execution status and compute resource availability of node C the extended knowledge scope around node B and C is represented using this brown shaded region around the nodes once the nodes synchronize the nodes can"
  },
  {
    "startTime": "00:42:00",
    "text": "coordinate with each other enforcing some decisions that alter the resolution decisions taken by their respective nfn resolution engines so in the third figure here node B realizes that node C is overloaded with interest and performs some fetching operation of functional data this enables the node B to now participate and share some of the load at node C and reduce the latency in responding to the computed result uh to the consumers question um there are still some challenges and open questions that needs to be answered in order to realize daiser so how do we group notes when and what information do the notes synchronize with the members of the group what kind of coordination decisions should the notes take with the collected information and how do we handle the network overhead that our raises out of such periodic synchronizations in order to answer these questions we split the concept of Tyson into four phases namely neighbor node Discovery synchronization group formation synchronization and coordination uh in the upcoming slides we present in detail each of the four phases the first phase is a neighbor node Discovery phase this is dedicated for the nodes to understand the network topology and the static characteristics of the surrounding nodes and in order to accomplish this we take inspiration from the name link state routing protocol the nlsr protocol for sending periodic hello messages from every node these periodic Discovery messages are broadcasted in periodic short bursts and in order to curb or restrict the flooding of network links with such Discovery messages we use interest hop limit to a certain range within which the nodes need to be discovered every node receiving the discovery messages would then respond with the static configuration information such as compute configuration Network link capacity hop distance path latency and"
  },
  {
    "startTime": "00:44:01",
    "text": "so on the table in this slide is an example of information data procured at node N1 I'll uh at Node 1 about the other nodes and once the nodes are discovered the periodic phase is now used to identify nodes entering or leaving the discovery scope the next phase is a synchronization group formation phase we Define synchronization group denoted by SG by three parameters namely members of the group VI the synchronization interval or frequency TI and the information synchronized which is pi one could theoretically group every node in the world into a single synchronization group and synchronize it on extremely fine details at very short intervals however this is not practical due to the network overhead and sharing huge volume of data and the processing overhead for making coordination decisions at the same time restricting the group size and the information may not help in bringing quantifiable improvement over nfn hence we are employing multiple synchronization groups of waiting size information and frequency the figure here shows three different synchronization groups sg1 sg2 and SG3 sj1 is a group of nodes one hop away sg2 groups nodes that are up to two hops away nsg3 groups notes that are three hops away and so on the size of the group sg1 is smaller than sg2 and sg2 is smaller than SG3 meaning the number of nodes in the group is in the increasing fashion and the darker the shade of gray is fine granular is the information synchronized so as the size of the group increases or as a distance from the node increases the amount of information shared is reduced to just the crucial information the Crux of the information and the synchronization interval is also reduced this helps in gaining"
  },
  {
    "startTime": "00:46:01",
    "text": "information across a large scope while maintaining the network overhead under control okay this slide shows the sequence of Interest data exchanges that happens in the group formation phase the synchronization group formation proposal interest is initiated by every node targeting the discovered nodes and inviting them to join the group The nodes receiving the interest would then respond with the follow-up interest requesting for other information pertaining to the sync group such as the mem other members of the group the scope of synchronization periodicity of synchronization and so on if the nodes do not detect any redundancy with already established groups then the nodes respond with an acknowledgment uh for joining the group and on discovering new notes to be added to an existing group or removed from an existing group all the other group members are notified as well as part of the group formation phase at the end of group formation where all the nodes have been created at the nodes all the groups have been created at the nodes each node performs group optimization to detect and avoid any relevant synchronization happening within these nodes an example of group optimization scenario is shown in this slide with a Phi node Network so the Node 1 forms two groups one group with nodes one hop away that is with node two and node three and another group with nodes two hops away with node 4 and note 5. similarly with respect to Note 4 it also forms two groups one with nodes at one hop distance with node two three and five and another with nodes at two hop distance that is with just node one here the two hop group at node 4 is redundant as all members of the group are already a subset of two hop group from Node 1 and therefore is completely removed node 4 also refrains from fetching the updates from Note 5 in the"
  },
  {
    "startTime": "00:48:00",
    "text": "two hop group of node one as they already fetch more find the detailed information as part of the one hop group that is formed by the note 4. additionally nodes belonging to multiple groups of the same scope also avoid fetching the changes twice per synchronization for example here node 2 and node 3 both belong to one hop group from Node 1 and node four and hence do not fetch the change updates if already obtained from the other group synchronization so such group optimization is mainly focused on trying to avoid any additional Network overhead from synchronization uh occurring from redundancy and therefore it detects such scenarios the next phase is synchronization phase we adopt the application data set synchronization protocol called the state Vector synchronization which is SVS which was formally developed for synchronizing uh synchronization and distributed applications with several participants an example of one such application is the Indian chat application the SVS data model is based on Vector clocks in this slide we show SVS in action for the network shown here with three nodes user a b and c the change updates at each node is mapped to sequence number and this sequence number is published as a power of change notification for instance there is a change update at user a and the sequence number is now incremented from 50 to 51. this changes corresponding to 51 needs to be updated to other users in the group this is done with the help of a change notification which is a sync interest initiated by user a the structure comprises of the group prefix followed by the state Vector representing user A's knowledge of states of all the users in the group the sync interest is now multicasted to all other members of the group each group also holds a local data store which records the history of changes"
  },
  {
    "startTime": "00:50:01",
    "text": "that each member of the group the slide here shows the data store at user B and C by comparing the state Vector in the change notification with the local state in the data store they detect if there are any updates in the other members in the group so now they detect that there is an update in user a that they are not aware of in order to fetch the latest change updates from user a corresponding to sequence number 51 both user B and user C initiate the fetch operation once the corresponding data packet reaches the users they update their data store such that the local state is synchronized with the group state therefore in order to fetch each change update it costs approximately three message exchanges one sync interest one fetch interest and one fetch data the synchronization phase can be invoked either periodically where the nodes publish a sync interest at fixed time periods or it can be invoked in an even triggered fashion where each change is an event triggering synchronization in daiser we Implement two types of synchronization groups grouped based on Hop distance the one hop synchronization group at each node groups nodes with one hop away while three hop nodes uh scope extends to three hops the node within one hop group Share Fine general information of State such as nodes resource utilization functions instantiated at a node functions requested with node list of unresolved function requests the current execution queue and so on uh since the three Hub groups are larger in size the synchronization information is aggregated and each node shares a cumulative knowledge of itself with its one hop group which we refer to as a Zone that is each node shares the cumulative knowledge of the network Zone it belongs to for and examples of those could be the average resource utilization of a Zone the most unresolved function or most popular function at a Zone and so on the synchronization interval for one hop"
  },
  {
    "startTime": "00:52:01",
    "text": "periodic synchronization is closer in order to enable the neighboring nodes to react quickly to the changes in the neighborhood while the three hop synchronization is invoked at larger intervals so that the neighboring zone is prepared to handle the gradual changes in the network across a widen scope after synchronization the coordination decision making phase begins where each node Aggregates all the information collected from synchronization with the information the node enforces some changes locally that alters the forwarding behavior of underlying and FN resolution engine each node identifies whether it has the necessary compute resource available for executing function requests if the node can spare some compute resources then it identifies any busy neighboring nodes which is congested with requests and the list of unresolved functions at such nodes this list is then sorted in descending order to ensure that the most popular unresolved function is resolved first or hand instantiated first and the free nodes fetch such functions and instantiates them resulting in a load balancing effect by sharing some of the compute load from The Busy node on the other hand if the node is facing a high compute load already it looks for any idle or less frequently requested functions that can be terminated such functions are forwarded Upstream at the cost of increased Network consumption and latency in responding to these requests now we Implement all the phases of the iso and evaluate them using ndn Sim which is an ns3 based simulator we implement the Inc sum module which simulates the behavior of NFL resolution engine at the compute nodes daiser is an application running alongside nfn taking care of the faces such as neighbor node Discovery group formation optimization synchronization and coordination the network setup"
  },
  {
    "startTime": "00:54:00",
    "text": "comprises of a hierarchical topology of compute nodes and consumers the simulation parameters that we use is presented in the table shown in the slide we generate the compute load by allowing consumers to request for function execution over different data components and the following slides would present the different evaluation metrics and performance of dicer against nfn okay uh the first evaluation metric is the orchestration map so orchestration map shows the function placement at each node in the network topology we also implement the next fit decrease in heuristics which sorts the function in descending order of popularity and places them at nodes closest to consumers in progresses Upstream and this heuristic does not replicate instantiation of popular functions at multiple nodes now we compare the orchestration map generated from dicer and nfn against that that generated by nfid and on the x-axis we have the increasing number of compute nodes in the network and on the y-axis we have the number of functions placed at these nodes by the different solutions with anything the number of functions instantiated is less compared to that of the iso and and nfid in an imbalanced float scenario so uh by imbalanced load scenario what I mean is that uh in a network setup half the nodes face High compute requests while uh remaining half of the compute nodes are pretty much idle in this case the nodes which do not observe any compute load uh in nfn remain idle and those nodes with high compute load are congested with growing execution queue on the other hand with daiser since the nodes are more aware of the network state in the zone it identifies and instantiates more popular function bringing up a load sharing effect Additionally the daiser coordination algorithm also ensures replication of"
  },
  {
    "startTime": "00:56:02",
    "text": "those popular functions with a high number of requests that cannot be handled by just a single instance of the function running at a node so daiser also takes care of replicating popular functions we also evaluate the reduction in completion time with the iso so completion time is defined as a round trip time from the compute request initiation of the consumer until the corresponding data packet with the result reaches the consumer we tune the imbalance Factor denoting the load distribution at the compute nodes from a balanced load where all the compute load compute nodes in the network face um equal compute requests uh to an imbalanced load which is ibf equal to one where uh the load is just um on 50 of the nodes on the x-axis the number of unique functions um and the setup is increased and on the y-axis we have the completion time we observe that in a balanced load Indian already performs well with identifying and instantiating the needed functions of the compute nodes but in an imbalanced load situation where half the nodes are idle and other half of the nodes are extremely busy Tyson brings down the completion Time by utilizing the compute capacity at the idle nodes the next evaluation metric uh in is the network overhead of using dicer and its effect on the scalability of the solution so in this graph on the x-axis we have the network compute scale denoted by the number of compute nodes and on the y-axis we have the generated Network traffic fundraiser synchronization we employ different synchronization intervals starting from two seconds up to 50 seconds and we see that the network overhead shows a quadratic relationship with the network scale as well as the synchronization interval group optimization to an extent already helps curve this overhead uh to a certain extent but there are also"
  },
  {
    "startTime": "00:58:00",
    "text": "other ways to reduce the control traffic for instance by employing even triggered synchronization in a parcel Network or one could also employ partial synchronization as one directional synchronization where nodes with high centrality degree collects information from surrounding nodes and enforces not local but zonal decisions and the nodes which are not well connected do not get any updates from the the other nodes and this kind of reduces the synchronization overhead even further so this brings to the conclusion of my presentation of tyser summarizing we presented the Default Resolution strategy of nfn along with its limitation following followed by the four building blocks of daiser concept and this was followed by a brief overview of the evaluation metrics and performance comparison of daiser against nfn and as part of future work we intend to enhance the cognition algorithm to operate on the objective of achieving joint optimization of computer network resource utilization as well as minimization of completion time currently the coordination algorithm only focuses on function placement and we next intend to also include the characteristics of data objects and placement during decision making here the references used in this presentation thank you and uh I'll be happy to take it take up any questions Sarah are there any questions um okay there's none anyway questions can be sent to the chat or be sent to the list uh and um well we're on time which is great so next one is Dirk I think if you are presenting so uh did you have a question Dirk or you're just presenting okay uh Dirk is presenting and uh it's"
  },
  {
    "startTime": "01:00:02",
    "text": "the use case update thank you hi sorry sorry if it's still here with my clicking the buttons you can see as I wanted to present um I wasn't entirely sure if this was the last version that was on the data tracker and then I swapped the presenter um uh icon I did a double play but I ended up doing the presentation yeah this is about the use cases for a notebook Computing um you'll notice and I'll come to this later on at the end um this is an expired craft that you I'm not interested I already mentioned that so you can find in the archive at the moment only um yes so the purpose of the draft is is to go after you know what coin Charter talks about in scope number two research and use case driven requirements analysis benefits of the type of networks of these networks remain functionality that's all text is written in the charter um what we've done so far with the draft was to collect use cases um and uh recently in the most recent iterations we worked uh more on structuring them the the draft if you follow the draft in the past a little bit it had an organic growth it's a it's it's a emerging of several activities into a single collection of use cases but the structuring that was looking more into providing insights into those benefits research question opportunities for coin to go really more on what the charter was asking for as as a goal um we we started with uh when it was adopted finally as a research draft and we are currently at version two which is the one that expired but in version one we already started recouping the actual use cases as a result of the adoption we have now four major groups which are the four main um sections which which are looking at new coin experiences um new coin systems um improving existing coin capabilities"
  },
  {
    "startTime": "01:02:02",
    "text": "and enabling new capabilities and we elaborate in these sections a bit what do we mean by that um we also try to sharpen and tighten up the taxonomy I'll come to that in the next slide and also prepared and start the analysis on research questions and requirements I'll come to that as well so this was there was quite a significant change when we actually went to um the the the the the adopted draft level so this is the current structure and that's the same for V1 and V2 we just added more more cases nothing really that did change um this is the use case recruiting that I mentioned so the main sections three four five six are the um four groups I mentioned before uh we have a I'm yet again got another use case so uh uh Xavier um joined uh the draft uh was a new use case in Virtual Network programming that was another one that's the latest one that we added we also had is it up there the personalized interacting Performing Arts as an example that we added with Miguel and David who joined I think also around the same time so uh my HSA joined I think with the extended reality so that's what I mean it was kind of like bringing various views and various people together at the end ended up on the um on the author list um I said start with the analysis in B2 so that's the latest section number seven and we also try to in the meantime to partially align our terminology um with the other um craft that that my children mentioned uh the coaches draft that has also expired um in the meantime the terminology as I said um uh you know we introduced language and try to also adjust all of the use cases to follow the same language given that they've all that they have had grown organically there was a bit of a language mismatch which we try to smoothen out a little bit so we introduced these type of terms that are explained in the draft coin program coin"
  },
  {
    "startTime": "01:04:00",
    "text": "program instance a function capability Etc these are the definitions from the craft excuse me and the green ones we try to align with duck's draft and we also introduced a few new ones the coin experience the current capability that I mentioned was in the recouping of the um end of equipping off of the use cases that we utilize we also introduced kind of like the devices we're using pnd a programmable and network devices it's kind of like the term we introduced for something that is programmable using something like P4 or similar right um then you also restructure the the use cases so we try to follow the same kind of taxonomy in all of them so that you can hopefully read them in a very similar manner um so there's a link to the category in the description the description is trying in a relatively closer language to outline what it really is that this use case provides as an experience and characterization what are existing solutions to maybe do parts of these use cases um in in some of the case in some use cases parts or almost entirely what are opportunities and we split up the objectives in the research questions um in in one of the latest iterations uh and then we also added requirements for the account capabilities um there and this is said all try to pull this through um every single use case that we described um I think we've done a decent job at least to follow that taxonomy but there are still cases where we need to maybe solution it a little bit more we started as I mentioned in V2 with the attempt of an analysis so for that we used I and we had a couple of separate meetings we were in the authors to think about how can we structure research questions along various categories and some of you who may recall material are presented something similar in the other entry we had um losing a bit of track of time I think"
  },
  {
    "startTime": "01:06:01",
    "text": "that was in um spring this year right when we had the other interview on how could you look at the various things that we're talking about so you know you can see Visions for coin at the bottom enabling technology is just doing Computing framework applicability areas uh and and we adapted this kind of uh way of looking at coin uh in order to structure research questions along those different categories right the requirement analysis is something we intended to we have requirements in the actual uh use cases but we haven't really looked at them coherently but that's the part that's still missing that's an empty section at the moment so the next steps where we are that we would like to do well the office first observation coming back to this the draft has expired there was a notice recently and there was a discussion among my HSA Ike and myself what should we do at the moment we let it expire um in order to present this to the working group in order to come to the questions in the next slide right so we need we would need to resubmit a new version that's the first step um there is work in the draft that could be done um finishing to align the use cases uh I said there are still some bits and pieces and Corners where we need to smoothen the taxonomy a bit it's a huge task that needs to be done uh aligning the draft with the terminology that we try to introduce and also obviously also agreeing on the terminology if possible the analysis part you know condense the opportunities we just question all this kind of things but it brings me to the questions in the first place so there's work that could be done if you wanted to um the questions though I have to to to to the community the working group is well first um do we want to collect a terminology that's emerging from this draft and also trying you know aligning with the coaches draft um where do we want to collect that is that a good place to leave it in this draft or do we want to have this somewhere separate um that's the first question um we also looked into should the analysis really be part of"
  },
  {
    "startTime": "01:08:01",
    "text": "this document we started with the analysis because we felt that there's material in there that could really help us to make sense of the discussion um or do we want to maybe finish this document and do the analysis in a more thorough manner maybe in a different document right we could keep this purely at the use case level and and leave the analysis for a separate document that's another question to decide on but I think the most crucial one really is given that this draft has expired do we want to see this work continued I mean the drafts are available they're in the archives if you want to read them but do we want to have the work continue do we want to have a last call for publication considering the editorial questions that are asked around the analysis um do we want to continue and then obviously the question inevitably is are there any other contributors that would help us um pushing the work forward and with that um I think there's a question for my HSA it's one of the co-authors or as a chair I don't know thank you very much there's more as a chair than um I think it's relates to your questions and of course we could move a lot of that to the list but uh I think collecting the coronary terminology is a good idea uh maybe we want it would be maybe better to put it in a small draft and making it on its own in a lot of groups have ontology drafts and our and rfcs so um I think that would be um anyway what I think and others can disagree uh I would say the analysis I agree with you uh that it maybe should be out and I think it's important to have the work continued and and this is me as a chair now it I I I I know I'm also an I co-authored but um I think it's it's an important um topic because of how Dynamic the the"
  },
  {
    "startTime": "01:10:00",
    "text": "the field is and the the use case that are in there uh have like some kind of also the historical nature uh you know we started this a very long time ago and I think it it evolves with the with the field and I think would have you know like some kind of a good uh overview of how this started and where it's going so um yeah so this is me as the chair uh I think as a co-author obviously I'm I'm open to continue helping with this but um yeah so those are my comments any other people questions comments okay uh so thank you thank you Dirk and thank you everybody uh for presenting this um and I think I'm lost in the agenda but um the next presentation oh is a new draft uh from uh jaoli from Beijing University of post and communication and it's also about uh some ml related uh work so uh ciao you want to present are you there I guess okay cool I'm sorry I kind of found my slides oh um there should be um they should be there no no it's not my I gave you the Share I gave you the control"
  },
  {
    "startTime": "01:12:02",
    "text": "okay [Music] there we go okay hello everyone my name is Charlie and I'm giving a speech on behalf of authors for our work topic of my species distribute learning are architecture together as Cloud collaboration okay I'll keep it the intolerance is an important question senior of this work which is which requires a large number of the federal intelligence models to provide service for iot device the iot architecture is showing figure which is divided to Cloud layer Edge layer and the rest layer and they have their own jobs and this is my motivation with the development development of 5G technology and the popularization of iot medicine is graduated by the date this period of the mobile Terminals and iot device when facing the training requirements of artificial intelligent models the Edge compute The Edge Computing under the cloud computing how they are open initial comments so distribute model training architecture business besides on"
  },
  {
    "startTime": "01:14:01",
    "text": "it Cloud collaboration it has become a feasible shame for artificial age tolerance mode print um okay okay very easy and here's our walks so first I will introduce uh beta's technology the model training Supply as shown the figure eight since H since each layer of an artificial intelligence model have a Independence input and outputs a model can be studying to multiply several models for Independence training where the training layer that links the models is called segmentation layers the this mask can provide the besides for age age Cloud collaborative collaborative training so the on the training Pro [Music] cesses have uh the the entire training process is showing you figure beyond the figure saying balance as it seems some mistake um so um let's add your no stands in a mode of training request to the cloud nodes after the cloud nodes received all training requests from the um I've noticed it prepare for more more training which is divided into the standardization and the model did main nation in the most domination the cloud nodes that means a model um architecture according to the training tasks on the standards to our age notes and in order to reduce the amount of"
  },
  {
    "startTime": "01:16:01",
    "text": "um of the Computing in the training process decided need to be standard reasons before training the standard Edition Master Edition they they turned by the cloud node of the preparation of is complete in terms of monitoring stage in order to ensure the quality of the monitoring it is necessary to ensure the cons considers of training delay and the consider consistence of the model transmission line the training deal and the transmission data are set by Cloud nodes based on experience in terms of the Trinity line combining with the most writing under the computing power Network and come calculate lets the training time of each layer of models and the number of layers to be trained by as node is calculated according to the Trinity line [Music] and same time is also possible to this means the size of the data point of the segmentation layer on design reserve fundwise for the model transmission station here in advance [Music] after the age eight nodes finish is 2004 preparing model and it sends the segmentation layer of the Preparatory model to the log to the cloud node after receiving the signature layer of the mode of the column completed separate screen training of model and design affects the mode weights um so far a random model training is is complete and after each thousand of"
  },
  {
    "startTime": "01:18:02",
    "text": "training the cloud the cloud node the current um to generate a global globe model so through the distribute learning such as the on the from the rotate learnings and the age node continuing to train them to train according to the color mode under the model Acres meet the requirements and there is some smooth simulation requires a strong our our architecture can can improve the curioso models and thus reduce the um reduce the computing pressure of its nose and the improves the query of Nano service thank you are there any questions uh what do you want to do with this graph because you want to continue it you want to um yeah what are your your goals with the draft for for from from moving on now I want to prove um [Music] foreign Computing balance I want to come I want to provide the compute balance most for for model training in in networks because in"
  },
  {
    "startTime": "01:20:01",
    "text": "uh yeah in order piece the training besides the clothes the clothes Computing is neither larger large Bond wires and the high neck injury consumptions item maybe some is maybe in those works under besides each Computing is a it doesn't have have much computing power and it can't it can't certifies the model training request month I want to find the balance when when since this is a new draft um any other questions things would be to maybe start a discussion on the list about how could how this could evolve uh within within the group so um yeah so let's let's uh let's do this is me yes yes my control to share my screen uh I I actually oh okay sorry yeah you have you have the the share so yes the next presentation is uh on on new ideas um this um first one is for data operations in network from Huawei okay okay thank you hello everyone I'm eating from Hawaii uh I'm very delighted to share my thoughts about our solution data operation in network and today maybe I just pay more attention to discuss some tone your stress and I will"
  },
  {
    "startTime": "01:22:03",
    "text": "introduce introduce what scenarios we think don't Solutions a traditional way and maybe I will I want to discuss the in network computer in another adapter from the network particle and maybe we want to talk about how some uh in network computing work can work in a real nice work light Data Center okays and okay nice page as a ISO first maybe I will talk more about our motivation we know that the recent Recent research has shown that the network device undertake some Computing tests can greatly improve the overall Network and the application performance in some scenarios like we talked about lots of first presentation we talked about today so we think that the door research should pay more attention to some scenarios while the data operations are required to be done at a synchronized node well the operation is simple enough to be done at the at the line speed that means the wasting the networks the network device build some Computing tasks will not affect for starting performance and maybe it's just a space it and the simple operation and so we introduce some three Dawn scenarios and we think it will be useful okay it's a nice page uh the first uh the first scenario is"
  },
  {
    "startTime": "01:24:02",
    "text": "about artificial intelligence scenarios and within interest scenarios so first presentation we talked about a lot within the increased the number of surveys does not lead to a Learner in linear increase in a service performance and we find many ways to Solutions this question uh for one day is uh where our parameter Center and for another way is uh we have already use solution uh part of all the night radios or some Computing in the network solution uh we know that the way to do some aggregation tests and we know that the switch in the center topology and the switch will aggregations message from the uh from the distributed notes and in this patients which will aggregation the information from the form machine and the reason that this aggregation so don't think that the aggregation may be a basic and simple simple operation like way kind of strategies operations like uh some arbitration and if we want the dancing uh the package from the zero machine to the switch the package will tell the switch the switch should do the sum operation and so and they will have a standard practical to tell the how the machine packages is in Computing the Computing information and also don't think"
  },
  {
    "startTime": "01:26:02",
    "text": "likely you know data center is has the data center has a more public network topology and not like some some some simple uh topology only have one computers switch under the dawn within the way should have a solution to rooting the the routing the package to the right for the right switch and the routing the package to the aggregations which notes the other switch and this is within the dawn should do underway want to discuss in the Dawn and this is about artificial intelligence scenarios okay it's a nice scenario we talked about is a log test and the way we all know that the traditional ways loading test is very common question in the database and the traditional way use the log manage but in the insert on we think we can use the net lock switch to the office test and replace the log manage Photon within such a lot manual can be obstructed as a compare and the swap operation or like the Fetch and operation for example we transcend uh you know the house will send a loud message to the net loss weight the net not switch can repeat the host whether they can get this lot maybe it's is a basically and a simple operation so we're saying so don't think the net notes which can do this and will"
  },
  {
    "startTime": "01:28:00",
    "text": "have that benefit from this solution and in the dawn will also sent away through the support the routing solution or other operation and Computing information package wait okay so tonight is the secret sequence and the second is a scenarios that uh the package the different package should have showed how a message to to decide which message is early to reach the to the server and in the traditional way we use the global transaction manager but in the dawn we think this operation we can't say that it's like a fashioned at operation and wasting we can get benefits and the which can do this and it will become more faster in the summaries in my Southwestern the door Network cannot support very complete computer operation the wasting some basic uh spaces application return of abstract to some uh basic basic and simple operation and the way since this operation we shouldn't can't affect the forwarding performance of the data plan because if we advise the reporting performance within this will be um there will be less useful and within who our don't can there always some bottleneck of the computer operation so in the dance dance"
  },
  {
    "startTime": "01:30:01",
    "text": "Southwest and we should have a general mechanism to the Oasis uh question and we should repair to tell the switch what to do and uh what's the package how the package routing to the right switch okay this is my thought about the Dawn and the way it says that the use case we introduced before return of abstract some basic operation like uh some uh the compare and swap on our surface and the action okay the next page and this is wasting that uh so don't device we can do under wait and consider in the dawn solution uh at first waiting a door Network can root the Computing package to the right Computing device within maybe uh the door Network have a lot of loan device but we think we should rooting to the right and device like uh letters aggregation test which the four machines should negotiate Asian waste which will do the aggregation test before so maybe with synthesis the first question and another question we think that adult operation should tell the device what to do like understand like they are a lighter face and eyes and other maybe like some basic operation and the third is that we should have a structure data description and we should understand which that tells the house how to"
  },
  {
    "startTime": "01:32:01",
    "text": "package the Computing as the message okay that's all for the next step we will find a solution maybe about the data plan and we want to find a general way to send a Computing information in the networks and maybe we will consider more about how to make this uh don't worry uh relative work like they say uh light Data Center and the other and the whole password to join us thank you Central Central uh thank you very much for this interesting presentation so what do you intend to do about this work in this group do you want to um write a document you want to continue doing this uh strictly as as a research initiative that you report on uh to the group and that eventually you or something oh okay maybe I um now I maybe I will plan to have a drafts in the ITF because uh as so you want to do it in the you want to do it in the ietfs not in the I or GF uh yes yes because I think I want to find a solution to make some in-network Computing uh work research to work in some light DC Center so maybe I think I will try to work in the ietf but I hope some uh some members in the topology will join us because I think maybe it's a same topic well yeah but you know they the thing"
  },
  {
    "startTime": "01:34:00",
    "text": "will be for you to find the right the right working group and obviously this is not uh something that we do in this group but if you uh want to continue at least uh keeping us informed of the work uh that would be nice and uh of course uh we can you know you can uh do the work also in the ietf I hope others to join us because uh our work is just a start okay thank you very much about the name there's some discussion in the terminology in fact this terminology issue so uh you think you'll talk about uh you name it data operation in network but your three use cases seems that very Computing more Computing than data operation for example the uh compare and the cas where I compare and and as well yes and uh those are typical computational sometimes they are called Atomic computation in context of MPI or RDMA I forgot it but that's typical computation so that's that's uh more computation than data operation I was just saying that's a quick comment for the name I mean yes yes in my sauce and so don't consider is that the application is changed very fast and the weight can't deal I will communication Case by case is not it's not a good way to make our research to become to work in some data center so maybe I think"
  },
  {
    "startTime": "01:36:00",
    "text": "of strategies operation maybe a good way thank you so we have calling in queue yeah hi uh so just following up on uh Jeffrey's comment and with no hats um I I was a little surprised that the operations were such low level um given the the more high-level use cases um and it wasn't clear if this was a a um computation in the network signaling scheme or an active networking scheme that was being proposed and it'll be good to be clear on um the distinction on where this where this was going thank you oh hi can you hear me please uh yes uh go ahead uh thank you um I would like to provide yeah it seems there's some back here and right the data operation generally uh in my view is that the data is carried in the payload so we provide the data we provide an operations so it's we want the switch to operate on the data following the packet right so that's why this work is called Data operation the data is provided and we have some simple instructions and let the switch do the work uh and also the reason why this all the operations are very low level is because we we believe that in these scenarios uh keeping the task at line rate is very important uh therefore the combinations of various different operations or Atomic"
  },
  {
    "startTime": "01:38:02",
    "text": "computations can take a much longer time and slow down the whole forwarding soon okay and that's why it's very low level operations and that's awesome thank you yeah I understand that your point uh from another Computing point of view you always need some data tool for computing and those data are usually also conveyed by some pacts otherwise how we can do another Computing so it seems that don't see honesty is I don't see a lot of difference between this these use cases and in for in computer internet computing uh right well the Computing always have uh always needs some data well the difference is that you fetch data from somewhere else or the data comes with the instructions right Okay so maybe we can discuss this further offline yeah I was thinking maybe it's worth taking this offline or taking it to the list again because this is kind of new work it would be maybe interesting to to bring into the list um the next uh so thank you very much again uh Eugene and uh so Stefano is now going to present us um and other another machine learning for networking actually that seems to be a theme today and actually uh knowing what I do in the rest of my life I think it's a theme for many people uh so Stefano please I'll"
  },
  {
    "startTime": "01:40:02",
    "text": "I will grant you the screen okay I'm trying to share the screen [Music] yes you have access oh there we go okay so can you hear me can you hear me hello yes we can hear you okay thank you thank you so uh welcome everybody so I'll present this uh update on machine learning for networking use case for uh uh EIP extensible impact processing this is a work uh jointly done by me and some colleague of Mines at University of Bronto vergada then uh Swami at this Stanford University and Muhammad shabbats at poor University and bakuri um so in the previous presentation here according to Shara and uh Shabbat had presented the um sorry yeah I presented a solution for per packet machine learning inference using Taurus so in this uh work we will extend the the Taurus solution which is based on a single node to a distributed architecture in which we have a feature extraction separated from the process of machine learning the inference for this a distributed architecture we need to transmit a feature that are extracted by"
  },
  {
    "startTime": "01:42:00",
    "text": "one node and should be sent to another node and we want to do it very quickly very in the data plane because we want to keep the idea of having a per packet machine learning inference that we want for example to detect anomalies in a very small time time frame that window and then we [Music] thought to use the EAP mechanism to transmit this uh encoded the fissure from uh OneNote to to another so a very short entry slides I will just recall the the Taurus architecture for a machine for machine learning and there is that there are several application of machine learning in networking like a normal infection a traffic classification or congestive control you see and all these most of these application really are good if they are applied on a pocket propagate basis but usually it's it's very hard for the processing requirements to do machine learning inference on a packet by pocket base this is why the Taurus um solution has been has been proposed and in this Taurus solution it's a as Rich pipeline which includes a machine learning inference engine so the idea is it's kind of the extension of uh programmables which like tofin architecture in which you have you see the the normal pocket parsing the"
  },
  {
    "startTime": "01:44:00",
    "text": "processing of packets based on Match action tables and in addition in the tower switch pipeline there is a a machine learning inference engine that can take that take for example classification decision based on the features that are extracted by the previous stages of packet processing and what have been shown in the towers in a paper is that this can work at line rate so it is really possible to apply a classification at the library with this as Rich architecture so the in the paper they show a single node model in in which you have a single Reach For example which receives the packets at line rain these packets are parsed and pre-processed because usually you cannot run the inference only based on the information that is contained in the single packet you have for example to do something like collecting per floor fissures so you should count how many packets of the same flow have been received in the last 10 seconds or you you do this you need to do this type of processing that we call it the fissure extraction okay so in the single node model you prefer the node performs the fissure extraction using a traditional architecture like a Tofino basic architecture then these features are extracted and they are handed over to the machine learning inference engine which can perform underlying rate the machine learning inference"
  },
  {
    "startTime": "01:46:00",
    "text": "so in this uh work that I'm reported reporting we want to generalize this idea and we think that in a network context we can have this approach with a distributed feature extraction and machine learning inference and there can be several scenarios in which it can be useful to extract some feature in one node then encode the feature transmit them to another node that can perform the machine learning inference and a typical example can be you may have a data center in which you have an aggregations which and this again which is receiving traffic from thousands of servers and each server has maybe tens of virtual machine inside and it is not possible for this node to perform the fish restraction for all these Flows In Parallel because this requires a lot of State information to to do the extraction of fissure for each single flow so there are this scenario which is very useful to have for example or every single server to stretch feature then you you need to transmit the feature to to the to the switch and because in the switch you have the hardware that is capable of running the machine um the ml inference at line line speed okay so this is a a more General generic Vision that we will have some nodes that are capable of such extracting fissure they will transfer transmit this feature to other nodes that could be typically switches these switches are capable of running machine learning but they are also capable of"
  },
  {
    "startTime": "01:48:00",
    "text": "again a structuring feature so there could be a two um two layers two levels let's say of a extraction feature before um using the machine learning inference or even this uh feature can be evaluated by one node and sent to another node for for farther uh processes so this is a rather General uh model that we are proposing so now we there is the problem how we can encode and transmit these extracted features from one node to another and now the solution that I have presented in a previous presentation which is called an extensible inbound processing comes into play so I already presented that with the extensible inbound processing we want to um put information in the IPv6 header using the for example Hopper option and this is a generic container for several use cases that I have mentioned in my previous presentation so this uh encoding of a feature for machine learning can be just seen as a new use case for the proposed EIP mechanism and in particular we I will we are defining um a record that we call the encoded fissure representation or efr record okay that can be transmitted as an information element inside of this uh proposed EIP option so basically we have a framework which is generic it is extensible and is used to transmit the information in the data plane among different nodes and in this case we are exercising this genetic"
  },
  {
    "startTime": "01:50:00",
    "text": "mechanism for this new use case which is the transmission of the feature for a distributed machine learning solution uh then we are making some some consideration that uh the encoding of this feature needs to be a very very efficient because we are putting we are adding information in the data plane so um we we prefer to to to have a representation uh in a record which is just a plain array of bytes with no explicit tagging of the features and so of course we need to find an agreement between the sender and the receiver of the information to specify uh what is uh the content of the features that have been transmitted because there can be different uh um applications uh different anomaly detection scenarios and so the set of features that could be transmitted that can can change and so we are defining this uh this simple solution that there will be some identifiers agreed between the sender and the receiver and this identifier will specify which is the structure of the the the record that is the transmit so for example if you have a given identifier then we are transmitting this feature number of packets per flow number of bytes and the the for each of this feature of course we know what is uh the length of the um of the information that is uh that is uh encoded in this in this in this record so we we can uh discuss how to propose a standard or rather a framework so we don't need really to to to to go uh with"
  },
  {
    "startTime": "01:52:01",
    "text": "with a standard in the in the complete meaning the point is that okay we want a kind of lightweight standardization we believe that we do not need to standardize the content of this record so the different application will be free to choose which feature needs to be transmitted so we want to leave The Innovation open of course in this exchanging of feature but maybe it can be useful to to standardize to use a common framework to exchange this uh efr Records among the nodes and in particular we think that this could be a good use case for the EIP option to Define any information element that basically just includes an identifier of the of the record and the record is set with with the bytes of the features that have been extracted okay so this is a very early this is an update and it just includes very early ideas and this the the idea of Distributing the fissure extraction and the Machine learning inference open UPS uh several research issues so we are just starting this this activity and so we we welcome we welcome comments and discussion on this uh on these ideas that's a that's it and I'm open for four questions thank you for for your attention um yeah so um are there any other questions so what do"
  },
  {
    "startTime": "01:54:00",
    "text": "you want to do with this um with this work it's already we missed you know you want to continue doing this work uh the outside corn inside coin what you want to do and Jeffrey you're next so yes I I like to um we are now uh working um on uh a position paper describing these things and then I will extend the um EIP um draft we have some draft of EIP that I will include the description of this use case and I'll be pleased to to submit to this uh updates on the when they are ready on on the mailing list of coin and receive uh receive uh comments and I I think that this is a an uh an example of uh of the future restriction in particular is is an example of a protesting that is done by inside the network by the network node to extract this feature so I believe it's relevant to the activity of this uh of this group but first I'm open to to to comments and then about it yeah okay uh Jeffrey yes so first this is I think that this is a quite interesting topic because stamina is talking about how AI can be used for Network all right so previous several uh plantations or talks are discussing how Network for AI but I have a question for you Steph no so you met you try to separate the feature exchanging and the Machine learning influence yes understanding about the Deep learning is that usually"
  },
  {
    "startTime": "01:56:00",
    "text": "they integrate the representation learning inside so those feature extension are some hidden States inside the machine learning model so you have some uh special specific application especially specific use case or Solutions so that we can separate each exchanging yeah this is a very very very interesting question yeah I I think that the use case as a there's a specific characteristics that may be made uh different from a traditional machine learning because maybe in traditional machine learning as you say you have a full data set and you want to automatically discover the feature as so you do not want to pray choose your feature but we have the the impression that this this uh will not scale uh that it's very difficult that it just based at the on the raw data and the road that are just the packets that are flowing in a node then you are able in a scalable way to do a good machine learning inference and the the existing uh solution for machine learning for networking they are already based for example on on floor level features they are not the analysis fighting the single packet so I think that there is already this trend that you need to extract the feature before you can run a a machine learning inference but I agree it's a very interesting uh question and it's a very interesting issue because uh if you choose the wrong feature then"
  },
  {
    "startTime": "01:58:00",
    "text": "you lose the possibility of doing a good algorithm so I I agree that is a a an open uh research issue it's one of the open research issues that I mentioned at the end of the presentation okay maybe we can see we can try some specific analysis on some use cases and see how we can do that separate feature exchanging and the Machine learning influence thank you I will declare that we're on time and um and that we can most essentially close uh the meeting um you probably have seen that we requested a slot in London um I don't know who is going to be there in person I know that myself won't be there but uh we'll see um you know hopefully people will be able to meet in person I will still be um virtual for a while uh and so uh we will produce some notes I took notes on the side that will coordinate with but Jeff about how we do that and thank you very much to all the presenters I thank you especially to people in Jeffrey's time zone where it's currently extremely late uh thank you so very much everybody I really appreciate it Jeffrey appreciate it and again Eve sends her best regards and she's also she couldn't join us today but hopefully she'll be there at least virtually in London thank you very much thanks Jeff"
  },
  {
    "startTime": "02:00:00",
    "text": "hi everybody"
  }
]
