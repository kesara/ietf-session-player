[
  {
    "startTime": "00:00:21",
    "text": "No so it\u0027s 350 is so we\u0027re gonna start pretty soon we\u0027ve got a very full agenda and I\u0027m making loud starting noises to the people in the hall can hear that and think about coming in I know Dewey goes in the door that\u0027s a good move actually that\u0027ll that\u0027ll make him nervous we\u0027re looking for a minute taker minute a taker Paul Hoffman you\u0027re an upstanding citizen in the ITF then you\u0027ll write very clear minutes because you\u0027ll force us to clarify ourselves I think thank you Paul you are a friend your gentlemen all right welcome to session one of a cheap abyss of two sessions we\u0027ve done this week this is the note well is Tuesday afternoon you\u0027ve probably noted well by now these are the terms that govern your participation I PR wise in the IETF if you do not understand them what your chairs and your 80s are more than happy to have a conversation to help you understand so we have a scribe thank you I\u0027ve got two sets of blue sheets that mark will distribute around the room and we will open the agenda for for boshane my understanding that someone would like to reorder two of the presentations and if that\u0027s probably something we can accommodate but this is agenda for today we\u0027re going to start by going through each of our active drafts that the working group has adopted that isn\u0027t at least off to the ISG and then we\u0027ll talk about some proposed work and we have a couple informational presentations after that regarding how push which was part of these Chiefs to 7540 standardization is playing in the real world would anyone like to make any changes to the agenda yeah Chris would you\u0027re right if we swap the key Lehman s and I held service drafts that the second one happens to conflict with the start of CFR G which is rather unfortunate and I would like to attend bolt so it took me with the presenters and everyone here is that okay yeah sorry I double-check this with the authors and the presenters and they\u0027re all cool with that for both drafts but does anyone else have an objection see no objection that\u0027s what we\u0027ll do well thank you I was wondering if we could move also CDN loop - I\u0027m also conflicted with CFR G so put helium as the third for the proposed work if that\u0027s all right are there any objections to that that\u0027s also fine by me I think we\u0027re all here so I think the constraints were working with here is that we don\u0027t have a current until three o\u0027clock we have a five whatever point five o\u0027clock whatever and then we have people who disappearing shortly thereafter is this a double session is there is that what\u0027s going on this I don\u0027t know the answer to that I think they might be another "
  },
  {
    "startTime": "00:03:21",
    "text": "session somewhere else this thing\u0027s split in two okay there\u0027s some overlap so let\u0027s just let\u0027s keep wets do all services and I then helium then CD and loop as the as plan of record for now but it\u0027s possible we\u0027ll be able to move through some of the active extensions like like quite quickly some of them have very short reports to go on so I suggest we get started on that and see how we\u0027re doing right all right it\u0027s not good okay I know that\u0027s the first time someone bashed in agenda I was very excited and now I kind of live in fear you understand punky yeah all right so we\u0027re gonna start with uh variance get your slides up here for you okay so the first draft is HSV representation variants draft IETF HP is variants we adopted this was it two meetings ago it\u0027s one day so yeah yeah something like that and it\u0027s been coming along pretty well next slide um I think it feels like it\u0027s getting close to done or at least ready for some sort of feedback slash implementation loop and I think that\u0027s the big question in front of this draft is you know we have I\u0027ve been talking to folks we have some in our interest we don\u0027t have any actual implementation yet and so the question at hand is do we want to go ahead and publish this you know once we resolve our issues and we go through a review process which is not we\u0027re not quite there yet but we can see there from here do we want to try and wait for implementations or do we want to just go ahead and publish it maybes experimental or something else and see how it goes yeah so I since since I wrote these slides I\u0027ve been talking to uh some folks first of all ask the question is does anyone in the room intend to implement this in the near future okay so I have a hand up I knew about that hand that\u0027s why I asked the question you never ask a question you don\u0027t know the answer to so I think what we\u0027re gonna do is we\u0027re gonna work through the remaining issues lathe has said that he thinks this is something he can implement as a plug in in in traffic server with Brian\u0027s help perhaps uh and and over a very reasonable amount of time I\u0027m not gonna talk about the time scale that you gave because I don\u0027t want to make a commitment for you but a reasonable time scale and then will based on that feedback and maybe if other folks get interested then we\u0027ll take a look at where we\u0027re at after that if that makes sense okay great and then and that\u0027s of course in the cash side and cash implementation the other half of the implementation here is generating the headers so that the cash can take advantage of them I\u0027m not as worried about that because that is is if you\u0027ve implemented very generation based on a file system for example this is a very small step from from that it\u0027s it\u0027s not a not a big deal and something like Apache next slide so I think there\u0027s just one issue that I wanted to talk about today which is the more expressive variant key issue where "
  },
  {
    "startTime": "00:06:22",
    "text": "whereby we had some discussion that that right now the formulation of the variant variants header yeah the the formulation of the variant key header is such that you list the available options for each a component so for example if there\u0027s a DPR and you can have a possible value of one or two then variant key says well this particular representation has one and there\u0027s another and and and then it has another way that says I can I can also express this key for that we\u0027ve been having a discussion about re expressing the variant key in a different format so it\u0027s basically a set of alternative variant keys for this representation that for the response that the variant key header occurs in and that I think the feedback we got on the list just actually yesterday from Yoav was that he really liked that because it\u0027s more expressive it allows you to to avoid situations where you know if there\u0027s a matrix of four different options that the variance builds and three of them can be satisfied by one response this gives you the ability to say okay well this response can be used to fill those three holes and that isn\u0027t currently possible in the current spec so we talked about that I feel better about that the only issue that came up I think is that gave life a little bit of heartburn and from an implementation complexity standpoint but we talked through that the other day and I think that we can address that by writing the specs such that only the the first variant key in the variant key header because it\u0027s a list has to identify the representation that it occurs within and that way if he wants to he can just chop that off and use that loosen cache efficiency but he doesn\u0027t have to fill all the other holes in that matrix in his cache by creating coming entries or whatever you want to do and from his standpoint that\u0027s a much easier thing to implement but it gives him a path to later on address the other support there\u0027s other values so that you get better cache efficiency instead about right life okay um and and so I think this doesn\u0027t need much discussion I just wanted to check to make sure that nobody else had heartburn with that sorry it looks like what you see here on the bottom very key so that says basically this would if we adopt this that variant key semantics would be this particular representation that these two headers occur in identify the the the the response with the values to 300 and off for dpr viewport and save data respectively but this response can also be used for the one 600 off and to 600 on responses and and the use case driving this was yo talks about this a lot you know when you have interdependent headers for things like "
  },
  {
    "startTime": "00:09:22",
    "text": "especially you know dpr viewport and save data are good examples often you\u0027ll have different combinations of those that could be satisfied by the same low-quality image for example or high-quality image you don\u0027t have all those duplicates in your cache you have a way to express hey you\u0027ve got one entry and cache could be used for all these different combinations of these headers is there another slide you\u0027re out of slides so i will go back to your first slide for publish now our wait for our experience and note that this is a standard stret document right and one cache has provided you know a lot of cooperation right but it has an implement again i think i can give you another cache in the horribly distant future excellent I was gonna point out I\u0027ve made some new friends from Comcast this week running one of these caches who\u0027s implementing I\u0027m wondering if they would be willing to suggest an intent to experiment with how this works if that implementation were made and you provide some feedback yes there is intent to experiment not a lot of experience yet though sure doesn\u0027t exist that\u0027s all we\u0027re asking for now that would be great things so my suggestion we would essentially Park this waiting for some implementation experiment yes and then that that\u0027s I\u0027m happy to leave it open for almost an indefinite amount of time as long as we actually have people doing things with it if we don\u0027t have people doing things with it we might as well just dispose of it in some other fashion so great ok thank you Thanks so you\u0027re still up there mark because we\u0027re gonna talk about BCP 56 bits next hi you don\u0027t have slides so I\u0027ll find a nice little interstitial here for us yeah that\u0027ll be good so um we\u0027ve been working on BCP 56 this for a while now we\u0027ve gone through role cycles of editorial additions and reviews I I feel like we\u0027ve gotten fairly broad review of it both from folks in the working group as well as folks outside I regularly and including yesterday get emails from people in the ITF that I\u0027ve never met before as far as I\u0027m aware but that are giving me very nice feedback on it saying that we like this this is very helpful have you thought about this or can you change this to make it more clear or you know what do you think about this and and so I feel like it\u0027s it\u0027s a lot of eyeballs have seen it and and the rate of change on the document has been slowing down so I\u0027m gaining confidence personally that it\u0027s it\u0027s getting to a stage where it\u0027s ready ish to ship I think the big open question on that right now is I have an editorial issue open on it to wait for hjp core so we can reference those nice new very well-structured core documents in this rather than referencing the seven to 3x series we\u0027ll talk about core tomorrow but I think the feeling is still that we\u0027re gonna finish core around the end of the year so I\u0027m personally as editor happy to leave it open for a while and maybe incorporate some more feedback as it dribbles in and get even wider review for this document which is always a good thing but I\u0027d be "
  },
  {
    "startTime": "00:12:23",
    "text": "curious to hear what other people think about whether there\u0027s an urgency and shipping this document or not some modern Thompson I think having it is more important than publishing it at this point and it will be really nice if this went out alongside those core specs because I think that really sends a message that this is this is part of the core and in a way the way he used the protocol is almost as important there\u0027s a definition of protocol itself I think and so my preference would be to sort of try to try to hold it until then as long as that process doesn\u0027t prolong itself and it there are signs that there\u0027s lots of changes coming in the core thing but they\u0027re all really minor things and I think we could probably say that the core thing is going to be done soon helps at least at least not not three years away if over a year I think I\u0027d be comfortable with the year I think any longer than that and I think we should started there feels that right Denis - yeah thanks who in the room has read the document BCP 56 bits okay smattering of hints please read it yeah so this is intended as a BCP right so we we want wide review in the room but as as positive data points there have been about 60 issues open to concern on github almost all of which have been resolved from a pretty wide range of parties including people that don\u0027t participate regularly in this group so it\u0027s getting that kind of review he recently added a privacy consideration section I want to make sure people look at that my experience with talking to people working in other groups there was recently interest in this from from the SOG group and the DOE group was also very interested in this right is it would be nice to have something not just to guide their definitions of their protocols but actually to be able to normally reference to say you know the security implications or the privacy implications of the HTTP portions of my spec are reflected in VCB 56 bits right so if folks you don\u0027t want to read it with an eye towards doesn\u0027t answer that question I would be interested in that mark I\u0027m relaying for Julian here he says we have a few core issues where we still discuss whether it should go into core or basically 56 bits I think that that\u0027s good and this is so he agrees with me on that point so that\u0027s what Paul Hoffman so the sag message that you saw you may have misread basically he will he was told puts BCP 56 stuff into a document that is blatantly breaking BCP 56 and it says so it says screw how they did it we just wanted to shove our data over HTTP "
  },
  {
    "startTime": "00:15:23",
    "text": "oh but there\u0027s the Stockman over there that tells us not to do it so so that\u0027s an argument for the completeness and the urgency of publishing perhaps well at least now I mean that document will go forwards um it\u0027s been waiting for almost a decade but basically it\u0027s specifying something that predates possibly even predates BCP 56 and it\u0027s a fuller specification of it and it\u0027s someone who could care less he uh he\u0027s over in your continent area so you could chat with him it\u0027s um continent area yes but as you know we know we all know each other yes right so anyways but but just to be clear that document is referencing BC P 56 saying yeah screw that so speaking to the privacy considerations part and well then and that general question I think that writing that kind of text to have it to be able to reference somewhere is a great idea and we should be doing that personally my initial feeling is is that that probably belongs in core the idea behind BC P 56 bits is it\u0027s a guide for people who are writing new specifications or applications using HTTP and it\u0027s also a guide for people reviewing them especially area directors so you know when that screw that specification gets to the isg they can compare and make their own decision it\u0027s not there\u0027s I don\u0027t think there\u0027s currently anything in there that\u0027s something I could see an application referencing unless it\u0027s oh we did do this and we\u0027re pointing to it informational II maybe you know or we didn\u0027t do this and we\u0027re pointing to an informational II but it\u0027s not really a referenceable document in that sense I just like the original BBC P 56 beside on\u0027t know anybody really referencing that one either but I\u0027m not stuck on that I mean if we want to change the nature of the document that\u0027s fine I just to me it feels like it kind of goes in core cause you\u0027re gonna be referencing HTTP anyway if you\u0027re using it there any other thoughts okay thanks thanks mark secondary certificates this next Martin micronet who\u0027s gonna do the honors here we have a clicker Mike I don\u0027t know if it works but you know alright so as far as the secondary search Draft I\u0027m gonna go with no it\u0027s dumb it does not work there you go alright so there\u0027s actually been a fairly broad set of changes in since London so if you go up to the next one for the first part in draft 0/0 they were sending the frames that were "
  },
  {
    "startTime": "00:18:23",
    "text": "related to the certificate and to the requests on the control stream but the things that we\u0027re describing for this particular stream I need this search and now I can proceed because this request has been answered those were happening on stream there\u0027s a slight problem with that flow though that is not fatal in its gp2 but will be fatal when we take the same thing dhcp over quick which is that the request with the headers likely close to that stream in that direction so requiring that you then later send a frame on the closed stream not so not so nice John draft one everything has been moved to control stream it says it includes the stream number of what it\u0027s referencing but unfortunate that we have to mention it but it makes the stream usage cleaner so next slide we\u0027re also being more explicit about where certificates are used originally there was a flag on a certificate that was marked as automatic use so that the client could say I\u0027m fine with you using the certificate for all future requests that I make and servers have to set automatic use because clients are deciding whether to send requests in the first place now there\u0027s an issue and that the server doesn\u0027t actually care whether the certificate is available but the client might the client doesn\u0027t know when the certificate was used or not and so we\u0027ve turned this around so that there\u0027s the ability to send an unsolicited use certificate and if you want to apply to your certificate to every request that\u0027s one more couple byte frame that you can send along with every request but if you have multiple certificates you can pick which certificate to associate with each request and now also it\u0027s not confusing if the certificate the server turns around and asks you for a different certificate after it\u0027s seen your unsolicited one next slide also in the camp of trying to be more explicit instead of having clients pick an unused stream and say I can\u0027t use the stream until you send me the certificate that I want which is kind of hokey I have to admit instead we just say certificate needed on stream zero this this connection requires that you provide me the certificate which is a little bit cleaner I would still like a cleaner cleaner still way to do this but this is the best we\u0027ve got right now next slide most of the big changes in the dock though have been through integration with the exported authenticators draft in EOS working group so the original version of the drafts that we adopted and that we still had I think back in London was that "
  },
  {
    "startTime": "00:21:24",
    "text": "everything was being done in the HDPE layer the certificate frame curing actual certificates then you had a certificate proof that contained a signature of something of an exporter by that certificate the ship request had oil filters and having all of this at the HCC layer was kind of ugly so moving the draft one there\u0027s no more certificate proof frame certificate curious and exported Authenticator TLS deal with all of that certificate request carries an exported Authenticator request all of that Singh capsulated that TLS layer gay sheep HTTP out of it please draft to we got to take advantage of a new feature and export of authenticators that was added during their first working group last call which is and exported basically an authenticated refusal the ability to send I do not wish to provide you the certificate you\u0027ve asked for previously used certificate just didn\u0027t include a certificate ID now you actually send along cryptographically signed I am refusing signed and I\u0027m refusing to send youth which you\u0027ve requested so we\u0027re shifting more stuff into the TLS layer doing less at the HTTP layer and this is goodness for separation of concerns last major change that we\u0027ve had so far is a way to detect me in the middle sooner because if you do have a TLS man in the middle on your connection unfortunately they do still exist under the old model you wouldn\u0027t discover that until you\u0027ve done all the work of generating an export of Authenticator and then it didn\u0027t validate now we actually stick an exporter in the settings instead of just saying one turns it on and if you see that the exporters don\u0027t match from your setting you know it\u0027s not going to work don\u0027t bother attempting to done to do the crypto to generate an exported authentic our next slide so open issues that we want to talk about first off there\u0027s the question of how all these frame types get bound together and the draft is improved at explaining this but it\u0027s still kind of a little network here next slide the certificate request has a request ID certificate needed because there can be multiple of those for various streams point to a certificate requests by request Eddie you answer certificate needed with a use certificate that\u0027s referring to the same stream ID saying which certificate you\u0027re using to enter the request for that street and then that uses a serve ID to tie it to a certificate frame that has an export an authenticator now the interesting piece is inside the exported Authenticator you "
  },
  {
    "startTime": "00:24:24",
    "text": "have the request ID that that certificate is bound to but it doesn\u0027t have to be the same request ID that was used for that street in the scenario where you would want to do that is you\u0027ve gotten separate requests for dub dub dub example.com image example.com and scripts example.com well likely you have one cert that covers all of those names and it would be nice not to have to generate three separate export authenticators for the same certificate all responding the same request none too different request IDs so we allow you to say I have already sent you this certificate and use it to satisfy that request so right now the draft says that you need to dig into the exported Authenticator with a POS that are defined in that spec retrieve the request ID and ensure that it matches some requests that you have sent previously but not necessarily the one on that street so next slide admittedly that doesn\u0027t line up perfectly but it\u0027s operationally a better choice so there is kind of a question of do we want to allow cross responses like that I mostly think that we do but it\u0027s not entirely clean and then there is an open issue suggesting that the certificate frame should explicitly contain the request ID so you don\u0027t have to ask TLS to get it out for you I\u0027m open to opinions on that I\u0027m inclined to think that if it\u0027s already there and it\u0027s just a matter of asking TLS to read it that\u0027s probably okay but it does make a dependency on the TLS library to interpret them which like comments now or - comments now Wow okay Naughton Thompson I\u0027ve always been a little bit sort of unhappy with the situation where we sort of ask a question and I mean indirectly of course we we have to go asking TLS to give us the answer to the question that we should be able to answer directly ourselves if that means that you\u0027re coding the information I think I can be comfortable with that I like the I like the separation between request identifier x\u0027 and certificate identifies I think that\u0027s a useful abstraction that we can use and particularly useful in in these cases where you get the CSS top example calm images so example.com what what have you all of that stuff is is goodness particularly when we\u0027re talking about something that\u0027s so grossly inefficient as certificates but not being able to know which request was being answered without going all the way into the opaque blob that we just got "
  },
  {
    "startTime": "00:27:25",
    "text": "out of TLS is quite annoying so I\u0027m inclined to say just duplicate it I will push back on one piece that you said there at the end yeah which is the request that is being answered for in terms of this stream is the request that was sent for the stream but that\u0027s that\u0027s not necessarily the same as the request that generated this export of authenticator in the first place that\u0027s that\u0027s the one that I\u0027m more concerned about the this the the carriage of the blobs if you go back to the picture the ones at the bottom I don\u0027t care about the ones that it\u0027s the ones at the top I mean it\u0027s yes it\u0027s all in directed throughout through the request on the stream and all that business but fundamentally you have to be able to there\u0027s some magic that goes on so you have multiple certificate requests outstanding at the same time mm-hmm the HTTP layer doesn\u0027t know which one goes with which until it\u0027s gone on talk to the tail I smile which I don\u0027t like I never I think doesn\u0027t care okay I was gonna say we can write the third co-author up and just sort of have an editorial meeting here yeah I would like to hear from other people on this one cuz right it\u0027s not a huge problem I mean frankly this isn\u0027t my presentation because Martin opened the issue I don\u0027t agree with him I would like someone to talk with Ben Schwartz is it possible to send a certificate frame that just contains a reference basically to make an explicit reference that\u0027s essentially what a use certificate frame is so you send a certificate frame that has the export of authenticator once and then on every stream that on which you want to use it you send use certificate with just the cert ID but then you follow the cert ID just the exported Authenticator and if you want you can find the request ID of the request that triggered that export of authentic error to be sent sure so could you go back a slide or Thanks so you certificate doesn\u0027t have a request ID in it correct so I guess the question would be you know could you have a frame it\u0027s like certificate and like request ID and also a pointer like here\u0027s the expert authenticators over there so effectively explicitly answering the request to say like yes I\u0027m answering your request the answer is over there okay so far the document has done that purely by "
  },
  {
    "startTime": "00:30:26",
    "text": "matching them up on streams but potentially we could add a fifth certificate type that says yeah I\u0027ll have to come up with a name but basically responds to a request with an existing certificate but not bound to a stream any other comments here or should me move well at least I don\u0027t having two things that needs to be compared by get it to be equal yes is something that I\u0027d rather not fall to have that\u0027s a good point that if we put the request ID and the certificate frame then you have you still have to check in the export of Authenticator what the request ID is to make sure that it\u0027s equal to the one in the HTTP frame so you don\u0027t actually save the : to TLS new sullivan so yeah you\u0027re saving a search essentially you\u0027re saving the search through all the outstanding certificate request to find something that matches the request ID that\u0027s that\u0027s why when they duplicated right are you because you you have to ask TLS to open the exported Authenticator to give you the request ID or you get the request ID out of the search for him but either way you have to go find that request right but if you have the request ID in this certificate frame then you have to also look in the export or authenticate or to but check the binding no Mon Thompson the something occurred to me but this picture is really good for that we have the problem that Kazuo identified on the left because the certificate request has the request ID in two different places you want to fix that pick one I don\u0027t actually here either you have to in both or you have one in both and I\u0027m okay with it either one but it would be good to be consistent if that\u0027s the case - and both sounds fine to me all right it\u0027s about last comment yeah so unrelated question on this particular issue so we\u0027re done discussing that so I wanted to kind of get an idea of what the scope of the certificate exported sorry certificate authentication is for instance is it only for things are applicable on this connection or for example does it extend the scope beyond this connection yourself for example can we apply to this to all des BC which may presumably "
  },
  {
    "startTime": "00:33:28",
    "text": "cash beyond the scope of this one connection so in the current the current version of export of authenticators it is only for this connection there is a proposal on TLS that would allow you potentially maybe to hook it into a resumption but that\u0027s not true for the current draft yeah it would be just make it explicit would be good thank you you had seven minutes left Mike yep so then there\u0027s the the issue that came up during our I thought it was going to be brief discussion last time that is actually kind of kind of hearing so next slide the problem is that you can use secondary certificates to make various other forms of attack on a cert either easier or worse so without secondary certs if you get a Miss issued certificate and let\u0027s say Mallory is trying to steal traffic from Bob calm Mallory has to get you to navigate to his site and get a Miss issued certificate that covers both his site that you own that he induced you to connect to and the attack site and if he does that then if you\u0027re requiring certificate transparency you have a record of that cert being issued and that cert covered both domains and you have at least a breadcrumb with who to go after as well as being able to revoke it in a secondary cert world those can be separate certs there\u0027s no certificate transparency link between the two these hacker still has to get induced navigation and you can still revoke the cert when you see it in certificate transparency but you don\u0027t have the breadcrumb to try and figure out who did that now how strong a breadcrumb is that going to be considering it was probably a throwaway domain anyway probably not very but it\u0027s something where this gets a little bit hairier next slide is in the case of a key compromise where they actually have the legitimate cert and the actual private key of that sir in the current world you either have to do takeover DNS or subvert IP routing so that you can steal the connection that was going to get routed to the real server and then present your fake cert and the target of the attack is blissfully unaware but there is at least some work involved in student land hijacking a TCP connection with secondary certs you\u0027re back to induced navigation which we know is easy and then you present the real certificate as a secondary cert on that connection and "
  },
  {
    "startTime": "00:36:28",
    "text": "the attacker is no less aware than they were before because they were blissfully ignorant before but it becomes easier to do so there is a proposal last time where perhaps instead of permitting normal certificates because we should have a new requirement that certificates used by surfers this probably doesn\u0027t need to be done for client certs butt certificates offered by servers might want to have some kind of void that says this is okay which kind of makes me sad from the deployment perspective because it means all existing certs and all existing ways of issuing assert that you have setup are not usable with this and that\u0027s going to slow adoption and also for in terms of an opt-in we already of the quote-unquote owners of the primary connection they want to be able to control whether secondary search are used on their connections and now we\u0027re also going to have to have opt in from the ones being coalesced onto those connections we know that opt-ins are hard to get uptake on double opt-ins exponentially more so and just from the optics perspective if we have an oil that says it\u0027s okay if you hijacked my circuit nobody\u0027s going to do that so we need to at least have some somewhat useful looking mechanism associated with it if we do this I don\u0027t think a blanket in or out is necessarily the best choice so next slide I got an interesting suggestion during a side conversation on this that instead it might want to be a list of primary domains that this can be secondary to so if you do steal the search from images macys.com fine but it can only be used secondary to dub dub dub dub macys.com so unless you also have that cert who cares or at least you know maybe not who cares but you are no worse because you can\u0027t use secondary certificates with it unless you have that primary service well honestly I\u0027m not wild about having this requirement at all but I can understand the security argument that we need to do something to mitigate a making attacks easier and if we need to do something this is the best suggestion I\u0027ve heard so I was like comments on whether we do this or none of it Nick Sullivan this seems like it\u0027s even stronger of an opt-in because you have to opt in to a list rather than opting into just one bit which is yes or no you can opt into a well car but that\u0027s still you have to accept a so wildcard means anything yeah so this is just adding this is reducing "
  },
  {
    "startTime": "00:39:30",
    "text": "the scope of the opt-in optionally it\u0027s more complicated or I guess more complex mechanism for limiting exposure to this attack and I think there is some value and if you are confident that the security on that this cert is probably not going to get stolen maybe it\u0027s in the hardware security module and there really is no way to get the private key out then fun dual wildcard throw it in go ahead and use it if you think the certificate is at risk you can tighten the scope and reduce your exposure Ben then Schwartz as an intermediate between these things could you imagine a tag that essentially creates a pool so like all all certificates on a connection must share a tag but we don\u0027t have to nominate exactly one of them to be the primary okay because otherwise you know I imagine in a CDN type situation it would be challenging to to know in advance which certificate is going to be the primary but you could you could mark everything in the pool with a shared tag yeah that\u0027s an interesting one so you just know you put an identifier that\u0027s probably going to be the CDN you expect to use or maybe your customer ID at that CDN and then they all can be secondary to each other that\u0027s simpler I like that yeah I was gonna say basically this is Richard Barnes I was gonna say this the same thing has been that the tagging so that the entry point domain seems really awkward for things like CD units and something like this tagging thing seems way more sensible I wonder though it seems like at that point you\u0027re kind of hinging some of your security on the difficulty of adding that tag inside I wonder if there\u0027s something to hit hook to like something that\u0027s access controlled for that tag like say an acme account ID or some other thing that\u0027s difficult to get that might be external or this specification it might be you know the rules that say a browser forum puts for what you put in that field but you know might be worth thinking about what the security properties are depending on how difficult it is to get that that binding identifier yeah cuz the downside with the tag is that the attacker would have to the attacker could issue a cert with the same tag if they\u0027re able to inquire it and then yeah an attacker can execute the attack if they can obtain a certificate that has the proper tag right so different suggestions so super banger so one of the options might be actually to hash the straw man here but hash the certificate itself contents itself of the primary certificate so that means that not only binds it to "
  },
  {
    "startTime": "00:42:31",
    "text": "ownership of the domain itself but also ownership of the private key associated with the domain the public key of the domain so that means like you have to not only get a certificate associated with with that hostname or tag or whatever opaque identifier but you also have to own a specific private key to be able to execute an attack on this particular but that seems like an even tighter restrictions it\u0027s a dire restriction yes so that we\u0027re going with typers if we\u0027re doing some restriction and we\u0027re doing it like pretty tight then that would mean that presumably with property we want here is that a server like when you revoke the certificate that you have and the primary certificate would also or you like destroy the key or something is gone or that certificate is not longer usable the authentication properties are the things that are secondary to it are also not usable as well so it seems logical to pin it all to that primary not just a a domain which is opaquely satisfiable by any private key but I think the property that Ben was suggesting here was that it\u0027s difficult to know which which of several certificates will be the first one to trigger the connection and therefore you want to be able to use any of them as the primary and the others are secondary and if you\u0027re tying it to a particular private key we don\u0027t have that problem right so in the you in this one you have an Auror case right it\u0027s a list any one of them can be satisfied so you can create a graph of authentication with an aware as well as similar to how do you do it but the hash you do the list as well but you cannot it\u0027s kind of do you have to have a root and after that you can have leads but it does restrict you in that way you have some sort of route yeah it\u0027s not exactly last call for getting into my client - cut electrets so rather than pinning to a specific tag we could say into a sir a specific CP log that all certificates in a particular set of shared certificates have to be in so as I\u0027m thinking about this more I think the property we need here is that whatever goes in that extension needs to be something that a CA can verify in order to prevent issuance of certificates having improper values there which kind of leads points toward you know using a domain name there because that\u0027s what\u0027s the A\u0027s are built to verify and I think that may actually be good enough so so use a domain name as this tat in the same way same tagging way that that been mentioned so that you know when I go to CloudFlare I get CloudFlare calm and all the certs because they have and I think "
  },
  {
    "startTime": "00:45:33",
    "text": "that the rule you would say is CA has to verify before CA issues assert with this extension it has to verify in addition to whatever it would normally do that the applicant controls the domain and they\u0027re putting in the tag field in the tag extension so you\u0027d be getting a certificate that says this the the server owns customer calm as well as cbn.com and then you\u0027d use the CDN com1 through through everything else I think that seems implementable I think it gets the right security properties and the security properties you\u0027re after here I see a lot seeing a number of nodding heads out there so that sounds workable to me I will admit that in terms of doing certain extensions I have no experience there so I would welcome contributions of text it turns out one of Watson crypts engineers is in the acne session are the acne rooms all hook you guys up later as I fill up right you should be able to help with that a little ad hoc design team to kind of refine this design might be good yep all right and you\u0027re out of slides okay all right Thank You Man thank you for the useful discussion everyone I think we\u0027re making some good progress and yeah we H it\u0027s not plugged into the laptop I think that\u0027s probably another problem okay next up structured headers so structured headers um we\u0027ve been working on this for a while now we have I think we talked about last time an implementation in JavaScript a partial implementation in Chrome because web packaging is adopted web structure that history some of their stuff we have a partial test suite going contributions are very welcome I think overall it feels like the spec is getting mature we probably need to go a couple more rounds with it but it feels like we\u0027re getting close to something that we would want to ship and and the big question in my mind is whether we\u0027ve hit the right balance between you know simplicity and capability whether we have the right set of data structures start data types and structures to cover the interesting use cases yeah so next slide and in that spirit there are a bunch of open issues right now most I think we can address on the issues list but there are two along the ones of that there are two along those lines that are thought of you sort of talk about here one is is that we\u0027ve received some feedback from potential header users of structured headers that it\u0027d be useful to have an ordered dictionary so a equals B C equals D but where you can you know access that as a dictionary is as a hash but also retain the order for that and and whether that\u0027s important enough to get into the spec or not is is a question of judgment I do notice that "
  },
  {
    "startTime": "00:48:35",
    "text": "the newest version of Python makes order dictionaries the default at least for C Python which was kind of interesting coincidence so you know we could we could take a couple different approaches to this we could require the exist exist in dictionary object to the order so a language like Python would be able to just use its normal dictionary data structure our language that doesn\u0027t support order dictionaries you\u0027d have to do it with an object somehow a little more abstraction or we can create a separate order dictionary or ordered parameter top-level type in addition to dictionary personally I\u0027m I\u0027m a little reluctant to add top level types until we\u0027re sure we need them or we could say no we\u0027re not going to cater to this directly and you can Jam whatever you want to into the existing data structures for example you can have a really ugly per M list where the the identifiers you know like if the premise is basically like a list of mime types you have a thing and then you have an optional set of parameters on it you could say okay thing a is just you know a marker then you put the real parameter after it then B is the marker so you retain your ordering but you have this key value pairs it\u0027s it\u0027s kind of ugly but it could work with the existing spec so it\u0027s wondered if anybody had any opinions about that not in Thompson these things are naturally ordered I mean we put them on the wire and it\u0027s a very specific order with cancer I think this is I think this is the sort of the default that we want to have but it would be fine for individual definitions for header fields to say they\u0027re using this structure but attribute no semantics to the order in which the the items appear I think that\u0027s probably the right way to do this I tend to agree and I think what we would do is house and maybe we needed new appendix which says advice for API is to structured headers and that would be when you implement dictionary make sure that you allow ordered and unordered access so as a hash or as a list no JavaScript is some ordered yeah what you can hear - right you know what yeah and it maintains that this is a common pattern that\u0027s that\u0027s coming up in in languages it\u0027s a facility that\u0027s being provided partly because you don\u0027t get indoor up on Jason without it in a couple of cases anything else okay I think we\u0027ll give that a try and see how it goes number six twenty nine identifier we removed identifier z-- from the item so you know we I we specify this thing called an identifier in a B and F terms it\u0027s roughly a token has a few extra characters but it\u0027s just a you know unquoted string we remove them in issue 505 because you know we just talked about how do you distinguish between a in order to or an unordered dictionary in an API the same issues here how would "
  },
  {
    "startTime": "00:51:35",
    "text": "you distinguish between a normal string and an identifier so that you could reliably serialize the right thing based upon a data structure in some language we we removed identifiers from the item payload so that it would simplify the API so that you know you wouldn\u0027t have to have an object to wrap an identifier so that it would serialize correctly if we reacted them I think that most languages would need to do that and and the driver for this is we got some feedback from an event estrin saying that he\u0027d really like to have their identifiers as payload so that he could back port potentially structure headers on to existing syntax and make it work a little more elegantly um does anybody have any feelings on this one where they are to be clear I was the driver for five out of five and I think this is a reasonable feedback to add them back in it would it makes the api\u0027s a little more abstract you know now you need an identifier object basically but I don\u0027t think that\u0027s the end of the world come if you\u0027re awake so Julian says this also depends on what characters would be allowed in identifies yeah right now identify um is if I remember correctly its token plus a few characters I think there\u0027s been a little discussion about adding a few more characters we need to keep it safe and extensible but also a bit flexible become a nice if you did by 64 or some some variant they\u0027re open inside we already have binary we don\u0027t need to go there yeah you wish okay all right I\u0027ll take that on board um that\u0027s all I have for structured headers like I said oh it does work I think yeah we need a bit more time on this but I\u0027d like to shift this maybe around Bangkok ish I think that we don\u0027t want to leave it out there too long right so they\u0027ve been 50 issues open mostly mostly closed so it\u0027s actually got an input from a number of different aspects which is always good can we get commitments from a handful of folks to review if we\u0027re on a Bangkok timeline looking for maybe five hands that will promise to read this from a core working group perspective excellent I know who you are thank you so much for your for your contributions thanks mark Kazuo we\u0027re doing a cache digest next so we\u0027ve got I think three pretty quick updates come in here and then we\u0027ll get into the proposed work session and so folks to be prepared for the ordering there I\u0027m going to suggest that CD and loop prevention get this first slot simply because that\u0027s the only one in which the presenter himself has the conflict rather than folks who are interested in the work so if you can\u0027t "
  },
  {
    "startTime": "00:54:36",
    "text": "live with that throw yourself in front of the microphone at the important time kazuo go yes so this is about cash dodges and there hasn\u0027t actually been new updates we use the new version four and it was to remove the support for steal digest and we\u0027ve done and there was a push back on the github pull request and that for REST API that serves a list of files there it would be beneficial to have a stale digest so that the server can push the stale ones when the index is being requested so we might reconsider this but we don\u0027t need to hurry because they open you soon that\u0027s being pending and which is about changing to yet another bit of digest algorithm and ultimately ultimately we need something by a browser that works well on the browser and that\u0027s also pump so your honor is continuing his research to find the best algorithm and I think we will rather wait for him then trying to figure out without having any implementation so that is the status of cache digest thank you so we just intend to keep this open for a little bit right it\u0027s an experimental draft so people are clear on on the status yes okay anyone have any comments from the floor remote cache digest okay client had in spark to you so we just opened a last call working group last call on client hints Ilya submitted the new version of the draft we\u0027ve held it open for a while because we wanted to make sure that coordinate would fetch the fetch special specification what working group as well as the HTML specification there so that it was a align with what we did and I think that the folks involved will agree it\u0027s it\u0027s settled down enough where we can go ahead and ship this back so we opened a three week working group last call since we\u0027re all here this week and a lot of us are traveling over a week in the next week we made it a bit longer than the normal one so please have a look at that specification if you have any feedback since the list especially um this is an experimental draft and we called it experimental because we only had one a browser who was committed to implementing it and so we\u0027re looking for interest and implementing it from other folks whether it\u0027s browsers or server-side I know we\u0027ve heard from some folks on the server side and also you know support for publishing it interest in in the general use case that it that it is designed for so please take that to the issues list or to the mailing list or if you have any comments about client hints now we\u0027ve got a little bit of time in "
  },
  {
    "startTime": "00:57:36",
    "text": "the mics no and ecers here that\u0027s good okay it is so we have one more active adopted draft to talk about and that is sixty to sixty-five this the cookies Restatement and update they believe my quest was gonna try and join us remotely is he yeah why can you go ahead and request do the media co thing oh wait I said make it new Nick never gonna come oops maybe they\u0027ll find Mike yeah there is okay here we go hello who\u0027s working yes yes technology on the first try amazing so our TV is 60 to 65 this there\u0027s not a whole lot of progress since the last meeting I have not had time to work on it happily though we have John Wieland ur from Apple who is who\u0027s agreed to hop in as an editor to help me out so I\u0027m very hopeful that we\u0027ll have actual progress to discuss in a couple of months there are a number of bugs have been a lot of those around the same set attribute which is excellent because people are actually looking at the same site attribute we have support for same site is shipped in Safari Firefox edge and Ivy over the last couple of months which is pretty exciting to see because of the attention we\u0027re actually finding bugs in the spec which is great we\u0027ll get those fixed as well as fixing them in Chrome\u0027s implementation which has been shipping for a while we\u0027re also in the process of porting over the test suite that Adam bar through a long long time ago into the web platform tests repository that browsers are generally using as part of the newest integration tests we\u0027ve got about 50 50 some-odd of those tests ported over and we aim to get the rest of them ported over now that what platform tests has added support for multiple registerable domains which took quite a bit of time I\u0027ll note a couple of metrics just because I think they\u0027re interesting around this time last year the same site attribute was present on 0.01% of setcookie operations that chrome user saw that\u0027s up about 5x over the last year so we\u0027re up to about 0.05 percent of secresy operations using the same site attribute likewise the double underscore host prefix was at 0.005 percent a year ago and it\u0027s hovering around 0.01 percent right now so about a 2x increase the double underscore secure prefix was hovering around nothing a year ago and is up to about 0.003 percent over the last month so we\u0027re starting to see more usage of the things "
  },
  {
    "startTime": "01:00:38",
    "text": "that are defined in this new specification we\u0027re also seeing as I noted earlier more adoption of the items in the spec so I think we have multiple implementations of everything that we\u0027ve added and as I said earlier since John\u0027s gonna join and help out with the editing I\u0027m hopeful that we can get through some of the disagreements between browsers and I disagreements between browsers and the stack resolve those in favor of the browsers and call the spec down any questions or feedback Eckerd did you know you\u0027re done okay anything anybody okay well thanks Mike cool thanks for staying up and good to see you yeah and that sounds promising thank you Mike Thanks okay so as I said I think that ends the presentation we have an adopted work we\u0027ll move on to some we have three proposals or things people are interested to see if the working group fields are in scope and has the energy to work on and we\u0027re gonna start with CD and loop prevention because there are several conflicts with CF RG coming up but that includes just one of our presenters who is Nick so okay so so Nick\u0027s declines to take to take the time and so Chris\u0027s we\u0027re going to Chris\u0027s conflict and have the sni presentation next got that wrong you know we can correct that too but you\u0027ve ruined my ordering of my tab so I just got to look around there you are so I\u0027m sure we all are familiar with the various approaches to encrypting SMI I know for me saying something is the easiest way to submit it in my mind and when I was a kid there was a game where they would tell you something and then do charades and I kept saying the thing they would tell me and I just couldn\u0027t stop myself and web browsers are doing the same thing they say it even when they\u0027re supposed to keep it secret and so we\u0027re trying to find ways to reduce that all right so the proposal in a nutshell for is to add an S and a parameter to an old service record to say for a given alternative you can also suggest what s and I value you ought to "
  },
  {
    "startTime": "01:03:40",
    "text": "use when connecting to that onto that surface and so you can pick something innocuous that you know is going to be in the same certificate or you can pick a certificate you know the server has and then you secondary sir to get the one you actually want afterward but either way you can pass the suggestion on along with alt service and all the various ways to deliver that so in terms of how you check that that first certificate is okay which you have to do you can\u0027t just follow what used to be there for opportunistic security and we took out before it went RFC of discard the first certificate just doesn\u0027t matter it actually does because then an active attacker could induce you to connect and then see what secondary certain request you\u0027re sending as soon as you connect and well now you now it takes a little more work to find out what your s and I is but you can still acquire it now the primary certificate on the connection has to be somehow related it might be the host name that they gave you the s and I for so then so whatever the innocuous host name is it might be the host name of the origin which is what alt service currently says or it might be the UM the host name of the alternative that you listed in the alt service parameter so if we\u0027re saying you can get dub-dub-dub on alternate example.com the cert has to be for dub-dub-dub or alternate and if alternates assert does not cover it up dub-dub then you\u0027ll ask to see it with secondary service this creates a way to confirm these potentially way to confirm where you going I think but up to thinking through maybe ever okay having all three of those like seems like it\u0027s really hard to think to reason about like if hit the claim that this says go to this says go to these guys right like the the alt service says go over here why am i returning type the original one that\u0027s like not that\u0027s a confusing semantics so I asked I asked for a delay example.com B dot example to org in yes and the old service like what under what rational circumstances should be dies able to or be sending mediate certificates for example my cop RFC seven alter of us right so so old service currently requires you just the alternative server to send back the original origin I understand but he\u0027s gonna send us a tie so if I can insert myself behind you and cue my confusion is is the dolt service requires the or "
  },
  {
    "startTime": "01:06:40",
    "text": "the original origin the you know the Georgian house name to be covered by the cert but it doesn\u0027t require the alternative to be covered by the search and I can understand the s and I being added that that seems like okay we\u0027re gonna do something in this a Sinai\u0027s name we should probably own it in some fashion but why add the alternative to the next so this is adding the alternative as an option so when you\u0027re using TLS 1.3 which cert which cert you\u0027re using is not exposed to the wire this given gives it a way to basically have a concealed option and I think the that I you can\u0027t do that because then it\u0027ll just cut and paste that if if you respond to the cover us and I value with the true certificate then I just say the person I thought you and send us the server I get the certificate back you have you have to respond or then knock your certificate as the curse to me now it might be a star certificate or it might contain both domain names but absolutely if it doesn\u0027t validate for like the it doesn\u0027t validate for the original for the sni that you put in the in the client hello then it\u0027s absolutely clear that us and Isaac cover us and I like why we spent all this effort trying like trying to climb hello the client key share to this and I in the first place so Ben Schwartz I agree with your analysis essentially this draft does it introduces a variety of modes essentially or a variety of combinations where it can be used not all of those modes quiet provide the the full level of protection against a probing attacker basically so yes if your if your threat model the threat model that you\u0027re proposing is a reasonable one if that is you the threat model of the site then they probably shouldn\u0027t use this in that mode they should probably make sure that that they return certificates that cover the sni the clients are providing but okay I guess I just in kind injectors as chair is is this aspect of the design crucial to meeting your use case because I don\u0027t want us to crater on this if you\u0027ve got more I think we can tweak this but okay let\u0027s not hyper focus on this one part when you\u0027ve got more and I think also the scenario that this last bullet was added for was when the alternative is an IP and so well the must is it it must cover at least one of them suppose anger so I\u0027m wondering why we\u0027re mucking around the sni itself that\u0027s nice several restrictions that must be like a s key character since we got that I forget and must be a domain name so it seems like this all this other stuff is problems will be great being created by the fact that like your "
  },
  {
    "startTime": "01:09:41",
    "text": "your multiplexing the s and I would be possible to just eliminate the SNI the usage altogether and use another extension instead of the s and I when the replay field or well no SMI yeah so we\u0027re gonna actually cut them like line right here occur but well we\u0027re gonna Rio just so they can finish the presentation but we\u0027re gonna reopen don\u0027t worry so yeah I\u0027m lucky not mucking about with the SMI value or changing that\u0027s not evaluating that\u0027s an ayah value altogether instead of doing as an alternative s and I like no SMI that\u0027s distinguishable on the wire from normal traffic an adversary who wants to blah no SNI traffic can just block it so that may not be as strong against an attacker who wants to first you into the position of sending us an eye right so the the primary point of this design is from the external wire image it looks like a connection to the innocuous thing overall no using alt service potentially but then when you get inside the connection the client knows it actually wants some other certificate or some other hostname on that certificate Erick Nygren clarifying question and response clarifying question these are in addition to the V it has to match the host header not instead of yeah because a lot cuz a lot to break the security properties of alt service which is if you if it\u0027s instead of so the primary certificate has to match these and if that doesn\u0027t cover the domain you want to make a request for then you need to get the these certificate that covers that domain using secondary service okay and then the clarifying one is the the other there is a like the the even just a simple use case for where you could do this just as a wild-card case if you yes there are there are a number of there are quite a few sites that have a big covering wildcard sort like star dot github.com and just being able to do underscore wildcard github.com in case in the alt service is a a very simple use case that could add a lot of value without needing to get fancy so are you suggesting that was just in response to supposed to comment on a case where this has value even if it was even if we weren\u0027t doing anything fancy to su boats point the draft also does state that you can specify empty string which means they don\u0027t use this an eye okay so we\u0027re switching so there are two drafts here that\u0027s one of them the other "
  },
  {
    "startTime": "01:12:41",
    "text": "one is DNS alt service in principle this is separate so this a this is applicable it\u0027s functional as a draft whether or not you have any of the this SNI munging capability in alt service it just doesn\u0027t necessarily cover all the same use cases so DNS alt service basically just gives you a way to distribute alt service through the DNS it does the simplest thing you could imagine the there are some interesting subtlety here one of them is that you can fire off a DNS query for the alt service and not wait for the response if you want you can just race ahead and essentially the the draft says that it\u0027s up to client policy whether the client considers it mandatory to get that information before before starting the connection well go back I\u0027m not done yet so a couple of new things that have happened here we we switched the order of the prefixes in response to a request from Martin at the last meeting and thanks to a very detailed review by Schumann puch in DNS op there\u0027s now very clear semantics for what to do if you have multiple of these things in an AR are set in the in the DNS and that\u0027s set up so that you can use this for load balancing within the DNS so it\u0027s yet another layer of indirection that can be used for load spreading okay I think that\u0027s enough and this is a totally unfair slanted comparison of yes and I our drafts because I know that this is this has to be what\u0027s on everyone\u0027s mind so I one thing I want to add to this in case anybody thinks that we\u0027re being too unfair is in my view these these drafts are they\u0027re compatible in a in a in a simple sense that a client could do both or one or the other server could do both or one of the other and and everything would work fine but I\u0027m really personally interested in ways that we could potentially combine them and basically get the best of both worlds yeah one thing that we\u0027ve discussed is could we put the ES and I record an old service sure so doesn\u0027t that everyone fair then perhaps inappropriate or irrelevant but we\u0027re not in competition here unless unless you\u0027re planning to send and letter for they should be working groups of tales for group saying do yes and I I\u0027m not sure we need this need this comparison I mean the relevant question is on so I mean the the the I don\u0027t think it\u0027s bad to have DNS all service um I\u0027m not sure it\u0027s good don\u0027t know I\u0027m "
  },
  {
    "startTime": "01:15:41",
    "text": "not sure it\u0027s bad either um I think I\u0027m like less persuaded that that the dolt service has an eye on the / to that if you\u0027re the environment where were you\u0027re doing is you\u0027re trying to go to like server a um and you do nice resolution um it\u0027s not clear to me that having the having an SN I punt and then followed by second if it gets the superior dev encrypted s and I um it\u0027s clearly slower and it requires an hour on trip um it\u0027s there\u0027s gonna be like one domain in the marker um and so there\u0027s gonna be one domain you\u0027re gonna be showing so it\u0027s gonna be relatively similar marketing properties um and um and also the thing is like basically um you could use totally right by the way about like it\u0027s obvious what you\u0027re doing but like basically when that one domain is like you know um that one that one that one domain is like I in the cover domain like it\u0027s kind of obvious that almost everybody there\u0027s the cover on and also I mean frankly onesies and make people sad about you know the various domain fronting debacles over the past like you know past like three months was that someone\u0027s name got wasted as like the cover name and those guys got somewhere under risk is selectively blocking and so so I mean this is why one motivates putting like a blank name in there but then when you\u0027re a blank name in there then you\u0027re like well I don\u0027t understand why I didn\u0027t do what why I got on that bike named Daniel Kondo more so if you could put up put back up your comparison I wanted to there was there was a line missing there I think yeah so so in particular yes and I seems to be designed to permit a client facing server to be distinct from the actual origin and I\u0027m not sure that\u0027s the case for alt services and I or DNS alt service maybe you could comment on that I think that is correct that with yes and I the server has the option to decrypt just the sni payload discover that it\u0027s not for that server and forwarded on whereas with our proposal proposals you would have to terminate the HTTP connection and relay the requests you have behind reverse proxy so to be clear in the alt service proposals we\u0027re saying that the the client facing server gets clear text access to the traffic correct just to set expectations we\u0027ve got about a little more than ten minutes left on this one o sonification question you said that how all this all this PC DNS thing provides new law passing under those defenses may ask how it\u0027s different from just sending a set of IP addresses you know address query because it\u0027s just sending a list of hosts rather "
  },
  {
    "startTime": "01:18:43",
    "text": "than IP addresses sure so well so I\u0027ll mention two things here one of them is that because the because the DNS alt service ties the the alt service parameters including the choice of SNI to the load balancing selection that is the the choice of our our it makes it possible to have to to load balance or switch across destinations that behave differently so if you have a classic the example here is if you have 2 CD ends and you want the ability to load balanced or switch rapidly between them then you might need different cover sni or an initial sni for those two destinations so you can actually do that with IP addresses indiana salt service by by specifying an IP address my point is that damn why do you need to introduce a new yes well you can do that with just an area code so you you can\u0027t do this with an a record that is there\u0027s no way otherwise in the DNS to say if you find yourself load balanced on to a particular destination IP address then use this set of alt service parameters but if you find yourself load balanced on to a different IP address and use a different set about service privacy so yes thank you yeah Eric my gran Akamai I\u0027m decoupling these three think the three things I think alt service DNS is extremely valuable and a lot of really good use cases for and I\u0027m very positive on it in fact I think that that most of my deployability concerns with yes and I go away or many of my deployability considered with with yes and I go away if we use the DNS alt service record as the thing to have either a reference to the AES and I keys or or the SMI keys themselves like the multi CDN case it solves it also solves it also means that you can have keeps some of those TTLs potentially independent the alts and I won it may be that that yes and I makes a lot of the Q\u0027s cases for all service s and I go away it may be that the wild-card label one in particular is one that\u0027s a simple enough case that having that\u0027s still having the alt service as an I attribute just to cover that one so people could use that without having to go full-bore yes and I might be worthwhile Patrick Ted and then let\u0027s let me just respond to that briefly and say I think there is a I think there could be a possibility there where we we say that you know basically the the secondary certificate case if this stuff is covered by ES and I and so we we use yes and I for those cases and "
  },
  {
    "startTime": "01:21:44",
    "text": "then we we only need s ni replacement for the essentially the cases that don\u0027t add a round-trip so star was just a short co-chair comment which is remember that we can consider these things independently but we chose to present them because they obviously have implications for each other if you chose to adopt them both right so then as an individual a lot of Plus Ones to what sort of Eric said I think DNS auth service itself very interesting I\u0027m interested in be able to know what protocol negotiation is going to look like before the first round trip that has a lot of actually really nice latency optimizations for the browser use case so I\u0027m interested in those the s and I stuff I\u0027m very uncomfortable with how it impacts the the core rule of alt service which is alt service has no impact on how you interpret origin and you know there\u0027s some interesting complexities there around the how the certificates are I understand the the actual rule is it\u0027s not violated but it muddies the waters a bit you know it\u0027s pretty the discussion about the certificates so I\u0027m less enthusiastic about the working group adopting that personally it\u0027s ed hardy before I start I will note that you had two remote people in chemo at least one of whom was ahead of me so if you want to do Luntz next it would be ahead of me okay I guess this is working now so thank you I\u0027m Luke - Jacob from Bloomberg I\u0027m quite interested in the old service SMI portion of this I\u0027m part of a group that is proposing a standard called trusted traffic we have an internet draft in and this would be very interesting for an origin to redirect authenticated traffic around a mitigation point that may be congested we met last week with would then also to discuss what we were considering and this would actually be an interesting way for an origin to redirect a client to a to an edge network that can very using some lightweight mechanisms validate the client have been forwarded on to the origin so our very interested in this and seeing this proceed for our use cases versus publishing those records wouldn\u0027t be ideal because we would we may want to send a very specific edge to a client once we know that that person was authenticated and trusted thank you it\u0027s at Hardy I I guess in part because of the the green background to DNS entries are human readable and should not require frequent maintenance because to me that that seems like it needed a blink red behind it of like oh you "
  },
  {
    "startTime": "01:24:47",
    "text": "lovely people that you think this is an advantage on the human readability aspect of it a whole bunch of stuff and sorry Louis could you meet I think we had a little bit of control issue up until you you\u0027ll be next okay all right so let\u0027s get back in the queue please the the the the critical question really here is that if you think that the DNS human readability here and the less frequent maintenance will interact with people\u0027s attempts to use the DNS for redirect i I I think that this turns into something where it\u0027s at at best a white background and if you believe that that\u0027s because everybody\u0027s gonna use you know IP multi anycast behind the specific names you\u0027re gonna put in here I kind of get that that might be the case but I really worries me that you think the human interaction with this record is important and I\u0027d like to understand why you think that\u0027s the case okay yeah I guess in general I I think that somebody needs to be able to configure this somewhere right so to me that I guess the question is basically do we need to create additional systems integration points where you know one of the one of the challenges that so I should I should be clear that I I am a big supporter of yes and I I think it solves my problem that I care most about really well one of the challenges I see with it is that it it creates an integration point between key generation somewhere in the TLS front end and and the DNS and that\u0027s an integration point that I think that has to be fully automated because as far as I can based on the discussion list it sounds like those are supposed to be relatively short-lived keys which means they\u0027re replaced pretty frequently so that\u0027s that\u0027s a new engineering challenge that seems like we avoid if we basically have a static record whose contents is easy to understand okay that\u0027s a much better phrasing than that I appreciate it could the mica lines are cut now we\u0027re almost out of time so I guess now I\u0027m clear on what those points were supposed to mean um so I think the or certian you\u0027re making is if you want PFS or the ESN eye then you have to then you have to regularly change the keys correct piece of you don\u0027t current PFS for the sni then you don\u0027t just keys like basically act right hurry now DFS is desirable but it\u0027s not doing the wall um yeah okay um like I said I\u0027m sure trying to figure out how you envisioned this looking it as a privacy measure so resolves I think like you know as Eric said I I have "
  },
  {
    "startTime": "01:27:47",
    "text": "basically the representation of like yes ni keys in the DMS if you don\u0027t think those Ruby an old service record rather than like an underscore yes and I record like I um I guess you know the I\u0027m trying to figure out what your sir model about how this works is because um is your model the same one we\u0027re floating that basically um everybody in the in in in everybody on the sort of CDN is gonna use all service to redirect to exert to redirect to the discover name I think basically that\u0027s where you wind up whether it\u0027s a single cover name or whether it\u0027s a set of names that you know are associated with that sir so the wild-card search was a good example but doesn\u0027t that mean that basically everybody in the world except says extra round trip to get the secondary sir not in the case of the star yeah but is it likes a Clow flat yeah if you if you put a certain name in the search that is not associated with the name you\u0027re actually making requests in yes 0tt turns into one RTT or in one RT he turns into two RTT yes I don\u0027t see how you get away from having that those names I don\u0027t see how you get away from having the names outside the surf um decision wildcards like so you know so um you know this is a staking CloudFlare where you have you know n main names in the cert right um under what conditions is safe to put like to put symptom surgery like you know to pretend like what do you I don\u0027t understand what you put in the initial search that like has any prod body succeeding so so consider the case where the cert contains ten customer names and CloudFlare then you can put one of those other names or put CloudFlare in the S\u0026I and get back the search yeah you have to put something that would allow it to identify the sort but no I understand but my point is like so this let\u0027s say Clark\u0027s I don\u0027t want to like complicate them they\u0027re not like the people in charge of this but you got a big CDN and it\u0027s got like 10,000 customers right and the all of them serve all service for innocuous domain X will say CDN comm right so when I so I go and I asked for a two example calm and I get all service in a CDN calm and like wonder why I was the only concern me is like the cert which sounds like a one in a one of one thousand chance at how you might have a minute so I understand how it works so basically there are two ways that that could play out the first one is that you have a search that is completely unassociated with the actual domain name yeah and then you have the extra round trip for secondary service the second one is that "
  },
  {
    "startTime": "01:30:49",
    "text": "you can use some other hostname that\u0027s in the same cert and then you have ambiguity from from an observer as to which of the host names covered by that cert you\u0027re actually dealing with okay I guess but I mean in the in me I mean that\u0027s like an enormous information leak right I mean so you know there are you know the CDN is me so I\u0027m like I\u0027m like an inspector and the CDN is hosting 10,000 domain names which 100 or like really interesting because I like cancer sites or you know or you know or dissidents right and so like then they have as you say they have ten dominions per search which means that like you know that but the vast vast majority of I only ten innocuous domain names in them and so then when I pull out all I get back one of the second it is and by the way like the search were not you see these search all and the clear encrypted but they\u0027re not because all I have to do is take that SMI and replay to the server and I get the entire list of co-located debate names assuming I didn\u0027t scrape it a DNS so like it\u0027s a huge information Lake so if this is true so first of all yes and I is strictly superior in this case I I think that\u0027s I I agree with that assessment the the thing I would say is it this is not as impractical as you might think because there\u0027s a small number of sites here and if the different if the choice is like just close is that the visitor is viewing my site which is potentially dangerous in different ways or accept an extra round trip that\u0027s a choice that that just that one domain owner can make without the rest of the CDN having to make any choice at all that one our TT would be a big cost across a whole CDN but for a single customer that\u0027s a that\u0027s a very small performance cost I think well again I\u0027m not trying to like draw comparisons between mechanism to trying to analyze whether this mechanism actually a plausible mechanism one one he is right and I mean it seems like yes as you say you could make that work but it\u0027s like you know it is I don\u0027t like I don\u0027t see any scenario which doesn\u0027t involve a nice two round trip I mean I said like I sorry I see one which is like some enormous domain which nobody is willing to censor combined with like you know a very very small number of like domains are willing to co-host that are sensitive and in that case that didn\u0027t contain like you know like smoogle calm and like the seven things they\u0027re fronting for but like we\u0027re on trip as far as I can tell so yes but there are other threat models to consider so one of them is I am the only domain on my IP address there is in fact no way for me to be confidential but I can at least avoid explicitly leaking my identity on the wire and instead I can I can zero my sni and then I can try to basically hop IP addresses and you know the IP address is all that\u0027s left to identify me right like these are not cryptographic threat models but they are real threat model so I\u0027m gonna interject here we\u0027re not gonna make a decision about this today sure and we\u0027re over time okay so we I think this is gonna be a great hallway conversation okay good endless conversation so so we wanted to just get a sense of the do you have you you\u0027re you\u0027re good right we want to get a sense of the group who is interested in interested in continuing this discussion "
  },
  {
    "startTime": "01:33:50",
    "text": "we\u0027re not talking about adopting documents we\u0027re not talking about any decision just who is interested in this discussion continuing up I\u0027m assuming that that\u0027s a yes right because this is the second time we\u0027ve we\u0027ve worked to this material so we want to we want to see if there\u0027s interest from the working group to continue or if we just kind of need to move on in a different direction so we\u0027re gonna do two moms right one presumably with the simpler the DNS all service right see if how much interest there isn\u0027t that and then you know separately and I guess gated on the first one um interest in the alt service SMI right I\u0027ve done this before parallel so if you I\u0027m gonna be careful here if you are interested in continuing this discussion of DNS alt service please hum now and if you\u0027re interested in continuing the discussion the vault service S\u0026I please come now it\u0027s fairly strong Brad you\u0027re you\u0027re putting the hum off of kilter a little bit there you\u0027re very good at this the first one I would characterize as a reasonable amount of interest the second one just slightly weaker okay no because we\u0027re just gauging interest yeah this interest is everywhere okay thank you very much showing so next the CDN loop yes um yep sorry we\u0027d close the queues thank you hi I\u0027m Nick Sullivan this is a draft that was put together in the last couple weeks about a specific issue that has been plaguing of a very small segment of the Internet but an important one continue so as a background CD ends you can think of a CDN as a lot of different things is something that that\u0027s hosting static content but a lot of modern content delivery networks are organized as a reverse proxy for websites often TLS terminating because customers can configure a CDN to point to an origin and this origin can be anything you can end up using CDNs in layers so one in front of the other in front of the origin or you can have them pointed to basically any IP address in order to prevent a customer from configuring a CDN to have a reverse proxy that points back to itself through another service they often implement specialized headers there are some specific ones that\u0027re "
  },
  {
    "startTime": "01:36:50",
    "text": "that are listed here that have been used to help prevent these loops so because it\u0027s possible to point these two e to one other and because it\u0027s possible to configure your CDN to modify headers along the way especially headers that are multi-use or custom to a specific CDN like some of the ones listed here next slide please you can get into a looping type scenario even if you if proxy 1 implements a special header that says hey I\u0027ve been here before don\u0027t forward me proxy 2 can remove that header and proxy one can remove proxy twos header and you end up in this infinite loop this sounds theoretical but that next slide please sounds theoretical and there was a paper about this which was a theoretical paper but it came with a practical implementation this and this has been confirmed you can you can actually do this on basically the majority of the top 20 CD ends in in some configuration or another the loop can be 2 or 3 or 4 as long as you can configure one of them to strip out the dedicated loop prevention header you can force them to do an HTTP lupine and this paper has some very interesting graphs and and experimental results from this so how are you supposed to solve this while in HTTP there\u0027s a header listed called via which is meant to indicate that a proxy has forwarded an HTTP request this via header is an example here it lists the HTTP version and then a canonical name of what the proxy is and you can coalesce if you see the same proxy multiple times and you basically just append what you see on top of this and so you should you shouldn\u0027t combine entries that have different protocols but in any case this this is supposed to be the solution next slide please so in practice dia is overloaded and in various web servers like iis six is-7 nginx and apache which as of the list records makes up I would say somewhere near the majority of all web servers and web servers behind CDN it\u0027s very hard to measure that number but it\u0027s well it\u0027s relatively correspondent with the public facing numbers there\u0027s an assumption that a via header indicates a proxy that does not support compression so an HTTP proxy so basically all the compression related fields are ignored so if you as the CDN send a request to an origin with the via header it will reply back with a response that is not connect not compressed unless explicitly configured to turn on compression next slide please "
  },
  {
    "startTime": "01:39:53",
    "text": "so the proposal for this is a very narrow n-- dedicated request header called CDN loop it has a very similar semantics to via and is meant as a replacement for via that is actually practically deployable and can be used to prevent this next slide please all right so the the rec requirements here are that conforming CDN should add a value if they are reverse proxying data and they must not remove this header so you add to it you don\u0027t ever delete it and as long as everyone agrees then this header can be used as a common loop prevention mechanism next slide please this was posted on the list with a bunch of comments about existing other methods that may be useful for this first which is RFC 7230 9 the for today HTTP extension it there\u0027s some some wording in here that may be a little bit ambiguous I wasn\u0027t able to interpret it fully but basically you can it says in Section four that you can remove previous forwarded headers this is something that that breaks the required semantics of of what we need for preventing loops the other proposal is from HTTP the one one the max forwards header field you could basically set a max forwards field and decrement every time that you go through so eventually you would decrement down to zero but the this field is mostly oriented towards trace and options and its use in yet is is not something that is widely agreed-upon or implemented and that\u0027s basically it so this is a proposal it\u0027s a custom header it\u0027s has a dedicated use case but it is one that is it\u0027s useful for a very practical attack so grid up for questions I\u0027ve got a few minutes go ahead yeah Thompson I made some comments earlier about the privacy aspect of this but looking at the alternatives here the alternatives of far worse because forwarded says yeah I forwarded it for that guy specifically identifies them there\u0027s pretty similar on that front as well so I think as long as you\u0027re using some sort of generic opaque identifier that only the CDN itself is is going to consume then this seems about right it\u0027s unfortunate that we have to do basically the same mechanism as another one but you know welcome to the Internet "
  },
  {
    "startTime": "01:42:54",
    "text": "Otsuka at Google so I think that forwarded that header is actually battery space here we go back one even if we just used the bike field that would be you know forwarded for MacGyver forwarded by University and so I said cetera as a side note contrary to what Martine says I think all CDs already include that connecting right IP anyway so this way if multiple CDN providers collaborate it could actually be useful chain that actually propagates the client IP all the way to the back end which I think has value pursues stripping it and you know adding the last hope and CF connecting ideal IP all of them self labor from Apple it\u0027s reading this raft it kind of reads very much sort of targeted towards cross CDN loop detection yep that\u0027s that\u0027s nice to say I kind of feel that it\u0027s it\u0027s much more generic than that it could also be used inside a single CD on right let\u0027s say you do in cash hierarchies child parent proxies that sort of stuff and you and your example kind of shows where you put in the host field as a parameter to one of those things but I really think that it would be better if if the spec would be generalized such it can be both both cross CDN and sort of intra-syrian detection I think we already have mechanisms for intra CDN I mean this is why there\u0027s loop detection headers that are used inside of CDNs and and generally if you understand your infrastructure you will not get into this situation this is this is really about multiple independent configurations from customers that can strip headers and if you\u0027re inside a CDN and you have multiple and you have this complicated set of proxies and some of them are configured to strips the looping headers then you really have a Mis configuration in your CDN no that\u0027s not the point my point is that I mean instead of having two headers right now that we would do say via header to detect intra-syrian look detection but if I can do this all one other one why bother doing the doing both right that doesn\u0027t make any sense so I will note from the chair seat the some flavor of this comment was pretty common in the in the thread in which you used this work I will also know that was kind of nice to see you know 25 entries about someone proposing new work so there is some interest in the space which is great my question to the authors would be if the working group were to adopt this with the resolution of this question about scope B you know something we would consider a consensus item for the working group to decide or is this just something we should not adopt if the "
  },
  {
    "startTime": "01:45:54",
    "text": "working group can\u0027t decide that beforehand mark Nottingham is author I\u0027ll and I\u0027ll say what I had to say before I answer that question specifically because I think it will inform it this is a very specific problem I know we all as engineers have this urge to generalize and to make things more generally applicable and that\u0027s admirable but you know as as Nick said you know we have CD ends where customers can configure to strip or add headers very flexibly that is a feature that we all like to support and that we need to support and if we reuse forwarded or we reuse via or we reuse something that can be used by other intermediaries then it becomes not a specific targeted mechanism for how do we avoid loops between us and CloudFlare and Akamai and everyone else too something that there\u0027s ambiguity about whether or not it\u0027s going to work or not we really need this to work the attacks here are interesting so to answer your question without talking to the other authors beforehand my sense would be that if we can\u0027t get to consensus that we want to targeted mechanism here I\u0027d probably want to go somewhere else because it\u0027s more important to me that this works and that it\u0027s simple for us to implement and we don\u0027t have any special handling around a header that a customer might want to touch for other reasons then it would be to have a general mechanism Kazu and and and to add one thing I would love for the people who want a more general mechanism to articulate why this proposal causes them a problem why they can\u0027t live with it rather than just the urge to make it more general I think must not modify our requirement is a very good thing because most power web programming web application programming ideas are designed to like whiskey or they are all in that way and that\u0027s causing the issue that we had us get getting getting dropped so it\u0027s very it\u0027s a very good way to say that there is a specific added that must not be exposed to web application programming interface that could be modified but rather which server I could have it I\u0027m there for what it so I think having a speaker is a very good programming interface last call for the my clients Eric Nygren big +12 what Mark was saying around value of having to have a specific header for these sort of semantics I think also on the the how much to come how much to generalize it also cover the entry within a CDN case some of that gets proprietary enough that could bog us down forever I think having like the "
  },
  {
    "startTime": "01:48:55",
    "text": "value of having something standardized is to really work through the issues of how different CDN czar interoperating and if the CDN wants to go and include could have extra hops within this that\u0027s that\u0027s up to them but um but I don\u0027t think that I think the the key thing is getting that that between parties thing working out or worked out Mike Bishop I will echo both of the previous comments that I think the real nah novel and useful piece here is that it\u0027s a header that must not be removed that the what is causing all the other things to fall down is that they get removed and the customers want to remove them and I hear the concern that if you use it for in domain things that customers might also care about then customers might start demanding the ability to remove it I\u0027m not as worried about that if it\u0027s just find up front as must not remove but okay and I don\u0027t think there\u0027s actually anything that breaks though if you allow a CDN to kind of internal I say oh we have effectively three subsidy ins with in how we handle a process so we\u0027re gonna add three tags okay that doesn\u0027t break anything but the important thing is you don\u0027t use it in any way that might incent a customer to challenge you one must not yeah likewise we\u0027re gonna your name Krystal Evans from Comcast the internal to the CDN we can use whatever we want we have lots of options and in fact the existing solutions work quite well but again we need something that must not be removed and making it simple making it straightforward making it not useful for a whole bunch of things does in fact and send people not to remove it and should help with compliance and so I am very in favor of looking at this work so two other rest mugs question why for words and not this specific solution migrant here is that it actually can provide useful information that user said like the IP and this is opportunity for citizens to collaborate and come up with something that works and to end kind of circuit purity is your assertion that forwarded along with this header is not a good idea no I\u0027m saying that forward that instead of this header would be more useful idea to the end users without them must not Brigid must not or you want to add must not remove to the forwarded head header "
  },
  {
    "startTime": "01:51:56",
    "text": "no I don\u0027t understand your proposal just I may propose to use forward that header instead of that the CDN loop right I like saying and you know additional draft that sort of RFC that you must not remove this header doesn\u0027t mean that everybody will respect that right if you want to be realistic about this mark Nottingham again I think the the real value here is that it\u0027s not only it\u0027s must not remove it\u0027s that it\u0027s for an incredibly specific purpose so if you\u0027re not interested in that particular purpose then you don\u0027t really have an incentive to remove it if it\u0027s used for other things you might have another reason that we don\u0027t know that right now to remove it and that\u0027s why we\u0027re using forward it is not a great idea in my opinion because people are already using it for other things which we don\u0027t even know if it\u0027s just carved out for the most specific thing possible then there\u0027s less likelihood of accidental or you know reuse or diverging use cases and that\u0027s why this is so incredibly specific stepping and and so you know if you want the properties of for did use forwarded that\u0027s great that\u0027s it\u0027s already there but that\u0027s not the properties we\u0027re looking for here we\u0027re looking for a different set of properties which are a little bit subtle and that\u0027s probably why this is a bit of a back-and-forth stepping back I would ask the working group to consider we have on this draft three CDN vendors who were all pretty highly engaged in this process now and they\u0027ve come I think this is the first time we\u0027ve done something specific to CDN usually our engagement is about more generic things I would love to get a sense and maybe not here and now but for the working group to start thinking about if we have other things that we want that our CDN specific because the CDN is becoming for better or worse or the architecture and I\u0027d like to see them become more interoperable is this a place we bring them or not okay setting aside now we\u0027ll go up here and put on a different hat right now you can judge a hub for your own document okay so we will do I think one hum here about whether or not there\u0027s interest in us issuing a call for an option that\u0027s something we\u0027re gonna have to do on the list you know no matter but if we get a strong indication here that we\u0027re interested in working on this document which is a proposed standard so you know it\u0027s an obligation of the working group to spend their time and energy on you know advancing it quickly and correctly we\u0027ll issue that that call for adoption okay so the caveat I would have here is that we\u0027re going to adopt this document and we\u0027re going to adopt this document if we choose to do so specifically within the scope of the CDN question and if that you know is not an outcome that\u0027s acceptable to you you should hum against at this point so those are those in the room who are in favor of issuing "
  },
  {
    "startTime": "01:54:57",
    "text": "a call for adoption and working on the CDM loop draft please hum now and those opposed well all right almost unanimous thank you Julie if you want 30 seconds we don\u0027t know when you have the queues of wool Julie and we can\u0027t understand you maybe a gotta go jabber Marty\u0027s gonna channel you if that\u0027s a run so Julian says I don\u0027t get why it\u0027s less likely that somebody strips the new head of field as opposed to fire and Roy says via already works all you need to do is to find a seedy and specific pseudonym and allow those to be removed by Citians I think these are points will address during the confirmation discussion I agree okay thank you so next up helium Lucas is gonna present for us remotely Lucas we are about five minutes behind so if you could make up half of that that would be really excellent do we need to press the button to make you go all right you\u0027re on put your mutant hello yes great you have to push two buttons oh that\u0027s good to know okay I had a lot of slides that I tried to compress down and so I need to compress time further so forgive me if I rush through this a little bit I here today to talk about network tunneling and whether there\u0027s space effectively for or solving the problem of UDP tunneling mainly for HTTP over quick and if we\u0027re going to go to that length whether we want to expand the problem and solution into something more generic like IP so there are two drafts here that I\u0027ll cover one is called hint which is something I prepared to look at the general problem and solution space and there\u0027s another thing called helium which is draft by Ben Schwartz that was presented at dispatch earlier in the week so I\u0027ll explain these in a few slides but first of all I just wanted to frame the discussion and kind of baseline on how tunneling works today so next slide please so what we have here is HB 1.1 don\u0027t don\u0027t pay too much attention to the one bit just just imagine it\u0027s okay and it\u0027s not h2 which I\u0027ll come on to you soon this is not transparent proxying this is "
  },
  {
    "startTime": "01:57:59",
    "text": "a HTTP 1.1 client on the Left trying to issue a request to the server on the light and being configured to go by a 8 proxy so you can see here there are two TCP connections it\u0027s formed and the proxy takes on the role or the responsibility of affording that on and it may be able to filter requests or enforce any policy there so that\u0027s ok but we don\u0027t live in a plain text world now so if you gone to the next slide please we\u0027ve got here HP 1.1 over TLS over proxy so what we need to be able to do is create a end-to-end TLS tunnel so you know this has been specified we have a new method called connect that we pass in a name to so this example comm server and that controls the creation of the TCP connection from proxy to server and the client then can operate the into any TLS context and issue its request there this is typically configured with something like HTTP proxy variable or something like that just to highlight on the right this this is kind of a protocol stack for you from the client perspective of things so we\u0027ve gone to the next slide this is hb2 over TLS so you\u0027ll notice it\u0027s not too dissimilar we we still have a client that\u0027s able to issue a connect request here on TCP you\u0027ll notice that it\u0027s a it\u0027s still a hp1 proxy so actually we were able to mitigate version here and still use this HTTP based initiation mechanism to create an end-to-end TLS context that we can then it to hb2 we negotiate that so using a LPN or whatever you\u0027ll notice the addition of a yellow box which is here HP to stream so this indicates that for a request response exchange we are consuming a single stream so next slide please this one gets pretty complicated so I appreciate people maybe aren\u0027t so familiar with quick but if quick effectively inherits the h-2b to definition of of how connect words so in this case on the left hand side we\u0027ve got a UDP association between client and cropsy this is theoretically possible I\u0027d love to know how many actual deployments of this there are how you discover that proxies interesting you something like old service or do you set it up using some proxy packer or something like that but regardless you would issue a connect request on a quick stream and that would reserve that "
  },
  {
    "startTime": "02:00:59",
    "text": "stream so then carry all messages and the client to the proxy that would then get unbundled from the stream say and forwarded on via a TCP connection so what we have here is a single click contacts plus a TLS context in the same UDP Association and we have streams within streams so that quick stream is a reliable byte stream it can be affected by head-of-line blocking so any duplexing of the HTTP two streams within that TLS session would be affected by the quick head of line stream blocking so you don\u0027t necessarily get some of the benefits of multiplexing but you do get the ability to connect out to the Internet which is possibly more valuable so next slide please so this got me thinking how can we do the same for quick from the client to the server can we create a UDP Association from the proxy to a server and looking around doing some research there was no kind of standardized way to do that by HTTP proxy there could be some options here in terms of easy turn or socks5 UDP mode I\u0027m sure that is used in some cases but hypothetically what might be neater or nicer is to if you\u0027ve gone to the next slide have an ability to have something very similar to the TLS handling case that would allow us to do end-to-end quick tunneling and you can see there there\u0027s red question marks is that a connect method is that some other new HTTP quick extension and this is where I began thinking about the problem space so if we go on for the next slide the the draft HP initiated Network tunneling is a generalization of connect based tunneling so this concept of converting either an entire HTTP connection or effectively stealing the TCP connection from out underneath the feet of HTTP or some part of it ie the streams and converting that into something that can can effectively be a TCP UDP or if there\u0027s interest an IP tunnel so that document presents a concept and other design considerations in this space so if we want to provide a solution or design one does it need to cover multiple HTTP version so is this something that could just be for HTTP quick we need to consider things like proxy discovery and its ability to chain that\u0027s quite powerful capability here and is that required for the kinds of interactions that we might want to do for UDP all right be something here I kind of glossed over earlier is the ability to have agile "
  },
  {
    "startTime": "02:03:59",
    "text": "argit\u0027s so the connect tunnel focuses on one server there may be lower level balancing underneath but from the application layer perspective we have one tunnel and its own to one place so would there be interest in having an ability to target different origins is more agility if we\u0027re doing message based tunneling we need to consider path MTU TCP avoids that issue so this is something to consider we need to think about the proxies role in in whether it relates messages as they go through or does it I hate like connect only kind of blindly forwards things back and forth hole blocking I already mentioned and one of the main motivating cases for of this work is the ability to kind of mask traffic within a HTTP connection or and pad it out for traffic observation so the ID Canter\u0027s through all of that it weighs up some options and the way pros and cons and prevent provide some technical proposals not fully complete but more indicative of how things could look so we need more more input before investing any more time in only one particular one next slide please so yes I kind of broke things into two areas the initiation whether we use a request method or some new h2 or H quick thing and then the transfer the steady-state framing of messages that we reserved a particular stream is this not even stream level and it needs some additional capability or something like quick so there\u0027s a lot lot of variability there lots of permutations lots of different ways to skin this cat so just to help direct some discussion in kind of extremes there\u0027s a spectrum of proposals in that document one is can we just take connect and augment it in some way come create a new method like connect but clearly separate from TCP for in this case UDP and have some kind of new framing can we use something called helium which I\u0027ll it\u0027s been on the next slide and carry that over WebSockets or could we go the next level and have some major framing and for hb2 or quick that helps realize at a benefits that slide please so this is the helium draft this is Ben Schwartz document he went into a lot more detail in dispatches as he live in a single slide lightweight flexible epoxy protocol based on IP designed for many use cases for and quick is what I\u0027ve mentioned here but you could do things like web RTC eg Fox in with ICMP support go the whole hog towards VPN the concept here is kind of abstract message types and then a concrete realization of that and the document contains one which uses sea bore that runs over WebSockets but we could also possibly natively frame that and that\u0027s captured in my "
  },
  {
    "startTime": "02:06:59",
    "text": "document next slide please so in closing need to be fully acknowledging of the fact that there are many ways the UDP and IP based network handling HDTV based or initiated tunneling has some unique benefits and in comparison for those some of the discussions we\u0027ve had leading up to this is that there seems to be interest but is there enough interest in empty that were honest time and effort and if so some import guidance would be required for us can we can we actually drive towards one of those solutions or permutation of those two options the things we\u0027re not considering does this work actually belong not in HTTP at the lower layer and that kind of that relates to what is suitable home in the ITF for this work so that\u0027s it there any comments or questions we\u0027re running short of time so if you want to get in the mic line do so now then Schwartz I just helium was well covered in dispatch so I want to encourage people to focus on the problem statement and not that particular component of the solution lines cut that was a short last call so I will comment but I like the architecture and the problem statement here that being able to use it CP over quick to talk to a proxy has its advantages and I believe that one of the previous Centrum\u0027s Google had mentioned they had at least for some purposes it should be over quick proxies yeah with Google quick and we\u0027re seeing benefits from that but the fact that you can\u0027t then do quit a capacitor Cox yeah I agree is a problem I like the division here of having something that is effectively hvp layer I\u0027m now going to send some of their protocol and then that protocol being a transport II thing that\u0027s really an encapsulation it\u0027s like an evolution of gru maybe so I like that piece I don\u0027t know that they belong in the same working group if we do the MGP piece here I don\u0027t think we want the transporting piece if we form a new working group that handles this problem then maybe it could do both and I think we don\u0027t know what what way would be best but my opinion would be we don\u0027t want to do and try and pick up the whole problem in this working group briefly the one thing that what I didn\u0027t see called out here that worries me a lot is how they can is how when you start doing this sort of thing congestion controllers start interacting it seems like there\u0027s a lot of transport stuff under the hood here that gets very messy quickly like running quick in quick what yeah that\u0027s "
  },
  {
    "startTime": "02:10:03",
    "text": "the page issues came up in the dispatch session and something I hope to capture better in the design aspects means more thinking but the account on the meta is sure I think it\u0027ll be worth through mail also feel free to skip it was covered heavily in helium but was and dispatch pull is is we should be really clear on why we need something new here like it seems like a I P SEC over UDP covers a lot of these a lot of the use cases here without requiring inventing something new okay thank you Taniya so I\u0027m familiar with one use case well as Mike mentioned something similar to this has been used already at Google and that\u0027s actually been very useful because being able to switch the last mile link from TCP to quick allows for a lot of benefits because you can do better loss recovery and things like that on the last mile and it\u0027s so it\u0027s quite useful to be able to run a proxy in the cloud so to speak and have the clients be configured so that they can speak quick up to the proxy and then TCP out the back of the proxy to the origin so it\u0027s definitely useful I I will echo what Eric said and there\u0027s one particular design assumption and quick right now which is different from TCP in TLS in that the transport and the crypto context are not separable and this is right now commit assumes that they are separable right we have TCP to DC connections that can become and one pls think that can be layered on top of them that is no longer true and quick we really can\u0027t get that kind of composition so you would have do you have to do TLS in TLS effectively that\u0027s something again I mean I think this problem is worth thinking about but the pieces might land in different working groups and might actually trigger some interesting work so yeah I think this is useful okay thank you for the presentation we did cut the dough on in there the dispatched chairs have requested discussion happen on the dispatch list so I would just mention that you might want to join the dispatch list if you want to talk about this more also that\u0027s the chairs to think of hey did you understand that in to the extent that we are introducing and modifying HTP mechanisms right in the hallway after the manor is gonna act be multiple you know dispatch versus HTTP chairs we\u0027re gonna kick their ass there\u0027s gonna be or there could be multiple discussions you know have it have it have it your own way I\u0027ll take you in the fight with Murray any day that\u0027s all right so we will take you no further inputted on the HTTP specs of this and I do want to reiterate one thing I said on the list that if you operate systems that use the connect method implement systems that use the connect method that "
  },
  {
    "startTime": "02:13:04",
    "text": "kind of thing you know put it in a forward proxy scenario this would be a great time to speak up because I think it\u0027s sort of under represented in this discussion or perhaps it\u0027s not under represented in this discussion and that itself it\u0027s a you know it is a point of input thanks Lucas we\u0027re gonna move on we\u0027re going to talk a little bit about h2 push data from yo EV has done some work that you must just share with us so this is this is a bit of an initiative to find out how some of things we have standardized our plain in the world and how that how that works out you are not you math I\u0027m not you earth as you can tell we he\u0027s remoted it so my name is Dominator I am engineering manager at Akamai yes I\u0027m not as tall I\u0027m here to share some results about h2 server push and this is over a period of 11 days we had collected some measurements and had done some analysis on it from June 14th to June 25th so at Akamai h2 server push is primarily provided through a product called adaptive acceleration and so what this park does is it analyzes rum data so this is real user monitoring data so data that is based on nav timing and resource time data speaking back into data warehouses where we do some analysis and determine what the critical resources are that are necessary for rendering on a given page look and so the idea is that once we\u0027ve identified these critical resources we push them during the HTML generation think time the advantage here is that we order the key is that we want to utilize the idle network time from when the page is being fetched from origin and also bring back the tcp slow-start so looking at a typical case where we\u0027re not applying push the HTML request is generated the request goes to the edge in a content delivery network and then goes on port origin to be fetched and this is where we have the idle network time and eventually the response comes back and you can see that tcp slow-start in effect so chunks start coming back and getting bigger and finally the browser\u0027s making requests for page sub resources SS in JavaScript for example and then those get fetch down so the way that we utilize push effectively is to take advantage of that idle network time so we want to be pushed down CSS and JavaScript that\u0027s critical to rendering of the page during the time that the page is being fetched from origin and so we\u0027re usually we\u0027re using that essentially that dead kind of time to "
  },
  {
    "startTime": "02:16:05",
    "text": "good use and you can see you know theoretically the tcp slow-start comes into effect earlier and by the time that the HTML pages comes down to the browser that hopefully we\u0027ve moved past that face so Jake Archibald wrote a great blog post about each it should be to push saying it\u0027s tougher than we thought there\u0027s and he has a quite long post that goes into the nuances of H to push scenarios where it\u0027s more effective scenarios where it\u0027s less effective the differences between browsers the how the caches within each browser are work and very from between them so it\u0027s a great post I recommend that you read it if you haven\u0027t yet so diving into the results now so just to give a preamble before I show the graphs on how to interpret this we measuring dom complete time in this case and so when you look at these graphs negative is better this isn\u0027t a relative difference between h to push on and h to push off so the further to the left that that you see the bar the better it is this is chrome only and this is only first few only so these this analysis based on measurements only on first view its excludes repeat view on purpose and that\u0027s for a couple of reasons one is that the repeat view case is not that effective with push right now because of the absence of cache digests for example and with adapter acceleration we try to avoid pushes on repeat views for that reason the difference that you\u0027re gonna see is going to be a bar with men and max values and essentially this means that we are 95% confident that the performance difference will fall within this range between the min and max so essentially you see green that means it\u0027s statistically that\u0027s faster red would be slower and blue would mean there\u0027s no statistical difference okay so what using here is mobile results only so hoping your eyes are not pleading looking at this the bars are kind of thin but in this case there\u0027s 11 results shown so what we\u0027re seeing here is actually the intersection of some websites that are using adaptive acceleration product and also using a new rum engine that Akamai provides which is based on the product impulse and so it\u0027s the intersection between those products and I mean the we are just transitioning to this in your rum engine so the initial customer base is small but it\u0027s scoring and we hope to be continuing to measure it up based on this engine so we use the nav timing metrics to fetch the Dom complete time in this case so really in this case "
  },
  {
    "startTime": "02:19:06",
    "text": "there\u0027s 11 results here for we can prove they\u0027re statistically significantly faster and 7 we can\u0027t say at all and I have the raw measurements on the side that you can see in the left that provide the confidence interval and the mean but for the blue ones we\u0027re not supposed to derive any kind of conclusion from it you know you might see the bar more on one side versus the other but talking to our statisticians to say no you can\u0027t say hey so that has to be completely on one side of the other to derive any kind of conclusion so here we have four that are better seven we can\u0027t see and you can see that the confidence intervals fairly large in many cases for desktop we have 13 results and we have six that are better and seven that we can\u0027t save now the the scale is kind of shifted linearly a bit and maybe slightly in terms of magnitude but you can see that the bars nevertheless are much shorter the desktop case the mobile and why is this the case as just because we see more variability and noise and mobile performance data so there\u0027s more variability presumably due to more variance in last mile networks and that\u0027s reflected in the measurements so it\u0027s harder to get statistics anything results in that the results were do be dead see a statistical difference we\u0027re seeing positive improvement with h2 server push applied how did we measure so statistical methodology we\u0027re using a linear regression methodology that\u0027s used for statistical calculations and it\u0027s based on following dimensions so geographic location client to west user agent hour of day day of week is P URL this is all very important because if you don\u0027t product control for these variables you get too much noise and there\u0027s several dimensions I can introduce variability to the results so a B measurements are actually quite complex especially using the real user monitoring data that\u0027s part of what we\u0027re finding as we go through this exercise and again so I talked about the set of customers that are used in this case so far so we\u0027ve split into desktop and mobile restricted to first feel requests top 10 URLs for each customer website\u0027s minimums and minimum threshold for hits that have h2 server push applied so what were we going to do I think what I\u0027d like we\u0027re going to continue to gather data as a set of customers expand and we\u0027ll report back later what we\u0027ve seen so far is that server push is it\u0027s a powerful tool and the tool set when used in the right circumstances so thank you very much and I I\u0027ve asked people to hold their questions because we have a similar presentation from a different point of view we wanted to make sure both of you could present before we run out of time and then you can maybe "
  },
  {
    "startTime": "02:22:07",
    "text": "address questions together if for whatever we have we have left yeah after we do yeah don\u0027t go far everyone Brad lassie vendor at Chrome so we also urged it in how push is performing in the real world so first a couple stats right now we see 0.04 percent of HTTP 2 sessions that have a push frame in the entire session that\u0027s not per delegation that\u0027s for the entire connection the average amount of push data in those sessions is 32 kilobytes after after reviewing this data we realized that more useful stat would have been the size of the pushed resource make that edit later some other stats we went when we see these pushes 63 percent of them are successfully accepted 23 percent 22 percent timeout and 13 percent are duplicate URLs which mean that the same URL was pushed in the same navigation there\u0027s some other noise for several other failure cases so we ran an a/b see experiment we compared disabling push by sending the settings enable push setting set to zero and we would still process the push frames if they came this was purely just sending the the setting we compared that to no treatment regular control but we also compared that to sending an unrelated settings changed to see if servers were miss handling set HTTP to settings the Deaf canary data was too noisy to draw any conclusions about push but we did satisfy ourselves that the servers were handling these settings for him properly so we didn\u0027t need to include that control when we propagated this to beta within the beta results for the entire population or sorry the entire test population we got a slightly negative non statistically significant result so the best way to say this is that for the entire population push makes no difference in performance that\u0027s rather unsatisfying either direction so we attempted to correct for that by removing the noise of servers that don\u0027t push so filtering by domains that according to HTTP archive pushed when they were recorded in that "
  },
  {
    "startTime": "02:25:10",
    "text": "crawl we do see across-the-board improvements by disabling push and all of these percentiles are statistically significant and now we get away from the data-driven part of this presentation so this is a wasn\u0027t intended to be so scientific looking but this is basically the idea of what is the it\u0027s a clarification question on the previous line so the latency of the page is reduced when push is disabled is that what this is shown yes this is time to sorry dimensionless this time to content of content for paint I see okay and the control does not have push disable this actually pushes enabled with max concurrent streams so blue is push enabled red is pushed disabled to call it thank you lowers better yes lower is better so this is basically trust trying to describe that absent server think time or the maximum benefit of push is either your the amount of data you can send in one round-trip or your initial well maximum table you can send on trip which is either calculated by your bandwidth your round-trip time or your congestion window and I\u0027ve heard some people argue that in emerging markets this is a even more important thing because of the large round trip times the interesting thing which mean when you take a couple samples it all turns out to be about the same amount of data you can send in one round-trip just that round-trip takes longer of course all of this winds up getting if it\u0027s the start of a connection capped by the initial window which fear of being idea of recommendations is about order of magnitude lower than that round trip data other than you know some people who are getting aggressive so one of the questions that we have is if we were to turn off push for everyone would anyone really care currently it\u0027s as I said only four 100 of percent of all the sessions we see it seems to be a performance look on looking at some of the individual domains that are pushing some really really get it wrong some do get a good benefit though so it it\u0027s all over the place oh I did have one stat which I think the side which was that four domains that had thousand navigations in both both test groups sixty-eight regress and eighty for improved when you disable "
  },
  {
    "startTime": "02:28:11",
    "text": "push so slightly better than the 50/50 and if we weren\u0027t working on push all the time we could work on other fun things thanks so Thank You Brad thank you for both presentations my clients for comments jonathan kazoo okay first crack because i told them to sit down last presentation i just wanted to comment that push is yes we fuel have a long but as we move too quick as we optimize HTTP to the buffer becomes shallower and americo push becomes small yeah just to add on that since you\u0027ve invoked quick there\u0027s also a theory that with there\u0027s been some experiences with headline blocking with server push that this we\u0027ve made efforts on improving that and that in the quick world that problem would also go away which is an improvement of pushover quick so how that interacts with that i no I\u0027m Jen I ain\u0027t got first he had a slide that they talked about firstly having an initial window of 100 I\u0027m trying to make it happen it\u0027s not happened yet so that\u0027s actually the the the paper I\u0027m familiar with the paper that you\u0027re citing there and I have no idea where they get that number from when I I do but the number is incorrect sorry we\u0027re talking story of the media the conversation yeah the on the I was it was it was interesting to see the amount of variability that he had in your results and I was going to ask you if you looked at the population sizes of those different sites it seems like you might have very low populations possibly but it\u0027s yeah can you speak to the variability in the data yeah I mean so there is variability and it\u0027s it\u0027s not unique to anything with Porsche it\u0027s something that our service por encima has been discovering as we do more analysis on ROM based data and so you so you know your new measures using synthetic environments you\u0027re much more constrained in terms of variability it was ROM data we see a lot of variability and there\u0027s been efforts at evolving the methodology to control that by using different dimensions as explained and so that\u0027s what we have now and there may be further protected ology to further constrain it but yeah there is variability there and so even in the results are statistically insignificant in this case you know we know that in synthetic tests we have seen an improvement but you don\u0027t see it based looking at the ROM data for example so um we\u0027re actually out of time but if folks are willing what since this is just an advisory thing let\u0027s go ahead "
  },
  {
    "startTime": "02:31:12",
    "text": "and drain the cues but no more discussions please and please try to keep your comments brief so movin I\u0027m sorry I don\u0027t know the terminologies I mean I guess I can go your name for the minutes go home fastly just really quick a couple of things since we\u0027re talking about numbers thank you for Chrome for sharing those numbers I just want to share ours really quick we don\u0027t store in a user data I just did a sample of 90,000 requests on a server 90,000 streams of h2 had they\u0027re there 150 of them that were pushed that\u0027s point one five percent depending on how much you believe in sampling like this when I did this three weeks ago it was like 80 out of 90 thousand not saying growth it\u0027s growing three times over over three weeks just information I have a couple of clarified questions for the presentation do you have any date of volume numbers for how many sign oh there was a minimum of a thousand but how many samples per I\u0027m guessing each other\u0027s eleven swear sites is that what they were those are sites yeah do you have data volume numbers and and maybe more interestingly what percent of the pageviews got nuked because they were repeat we do have those numbers I don\u0027t have them at this presentation but we do have those numbers okay and did they did those sites any of them includes sites that had their HTML cached did everything go to origin to get so everything time no not everything so it depends some sites may have it configured to do pushes despite the fact there are HTML pages may be cached at the edge so if those 11 sites may have had HTML cached pages it\u0027s possible some subset may have so so that would totally negate the advantage that push would give for those sites well minimize well theoretically you should minimize the effectiveness of push in those cases because we couldn\u0027t have the Idol Network right great Thank You Alan from Dell Facebook I guess I just wanted to say we have run the the slide the blog post about pushes challenging and there\u0027s different implementations and browsers it makes it hard it\u0027s not uniform amongst browsers how they implement it and it makes push challenging for us and particularly the way chrome works in how it does its push cache is incompatible with the way we want to operate things and we\u0027re not able to really push effective to Chrome so you know I think that maybe one reason why from the chrome side you have one perspective and then from the CDN side you may see something else they\u0027re seeing other browsers that have different different implementations on push Eric Nygren I\u0027m question for Brad I think one thing that might be interesting to do is also look at if there\u0027s a way to filter that by thing by cases where the push is happening in the server sink time so for example cases "
  },
  {
    "startTime": "02:34:13",
    "text": "where all the pushes that are coming in are coming in before you actually start with receiving the response for the base object that was requested because some of this might it might be that that one of the side effects here is is if you have other stuff you can send you should never push and maybe that\u0027s useful guidance and it might be interesting to see if we can of decouple the these cases of pushes pushes useful if you have nothing else to send yet because you still doing something on the server side from the the people really badly misusing it by pushing stuff when they really should be sending something else instead that\u0027s a good point Mike we shop I will also observe that both of you are collecting data off of Chrome and off of particular sites so that\u0027s a good place to try and figure out where and where your methodology is diverging because you have divergent results and I\u0027m wondering if well one thing I observe is you did all navigations and you have only first navigations so that\u0027s going to impact things a lot I think and Chrome\u0027s numbers would probably improve if we had cache digests because of that and I\u0027m also wondering if it might be useful to pick a set of sites and do a joint experiment from both sides and see if numbers are still divergent and if so why we had a hallway conversation along those lines well thank you both that was really interesting I think we want to encourage more data like that as we go on that that\u0027s really good thank you right before we go to things real quick one we\u0027re having a session the barbed off next door in about five minutes for SRV and HTP if you\u0027re interested in that and to where the blue sheets does anyone know where the blue sheets are there\u0027s one and this is another one floating around there somewhere okay well hopefully we\u0027ll find it oh thank you great all right thank you all very much and we have another session tomorrow see you then thank you thank you yeah it\u0027s all we have this is Mark "
  }
]