[
  {
    "startTime": "00:00:11",
    "text": "town okay let's give it a couple more minutes to give people a chance to join hello"
  },
  {
    "startTime": "00:02:01",
    "text": "thank you oh uh so all right it's a little after half past one yeah i guess uh i've left a half"
  },
  {
    "startTime": "00:04:01",
    "text": "past two in vienna from what i can see it seems we have uh a reasonable set of people in the room and online so uh we should get started uh welcome everybody uh this is the irtf open meeting uh my name is colin perkins i'm the irtf chair uh and uh i as you'll see i'm i'm remote for this meeting uh thank you to brian trammell who is standing in in person and looks like he he is very at home at that desk at the front so he should perhaps be a little careful uh thank you for standing in brian uh and uh yeah let's let's get started um so as as usual uh i'd like to start with a reminder of the uh intellectual property rules um this is an irtf meeting and by participating you agree to follow the irtf's processes and policies uh around intellectual property uh and this means that you uh if you're aware of any patents or patents applications uh relating to um the work you're talking about in this meeting then then you need to disclose those uh and the details are in the documents linked from the slides i didn't remove one in addition the irtf routinely makes recordings of the online and in-person meetings and this includes audio videos and photographs and those recordings are published online if you're participating in person and you're not wearing one of the red do not photograph lanyards then you can send to appear in such recordings um and if you speak at the microphone um or um if if you're"
  },
  {
    "startTime": "00:06:00",
    "text": "participating remotely and you turn on your camera and microphone then then your you are consent to appear in these recordings and a reminder that this meeting is being recorded and live streamed um also a reminder that uh as a participant in uh the the irtf as matandi at the irtf meetings um you uh acknowledge that uh these written audio and photographic recordings uh will be made public uh and you acknowledge that uh we will be handling any personal information you provide accordance with the privacy policy listed uh also um a reminder that um as a participant or an attendee uh in these meetings uh whether that's in person or remote and on the mailing lists as well as in the meetings uh that you agree to to work respect respectfully with the other participants and you agree to follow the code of conduct uh and the anti-harassment procedures and so on which are listed on the slide if you have any questions about this please do contact the ombuds team and again the contact details are listed there uh and that they will help you out and this is the same set of procedures that are in place for the ietf they also apply for the irtf sessions um and a reminder that since this is an in that this is a hybrid meeting with a mix of in-person and remote participants um if you are attending in person in vienna then you need to sign into the session using uh miteco using uh probably the light version of the meet echo tool and you can join that uh there's a special icon in the data"
  },
  {
    "startTime": "00:08:01",
    "text": "tracker agenda which allows you to join using that tool um and you need to use uh meet echo to join the microphone queue since we're managing a uniform microphone queue for both the local and the online participants and if you're using the the on-site version of muteco keep your audio and video off for remote participants please leave your audio and video off unless you're cheering or presenting or asking a question during the session and again there's a raise hand button you can use to join the queue if you have questions um if you're having uh issues with the the av equipment uh the the url on the slide um allows you to report those uh or um mention me tekko in in the chat okay um so as i say this is the irtf open meeting um and the the goals of the irtf are a focus on some of the longer term research issues relating to the internet and the irtf works in parallel to the ietf uh which does the the more short term issues relating to engineering and standards making um my my usual reminder that the iatif is a research organization no it's not a standards development organization um in the irtf we're here to do research and we're here to provide a venue where the uh academic research community the industry research community uh can interact with the the engineers and and the standard setters uh in the ietf but this is primarily a discussion forum and it's and it's a forum to make connections um and while the irtf um because it's associated with the ietf can publish uh informational or experiment uh"
  },
  {
    "startTime": "00:10:01",
    "text": "experimental documents as rfcs the primary output of the research groups is expected to be understanding and research results and academic papers primarily rather than necessarily rfcs the irtf is structured as a number of research groups um the the slide uh shows the groups which are currently active those which are highlighted in dark blue on the slide the crypto forum group the pathway networking group information-centric networking measurement and analysis of protocols network management and human rights are still to meet those which are highlighted in in light blue quantum internet group met in the previous session the the global access group earlier this morning and the privacy enhancements group uh yesterday uh and the others are not meeting this this week a number of them have um interim meetings uh closely located um so do please look out for those those sessions uh later in the week and do do do join those sessions also for the research groups uh i'd like to welcome sophia jelly who joi recently has joined as co-chair of the human rights protocol considerations group which is meeting tomorrow morning uh and who's uh replacing every doria who who stepped down last year uh so thank you uh avery for your service and welcome sophia and and thank you for for us stepping up and uh we look forward to working with you in addition to the research groups we also have a number of other activities which we we coordinate in the irtf"
  },
  {
    "startTime": "00:12:00",
    "text": "one of these is the applied networking research prize um which is something we we organize in conjunction with the internet society and the applied networking research prices are awarded for the uh that the best recent results in applied networking uh it's awarded for interesting new results that are potentially of future relevance to the standards community and it's awarded to try and uh recognize upcoming people that are likely to have an impact on the standards and technologies in the future and as i say this is something we organize in conjunction with the internet society and with sponsorship from comcast and nbc universal we have two applied networking research prize talks today uh the first talk will be by uh sangeeta abdul jyoti uh for her work uh on uh the resilience of the internet infrastructure to solar events uh uh coronal mass ejections and so on uh and the second talk will be uh by bruce spang uh who will be talking about um a b testing and some of the uh the fairness problems that arise and the biases that arrive when conducting a b tests at the presence of network congestion and we've got two two really nice talks coming up uh and um there are also recordings of these talks and slides available links to the papers uh on the website so so uh pl please please do stick around for those we got two two really good talks coming in uh addition to the uh the prices we also run the applied networking research workshop which we organize in conjunction with acm sitcom the 2022 edition of that workshop will"
  },
  {
    "startTime": "00:14:00",
    "text": "co-locate with the itf-114 meeting which is taking place in july uh this year and it's expected to be a hybrid meeting with the in-person components in philadelphia i believe the program chairs for that workshop this year are tj chung from virginia tech uh i'm marwan fayad from cloud cloudflare um the call for papers uh has just gone out recently uh the paper submissions are due at the end of april um and the the link on the slide again points to the call for papers and more details about that um please do consider submitting your research results to the applied networking research workshop um this is a forum for for the researchers vendors operators and the standards community to come together and talk about their uh their latest applied networking research results uh we've had a bunch of really good papers in previous years so so please do consider submitting this year and the final thing i wanted to to mention uh before we go into the talks uh was that um we are pleased to have been running a travel grant program um sponsored by netflix and comcast um uh and it's it's good to get back to this um it's uh good to um you know have have people back in in person uh to at least a limited degree at this meeting and we're very pleased to have provided a number of travel grants for early career academics uh and phd students from underrepresented groups to attend the irtf meetings that are co-locating with the ietf this time going forward we we do expect to offer travel grants uh for the next itf meeting in july and for the applied networking research workshop uh and keep"
  },
  {
    "startTime": "00:16:01",
    "text": "an eye on the url on the slide for details of those going up over the next few weeks uh and with that uh i am done um what we have coming up next uh is the applied networking research price talks first being sangeeta and secondly in about 30 minutes also um bruce will be talking about unbiased experiments in congested networks all right so that's all i have uh sanguita you should be able to press the share pre-loaded slides button and that will let you share the pdf of the slides rather than the screen share button okay that will work better should be a little uh document icon just next to the hand icon in the top left okay uh so while that is loading um the first of the awards today uh goes to sangeeta abdul jyoti for her work on the resilience of the internet infrastructure to solar superstorms uh sangita is an assistant professor at the university of california irvine and an affiliated researcher at vmware research and she leads the networking"
  },
  {
    "startTime": "00:18:01",
    "text": "systems and aaai lab at uc irving and her research is on internet resilience systems and machine learning uh the paper underlying this talk uh storms planning for an internet apocalypse was first presented at the acm sitcom conference last year singita over to you i think you'll meet it still uh can you hear me now yes hi everyone i am sangeeta abdul jyoti i'm an assistant professor at university of california ry and an affiliated researcher at vmware research today i'll be presenting my recent work on understanding the impact of a cosmic phenomenon that is capable of taking down the internet which is solar superstorms what are the impacts of losing internet connectivity we know that it can affect our professional and social life significantly especially now during the pandemic for large businesses the impact can be devastating the revenue loss per hour for some of the most companies are estimated to be tens of millions of dollars and at the scale of countries it can be hundreds of millions per hour or in billions per day and it's not just the economic impact many of the other critical infrastructure such as health care uh depend on the that depend on the internet and will be severely affected now what if we have an internet outage"
  },
  {
    "startTime": "00:20:01",
    "text": "lasting weeks or months and spanning large areas across the globe one might think that such a global outage is never going to happen because the internet is racing and distributed systems um but unfortunately this is a worst case scenario that could happen in our lifetime the next once in a hundred year disaster after this panoramic could be a solar super storm that is capable of taking down large parts of the internet and today network researchers and operators overlook this threat while building a infrastructure in this talk i'll present a deeper view of this natural disaster and its impact on various internet infrastructure components there are two types of solar events that are popularly known as solar storms they are solar flares and coral mass ejections their impact on in global infrastructure varies widely solar storms are sorry oh yeah solar flares involve large amounts of emitted energy uh in the form of um electromagnetic radiation so they essentially flashes of light that reach the earth in just eight minutes fortunately they mostly affect only the upper legs of the atmosphere and do not cause any damages to the telescope infrastructure but they do affect satellite communication now coronal mass ejections involve emission of electrically charged solar matter and the accompanying magnetic field into the space and these cmes can take anywhere from 13 hours to 5 days to reach the earth based on their speed"
  },
  {
    "startTime": "00:22:01",
    "text": "and these cmes are capable of causing significant uh damages to crystal infrastructure so cmes are the focus of this talk and we'll discuss their impact more closely soon both solar flares and uh cmes originate near um temporary dark spots on the sun caused by concentration of magnetic field flux and which are sunspots and when the number of sunspots on the surface of the sun increases there is a higher probability of cmes occurring now how do how exactly do these cmes affect us the magnetized uh cmes are highly directional and when the earth is in the direct part of a cme it interacts with the earth's magnetic field and induces large electric field on the earth's surface through electromagnetic induction and this can cause geomagnetically induced currents to flow through long cables that have ground connections that are located far apart on the earth's crust and this could include power transmission lines oil pipelines and internet uh cables that have an electric connection that spans hundreds of thousands of kilometers now next let's look at some of the characteristics of these induced currents to better understand the risk so due to the orientation of the earth's magnetic field higher latitudes are at a significantly higher risk and it's for the same reason that auroras are common closer to the poles and since the um this impact is caused by interactions with the earth's magnetic field the induced currents can affect wide areas and are not uh just restricted to the portion of the earth that is facing the sun"
  },
  {
    "startTime": "00:24:02",
    "text": "the gic is only used in cables with ground connections and a conductor uh where the ground connections are located far apart on the earth's surface and finally the orientation of the conductor uh does not uh impact uh the risk associated with uh gic uh so the north south or east-west orientation of the cable does not influence the induced current so that was a quick overview now let's move on to the more important question when will a large event that affects the earth happen next so solar events are extremely uh hard to predict just like earthquakes or any other natural disasters small scale solar storms happen all the time what we are more interested in are solar super storms that can have a significant impact on our lives so the largest solar events on record occurred in 1859 and 1921 long before the advent of modern technology the 1859 event is popularly known as the carrington event and both these events triggered extensive power outages and caused significant damage uh to the communication network of that time which was the telegraph network and the carrington scale event missed the earth by just a week in 2012 very recently now the estimates of uh estimates for probability of occurrence of such extreme space weather events that directly impact the earth ranges from 1.6 percentage to 12 per decade but that's not all the risk is not uniform uh across the years because the solar activity goes through"
  },
  {
    "startTime": "00:26:00",
    "text": "cycles so here we have the variation in the number of sun spots across years from 1700 or so to until recently um and here we can see that the solar activity waxes and rains in cycles with a period length of approximately 11 years so during solar maxima there's an increase in the number of sunspots and hence an increase in the frequency of uh cmes and other solar elements and it's not just that the solar cyc solar activity also goes through a longer term cycle that is approximately 80 to 100 years uh so the peak solar activity very significantly across this hundred year cycle as well so this causes the frequency of uh solar events to vary by a factor of four across solar maxima uh in this hundred year cycle and if we zoom in we can see that modern technological advancement coincided with the period of weak solar activity in the uh past three decades now we are at the beginning of solar cycle 25 and the sun is expected to become more active in the near future uh but due to the absence of extreme activity in the recent past we have overlooked the impact of these events on the internet infrastructure so the bottom line here is that large event could happen soon and the impact is enough that we should prepare our infrastructure for it today we have very limited understanding of how this would affect our internet infrastructure so how are various internet components affected so let's start with a wired infrastructure localized infrastructure such as data centers can be protected from voltage surges caused by cmes using transient voltage surge suppressors"
  },
  {
    "startTime": "00:28:01",
    "text": "which are really relatively inexpensive so they are mostly safe now long distance cables constitute the most vulnerable confidence in the internet infrastructure so today we know that long distance land and submarine cables carry signals in optical fibers and this optical fiber itself is immune to induced currents uh because it carries light and not uh electric current but these cables also have repeaters at 50 to 150 kilometer intervals and uh which are connected in series and powered by a conductor that runs along the length of the cable and this conductor is susceptible to damages from induced currents so today we don't have any good failure models for understanding the failure characteristics of long distance cables uh but the expected induced voltages along the length of the cable could be uh one or two orders of magnitude higher than what the power system associated with the cable today can is capable of handling and today's cables have not been stressed tested on the uh induced voltages that could be caused by a carrington scale event so long distance cables are in short vulnerable internet routers can be protected from direct voltage surges using voltage suppressors so most of our localized infrastructure can be protected so they are safe now moving on to wireless infrastructure um satellites are directly exposed to solar storms they generally have radiation shielding for protection from high energy electrons but with very strong storms uh can cause electrons to penetrate deeper into the interior regions of satellite and damage its electronic confidence"
  },
  {
    "startTime": "00:30:02",
    "text": "and uh solar storms can also cause drag on satellites which can lead to uh satellites losing their orbit and re-entering the atmosphere and burning up so very recently uh there was news on 40 starling satellites that were lost to geomagnetic storm when a small scale storm hit uh but the the satellites were just being deployed and they were at lower altitudes where this risk of drag is much higher um and that's how they were lost so satellites in general are vulnerable and um and this is well known uh cell towers are protected from direct exposure because they're on the on the ground and protected by the atmosphere similarly personal devices such as laptops laptops and mobile phones are also safe so to summarize long distance cables and satellites are the most vulnerable components of the internet um other components could suffer from power outages but they are not susceptible to direct damages now analyzing the uh impact so to understand the impact on internet infrastructure i looked at a broad set of data sets uh comprising of various internet components so uh the submarine uh cable map uh consists of submarine cables in connecting various continents and then the itu cable map contains land cable information from across the globe collected from regional entities uh inter tubes data set includes long-haul fiber cables in the us and then i also looked at publicly available internet exchange point locations dns root server locations and finally router locations"
  },
  {
    "startTime": "00:32:00",
    "text": "with autonomous system mapping from kaira and and here we look at the distribution of summary in uh cable endpoints so we know that uh higher latitudes are more vulnerable to uh induced currents from solar events particularly latitudes above the 40 degree threshold so above 40 degree north and below 40 degrees south so we evaluate the distribution of infrastructure components in this region so here we have the probability density function across various latitudes plotted on the x axis so we look at the distribution of population and summary cable endpoints and we see that the population is more concentrated on lower latitudes while submarine cable endpoints are concentrated on higher latitudes especially between the us and europe uh we see a higher concentration of submarine cable in points at more vulnerable regions now uh next we look at uh distribution of various internet infrastructure components across the way across the latitudes so here on the x-axis we have the latitude threshold and on the y-axis we have concentration of infrastructure component the internet routers xps or dns root servers above that threshold so the most vulnerable region is above the 40 degree threshold which is denoted by the dotted line here and here we can see that only about 16 percent of the population is in this region but 35 to 45 percent of the internet infrastructure is in the vulnerable"
  },
  {
    "startTime": "00:34:00",
    "text": "region and this holds for other components as well you can see more analysis in the paper so the key takeaway here is that internet infrastructure components are skewed towards more vulnerable regions next we look at um cable length analysis so here on the x-axis we have a cable length in kilometers in long scale so this is the this is a comparison of cdfs of length of lan cables versus submarine cables here the submarine cable data set is complete uh it it comprises of almost all existing submarine cables uh the lan cable data set is not complete but if this was the largest publicly available data set so here um shorter cables don't need repeaters only cables have a longer than 150 kilometer need repeaters and need that conductor along the length of the cable which is susceptible to the damages so here we observe that uh more than 70 percent of the land cables don't need repeaters and hence are not vulnerable but only about um 20 percent of the submarine cables are shorter than this threshold which means that nearly eighty percent of the submarine cables need uh repeaters and hence are susceptible to damages from induced currents so in short submarine cables are more vulnerable than land cables and next we also look at we also conducted a repeater failure analysis based on latitude based failure models so there were several interesting observations the u.s faces a higher risk of losing"
  },
  {
    "startTime": "00:36:02",
    "text": "connectivity to europe during a solar superstar so in the east coast most cables between the us and europe are concentrated between the northeast and the northeast of the us and the uk which is most likely done for lower latency and there are no connections from florida to southern europe uh which is in the less vulnerable region um but when we look at cables between brazil and europe um they're less vulnerable so it was surprising to find that the cable between brazil and southern europe is shorter than all cables between the u.s and raw europe and it's also located in the less vulnerable region closer to the equator in asia singapore has a higher chance of retaining connectivity to neighboring countries even under severe storms because cables are much shorter and also located in lower latitudes that are less vulnerable and here we have uh locations of public data centers across the globe red shows uh higher density followed by orange and then blue so we observed that data centers are concentrated in um vulnerable high latitudes and this observation also holds for hyperscale providers such as google facebook microsoft etc um now while data centers won't suffer direct damages access to these data centers can be affected uh during a solar event what what other infrastructure components so here we have some results on analysis of on over uh dns and the domain system and the autonomous systems connectivity um dns root servers are highly distributed"
  },
  {
    "startTime": "00:38:01",
    "text": "and hence they'll remain reachable even under very high network partitioning however location data on top level domain servers and other authoritative servers was not available so that analysis has not been done yet but root servers are highly distributed and reachable when we analyze autonomous systems we see that a large fraction of ass have a presence in vulnerable regions but the vast majority of asses have a smaller spread so uh next i discuss some of the open questions and challenges related to this thread so the paper my work primarily looked at internet partitioning based on submarine cables alone um understanding the impact on end-to-end behavior of applications remain an open challenge this needs to take into account both land and submarine cables and also better failure models for cable failures so currently there are some geophysicists working on developing failure models for um internet cables um now the primary analysis only looked at dns root servers to understand availability of the infrastructure we need to analyze the distribution of top top level domain servers and authoritative servers uh the extent of caching etc uh to understand uh the resiliency of the entire system another interesting question is how is as level connectivity affected by large scale network partitioning next we need to devise solutions for"
  },
  {
    "startTime": "00:40:00",
    "text": "improving long-term resilience of the infrastructure for example this could involve um adding new cables in locations that are less vulnerable uh another interesting direction could be the design of inter-domain protocols or extension of current ones to improve path diversity in the wide area network during um disasters um another interesting thing with cmes is that uh the lead time so cmes that originate at this uh on the sun will reach the earth only 13 hours to five days later so this gives us a short interval to prepare for impact so how can we use this lead time effectively we can develop plans that allow us to prepare for an impact so maybe we can cache data across data centers uh globally in an effective way uh how can internet service providers react do we need to increase cache time in the dns system so there are a lot of interesting questions on how we can use this lead time effectively and um more importantly we also need to rethink our models of failure analysis so current best practices on fault tolerance and resilience evaluation rely on failure models that consider a limited number of failures so we need to test our systems and applications under large scale failure scenarios um um another interesting uh direction for research is related to uh reconnecting a partition internet uh after a solar superstorm the internet may be partitioned or uh due to failures in long distance cables or power outages in certain parts of the globe now um can we look at alternative solutions that can provide temporary uh"
  },
  {
    "startTime": "00:42:02",
    "text": "connectivity after an impact so examples could be uh internet collected balloons or high altitude platform stations that are mobile and um ideally these solutions should rely on renewable power sources such as solar energy to guarantee connectivity even during power outages so that's another interesting direction of research finally internet and power grids are both uh designated as uniquely critical because all other critical systems rely on them they are also interdependent on each other and both are susceptible to failures from solar superstorms hence we need to study the joint failure characteristics of this complex in the dependent system um they also have very different failure characteristics for example if we consider the united states there are three regional power grids the western interconnection eastern in the connection and texas interconnect now if the power grid in the east face it only it will not cause any significant effects or overload on the western inner connection but on the other hand if all cables connecting to uh the east coast fail um there'll be significant shifts in bgp paths and potential overload in cables on the west coast so internet is more global compared to power grids and even regional failures can do something significant consequences for the broader internet so understanding the interdependency is an interesting question so these are just a few open problems there are many more uh now to summarize um so space weather particularly coronal mass ejections from the sun pose a significant risk to our internet infrastructure"
  },
  {
    "startTime": "00:44:00",
    "text": "and modern technological advancement coincided with the period of weak solar activity so the internet infrastructure has not been stress tested under strong solar events and the complete um extent of this threat is yet to be estimated uh internet infrastructure components are skewed towards highly vulnerable regions in the higher latitudes and this can affect the overall resilience of the system based on preliminary analysis but a lot more work needs to be done in this context and finally uh we need to work towards better understanding and improving the resilience of the global internet infrastructure um with that i'll conclude my talk thank you and i'm happy to take questions okay thank you computer very interesting talk uh a really interesting problem you're looking at here um where's do you have a question i do uh so first off thank you for the excellent work um i'm a ham radio operator and we'll pass this on to some friends that are interested in it because we we consider that type of stuff all the time there's a number of ham radio operators in the atf um i'm also a root server operator and so one of the things that i'm interested in is is it's worth noting first off that the root instances or dns instances you know maybe that their particular installation does not you know pose a problem but if they become disconnected from the rest of the system then they will fail to get updates at which point dns will stop you know uh validating after you know a while as the signatures expire in the regardless of the zone um did you study islands or did you find you know isolated islands of communication where they would be functionally cut off i"
  },
  {
    "startTime": "00:46:00",
    "text": "mean the nice thing about the routing system is that you know if a major cable gets taken out but maybe there's a link to greenland to you know something like that that you can actually still get traffic through it'll become congested but did you were able to identify any uh particular areas that are completely shut off where you know they'd be disc uh completely cut off from communication for a while yeah that's a that's a very important question uh so i don't have a complete answer yet so my my initial paper only looked at uh primary regions of uh that will be affected but not the end to end disconnection on the complete graph uh so currently i have students that are working on this topic so i don't have a complete answer yet but um it could be possible that there are very big islands but not um not a lot of tiny islands it seems like uh so that based on the preliminary analysis it uh most of asia and so the asia to europe connection will probably stay but uh the us uh to europe might be more affected uh so uh complete disconnection uh i don't know at this point but a significant reduction in capacity between the bigger land masses is possible okay thank you anyone else have questions first in jesus a little bit of chat about vulnerability of satellites in in the chat um i don't know if you saw that"
  },
  {
    "startTime": "00:48:03",
    "text": "yeah so yeah the question on um uh the satellite impact on satellites so uh nicolas has there are regular solar storms and apart from startling accident with very low earth orbit satellites satellites are fine do you have pointers to justify the assumptions on satellites yeah so that's a very important question um so uh as i mentioned in the talks these satellites have protective covering so uh so uh typically they're deployed they have a children that can protect them from solar activity for their lifespan which is five to ten years or so so uh it was only in the past decade or two that the number of satellites grew really exponentially um and we have not had a very large storm since then um so the starling satellites were affected because uh after they were installed at the lower altitude than their final orbit um two back to back very small scale stormed uh storms happened so uh the solar storms are ranged uh thus the the uh intensity of these storms they are uh they range from g1 to g5 where g1 being the weakest and g5 being the strongest so the storms that hit the starling satellites were g1 which were really weak but it took down the satellites because the satellites were at a lower altitude than their final destination altitude now if a g5 which is the most intense storm happens we don't know the impact what the impact would be um at even the current uh orbital altitudes so uh the radiation shielding can offer some amount of protection but it's not"
  },
  {
    "startTime": "00:50:00",
    "text": "guaranteed that um they'll be completely protected and not just that um these satellites fall out of orbit very often and they have mechanisms to uh they orbit and to in order to do that we need to we need to have connection with the earth ground stations which can detect the current altitude and correct the uh satellite's position and the solar storm can ha um extend for hours maybe 10 to 12 hours in the case of varying then storms so if we lose connectivity to a satellite for such extended periods and they fall out of orbit uh it is possible that they could fault even though there are thrusters if we cannot communicate with the satellites they could get damaged so before the current international space station was installed u.s had um something called skylab uh which was the previous space station uh and that was destroyed in the 1970s during the uh large solar storm and they couldn't connect with it uh to push push it back up into the orbit so with very large storms that extend for hours it is possible that satellites could be destroyed but we don't we have not experienced something that large uh very recently all right does that answer your question or are there anything yeah so there is a question on uh is the impact focused on high latitude or sun facing side or are induced currents the same globally so when it comes to the direct impact on satellites satellites that are facing the sun are at a much higher risk but when we look at the induced currents these are caused by interaction of these magnetic particles with the earth's magnetic field so in the case of induced currents um"
  },
  {
    "startTime": "00:52:01",
    "text": "it's not just the sun facing side the dark side is also equally vulnerable but higher latitudes uh on both the sun facing side as well as the dark side are equally vulnerable okay thank you i i had a question i mean since this is an i etf and irtf meeting um is there anything we should be doing differently when we design protocols which would help with this type of you know help with resilience to this type of event yeah so uh i i think we need to rethink uh resilience at every layer of the uh stack so uh with dns as i said like we know that root servers are uh uh very very well distributed but we don't know how the uh entire uh tree would hierarchically would uh uh kind of fair under a solar storm do we need to change our caching intervals or uh change how we uh manage these dns records that's not clear and then when it comes to protocols it seems like bgp uh which allows only like a single path um that might be too restrictive when the capacity is severely limited so that's another analysis that we what we're planning to do like how how would um uh bgp affair uh in such a scenario uh but within an ais like um ospf or um uh the other entrance routing protocols will fare very well because they are decentralized and they can use whatever parts are available uh in a decentralized fashion so they seem mostly safe it's the in their domain protocol that needs more investigation uh yeah okay that makes sense i guess there's a whole bunch of coordination issues and management's issues with large-scale um"
  },
  {
    "startTime": "00:54:02",
    "text": "circuit click cloud cloud provider infrastructure networks and so on as well yeah yep so some interesting problems yeah okay uh are there any other questions for century should we move on to the other talk dtn over rfc 1149 is clearly the answer i assume that say being carriers okay uh i guess there were no more questions so uh in that case uh thank you sanduta excellent talk um [Applause] all right uh next up is uh bruce spang uh bruce there yes uh so the second award uh today goes to bruce spang uh for his work showing that networking algorithm a b tests uh can be biased because of network congestion uh bruce is a fifth year phd candidate at stanford uh studying uh internet networking video streaming and theoretical computer science uh he's also been a graduate research fellow at netflix for the past three years uh working on video algorithms uh and in in the past he studied at um umass amherst and also worked as a software engineer at fastly the paper he'll be presenting today is unbiased experiments in congested networks which was originally presented at the acm internet measurement conference last year um bruce you should be able to press the share preloaded slides uh button great uh let me just share some stuff slides not the screen just listen to you"
  },
  {
    "startTime": "00:56:02",
    "text": "instead just for your animation itself uh it works better if you can do it with the slides but so we can scream okay uh already when you uh great well uh thanks so much for the introduction um and i guess first off i wanted to start by saying thank you so thank you so much for this award um some of my all-time favorite research papers have received this award and so i'm really honored that our work is also receiving this award our work is about running experiments in congested networks um and this is some joint work with veronica hannan sravya from netflix and my advisors nick mckeon and ramesh jahari from stanford so we're talking about networking research and it's very common networking research that we come up with some new algorithm and we want to argue that this new algorithm is better than the existing state of the art so we've got a pretty typical way that we do this and normally we come up with some intuition about why the new album is better maybe we run some lab experiments that show specific instances where the new algorithm is better and then finally we'll go and we'll run a production a b test so like a randomized experiment on actual traffic where we try out our new algorithm and see if it actually works well in practice so this is a pretty standard way to evaluate algorithms on this slide are a couple of papers from the past decade or so where they rely on an a b test to argue that their approach is better than the state of the art and so this is just a subset of the the papers that have been published and also there's lots of unpublished work so the team that we work with pretty closely at netflix they run an av test to test basically every single change they make to the production algorithm and probably over the course of the year they'll run a couple thousand ap tests so what is an av test"
  },
  {
    "startTime": "00:58:00",
    "text": "so in a b test we have a whole bunch of users and so there's normally a bunch of smiley faces on this slide so imagine a bunch of smiley faces here these smiley faces are going to be units and these units are something like users they're like video sessions they're servers and what we do is we randomly assign traffic to either treatment or control we wait for a while we collect some data and then we look at the outcomes and so the outcomes maybe in this case the treatment algorithm gets better performance than the control algorithm and so we say okay we've run a randomized experiment we found that the algorithm improves performance so this is a generalization this statement that an algorithm improves performance we're kind of making a statement about what the world would be like if we went and deployed the algorithm based on the results of an a b test and typically an eb test is like a pretty small scale experiment maybe one percent or less than one percent of global traffic so this is a fine sort of generalization to make but it's a generalization that requires some assumptions and one of the big assumptions is that the outcome of one unit in the test doesn't depend on the other units in the test so for instance like what happens to me when treatment is applied to me doesn't matter if treatment is applied to you so this assumption this is called interference and when interference is present units sort of interfere with other units in the test and this can bias the outcome of a b tests so this is sort of a hot topic in an area called causal inference which is a field of research where we think about like experiments and how we know what we know is true is true and so let me give you a couple examples of this sort of interference so let's imagine you've got some social network and you've got like a messaging application for this social network and you want to test some new feature for this messaging application if you randomize users to treatment and control and this this feature does"
  },
  {
    "startTime": "01:00:00",
    "text": "something that like increases usage for the treatment group then uh maybe this user who's in the treatment group goes and messages all their friends who are in the control group and this will sort of increase usage for both the treatment and control groups and this will bias the outcome of the test another example is from online auctions or markets and you can think of like ebay or airbnb here if you've got users that are randomly assigned to treatment and control and then they compete against each other in some sort of auction or to buy some sort of product then if you do something that makes treatment users more likely to win this auction then it makes control users more likely to lose this option and again you have this interference between treatment and control there's a lot more examples of interference out there um we've got a link to all sorts of papers in the in the presentation or in the paper so if we think about it for just a little bit i mean this is the rtf we're talking about networking it should become pretty clear that interference exists in congested networks so we've got treatment traffic and control traffic in an av test and this traffic shares the internet it shares networks in the internet it shares cues in the networks routers and the networks and links in the network and there are things that we know from say congestion control research that the treatment algorithm can do to affect the network so it can do things like increase or decrease the length of the queues in the network it can do things like use more or less bandwidth and if it does this it'll impact the control algorithm like the control algorithm could see the longer queues or could have less bandwidth available to it so interference clearly exists in congested networks but the question is sort of raises two questions the first question is like does this matter so maybe there's a little bit of interference a little bit of bias but it doesn't really change the outcome of the test we run at all or maybe it does and maybe it actually would sort of significantly change the decisions even we would make as a result"
  },
  {
    "startTime": "01:02:01",
    "text": "of these tests so the first question is does it matter and if it does matter then what can we do about it is there any way we can avoid this bias and so in our work we try and answer both of these questions let me start with the first question and tell you about an experiment we ran that shows that a b test can be extremely misleading if there's congestion interference present so this experiment we ran in cooperation with netflix and it was an experiment involving bit rate capping so first let me tell you about what bit rate capping is so back at the beginning of poke 19 everyone started staying home load on the internet went way up and governments around the world worked with large video streaming services like netflix to reduce load on the internet and the way they did this was by capping click rates and overall this reduced the load that netflix was sending out by about 25 so what did this mean technically so netflix videos are split up into segments and each segment you can think about as a couple different seconds of video each segment is encoded at a number of different qualities there's a very high quality one which looks good but tends to be pretty large in terms of bytes then there's a range of qualities that goes down to some low quality which is looks less good but is like much smaller and faster to transfer so an algorithm goes and it can pick each quality level at each segment and then at the next segment it can pick a different quality level for bitrate capping what netflix did was just stop serving the highest quality levels so they just got rid of those and so for the same sort of seconds of video the number of bytes you sent would be lower and there's a whole bunch of technical details here about how to do this while respecting the various like plans that people had paid for and the quality that netflix had promised and you can read more about that in the paper if you're interested but at a very high level netflix limited the quality of video and this reduced"
  },
  {
    "startTime": "01:04:00",
    "text": "the overall traffic so this is a an algorithm that we could test out and i think it's a pretty natural question to ask is like okay so let's say we did run an a b test for this algorithm what would an a b test look like so let's imagine we've got some congested link so this link up here on the top left and we're thinking about a congested link here because the whole point of this bit rate capping thing was to reduce congestion in the first place so let's imagine we took all the traffic to this link and capped this traffic so this would reduce the bandwidth usage and let's say in this case it will reduce the bandwidth usage by half and what this means is that now there's some free space available on this link and the link will become not congested so there's sort of two treatment effects here of bitrate capping that we're interested in measuring the first one is how much less bandwidth we're using and then the second one is how this impacts congestion so if we were to go and we were to run an a b test one possibility is that so we've got this captain control traffic which is sharing a link the cap traffic could sort of reduce the data that it's sending the control traffic could be not affected and there could be some free space available on the link and in this case the link would be not congested so in an a b test the results that we would see is that the cap traffic would use less bandwidth than the control traffic in fact here it uses the correct ratio of less bandwidth so it's using half as much bandwidth as the control traffic but what we wouldn't see is the impact on congestion because we would be comparing the cap traffic to the control traffic and they're using the same link which is not congested and so we would see similar metrics for congestion we'd see similar packet loss we would see similar queueing delay and so on a second possibility if we were to run an a b test is that the control algorithm could be running some sort of congestion control algorithm it could be doing some sort of adaptive bit-rate algorithm and it could notice that there's some free space available on the link and it could start sending faster"
  },
  {
    "startTime": "01:06:01",
    "text": "or higher quality data and fill up the rest of the link in this situation the link could stay congested and in the result of an a b test what we would see is that the cat traffic is using less bandwidth sort of directionally we've got the right direction but here we will get the wrong ratio so we'll see that it's using about a third as much bandwidth instead of half as much bandwidth and then secondly we're also going to miss this impact on congestion because both again the captain control traffic are going to be using the same link which is going to be congested and so in both of these situations when we run an a b test we're not going to see what happens with congestion and this is sort of a huge bummer because the whole point of this algorithm in the first place was to reduce congestion so now i'd like to tell you about an experiment we ran to measure congestion uh to measure the sort of actual effect of bit rate counting and in order to do that i need to describe the experiment i rented to you and in order to do that let me introduce another way of thinking about the same thing that might make the experiment design a little bit cleaner so let's imagine we're in this setting back here on the right hand side where the control traffic is going to respond to the amount of cap traffic and sort of fill up the rest of the link so in this setting we can visualize the behavior as follows so on the x-axis here is the fraction of cap traffic on the y-axis is a measurement of per session throughput and so we've got a bunch of different video sessions here that are sharing one link and so we can look at a per session throughput for those video sessions this line here this is going to be the throughput for the cap sessions and it doesn't depend on the fraction of traffic which is kept because there's some sort of upper limit that we send at and as we increase the fraction of traffic which is capped there'll be more space available on the link and so we'll continue sending at this upper level this line here this is the throughput for the control algorithm the algorithm which is not doing bit rate capping and as more and more traffic is kept we're going to be back in the setting where"
  },
  {
    "startTime": "01:08:00",
    "text": "the control traffic is going to increase the throughput that it gets so what we're interested in measuring here is what happens if we were to go and we were to deploy a bitrate captain so that's the difference between 100 of traffic is capped which is over here on the right hand side 100 of traffic is running the control algorithm which is over here on the left-hand side and the difference between these two points we call the total treatment effect and this is sort of the quantity that we're interested in measuring if we were to go run an av test we would not necessarily measure the total treatment effect instead what we would do is we would pick one point on the x-axis where we allocate some fraction of traffic to treatment and control and then we would compare the two points vertically along this line so we could run this 50 a b test and we could look at the difference between cap and control along this line as you can see from the graph this is just uh one point and it's a might be a biased estimate of the total treatment effect so here there's some bias and this can make our estimate of the total treatment correct incorrect so sort of the problem here is that we're just looking at one point and if we start looking at multiple points we can measure capping effects and compare this to the bias of ap tests so let's say we did the following let's say we could somehow run a five percent a b test and a 95 a b test simultaneously then we would get four points on this graph here and we could compare this like point on the far left over here when five percent of traffic is capped or 95 of traffic is not capped to this point over here on the right where 95 of traffic is capped so this is sort of when we're switching from most traffic not cap to most traffic capped and we can use this as an approximation of the total treatment effect and then we can compare that approximation to the results of these two a b tests and we could get an estimate of the bias"
  },
  {
    "startTime": "01:10:00",
    "text": "so this is going to be our goal one thing that is tends to confuse people about this is why here are we picking a five percent and a 95 a b test instead of say a hundred percent and a zero percent because the total treatment effect is the difference between over here on the left 0 and over here on the right 100 the reason we're doing this here is because we're also really interested in measuring the bias of a b tests and if we were to pick this allocation of 100 and zero percent we wouldn't have a b test to compare it to so this uh we thought was a nice compromise between coming up with an approximation of the overall effect of deploying bitrate counting and then also still having a b test that we could compare against so this is going to be our goal the real crux of measuring this though is going to be to run these two simultaneous experiments and the problem is like as soon as we start running this five percent test like it's going to be difficult to run a 95 test because kind of we're allocating the same traffic so this is where we got extremely lucky and so netflix has this content delivery network called openconnect and openconnect has a bunch of servers located all around the world that connect with different isps we found this one location that had a pair of very reliably congested viewing links to the same isps so there's this isp there were these two links and then there were two routers and sets of servers that were connected to those routers these two links were very very similar so we had the same set of servers connected to the links the same routers here the same isp the same content was available on all of the servers and when we learned and looked at historical data we saw like no significant differences between the two links and most of the metrics that we cared about so these two links uh were both congested both pretty separate from each other and so what we could do is we could run one a b test here on these treatment servers and look at how it impacted the congestion on link one and"
  },
  {
    "startTime": "01:12:00",
    "text": "then we could run a separate test on the control servers and look how it impacted the congestion on link two and so this is what we did we went on link one and we ran a test where ninety-five percent of traffic was capped and on link two we ran a test where five percent of traffic was kept this gave us those two experiments those four points from before and we could look at and compare the difference between these two things so now let me tell you about the results of the experiment so this is a similar graph to what we saw before on the x-axis is the fraction of traffic which is kept on the y-axis is a measure of normalized throughput in this experiment what we saw was in each a b test we saw about a five percent decrease in the throughput so capping decreased throughput by about five percent but overall when we compared these two treatments we saw an improvement of about 12 percent in throughput so when we switched most traffic from not cap to cat throughput went up by about 12 so the important thing here is that the av tests are biased and they're not just sort of like kind of biased like even the direction of improvement is wrong so this is i think uh surprising and one thing that might make this a little bit clearer is if we look at the behavior of throughput as a time series so over here on the left is a time series of throughput before the experiment we've got two links and as you can see the two links have pretty much identical throughput over time over the course of the entire day during these peak hours which we shaded here these are the hours of the evening load increases at some point congestion sets in and when congestion sets in throughput decreases pretty significantly and then it goes back up to normal so during the experiment we again have these two links and this time the links are split between capped and uncapped traffic again we see during off-peak hours the"
  },
  {
    "startTime": "01:14:00",
    "text": "links are both pretty similar but during peak hours we start to see a difference between these two links so these top two lines here this is uh the first link where 95 of traffic is kept and the bottom two are the other link where five percent of traffic is capped because we're doing capping here uh there's sort of less data that we're sending and in order for the link to be congested we're going to need higher load and that higher load will happen later in the day so when we cap we delay the onset of congestion for the link which is count and we also make congestion end earlier and so this means that between the two links the link which is mostly capped is going to have sort of less congestion and higher throughput but within each link within the a b test the traffic won't see any difference yeah and so there's no difference in throughput between the links but there is across the links and this is why the a b tests are not giving us accurate estimates so that was for throughput we also saw this behavior for a couple other metrics that we were interested in one was round trip time and the other one was play delay or sort of how long it takes for the video to start up in all of these cases we saw improvements in the total treatment effect like capping improved these metrics but the a b tests sort of didn't even get the direction of improvement wrong they either reported that the metric would get worse or that the metric was not affected by the treatment and the the a b test was like extremely confident about this like you know you compute the 95 confidence intervals they're very confident that it's 5 to 15 worse in an a b test when really it's 25 better so what this experiment shows is that a b tests are biased when they're running congested networks and this to us was pretty concerning because we use a b test really heavily to evaluate new algorithms and if they're not giving us even the right direction in some cases we're worried about this decisions we might make as a result of these tests"
  },
  {
    "startTime": "01:16:04",
    "text": "so let me be a little bit more specific about these risks here here's a pretty common development process you might use if you're using a b tests it's pretty similar to the process that the team we work with pretty closely at netflix is using so maybe you'll come up with some idea you'll implement that idea you'll run an a b test for that idea and you'll get a sense of how the idea performs and then you'll iterate these steps one through three until you'll come up with something that works pretty well and then you'll go and you'll deploy that idea so the risks here is that when you run an a b test you don't get an accurate estimate of what happens when you deploy the idea and this can have a couple of different consequences within the development process it means you could give up too early on an idea that's good that would work well in production just because you don't see it work as well in the a b test or you could continue on with an approach that doesn't work like when you go and you deploy it but seems to work well in the a b test when you finally deploy the thing you could deploy something that doesn't work as expected and in the worst case this could cause a production impacting issue or in probably a more typical case you might just have to go through this process many more times and improve the algorithm so so those are the risks but another thing we saw is that we can run experiments that help reduce and remove this bias so this perdlink experiment we described is just one example and there are other sorts of experiments you can use that are also less impacted by bias so i'd like to tell you about two of these experiments the first kind of experiment design is called an event study in an event study you have some event and then you compare what happens after the event to what happens before the event so in this case on the right we can look at bitrate capping the event here is going to be sort of switching to 95 bit rate capping and we're gonna do that say right here on friday so what we would do we would switch on friday to 95 capping and then we would compare after and before"
  },
  {
    "startTime": "01:18:01",
    "text": "this would give us an estimate of the total treatment effect and if we switch from say five percent capping to 95 capping we can also compare this to the results of the b test and get a sense of whether or not the a b test was biased another good thing about this sort of experiment design is it's pretty common folks might already be doing this when they deploy things so if you go and you deploy a new feature it's pretty common to look at the metrics before and after the deployment and see how it the deployment impacted those metrics when you do this you can compare those metrics to the a b test and you can start to build up some intuition about whether particular changes are impacted by congestion interference the big problem with event studies is an issue called seasonality and this is basically so you look after some event and compare it to before that event but there might be other events that happen around the same time and you're not going to be able to rule out things that were caused by that event so in this particular experiment one event here is that we're switching to capping and another event is that it's suddenly the weekend and so we're not going to be able to disentangle here the difference between things that are caused by the weekend and things that are caused by capping one way we can deal with this is with a switchback experiment this is another sort of experiment design where instead of just switching once we switch back and forth between treatment and control so maybe on this day we use capping on the second day we use no capping and then we switch back and forth we compare all the days where we're capping to all the days where we're not capping and we use this to estimate the total treatment effect um so again this gives us an estimate of the total treatment effect and this is going to be more robust to seasonality issues because sort of in order to get the bias here from other events those events need to line up with most of the switches throughout the experiment and especially if you randomize the days on which you are using treatment or using control this can help avoid these"
  },
  {
    "startTime": "01:20:00",
    "text": "issues the major con here is a carryover effect and this is when sort of the fact that i'm doing capping towards the end of thursday is going to change the initial conditions that you see on friday and then maybe the whole path of the friday say throughput in this case is going to be different than you would see with control so this is um this is certainly a risk but it's sort of a risk that depends on your system and if you're running experiments i think you may be pretty familiar with like the carry over the sort of behavior the way your system changes over time and you can design this experiment accordingly so those are just two sort of experiment designs you can use to avoid some of the impacts of congestion interference and so i'd like to conclude and say there's a lot more that we can do here so i think what our work showed is that if you have an a b test and this a b test uses a congested network this a b test has the possibility of being biased there's this pathway for treatment and control to interfere with each other via the use of their shared links what we don't know is sort of how much this bias matters we don't know if the bias is going to be a small fraction or a large fraction especially in experiments that you might be running like we know what it is for this one particular experiment but we don't know what it is for all experiments and so we would encourage you to go and run some measurements if you are running experiments to see if this kind of interference matters for your experiments and i think personally one thing i would really love to see is new papers and new proposals talking about total treatment effects and sort of reporting the results for instance when they go and they deploy the algorithm instead of just the results of a small scale a b test i think mainly what this work highlights is that we have a need for better experiment methodology for networks"
  },
  {
    "startTime": "01:22:00",
    "text": "so our the experiments we've proposed are just a couple of different ways that you could hope to run experiments to measure the total treatment effects and to be honest their experiments we took pretty much without much modification from causal inference work and other experiment literature i personally think that if we were to sit down and design experiments for networking algorithms using what we know about networks we could probably come up with more sophisticated experiments that could give us better results and like more confidence and better estimates of the total treatment effects and personally i'm really excited to see the kind of algorithms we could build the kind of results we could get by using these experiments so thanks okay thank you uh excellent talk um does anyone have any questions uh janna i think i'm in okay um thank you bruce that was a great talk and good work and hi by the way it's uh good to see you presenting at the irtf i will say that uh this is very very eliminating um at a minimum i think it's it's it's something that as you say everybody should take into account when doing these experiments and it's great to have the data to back it up and to argue that simple small a b tests which i've relied on for a long time aren't necessarily good enough but i was going to poke a little bit more at the thing that you just said which is the um the the uh dominance of the bias that you see there um in particular one thing that occurs to"
  },
  {
    "startTime": "01:24:01",
    "text": "me is uh i don't know if you've looked into seeing if you made like for example a client sticky for a particular a b test as against i don't know how exactly i haven't read the paper in detail but how exactly the uh choice was made for serving particular type of content but i imagine that i guess where i'm coming from is that the it depends on whether bottleneck really lies if the bottleneck is a shed the bottling has to be a shared bottle like in this particular case for the the control in the experiment groups and if as is commonly the case the bottleneck is closer to the user and not like it appeared appearing link or some other place then in that particular case i imagine that if the user is in a bucket that is either control or treatment then the user would basically effectively be at the far end of your of your scale right like for you by seeing that effect um have you looked into that have you were your experiments sticky to users yeah so this is a good point that um there are things you can do on the allocation side to kind of avoid some of these issues so the experiments we ran were sticky to users um although it's a little bit tricky because it's like the users that are going over these two different paths um but the the sort of overall point you're making is is true that like if you um sort of can guarantee that the users are not going to share any resources with each other over the whole way then you're going to be able to avoid this bias because there's no pathway for them to interfere with each other and if you sort of believe that all of the users are not sharing any of their bottleneck links with each other then you've sort of avoided some of the possibility for bias we didn't explore this too much because we found it pretty hard to measure whether or not users were sharing links and we didn't have a good sense of like how often this was um"
  },
  {
    "startTime": "01:26:00",
    "text": "but i guess okay so if you were to allocate users instead of sessions my gut instinct would be that this is an improvement over just allocating sessions in terms of interference another thing you could do is you could allocate networks or particular servers or sort of try and reduce the probability that there is sort of treatment control sharing the same link and if you do this i think this will reduce the bias that you would see in the experiment yeah that's very helpful um i was asking this because if you actually did the experiment by trying to if we try to uh it could tell you something about where users end up sharing bottlenecks as well if you actually try to do it the way that you are suggesting any of these ways those ought to make things better when the bottleneck is closer to the user versus not and those experiments can actually shed some pretty interesting light on exactly where bottlenecks are on the internet yeah absolutely you again uh thank you jenna uh brian do you have a question is this hi brian trammell not speaking as um sort of i guess a solar flare backup chair um so there was something that you said earlier that that was i think kind of fundamental and really interesting that i want to repeat back if we use what we know about networks to design experiments about networks we can get you know we can get a lot smarter about this and then i was looking at the graph that you had of the switchback experiment and what it what that reminded me of more than nothing else was like a diagram of time division multiple access from sort of like a physical layer textbook that i looked at"
  },
  {
    "startTime": "01:28:00",
    "text": "in college right so a b tests are basically code division partition of the space switchbacks are time division partition of the space like all of the stuff that john was talking about were other sort of ways to split the space with different sorts of ways to code it are there other bits like ways to use this multiple access metaphor to find other ways to split the space in order to figure out like how to split this apart because like my sre mind basically wants to say well we could basically automate this code partition in a whole bunch of different ways and then you know watch which things show up in the power spectrum which things get amplified out of that but i'm wondering if there's a more fundamental way that you could split this up in order to be able to i'm not expecting an answer to that question right now um but um if you have one it'd be great but yeah it's a super interesting question i don't have an answer up hand but it's yeah it's really interesting to think i would be interested in following up with you on an answer to that question because i think there's something like really kind of deep there yeah i'm happy to chat about it definitely um i can share a little bit about other experiments that people run and maybe this will be interesting um so one thing people do in social networks that i think is pretty cool is you would um sort of allocate a user and then all of a friends users to a particular treatment and you sort of think about this graph structure and how you can allocate based on this graph structure in order to get optimal behavior and so maybe there's something cool we could do with that for networks yeah uh try something uh yes this is jonathan morton and um i think the one of the big lessons to take away from this is how important it is to understand the uh the testing methodology in detail"
  },
  {
    "startTime": "01:30:02",
    "text": "when we look at a set of um of purported results particularly ones that are used for marketing so the fine details of the testing methodology can have a big effect such as exactly how this a b test may have been done and whether the uh the potential bias that you've just highlighted um has been mitigated in any way yeah definitely okay uh apparently there may be problems in the room but hopefully you can still hear us if not see that you can still hear us okay good good okay um so i guess um i guess uh i it's the same sort of question i used to send you to earlier which is uh you know obviously you're talking to the the ie etf and and the irtf um and you said there was a need for for better experiment methodology um what it what if anything should we be doing um when we're designing protocols evaluating protocols to to to make sure the evaluations and the critical designs are right is there some sort of general guidance we should be providing or is it read this paper and think about these issues i guess uh overall what i would say is i think all the stuff we're doing today is good and we know it's good because we like we build good systems with the the"
  },
  {
    "startTime": "01:32:00",
    "text": "stuff we do today and i think what this gives us is like another tool to think about how the algorithms work and try to measure like the way these algorithms work and so i guess kind of when designing new algorithms like in the itf i would think about like um the way the experiments we run to validate those algorithms can be biased and then i would try to think about other ways of running experiments that might reduce that bias yeah okay that makes sense make sense all right are there any other questions for the speaker okay uh i guess not so in that case thank you uh bruce thank you [Applause] uh and uh again thank you to to both of you and congratulations on on the awards um i i do do wish uh we could present these in person but uh i'm sure we'll uh find ways of of getting the awards to you um uh i'm also sure that uh if people have any any further questions uh both sim peter and bruce will be happy to answer them and to discuss the work further uh so again uh congratulations to both of you um we have a few minutes left in the session um if there's any um other questions or if anything people want to discuss uh i guess we have time for any other business um but uh other than that uh i think we're at the the end of the published agenda so there's anything else people want to raise do you join the queue"
  },
  {
    "startTime": "01:34:05",
    "text": "going once okay in that case i guess you all get 20 minutes or so back uh thank you everybody congratulations to to bruce to sangeeta and some really nice uh presentations there and uh i will look out for you in the rest of the week uh and perhaps in in philadelphia thank you everybody good you"
  }
]
