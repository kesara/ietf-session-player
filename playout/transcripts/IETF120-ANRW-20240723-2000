[
  {
    "startTime": "00:01:59",
    "text": "H-mm-mm"
  },
  {
    "startTime": "00:02:11",
    "text": "Hello, everybody Hope everybody had a nice lunch and had enough time to grab a coffee And to accompany that coffee, we have a Sabbath Katanakis, who is going to be presenting investigating location-aware advertisements in EniCAS IP networks. And with that, Sabas, the room is yours Thank you very much I can hear myself, but I can fix that that So, we hear you. Okay, okay So we hear you. Hello. Okay, okay. The problem is that I can hear myself, but I'll just volume down a little bit. OK, so hello everyone. My name is Alex Kastan and I am a PhD candidate in Lancaster University under the supervision of Professor Nira Tsuri and Dr Vasilios Giotas. This is a joint work with Dr. Iwana Lvadari from Simulamette, in which we investigate location-aware advertisements in any Kastipi networks. Over the last four years, we focused our research on trying to improve the well-known poor predictability of inter-domain routing simulation So in our first work, published in IMC the last four years, we focused our research on trying to improve the well-known poor predictability of inter-domain routing simulations. So in our first work, published in IMC-22, we identified and quantified the confounding factors that prevent us from conduct high fidelity simulations. Among those, factors, we pinpointed two things the complex routing policies that autonomous systems configure and deploy on the inter-domain routing system, as well, as the geolocation agnostic BGP best path selection process. A year ago, we studied the first confounding factor, which is the actual routing policies of autonomous systems on the internet and in that work, published in IMC 2023, we observed that currently AASs do not configure the route policies based on the business relationships with their neighbors, which is an assumption that holds"
  },
  {
    "startTime": "00:04:02",
    "text": "in controlled experiments and simulations but they prefer more expensive routes based on their performance, security or traffic engineering goals In this work, we focus on the second important confounding factor that we identified, which is the geolocation agnostic BGP best path selection process and we study the prevalence of regional trends and selective announcements in Enigas IP networks networks So Ennecass is one of the fundamental modes of communication in which a set of Enkast replicas, also known as point of presence or pops, all serve the same content under a shared identity In IP EniCast in particular, pops at multiple geographic locations advertise the same IP address using the Border Gateway Protocol and clients are routed to a replica or POP based on the underlying BGP routes And from the client's perspective, all of these POPs are address using the border gateway protocol and clients are routed to a replica or pop based on the underlying BGP routes. And from the client's perspective, all of these pops offer equivalent services Now, what makes IP Enigast attractive when deploying a globally replicated service is the mental model that it seems to permit. In particular, as a network operator, as an autonomous, system, adds more Enkast replicas in locations with many clients, it is believed that the over the other latency will decrease and load from near clients will be evenly distributed across the pops. As an example, the blue Enkast network in the bottom of the figure that you see has replicated its content to three pops, one in the America, one in Europe and one in Asia and if everything was working based on our mental model of Ennecast routing, then the orange user would be directed to the orange pop, the purple user to the purple pop and the green user to the green pop As we said, though, Enika Strouting depends in practice on the border gateway protocol or BGP to add advertise the same prefix from all pops to the rest"
  },
  {
    "startTime": "00:06:02",
    "text": "of the autonomous systems on the internet However, BGP is not guaranteed to be optimal internet of bandwidth, latency or geographic proximity In the best case scenario, BGP can be relied for connectivity, reachability and policy compliance And although user to site mapping, generally follow geography, prior studies have so be relied for connectivity, reachability and policy compliance. And although user to site mapping generally follow geography, prior studies have shown that the actual network topology can vary and research observations have shown that the mapping depends heavily on the policies of many networks on the inter-domain routing system thus can be a unexpectedly chaotic and unfortunately other prior studies have shown that IPNECAS performance does not match even the most basic expectation of network operators since clients are often routed hundreds of kilometers away from the closest replicas, resulting in increased latency, which is counterintuitive with the parallel of Enika Stroughty. And it has been known for over a decade, approximately 15 years that IP Enncast can be inefficient, yet there are surprisingly few explanations of why or how to fix it. But this explanation include the complex routing policies of the networks the geolocation of the replicas, and the traffic load of those pops, those replicas Now, to make things a little bit more complex, an autonomous system, either Enigast or unique, might select to restrict the propagation of certain routes for traffic engineering and load balancing reasons. This phenomenon is called selective analysis announcements. In the Enigast use case specifically, an Ennecust AES might strategically employ selective analysis announcements because they want to load balance their traffic and costs to certain pops based on their location We have currently identified two types of selective announcements Selective announcements per autonomous system and selective announcements per location in which a network might selectively advertise their prefixes to a subset of its neighbors or to a subset"
  },
  {
    "startTime": "00:08:02",
    "text": "of its pops based on their location, respect Nonetheless, in this work, we focus only on the second type of selective announcements. And specifically what we try to do here, is, first of all, to measure the adoption of the selective announcement phenomenon across any Castaises, to identify the receivers of those selective announcements and then to measure the regionality of those receivers, or in other words, the degree at which those receivers are either regional or global networks Now the results of our work, even though preliminary underscored the necessity to take geolocation into consideration when we model the interdomain routing system so that we go to a more BGP aware geolocation aware BGP best passage selection process Now, I want to stay a lot in this slide. It's our methodology over aware, it's a location aware BGP best path selection process. Now I want to stay a lot in this slide. It's our methodology overview. We'll dive into the details of the method later on, but what I want you to see here is that we mainly identify the ANICAST ASI stayes and the prefixes that they advertise to the rest of the industry we infer which of those prefixes are selectively announced to a subset of the ENIC origin neighbors, and finally, which geolocate this subset of neighbors the receivers as we call them, in an effort to try and capture and uncover regional trends in Ennecastrouting so that we can later on use these regional trends and embody them in our community and refine the accuracy of the simulations Now, the first step is a pretty straightforward step what we do here is that we map any caste prefixes to the anycast origins and we leverage the BGP Tools API and the BGP Tools GitHub repository to make this map"
  },
  {
    "startTime": "00:10:00",
    "text": "Actually, it's a dictionary that you can see on the right of the screen The keys of the dictionary are the ASN numbers of the ANICAST AES and the values are the ENICAST prefixes originating by the respective network Now, in the second step, we use the BGP Stream API to collect the routing tables of all the vantage points that are available in the ripries and route views projects on the 1st of November in 2023 Now, these tools, these platforms are BGP data collection tools distributed across the globe, but from the BCT day, that those tools collect, we only can about the bold things that you see in the box, about the prefix and the AS path to that prefix. For those of you who don't know what an AS path is, it's a sequence of network a sequence of AES that are going to be traversed from a source autonomous system to a destination autonomous system and a destination prefix, respectively And since this study is focused on any KASIP networks, we narrow down our collection to any cast prefixes only, hence we filter out all the rib entries that do not include a prefix in the previous dictionary that we mentioned in the first step step Now, let's go to one of my favorite, the most interesting steps of this methodology, it's the inference of the selective and announcements. And to give you an example, what a selective announcement is, considering the following graph in this graph meta is a multi-home customer to both lumen and cogent and based on our mental model of how the internet works, the gaur export model a network will prefer customer routes over peer-or-provided routes because customer traffic generates revenue and provide traffic incurs a cost. Hence, we would expect Lumen to follow a customer route to reach a customer prefix because customer traffic"
  },
  {
    "startTime": "00:12:03",
    "text": "generates revenue. Nonetheless, meta who owns to any cast prefixes p1 and pt reach a customer prefix because customer traffic generates revenue. Nonetheless, Meta, who owns two anycast prefixes, P1 and P2, decide to propagate an announcement to both providers for prefix P2, but decides to load by balance its traffic and costs for prefix P1 by propagating an announcement only to cogent to their provider cogent, which later on is going to propagate that announcement to their peer Lumen Hence, when an end user behind Lumen, wants to access prefix P1, they are going to follow a p-route, they want through Cogent, rather than the less expensive route or the route that generates revenue for the company through META since they have not received an announcement by META to direct access that prefix. So in a sense, the definition of the selective announcements is that when an AES receives an announcement to an endcast prefix, through a more expensive route than what expected, then that prefix is called the selectively announced prefix We measured the prevalence of selective announcements across ENICASs ASEs, and we observed that 84% of the Ennecast networks announced at least one ENICAST prefix selectively, and in this CDF here, what we plot is the cumulative distribution of the selective announcements per selected announcers. And what we observe is that this red circle over there tells us that approximately 80% of the selective announcers advertise all of the RENCAST prefixes selectively, hence selective announcements is a common practice across EniCast net networks Now the main goal of this paper though is to group the receivers of these selective announcements per the geographical region of the end users in a effort to try and capture regional trends that might arise so that we can later go and embody them in our simulations and refine the accuracy"
  },
  {
    "startTime": "00:14:02",
    "text": "And we rely on the intuition that when any castrouting is deployed, the nearest pop site to the end user is going to attract the traffic. Therefore, when the client routes to an Ennecass prefix, probably this route is going to be a approximate to the origin AES in terms of geographic distance. For example, they might receive in the same geographic region. And as illustrated in this figure, the Ennecast AES in the bottom connects to different sets of AES in different pops and selectively announces its prefixes to four out of its six direct neighbors A bit of information that you need to know here is that large pops are typically connected to both regional and global providers while smaller pops are mostly and usually connected to regional AES know here is that large pops are typically connected to both regional and global providers, while smaller pops are mostly and usually connected to regional Aces. As a result, a client in Region A, which is a region of a pop with only regional, with mostly regional ASIs, is more inclined to reach the NICAS Enigast prefix through regional providers rather than global providers. And this information if encoded and incorporated properly, in the simulations, can allow us to infer more accurately the providers that would transit the traffic from a specific to a specific prefix prefix so to achieve that to identify virtually to a specific prefix. So to achieve that, to identify, first of all, to identify the regions of the end users and then to label autonomous systems as regional networks or global networks, we need to join relocate ASEs and at least in this work when we geolocate an AS we consider two dynamic dimensions, the prefix is that an autonomous system announces and its public peering locations And towards that goal, we extract country-level information for all prefixes announced by a network through MaxMind, through the RIPST API as well as extract the countries of its public peering locations through PeringDB, through the PiringDB API. Now, these datasets over here give us a glimpse of the geographic footprint"
  },
  {
    "startTime": "00:16:02",
    "text": "of an AES. For example, to the left, you can see that AS-18106 announces more than 90% of its IPV4 prefixes in Singings More than 90% of its IPV6 prefix in Singapore and Hong Kong. And to the right, we can see that the same AS publicly appeals with more than 50% of its neighbors in Singapore In order though to reduce that country, geographic footprint of its AES to the region-level granularity, and be able to label ASS as regional as global, in this step we leverage the United Nations region's data set, which is a data set that maps countries to the respective regions and continents. And for example, an autonomous system might geolocate in multiple countries such as Portugal, Malta and Greece, in the country level granularity, but the same AES will geolocate to southern Europe only in the region level granularity, hence in our analysis we can label it as regional and we leverage this country to regional mapping along with the previous ASG location data set to label an AS regional or global if more than 90% of its prefixes and more than 90% of its prefixes and more than reside under the same region Now, to bring it all together the last step of our methodology is to actually group the research of selective announcements per the end user region. And to do so, we extract all the direct neighbors of any Kastei-Sis per area Enncast prefix. Those direct neighbors are the penultimate AES in the AS paths that we collected in step two. Then we group the receivers of the selective announcements that we inferred in step three per the end user region using the country to region mapping that we compiled in step four. And the result looks"
  },
  {
    "startTime": "00:18:02",
    "text": "like the snapshot in the right where AS 102 announces an elicast prefix to multiple regions like northern America and Western Europe. Nonetheless, the receiver the list of receivers that we can see here differ from region to region, allowing us to come like Northern America and Western Europe. Nonetheless, the receivers, the list of receivers that we can see here differ from region to region, allowing us to capture regional trends and disparities Now, we leverage this data set to measure the regionality of the receivers of selective announcements And as regionality, we define the number of regional neighbors that receive a selective announcement over the total number of neighbors that receive that announcement So in the x-axis, you observe the regions of the clients or the end users per which we grouped the receivers. And in the y-axis, you can see the regionality degree of those receivers. So in these figures, we observe that selecting a regional app upstream provider or peer to carry the traffic to a specific prefix is a common practice across these ZENICAS, specifically in the use case of Cloudflare to the right traffic is routed to the endcast prefixes through region AES when the source locates in the Sapsarha Africa, the Southern Asia and the Western Europe in the than 45% of the times And when it comes to Southern Asia, more than 80% of the receivers are regional networks And furthermore, Amazon and Google exhibit more than 40% their neighbors basically, exhibit more than 40% of regionality levels when the source age locates in the sub-saharan Africa, the southern region and Western Europe, which indicates that in this regions and for these networks, there are specific regions trends that are followed On the other hand, though, AS is like fast-clim byte plus and zen layer, rely mainly on global Lies to carry the traffic to the rest of the industry since the regionality levels are low across all end users' regions"
  },
  {
    "startTime": "00:20:02",
    "text": "Now, as part of our current and future research work, we aim to reason about the factors that drive the need for selective routing towards regional neighbors. Some of these factors from what we believe might be the deficient of the lack of a centralized backbone by the Anikasteas, lower transit fees that risk regional providers offer against public and global providers, and strict regulatory conditions Yes. If you can wrap up fast, otherwise we can run out of time to make some questions. Ten seconds, ten seconds And strict regulatory conditions that might apply in specific countries so to wrap it all up, usually to site mapping, relies heavily both on the geolocation factor and the routing policies across AES. Large part of any castes, EAS deploy selective announcements for all of the NECA Enncast prefixes, and Ennecast networks often resort to leveraging regional providers to establish connection between the POPs and the end users. As a final remark, from our experiments, it is evident that network operators take different decisions in the different areas. So when we simulate routing scenarios, if we do not take the location into consideration, that might have implications to the accuracy in the future we simulate routing scenarios, if we do not take geolocation into consideration, that might have implications to the accuracy and the fidelity of our results. Thank you for your time to watch my presentation and I would be happy to take any questions Thank you very much, Abbas Do we have any questions from the floor? Maybe one question from my side will others think about it so do you geolo much, Sabas. Do we have any questions from the floor? Maybe one question from my side, will others think about it? So you relocate the prefixes, right? Would you locate the prefixes? at least for the first time? do you relocate the prefixes right um would you locate the prefixes at least for the first dimension would you locate the prefixes yes that's what might my question was a little bit of neat picking so do you geolocate every single IP? in a prefix? And if they are not all in the same location, how do you go around? that? It's not a problem that we deal"
  },
  {
    "startTime": "00:22:02",
    "text": "ourselves. It's a problem that ripe that API actually does, so that would take an enormous amount of machines that we don't have in our hands to do it. But if you go to the RipeStat API and find the MaxMind tool, you can see that they geolocate an AS per the prefixes that they announce So it's, we took it as a black box, if that answers the question Yeah, yeah, thank you very much. And we have a question from Matthias Wallyish. So a brief question regarding so you found different groups of let's say, CDNs that behave differently? I mean, can you go back to the slide? I don't think it was right before the conclusion Yeah, exactly. These three compared to the other three. And did you find? any other indicators that might give a reason why it behaves the way that you found so we're good have a large discussion on that. There are many reasons. For example, I'm there are some, we spoke with some network operators in a few months ago and they said that in specific regions they don't care about latency, they only care about Jeter So it depends on the business model of its autonomous system system but but if that was not just a short paper but a full paper, one of the things that we would try to do is group the this this regional trends per the AS type so that we can tell, for example, how an ISP behaves, how CDN behaves, how other types of networks behave but we didn't at least in this work. It would be interesting to the latest us to additional features, let's put that way. Okay, thank you very much. Yes, thank you for your suggestion All right, very well unless there are any other questions let's thank the"
  },
  {
    "startTime": "00:24:02",
    "text": "speaker. Thank you, so much. Thank you very much very much. Thank you for your suggestion. All right, very well. Unless there are any other questions, let's thank the speaker. Thank you, sobas. Thank you. Thank you very much And is the next speaker in the room? yes Thank you Click here just one second Yes. Okay here we are. So we have now Bernhardt again. Apologies again for mispronouncing the name, but it's not the only thing I mispronounce. So who is going to present an empirical character say? of any cast convergence time. So continuing with any cast The floor is yours. Yes Thank you I will be talking about any cost, but from a different perspective at this time I mean from the convergence perspective This is a study I did together with Matthais-Jonker, Holland van Rijsweig-Dai, Kvales-Mese are all at the University of Twente So what do we mean when we say convergence? As a quick reminder, convergence is the time between a BGP update and when the global routing system reaches a stable state We know that BGP's convergence is impacted by factors such as the minimum route advertisement interval and route flap dampening It's not clear how much exactly and in particular we want to answer the question how long should we wait after an announcement? but before its effect can be observed"
  },
  {
    "startTime": "00:26:00",
    "text": "And we're particularly interested in this in any cost scenario Should we wait? Now I am waiting It seems to be stopped there it is. Should we wait? for two minutes? 10 minutes? 50 minutes? We have seen all these values in literature without much substantiation And this is particularly interesting for research, but also for operations For research we would like to know how long we should wait before we measure so that we are, we ensure that we measure the right thing And for operations, it's very relevant so that we can swiftly this distribute traffic over vantage points over points of presence So the goal of the study is to put PGP convergence style on an empirical footing in an any-cost setting Now I will go over the methodology that we use in this study We run our experiments on an any cost testbed that we maintain ourselves, and we measure a we do a set of announcements and we measure the changes it brings about So as a first step, we have a hit list we use a slash 24 hit list which contains the most likely address to respond per slash 24 network which is the maximum routable prefix lengths to the default three zone. And we pre-processed this hit list by rejecting addresses that are not responsive responsive And we partitioned this for us to quickly perform our scans for optimized the turnaround time, because it doesn't give any extra information if we scan each address in us slash 24 hit list or if we just select one representation address for that slash 24 prefix Now for each partition we announce a brief"
  },
  {
    "startTime": "00:28:02",
    "text": "from a set of vantage points. We start a catchment mapping procedure every 10 seconds for five minutes So one catchment mapping takes about 10 seconds A catchment is basically the set of IP addresses that is routed to one anycast pop And we repeat this for four sets of VPs We do this in total for four sets of VPs, six, two 18 and 31 to represent a we repeat this for four sets of VPs. We do this in total for four sets of VPs, 6, 12, 18 and 31 to represent any cost deployments of varying sizes We use two catchment mapping methods to ensure that we use the right approach So we do forward probing or forward measurements, which is basically the white wide-scale probing that I just explained. And we do also something we call reverse probing And that is not sending the probes from our testbed, but rather using KDAS distributed arc measurement infrastructure to probe our vantage points. I'll explain these two methods and the next slides So what do we mean by forward probing? This is basically what is referred to in literature as fairfutor style measurements, where we have a server and a bunch of vantage points In this example, we have three, vantage point one and two have an active announcement and vantage point three doesn't So the server delegates the part of the hit list to a vantage point, which sent out the probes. For this study, it doesn't matter which vantage point that is because we are interested in where the replies are routed to, and not so much where the query is coming from So the client at vantage point one sends a ICMPI echo request to the target on the hit list And that target then replies in this case to vantage point two, but it could have also replied to vantage point one"
  },
  {
    "startTime": "00:30:02",
    "text": "Vantage point two then sends the result back to the server Now what is the result of this? We get actually a fine-grained measurements of the catchments at the slash 24 level level In parallel to the forward probing, we do reverse probing and we use KDAR k-darknotes for this which query our vantage points via DNS we use dnsa queries and each name server run DNS. We use DNSA queries and each name server running on the vantage point has a unique identifier for the same query So based on the reply, the DNS reply, we know which vantage point is replying The downside of this approach is that it has less granularity. It's only a 140 arc notes at the time of measurements as compared to 3 3.92 million active hit list targets we use in forward probing Then I will go over some results So the first result is a graphical depiction of how catchments are distributed. On the y-axis, we see the catchment distribution, which is normalized to a value of 1. We can do this because there is very little variance in terms of number of total responses per scan And on the x-axis, we see the scan index So we start off by six analysis announcements, and we see the resulting catchment Now we do six more and six more, and the final batch of 13 Now a couple of things become a apparent from this plot First, we see that catchments vary vastly in size and even though we announced six at the beginning, six vantage points, we only observe five of them. The sixth one is actually there but it is so small, it is too small to see in this picture"
  },
  {
    "startTime": "00:32:03",
    "text": "Next, we see that some vantage points take preference over others So after we announce the second batch, of vantage points, suddenly the blue catch in the first region completely disappeared because clients shift to other vantage points which take preference Now, the main result of the study is of course the convergence times, that's what I promised And on this slide, you see on your left a table which lists the convergence times at the 50th, 80th, and 95th percent times along with standard deviation and on the right, you see the same thing, but in a CDF. We define convergence here as the moment at which a prefix no longer changes between catchments. So when the routing of one prefix remains stable What we can see in this graph is that 80% of the population that we measure already converges within the duration of one scan which is about 10 seconds What we also, what you can also see is in the CDF there is a straight line between the zero and the 10 seconds, which corroborates this result because the probes are sent out linearly and at the uniform intervals We also see a noticeable decrease when we announce more vantage points, particularly when we go from 12 to 18 We surmise that this is because catchments become smaller as there are more active vantage points So less routers are impacted by the announcements Then for the reverse probing results,"
  },
  {
    "startTime": "00:34:02",
    "text": "we were surprised to to see this graph where it appears that most clients seem to be converging near the end of the measurement period, particularly when 31 vantage points are active So this warranted more investigation What is going on here? It turned out that 11 of these notes are not confirmed at all and were flapping between two or more vantage points points We thought about this result and the most likely explanation that we found is that these are the queries being sent by K-da's arc and Kda's arc API does not allow tightcom control over the exact packets that are being seen out so every packet has a unique source address which make our measurements subject to load balancing After excluding these notes that are not converging, we end up with a graph that looks more similar to our foreword measurements Still, the difference of 31 vantage points is there However, we only have 12 in total that change catchments in this measurement round So we don't have enough information In conclusion we devised a method for measuring any cost confirm measuring any cost convergence, which measures it from an end-host perspective It doesn't require cooperation of end users and it measures the global routing system. We find that 80% convergence converges within 10 seconds, which is the duration of our scan"
  },
  {
    "startTime": "00:36:02",
    "text": "And we find that converging happens faster when there are more active announcements We have some further ideas namely, first of all, we will like to increase the measurement resolution if we scan faster we can obtain more precise results And we would also like to zoom in on regional differences Right now we use our entire testbed of a the measurement resolution. If we scan faster, we can obtain more precise results. And we would also like to zoom in on regional differences. Right now we use our entire testbed of 31 free piece. However, most of the VPs are located in the global north which doesn't make the measurements representable of the internet as a whole And finally, we would also like to look into how IPV6 behaves in this regard Thanks for your attention. The paper is online at this link along with the complete measurement data sets I'm happy to take questions now, or you can always send me an email Thank you very much Bernhard. Do we have questions from the floor? uh yes i see matthias uh with a question Hello. Can you have a couple of questions? So first, when you talk about testbed, you are still running your experiments on top of the Internet, right? That is right. So you mean that you have the vantage point under control and injecting, okay. Other questions regarding the hit list and a second slide um and the 24 prefixes um i didn't get the point with a splitting can you go back back back back back back Back even more back, back, back, even more, okay, shuffling pat partitioning. So does it mean that you split up the 20? even more specific prefixes? No, right? No, not more specific prefaces But we take the entire hit list and we ask randomize it and split it into 40"
  },
  {
    "startTime": "00:38:02",
    "text": "partitions. And we do this so we can quickly perform one scan without losing generality So we still target every slash 24 prefix on our hit list but we have a quick turnaround time of one scan in 10 seconds And the hit list includes multiple slash 24 prefixes? Yes, that's correct. Okay, great And obvious question did you compare the conversions times with non any customer prefixes? Um have not done that yet We did look at converters times in literature, but we have not measured this Okay, good. Thanks Thank you, Matthias. Next in the queue is Robert Robert Story, USC ISI. Did you run? the arc, the reverse measurements? at the same time as the forward measurements or different? Yes, we run them at the same time to ensure that we are measured the same thing And maybe one quick question from my side, did you compare? with route collectors to see if the active measurements that you are aligned with what you see there? um we have not done this We did consider this. But our focus was more on the end user perspective, so how routing propagates within a network Not very well, thank you very much Let's thank the speaker speaker much. Let's thank the speaker. And if the next speaker can approach to the table, I'd really appreciate it if people could scan the QR code and be available in the data trucker, in particular if you are on a speaker, so it can actually ping you and tell you in advance to come and get ready Hi, for that So you will use this on the slide"
  },
  {
    "startTime": "00:40:02",
    "text": "deck will be there in a second At the moment now it's the person that is uploaded there, so So then that's this one. Yeah Yes. Yes Yes Yes. Yes. So we have now there making diffusing compute more efficient for loop free short-spat routing with mortesa. I won't attempt to pronounce your sorry and apologies for that. No problem So good afternoon, everyone. It's really a pleasure being here with you today Today I'm presenting, by the way, my name is Morteza and I am a recession research-associated University of Toronto. We're not blind time ago, I also obtained my PhD and completed my postdoc in the same place. So today I'm presenting there, which is a new routing protocol focusing on hopefully making the diffusing competition more efficient and that this works is co-authored by Professor JJ Garcia, who novel brain and ideas have led us today the invention of dare So basically throughout this, presentation, I will focus on discussing three important things with regards to there. These three topics is first the motivation that speaks to why do we need another yet diffusing computation We have dual and it apparently works in the IGRP right now. And then I will cover a discussion about there and when we say efficiency what do we mean making it faster to converge and less overhead in terms of signaling and the third one is we compare to there in dual in terms of the routing methods and hopefully to get you there to understand where we are putting our eyes on the improvements So it's good to begin"
  },
  {
    "startTime": "00:42:00",
    "text": "with a bit of context that why do we really need a diffusing competition in the first place is that if I drag you back a bit to distribute it better, the algorithm is really working and it's efficient, especially if you consider the minimum hop routing into your routing, and it works in many scenarios, but the main problem that we have with Belmont Fort is the count to infinity And the count to infinity problem, regardless of how we injure, the network to get rid of that but the problem with the algorithm exists and that lead us to the emergence of short or sometimes even long-lived temporary loops in the network which then it means for us that we're losing packets or sending packets to a network where there is no actual destination for, let's say, a temporary amount of time That's evasive resources. And in an example that I continue to be persistent with dual and there over this set of notes that you see here is that if I have this five-network, five-node network, if the link between A and D fails, then obviously the node A in Bedmanford is starting digging around its neighbors to find a shortest past and that would cause the node A to select the past to go to C because he advertises 4, and then adding the link weight 1 it becomes 5. So apparently 5 is better than this past that is going seven through E. So the node A picks that and then start advertising to its neighbors which then causes node C who was before going through node A to pick that and increase its own weight and start updating others and in a sequence of updating each other that flow continues to be there on until something happens like the value of the count to infinity increases beyond the value 7 that can be advertised by node 8 to go through D and then we stop there So sometimes this is just a few steps. Sometimes this is like a much longer step and we stay in loop and unfortunately this is happening every day. So by that"
  },
  {
    "startTime": "00:44:00",
    "text": "we can see that not even far into the algorithm in the first few steps, we see the temporary loops to emerge And I would say that's a bit of wasting of the resources. However, the algorithm works overall I'm not debating that So now, this is a this is a form of distributed company of course, but if we go to dual, dual is also doing a distributed computation for us. However, the language of a dual is a different form of doing this distributed computation called diffusing computation So basically, there is a system of query reply between the nodes to see synchronize over the failures or changes or update in the network. So each node each router that you have in a dual network, consider itself either passive or active. That's simple If you are passive, that means you're good and what is your next node to a destination that can stay. But if you don't know, because of a change, that what would be the next node, then you have to choose the next one. And if certain conditions which we call this a local condition, cannot be satisfied, then you have to ask the you have to choose the next one. And if certain conditions, which we call this a local condition, cannot be satisfied, then you have to ask your neighbors. So a local condition would be, let's say, if we consider the hop count here too a local condition would be if any of my neighbors can give me a better distance in terms of the hop count than my physical distance. And by feasible distance, I mean my current path to it destination. And that is also the short pass, then I'm happy. But if I'm not getting that from my neighbors then I don't know where to go so then I go into an active mode and set my physical distance to infinity because I no longer know where to go and that the causes me to also estating new distances to my neighbors, which we refer to that as now diffi So then the problem is I have to wait and wait and wait until all my neighbors reply to me and telling me where to go to make a decision and if that becomes a domino effect that I talk to many"
  },
  {
    "startTime": "00:46:02",
    "text": "people and then to many neighbors and then many neighbors talk to many neighbors then all of us have to wait on each other to make a decision And that would be also a problem So let's say everything goes well, hopefully, and then the computes the competition of new distance happens then my local condition becomes satisfied and then I go to a passive model so with that in mind let's take a look at the same network that we were doing with DBI now with dual, the same link breaks of course. So the moment that that has happens, node A set its visible distance You see two weights on top. One is the feasible The other one is your current distance. So you set your physical distance to infinity, and you don't know where to go but then you can send updates to your neighbors and then you have to wait all of the replies to you to come back to make a decision where to go but unfortunately in a step c you can see that that can cause the next node to go to a diffusing computation or active mode, because the update you set to that node, the node was set to go through you. So that means the node doesn't know where to go. So, and that's the domino effect here that you can see. The unfortunate situation is you're already getting a reply from node two that you can go via node e to d by two which means you can go by 7. And that's a valid advertisement. But you cannot pick that in dual because you're stuck with all of the replies to you first to come and then make it decision. So short story, that goes into a sequence of query and reply until you hear from all your neighbors and you then converge yourself first which then that means you can now choose to go from A to D and then you can update all your neighbors and then they can converge Well, of course you see more steps here but then no router has actually put you into a loop. So packets in the network, at least don't wander around and you're not missing"
  },
  {
    "startTime": "00:48:00",
    "text": "the networking resources just because of the routing and signaling that you need. However, with believe that this algorithm even though it's perfectly working and it's efficient in its what it is intended to do, may have its own issue So the key issues here is what I just referred is that your set to only do a single loop free next hop How about multiple of them being available to you? and you cannot choose them? But if you could do, then you could reduce your signaling significantly And the second one is, well, I have to go into sending a query and then making a diffusing competition, but then I have to wait until I can choose my next hop, and that would be some of my neighbors can actually offer loop-free routes that are valid to me but I cannot choose I have to wait and that waiting time is unknown depending on the domino effect that you put on the network you may hear from them at different times. And then so this unnecessary blocking, we believe that it also slows down the flow of the data in the data plane because it does right when you block and you wait until to make a decision so that the flow of information is uncertain. So the data. So considering these challenges had led us today And of course, uh, I want to say the first thing that you know, you need to do these challenges had led us today. And of course, if I want to say the first thing that you know, you need to know about there is, well, there is compatible It's not that we are making a whole change in the way that the language of the protocol is or the way that it is working. So hopefully it can stay within the same infrastructure that we have today or used in the same context however we there we change the definition of what a feasible next hub is and that's very important because in dual all all you make a decision is who's the next node that can lead me to a destination, right? So the last language tells us that a feasible successor in dual a visible successor means your next hop to a destination is any neighbor that can give you a smaller"
  },
  {
    "startTime": "00:50:00",
    "text": "distance that you believe right now, but it has to also be the shortest pass. Emphasizing so much on being a shortest pass, make a type of restriction and limitation that may actually not be necessary at the time of the competition but you can lead on to afterwards So in there, the physical successor is any of your neighbors that can report the distance to you smaller than the distance that you had when you were passive So before you become active, you had a distance that can be considered as that. And that is small change allows your neighbors to report to you sure distances that you update them with you stating new distance. We will see an example of that coming up and how that actually rolled the network with that new updates Then basically we call any of those neighbors that can give you a shorter distance than your kind of understanding, of the distance or your feasible we call them ordered routers so you only need to go to a diffusing competition if you don't have any of the ordered routers or not of the ordered routers that you have can give you a strictly shorter pass. Maybe the passes are equal, which you don't consider them as ordered anymore So with that, I'm just trying to say that that implies that also in there we decoupled the calculation we need to do the shortest pass calculation plus the selection of the success or no. You can take temporarily select the successor node just to avoid me misusing the networking resources but then the calculation converged to a the shortest pass later Okay, so the second note about there is, they remember what dual forgets, and that's very important because in dual what happens is dual routers don't remember which of their neighbors put them into this diffusing home combination, and because of that, what happens is they need to reply to every neighbor after they figure out what they need to do, and then that is an excess overhead in the network"
  },
  {
    "startTime": "00:52:02",
    "text": "plus that they can only now participate in a single diffusing competition, because if I don't remember, and I'm going to talk about my beliefs to everyone, then I don't remember which one was that so I can run only one diffusing competition so but then in there there remembers exactly which node forced the router to go into a device which one was that so I can run only one diffusing competition so but then in there there remembers exactly which node forced the router to go into a diffusing competition so it only replies to that particular node, which means much less signaling in the network plus that I can run multiple diffusing computation in there, and even I can combine them for different decisions if I am in the active mode so then I can be faster in the in my convergence The third to know about there is there is fair, not that I'm referring to the fairness concept of networking, but also to the position that dual routers can only change their successors when they are basically satisfied with their local condition or everything about the diffusing competition is completed and they go from active to passive again so they can they can make a change But as I said, and I just showed you in an example that would be a dominant effect and takes so long but in there you can change your successor at any time regardless of whether you are active or passive. Just make a change if you need to. But the thing is you only need one of the routers to tell you a shorter distance than when you were passive, which happens quite a bit in the networking that you get that neighbor that tell you this and we are going into an example to show that. The other thing about about that we think there is fair is dual estate machine is so complicated because of all of the challenges that I said the state machine is complicated, so that makes the evolution of the protocol a bit challenging and also hard to imagine if you want to adapt new criteria for your performance in the network. However, we dare, their state machine we can be hopeful that because it's much simple we can evolve it faster and adapt new performance criteria. So I thought to be the"
  },
  {
    "startTime": "00:54:02",
    "text": "about the state machine. Let's see the state machine in there. It's pretty simple. We still have the same language that we speak as dual, passive and active mode for a the router. And when I say passive and active, I there. It's pretty simple. We still have the same language that we speak as dual, passive and active mode for the router. And when I say passive and active, I'm trying to refer to the fact that a router either needs to make a decision on its own knowledge, which means you are in a passive mode and you believe you know what you need to know or you don't know and you become active talking to your neighbors to find out. So you can be in passive mode with there up until the time that any link change or update still keep you satisfied with your local condition or any query that you receive from your neighbors. Does that? change your belief. An example in the same five notes, network is that if the link between DNA, changes with from 30 to 20, what you can see is that doesn't change the belief of node B because B is set to go via C to destination B and that link is 3 so 30 or 20 doesn't make any change, so you're still in the passive mode and you can see three values on top of the there here One is your current distance and then your feasible distance and the one that you also report your neighbors but then you go in from passive to active if the local condition doesn't satisfy anymore, what if I lose this link? Well, if I lose this link, the weight is three. So then I don't have any access to this node which is my destination. So node E now has to make a decision However, the next node, which is the only neighbor here, advertises the V8, which is above the tree that I have here. So I don't have that order neighbor anymore so what I do is I can, at the same time, I can send a question above the tree that I have here so I don't have that ordered neighbor anymore so what I do is I can at the same time I can send the query to find out who's my next neighbor to that destination, but also I changed my successor and picked the pass from E to D via A by weight 13. So I'm not stopped and I'm not locked. I can still forward your packets to that destination Maybe the pass is not your"
  },
  {
    "startTime": "00:56:02",
    "text": "preferred, is not your shortest pass, but you can still work until all of the diffusing competition completes and then we get a complete. So you can stay in that if inactive mode if the input even still is there and then you need to coordinate between the neighbor, let's say the neighbor V, you need a response from it so you wait for the response before you become passive And if the local condition is still false and you may not coordinate, but still your local condition is false But the moment that these are going away, you can easily go to passive mode by saying that my local condition is satisfied, and I don't need to coordinate with anyone else. And that would be in this scenario, that would be just a single reply back from no A to E telling you, hey, you can go through me by 5 and then you adapt and then become passive right away So no bit of challenge in the network, if that happens So now let's take a look at the same scenario that we ran with DBF and then later we do all and now we dare Well, the event that happens in the network, consistent with the same We lose the link between A and D three. But the moment that happens, then you pick the next neighbor that advertises to you a shorter distance than what you believe Okay, so this is advertisement of seven because the pass here is five and two seven but hey you lose the link right so after you lose the link you believe that you no longer can reach to the destination. So seven here is 5 and 2-7, but hey, you lose the link, right? So after you lose the link, you believe that you no longer can reach to the destination. So 7 is actually pretty viable for you to pick so while you are in diffusing competition and you can see I changed the color to indicate that I am in active mode, I choose to go from this path until I hear from all my neighbors regarding the query that I send to them and then make a decision that better my decision at the beginning was okay or I can go now. And now I sent to them and then make a decision that whether my decision at the beginning was okay or I can go now and that would tell me that I can send updates with weights of seven where my neighbors can report back to me with short distances but I already picked one so I wait on"
  },
  {
    "startTime": "00:58:03",
    "text": "until this cool reply goes on and then make my final decision. But I did not, as a result of that I did not block the network and I never sent excess overhead in the network because of that. So hopefully the loop result of that, I did not block the network and I never send excess overhead into the network because of that. So hopefully the loop-free routing stays there with a better purpose the network and I never sent excess overhead into the network because of that. So hopefully the loop-free routing stays there with a better performance. So to make a few final remarks is that there by itself is a new routing algorithm only improving the existing work, but it is by nature a new algorithm and there speaks the same language as dual does, but it does that so that it can make your state machine much simpler and less signaling and faster convergence and gives you the flexibility to choose any available neighbors. So the capacity, of the network is always available to you as a routing protocol to choose, regardless of what state you have. And finally, we hope that the next step for us is that we can incorporate there into a routing protocol to see the result at larger scales. So many thanks for your patients and I'll be happy to take any questions Thank you very much. Questions from the floor? I see Juan Camilo has entered the key queue. Juan, please go ahead Can you hear me? Yes Hello, thank you for your presentation So in the practical side, it wasn't clear to me, could like a network in which half the devices are using data? be compatible with other half of the network using dual? for instance, or everything has to be converging? to direct for this to work well okay the moment that i discussed I guess you're referring to this charge When I said there is compatible, it means that the dare language of going from passive to"
  },
  {
    "startTime": "01:00:02",
    "text": "active and active to passive stays the same as dual and the transition from passive to active only changes because of a configuration that because of a change in the meaning of a feasible next time which is hopefully a software or let's say any implementation change that we can apply, but no need to change the infrastructure that runs the algorithm That was the main point Right, but again, if we'll Dara device interoperate with a dual device or not really? it's okay if it's no i'm just i'm just i'm I think we love you Juan I'm just asking whether a diary device will interoperate with a dual device Oh, no, in terms of interoperate, yeah, yeah, sorry think we lost you, Juan. I'm just asking whether a diary device will interoperate with a dual device. Oh, no, in terms of interoperability, I guess, Darren, dual cannot speak to each other if that's what it meant from the interoperability perspective It is, thank you, that's okay. Yeah, sure, okay, yeah, yeah you and I see someone in the queue Well, I see someone on the microphone, not on the queue Sorry, it's Tony Lee Juniper, on-site devices being weird. Anyhow, have you done any sense? simulations to compare against LinkState? protocols? Sure. Our simulation is undergoing in NS3 However, we have done some early assessment the figure that you see here showing a particular topology that is intentionally making domic effect. If this link goes down and a go to D, if it was dual, all of these nodes when to diffuse in computation and then all of them should come back while this alternate pass WAB is available So in dual, that would cause sending third"
  },
  {
    "startTime": "01:02:02",
    "text": "messages for a 10 node network if by 10 node i mean 1 2 and 3 and then the rest 7 that would send 10 messages I'm sorry, 30 messages for the 10 and network and also 14 blocking the steps By that, I mean A to V2 is blocked and V2 to A is blocked because it's bidirectional, right? However, if it was there, zero blocking happened as a result of that and then the number of messages is much smaller Now, if you're skilled to go to a 50 node network, you would see that 94 steps blocking can happen as a result of a link failure, but in there is zero but that's one of the earliest results of behind we have, but we are continuing to do this Okay, comparison against ISIS would be interesting of course yeah thank you we'll have thank you very much Well, thank you very much. Thank you very much everybody. Let's thank the speaker And right here is our next speaker, so now we have okay, this is going to be hard, I'm just going to say your surname Nadas, is going to present It's gonna present to QA or not to QOE towards QOE, our resource allocation for real time media. The room is yours yours Sam Cylester from Erickson, Santa Clara So when we talk about real time media, we focus on mobile video use cases We group mobile video into three different categories real-time media real-time, and non-real-time. And real-time is when some kind of interactivity is included, either a human or an organism in the loop. And therefore, like very small latency is required to keep that interactive active besides 3,200 either a human or an organism in the loop. And therefore, like very small latency is required to keep that interactivity active besides 30, 210 millisecond-fingered photon latency. And up applications for that include remote rendered XR, cloud gaming and remote control of different drones, cars, and heavy equipment. Video conferencing like the one we have a"
  },
  {
    "startTime": "01:04:02",
    "text": "the moment are to some extent interactive, but the interactivity is much lighter, so we say it's 100 to even five milliseconds, the RTT is acceptable and then when we we watch video over the internet whether it's being live, like a sports event, or just video on demand much higher delays are acceptable, like one second for live content and much, much higher like 10 to 90 seconds for video on demand demand So what characterizes the QE of real-time video? We say there are three different components of QE, spatial quality, temporal quality, and input lack quality So temporal quality requires high and consistent frame rate and no visible freezing, for example, due to frame drops. And input lack quality requires the already mentioned finger photo latency to be small, so for example a button press until the resulting action is somehow visible. And because for real-time video, these two have to be kept low, the frames must be transmitted in a far in a few frame time, so each and every frame is probably transmitted in a different series of packets and there is no long time ever averaging like for that, so we cannot create chunks of content of 2 to 10 maybe even 30 seconds long and aggregate the frames and have basically this downloads. It has to be some kind of real-time protocol And because of that, the spatial quality becomes important in a way so what is spatial quality is the visual picture quality versus the perfect picture. It's not simply the resolution it's the effect of the bit rate depends on the reprate on encoder, on encoder configuration also in the content itself. And there are different metrics for that. For example, Netflix is VMAF metric or ITU's B-204, the tree and other metrics. And that's the focus for our talk. And again, it becomes more important, more bursty, because each and every frame has to be transmitted separately"
  },
  {
    "startTime": "01:06:05",
    "text": "How do we characterize spatial QE? It is something we call the spatial complexity curve and that measures spatial QE for example, VMF score as a function of average rate So what we did to determine this curve is that we took 10 seconds long homogeneous HD video game clips and encode them with different target rate settings. We calculated the VMS spatial QE measure for each and every end determine this curve is that we took 10 second long homogeneous HD video game clips and encoded them with different target rate settings. We calculated the VMS spatial QE measure for each and every encode. That is spatial quality measure by Netflix. That was developed for movies It's open source and it is to some extent applicable for all kinds of contents. And we say that 50 is a minimum acceptable quality 70 is all already good quality and 90s close to perfect quality. We took a bunch of games throughout in total, from racing, third person, isomet first person shooter, platformer calls with M-Seltagin sport. And we actually ordered them in the order of complexity decreasing. So racing one is our most demanding game It requires more than 11 megabre second for to reach VMAF 70 while the sports game which is basically like most in this case, a field from above and not changing very much requires one less than one magnet per second but again I want to emphasize that these are homogeneous clips on purpose the games might be more more changing than what is visible here. This is like one state of the given game instead of characterizing the whole game So what can we do in our net? if we want to take advantage of this? heterogeneity? So how does cloud rendered gaming looks like? We have cloud rendering servers where the game is being executed the frames are rendered by the server they are encoded into video frames, center over the transport, center"
  },
  {
    "startTime": "01:08:00",
    "text": "over the network, decoded into the cloud gaming, clients and the cloud gaming index the client, the user actually plays the game, takes game actions game actions are being sent back to the cloud rendering server So if we, if we can have a Q controller somewhere in the network, how can that allocate resources more effective? than in the betrayed domain? It can get some kind of key metrics from the cloud rendering servers It can get also some metrics from the network I will mention details of that later And based on this, for example, the QE matrix can be the spatial complexity curves on the previous slide So based on these, it can calculate an optimal allocation from QE perspective and apply that optimal allocation by sending rate control messages to the encoders and maybe also control messages to the network So, but this is pretty complex, so what is our game? What motivates thinking? about this additional complexity? So we calculated example of QE aware of QE awareness, still the same data, and we said that our VMEF target is 70, which is QE and we determined the maximum number of flows for two different strategies. The first strategy is called equal bitrate All flows get the most demanding games rate requirement. And that is because we don't have any information about the game. We don't assume any kind of cooperation between the network and the game. So if you want to make sure that a good QE thought is met for all of the games we have to take the most demanding games requirement why the other strategy, the equal QE strategy, assumes that we have this architecture implement and we are targeting equal QE for all of the games. And then all flows get just enough rate to reach you have 70 So what are our gains? Like if we take the two most excellent games, racing and sports and just have games from these two then"
  },
  {
    "startTime": "01:10:02",
    "text": "we can only support 13 games with the Equalade Strategy and twice as many games with the QE and sports, and just have games on these two, then we can only support 13 games with the Equalade Strategy and twice as many games with the QEEEE strategy, part of the gain is from granularity. We simply couldn't put it games with the Equalade Strategy and twice as many games with the QE equal QE strategy. Part of that gain is from granularity. We simply couldn't put like one more racing game into the system and the sports games requirement is so small that it actually fitted into the empty space What happens if we take less extreme game pairs like this? green or the purple one? Then, of course, our gain decreases, like from two to we take less extreme game pass pairs like the green one or the purple one? Then of course our gain decreases like from from 2 to 1.72 or 1.33 there's still some game but if we have a mix of game like all 12 games then because some of the games have very high rate requirement compared to the others, the games can be extreme so if we consider all 12 games with equal probability coming to the system, then the game factor is 3-62, which means that we can have 3-62 times as many games as the equal trade strategy. And even if we exclude the two most extreme games, the two least demand and the two most demanding ones, and just take the eight games case, it's still twice as many flows, which can be an added to the system So these are the good news What are the complications? First, we have cellular network characteristics. Base station have a given amount of radio resources and in an abstraction level spectral efficiency determines the bit rate achievable with these amount of resources. And spectral efficiency varies as the radio channel quality varies due to mobility noise and interference And in the analysis in the previous slide, we just assumed constant spectator efficiency. There is more on dynamic spectral efficiency in the paper, but of course, if the spectral efficiency changes the allocation has to be updated and it's not that simple Also, an important fact where we actually received reviews and it wasn't like highlights enough in the paper is that resource sharing over the area"
  },
  {
    "startTime": "01:12:03",
    "text": "interface is determined by the air interface scheduler because packets are queued into per user or even per user cues or potentially even multiple cues per user and dues cues are scheduled and any kind of end-to-end desired end end-to-end algorithm behavior when it comes to resource share is overridden by this queuing and scheduled so the end-to-end algorithms can quite help unless we change how the Bay Station is working, but there's a reason for that Other issue is spatial complexity variation. For the 10 seconds long clips, which are designed to be homogeneous, we call calculated the 1-second average rate required to reach the beam of 7 each second. So this is, again, the good QE the same target as all this slide. But instead of calculating the rate, requirements for the whole 10 seconds, we calculate it for each and every second. And you can see that, for example, for third production, target as on this slide. But instead of calculating the rate requirements for the whole 10 seconds, we calculate it for each and every second. And you can see that, for example, for third percent two, the requirement varies between 12 and 6 megad per second So it's already this high variation for clips which were designed to be homogeneous So if we implement a QE controller, it's has to follow this variation somehow Also, if we have non-homogeneous clips higher variations are possible, these are our two extreme games but different clips, the racing one and the sports And the sports is especially interesting because in the beginning, the sport game is having the same kind of behavior like you know, 10 seconds. So it's a, it's a field from the top not much changing. But then it even happens in the game and the game breaks almost like a real-life experience. And suddenly the requirement jumps up from about 1 megagre second to 10 megast seconds, it's 10-fold increase If we, again, if we implement a cubic controller, it has to be able to follow all these variations to some extent. So how? we would design such a QE controller?"
  },
  {
    "startTime": "01:14:00",
    "text": "The simplest algorithm, and that is what we have calculated the gains for also, is to provide equal QE for all those. And that is when we have full knowledge about the spatial complexity curves and we send rate guide to the encoder in the servers. And we always, when we calculate the amount of resources, we always give resources to the flow with the smallest QE and among equals for example, the one with the highest spectral efficiency potentially also have a minimum QE requirement and reject flows for which these minimum QE requirement cannot be met But again, this calculates the amount of resources to each flow to reach the equal QE and send rates guidance And if you want to handle spatial complexity variation, and apply the single algorithm, we have to recall the QE controller allocation with some frequency like every second, every two seconds, and so on. Similarly for spatial complexity variation, the same thing But there can be improvements in this algorithm and and kind of my favorite improvement is to use QE guidance as opposed to rate guidance So if you have a look at the algorithm itself, it says it looks at QE metrics and send rates control. But what if we can send instead? QE control, which which means that we send okay target we VMEF 70? What will be the result of? that? It will, the QE itself will be much less bursty because we provide the QE guidance, but we need an encoder which can actually do that, which can actually do QE target encoding. And we trade burstiness in the QE domain to burstness in the rate domain, which might still be okay if we have several of these flows and they are statistically multiplexed, the the resource demand of the the whole set of real-time flows might still be reasonably stable we can if it's over a given amount of resources, we can still react on that by decreasing the QI target"
  },
  {
    "startTime": "01:16:02",
    "text": "The other potential improvement is to work with only having partial knowledge about spatial complexity. Calculating these spatial complexity curves all the time is probably is quite computing intense What if we only use the current key? level and the current rate rate and try to estimate what happens? if we change the QE order? the current QE level and the current bitrate and try to estimate what happens if we change the QE or the rate guidance? We can also take into account the resources needed to reach a given QE. For example, by using YouTube to function The reason for this, if there is a flow which requires disproportionately high amount of resources, either because it requires high rate or because its spectral efficiency is low, then we might still want to give it reasonable QE, but if we don't want to give it the same QE as other because that would be like a very disproportionate decrease for the QE of the other theories. It is still QE aware, but it's not equal QE anymore We calculated the equal QE because it's very easy to compare, very easy to calculate gains for it, but that's not quite a realistic thing to do in a realistic system And lastly, it can also implement service differentiation like gold flows having higher QE than silver flows And in conclusion, we demonstrated the high potential for scalability gains by moving traffic management from the bitrate domain to the QE domain for real time media flows. But for that, an interface for application, network collaboration is needed to achieve this gain. And there is very serious trust issue which is very far from being trivial so solve. There are benefits and users have increased QE carriers, have lower contention, cost saving, less capacities needed in the net networks in general. Service providers can offer a more consistent QE, but high amount of cooperation among players needed The trust issue have to be solved somehow and we are active in this area, and we welcome discussions in this area"
  },
  {
    "startTime": "01:18:00",
    "text": "I know it's, to some extent, it relates to the Scompro discussion we had in the other room this morning but it's it's a much, much higher complex much more trust is required in this case Thank you for the this morning. But it's a much, much higher complexity, much more trust is required in this case. Thank you for your attention. I'm open to questions, comments Thank you very much. Do we have questions from the floor? Maybe one question from my side. Do you think the it might interact with neutrality regulations? Yeah, that's depends on how you implement it, but of course in general when you move traffic management from the betrayed domain to the QE domain, you have to think about net neutrality if there's a perfect trust among players and it's well coordinated then it might be net neutral in the spirit of net neutrality. I don't know about the low of net neutrality whether it's not neutral in that case. But that's part of the puzzle to solve here Yeah, fair enough. Any other questions? if there are no other questions let's thank the speaker thank you very much And the next speaker approach? Light will be up in a second and then you can just control the clicker Let's hold in a second Snow Right. So we have now Fatif Fati Berkai presenting to switch or not to switch to TCP Prague Incentives for adoption in a partial L4S deployment"
  },
  {
    "startTime": "01:20:02",
    "text": "Hi everyone, I am Fatih Berkai Sarker switch to TCP Prague. Incentives for adoption in a partial L4S deployment. Hi everyone, I am Fatih Percaya from New York University, a PhD student Today I would like to present our work about partial Air Force deployment, actually Let's start with what is Alphoros actually? student. Today, I'd like to present our work about partial alphores step point, actually. Let's start with, what is Alphores? Alpharet is an architecture for low latency, low-loss scalable throughput And scalable congestion control is the key part of this Al Forest architecture We can see this by looking at this blow section actually these are the plots that we generators, it's going to our X experiments. And you'll see the congestion window over time plots here. And in the first plot, for instance you can see that with tv use as a classic TCP congestion control, we use TCP Renault And in the first plot, we see that if there is no ECN support, as we already know, the classic TCP increased its control window size until it feels the buffer And in this case, we can have very high utilities but also we can, we will have a high late actually, because of the queuing. But in the second floor, if you look at here, if we use let's say, 20 millisecond these hand threshold in this case the sender will increase it is congestion window size until the ECN limit instead of the whole, the buffer size So in this case, we will have lower latency because of this ECN limits And we can have again maybe high utilization but the problem starts when we want to get very low queuing delay. If you want to get very low quing latency, we want to set the ECN threshold very low. For instance, one millisecond ACNT results. If you want to do that with clear, TCP congestion control like Renae here, as you can see here, it's again, it starts to increase its congestion, but we have very low ECN limit. So in increases its congestion window until that one millisecond But in this case, we can get maybe low latency but we will have very high underutilization, actually. We will see lower utilization because this classic"
  },
  {
    "startTime": "01:22:00",
    "text": "DCPs requires some queuing to increase their congestion window. So in this case, it won't be able to find enough time to increase its congestion window so we will see some under utilization. In this case, scalable congestion control, such as Prague here can solve this problem. In this case, as you can see here in the last plot, even with one millisecond ECN threshold, because of Prague's congestion window, we have very fine softos, very small software to adjust this congestion window according to the congestion level on the network. Because of that, it can get high utilization, even with very low ECN thresholds. So because of this scalable congestion control is the key part of this design for this architecture But it's not, this air forest architecture is not restricted to this scalable congestion control. We have some more elements You can see here the whole network architecture. We have sanders and hosts servers and those routers And in this case, I already mentioned that we need scalable congestion control on the sender side at least. Let's say this is a server here And also we need accuracy and support to get this very small soft tools for TCP product to get this very accurately change, to accurately change our congestion window we need accuracy scan support and on the ends and size. And also on the routers, in the middle boxes we need ECM support, of course, and we need a shallow ICM marking we need to implement this to get low-way latency benefit. And also, we need to, because of this, difference behavior of classic TCP and scale TCP we need to isolate them because it's obvious that we need different TCN thresholds for this different kind of TCP So we need to isolate them. So we need multiple queue or dual curing system on the router. So as you can see, we have so many things to implement on the network. So it means that we cannot do this all at once. We need some incremental deployment to Al4S And here, as I said, we need incremental deployment and you are seeing now some example scenario from RFC 93 document in this example"
  },
  {
    "startTime": "01:24:02",
    "text": "scenario, they assume that first we have this some example scenario from IFC930 document. In this example scenario, they assume that first we have VCTCP on the center side. It's again scale another scale consciousness control, but for the data centers actually. Then after we first implement the IFRS AQMs AQMs, this is a dual P2 actually. So it isolates the Alphores AQM isolates the scalable congestion control and the classic one So we first implement this, then we upgrade our sanders to have TCB product and accuracy DCN. So it means that in this ideal scenario, let's say this is just an example from the document, but let's say this is an ideal scenario In this case, Alperest flow remains isolated from classic flows because we first implement the Alphores AQM on the router. However, we can say that in other sequences scalable flows can coexist with classic flows at Al-Forest or non-AlForest puddleneck routers So there are like to be scenarios such as Al-Forest flows will traverse non-alforest acramps, such as peering points, non-cable access things for G5C Wi-Fi satellite This could also be also, could also have another challenge is upgrading legacy access network middle boxes, such as Wi-Fi routers Worldwide to support Alfores will be a challenge actually. We need to upgrade this middle box to support Al-Forest, so it will be also a challenge actually these are the points that you want to list list and and this during this incremental deployment there are some to encourage this incremental deployment, actually we have some items we list First one is, an alfrest flow should have throughput and delay characteristics, at least as favorite as a classic flow even if some elements of the full architectures are missing to encourage us to implement AFRS widely actually And also, of course, an alphores flow shouldn't be harmful to classic flows during this incremental deployment. So encourage us to implement alphores actually"
  },
  {
    "startTime": "01:26:02",
    "text": "actually And in the literature, actually, if you also look at it, if you're meeting notes, we have some also early tests already. And this tests raise some concerns. I just want to put some of them here. You can find more. These are the codes from that early tests. First one is for instance alpharest flows dominate non-alphores flows, whether ECN enabled or not when they occupy a shared RFC 3168, is the classic ECN, single Q actually This case is problematic. It's also listed in the early test, and it's numbered as issue number 16 in IATQ meetings And another one is the dual pi Q is that for us a QM actually, QDIS, introduced a network bias for TCP Prague flows over existing cubic flows. And another one, says that his product behaves approximately like Nevitano and is outperformed by cubic in FIFO bottlenecks And it is difficult to see, in this case, we can say that it's difficult to see where Alpharist scalable throughput claim is justified here So these are all the three codes to just show you. We have some concerns from the Earth test also. So also we try to evaluate these things in our experiments. And we try to evaluate TCP product, a scalable congestion control, I already mentioned, with this question in mind, actually, given that bottleneck router may or may not have a dual QA a scalable congestion control, I already mentioned, with this question in mind, actually, given that bottleneck router may or may not have a dual QAQM, this Al-Forest ATM, and given that the other flows sharing the same bottleneck may not be TCP prep flows, what benefit can ascender expect from unilaterally switching its own congestion? control to TCP Prague? This will be our question in this work by considering all of these things I'm already mentioned And we can continue with our experiment design We used we did our experiments on fabric tested and this is our topology. Here you can see we have two senders, two receivers, and we have one alfrest sender, and this sender sends single TCP practical for 60 seconds, and we have a another send for classic TCPs. It sends single"
  },
  {
    "startTime": "01:28:03",
    "text": "classic TCP flow. I will mention this, which classic TCP flows are we used for 60 seconds again And we have Alphoris receiver and classic receive And on the middle, actually, in the middle, actually, in the middle, we have delay note and to give the base RTC and also we have bottleneck routers And we think that, actually, if flow is most likely to encounter a bottleneck either at the peering point or at the X access link. So we try to emulate network connection that are representative of an access thing, in this case 10 millisecond base RTT and 100 megawatt per second buttonneck link capacity. We use these parameters in our experiments experiments And we use six different bottleneck types in our experiments. You can see that in this table the first one is just single FIFO we are already know that I think it's just a single Q drop-tail Q actually, it's FIFO. The second one is FIFO, but with ECN support The third one is KODAL ACM this is an ACM and with ECN support. The fourth one is FQ, it's the FAIR Qing, with means that there's some flow isolation in this button and it also has ECN support And the fifth one is FQ codal. It is actually similar to FQ, but with codale ACM for each Qs, for isolated Qs, and also it has ACM support. And the last one is the proposed IFRS AQM, dual pi2. It has a dual queuing system, AQM, and ECN support, and also it has shallow ACM marking. It has two cues for AQM, dual PI2. It has a dual queuing system, AQM and ECN support, and also it has shallow ECM marking. It has two cues, one for Al4S, one for classic, and for Alphores queue, we can apply very shallow ECM marking, so it's one millisecond And as I said, we use different classic TCP congestion controls. First one is cubic. It's a lost lost-based control and it can support ECN The second one we use BBR version 1. It is a rate-based one and it doesn't have ACN support. And the third one, we use BBI version 2. It's also rate-based and"
  },
  {
    "startTime": "01:30:02",
    "text": "it can have classic ECN support and also in the later versions, it can also have accurate SCN supports And all of these classic TCPs will compete with TCP Prague, which says scalable congestion control with accuracy in our experiments And we can continue with our key findings We will examine all of our key findings actually, and under first one is under what circumstances is TCP performance at least as favorable and not harmful to classic flows? and we observe that i can show you here you can you observe that here the throughput of TCP Prague in these plots for different cases And on the y-axis, we can see different AQM types button-neck types, and on the X-axis you can see the different buffer sizes and we can say that practice on the y-axis, we can see different AQM types, button-neck types, and on the X-axis, you can see the different buffer sizes. And we can say that it is fair share of throughput with fair coordination actually a bit obvious thing with fair queering we can get almost fair share but he's your program but we just want to show this, to show that, yes, fair curing is worked very well with TCP Prac actually with cubic and also with BBI version too, when they share the bottleneck with And another one is under what conditions, TCPRug performance is not as favorable as but still not harmful to classic flows. And I can show these conditions in our experiments like visually here Here we can say that with cubic without ECF support, compete with product with ECN in with ICN, in FIFO with ECM. Actually, in that case, cubic, the flow doesn't support ECN, dominates the product actually. It's also a bit obvious but here we want to show this because it can happen if any of the sender or receiver doesn't support ECN or there's some ECM belief in the, in the network, actually, the internet, such as intermatives network device can clear the ECN flag can all of these things could happen so this could be a"
  },
  {
    "startTime": "01:32:02",
    "text": "problematic case if there is no ECN support with FIFO plus ECN case with classic VCP flow without ECN there's some fairness problem. Actually, you want to show this And also similar thing, of course, it happened with also BBI version 2 here, BVI version 2 versus Prague Also, we can observe this And we have, we observed another thing for dual pi 2. We, in our experiments, we observed that BBR version 2 without DCN support compete with a TSI Pract with accurate DCN on the dual pi 2 aQM, which is the proposed RFQM We observed that you see now the TCP Prague flow numbers actually it means that BBI version 2 without ECN almost dominates the prack with ESEAN even with dual pi 2 Q actually We want to mention this actually, we think that it's not well been, have been studied well so and this could be a problem in the case because it shows that even you implement dual pi 2 on the routers there's good be some problematic cases in the in the real networks And another condition I want to mention, under what conditions TCPABAC performance, not as stable as but still not. Actually, it's the same condition previously, but now I want to emphasize the BBI version one instead of version two or T cubic in this case we observe that for shallow buffers, with shallow buffers, actually BBI version 1 dominates Prague in most of the AQM types, as you can see here But this is also a listed issue for Renault and Kubik in, let's say in IMC 2019, paper from where. They also shows that BBR dominates Renault or cubic, which shallow buffers. So this issue is not directly related to Prague but it's similar issue with Renault and cubic we also want to mention this Another thing, actually, this is the problematic case Under what cases, TCP of Prague performance,"
  },
  {
    "startTime": "01:34:02",
    "text": "harmful to classic laws. It means that product dominates the classic flows actually I want to show this conditions shortly Here, as you can see, actually, this already a well-known issue Cubic with classic ECN, if there's a single queue with classic ICN support, if TCP Prague competes with classic ICN with ICN, classic TCP flow with ECN support, Prague dominates the classic ones because of these different soft-sooth behavior, as I already mentioned in the beginning of the presentation, I showed the different behavior of classic ones and scalable ones because of this, because of different reactions of different TCPs, product dominates the other one, actually. This is the problematic case And also we observed the similar thing for BBR version 2, Prack versus BBR version 2. We also observed that practice with single QAQM, with classic ICAN support Prack dominates BBR version 2 2 And also in, you observed same same thing, Prague gets more than its fair share. We also codel with ECM, but cubic without ECM case. In this case, we observed this because in Codell EQM, in our implementation at least, Kodal dropped the packet for the flows who doesn't support ECN when it's reached the ECN threshold. So because of that, product dominates the cubic flows without ECN, it's CODALAQA.M, actually And also the similar thing have for BBI version 2, also I am showing the old cubic and version 2 results, BBRIDV2 results And also we observed that it FIFO without an ECN support, a single drop tail queue we observed that product gets more than its fair share when competing with BBR version 2 This could be also a problematic case in terms of fairness And to solve this single Q with classic YSien,"
  },
  {
    "startTime": "01:36:02",
    "text": "unfairness problem, as you can see in the in the middle part, I already showed this. This is a problematic case To solve this, there is some option in the TCP Prague to ICN fallback HERS stick, which is called holdback holistic. In this, in this, it's actually optional in the TCP Prague, but my it's not owned by default, but we manually own or enabled that and we observed that it's solves almost solves this problem famous problem because in this holistic, it tries to detect the Q type and fall back, the TCP product falls back to the classic TCP behavior. So it solves the famous problem for single QAQ with classic ICAN, but we also observed that it could call some other problems. For instance, in our case, we observed that with dual pi 2, if we enable this algorithm, Prague starts to get very low throughput compared to the cubic for instance, in this case. So this optional feature can call some other problems when it solves this main problem actually. And also this similar thing also happens with BRAC versus B can cause some other problems when it solves this main problem actually. And also this similar thing also happens with BRAC versus BBI version 2 and we can it's the last one actually it is her product performance is better than classic laws. We want to see this uh, these conditions. Now we will see we will look at the latency values here. These are not throughput. These are the quing latency values for TCP Prague. And we observe that with different ACM types and different paper sizes and different scenarios, cubic VBR version two with DCM or without UCM, we observed that you can have the latency benefits only with dual pi 2 actually. It's the level is still one minute second. Here, you can see that with dual pi 2 product have very low queuing latency because it has one millisecond threshold for low way latency queue, we can say this and also as a"
  },
  {
    "startTime": "01:38:02",
    "text": "said, there's another option for BBI version 2 with Accuity ECM We also tried this In this case, you see again, latent queuing for Prague and BBR version 2 this time. In the left side, you see the values for TCP Prague. And in the right side, you see the values for BBR version 2. And they compete on the same bottleneck And in here, again, we can say that BBR version 2 Accurudicien also gets the low latency benefit of dual pi 2. You can see that the green latency is less than one millisecond for both program BBR version 2 with Acute DCN. So we can say that dual pi2 works for this case also As a summary, we have a table. This table shows that it is okay to turn TCP Prague or not And in this table, we can say that as I show in the beginning of the results, with fair queuing it doesn't depend on with ICN or without ECN. Actually, with ICAN, fallback of our own, actually, it's horrific actually, or it could be called BBR version. It doesn't depend on this. We can, it's safe to turn on TCR product because it provides fair share between the program and other classic TCP ones but the other cases, it depends on the case actually. Sometimes it's not good to turn on TCP Prague, sometimes it's good in terms of fairness of course And but we should note that of course the sender, doesn't know what's the bottleneck type. So maybe it's so because of that it may it might be reluctant to turn on TCPa Prague we should keep in mind. And yeah this all actually thank you very much thank you very much questions I did see greez asking a clarification uh don't know if chris wendt to bring it up or with this slide it's 50 the perfect result. I think the further away from 50 the worse. It's considered, I'm not really sure what he was talking about"
  },
  {
    "startTime": "01:40:02",
    "text": "but result. I think the further away from 50 the worse. It's considered, I'm not really sure what he was talking about. With this slide is 50 the perfect result I think the further away from 50 the worse it's considered But anyhow, maybe Chris. In the results, the values you mentioned i think the values here shows the prior, it shows the, actually throughput values. So throughput person to show TCP products. So 50 means that it's a fair share between prague and other at these issues so wen lin our case we evaluate this result in terms of fairness. So 50 is the, yes, most fair case if we say, yeah Thank you. So we have Silberster in the queue please go ahead hi i'm sylvester from erics and uh Have two questions. Have you considered rerunning this analysis when BBR weather to verify whether you should introduce BBRV2? Because like I think that Google is not this cautious. They just like put BBR out in the wide and it's like similarly it can create similar situations. So should we, should the research community be? discocious with introducing a new congestion control? yeah we just we just evolution actually yeah with TCP product one PCB product flow, one BBR version two flow, and yeah of course we need maybe further evolutions, but yeah, I have just this and yeah, we didn't consider too much about the other version 2 case, but yeah we we have just these results for now and yeah we can discuss maybe later more and other questions is what's your opinion about congestion control evolution in general? Is this kind of like TCP fairness? keeping TCP fairness as something we are targeted? Is this a very forward really in condition control evolution or should we ensure? fairness in other ways? Or they're really like these shared buttons? why are you considering just two flows and these reason? small speeds? So is there another way to evolve condition control and make sure fairness is solved, for example?"
  },
  {
    "startTime": "01:42:02",
    "text": "by some other manism? you say is there another way actually i yeah, I don't know. For now, actually, we just try to evaluate with this existing architectures and Yeah, I need to think about that more actually because there's like more less and less bottlenecks like out in the wide in the internet, more and more in the access network access network can be regulated more, even flow fairness can be maybe provided by other mechanisms than condition control itself yeah yeah thank you thank you very much Thanks, we were there. And the next one in the queue is christian please go ahead hello this cushion I have first a remark on the previous question The answer on that previous question, is in the draft of 50-43Bs which states that we are not supposed to actually go for fairness but you have to make sure that you don't starve the other guys. So, for example, if you end up with doing 6040 instead of 50-50, that's probably fine And even 7525, we fine. 99 will not be fine. That's clear okay so that that's just a side effect technical question on your measurements I see that you are doing a lot of simulation and that's good that's a great spread of measurements it's interesting did you what kind of latency did you use? for those simulations? What do you mean the basic RTC? How do I implement the base RTT, you mean? Yeah, what? would be the mean RTT of the of the links? Actually, yeah we have 10 minutes second base rt and we implement with netam I implement the base RTT, you mean? Yeah, what will be the mean RTT of the links? Actually, yeah, we have 10 milliseconds base RT and we implement it with NetAM on the Linux. These are the real experiments, actually on the fabric testbed. So we implement base delay by using the delay note we have. We use NetAM"
  },
  {
    "startTime": "01:44:02",
    "text": "delay on the software on the Linux So this shows the queuing latencies actually for this is your track these are the so what the figures we have here are the total end-to-end latency It's a queuing latency, not ent-ent. No. The number are the absolute values for queuing latency, for TCE PRAC. The queuing latency at the bottleneck or the queuing latency at it and see it this is the entrant latency minus space RTT. Okay. Now, what would happen? if your base RTT was different? Yeah, actually, we can also measure this, but I don't believe the main messages will change Oh, it does. Yeah well, it does, because the problem is that Prague, specifically is best on the winner well, it does because the problem is that Prague specifically is based on Reno and we know stops operating if the BDP is too high when basically because we know in a congestion and transport ramps up the congestion control very slow if it goes down, it will then take it a long time to catch up if the BDB is very high. And that's part of the range there that could be shown, I mean, if you if you read it those same thing with a hundred millisecond delay I am concerned that you will find something very different you are right but yeah in this work we've specifically focus on 10 milliseconds base RTT actually in other experiments, we try to use 10 milliseconds 50 millisecond, 100 milliseconds, RTTs In some cases, yeah, we have some different results, but generally, yeah, we should specifically look at these different scenarios. We are right. I cannot comment on generally what happens in the different artists. It's not easy to say that"
  },
  {
    "startTime": "01:46:02",
    "text": "because we have different ATM types different buffer sizes But yeah, it's probably, yeah, it's cool to be worth to look at also that scenarios. I mean, different base RTTs, we can also evaluate that you are right But in this work, we don't have that. Yeah, that'd be nice too have. Thank you very much. Thank you very much person in the queue is Greg just go ahead thank you for doing the study um one thing i want to point out is the six bottles types that you tested, two of those are not really deployed much in the Internet, the FIFO with ECF and the single Qconnell And there are a couple of other bottleneck types that might be interesting to add in place of those or in addition. So an NFQ bottlene with a shallow threshold ECN marking so that's basically L4S aware or FQ And the other one would be a single queue with a classic packet drop AQM. Those are commonly deployed. Yeah, you're right Actually, I'm aware that FQ with ICAN support also available, and yeah you're right. You can add that. Thank you very much for your call And I'd like to second what Christian asked about testing other RTTs and also other rates as well because congestion control dynamics definitely depend on the BDP of the link Yeah, thank you very much. Thank you Next person in the queue is tianji jiang Yes, Kenji SamCC here uh especially for this uh to set up on this page the first line of bottleneck being shared between the Prague and the Lexi flow I remember in your architecture, you have a single bottleneck It's like only simulated rather here Have you ever tried the parallel paths, but that is also the bottleneck, but the one, you don't know which path, Alexi, or L4S may go through. So have you tried some? like that? No, you just tried for now"
  },
  {
    "startTime": "01:48:02",
    "text": "That's just the topology, but you are right. We can use different various scenarios also We can generate, but we haven't tried that yet. So for this case, you know, because sharing the same bottleneck, it might have some coordination between the you know the lexie and the ECN marking L4S flows But if you have parallel paths on the same on the same point with the bottleneck how can you do the coordination among the two parallel? paths for this type of study? Do you mean parallel? Do you mean? different routers? Yes, different routers. Different brothers Yeah, yeah, yeah you mean parallel pattern do you mean different yeah yeah yeah so yeah it depends on actually this works about where is the bottleneck actually? The important thing is that so actually it's a routing problem I think it's a bit very, where it's goes to flow, which, and it's, if the flow goes to the actually it depends again depends on the which but which a cam type will be on that, but it on that router so i know which one is with the bottleneck so we can also figure out okay no problem yeah We haven't done it. Okay, sure. Thank you Thank you. And last question from Sylvester again Hi, again. I was always wondering why TCP Prague is based on Reno and how hard would it be to change it to be based on cubic? So do you do you have an opinion about it? Do you have an insight? Why is it still based on Reno? I know this TCP was based on Reno, but that's not the reason for Prague to be based on it. Probably it's because of Rena is the most common still, or at least it started Yeah, but I mean I think, yeah, I think because of the, because of other than still, or at least it started. I see, yeah, but, I mean, I think, yeah, I think because of the, because I don't see some reason, because of it, it's whole, it's cool it can easily fall back to the reno behavior to compete, to get famous sometimes actually. For instance, in the FIFO case, without an easy support, it supports the Renault actually, instead of Kubik. You are right, the cubic also could be implemented"
  },
  {
    "startTime": "01:50:02",
    "text": "You are right, but I don't know the exact reason, actually, but it could be the inventors of this thing about that Renault is to say this one, if you want to fold back to that behavior, it would be good to have Renault in that case of this thing about that Renault is to say this one. If you want to fall back to that behavior, it would be good to have Renault in that right now, actually, the exact reason but yeah but it's still feels stupid too basically to have reno in that right now actually the exact reason but but it's it still feels stupid to base it on reno unless there is like a fundamental reason to me not be able to apply cubic yeah yeah don't know actually do It's bugging in my mind for a while and it was mentioned before so I thought that maybe you have an opinion, but thank you. Yeah, yeah, thank you it was one to the last alt and i had one clarification if you can get to the microphone Yeah, hi. So, there is a phenomenon called ECEN bleaching where the middle devices just erase these ECN markings. And I'm wondering if you have already looked at that issue and how it affects the performance because it's quite a prevalent issue Yeah, actually, we haven't checked it, but we, in the literature, we observed that this could happen so this could be a problematic case, but in our specific case, we haven't studied that, actually, yeah Okay, thanks thank you uh well thank you very much. Let's thank the speaker for answering so many questions also We have now a break and the next session will start three, which will be about security and energy. See you later later later later"
  }
]
