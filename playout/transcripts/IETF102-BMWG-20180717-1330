[
  {
    "startTime": "00:00:04",
    "text": "and all all we ask is that during the course of the meeting someone occasionally captures all the text because there are experiences in our not so distant past where the whole thing is disappeared but this is but it\u0027s really easy we\u0027re gonna stick to this agenda it\u0027s all right here and for example you just you know stick your cursor there and press return and then you\u0027re typing notes isn\u0027t that wonderful so easy to do all right I\u0027m gonna get off of that for the moment and again you get to that from the BMW G tools page under the minutes late so we have one note taker but he is a presenter could we get a second note taker to help shore up the notes you\u0027d be surprised how often that\u0027s beneficial and there\u0027s major kudos for that there are major kudos especially to the gentleman in the red Kia at the very back of the room i mike you are next on my list good try though but um yeah would you would you mind backing up and taking a second for the notes for us thank you alrighty okay okay so the rest of you can stop hiding now very good all right so just to let you know we\u0027re the the working group chairs have a new facility at this meeting which is the Chromebook which is sitting in front of us and the Chromebook means that Sarah and I don\u0027t have to dedicate our own laptops to note-taking and slides sharing and and things of that nature that that generally take up our own facilities to ask good questions and refer to our own notes and so we\u0027ve got a little it\u0027s 1331 now we might as well get going so I\u0027ve already pressed the magic button here that that scores a full screen display and then this button which gets us right to the truly useful full screen display and this apparently works with us slides that are sixteen by nine or four by three and you can advance them with with this thing right here the clicker so and that works apparently so so welcome to beyond WG I\u0027m Hal Morton this is Sarah banks and we are the co-chairs and we\u0027re going to entertain you as much as we possibly can today with all the things that we\u0027ve prepared to hand off the mic to our friends and neighbors who have worked so hard to prepare slides drafts and done research at home to make this an interesting meeting our ad advisor is right here in front Warren Kumari welcome back Warren incidentally did you ever get your benchmarking setup running cool cool well that\u0027s what will spare a couple of minutes at the end to let you tell us about that "
  },
  {
    "startTime": "00:03:06",
    "text": "very cool all right so if you\u0027re not subscribed to the BMW G mailing list and you would like to do so there\u0027s a link that you can go to and get to that obviously all our slides are available on the meeting materials page so feel free to grab all of them and there\u0027s actually going to be probably especially in my decks of slides there\u0027s gonna be backup slides that you might want to refer to later so feel free to do that okay the note well it\u0027s still early in the week we have an IPR policy it means that you have to disclose all your IPR in a timely manner if you guys have a new draft today or some new material that\u0027s covered by IPR please let us know and also follow the various processes here with all these BCPs any questions about that a contribution is not just a draft or an email to the list but it\u0027s anything you say at the microphone there are a lot of new faces in here so if you don\u0027t know what the IPR is all about and you don\u0027t want to raise your hand now I didn\u0027t the first time I\u0027m either so come see us afterwards well I\u0027ll run you through but and here\u0027s a quick thing how many people are attending BMW G for the first time welcome about six or seven and one did you want to say something so yeah in addition to what L and Sarah said about the IPR stuff please go read it and talk to your legal folks and stuff if you don\u0027t understand it don\u0027t just what they said very good so here\u0027s our agenda first we do status and we\u0027re gonna look at our new charter and and then we\u0027ve got this one draft which is I think it\u0027s been working group accepted actually at least that\u0027s what I did the last time but if the draft name doesn\u0027t reflect that so that\u0027s on the EVP NPPB we\u0027re gonna look at the vnf benchmarking methodology which is now about automation according to Rafael the title has changed I didn\u0027t change it here sorry about that consideration for benchmarking Network platforms actually Jacobs going to talk about that in the online agenda I fixed that benchmarking for modern firewalls is Tim winters here yes Tim welcome thank you for joining us and for filling in for the rest of net SEC open much appreciated I\u0027ll talk about the back to back frame benchmark which is followed up with some real testing and this is what gets everybody exciting here when we really talk about laboratory work that\u0027s been done and right behind me is Paul Emmerich who\u0027s going to also talk about real stuff and the implications of the real testing on traffic generator calibration accuracy and precision so that\u0027s our planned agenda what I\u0027d like to do is I\u0027d like to I\u0027d really like to get to item 7 about 20 after 2 something like that so that gives us about 40 minutes for the testing stuff which i think is going to get really interesting in Rawkus so "
  },
  {
    "startTime": "00:06:08",
    "text": "let\u0027s let\u0027s leave plenty of time for that and and two minutes again for Warren to tell us about his benchmarking experience okay any questions about the agenda any bashing needed everybody\u0027s looking at their laptops let\u0027s move on okay so here\u0027s the quick status the SDN controller drafts for benchmarking their approved array congratulations to the authors Sarah was one of them and and I supposed to the you know that approval process will not approval but the editing process will continue now so you\u0027ll eventually be contacted by the RFC editor to clarify a whole bunch of stuff I\u0027m sure nothing goes unscathed there so the proposals keep coming I\u0027ve already talked about these industry discussion topics this is what\u0027s going on everywhere in our industry we\u0027re talking about buffer sizes the assessment of them search algorithms and the traffic generation calibration that\u0027s it\u0027s actually happening everywhere but we\u0027ve got some good talks on all of this today so here\u0027s our current milestones boom excuse me and I I let\u0027s see so the one that we originally planned to get going very quickly was in August 2018 methodology for next-generation firewall I think we\u0027re probably gonna be a little bit behind and missed that but that\u0027s alright all of these were really aspirational and the next step here is to I wanted to I wanted to mention that you know our reach are during discussions basically two things happened one is that we codified our permanent attention to the virtualized network platform benchmarking here that\u0027s now sort of a written part of our charter it was always it was a bullet item for the last three years and now it\u0027s a explicit part when we where is it oh yeah the one in the middle there draft on selecting and applying models for benchmarking to iesg review that was actually the the thing in our Charter where we got the most comments it actually blocked the approval of our Charter for a while and and the reason that happened was that in fact there\u0027s lots of models being prepared for network services and possibly applicable models being developed around the ITF that we need to pay attention to so so we have this draft on the table it was updated it was updated back in May actually but the author Shaun Wu didn\u0027t send anything to the list about it so it actually kind of surprised me to see the update there but we\u0027re still I mean we\u0027re still looking at this topic and what we need to do though is look a little more broadly than the proposal we\u0027ve received that\u0027s the feedback we got from iesg it\u0027s good it\u0027s good cross ietf feedback otherwise we might get all the way up there with something that is "
  },
  {
    "startTime": "00:09:08",
    "text": "just gonna get crushed with discusses blocking comments any questions about the milestones I think actually that that draft on selecting and applying models we changed that to December 2019 - so I\u0027ll fix that next time all right nothing on that so we did our Charter update as I mentioned we have a supplementary being busy page thanks to Sara\u0027s efforts of restoring it and we sort of keep track with the work proposals we\u0027ve adopted the work on evpn PBB evpn but we still have to have you know more review of this and more comments and so forth before we go further with it it\u0027s definitely on the Charter though and and so that\u0027s good the rest of this stuff is kind of represented in the Charter as well although we haven\u0027t seen much on SFC service function chain chaining lately and actually vnf is gonna have to be really titled vnf benchmarking automation so that\u0027s actually evolved a couple of times here now back to back frame service layer abstract model should retire titled actually that\u0027s what the draft says so that\u0027s what we have and next generation firewalls so we could sort of keep score here helps to organize we also have a standard paragraph that helps people understand who review our drafts from outside the area this is kind of like you know this is this is testing for the laboratory security people don\u0027t flip out because we\u0027re saturating interfaces and stuff like that I mean it\u0027s just it\u0027s just advice to the world that that if you haven\u0027t read our Charter it\u0027s kind of encapsulated right here in these few sentences so when we write drafts or try to include these another key point and and I\u0027ll mention this later again but there\u0027s a new draft on not draft an RFC on our requirements language are our definitions of words like must and shall and recommended and not recommend it is actually kind of a new one though all those terms were originally defined in RFC 21 19 by scott bradley and the original version of that was actually developed here in the benchmarking methodology working group but RFC 8174 which is fairly recent improves that terminology and and explicitly makes it clear that when those terms are used in a draft or an RFC in all caps that means they take on the meaning that appears in our c 21 19 so shall is a requirement an absolute requirement must is an absolute requirement and when those you see those capitalized words that\u0027s exactly what it means that\u0027s now clear with 8174 and there\u0027s slightly new version of this typical terminology paragraph so I encourage "
  },
  {
    "startTime": "00:12:08",
    "text": "everybody to adopt that I just had two graphs two drafts go through the iesg and got dinged on this for both of them so I went back and fixed all of my stuff and I encourage all you draft authors to please do that and that\u0027s it for the chairs unless you want to add anything there sorry no okay good well this thing worked astoundingly well and so we will now ask Jacob not Jacob suiting Jacobs your last thing and shooting Jacob it\u0027s going to come up and talk about our tenants pardon we promote it in use I stop in here do we have remote attendance attendees yes we do yes we do we got Brian I\u0027m Paul Morrissey and and Renzo\u0027s part of the team Sam so welcome remote attendees all right so we don\u0027t like mostly done with this yeah look here this job and we\u0027re doing oh yeah Sarah\u0027s gonna monitor Jam jabber but there\u0027s really lots of ways to do this now I still can\u0027t access it yeah yeah shall I start please yeah a Gruffalo everyone my name is sue Dean Jacob I walk for Juniper Networks this draft is evpn PBB VPN benchmarking so evpn enter PBB VPN became the RFC in best workgroup and it is widely deployed in service provider network since to the 2014 it started as a draft then it became RFC in 2016 so it is widely deployed so there was no you know benchmarking criteria to rate this particular services when deployed in two different service promised ly the service power consumed this EVP an MPLS and PB ve VPN so we defined certain parameters to benchmark this yeah I caught it correctly I was pressing the other key okay yeah this were the comments received from the previous I mean IETF so the terminology is sections expansions and the ordering of the test cases and objectives so you know the placing of terminology section about the topology and the topology requires some more dig into it and so this were all "
  },
  {
    "startTime": "00:15:08",
    "text": "the comments so we have addressed that and these are the basic you know we had defined certain parameters you know we define eight parameters to benchmark these services then deploy it how how to differentiate you one particular when your duty when we are testing a one particular box from other howdy how do you differentiate this so based on this particular euro parameters we defined it based on that we benchmark this so it is based on the Mac learning the Mac fresh the Mac aging high-availability are scaling the scale and the scale convergence and the soak test so these are the parameters we define to benchmark these services so based on that it will be you know it will be test will be conducted and the measuring measurements will be taken and it will be plotted and rated on you know how the each box is performing it so acknowledgement as thanks Sarah for helping us and alpha the support and a lot of feedbacks and back and forth changing this and thank Sarah and Al for this support and next as her option so I like I said I think I think we\u0027ve actually adopted this but we still need to see wider review I think and so let me ask right now MIT how many how many people in the room have read the draft see two hands and he any of those actually anyone have any questions about this topic now okay all right so good does anybody in the room have routing experience testing wise besides the folks will raise their hands so it\u0027d be very helpful for the folks who have the experience to read will definitely cross pollinate this outside of the working group but from an editor perspective on helping students sort of lock into place the different steps so I think this dropped this version and the next version student has coming will be really approachable and and I think you\u0027d be able to go on and read this and say yep this makes sense or no this doesn\u0027t but it would be very helpful if you folks would take a look and give us or gifts who didn\u0027t feedback ahead of that cross-pollination into the routing area hello hi Jimmy taro AT\u0026T just a question so evpn encompasses a family of services one of them\u0027s VPLS there\u0027s access there\u0027s a number of different sorry vp \u0026 v p WS c d pn f XE v pn v pls so when you talk about benchmarking it is it specific to the EVP NV pls with the "
  },
  {
    "startTime": "00:18:09",
    "text": "multihoming and all of those features or intending also to do these other aspects of evpn no because that\u0027s why we specifically said based on only two RFC\u0027s that is you add the Ranelagh EVP and mpls a feature that is replacement of the VPLS and the PPP VPN so 7432 actually articulates the control plane to realize not only the creation of a VPLS but also a vp WS and also an FX c so I\u0027m just curious maybe you might want to I mean what I\u0027m seeing is many people using a VPN for a lot of different use cases one of them certainly is in the data center with active active active active active active multihoming and that\u0027s a huge thing but there\u0027s a lot of people also using it for access like bring me circuits across you know layer 3 network and land somewhere that\u0027s a good point Jim but because that is beyond the scope of it because fxe you know it like it is it\u0027s a point a point a point you\u0027re bundling does occur ASIS so here because we are benchmarking the parameters for a VPN granular features so you can be totally different I guess if you can efficacy takes VLANs and treats them as state and appetize is that state to index up and says these VLANs live at this next half in this context of fxa so you know if you\u0027re if you\u0027re looking at programming that on a box and that\u0027s part of the evpn scope that might be useful to people but currently anyway that\u0027s a good point because that cannot be taken into the VPLS per se because the parameters we defined it that\u0027s a good point because the fxe came later it\u0027s still running as a ietf drop zero zero earlier it was individual draft now they adopted it zero geez I\u0027m sorry yeah this is a this is a basically benchmarking this RFC because it\u0027s widely deployed since 2040 so we want to make this as the platform then the this add-on services come so so we are benchmarking the base RFC\u0027s this tool so thank you so I think what I\u0027m hearing Jim say is if we\u0027re not clear you\u0027re not clear on what your scope is in the draft and could you clarify it would that be correct slash I think he\u0027s also maybe potentially saying you may want to consider a larger scope and I think adds an author it\u0027s your right to not do that however you also made the point so my comment as a personal contributor if you\u0027re going to say it\u0027s them widely deployed since 2014 I think Jim just laid out two very typical use cases you may want to consider one I agree define the scope but I do potentially think maybe you should broaden that scope because it\u0027s been out there for four years so if we\u0027re gonna put out something that says here\u0027s how you benchmark it you\u0027d want to cover the use cases of what folks are deploying right so if there\u0027s two widely deployed "
  },
  {
    "startTime": "00:21:10",
    "text": "or two decently used use cases or three then I think you might consider covering all you can covering them at this at this point I cannot comment I need to think because the parameters because the services what Jim mentioned is totally different what evpn is kind of a you know super VPLS and this is a point-to-point service with VLAN features and all so the parameters will change but I need to think I cannot comment Jim I take your point but I cannot comment at present because I want to make sure we\u0027re capturing it the feedback correctly and giving it to you in it\u0027s totally your right to take some time to think it 3-1 come back yeah Jim I really appreciate your point just give me some time because you know I need to think I\u0027ll get back because at present I really apologize I can\u0027t give you the comments on that because the parameters I need to think because you know it right the services are different but it won\u0027t so name a pin but that one of the key points I need give me some time sir ah so I get back thank you so that\u0027s it that\u0027s a good ending point I think well well sort of leave it to you to think about this there\u0027s two ways to proceed we could add the stuff to your graph if you prefer to go that way Sudi but also it could be a it could be an additional effort that folks propose if they see a need for that so that\u0027s a you know there could be you know a sort of a part 1 and part 2 kind of approach here and I think that that could work well yeah that\u0027s a good point because anyway services are deployed a little bit different so I need to think and how to act I will say a sync up with you later or how this cabinet okay thank you thank you thank you this and we can hand that to Jacob so this draft is on benchmarking basically virtual networks or virtual network platforms so as we\u0027ve gone through this actually says NDP 0-3 should the draft there currently is actually MVP zero to because we missed the cutoff by like few hours so I have some slides on the work-in-progress of what\u0027s the next what\u0027s coming very soon as soon as we can submit again but for those that haven\u0027t seen or read the draft we\u0027re really looking at how to benchmark the network overlay and the network overlay and how to benchmark when that when that actually lives in the server world right so there\u0027s lots of different considerations when actually benchmarking where the edge of the network is now within the server or connected the VMS or containers or various different other workloads that are out there so I\u0027m really looking at application layer benchmarking the "
  },
  {
    "startTime": "00:24:11",
    "text": "different tools and tool sets that we can use for that so some of the changes so one thing that I think Samuel the co-author of the draft did we went to the nvo three working group off and kind of talked a bit about this so one thing that\u0027s actually coming in this next revamp is quite a bit of changes and actually some of the scope as well as some of the terminology so when we look at the nvo three draft specifically the architecture for network virtualization over layer three or the split and the control plane requirements those aren\u0027t really covered or clarified as well in the scope and the scope is what\u0027s kind of ambiguous because before we did that so I think with us aligning the terminology to what\u0027s already there in the nvo three working group it\u0027ll make this draft a bit more clear of what exactly we\u0027re going after in term of benchmarking and then once we update the actual good terminology as well in this it\u0027ll dice the aligned and then one thing the split MBE is one thing we haven\u0027t covered that we\u0027re adding in it\u0027s kind of a work in progress the split NDE is actually the network then de for those unfamiliar as a network virtualization edge so the split MBE is when it\u0027s split between say part of its living within the server and part of its living in the top racks which isn\u0027t probably the most common example and there\u0027s a lot of control plan considerations that go along with that very specific on your what\u0027s being tested and where it\u0027s being tested yes exactly that\u0027s what that\u0027s what their needs that\u0027s exactly like why we want to kind of define in each test like here\u0027s here\u0027s if it\u0027s co-located with hypervisor vs. but and have kind of different considerations for each and that\u0027s kind of the big the big updates that we\u0027re going after in this specifically I mean if you look at the 83 94 there\u0027s there\u0027s some ambiguity and how things come to an agreement they there\u0027s literally the one sentence it\u0027s like okay when when they come to some sort of agreement and it\u0027s like okay hey that\u0027s an interesting yeah okay so we need to kind of come up with benchmarks to say okay how do okay I get it this ambiguous and every implementation is different but how do you benchmark it so that you know exactly what\u0027s happening when it happens for it so so Jakub this is this is the start talking about the overlay layer yes in the previous slide which I don\u0027t even want to try to go back to yes there you\u0027re talking about application layer benchmarks yes so we\u0027ve got kind of this you got overlays an application layer I\u0027m trying to get the scope clear in my head here it\u0027s got its got application layer stuff and over weary stuff which is I don\u0027t know I mean I\u0027m not sure how that fits together yet I think the the application meaning you could call application or layer benchmarks but I think the goal is that defines also some benchmarks that when you\u0027re benchmarking from a server itself "
  },
  {
    "startTime": "00:27:12",
    "text": "what are those have those benchmarks change so instead of like instead of just having a traffic generator that\u0027s blasting traffic at unlit Sun stateful you can actually go in and okay here\u0027s what are the application layer benchmarks you can do to actually benchmark this in a different way I think that\u0027s the okay okay the thought behind it yeah I mean I have read most of this and I think I see where you\u0027re going and let\u0027s work on some additional worries I definitely got some comments to share with okay but I go yeah but that\u0027s the first one I mean it\u0027s all it\u0027s usually all about carefully defining the scope and then we can yes usually do this I agree and that\u0027s I that\u0027s I think the biggest the comments but here\u0027s like the scope the most comments and questions around clarifying this go so we\u0027re hoping if we can if we can go actually use the nvo three working group and the RFC had already defined there as the scope then we can kind of get to something that\u0027s more kind of clearly defined okay so yeah if I agree though the application piece needs to be I mean but really what you\u0027re talking about is transport layer right yeah yeah TCP yes exactly so I\u0027m a few things so from eight zero one four which is the architecture the updates that you\u0027ll see coming shortly of from the terminology standpoint aligned to the terminology and additional updates around attaching and detaching state and the state state changes specifically when when the tenants are like means AVMs are coming out beam\u0027s change it\u0027s also clearly defined and 83 94 for the split and B as well where the VM state changes our creation events migration events termination events all these things we\u0027re not we\u0027re kind of increasing the scope from just the gears of the traffic pattern but how do you benchmark the kind of that state changes so these are these are like life cycle events it is yes yeah and and and it actually says it right there we are and we haven\u0027t had we haven\u0027t had any benchmarks on that kind of activity before so that\u0027s a great place for this to do some expansion for yeah exactly exactly do you plan on bringing a test result in yes absolutely we have a bunch cool cool okay so look forward to the next the next meeting when we actually have all the drafts of his release submit it again and then put some test results in there and and and just to get it out here it looks from what I\u0027ve read and and what I see here now again it this is purely hypervisor focused a lot of the world is thinking cloud native and Linux containers systems so I mean but that stuff\u0027s not there yet the hypervisor stuff is here now yeah so they yeah even in the 84 what is it 80 14 they they kind of call that out as well it\u0027s like these things could be used for non hypervisor workloads like just need a container workloads but it\u0027s very specific to hypervisor today yeah but though I think they have to do even "
  },
  {
    "startTime": "00:30:12",
    "text": "further work to get to that point where it\u0027s container but I mean it\u0027s they\u0027re actually in the world today so it just will have to work its way way in there now no yeah that\u0027s pretty much all I had how many people actually read this thing or have take a look I have you okay appreciate any more input feedback other wait the as soon as a it opens again will submit this new one okay all right that\u0027s all fast a minute now do you mind sending a note to there yes it\u0027ll list and and I will I will promise to send my comments on o2 okay sounds good all right oh cool thanks thank you thank you any other questions I guess yeah for the folks who\u0027ve read it or just have some general questions Rafa in the race of convenience my only concern is like I\u0027ll say initially I think I read the draft like like six months ago but my main concern was like about the approaching not only VMS but also containers so i thought that the the draft was like very folks on a hypervisor but if you bring that world I provide her two containers it might might change the terminology little bit yeah absolutely I think I\u0027m yeah definitely the I don\u0027t know if it if in this scope of this document or if it\u0027s like then containers or other workloads outside of the hypervisor and if there is no hypervisor what does that change from because we\u0027ve known being very specific around like okay here\u0027s the or for the splits there\u0027s like there\u0027s at env and then the and then enemies and this external and II thing and and there\u0027s very specific things to like okay how the hypervisor does this but they\u0027re so I mean it\u0027s it\u0027s if we if we go this direction it\u0027s going to be hard to D cup alit and say okay now here\u0027s the container thing but I agree there needs to be a separate container work work effort I don\u0027t know how to do that but it\u0027s just an idea maybe another draft yeah I agree I think if it was another if anyone\u0027s interested in taking on that work after we finish this one up you\u0027ll be really great to have kind of a follow-on to the container networking or container plus plus I mean there\u0027s a bunch of other things that have to go with containers and service meshes yeah so I agree just with a personal hat on to I think getting through this will set the bar and all the bumps we\u0027re gonna work out and then how and what and what applies to containers or what new stuff comes in afterwards I think it\u0027s a better round to and a new doctor yep good thank you no questions thanks thank you oh that\u0027s my right I\u0027m just one let\u0027s go "
  },
  {
    "startTime": "00:33:12",
    "text": "back here okay Tim welcome this is your first time doing this here stand in the pink box yep pretty oddities right you click it click the clicker got it the slides advanced and everything hopefully goes well correct so this is next-generation firewall benchmarking this is actually the work of a group called net sec open that is a group of about 15 individuals who are working to getting together we get together once a week have a comments call and talk about these types of test cases so we\u0027ve done two drafts this is a third wrap I\u0027m really going to focus on the differences that we\u0027ve had if you want to ask any questions about any of it feel free there might be some of the test cases where I\u0027m not slightly less involved where I might defer I\u0027ll go back to the group and come back to you guys if you have any specific questions so the first thing we did in the third work of this draft we reordered it so if you look at the diff it looks like a mess because we deleted like entire sections and move them to the bottom and move them around so they\u0027re just kind of hard to read worse I won\u0027t do it again this was a one-time thing so there\u0027s a lot of reordering so it makes it look like they were massive changes the only other section we touched editorialize that was worded kind of funny because different authors and this kind of gave it one voice so 7 7 through 7 one got a little bit of a rework those are mostly editorial changes not major detailed tests test cases 7 we had a bunch of test cases in particular HTTP throughput in HTTP throughput and we removed the binary search requirement after talking to some of the test and measurement guys this was kind of overly complicated it was just easier to take out that requirement that we always do a binary search so we removed that from the version to the other draft we cleaned up the objectives of 72 and 73 in particular we had a spirited conversation about object sizes and those are the ones we settled on if anyone has any feedback about those types of things feel free to read the draft and send comments or get up to the mic those are what we\u0027ve chosen for now the second version of the draft did not have any real test cases for 7 3 through 7 6 those are all brand new in this draft where we wrote down the individual test cases so we\u0027re closing in on getting some of these wrapped up some of them also it\u0027s sort of a rinse and repeat situation where once we wrote it for one technology we could just input input the different parameters just made it pretty easy so once we got the general setting up we were able to update those test cases um before you leave that one yeah can you say again why you removed the binary search requirement I guess I missed that overly complicated was what "
  },
  {
    "startTime": "00:36:13",
    "text": "the test and measurement guys were very concerned in particular we work with this group includes xes by those types of test and measurement vendors said the binary search will really complicated some of the results that we were going to have to get out like there is easier ways for us to get to the throughput instead of actually making a binary search be done for over all of the sets of results in particular it\u0027s not easily automated was their explanation so we just removed it I think the current text is Fluffy\u0027s not the right word I think it just says how you get the value is sort of up to the test or how they want to go about getting the individual food plots alright we\u0027re more than willing to take feedback on that sure but let\u0027s talk about it after you see all the stuff that we\u0027ve just done to try to improve binary search yeah that\u0027s sort of what we started to go down that rat hole and decided it for the moment we just took out the text as we didn\u0027t think it was super it was stuck in there I don\u0027t think anybody felt super strong about it okay okay all right good so you know going forward I don\u0027t know how many people have read the draft obviously we would love to get some on lists revision I will say there is lots of review going on inside of this group before it gets listed there\u0027s like I said there\u0027s somewhere between ten and fifteen participants who are reading these test cases as we write them but the more the merrier you know and let me just say that there\u0027s another way to do this and that is I mean you guys have face-to-face meetings but you know if if people were willing and in this group to join our mailing list and to share comments that the during the development of this that will look a lot more yeah it\u0027ll look a lot more engaging to our group yeah the folks who are kind of being you know just presented with a big document once in a while yeah and and an activity on the list is the best way to get some attention pay attention get some additional readership so yeah I think we\u0027re getting to the point now where the test cases are stabilizing that I think we should start to move the conversations over I think before we were working out lots of details might it kill the list but I think maybe after this next revision I think absolutely they should start to do all of the stuff on the lists great again like you said just to show interest because there are people that are interested absolutely on it absolutely yeah so I will make that suggestion yeah we\u0027re working right now one of the things missing before we think this is ready to go is it doesn\u0027t have the security effectiveness in particular you\u0027re talking about vulnerabilities and things you need to do the list is closed we have an idea of what we want to say about this or how we\u0027re gonna go about getting that list so that\u0027ll be in the next revision of the draft and then we missed a cup we still have one test case that we didn\u0027t just didn\u0027t make it to the draft so we\u0027ll get that in the next go-around okay very good if anyone has any "
  },
  {
    "startTime": "00:39:13",
    "text": "questions I don\u0027t see any line to the book no one\u0027s running to the mic so I think we\u0027re gonna be oh you are coming for me alright go ahead Jacob rap viewmarq so this document itself is really focused on like the physical appliance based firewall correct I know you type in a thing as well but yeah it it doesn\u0027t specify it I would say I think most of the people involved are thinking hardware box but a virtual firewall could run this test because as soon as we go over to virtual firewall versus a distributed firewall two different things as well like a VM firewall based versus a agent and a you know and a guest versus something living in the hypervisor very different yes absolutely so in our case I think we\u0027re really thinking more self-contained not distributed firewalls okay okay is there is there worth noting in the scope or something like this is focused on I was just trying to think about that when you\u0027re on ask me I have to go back and read it oh I will go back and read it and make a note myself I think we\u0027re not specific enough about what we actually mean when it comes to that because you\u0027re right I\u0027ve distributed firewall is an entirely different okay situation so I will make sure that we\u0027re very clear about what type of firewall we have listed in there and there was a point there was a point I wanted to make while you guys are facing each other and that is when you were thinking about application quote code TCP layer yes benchmarks that\u0027s some of what this team is doing as well yeah there\u0027s some chances that was when I saw the one actually where we can look at some terminology and definitions yeah they\u0027ll be awesome like just like things like what a stateful mean like there\u0027s okay everybody has a different definition and stateful when it seems like it should be something that this clearly doesn\u0027t we can agree on yeah absolutely well take a look we\u0027ll take a look at your draft to see if we can sync some of that up so that we\u0027re talking the same words for sure all right any other questions from the group no okay well thanks for the excellent progress yes the only thing they did want me to ask is what are we doing for working group adoption for this dress so what we\u0027re saying is not many people on the working group reviewing this oh we\u0027ve got it we gotta have activity to demonstrate an interest and then sort of like the last the last three columns of the work proposal summary matrix and once we see significant support at meetings and on the list and then we can start to ask for a group adoption perfect that\u0027s what I thought you said but I just didn\u0027t confirm thank you so much good thank you Rafael I completely blew the agenda you were supposed to go before Jacob and him and and uh and I\u0027m now gonna demonstrate how it happened let\u0027s see here so I got to back this thing up and I thought I had them all in order here but that\u0027s now see that\u0027s mine that\u0027s mine and somehow that one "
  },
  {
    "startTime": "00:42:14",
    "text": "got mixed missed so let\u0027s see here yeah that\u0027s right right there it\u0027s that one all right so this magic button helps and then go this magic one else yeah okay go so I\u0027m here to talk about the the updates that we made in this draft starting by the title I think we we had a lot of reviews in the last meeting about okay you\u0027re addressing generic ways of vnf Mitch marking soul and it\u0027s like based on the research behind this dis draft there\u0027s a lot of focus on automation so we said yeah we agree with that I think that\u0027s some consensus we had and then we change the title for methodology of net bitch marking automation so why we dated basically the main changes are about automation as we discussed in the last meeting I mean there there\u0027s the new title the scope has changed we added there\u0027s some modifications in the methodology the idea is that we do not aim to explore all the UNF bitch marking methodologies but ring we aim to create this kind of like approach of methodology for if you have a DNF bitch benchmarking methodology we have this cope of how to automate that when created this is collection of tools that we have this I mean efference implementations that we can we can play with so because we also have the idea that the vnf developer knows better the metrics and how to benchmarking did the methodology so if you provide all the tools and kind of like a generic methodology for that to be automated we we understand that the developer can do that better than us and I think that new coming gnf method benchmarking methodologies might use this draft as a reference implementation for that we have also new contributors so from Paderborn university that are doing similar work as we are doing we have for its implementation also added to the draft that\u0027s live for just a sec for me the the biggest problem I have with what\u0027s written here it\u0027s a large part of what we want to be able to do in BMW is have repeatable results and have if I execute the test and you execute the test and we were to execute them against the exact same thing we come out with the exact same results the problem is if you leave the metrics in methodology up to the developer I\u0027m not entirely sure I\u0027d trust my developers I mean I love my developers by trust them and I\u0027m sure they can\u0027t trust but verify if I follow this methodology so I have some I have some sympathy I think I\u0027ve been at the receiving end of these comments I think I\u0027ll gave them to me once upon a time to you so I definitely have some sympathy there but I do suggest maybe we take a look at how to have at least something be automated and give yourself some "
  },
  {
    "startTime": "00:45:15",
    "text": "wiggle room but something needs to be there so that I could repeat this and get the exact same apples to apples comparison okay okay let let me get there okay all right no no I mean this is a generic I think it\u0027s a major concern about the graph how much how it can be not generic or not to be specific about the automation but I mean and here he is we\u0027re trying to restrict it to have this developer or I don\u0027t know this the creator of the benchmark methodology to come here and specify what we call it the the benchmarking descriptor and this comes the final I mean the main of the changes in the draft then we focus on say like the main focus of the the draft is to orbiting a vnf niche marketing report in an automated way and we do this by having the benchmarking report composed of two parts one is the benchmarking descriptor and the other is the profile so the descriptor is that one that is the one that I give it to you and you say like now you can repeat it and it gives some kind of like boundaries and and I Street kind of like environment and requirements parameters that you can say if you put these in the environment that you describe you it will give you the same performance profile so that\u0027s what you\u0027re trying here I know it\u0027s like we are in a zero two version but we will get there so then we in the benchmarking descriptor made mainly we described the procedures configuration the overall description of the the benchmarking the automated benchmarking scenario the target informations about the vnf itself diversion I mean that the target vnf diversion the model although the specific specifics of the vnf we have the deployment scenario that basically I forget to tab here it\u0027s basically the topology the requirements and the parameters for each one of the components that we have described it in the BNF for benchmarking setup decision in the draft and then we have the vnf performance profile that it\u0027s the result of the execution of the benchmarking descriptor I mean it gives you the execution environment that means where the benchmarking descriptor was but and then and how it was executed to create that so it gives you hardware specifications and software specifications this is also detailed in the draft and also the measurement results we we put the measurement results as active matrix and passive matrix the active ones are from the direct relationship of agents in the vnf and the passive is the monitoring or the infrared matrix from the dns when it\u0027s possible so here\u0027s the basic I mean scenario I\u0027m just saying could you go back to the last one really quick yeah where I think in the benchmark descriptor is where you would have information that you\u0027re going to basically send to the traffic generator done receiver exactly so so is so which one of these bullets covers the "
  },
  {
    "startTime": "00:48:17",
    "text": "requirements in the parameters okay okay yeah we have a requirements and parameters for each one of the components inside that and I mean when I say components is it specified in the considerations of the graph like we classify the components as manager agents and monitor hmm so the it would be a parameter for the agent like the packet tracer being used the rate and all the parameters of the - and and this goes also further because we have this idea that the agent can have Apache 8-tooth to it and call them pro base with specific interfaces to these specific technologies like moongeun for example yeah and then in this case you just need specify how to attach moonshine to as a probe into agent and put the parameters to configure that and that goes as a I mean a requirement for the test itself I mean you have must have an interface to an agent with moonshine and you must have required the parameters to execute with certain I\u0027d say generic guidelines and packet traces so so then one comment I came up with that as reading through I saw this term prober and I think in most of our our literature and BMW G we\u0027ve called that device the test or the traffic generator or some people getting those lines so it\u0027s probably it\u0027s probably better to pass through here and try to use as much well-known terminology as you can because you\u0027re inventing a lot of new stuff in order to bring this automation there and work to life but but if you can hook back to terms we all know it\u0027ll it\u0027ll improve understanding yeah I totally do new contributors now I mean we have a lot of psychos offi yeah I mean what do you find the draft and giving the ideas and the terminology was changed from the last draft and I think it will you come out to change again so nothing is written in stone in a draft so basically the idea here is that I told you have this this picture of you know you give the vnf benchmarking descriptor as the definition of the method how to benchmark the vnf you have the benchmarking process that generates the benchmarking report that contains basically the descriptor and the performance profile and with all of that you can compare repeat experiments for further this benchmarking descriptor in this case we considering the Draft at multiple steps or procedures of this benchmarking of the benchmarking itself can be automated as and we define like orchestration of the components itself the placement can be manually or automated please put in your menu or automated way the management and the configuration of the components also and then you have the execution after of the the benchmark itself and the output we consider also possible ways of pricing the matrix in a in a row format in a specific format extracting some new "
  },
  {
    "startTime": "00:51:20",
    "text": "analytics ways for example clustering matrix or or so that might be specified in the vnf paint performance profile and there are many issues that are unresolved in this draft I mean at least now we we understand we the co-authors that we have a good skeleton of I thinks that we can work from now on for example we need to clarify the automated benchmarking procedures I mean which what is happening actually in each one of the bullets that we have in the draft we need to specify clearly each one of this particular case in subsection 5.4 I mean for example when you have the case of a noisy neighbor when you have the case of failures or you have the case of flexible V\u0026F that inside of the NF itself it might have multiple components that might scale depending depending on the the traffic workload and this must be represented in the VN MH marking report yeah and so what\u0027s still missing we think that we must detail these interfaces like like you say of the agent monitor with the prober and listener this terminology might might be updated we need specify how these interfaces work actually in the draft the the action of the each one of the components like the manager agent and monitor might take on the messages and how they parse the the actions that they must take to run the benchmarking stimuli and parse the metrics the possible issues of the automation approach I think we need to find out like for example there if you are going to but for example a binary search as some way of a procedure in this automation scope I think we need to say hey this might be a problem here here and there and I mean as a consideration for for the reader of the draft in parallel we really the quarters we are developing an information model that we because we have this reference implementations there are a gene the 1i I coded in the other one TNG bench from from the manual the other quarter and we are doing tests side by side and we are creating this half restore model information model that we might be useful further the whole draft I don\u0027t know if it\u0027s in this as we talked earlier it might be in the scope of the draft or not we need to still see how we we gonna put in inside the draft or or as a half restore I don\u0027t know if formative reference for the draft and now we he I put that RFC 21 19 but it\u0027s no 8174 so yeah we are going to update the draft if that I thought I don\u0027t think we are still in the in disturb to update according to that because we have internal discussions about what we "
  },
  {
    "startTime": "00:54:20",
    "text": "should what we must and that\u0027s always kind of the most interesting yeah so and that\u0027s all note I think you don\u0027t want to just a draft being your bottom point there about 21 19 I think you also need to account for 80 170 we just say okay to go pro I had another question here what about um I mean Jim the project Jim do you I am was a was a an open source effort yeah the release is now scheduled for this this next next quarter I hope I have a PhD pieces on the same way and what about Z tango but this is already open source yeah the interesting thing is that there\u0027s some open source work supporting this yeah yeah and that\u0027s I mean that\u0027s gonna be valuable because you know you\u0027ve got the specification you\u0027ve got tools that hopefully align with it and then that means that this can land with some ability to do test automation like right now so so that\u0027s that\u0027s a valuable connection to make in the next meetings we have the plans to bring the results here I mean scalability I mean automation timing or precision accuracy or all these issues that we might face in our tools and this goes with the information model as well all right any yeah any other questions the co-chairs have done a good job grilling you but maybe someone else has something I appreciate it all right well um thank you rocky but they did good work you\u0027re next yes I am all right all right so before our while al is loading the slides where are the blue sheets did we get it back I don\u0027t know yeah one is out there somewhere you can take a look around you maybe it gets left on a seat Oh has everybody signed the blue sheets if you haven\u0027t could you raise your hands just want to make sure we capture your signature thank you [Music] "
  },
  {
    "startTime": "00:57:25",
    "text": "all right so this is the draft on the updates for the back to back frame benchmark this is where we\u0027re desperately trying to measure the size of the buffer of the device under test and RFC 25:44 our most fundamental RFC specifies a method to do this and that the thing we\u0027re trying to measure is actually the longest burst of frames that a device under test can process without loss it\u0027s intended to examine the extent of data buffering the the material there was extremely concise in terms of the procedure and reporting in other words there wasn\u0027t much there it was like one page so so we ran some tests in the vias perf project as part of the open platform for nfe last year publish the results at the at the summit and what we what we basically decided there was that a quite a few considerations could be improved I\u0027ve skipped over a lot of those here but the main one is this idea that when we measure the longest burst that a device can accommodate while that burst is being transmitted into the device the device is also thumb reading packets basically forwarding frames and the previous calculation didn\u0027t account for that at all in other words by the time you try to assess the size of a buffer and you count the number of packets which have been sent some of those packets aren\u0027t in the device anymore they\u0027ve actually gone out so so that\u0027s that\u0027s what the correction factor is simple and that\u0027s what it\u0027s all about accounting for so when we tried to calculate the number of packets in the buffer we calculated the average length of this burst the number of back-to-back frames we divided it by the maximum theoretical frame worried because they\u0027ve been sent at the back-to-back rate and that gives us basically the time of that is represented by though all those back-to-back packets put together but now I\u0027ve explained the second part of this that basically we know that the throughput over that amount of time is going to have extracted the maximum throughput it\u0027s going to extracted some number of frames so that\u0027s why we have this corrected factor here so we have the implied buffer time and we correct it by this fraction which is the measured throughput over the maximum theoretical frame rate and it turns out that reduces the buffer size quite nicely to something that\u0027s more probably a little closer to accurate now this this test only ever intended to work with a single egress port where we\u0027re sending traffic and and a both a single "
  },
  {
    "startTime": "01:00:25",
    "text": "ingress and egress port so it\u0027s completely different from the test that was described in the data center benchmarking 80 to 39 I think it was that was that\u0027s where multiple ports were used so here\u0027s the improvements mister Yoshitaka Ito correspondent with me and what we saw was basically we needed some text to clarify in the draft what these parameters are all the stuff that I just mentioned and waved my hands about now that\u0027s explained in the draft so that\u0027s that\u0027s actually better and and there\u0027s a we\u0027ve also described the potential benefit of using the correction factor so and then finally we\u0027ve clarified in the scope that this does not apply to these multi multi port and the data center techniques that were defined by a Jacob and Lucien in RFC 82 39 so we had some questions in the draft for discussion and we said should just particular search algorithm be used we think yes the answer is yes we think it should probably be binary search and and and also should search trial repetition should include trial repetition whenever there\u0027s a loss observed to avoid the effects of background loss unrelated to buffer overflow so now we\u0027re thinking of two kinds of sources of loss here kind of a transient loss that might be present on the links really high-speed links or in the devices itself if it\u0027s a virtualized device and and and so yes we think the answer is yes there as well so we\u0027re gonna see some results about that in a moment so the next steps who\u0027s read the draft one hand thank you Oh two hands thank you so if you guys have comments that would be great to hear about now but otherwise we need more readers before we can adopt this we\u0027ve got a milestone on our Charter but we can\u0027t adopt this if it\u0027s of course I\u0027m speaking as a participant now I\u0027m glad to qualify that so other ideas are always welcome here for doing this kind of benchmarking we\u0027ll hear more I think a little more about that later to possibly so comments are you clarifying the draft to state that you\u0027re only expecting one egress port it\u0027s it\u0027s always been it\u0027s always been an RFC 2544 thing I could do that I think maybe I\u0027m not sure whether that\u0027s in there or not but I don\u0027t but it\u0027s but it\u0027s but it\u0027s definitely clarified that it doesn\u0027t apply to 80 to 39 so I think clarifying it is good because literally non IETF just with my day job hat on yeah we\u0027re running into this a lot from customers who are asking us about buffer and how things are run and there\u0027s "
  },
  {
    "startTime": "01:03:25",
    "text": "generally no box that\u0027s left anymore with a single with with memory per port it\u0027s a shared pool and so while I applaud and I think we should have this refinement don\u0027t get me wrong I do think it sort of begs the question but but how big is the the pool and how big is the buffer when it\u0027s shared across imports right right so Jacob go ahead yeah that was pretty much my question that yeah okay it\u0027s kind of like the I think this dopamine may be a correct me if I\u0027m wrong here\u0027s like it\u0027s a device is not actually capable of doing full lane right so think you\u0027re able to that\u0027s where your we\u0027re worried about that but they do agree was there with your point that okay what about multiple ports going at the same time with one ingress one ingress so so that so we\u0027ve got a good taxonomy here we\u0027ve got devices that can\u0027t perform at full rate and and that and their single port testing is enough and and that and that applies to a lot of the virtualization world frankly I mean that\u0027s why benchmarking is cool again all this virtualized Network function stuff alright so so so that\u0027s a good clarification be sure you type it there so I can see it again sometime [Music] we\u0027ve covered your point but my next slide here procedures now don\u0027t go away Jacob oh yeah yeah so Yoshiaki to assent to our list this examination of the the buffer testing that that he\u0027s been running in his lab and he was sort of thinking that there might be some ways to clarify what is currently in RC 82 39 so now we\u0027re getting into the test results here what what Yoshiaki is showing us is he\u0027s measuring latency per frame and he\u0027s got one percent rate on this combined set of ingress ports which slightly over scribed the egress and so he\u0027s tracking the the delay there and when he first sees a loss he\u0027s saying okay that\u0027s 24 frames so that\u0027s the latency any thoughts on this yeah so there\u0027s something about that diagram that rubs me wrong the buffer scope on like any box at least physical box that\u0027s shipping today sure as heck isn\u0027t being just restricted to the one link it would be you would most likely see packet drops across both yeah so when I saw that I wanted to digest it a little bit but since you asked I guess it\u0027s now knee-jerk coming out you know I\u0027m not sure I I don\u0027t think it\u0027s as simple so he\u0027s got more diagrams oh and here\u0027s one so so so now he did 100% rate on two ingress ports and he tracked the same latency and what he basically sees here again is 24 frames for each of them right because when you see the inability if it\u0027s gonna drop on one it\u0027s gonna drop on both that doesn\u0027t surprise me at all yeah "
  },
  {
    "startTime": "01:06:25",
    "text": "yeah so he kind of expanded his mother\u0027s methodology here to be able to track the latency and the Lawson in both of these separate streams he\u0027s got one more where he did three of them and now he sees oh look now the buffered frames are 12 not 24 plus so which is exactly why we in the in the RFC we put that you should be doing these tests with multiple ports and multiple iterations of the ports and multiple agencies are multiple ingress rates and that\u0027s exactly the reason right because you\u0027re gonna see different results depending on what you\u0027re gonna do as long as you benchmark all of them then you actually have a clear vision view of actually what the buffer is doing so this is why you\u0027ve got multiple iterations exist in 80 to 39 yeah multiple iterations but not this multiple iteration but also multiple combinations of ingress and egress ports yeah you know some some I\u0027ve seen some of the devices that have four ports per buffer so when you add a fifth port it completely changes everything else because the ingress of the fifth is taking a more buffer I mean same thing with this right so this is exactly what we want to see actually yeah these results you want to make sure that those egress ports are all on the same buffer which you know just non you know personal hat on for a sec we\u0027re finding that to be all over the place they\u0027re not laid out the way you\u0027d think they\u0027d be at all so you have to test to find that out I guess I guess the bender tells you but yeah right even then you I was one of us some of the early comments when we did the Darcy was that like what you really need to do all these ports and all these iterations like the answers for yes yeah you can automate it so if you can script the heck out of it so right right Automation get on all right but and and and just to distinguish it I mean 100 percent rate for one port to one port that\u0027s going to go through here Blas free so you\u0027re never going to see a burst or in this particular circumstance he had to go to some level or oversubscription in order to create the box that can come forward at that line right yeah that\u0027s exactly right so and and and the in the case I\u0027m worried about is where you put if you put a hundred percent of line rate here now you see losses immediately so you\u0027ve got to back down from that and that\u0027s where the burst test is worthwhile so two different categories of things yeah all right good I think that was it other reporting well you know we\u0027ve got good reporting I already took care of that all right so that\u0027s it any other questions I\u0027m very thankful for the discussion on this and also for Yoshiaki who got in touch and is really doing some some of that some of this stuff glad to get this feedback thank you so I\u0027m gonna follow up now with what I threatened to do which was the OPN Fe hackfest plugfest results so this so we\u0027re so now we kind of reached the point where we\u0027re not talking about drafts anymore but we\u0027re talking about measurements which are going to influence our future and development and "
  },
  {
    "startTime": "01:09:25",
    "text": "and so forth and it\u0027s always good to bring this stuff in we\u0027re always going to try to fit this stuff in if you\u0027ve got measurement as well then then this is what we get excited about here so let\u0027s let\u0027s do that yeah yeah and no there\u0027s no no question no knowing it\u0027s what\u0027s gonna happen if we change drivers here all right here we go this thing has to be clicked and then this thing has to be cooked all right so sweet RL is the project team leader now of the OPN fe open platform for BIA for network function virtualization it\u0027s a Linux Foundation open source project he\u0027s the project team leader for the vs / V switch performance project there and we\u0027ve been contributing I\u0027m a contributor committer on that project for since almost since it started and we\u0027ve been contributing to BMW G for a long time got a couple of drafts as a result of it as well so it\u0027s really his work my work in the kibitzing and designing and the vyas project team for keeping everything up and running I mean it\u0027s all of us really getting this stuff done so here\u0027s the quick problem statement there\u0027s lots of ways in which our test results and repeatability can be sort of threatened when we test overtime over minor changes identical nodes different test management tools and so forth using different test equipment we actually examined this as part of the Danube plugfest and presented some results here last summer and at the OPN Fe Summit so so now we\u0027re covering the time aspect here we were planning to do it with multiple nodes if possible we did do some testing with multiple tools but that\u0027s that\u0027s yet to be included in our summary here so our initial focus was on search algorithms and looking at the search algorithms in ways that we could improve them so that we might improve our repeatability so a quick view of the test combinations we looked at some of the test configurations where we\u0027re already running a continuous integration tests to give us some view of the potential issues we might see but we used the t-rex traffic generator in order to do this testing the reason we did that was because we could easily control the search algorithm there and basically stitute our own modified search algorithms other than the ones we had previously built so a point I want to make about background here is that search algorithms can actually have different roles the roles are sort of born out of the testing that they serve "
  },
  {
    "startTime": "01:12:25",
    "text": "if you\u0027ve got highly repetitive continuous integration or system integration testing then you want the test to run fast and there\u0027s a proposal to do that in the FDA OSI set project that called MDR you could also have multiple search goals that you try to collect while a single search is in progress like a partial loss ratio and a zero loss ratio or a mere zero loss ratio there\u0027s another project NFB bench in no piano fee that\u0027s using an algorithm that does that but the one we\u0027re focusing on here is our traditional benchmarking role of reliable results in the purely experimental scenario and we\u0027re focused on the zero loss throughput as the basis for this testing and and as a basis for many other benchmarks so here\u0027s a quick view of our deployment scenarios which I will call Phi 2 Phi as labeled at the bottom there and PvP when it includes a VM and PV VP where it includes two VMs and the simplest scenario is is is is this Phi 2 Phi case it goes in and out of the V switch but notice that our test device is always a physical test device it could be moon gen don\u0027t worry but it\u0027s always separate from the device under test by these physical interfaces and that\u0027s to avoid the possible you know conflation of workload versus the test traffic generator we\u0027ve we\u0027ve stayed with that and and believe me this is I mean these are realistic nmv deployment scenarios as well you have one or two and network network functions virtualized network functions vnfs or a V NFC and another V NFC that are working together and and there they out of the physical port that\u0027s it so we deployed that the terminology from tester zero nine which is an Etsy specification that we\u0027ve had a chance to comment on here in our bio is on and the way we\u0027ve worked this is similar to our terminology of trials where we are searching over many different load levels what we\u0027ve collapsed many trials with at different offered loads with a into a single test which has a measurement goal trying to find the maximum throughput it\u0027s your loss something like that if we\u0027re gonna repeat those tests and and actually we do that in this study then we have sets of tests and when we change any of the parameters then that\u0027s part of a method so I\u0027ve Illustrated here as a set of multiple frame sizes but we test with fixed frame sizes so this is actually Raphael this is something that you could "
  },
  {
    "startTime": "01:15:25",
    "text": "include this set terminology in your hierarchy of testing - I think that would that would be good we\u0027re about to see the value of repetition at the test level so doing good on time okay so this is binary search this is the day in the life of a classic benchmarking binary search this is a case where we start out at a hundred percent of the this thing work this oh here we go so we start out at at a hundred percent of the maximum frame rate so that\u0027s the maximum theoretical throughput for a given frame size and then when we see loss lost frames something like 42 million there you reduce it in half that\u0027s the way the binary search works but we have to go all the way down in this case we have to go all the way down to six point two five and and we you know that\u0027s that\u0027s actually fairly low throughput this is a p2p case here Phi 2 Phi case we see this loss in the 700 range hmm interesting so we search around and we we end up with a value that\u0027s four point two eight percent of the total maximum offered a load the and one of the other things that we saw here the duration of the test it what that really means is that we were we were sending a constant number of frames to set our trial duration and in fact the device didn\u0027t deliver them at I see Paul laughing but the device the test device didn\u0027t deliver them at the maximum rate so this duration actually exceeded the fifteen seconds that we planned so it\u0027s good to have these checks you know you you you always want to be testing the tester that I\u0027m stealing pause line but that\u0027s that\u0027s a more evidence of it here I couldn\u0027t resist so it this this yellow case here it\u0027s kind of an interesting case and and the bright in the binary search basically what we\u0027re you know every time you\u0027re kind of changing your search area and and if you see no loss you search above it if you see lost you search below and if you have a transient impairment along the way here like this one this I\u0027m going to I\u0027m going to tip the scales this was actually a transient and some of these are transients too so when we when we see that we we greatly influence the future search and how are we gonna handle that that\u0027s the problem so before you go on out yeah doesn\u0027t the fact that trial number four and five at 12.5% you saw 237 lost frames but again like can you adjust anything you\u0027re "
  },
  {
    "startTime": "01:18:26",
    "text": "seeing at this point I mean right why is it more yeah at six and a half yes that\u0027s exactly right that\u0027s except for that\u0027s that\u0027s kind of a tip that there\u0027s some sort of transient thing happening here so so we\u0027re gonna combat that but well spotted so I should have said first in fact I plan to switch around here so when you have a search space you divide it up into kind of like your minimum tolerance and and in this diagram on the on the top of this thing here I\u0027ve divided a 12 mega frames per second into equal 1 mega frame per second steps so this this creates the search array and what we\u0027re going to test for is whether a loss occurs or not and that\u0027s our indicator of whether the resources have been exhausted in our device under test so I when I was I I wrote this part in the draft about retesting and and checking test results and should we repeat the results but I but I went deeper into this after writing some of those sentences and talking to people about it and what I found is is this tiny little reference here at the bottom here unfortunately it\u0027s it\u0027s searching games with errors 50 years of coping with Liars and that\u0027s exactly the situation we\u0027re confronted with when we have two different sources of loss in our device under test so our questioner I\u0027ve drawn the model up here that sort of fits this this graph this uh what they described in this paper the questioner asks a question the questions delivered error free to the responder the device under test and what we\u0027re basically asking there is whether the resource limit has been exceeded by the offered load that we\u0027re generating there and when the when the resources have been exceeded we answer with a lot weather loss occurred or not so if Glaus occurred that\u0027s true if the loss didn\u0027t occur that would be false now there\u0027s a background error process and that that\u0027s the liar here that changes the the answer that the responder gives back so an error converts a zero loss or a false response to true but we\u0027re working with a system of half lies and that\u0027s terminology from the pulk paper only one of the answers can be influenced by this this lying phenomenon so if in fact we\u0027ve seen loss due to resource exhaust and a background error process jumps in and causes more loss that\u0027s not going to influence our search results where that\u0027s actually a very happy outcome because it means that "
  },
  {
    "startTime": "01:21:27",
    "text": "we\u0027re we\u0027re not going to have to repeat all kinds of outcomes oh we can trust all the false ones we have to we have to sort of retest all the true ones the cases where we have loss it\u0027s a time saver and that\u0027s a good thing and this is the timing of it so what we\u0027re hypothesizing is that these transients that cause loss appearing over time near the red vertical arrows if they occur during a trial then they\u0027re when we\u0027re gonna see loss due to that and on the other hand if we repeat the trial with the right duration fairly quickly now we\u0027ll see now we have a chance to see whether you know that loss was true and it\u0027s really resource exhaust or whether it was caused by transient phenomenon but what\u0027s also clear in this diagram is you have to choose your trial duration very carefully and that means you have to do some long duration testing I\u0027ll try to get that in a moment huh so um a long slide here but basically basically here\u0027s what we what we chose in this testing minimum step size of 0.5% it works a little differently than the the fixed frame size thing we were looking at percent of the the maximum the maximum times that we would repeat a trial is two with a and and we\u0027ll also created a loss threshold on repeating trials in the v2 version of the algorithm we tested 30 seconds and 15 second durations but that as I said that\u0027s actually best determined after doing some on a long-term tests we did 64 and 128 frame size and the maximum number of repeated tests we did was actually four in this book but I think mostly would not show today\u0027s is these are four three so so we we also calculate these interesting metrics about our test the number of repeated tests where the loss outcome changed so basically this this state right here and we also look for a metric on consistency of the results basically to be able to tell is this repeating trial is actually doing anything good for us so here\u0027s the long duration test that would be planned we haven\u0027t actually done them yet we actually need a better tool to do this you can\u0027t just look at the results at the end you\u0027ve got to be able to you\u0027ve got to be able to see how much loss occurs and and when it\u0027s where it occurs in the narrow time space to be able to accurately process so we\u0027ve got a we\u0027ve got more to do with that so here\u0027s some results for the 64 octet and the 128 octet results so I\u0027ll work through this search algorithm details with you in fact I\u0027m gonna do it over here you know I\u0027m gonna do it right here sorry you can see it but I\u0027m gonna do with it I\u0027m basically going to do it with a pointer because that way it that way it gets recorded where\u0027s the pointer "
  },
  {
    "startTime": "01:24:27",
    "text": "all right so this is the Phi defy what we call p2p case and here we\u0027re looking at binary search with the number of received frames counted this is the percentage of the maximum offered load along all along here so these are repeated trial uh sorry repeated tests at 64 octets after we\u0027ve done the binary search these are this is the outcome and the binary search always took twelve iterations so that that was true across all of these cases alright so let\u0027s look at binary search with a loss verification and basically what we do is exactly what I showed we if we see who us we again and if we see noble us we trust that outcome we re stoke the search into what it was supposed to do with zero loss and move on and what that means is that now we\u0027re testing protesting above a point in the search space where the loss was telling us to search below before so the interesting thing is that with the the phi 2 phi e case you remember this this goes from the physical up through the v switch and back out again we\u0027re not seeing very much difference here in fact these are pretty consistent results in terms of the percentage for the binary search and we\u0027re also seeing consistent results here for the binary search with loss verification and the Devils in the details isn\u0027t it how many cases did we see reversal when we tested a second time none so our results for the Phi defy case are really troubled by this transient phenomenon and that\u0027s actually very interesting news because it tells us that when the transients come around to bother us they\u0027re coming from someplace else in the architecture so this was this was a very valuable result so oh go ahead here I\u0027ll just use this one thank you so at 64 64 offcuts I agree but at 128 it\u0027s curious you see a couple of reversals and your throughput your loss verification ranges from 86 to 100% which is not at all consistent with what you saw consistently in the 64 by dog pets so it would almost argue frankly them I agree anyways I suspect your transients are coming from somewhere else but I\u0027m not entirely sure you can say that with a straight face in 128 octet case and and we\u0027ve always we\u0027ve only seen this once we\u0027ve seen this once in the in the in the actually we\u0027ve seen it occasionally in the in the continuous integration testing that once in a while for some "
  },
  {
    "startTime": "01:27:28",
    "text": "reason 128 octets goes through it maximum through it actually happens and we\u0027ve seen it in the burst testing as well occasionally 128 byte packets for some reason they they make it through the architecture that we\u0027re testing here so it it\u0027s it\u0027s a real result but it\u0027s it\u0027s a it\u0027s a platform variability that we don\u0027t fully understand it\u0027s it\u0027s clearly not it\u0027s clearly not caused by the phenomenon we\u0027re trait chasing and trying to remove here it\u0027s something else so it could be something that Paul is going to tell us about later we\u0027ll see so back when we used to do 25:44 our routers there used to be that point where we see folks vendors optimize their architectures for 64 or because they knew the 25:44 test was happening I\u0027m wondering what happens if he were to do this with 65 octet or 129 octet yeah that those are those are always interesting tests to run and I think that\u0027s I think that\u0027s worthwhile doing in fact that\u0027s what the new spec we\u0027re writing in Etsy testers are nine recommends so absolutely absolutely worth worth a shot for that so let\u0027s look at okay I meant to mention these are all 30-second trials we chose 30 seconds because it was what we were currently running in our continuous integration test but we didn\u0027t do any long-term testing yet so we really had no idea whether we fit in between the transients or what was really happening here okay so so now we tried pvp and now we\u0027ve got something really interesting we\u0027ve got for the 64 where\u0027s my cursor there it is for the 64 octets very consistent 4.5% of offer to load for binary search with loss verification some more variation that also much higher values and clearly I mean this is this is fairly consistent although you\u0027re seeing some jumping around here and and and quite a bit higher percentage of offered load for the binary search with a less verification fairly consistent thereto and notice now the reversals are happening they\u0027re working in each one of these cases so the the loss verifications doing some work to try to characterize the resource exhaustion alright so let\u0027s press this so we we went we went about investigating this so I potted here the number of the number of losses for each of the binary search "
  },
  {
    "startTime": "01:30:28",
    "text": "trials that were at six point two five percent of max offered load remember I showed that as a special case before all of them had packet loss counts on the order of 700 when we had when we ran loss verifications sometimes we saw that too but other times zero less verification again this was the first trial it\u0027s Aussie row another one here a loss verification at six forty four and then lost verification repetition it seems to have seen now two of these two of these transients that are roughly accounting for about seven hundred packets so this so this if we go back one slide here and let\u0027s see just making sure that this is the AH yeah this is PvP right are you gonna be able to tell us where the transients came from at the end of this presentation now I wish we could we\u0027re still searching for that that\u0027s that\u0027s what Serena Rao is at home doing right now so what we\u0027re gonna what we\u0027re gonna track that down and and and this is that this is the trial where we saw us twice during less verification it\u0027s it\u0027s a really low value so we\u0027ve obviously got some improvement to make here in terms of trial duration because we\u0027re not really avoiding the transients so we looked at this in 15-second trial duration and now look what happens binary search saw the saw this nominally 700 frame transient about half the time and look at the effect on throughput when you saw it it\u0027s 4.5% when we didn\u0027t see it it\u0027s 9.1 and so on and so forth and and interestingly almost all of the Lost fairy K ssin cases didn\u0027t see it though in some cases where actually where we saw zero loss at 12.5% of offer flood and and the one case where we did see it look it\u0027s still the same sort of standard value kind of I mean there\u0027s obviously something that we need to track down here so then we looked at 128 octet frames maybe this may be this frame count this lost frame count is somehow related to the frames per second well it turns out it looks like it\u0027s not there I mean these are actually a little bit higher but they\u0027re all in the 700 frame loss account range and we\u0027re seeing the same kind of thing with the 30 seconds all the binary search was was bothered by it lost verification once but the repetition took it away and if we go on to if we go on to 15 seconds "
  },
  {
    "startTime": "01:33:28",
    "text": "now we\u0027re really seeing some payoff here we only saw this transient once at the 128 octet frame per second so I\u0027ve got here that show kind of how the the you know graphically how the binary search works one of the things we saw was that when we tested twice at a hundred percent of maximum offered load we were kind of kind of wasting our time as I said we\u0027re seeing like 50 million frames lost so we put a threshold in to reduce the number of repeated trials when we see loss over a certain threshold there\u0027s no reason to set this like you know like really aggressively but it makes sense not to repeat some of these where where the loss ratio is really high so so this this graph shows kind of a the in Orange we\u0027ve got the lost frame counts and we\u0027re seeing those let\u0027s see here this is for a PvP setup 128 octets and this is binary search with loss verification so we\u0027re seeing we\u0027re seeing some losses in the kind of in the onesie twosie range here and some are greater than greater than 2000 they\u0027re not plotted they\u0027re above the of the range so we may have we may have more phenomenon in this this PvP case that we need to account for here it\u0027s it\u0027s not the same kind of thing we saw it at six point two five percent of loss that would those would actually in this particular case we don\u0027t even get down that far so roughly twelve point five percent would have been about the equivalent okay another similar thing I\u0027ll just skip over that so let\u0027s see here oh and this is 30 second trial for P V V P this is where we got two VMs in the case and I mean we\u0027re seeing about the same consistency here for binary search with the lost verification really bad consistency for binary search alone it\u0027s almost doubling here in some cases so you know that and and and obviously again binary search with the West verifications doing some work we\u0027re seeing some reversals then we did this 15 second trials and we did four trials at 64 octets and for binary search we kind of reversed the the rows here a little bit so watch out we\u0027ve got the we\u0027ve got the the same kind of banging across the the range here almost a doubling in the results the standard deviation of those percentages is 2.41 and the received frames per second that we\u0027re seeing is you know almost a million frames per second with less verification and unless "
  },
  {
    "startTime": "01:36:31",
    "text": "verification v2 where we added the loss threshold now we\u0027re seeing much better consistency and we\u0027ve got that for the 15-second trials in 128 octets to roughly the same kinds of improvement here but a factor of fact or two roughly something like that so we\u0027re seeing improvement but have we solved the whole problem no probably not you go through this but I looked at the histograms of lost frames for each of the cases we examined and so we\u0027re seeing kind of the difference between 30-second trials at 64 octets and 15-second trials at 64 locked hats and what we and what we see is that some of the some of the instances of counts in the Iligan that we expect a lot here near 700 right we saw a lot of those at 6.25 those all went away there they\u0027re kind of gone now there they\u0027ve got distributed you know maybe down in here or or maybe even lower so there may be more than one transient causing those causing those 700 nominally 700 counts and and unfortunately there was multiple more trials at the 15-second case so you can\u0027t really look at the exact numbers but the but the big range is kind of show the difference same thing here for which one is this though this is P V V P so two VMs and you know it\u0027s shifting again so we\u0027ve got more to understand about these things so our next steps I asked already but you know we still need to identify the processes that costs that caused the loss we need a better measurement tool that can help us characterize the bursts and run in this long term mode I\u0027d like to add a heartbeat check to our test VMware it just loops back to the packets I think that would be valuable because that way we can tell when that when that VM itself has been interrupted and observe the logical interface drops on the sort of the northbound side of the OBS I think that would be helpful too any questions or comments have you guys figured out how you\u0027re going to timestamp the loss events yet no now we need it we need like a really inexpensive open-source tool that will help us do that are you working on this project yourself are you just presenting the output I\u0027m working on the project myself yes I might be able to connect you with somebody from our side who works in OPN IV as well who might be able to help you with that time stamping okay okay good kick me out remind me after yes yes I will not kick you I "
  },
  {
    "startTime": "01:39:32",
    "text": "won\u0027t mind you alright anything else good thanks for that recommendation I hope this was informative and now we\u0027ve got Paul some more interesting stuff all right I think this is it let\u0027s put this back okay okay thank you well I\u0027m well I\u0027m a researcher and this is joint work with my colleagues Sebastian our student Alex and our professor Georg and we are looking at packaging artists in particular researching package generators and whether they are reliable - oops this the wrong mode I\u0027ll cut you this is crawling the wrong way oh it was like it\u0027s growing is it a PDF yeah it is no no it\u0027s hang on a sec honey yeah did I you know what I did I didn\u0027t press this button here we go okay and that starts again so pick it generate yes there are expensive package generators they are of course also man work perfectly then they\u0027re cheap packet generators on network as lackeys intercuts are they awesome we don\u0027t know maybe they are but that\u0027s something we are looking into so the first question is of course what are the metrics that you are looking at what do you want of your package generator what do you expect your package generator to do like that would be functional features or reliability features then can exceed package generator be reliable and how to measure it and how would you relegate a packet generator whether it works as advertised if you look at the package or if you have a new package generator can you test the package generator to see if it works as specified and then is that precise and this is accurate and one thing you\u0027ll notice I am asking a lot of questions I don\u0027t have a lot of answers I just have a lot of questions to maybe get something started about how to answer these questions maybe hopefully yeah so messed up was the first question what should your packet generator do here I just looked at in the Etsy specification that was previously mentioned a few turns here and it has actually a section on requirements for the packet generator to be used for the benchmarking there and her first section here he said there the main requirement it says he has to accurately generate constant frame at specified rates so you have to configure "
  },
  {
    "startTime": "01:42:32",
    "text": "some written or incense packets okay it seems like a very reasonable requirement then it also wants you to generate bursty traffic that\u0027s a specified rate specified burst length okay nice and there are two some more requirements I haven\u0027t listed here like multiple flows and so on and then the third main requirement is it\u0027s supposed to do accurate latency measurement and the time stamp is applied this is very crude as close as possible to actual transmission and whatever that means if as close as possible is somewhere a millisecond away and there just can\u0027t move it closer than it would be okay for that specification because was as close as was possible was there picture no it just wasn\u0027t it would pick a generator and and and one thing I want to talk about is since since the specification here only men should accuracy and never precision I just wanted a short picture about these two terms that are often intermixed and so on accuracy versus position in on the example of latency measurements but can be applied to other metrics as well and it\u0027s two different things for example you have a packet generator here and we just connect two parts of the package owner that was a cable like you\u0027re measuring the cable the ideal packet generator worked with well the cable of course doesn\u0027t lose any packets and so on but if you measure the latency of the cable it should be the same value every time but at least as close as possible to the same value so the position here would be that the deviation between individual measurements of time stamps is low meaning each each time of flight of the packet is measured as X nanoseconds and the typical source of a measurement error in the packet generator here but it includes a kingdom a on the generator or receiving site at the timestamp is taking in software before it\u0027s transmitted and to the card of the packet generator yeah then the second thing is you can look at it accuracy is that it reports the correct latency as well the correct latency would only depend on the length of the cable and yeah the typical source here is that some processing time is included and there\u0027s some static overhead and the big question here is that is there they gone to what is the correct latency of our cable you can estimate that and we\u0027ve done this measurement with a million package generator we have done it actually with a few different cable lengths here we have just attached 30 meter cable to the packet generator and we send packets to it and the idea is the cable should always report the exact same latency and on this graph you can see the x-axis is applied load to the cable so we apply an increasing load to the cable was the 64 byte packets and the device axis is there measure latency and it\u0027s a boxplot meaning the big boxes "
  },
  {
    "startTime": "01:45:34",
    "text": "are the tens to 90 percentile mark thing in the middle is the median and outlier that\u0027s a minimum and maximum reported latency of the packet generators so what we can derive from that year quite easily is the position of the time stamping of the packet generator so that would be around 37 nanoseconds there\u0027s just the deviation between the minimum and the difference between the lowest reported latency of the carry the maximum reported latency of the camera so decide nanoseconds that\u0027s quite good I think it\u0027s better than you usually need and the accuracy is more tricky to measure because what is the ground to is how fast should a cable be and and this we can estimate we can say ok the propagation speed and that it was a single mode fiber optical cable should be around 2/3 the speed of light you can estimate it should take a hundred and 50 nanoseconds for a packet to pass through that and the error rate reported latency here is 161 nanoseconds so it\u0027s kind of close but we can\u0027t really quantify how good it is because they can only say it should be low but how low should it be one one thing you can the could be done is it\u0027s like use different cable lengths and see if the difference between carry lengths is somewhat reasonable and so on but it\u0027s it\u0027s a tricky thing and I don\u0027t have an answer to this maybe one can use a very short cable and say okay this should be basic no latency does its report basically zero or does it report some absolutely high value and this also something there lots packet generators will already fail we have also done this test worse software time stamping that you have implemented as good as beaker to me like it took all the precautions and just in this test this fails completely they\u0027re like micro seconds of latency of the cable and sudden picture and the latency increases and so on then I\u0027ve there\u0027s two things I\u0027m basically want to talk about the first one was this latency measurement how can that be well measured and the second thing is traffic patterns or not only traffic patterns also the the speed that packets are sent out the the main thing the main traffic pattern that you see is the typical constant between traffic pattern that is defined as same space between all the packets meaning you want to send a thousand packets per second you configurator to do that and then you tell it to do CPR and then you expect it to insert a space of one millisecond between each packet then they of course bursty traffic "
  },
  {
    "startTime": "01:48:34",
    "text": "bursty some packets are sent it was no gap between them and then a larger gap more and quite nice one is the Poisson distribution whereas an exponential distribution of the gap of the packets then what is usually used well CBR and FC 25:44 just says by default you see BR but you can also do more tests with other traffic patterns if you want to I don\u0027t think anyone does that but yeah then the a T and a Fiat is t9 they won\u0027t explicitly CBR and bursty traffic which was fine because bursty traffic is really easy to generate if you tell your software package generator you want some traffic it will give you bursty traffic by default because it\u0027s easier to implement bursty traffic because that\u0027s just how all the api\u0027s ends and all the hardware\u0027s optimized it\u0027s optimized to work in a burst of traffic the problem is sometimes it will give you even bursty traffic if you explicitly asked for a constant bit rate traffic which is bad for your sites then CBR is actually it sounds like the easiest pattern because just use the same gap every time sounds easy but it\u0027s actually hardest to implement and the the most annoying to implement in particular it\u0027s annoying too scared to scale to multiple cores because then one coil would have to know when the last one was sent without the support that\u0027s annoying per so traffic it has a few nice nice properties for example if you take Prasad traffic from two different links and just mix them by a switch on a network Arthur\u0027s multi cue just somehow multi-processing mix stuff together and adding to a Poisson distribution use a new Prasad distribution so you can easily scale up and Prasad is arguably the most realistic traffic pattern for a real-world traffic it\u0027s not quite how we worked or if it looks like but it is way closer than CBR traffic and bursty traffic is also alright for some situations but let\u0027s look at whether that actually matters because it sounds like such a distinction like who cares how the packets are really spaced on the wire just pack and it\u0027s fine so this is just an example measurement setup really simple one host sending packets one host running open V switch no fancy open V switch just this standard open research in the kernel no fancy optimizations and we apply an increasing load to two ports where it forward packets was just a static open flow rule no magic there we are using the entry xgb driver which has some dynamic interrupt sorting and we have the kernel point mode map e and yeah but you can see in this graph is we apply an increasing load measured in million packets per second on the x-axis and measure the latency of the traffic in "
  },
  {
    "startTime": "01:51:35",
    "text": "microseconds on the y-axis so what happens here is everything seems kind of fine packet rate increases latency also increases but then suddenly something what happens down here turns out that switches into a different mode it goes into poly mode and it\u0027s complicating with our paper about it I won\u0027t want to go into it but it\u0027s basically just an example of what a measurement could look like and what you can now do is we can repeat the same measurement and the only thing we change as we changed the traffic pattern meaning now we don\u0027t use a constant gap between the packets but you just use a random number generator here\u0027s a nice exponential distribution and it resides in the same ever watch packet rate we sent the same amount of traffic over the same time it\u0027s just sometimes they\u0027re closer together sometimes they are further apart and now we get a much more realistic and nicer response yeah and the point is we get a completely different response from the device under test just by changing one parameter at the packet generator and it\u0027s a parameter that is often ignored by packet generators not supported well and it can be a problem if you\u0027re looking at latency measurements and we found it to be not a big news for throughput measurements but for latency measurement it\u0027s kind of a big deal so I\u0027ve more questions here he\u0027s to two things I\u0027ve mainly talked about is say for example if we if we wanted to specify how to benchmark a package generator and it would be probably defined several tests that would start with say this package generator suppose latency measurement so let\u0027s measure how good is the latency measurement by measuring the latency of different cables of different lengths of a specified cable type and report what is the latency that the packet generator reported and then this test needs to be repeated by applying different different types of traffic to the cable because that\u0027s changing anything in the packet generator affect how well it performs especially increasing the load can can affect your latency measurements if it\u0027s not done properly because queues might fill up and the open question is what is the ground to use for the latency of the cable can you just use a shortest cable you can find and say it took me around 0 Tecna saying RBS traffic pattern that\u0027s what\u0027s really hard to measure to to quantify what this is really really going on maybe that once um two years ago and it was really annoying to do because we it\u0027s really hard to measure on commodity hardware if you receive a lot of packets and it\u0027s really hard to take a time some of each packet on just your normal commodity hardware with the "
  },
  {
    "startTime": "01:54:38",
    "text": "necessary position you have done it using an FPGA and that bit work at Versailles quite annoying setup and realistically speaking yeah it\u0027s not that easily repeatable because no most people don\u0027t have FPGA is lying around and their service yeah and two things since I\u0027ve just heard about the packet generators apparently fail to even generate the packet rate that was specified then there should also be tested but I just assumed that get the basic stuff right and then well I actually knew better and the basics for that but I thought that got that our last year or well last open question I had here is the traffic pattern at this graph comparing CBR pasado traffic and often see the ass who cry out or wanted is it really a good idea to have CBR traffic as handy for it because it\u0027s not a realistic thing that your device will encounter in the universe so the benchmark shot somehow modeled a rewrite yeah I\u0027m also going to show one more back up slide that I had here this is an example that we get is the FPGA setup that I mentioned we had a few packet generators here and this is a histogram where we configure each of the packet generators to send constantly to air traffic and to histogram the buckets are the actual into a packet cap and the land in the middle is one moment is one microsecond so the target was hitting exactly that line so the idea was that would have been run histogram bucket at the exact middle and the way left is back-to-back firm and the right as I think double that was what was configured so usually a tree size and something that is certainly not completely a CBR but it\u0027s usually quite close accept some package a notice to just like to ignore the configuration and and some things because when you increase the packet rate here we increased it to four million packets per second and then one of them just decided it will send bursty traffic anyways because that\u0027s easier and the other ones are not that close and thurmond Ronde results are with software everything we have otherwise everything matters that actually hit that quite precisely like that that\u0027s what it should look like and this is what it usually looks like so put but and was it something that could be could be reported for package generator but it\u0027s hard to measure so not sure if it\u0027s a good idea and there are some fresh so that someone else have questions or maybe even answers you\u0027re saying CBR is bad you seem to be saying what song is "
  },
  {
    "startTime": "01:57:39",
    "text": "good yes are you saying you plan to come back and issue an update to 2544 to remove the recommendation for CBR oh sure why not okay what are your next steps then no do you tell me there\u0027s some it seems to me there that one of your one of your tests which is very simple but also very informative is something that appeared in the kind of in the I ppm framework a long time ago the idea of testing the measurement system with this simple cross connect cable and it seems to me that that we could make a series of recommendations like that for people putting together their test setup and turning instead of a lot of hand waving and I don\u0027t really know what happened here into science if we if we give them a few prerequisites to especially associated with their with their traffic generator I think that\u0027s that there\u0027s probably there\u0027s probably space and and and and welcome audience for a draft like that in this working group and Warren quarry this is probably kind of a tangent but I just interested for the 30 meter cable you had 0.66 see did you get that just from a lookup table or did you actually look on the cable because you can get cables which are actually you know when they report what their propagation speed is that was just our right guess Vargas okay so that so that could be a consideration in the calibration too that you that you yeah a little extra and get your cable already the other obvious thing is the SFPs with serializing and deserializing delay well there is actually I can\u0027t remember who one of vendor specifies witches yeah in this specific measurement was actually a more complicated setup where we had five attempts before after a cable and had the package generator and the latency measurement separate and like a packet generator inserted sequence numbers and then we had two tech ports on the fire attack going to a packet connector and taking the timestamps there so assuming that both the fiber the transceivers on the on the receiver have about the same than would cancel out and there\u0027s actually if you if you measure that was without the fancy setup then you got the latency of around four hundred nanoseconds I think this is a good stuff I think we tried to go down a little bit of this path when we did the data center benchmarking but we included it mainly just around okay you got to use if "
  },
  {
    "startTime": "02:00:39",
    "text": "you\u0027re going to use a cable the test multiple systems use the same cable basically use the same set of transceiver so you can kind of like at least take out that that drift and then then and I think another thing that I didn\u0027t mention is that also if you test different especially from two two different systems are gonna have two different clocks so they\u0027re gonna be Ethernet has like a 100 parts per million drift yeah I\u0027m gonna play with those different systems that\u0027s just asking for trouble yeah it\u0027s something to take a look at yeah well thanks very much for bringing this in from me informational testers on to us and making some recommendations about how we can do this work better I hope there\u0027s a I hope you can imagine sometime in your future to all up with with some of these things work in this calibration recommendations area I think I think you\u0027d have some welcome reviewers here and maybe even some welcome co-authors yes very good thank you well thank you thanks it thanks everybody for playing along for especially for a few extra minutes here this afternoon has everybody signed the blue sheets everybody okay cuz we need we need to get every name on there so we don\u0027t get assigned a tiny little room that\u0027s that\u0027s the that\u0027s the defensive participation so thank you okay please everybody read all the drafts that\u0027s how we get good work done here we really appreciate you doing that thanks for joining us folks who joined us for the first time and we\u0027ll see you on the list yeah yeah that yeah this is this is where somebody like that for me so I\u0027m gonna they can certainly scroll "
  }
]