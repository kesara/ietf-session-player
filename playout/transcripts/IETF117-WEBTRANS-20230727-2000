[
  {
    "startTime": "00:00:20",
    "text": "Okay. it is 1 PM. I'm just just the web Trans Working Group meeting. atitfone17, Hopefully, David will be here soon, but I'll just go through the preliminaries. David said he would be here momentarily. Ah, okay. maybe we'll wait a minute then. Yeah. Let's give them a minute. I mean, if we can always get started as we go."
  },
  {
    "startTime": "00:03:11",
    "text": "Also, Bernard, I just added a couple of phone control slides if you wanna refresh. Thanks, Derek. I'll reload. We'll see if that changes things, Thank you."
  },
  {
    "startTime": "00:05:16",
    "text": "Eric, are you seeing your new slides? should be a few after this. Keep going forwards. should be at the end of each 3. At the end of H3. Ah, okay. Yeah. After that one, there should be a slide. After after this one, Yeah. And for The next one is sometimes it it's stuck in a cache. Okay. I think what I can do is try to do the screen share thingy and Hopefully, then it'll show up. Gotcha."
  },
  {
    "startTime": "00:06:14",
    "text": "Okay. Hopefully, this will be better. And now, Eric, Here's your flow control by -- Yes, sir. Excellent. Very cool. Okay. Let's do that instead. Alright. service. and go on back. No one pay attention to David. Hi, David. Oh, you didn't see me here. I've been here all this time. Sorry, everyone. We've been fussing with the slides that we're doing screen sharing instead of PDF sharing. to get the latest and greatest. But, sadly great? Great time. Which part how much of the intro have we done? Oh, man. Should we get started? Why don't why don't we get started? Alright. Good afternoon, everyone. Happy Thursday, we're Almost done. Then we get to sleep. Welcome to the web transport session at ITS117. Next slide. I may figure out I'm One of your chairs, David Skinazi, and my cochair, Bernard Aboboa, is remote in controlling the slides in theory. Theory. Let's see there it goes. in theory, and hopefully, soon, in practice."
  },
  {
    "startTime": "00:08:00",
    "text": "So -- This is the note well. It is actually not on the screen. But it is -- That's a good reason for that as well. So doesn't even need to be on the screen because you can't read anyway. The font is too small. If you haven't read it, please do. I'll highlight 2 parts of it. 1 is the ITS IPO policy. which requires you to Thank you. To anything that you discuss, be it on the list, right, the microphone. If you are aware of a patent, you have to disclose that. have to license it. You just have to disclose it, so be aware of that. and the other part is our code of conduct, which set some rules and guidelines about how we collaborate as as respectful colleagues. Alright. And if you have any concerns with any potential violation of such code of conduct where the chairs are always happy to listen, and you can also report it to our area direct Azure or the inputs team. But we are here to help and ensure everything smooth Alright. You wanna go to the meeting tips? In case this is your first session. Yeah. We I totally prepared for this. I know the order that slides by heart. If you're If you're in person, we have these blue sheets. Actually, Spencer, since you're up can I ask you to hand this around? You don't actually need to sign the blue sheet, but by the act of grabbing and it's sending it to the next person that reminds you that you need to sign in to meet cow. But but, yeah, This is our post digital era where we now have printed out QR codes that we pass around. The future is now. Alright. And for remote folks, when you join, make sure you're audio, your microphone and your video are off unless you're actively speaking."
  },
  {
    "startTime": "00:10:03",
    "text": "and the buttons look like this. raise your hand. We'll be using the virtual queue as we've been doing to sure that remote people have as much access. And then once you get to the front queue unmute, and talk about whatever you wanna talk about. Alright. What haven't we covered in the intro, Bernard? Here's some We need a notetaker. Ah, right. Thank you, Nidi. Much appreciated. Alright. And then I will watch the Jabber messages as well if I find the link. Alright. Any any other I'm gonna switch Oh, yeah. It's to the agenda bash. Alright. This is our agenda, if it looks like every other agenda at the web transport session for the past 2 years. That's not a coincidence. We copy and pasted it and changed the numbers as per usual. So we're gonna be talking about where we're at with the w three c, then talk about each through. X3, then wrap Would anyone like to bash this agenda? you'd need a good reason, probably. Alright. You can remove the numbers, and it'd be easier. You know what's a great idea? I would save some time. We don't pro particularly follow them anyway. No. Yeah. We're Just one thing we may wanna Spend Mike. 60 seconds talking about interim. Interim Yeah. Let's do that. Alan, you wanna talk about that? I would say I was not was at the web trans table for some of Saturday and some of Sandip, but not all of it. But We have this new draft called the devious baton protocol. which can be run over web transport, exercise both streams in datagram functionality, And I think there are at least 3 implementations or maybe door. And they I think achieved at least some degree of interop"
  },
  {
    "startTime": "00:12:00",
    "text": "with each other. I know Luke has 1, I have one Victor, And lost some Minnesota of the bone. That's Oh, Christians. and the JavaScript. Okay. So there's a there's a bunch. And so if people are on their transport and they wanna try some interrupt, then we can do So that's awesome. Thanks to everyone for working on this. The IETF works on running code, and that's what's gonna get us to standards that actually work. And so thanks for implanting it and testing it. And as per always, as you're implementing and something is a pain remember to follow an issue so we can fix it. Alright. Thank you, Alan. next slide. Well, I think that's you. Thanks. Testing. Okay. Good afternoon. Will Law from Akamai I co chair the w 3 c web transport working group with along with the Yanivar brewery who's also online. But I'll be presenting a short update here, and then we have some questions. which can help unblock some of the current issues on the w3c side. it was very actually very useful last IETF. We had 44 issues. We got feedback on. and those did move those forward. So it's the the feedback we get here is is very useful. Status wise, we've updated the working graph. The latest version is July 12th. That is the 7th draft that that has been released by the group, and it represents the latest Version of the spec, our charter is set to spire at the end of December, we're probably gonna need to renew it again. We've already renewed it for 1 year, and we'll renew it for another. We don't anticipate any problem with that. Tim table wise, we've slid 2 months from what I presented to you back in May,"
  },
  {
    "startTime": "00:14:02",
    "text": "We're still hoping to get a candidate recommendation together by theendofSeptember. that might seem somewhat aggressive But if you look at the milestone status beneath that on the candidate recommendation side, we have 13 open, but 13 are ready for PR. So along aside from the outstanding issues here, it just takes some PR work which is more of a mechanical process then having to invent or debate a solution. So we can go to September 30, then timeline wise maybe in q12020 full, we can actually release a recommendation. we don't wanna do that ahead of IETF. And though, we should probably synchronize the 2, so there should be some talk there We have an annual face to face meeting for the w 3 c site, much like the IT meeting here. So in 1 per year, it is planned in Seville. in Spain on Tuesday, September 12th, from 5 to 6:30 PM, Central European Summit Time. For those of you who can make it, please do. It will be online. as well. Next slide, please. We wanted to summarize some of the major decisions and updates since March 29th, which was the time of the the last update. In that time, there's been a send rate estimate edit two stats. It's the estimated rate. I'm I'm not gonna read through all the checks there. It basically gives you rate at which data can be sent by the user agent in bits per second. and it's also available for pooled connection. but no pulling across origins, The rate applies to all streams and data grounds that share the web transport session. We specify the bf cache interaction It will close the connection when navigating away. networking privacy improves spec language, support, bring your own buffer readers for Datacrams, sort of fun, fun, fun,"
  },
  {
    "startTime": "00:16:02",
    "text": "similar to where transport receives streams, error code has been increased. Erico Lens, sorry, has been increased. you can add a send order to incoming bidirectional streams that modify the send order of creation We're actually we have an issue around this. We need to extend it, but this this work was done in in the get. And we change language that the user agent should divide bandwidth fairly between all streams that aren't a star, and there's some nuances there. when we talk about send order and implicit setting, send order, there's an implicit group of streams that don't have any send order attached. So those are the the major PRs that were pulled in. Next slide. is an update from Mozilla. I thought Yanivar might want to give this. This since he works for Mozilla, Yanivar, are you online? Yes. I can hear Yes. It's it's it's metallic, but it's Understood. o. Okay. On Well, I hope you can make out what I'm saying. I just want to announce that web transport is now in Firefox. general release version 144. 114. This means we now have 2 independent implementations of web transport. in the wild. among browsers. and 5 trucks, supports our support includes Datograms. as well as BYOB readers. was the last minute feature, an update from last meeting. which and our congestion control again is quick, protocol implementation is largely written in rust as part of our HTP 3 support, We passed 93% of web platform tests. Yay? And we have a few more features that aren't landing yet. including the send order that was mentioned,"
  },
  {
    "startTime": "00:18:01",
    "text": "as well as catch stats. And I entered the link to a JS Fiddle for demo. There's also we would love to run the mock demo, which is more exciting. Unfortunately, that's still blocked on web codecs, which is still upcoming in Firefox. but we hope to have that soon as long. and that's kept it. Thank you. in Senegal. Next slide. So we have a very wordy slide here. We apologize for that, but we wanna present the problem, and then we're posing a series of questions. And it's it's hard to ask you a question and have to remember previous information, so we put it all on the one slide. This concerns priority groups. So prior work out of the ability to opt streams into strict send ordering. It's an n 64 Send order number, because the number of higher priority It was designed with per media segment streams in mind. But if you consider 2 flows, one of which is adding streams with, say, pergop and the others doing it per frame, you very quickly get out of sync on the number of of your your send orders. And if you wanted them to be fairly or equally sand, you would have to store multiplexing, like, jumping with discontinuous send orders. it it complicates the solution. So the proposal was to introduce explicit priority groups in the API to separate these flows and basically create numberspaces or name spaces for the same orders. and you can read the proposal there. Every writable in the session gets a priority group. is a default group. All groups have equal fairness from a same priority standpoint, And within every group, you then interpret the same order. There's a proposed API. below it. And there's 2 options. Either you can group equals new web transport priority group, add"
  },
  {
    "startTime": "00:20:01",
    "text": "and link it to the web transport connection as an attribute or else it's a method of of web transport itself. a couple of questions that we'd appreciate feedback on. Firstly, is this group construct sufficient to meet expected media and non media transmission needs. We we don't want to add anything too complex but we don't want to disable future applications. That's Christian d. Christiane, is currently fairness is implied between groups. do we need an explicit waiting between these groups. other words, if I have 2 groups of flows, I want 1 to receive twice as much of the available bandwidth is another. Thirdly, should data grants be in their own group. And if so, what should the default weight be is it equal to streams, above streams, subservient to streams. 4th, our groups actually flows. and and We had proposed an API around flows at the t pack last year, it was not adopted. But when you start to think of prior groups. You are actually creating flows and when applied to Datagrams, it would allow you to send parallel flows data grants, which you cannot do today because Datagrams, fall into a big bucket. Fit, if flows were implemented on the send side, is there any mechanism in the transport to replicate these flows on the receive side? would be a question for That IETF, And then lastly, does such an application level priority group construct match or not match any web transport layer construct around prioritization and Famous. that's signed remote So not Thanks. Yeah. It sounds like if anyone has thoughts on any of these questions or things, please join the queue. Moe, go ahead."
  },
  {
    "startTime": "00:22:03",
    "text": "Mozanati, I think one of the things that may be missing is that these applications are probably gonna have a combination of different media types or transport object types that they wanna send and and it seems It seems a bit hard to chew on all of this into a simple concept like strict ordering, even grouped ordering. you have things that are naturally application limited data flows like media a very low rate media like audio, and then you know, moderate, media like video. versus file transfer type flows, and things that are not application rate limited that could that could saturate, you know, immediately a link. It's very difficult to harmonize all those into one simple, discipline. One simple ordering What's the priority mechanism? So I think there really needs to be thought about what are the application needs, and do we really need to have different carve outs for those different types of flows that you wanna send wanna send application limited data, What are the What are the parameters that I need to send? Is it you know, a a rate limit for each one of them. I have saturating data, what are the kinds of things that I want, and I doubt strict ordering write out group priority ordering are the kinds of things you wanna do there. So I think we really need to look at what the application semantics are for sending these objects many different ordering priorities, and even queuing disciplines for them. Okay. have any suggestions for what these would be, or could you open up I'm actually working on a priority draft right now. So I'm hoping to have it done before this session for this meeting, but Still work in progress. and it's it's specific for mock. It's not generic enough, but I think A lot of the same concepts apply across the board. it would it would it would apply equally to any any application over quick, even h HTQ streams. So I'm happy to share you the the work in progress point of that, and then we can get some of the early thoughts on that,"
  },
  {
    "startTime": "00:24:00",
    "text": "Let's get back to this. Thank you. Allenford. Alan from Dell. Priority and meta, priority enthusiast. My concern is that there are lots of it's gonna be challenging to come up with an API for priorities. it's that's gonna meet every application you need without this move many of these people have not written the applications yet. So you're sort of in a a bind where you gotta come up with an API, that you think is generic enough handle every possible scenario and You know, we've been playing around with different APIs with H2 priority, H3 priority, extensions three priority for a long time, and we're still kinda experimenting and iterating with the server segmentation, etcetera. So it I I think The best bet is to probably make something very simple that people can easily understand what they're trying to do, and then also create something that is I don't know, like, an up call where, like, when the user agent's ready to send a packet, it the JavaScript. Which one of your things do you wanna send next? So basically defer it all. rather than trying to perfect this API between the application and and the transport, so that you can communicate that perfectly because I don't think gonna be able to get one API that's gonna cover, everything without making it too messy and hard to use. Yep. I think from the group side, there's no expectation of creating a complete and perfect API. the goal is actually to make the minimal API set now that attempts to meet the majority of needs without obvious blockers. So with what you said, would you still we we considered send order as a as a as an optional baseline and simple priority scheme that should be caught it Do you disagree with that? And if you do agree with it, do you consider groups another reasonable and simple extension, or or are you suggesting the group's"
  },
  {
    "startTime": "00:26:00",
    "text": "grouping should not be supported So Send order seems simple enough to understand. I haven't ever written an application that tried to use it. an implementation to, like, measure if it's effectively doing What if if you're able to capture the applications intent, which is, like, one of the hardest prioritization. and and it's easy enough to implement a key that works that way when you're sending. So I can't say, is it going to work. There was a time when people got together and thought, this like amazing hierarchical tree structure was gonna be, like, the greatest thing for prioritization, but nobody built anything with it. then they found when you actually did it long after the RC shift, you're like, oh, And it's really hard to capture the application intent. So I don't know. I think Send order probably can be fine. I haven't thought a lot about in groups, and maybe it's okay. But I think you're gonna come up with You do groups, send you to your order, almost guarantee you that you're gonna find some other application that's not gonna work for them. they'll be like, well, I I need something else, and you're gonna be don't know how easy this for you to update add new APIs. That's why I'm sort of suggesting some kind of a a scheme that's, like, completely flexible to first everything back to the app location because we don't ever get perfect API for expressing every use case. Next. looking at RSC 9218 and HPP extensiblepriorities s t incremental flag. And if I understand, send honor correctly, what we have is basically a lot of urgency groups, and they are all incremental. wondering if as a need to define a non incremental bucket as well. I think for compatibility, it seems there would be. So that's a good suggestion. Victor was sort of huge priority to just"
  },
  {
    "startTime": "00:28:03",
    "text": "I don't think we need incremental in case when we'll have institute for priorities because you can just order sentences the way you Black 2. or 8 with with 3 big priorities, I think, So Regarding the up call, is that Alan mentioned, that would be the ideal solution. Unfortunately, it is not really feasible to have an tow in JavaScript for every packet because that will make your browser your networks that perform extremely slow. at least as far as what my current estimates are. not really viable. Now I have considered various solutions. off. one of which was a terrible thing called WebDPM. which we will not meant, of course, the rest of this meeting. But in general, I feel like the the the solutions that the currently on the slide is, like, tries to accommodate 2 problems while being as simple as possible. 1 is being able to stricter order streams and to is to have some carve outs between people straight or drawings, and it is probably one of the simplest It's currently the simplest solutions that we file that seem to be able like, to some extent if we're able to all implementers. I definitely understand it, sir. Or might be use cases that are not addressed as well as use case for which this is too much, but in general, assistance at this pharmacy in the sense that I Tennessee is being actually implemented the chip. with Thank Mozinatti. I agree with Alan that we should try to keep things simple."
  },
  {
    "startTime": "00:30:00",
    "text": "I disagree that this is gonna be ever be simple. I think if you look at the history of prioritization mechanisms, all of the simple ones are we're always duplicating. But I've heard a lot of people say we just need you know, strict ordering, and I'm convinced that that will never ever provide any useful semantics for the majority of applications if you look at the the things which absolutely demand, that you you've made prioritization work. thread scheduling, network layer queuing, all of those, you know, 20, 30, 40 plus years' worth of you know, prioritization work That can teach us a lot. And the mechanisms that we know have issues that are not useful to deploy. in those areas, probably are also not useful to deploy application level areas too. one of the things I see as a problem is that the application folks are just thinking of this purely in the application sets, not realizing that you're gonna interface down to is going to have hearing. and it's gonna have it's not it's gonna have schedulers of its own. you interface your OS layers, there may be multiple queues available there. that OS interfaces to drivers, there will be interfaces to probably have prioritization there. Down to the network, that'd be more So understanding how the application level, prioritization semantics map all the way down to the delivery is an important thing. trying to do it in a vacuum of just okay. I'll just manage my internal queues and hope that what I output you know, works for the app. that's too naive, and I don't think this is a simple problem. And I don't think a simple solution is gonna give the applications the performance that they want. I'm gonna cut the in the interest of time. I'm gonna cut the queue after colon on the topic of priorities. So Luc here, pretty much agree with everything Victor said. I think when you get down to the the basis with send order, You you don't have the callback you want in JavaScript where you get to choose which packet to send next. not gonna work."
  },
  {
    "startTime": "00:32:00",
    "text": "But send order the fact that you can update it dynamically means you can at any point in JavaScript can say this is the next packet to send. When your scheduler eventually wakes up and pulls the next packet. So really powerful. You like Victor said, you can do incremental yourself. When you end the stream, you just change to set an order, But I think this group abstraction is nice. one of the hard things to do with send order is round robining. effectively have to set a timer and rapidly change the send orders if you wanna do that. I think that this group abstraction is kinda what you get on the Internet. You get these round robinflows. So I think having that at checking JavaScript It's nice. It's simple. But, yeah, I agree with everybody here. Like, there's And Prioritization is a never ending topic. We'll never find a a good a good even acceptable solution. So I think this is a a good starting point at least until we get more real world data. Thanks. sluffy, Conference, I I was gonna ask about As a thought experiment, how much of this can be done as a polyfill on top of Send ore? understands a little bit different, but seems like you get a very close approximation of this. which made me wonder, that's true, that's a hypothesis that you could basically get a very close approximation into the polyfill. made me think like -- Nah. we don't need it. Maybe we should just punt that over to the application or something. But I I you know, I think I think you need something, and I think you need something that maps all the layers below you as well. And keep in mind this this could be used in a lot of different places with on a lot of different hardware. and and it and that's all are gonna have all kinds of queued well, the same thing Moe was saying. except keep in mind that goes right down even to the Nick card on the new hot Good points. Thank you. Go ahead, Victor. Make it quick, though. I was going to answer to the"
  },
  {
    "startTime": "00:34:01",
    "text": "calm self experiment, it is possible to emulate this to send order. If you're frames are short, we have to action the dates also, Keith. It's where, like, you two strings that are long lived file transfers. So and, like, they have to share them. So from that perfect, if you can't throw a simulator send order, and once you like, small bit different. that Thanks, Victor. Well, I think That was some useful information. Should we go to the -- Yeah. Let me go to the next we have we have one more slide. This issue is a little simpler. Perhaps not, though? Don't jinx it. Retransmissions and send order. So should new data on a higher on a higher send order stream preempt retransmissions of data that's being lost and is now being retransmit it on a lower same order currency script. So some questions. Should there be a Boolean API toggle whether retransmissions should inherit inherit, stream order, The send order of the stream which they live within. Should there be an API to specify a time window within which it makes sense that the retransmissions be given priority then after that, because they're too far behind life, They don't not important, and you would simply do high priority. Number 3, this may not be a problem at all if transmissions are rare and short lived. And number 4, this may not be a problem at all because lower priority streams can be reported by the sender. any opinions, on these items. I'll be really quick. I mean, if you just did this by default, it'd be a disaster. It would completely defeat the purpose prioritization in the first place, and it mean that on a loss in networks, you had a completely different priority inversions from other networks. It it would be impossible to sign out application and sell on top of this. Now if I can imagine saying, going to set 2 or send orders that the application can set 2 different priorities, which may be"
  },
  {
    "startTime": "00:36:04",
    "text": "hire or lower each other in any particular order about how retransmissions what what they're you know, 1 send order for the main packets and 1 send order for retrans I can't imagine that. working, but you have to pass that up to the application to control or this will just or or or you'll have complete inversions that are impossible to debug that. Can you clarify me saying when it or this what we are assigned to? So if yeah. Yeah. Thank you. Thank you. That's great. So that the application You know, you design an application, you think it's gonna be it's advertising, you know, you got something here and then something that's sitting on a higher priority of and suddenly, you're losing stuff here. So suddenly, that the the cyst depending on the packet loss in the system, Which ones of your streams get priority over top or each other start changing? That's a very, very difficult thing to design for. And remember, priorities are mostly Everybody thinks about them about improving the priority of something, but, usually, you're trying to Make something worse, not make something better. And this is when I'm deliberately trying to make this stream worse, the the the suddenly, if I have packet loss on it, you're gonna make the the application running the JavaScript is trying to make it better, and the the underlying web transport implementation is depending on network loss is trying to completely circumvent what the application was trying to attempt to do here. So I don't think you should do that. So I think that the if you're gonna do anything in this space, you should be allow the application to set a send order 2 to set 2 send orders, effectively. 1 for retransmission, 1 for non retransmission. And I don't think that you should assume which one of those will be bigger than the Okay. Thanks. Hi. So I implemented this. I done retransmissions based on send order and without it. there's no difference, honestly. Like, it's it's it's really quite like,"
  },
  {
    "startTime": "00:38:01",
    "text": "Yes. In theory, if you send retransmissions according to the new send order, like, if you have stream that was high priority, and now by the time need to retransmit its lower priority, This is flow control issues. Like, if you don't retransmit it, it's hard to do right and honestly just send that packet. Like, you know, like, unless your packet loss rate is really high, it won't make a difference. So think this just leave it undefined personally. Like, it's just up to the implementation what they do and In that case, defer to the easiest thing, which is just always retransmit first. Otherwise, Flow Control is hell. You just run into this case where you've got gaps and streams that still consume flow control. Mozanidi, I like Colin's idea of having a specific retransmission send order because I would like to use that as a hack to avoid having to do datagram implementations. If I can if I can lower the you know, you know, put the floor the lowest floor value of my retransmissions as a Hak Fir, for doing dataograms, and I can still use the streams metrics. I think that's pretty cool thing to do. speaking as an individual contributor, as a Datagram enthusiast. I'm very mad that you would consider not implementing data grams. That's not how it's supposed to be. That's it. Indigrams didn't require extensions for a 100 other things, and require fights with everyone that doesn't wanna do Datograms, I would also be happy to do Datagrams. But There's a lot of nondedagram enthusiasts. Well, send them my way. I'll I'll teach her teach them our ways. basically, Datagram Murder enthusiasts, I think they would be close to what they are. Oh, boy. No. Actually, I didn't know that. No. Okay. I think that concludes w 3 c section. Thank you all for your input. We'll take the notes back. to to the w three c. friction. you very much, Will. And just to double check, you're taking the action item of summarizing this conversation into the w three c issue. Yes."
  },
  {
    "startTime": "00:40:05",
    "text": "Thank you. It for the remote folks, he said, I guess, I am, which I heard just said as yes. Alright, Eric. Just real quickly while Eric's coming up. If anyone wants to continue discussion about priorities, There's a set of video media tonight from 6 to 8 that oh, that guy's gonna be able to do a presentation priorities that was booted out of the mock session. So if anyone wants to wrap about it there, It's tonight 6 to 8. doesn't wanna talk about priorities. Allen from Delmont Coacher. No one was booted. We ran it time. Defer that presentation to another time. So you're saying the presentation wasn't prioritized? That joke was already needed. Follow Ireland. Bock. dangerous games we're playing here. Alright. Let's talk a little bit about h Two. Only a couple of slides here. not super major updates, which I think is starting to show that that a lot of these documents are becoming pretty mature. We submitted a draft 06, which adds initial flow control limits. those work in two different ways. There are settings. So you can say, hey. on this overall connection, every new session that you start, is going to have these initial flow control limits. And then separately, we have a header field, which is the web transport init 1. And that lets you get a set of defaults that you can provide within each session as you send the connector request in response and all of that. So that's a little bit nice. Next slide, please. We also updated some of the examples to life easier for implementers. Next slide, please. And other than that, the question becomes what's left? So one of them is error handling, which I think we're pretty much ready to"
  },
  {
    "startTime": "00:42:02",
    "text": "right up, but we'll have a slide about that in a second. And then the other one is we filed a bunch of new choose from some implementation experience, which is fun because we haven't historically had as many H2 implementations, but now we have some some good code happening and and some things that we're finding as we do a close reading of some of those documents. So Fantastic. Next slide, please. Error handling. This is a fun one because way back when, in a much lower numbered IETF, we said, let's do the same thing that h three does because that sounds idea. And then a little while later, we said if you discover a problem, In H2, you can just reset the stream when the whole web transport session is gone. Gone. That's a really nice way to handle layers. So think the proposal is let's do that. which is not that different from what h three does, but, like, you don't have all these other streams that have bled off into the rest of your h three connection. you have to go chase down. Here, you have a single h two stream. And if you nuke it, then your stream is gone. Excellent. I see no one diving for the mic to tell us. terrible idea. Does anyone have thoughts? I see a thumbs up. Any more thumbs up from the room? Yeah. Alright. Any any thumbs down? No. Alright. Well, we're just gonna assume that people are okay with this, unless if you disagree, please come up now. Sounds good. Alright. Next slide. Please So one of those things from implementation experience is a question about final side. So this is an interesting one because when we pulled over the reset stream caps to look a little bit like the quick reset stream frame, and I stuck them both on the slide here. you'll notice that we skipped this field called final size. And if we go to the next slide, we said that's not an accident. We'll put in a paragraph here or really a sentence or 2. that explains why we did that. said, well, we don't have a final size in order delivery of"
  },
  {
    "startTime": "00:44:02",
    "text": "stream capsules ensures that the amount of session level flow control consumed by a stream is always known by both endpoints. And depending on how late you are after whatever delicious foods you paid for lunch, that may or may not make a lot of sense to you. But one of the questions is next slide, Is that actually sufficient? So this is a place where we'd really like some input. We know that final size is especially useful for things like Flow Control where you need to be able to say, hey. This is how far I had gotten by the time you by the time you said this is gone. And in h three, that's especially important things can come in out of order, and so some of the data that's going can can can can you know, show up ahead or behind of a reset. Is everybody okay if we leave that out? Isn't it kind of nice for the person on the other side to say, hey. Here's how much tried actually processed by the time this is going away. or how big I think this means. So Victor, go ahead this time, but use the q 2 otherwise. I have a question. So what does HTTP 2 itself I appreciate it. I think we just reset the stream. no final size. it sounds like we're fine. Okay. Alan for doubt. Is Remind me because I've forgotten this. This is the only difference between the capsules, that form web transport of rage 2, and the frames that make up quick. this? The absence of this field and this frame, for I think so. So I I kinda want it back because I kinda wanna have one parser for both. I lost that argument a while ago. So it's if anybody wants it. We visited now while we're here, but I really do not I don't think you ever need it to perform flow control reasons, you should always know. I think when you get the reset."
  },
  {
    "startTime": "00:46:00",
    "text": "that was the question. I mean, we we we did discuss the the partial code release, etcetera, at at length. And I think the question is, is there any new information, especially as we're actually implementing it hey. I would have liked to use that size to do blank. a little bit different than how I would have liked to use my parser to ignore it. I have no such information. Although I'm curious, I know I keep asking this and forgetting the answer. who has implemented somewhere all of transported range to. think we have at least two people in this room who are doing Okay. I'd love to hear from them how this works. can you clarify what you mean by consumed? As in, has the transport implementation reddit versus has the application That's not what the final size is for. final size is how much data have I sent on the street. and how much data was sent on the screen should be obvious in h 2 because of the ordering of prime because your reset would have to come in after Yeah. I think was our reasoning originally when we were saying, hey. It's nice that we wanna use the same parser, but we don't really need this. Cool. Cool. Alright. Next slide, please. That sounds like a pretty clear answer. Yeah. So just For the minutes, it sounds like we're good with keeping it without the without the final size. Right? Does if anyone objects to that, please come up, but sounds good. Good. The last one here, a bunch of stuff that we'd kind of assumed, but not actually put in the document. And so as we read that document more closely. maybe there's some nice editorial text that we should add, and so I just wanted to run this by everybody to make sure that we were all looking at this text and saying, yes. This sounds like a reasonable thing to say. The 2 principles here are one. When we talk about max sessions after you've said, hey. My max session 10. You can't turn around and say, no. Actually, it's 5. Just kidding. You broke on the rules. so bad, and I'm so upset with you. So if you if you promise somebody some flow control space, you can't renege on that promise."
  },
  {
    "startTime": "00:48:01",
    "text": "That's usually a principle we apply to also control everywhere, but We never actually wrote that down. And the other one here, is the streaming limits being cumulative. And we said the word cumulative in the draft, but as We read it. and try to explain it It seemed less clear than it could be. And so I was thinking maybe an example would be helpful. i. So for example, if I said, you can 10 streams, That does not mean that you can have 10 concurrent streams. And if you open 10, and then close one, you now have room for 1 more. that says you can have a total number of streams that have ever been opened and that number is 10. And so if you open 10 streams and then close one, do not get to open another one until I say that you can have a total lifetime number of streams that is 11. And at that point, you can open one more. And judging by some of the expressions of the people in this room, it seems like it would definitely be worth writing down some example similar to that. rather than just relying on the heavy lifting of the word cumulative. thoughts, feelings, is this how we want it to work. No. Mozni. I think it's important to clarify this terminology because I had this some more arguing about Maxstreams with with many people that I thought knew much more about Quip than I did. and I was floored that that they were under the miss, you know, interpretation that it's or you can open 5 more if Maximus is 5. So I think that's something very important to clarify for all implementers. Thank you. Yeah. can we can bake in a cute little example or something like that to make life we sure if cumulative is what we're you know, stream number is, you know, it's it's the stream numbers that that you're really setting limits on. not how many, not account of them, but the actual value, the string numbers are the thing that matters. Yeah. We could name it that way and and run into that direction. I think right now, it's a little bit wishy washy as to whether it's actually that or if it's the count of them that are then every other, and so it's slightly different."
  },
  {
    "startTime": "00:50:02",
    "text": "but let's let's say let's at least straighten that out. remove some of the confusion there. Alright. Sounds like we have a consensus on this one that editorial clarification is a good idea. if you disagree, come on up, but let's go ahead. And in that happy case, next slide. Thank you, Eric. Any other questions about h Two? Otherwise, let's go to h 3. It's just a thesis there or 3 issues. And The first one is just ticket in your hand. It's way easier. Just pull on it. Yep. Yep. Pull it towards you. There you go. Alright. Geek Sporters. So people have filed an issue in WLO's free feedback tracker for the APIs that there should be API. It's provides key supporters since this is something that quick libraries generally provides by TLS libraries and some people clients that's useful. And I start to try an API feature and notice. That's alright. So an issue is that you have multiple sessions, they probably should have different space for Kickstarter. So it's just a protocol issue. So I wrote up a proposal how to derive key supporters for web transport sessions. And the in general, we seem to agree with us. of proposal is sound, but there were also questions raised too. Whether having key exporters in protocol or say, okay, is in general useful. Oh, so if"
  },
  {
    "startTime": "00:52:01",
    "text": "You believe those are useful, place contours and microphone. Martin. Yes. It was me who opened this proposal, and I'm sorry. I haven't seen your your sketch okay. The solution is really solution yet, so I can't comment on that. Oh, the solution is like you odd have a derivation set that's similar. Like, CLS has kind of have derivation from, like, zip I think it's, like, from semester secrets, you derive the exporter secrets with, like, some strength. So here, it's similar except for from 6. like, exporter of secrets, you derive the one that's buying the specifically is the session ID. So you put the session ID in a in in a context or something like that? Yes. Yeah. Yeah. So let's let's general how it works, but it provides the same API still as accepted, like, call some Python session. It did. Okay. Yeah. That sounds reasonable. reasonable to me, but I I don't know a lot about crypto. So So so before we jump into how to build this. Let's talk about why we built this. you have a can you do you have any use cases for this? Yes. I know a guy from the HDP working group who's writing a draft about unprompted education. And to build something similar to this, on top of web transport, it would be really helpful to have access to a key exporter. can are you intending to build? So I'm just trying to look for use case. Don't take this as me disagreeing. And what what would you build with that? I I do have a use base. exactly from that. Oh, and what is it? maybe we can take that offline. Okay. Fair enough. Thank you for sharing. Bernard. Use case. which is I would claim that this will not be useful"
  },
  {
    "startTime": "00:54:00",
    "text": "for end to end encryption in mock or RTP or for hop by hop in and an RTP. So I'm not sure there's I I can't think of any use case for media. and so so be the oriented people here if anyone can think of one, please come up, but I can't. and Bernard just clarifying, I'm assuming that was as an individual contributor. Right? Right. Right. Basically, the the thing is this would anything you would do would be hop by hop you can't use it end to end, and Quick already provides end to end security services sorry, hop by hop security services. Right? So You you don't really in my opinion, it's not needed, but Moe, if you have an opinion, please come up. No. No. I agree with Bernard. I don't I don't think there's a need for this. don't I don't see unless you Unless an unless we wanted some applications to have some interoperable, ways to do things. don't think it's necessary. It's like a like a helper. Applications can do it. you wanna give them a helper, they can they can get their library for for a helper. No. This is specifically to ensure that whatever you're doing is bound to the specific TLS channel you between the client and the server. What do you mean by bound? It means you cannot like, you cannot set up a scheme in which you have you authenticates to one party and to use a server you sync your working, but you it's really it's proxy. authentication to another entity. that is the typical use case for key exporters. finding. But all all these exporters, they just they just add another label into, you know, the the current key material, some fixed label that identifies the exporter type that you're gonna ask for. So I don't see how that's really gonna give any kind of binding. It doesn't bind anything. It just gives you another permutation of the key material that you have with some fixed label. That's all you get from the exporter. It's not binding anything. Okay. You as a TLS client or server have access to key, but the web of, like, feature does not. That's a important distinction here."
  },
  {
    "startTime": "00:56:06",
    "text": "At that, I'm still thinking like Bernard, I don't see a I don't see applications needing this. Quick question. Can you blame what this even means, like, does this impact the transport, or is this just an API thing? it is an API saying. but it needs, like, some well defined semantics of how to interact with t So, generally, it's an API to a TLS sections that provides you. a key material that is derived from, like, the secret of the TLS session key. I'm saying this would be, like, a w 3 c API. Right? It wouldn't not changing anything on the wire. Yeah. Yeah. So this does not apply to actually add any burden to any implementations that are not, like, browsers or servers that's actually Yeah. Just to provide some AdEx. This has been used in in stuff like DTLSRTP, an EAP TLS, those are those are the things that use exporters currently. I don't think any of those things relate to web transport Okay. I'm getting a sense. speaking as chair that this sounds more like a w three c issue to some extent. given that it's really specific about, like, a layer that can access the key material that makes a secure thing of generating the the export and then handing to the JavaScript, which cannot. And it sounds like we have Not a whole lot of use cases here. Martin seems to have one, but some folks saying that there's not enough need. I propose to kind of send this to the w three c. and ask over there if they have use cases, and if they comes back saying there is a strong need for this, then we would add it. But"
  },
  {
    "startTime": "00:58:01",
    "text": "I got feeling as chair is that right now, there's not consensus to add this at this time. So, Martin, since you seem to be the the main proponent, Can I ask you to file an issue at the w three c and try to get more use cases listed there? Thanks. So for the minutes, Martin says said, Walter. Okay. Let's go to the next issue. protocol version negotiation. Okay. It's just a recap. in draft 2 version of the protocol, we negotiated that we support web transport using the settings that's called. settings enable web transport, or it's a full name if settings enable web transport to to because it was the version of the props and calls that that negotiated. Current we added mandatory settings as called settings about transfer max sections that is currently used in a similar way. And it's just currently you're required to set it in both directions. server has to send to the client, max settings, max sessions because by default max that sprints at 0. It just means you can't open web transport. So you can send max sessions of once, which means you can only open 1 with transport sessions connection, or you can open it more than 1 if you're as a server are willing to it's more than one session. At the time, And owns a client, we currently require you to send this, but value has to be any values that just known to. So the reason we're required to send it is the way we currently perform webtransport extension version negotiation is declined to offers multiple versions, and the server offers multiple versions. and some versions that ends up being supported. It's the max versions of this build."
  },
  {
    "startTime": "01:00:00",
    "text": "supported by the server as a client. There is a proposal. that we stop doing that and we require sets web transfer max sessions to be only sent on this server and not wants a client. I see Eric is in the queue, and he was one of the big performance is this. So I'll let him speak more about that. Eric, can you hear Apple? Yeah. So I think we'd kind of been talking ourselves around maybe to a place where we were thinking it was a to not require both sides to send this. there's lots of ways to do version negotiation here. if we want to commit mint version specific tokens for use with the protocol header in the extended connect. We can always keep using settings with transport max sessions with different code points from the server. But given that we're still in place where we require the client to initiate web transport sessions, it seems as though the server can still offer versions and the client can pick that version and initiate web transport with that version. knows the server supports. And so Perhaps what we're doing interop for some of the draft versions and stuff. It was nice to have both sides kind of agree to some extent on the version, but anything other than our typical you know, here's the set of what I am willing to let you talk to me with and the clients can come and knock on the door and say, hey, I picked this one. It doesn't seem like we actually need anything more than that long term. my personal position, I I I went back and forth in it. I first did not like that proposal, then I decided that I like it. then I went and implemented rafter 7 support in the Google quick implementation and decided that At this point, I really don't."
  },
  {
    "startTime": "01:02:01",
    "text": "like it. And the reason I don't like it, it is really easy right codes when you know which version of the protocol you're speaking. And as soon as you lose, like, certainty as a version and have to, like, be really flat a vote. you now have to think, like, oh, in this version of the draft, I defined Frame Foo. But in this version, I define frame v 2. and, like, use and well, is there a difference in semantics you can vary between Fu and seafood version 2, but now you are, like, okay, as a receiver, I can process this, but now as a sender, need to figure I support both versions. I need to figure out which one I support, and I need to do the telephones as client and owns a server. So I tried to come up with that logic, and I found it extremely painful to deal with. So I have a strong preference for a scheme in which that both peers have at any points that they speak, they know which exact versions are speaking. That's just like current. personal position. Uh-uh. do people have authors. So -- I I'm a little just to clarify. in Victor unless I'm misunderstanding, it sounds like what you're saying doesn't quite match your slide, or I might misunderstand that. This this slide is the explanation of the proposal as it was written up, Okay. As a as a yeah. So I I I kind of feel like There were 2 proposals. Nope. Mold. This one and one for Eric on the on the issue where both of them say, we only send the setting from the server and not from the client. And then one of them, let's say, call it yours, is when we change, we renumber the capsules. And Eric's proposal is when we change, we chain total the value of the upgrade token. Am I understanding this correctly? Either is fine. I think the the thing you would"
  },
  {
    "startTime": "01:04:03",
    "text": "twiddle in the first proposal is the setting code point. which is how we advertise support for our transport anyway. Right? quite good. then what's that? So right right now, you send settings web transport max sessions something other than 0. And tomorrow, we have web transport v2, and I send a different HTTP setting, which is settings web transport to max sessions. and that is some number that is greater than 0. Hey. point, the I think one of the questions is And at that completely reasonable to say that, you know, when I'm writing a bunch of code, I'd like to know what version is in use when I'm gonna friends, But are required to have that setting before you could talk about transport at all. So before you're sending any capsules or any frames that are related to web transport, you would have that setting present. Okay. You would have it between user setting from the peer. How how do you know which version client is actually speaking? Oh, when the client sends the connect request. That's why you wouldn't mess with the opportunity. Yeah. So so that's why I think just to again, I think I'm trying to synthesize your non expressed an opinion. the server sends this setting, and it can send 2 separate settings if it's supports 2 versions of our transport. And then when the client picks one, you you're suggesting that it uses a different upgrade token to tell which 1. And I think Victor's saying that it uses different capsules to the indicate which one Well, this is well, that is That is one possibilities. So the problem of specific is a great talking is one upgrade token is per request, so you can send different talking to you can send, like, the web web transfer to v 1 and web transfer to v 2. r. and this is problematic because they could change the behavior of your parser and"
  },
  {
    "startTime": "01:06:01",
    "text": "that state is global per connection. Okay. But then what are you proposing, Victor? I'm not understanding the technical I'm just proposing statuses. Uh-huh. Okay. So your proposal is not what says proposal on the slide. Yeah. I think this was supposed to be. This was was the issue that Eric was supposed to write that slide, gasp. think so if I'm understanding correctly, Victor, what you're saying is that I the client and the server both send this setting as many copies of this setting as they need you for whatever they do. and we use our little max of max of max. to to Alright. figure out that that's the version that everybody's talking. And I think the the counter proposal was it's a little bit annoying. as a client to say, hey. I support 10 sessions. when you can't open anything. Yeah. Run that doesn't ruin me. Right. And that was fine. It was nice to go down to just one setting instead of having dual settings for that. So, like, I'll take that over having 2 seconds. but that prompted the question. Why do we need to a max of max of max? if we could have these servers say, I support these versions and the client then fix 1. which is often how we do version negotiation. given that the client is the one who has to initiate all the settings in ourselves, If you speak XER RTT, ours is necessarily in set order. We did all this fun stuff for how you save and remember settings for CRGT. Yeah. I I I just like when these are to tease the clients, I think, first since then the server setting and then the client settings. Again, it's on the emergence of No. For 0 RTT, the client has remembered the settings from the previous connection for the server. Alright. Well, it sounds like this is quite confused. so we're gonna take it back to the issue. But, Martin and Luke, if you wanna"
  },
  {
    "startTime": "01:08:01",
    "text": "I'm confused by the put some quick comments in. max of max of max. Should that be the max of the intersection of the 2 versions? Thanks. Yes. Sorry. It is Max of it. I I I just realized that this is melted to It's massive, like, Yeah. The intersection of the two sets. Yeah. And my question is, I I wasn't aware that the client had to wait for the server settings before it sends its own settings. Is that a requirement, that round trip? No. In fact, this does not require. Yeah. Like, this computation can be performed only when you have both of them. But you can once those settings are sent independently, that's why you clients puts all version 8 supports and server console version 8 support. Yeah. So my my preference would be the client optimistically sends the settings frame with no knowledge of the server settings frame, So it just doesn't it's not forced to incur a round trip if there's the server's settings is lost or something. So I think we have 2 clear proposals. Well, clear might be a strong word. We I think with 2 slightly clearer proposals. 11 is both of them send settings, and then when you get do the max of the intersection set. And the other proposal is the server only sends And then the client sends edit in the upper grade token. I'm gonna do a quick show of hands just to see, and if it's not clear, then we'll take it back to the issue. But just as an attempt to resolve this, Does does this sound like a question that I can ask the group? I don't know. I'm seeing fluffy, making very confused knives at me. I'm just confused. Alright. It did ignore me. Okay. Anybody else Alright then. Actually, anyone in the audience part of my confusion came from what's on the slide is and totally wrong. And everyone else understood that, but I took me a little while. No. No. Trust me. It took me quite a while as well. Yes. Oh, please raise your hand if you're not confused. I'm just raising my hand. Alright. So I guess we can't ask the question because there's too much confusion. Let's take it back to the"
  },
  {
    "startTime": "01:10:02",
    "text": "you, and maybe next time, let's do a better slide. Maybe I'll do the slide. Fine. and then you can make fun of me. This slide is actually I think Eric is supposed to talk about and I'm gonna say if you want some less good slides, Challenge accepted. I'm gonna stand over here because it's fun to confuse people. of the last things to talk about in h three is this document that One Martin did a lot of the the heavy lifting And Thanks, Martin. somewhere for writing up some of the stuff for can we go back our email is going back to Slack. When Thank you. I wanted to stare at the name of the header for a little longer. that there there is a document that writes up some of how you would do session limits for flow control and and stuff like that in h 3. obviously, some of this depends on the document too quick. for being able to tell how many streams are actually open and when they were closed. But the other thing that's we're talking about here is is we also kinda mentioned do the same thing that we did in H2 where we include the header when transporting it to include all the different initial values and and that stuff. And so I think that's one thing that would be good to get some feedback on is if we're doing this, we wanted to match the way we did in H2. Do we think it's actually having that header, all of that stuff. general thumbs up, thumbs down. like, an idea. Cool. Alright. Next slide. And the main overview here is it grabs the same capsules that we used for 2 makes a WT max data, max streams, and then the blocked variance of those And the important part splitstream split out into its own X Three stream."
  },
  {
    "startTime": "01:12:00",
    "text": "We just use the native flow control there, and so we don't have any extra data council. Otherwise, all you're doing is just the very straightforward thing, which says, hey. Here's your total amount of data that you can have outstanding on the session. You say here's the total number of streams that you can have inside of this group of streams that we call a web reinforcement session. and all the other layers of flow control work just like they used to. Nothing super new, nothing nothing super fancy. It turns out the the actual text to specify this is is Pretty straightforward. very analogous to how it already works in h three, very analogous to how things were. inh2. nearly as as painful as we were. did not end up being concerned it might be. is the end of that? That So I guess That is the proposal that's the question here is, do we want to do this? And do we want to always do this? Do we want to require this? Do we want to require in some cases or do we want to Just not this ever. And, like, decided. The mic line is open for answering that exact question or which I mean these 17 questions. Oh, this multiple choice buffet. Alan's and go. not running out of memory enthusiasts. I know that I've wanted this because I don't wanna run out of memory. that I right now, I don't feel like implementing today. So I'm guessing that other people have implemented they're further along, and they're like, yeah. And everything works great, and I'm ready to, like, make sure that I have this, like, additional flow controller working. And maybe they already wrote the one for h 2, and they're just gonna pop it into h 3. So I like that we have it button. not ready to commit to it. like, yes. We must include this linear."
  },
  {
    "startTime": "01:14:01",
    "text": "I don't know where that leads us. So, Alan, are you suggesting you're inclined to support this, but you wanna implement it first before you decide, except you don't wanna implement it right now so we wait. I don't know. I think I think it's a good idea. in the long run. I think if we don't do it, then we may regret that decision later. and but I'm also not, like, just makes them I feel very behind in Linting Boot Transport and, like, jumping this on there. It's, like, gonna put me further behind. So I don't know. I I feel like Victor is, like, So Alan is ambivalent. So Unfortunately, I think this is required if you session pooling. I just don't want a world where some background tab is causing my foreground tab to to be starved because, I don't know, JS is just throttling it So I feel like if you need to do session pool and you do, you also need to do web transport flow control. That being said, I I don't wanna do session pulling, Personally, I wanna set max sessions to 1 and never implement this. So I would like this to be optional in some way. Actually, stay there. That's that's a very good point. Here here's a straw man. we could say that this is only required when they're when the max sessions is more than 1. how to feel. add that to the list. How would do you feel about that? It also depends of h 3. is eating your flow control as well. again, personally, I'm not doing h three and web transport together, but I think that is a That is better than requiring this for sure, sir, for sure. or or require yeah. We could also say requiring it if the connection has something is not dedicated to a single web transport session. I"
  },
  {
    "startTime": "01:16:01",
    "text": "you so that that would be a proposal that would work for you, for example? -- sessions is greater than than 1 then I have to do flow control, I think, is a fair requirement. Okay. Alright. Bye. And that Alan, can you get back in the queue just for that one? I would love to or quickly, what what does that optical how you're gonna prevent additional HTTP key requests on this connection, Yeah. Yep. don't hurry to do that. But In our implementation, it's just it it's not fed into the connection pool. Well, I mean, you're saying you must implement this if you said it. Right. Yeah. Great. I mean, I I don't I don't know how you enforce this mess and and what that means Oh, we we would have to okay. So so maybe let let's not rat hole on how we exactly say. But Say only require this when you're doing pooling, handwave handwave. some contour of that, Okay. Alright. But I'm the details matter. Yeah. That's what we're really good at. Right? Holding on the details, so we're in good shape. Eric, So I think that that's kind of the the challenge with that approach. As we'd said, it shouldn't matter if you pool or not. Each web transport session that you have should not be able to tell everyone else that may or may not be present. And I think the the place where that runs into issues is you have other h three traffic as well. And so Sure. You can say just don't put it in the connection pool, and then that's kind of a little bit nice handwave handwave. But If we're saying our our general principle is each web transport session, is it something and shouldn't be able to be stalled out killed, run out of memory by other tab x, other thing you did y. you kind of end up needing to be able to do this. When you say, hey. If if max sessions is 1, what you're really saying is we still have this. We just call it by different things, and we use the against their full control. So it's not like the bugs you're gonna have in flow control are gonna be different."
  },
  {
    "startTime": "01:18:03",
    "text": "it's just you have a small delta of additional limits within your session. don't know. kind of end up in the in the shrug phase of what Alan said, which is don't think anybody's like, oh, yes. This is the best thing I'd always love to add to my web transport session. But I do think we end up in a place where we're kind of sad that we don't have it done Thank you. Thank you. Lucas Pardy, Flow Control bug, an enthusiast bike. I'm at I'm at where Alan is. I'm lagging behind in implementations and and and the idea like, how can I do this as well? doesn't fill me with jewelry, but it could be something that we want like, I I just I yeah, like, how can we test this and make sure we don't keep adding more focus for bugs that just, like, make everyone stay bad. That could be a question for the EDM streaming in the things or or either interrupt testing or the web platform tests that we're doing, but I just I I don't know. I wouldn't say no. can we defer for a while? Is it gonna hurt anyone if we did? I don't wanna take forever to do this, doesn't seem is it Egypt? To me, it's not. So I'm just clarifying question on that, are you saying that we could publish web transport with this, not edit it and add it later? Or are you saying can we just for for now before publication. you should make the decision before publication, trying to retrofit that after the fact. is is I don't think we we can make that decision yet. Later down the road, maybe we could decide actually will do web transport version. 1, without flow control and be too without no. Don't do that. But that might be the decision future us, can make when we've done some implementations and convince ourselves. Maybe actually we we just we just we just over over indexing on on what we think the problems are. Thank you. Martin."
  },
  {
    "startTime": "01:20:06",
    "text": "I agree. We shouldn't ship this ship multi session without low control, So I'd I'd rather go through the pain and implement it now rather than doing that later and potentially delaying the document. because people still need to implement this proposed Thanks. Alright. So the sense I'm getting from the room Oh, go go for it. I can't -- -- since I can't actually put myself in secure. So My specific proposal would be for the course of action. that I have, and I think we'll want. So there are in the browser, there are two ways you can create some web trans you can create it with without the flags that tells you to allow pulling. or you created with a flag that tells you to allow pulling. So this creates a possibility that if we shifts us as an extension, what we will do is is If you do not request pulling, you'll get at dedicated per connection where your what transport is the only sense. That's a us connection and you don't miss us and everything. spine. Now if you're asking for pulling, What happens is the browser will attempt to reach to its like, HTTP free session pool, and it will find the connection on which it could potentially open this web transport session. And if it finds it and when it finds it, it has to check some criteria. And one of the obvious criteria is that, well, that's a session support web transport, and we could add another criteria whether it's reports will transfer to a Flow Control, So you would require both. And if you don't support Flow Control, you would fall back to the dedicated session and that will work out as well. So it's just a"
  },
  {
    "startTime": "01:22:02",
    "text": "one path forward I see, which would unblocks the dedicated cased. r, but would would still leave us the room to candles if needs to flow control correct. does this idea sound? reasonable to people. Bernardo, I'm unlocking the queue because I think this is the last slide, and we're we have, like, than 30 minutes on the clock. So Does that sound alright? Yeah. So I was coming up just talk about this too, which is funny. But Would anybody do session pooling without Flow Control? as in would you have multiple, what's a tab that share a quick connection, but don't have flow control. because that seems like deadlock central. It seems like it's very easy to start. So I think what you're saying, Victor, where I think sesh max sessions and flow control of 8bound, Well well, max sessions is not an No. Max session is a declaration of a server from what it supports. it is distinct from what the client has actually go to do. Okay. I mean, I'm like, ultimate was a decision to pull or not to pull as a client's decision. Yeah. So if you pull, that means you also support Flow Control. Is that a is that a requirement. I think, yes, but that's also I wanna know if there's anybody in the room that wants to do session pooling, but doesn't wanna do flow control. That that's a good question. Love to get folks's opinion. if you do, please come up. Otherwise, of course, other things in the queue are welcome. Yes, Eric. wanna do that, but I think we're in this weird spot where, like, if I say no pooling, we need to be super clear about whether that is no pooling with other web transport or no pooling with other random debt request on the h three connections. So currently, Ethos and problem. No pulling means. Whenever you ask something with no pulling, I will go and create a dedicated connect and and it will not go anywhere into step."
  },
  {
    "startTime": "01:24:02",
    "text": "which is a fun thing, but not necessarily what setting max sessions to 1 Well, yes. It's it's what settings the API slot when it's a client base. Did we write that down? I will try to make sure that the tax is extremely clear. It's And the reasons that this behave is long even, like, due to, like, pulling a deadlock. It's the reason is that if you do that, you have much clearer expectations around the same like, it's like stats and estimated congestion controller states. It's stuff like that. That's the south for a reason. we have some. Lucas Pardo responding to the question. I think I what I don't have a bit of handle on is hear a lot like client sides doing things, and does it mean to support? these things. Like, I need to go and update my mind on this back so I walk in late as well. play shoot me. But I supporting things versus, like, not is ambiguous to me. If we're gonna say, yeah, I can do something, but then the client chooses not to do that. what position does it put me in? Is the expectation that me supporting flow controllers, I will not overflow the client. or Falcon put a flow control on what the client can send to me. Obviously, web transits is a bidirectional thing. that's why I'm unclear on. But if I need to go and do some homework, I will. But I just wanna make sure the discussion is reflective of I I think it's just a case when we say support flow control those bids. if you both client and the server say that they are ports of the spray and they will send those frames. Right? And will a will honor sales. It's the limit, let's say. threats inputs. does not put a similar way position because it's saying, like, make our words, this if you try to do it? Like, I don't know. Let let me go away and see. That that would be my general concern. but it's maybe not as much close as it sounds. but I'm I'm happy to be in the rest"
  },
  {
    "startTime": "01:26:03",
    "text": "Thanks, Lucas. I mean, it does sound like if we decided to Make this tied to pooling, we'd have to write down exactly how and I think some people need to see the how before they decide they're okay with it or not, because it sounds like the how might not be trivial. Bernard. I I have a little bit of a concern about how we would roll us out and the effect on the API. Because right now, you know, we don't have pooling in the client, so nobody's really using this. and Am I correct, Victor, that this would be JavaScript observable? I actually believe this is to some extent JavaScript neutral. in the sense that you currently have to already handle the situations where you run off run out of connection global. flow control limits. This is just adding extra limits on top of existing limits. So you already have to handle limits. us say mic change. the way they behave. Right. Right. So I I'm thinking kind of like how you would introduce this. I think you would have to introduce it along with pooling and baby Well, at that time, people would begin to observe changes in behavior. in their apps in their apps So a it's just a little bit tricky. I I personally would probably not use this and wouldn't use pooling either. But, you know, I think in some sense, it's probably good to under stand it now and have it in the spec. but I I think, you know, it'd be one of those things you'd have to test with developers to think if they if they get it or understand the behavior."
  },
  {
    "startTime": "01:28:03",
    "text": "Alan went down. So responding to the question, would I where do ruling without this, I think the answer is no. So it's fine for me to let those states get coupled. and It's still And I don't I think I may even think about it more as well, but it it's still strange to me that You can put these limits on the web transport pieces, but there is no mechanism to limit what the h three side can do. and so the h three's you could try to be very careful to make sure that web transport doesn't eat your h three data, but h three can meet your own transport data. I don't know if we need to solve that or not. Thanks. I'm team. No. No. convenient. I don't think this is normal. because sorry. The the the thing Matt was talking about. -- Define this. I'm I don't think it's a problem because I I As far as I'm aware that if your connection's choked up. you can't make more HTTP requests, then it's fine. use magnetic connection. client If web transport eats the entire wise. Yeah. whatever resource on that on this list. The that might be exhausted. then then you can make another connection. And and the other way is definitely always gonna be the problem. So I I I really don't get why you would add extra complexity Here, by like, we need these things. it's easier to always implement these things. So I don't know why you would try to not have them So Yeah. Yeah. That's not lazy. I mean, if we if we agree that we need them, in in any context, then we might as well have them in every context. as far as I'm concerned."
  },
  {
    "startTime": "01:30:02",
    "text": "So so, MTT, to clarify your position, you're saying that the idea of having this optional and only when there's pooling. You don't like that. Yeah. Very much don't like that. I'm just gonna quickly ask. Is there ever a limit on how many connections like, I don't know if there's any written down anywhere. Like, we back in the olden days, we used to say 2 connections 21 server, and that that doesn't make any sense anymore. more or less down to one for the most part, for HTTP. But as soon as web transport comes into the picture, I think the number of connections might be equal to the number of web transport sessions plus 1. Worst case. Marco Minis Sire. I I I see, like, a lot of complexity in, like, implementing stack flow controllers, But And and I might have missed this. Apologies. But what's the benefit that we're getting when we're, like, pulling these sessions or pulling the connections. So I think that that was something that we we talked about a while back, which is in HTTP through an HTTP 3, you can have multiple requests on the same connection. And we we decided, like, oh, well, web transport is an request. So unless we do anything special, it might end up on a connection with things, do we want to allow that or not? and the decision at the time was, yes, we want to allow that. because it allows you, for example, if they're on the same host, to have your gets and your post, you know, transfer all within one envelope one transport connection and one encryption context. to interject with that. I think given the amount of hoops we're usually willing to jump through to sayground trips. in other areas. that seems -- That's another good reason. Yeah. Thanks."
  },
  {
    "startTime": "01:32:02",
    "text": "So, yeah, those were the motivations. We we ever I think that there was an in a sense in the room that pulling was gonna be hard. So we set it out, support it, but no one implemented it. But, yeah, turns out it is hard. Let's see. Who who's next in the queue? Sorry. Okay. Really quickly, I think to what Bernard was saying about kind of impact on on apps and and in the JavaScript client. I think part of the point here is that a pooled web transport session and a non pooled web transport session should not appear to be meaningfully different to a JavaScript developer. And this is part of how we make that happen. But fundamentally, saying, hey. I want a dedicated connection. when I create my web transport session and and having a goal there say this doesn't allow cooling versus this doesn't allow cooling. If that makes sure app work better. then straight up somewhere. because, like, if you say, hey. I I need my own dedicated one because it feels good and I'm special and I wanna go fast. And then what's gonna happen? The browser's gonna start sending the same UDP packets from maybe the same five people, maybe a different five Depot. like, and you're gonna hit a queue that is lower in the stack that you probably communicate with less well. Like, you're not getting there any actually faster. So, yeah, there's there there's minimal complex that we're gonna have to do anyway. But I mean, I I I could see an argument that you shouldn't even in the constructor be able to say, is allowed to pool or not, because if if if you can do that and observe a difference"
  },
  {
    "startTime": "01:34:01",
    "text": "than, like, we have a bug. Yep. Thanks, Eric Martin. there's also the option to set to solve this problem at a different layer. namely the quick layer. I'm not sure if that's the right layer to solve it, but it would solve the problem of having HTTP free, eat all your web transport screens, which this proposal. cannot prevent, a while back, I wrote up a proposal. I don't think I posted it to the list how to define stream groups inside of Quick, and then you put you would basically have separate limits for for groups. and then you could assign all your HTTP stream streams to one group all your web trans, but strange to to another just throwing this out there. Thanks, Martin. Luke. I wanted to ask Victor a question because I think you mentioned something important that I don't know if anybody caught on. You said when Chrome you establish a web transports connection, And then it sounded like you no longer use that quick connection for any h three request going forward. Correct. That this connection, in fact, is not related to any pulls it is, like, entirely owned by your JavaScript subject. Yeah. So based on that implementation detail, At least at least how Chrome's been uploaded, you do not Web Transport and H Three do not share a quick connection. if you disable pooling. Yeah. Specifically, it's like if you disable, but that's correct. So if you do not maple pulling. Yeah. And you said that was necessary to implement some of the web transport API? It is not entirely necessary. But if you are relying on things like your congestion controller providing your estimates for your sent rate and things like that, congestion control state is connection global, so if you pull this stuff up work. clarification question."
  },
  {
    "startTime": "01:36:00",
    "text": "if I'm asking my congestion controller to provide me an estimate of send rate, and I have a non pooled second web transport over here right next to it. gonna track send at the same rate, is my congestion controller not equally wrong, but now it doesn't know what it's talking Well, congestion to if you have 2 competing connect they can provide you to accent rates because computing connections contraction control will equalize them. And, like, you will have, like, correct estimate of half of your bandwidth in long term. But can also just do that with your one that knows about that is a completely different problem that is much harder. So But I -- Like, Seth is like you are trying to do bandwidth allocation. Yeah. the order. Clay. as opposed to doing it by accident where you don't -- Well, if if there's no side accident, then that's for know about it or have any Yes. Yeah. So that's kind of my preferred well, the way you've influenced is my preferred way. kinda like a WebSocket where you just take over the connection. No more pulling, no sharing, expo you know, use quick flow control, use quick congestion control. that might even be an option too. Like, almost like a 3rd mode. You have, like, dedicated connection, I can share with h 3, or I can also share with web transport other sessions. But but I I hate adding a third enum to the -- -- toiletries. Things are, like, set them up difference between, like, sharing with only CTP and sharing with CTP and other webcast part. If you're sharing it per se into your it's you look at all that, like, sharing with same problems. Moznadi, I think the reasons for having different connections seem to be mostly around multiplexing, and congestion control. So affirm we assume that the condition control has a good design so that it's distributed if you create 2 connections, yes, they won't necessarily fight each other, but that's a distributed decision that you hope that your algorithm converges"
  },
  {
    "startTime": "01:38:01",
    "text": "between the two. But if you put them in the same connection, then you have a central congestion control that can understand No. That doesn't have to guess or rely on its its theoretical fairness of a distributed, you know, flow control. can have them under a single context and and do it that way. And you and you don't have any multiplexing when you have multiple connections. So there's, you know, there's an efficiency loss if you're relying on multiplexing, which is, you know, probably the main advantage of quick period. So I think It's it's it's not good to to one a bar or one or the other. If someone wants separate connections because they wanna be independent, that's fine. But if someone wants the efficiency, and single congestion control context of pooled connections We shouldn't know, we we shouldn't cause those people to to suffer. And to Martin's point, not liking the this being optional I heard the argument support not wanting to do this is laziness. And Martin's argument against it is don't be lazy, Just do this. don't see why it's it's it's a It's so hard just if you don't care about it, set it to a giant number. If you don't care about these these limits and you don't wanna actively manage the flow control, just set a giant number and be done. You can be lazy, and and still set a junk number and be lazy. And then it satisfies Martin's desire of having uniformity. There's not if this then that, then this parameter applies, it doesn't You can always have this parameter globally. And the people that are lazy just set it to a nonsensical high number, late lasted this is part of this, but also part of the issue is that you have a dedicated connection to yourself, it is not Just Not necessary. It is redundant with the connection level float construct. You're just effectively doing the sensing you're already But if if you do if if you effectively don't care about this, you could just send it to a a huge number. What is what is that harm? What's the what's the downside other than having to do that one second?"
  },
  {
    "startTime": "01:40:02",
    "text": "There's no technical harm. Right? Exmo, Bernard. Yeah. There there's just just one comment, which is people mentioned But when you don't have pooling that you get the connection to yourself, you get a bandwidth estimate. which is just for your app, the other aspect of pooling aside from having to share bandwidth is also having share the congestion window. and and that's actually not a negligible thing because when you're sending keyframes, The congestion window actually is what determines your latency. So the key frames are often bigger than the congestion window and require multiple RTTs. So not understanding what that is or having it affected it actually can affect it can improve your efficiency to pool. in particular, if a bunch of stuff pushes up the congestion window, you know, when you start sending media, you'll you'll discover that suddenly your keyframes get through a lot faster than they would have otherwise. So, anyway, that's another thing to understand. But as was noted, you're gonna pool, you really have to have glow you know, understand exactly what's being sent. to manage your app. It's it's your responsibility. Thanks, Bernard. Spencer. Spencer Dentalkins. I just wanted to rewind back to what Eric said. a degree with him about the idea of applications getting different behavior using pooled and non pooled connections. I wanted to add on top of that There is It turns out that applications can do that. there's a pretty good chance that application developers will try to exploit that, and then it it will be up to transport people to try to unwind what happens so that they can Make sure that you can't tell them, you know, get different behavior. is your email"
  },
  {
    "startTime": "01:42:01",
    "text": "that just seems like something we might be able to head off by saying quite cool. Lucas Pardo. Responding to to Moe's point about, like, separate numbers and it kinda problem goes away in my experience having implemented and debugged things to do with quick level flow control. doesn't work because you've got a load of accountancy that needs to be happening. not just related to these frames, but the other things that we're doing, like, when you reset stream, do you get some of that flow control back? or not. Like, there's there's many ways we can do this. If you're not lazy, you do the work. But there's there's that are just tricky. And and so if you do this, you have to do it properly, and that's okay. but you can't do it and be lazy and do it. because you will hit issues at cause interrupt failures. They're incredibly difficult to debug that Yeah. And even if you do it, you're not guaranteed that you're peer old. Correct. And and that's so that, again, that goes back to my earlier point that if I was a server, I'm not as good as the I I with all this stuff. Like, I go, yeah. That's fine. like, and I do something. There's an upstream failure, and I need to close some some parts of the web transport session and not others you you end up in some situation where there's a mismatch in the accountancy on both points. and and that's terrible. Again, the the zombies of this happening and and being fixed eventually. And I just don't wanna we try to have mistakes. It's like we're covered with interop testing. Brilliant. But, basically, I think we would need some people to put some thought into that. Lucas. Luke? Last time did the mic, I swear. But replying to to Moe, if you just set these infinite values, you're gonna run into the quick low control. And what's gonna happen is you're gonna have, again, a background tab that is just not reading from streams for whatever reason."
  },
  {
    "startTime": "01:44:02",
    "text": "If you make a new tab that then makes a pooled connection that joins an existing connection, it immediately hits a quick flow control limits, and it deadlocks. Like, it's not risk It's unresponsive. Whereas if you make a brand new connection, you get brand new Flow Control and you're on Block immediately. So I think Without Flow Control, you can't accomplish that dream of having pooled and non pooled connections appearing the same to the application. you effectively need to make sure that The sum of all the flow controls for every session is less than the quick flow control. And and I think the only way you can accomplish that dream, and I That seems hard too. Yeah. The the If you don't use these, then they're effectively infinite because You're only subject to quick. If you do use these, you're still subject to quick. you're also subject to this on top of it. So it seems like it's the exact same semantic if you if the app is dumb and says these infinite, or these don't exist for the app. It'd still be, in both cases, you're only subject to the quick flow control. Right? it seems like it's the same thing whether So you require this all the time and set it to something nonsensical. or you only require it sometimes And in that case, it's the same. It's still requiring it and setting it in at something huge. Thanks. Wow. We have drained this queue. So let let's see if I can try to synthesize a few of those. points. I think we have a bunch of agreement that we can't do pooling without some kind of flow control, otherwise, bad things happen. And I think we also have agreements that or that one I'm a bit less sure, so speak up if you disagree. on this property that if you select be pulled or be not pulled. It shouldn't be"
  },
  {
    "startTime": "01:46:02",
    "text": "too visible to the JavaScript. And think we have agreement that that means that you need flow control in both cases because if you only have it in 1 and not the other, then you can detect that difference. So that kind of point to we, therefore, need flow control. What I'm also hearing is Flow control is hard, and we're engineers. We are therefore lazy in part of the job requirements. So We need to do this. No one wants to do it now. So the sense I'm getting is we kind of maybe put a pin in this in terms of, like, not necessarily, you know, acquiring it in the spec right now. But I'm getting I'm hearing that we not no no one wants to ship this until we've we're actually figure this out. So maybe what we need is implementation experience from this and then we can revisit this conversation. Does that make sense? While we agreed on a few things, that's nice. Alright. I'll type that up in the issue. Any What's on the next slide, Bernard? to wrap up. So this is the last slide that we rolled with. Alright. Any other topics around h three, your questions for Victor. Oh, the answer is probably no, Ozone. Alright. We do have 13 minutes left. Any other business that folks would like to discuss? I see our friendly AD. Murray, area director enthusiast, I guess. Just not -- No. You're an area director. I'm an area director enthusiast. Okay. Alright. Just a quick thing. I heard much earlier that"
  },
  {
    "startTime": "01:48:00",
    "text": "both of the tracks of stuff you're working on are getting close to last call or there wasn't much left largely to do. Just reminding you, Francesca is out for still another month So if you pubrec anything, please email me because I won't I don't see her pubrex. Gotcha. I I would dream to live in a world where we send something your way in less than a month, but I don't I'm not too worried. If you don't, that's totally fine. Awesome. Alrighty. And that Bernard, anything, or should we wrap up? Yeah. I I think we're ready to wrap. Although, if we have action items for us, David, maybe we should discuss what they might be. Is there anything that we need to do, like, Consensus calls or other follow-up, for the notes I'm I think I mean, I'm gonna I've been typing up what we get agreement on in the room and the issues. think some of these were pretty minor, so we don't need to, like, do formal con like, confirmations on the list for anything. Yeah. I think we're I'm not seeing anything. Okay. Yeah. Go for it, Allan. Allen from Dell. So I I I I I see the we made lots of good progress in closing lots of stuff. And though we're not in the world where we're gonna send to publish question in the next. month, do we have a sense of what what how it's gonna play out from here over the coming time period. Sure. That's -- What have you been done? Yeah. I wanna go home. I'm hungry. Yeah. And that's So I think in terms of what we need, a big I think for various things, the the blocker is implementation. We don't wanna ship specs that aren't implemented and deployed. So, like, take page 3, we have dependency on the reliable stream reset drafting quick, which is making good progress."
  },
  {
    "startTime": "01:50:00",
    "text": "Thanks, Martin Kazuko. And that needs implementation. The h two document doesn't have any dependencies, but it that needs way more implementation and then in the h three. So one of the things Did we ever figure out what we wanted to do about the server associated bidirectional streams in h 3. I thought of kinda remembered we needed to write a document in HTTP based, and then no one actually did that. Is that gonna, like, pop out of the woodwork and then delay us by a year. anyone care? Should I go talk to the it Does HTTP based chairs. Is that a Tommy in the back? Alright. I think I'll take an action item there that chairs, talk to the other chairs, but that was, like, a year ago, and I don't remember things far back, so I'll do that. But, anyway, yeah, we're making good progress. I think what we need is much more our implementation and then wrapping up NMCs, and we're but we're already getting getting closer to done and There's also similarly, in the WVC, things are progressing quite well as well. now. So -- Yeah. David, is it correct to say that we've that 07 is the last draft that we believe will have breaking changes? But, you know, if we say that, we're gonna it. I I don't want. Okay. I'm just wondering if there's if at some point, we're gonna declare an inter testing event or something like that. I don't think we're quite near this. No one. Should we have Interim be scheduling it around testing event either Are we fine waiting until prod? Do we wanna set up another time target in interim to try to accelerate progress, keep people honest, because I only work on what transport in the week before I That sounds like a good idea. thoughts from other implementers? Is there any time in the, I don't know, September, October, over time frame. That sounds like a fun time to"
  },
  {
    "startTime": "01:52:01",
    "text": "type some web transport code. Alright. I get a yawn from the audience. Alright. Let's take it to the list and discuss there. Alright. Thanks. everyone for coming and for drinks. Thanks in particular, Needy, for taking minutes. much appreciated, and thanks again for a pleasant meeting. see you all on the list and on GitHub. And We'll see you in Prague."
  }
]
