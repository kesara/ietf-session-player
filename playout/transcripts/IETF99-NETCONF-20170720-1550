[
  {
    "startTime": "00:00:06",
    "text": "Nakia earlier this year and he announced that he will be stepping down as chair of net Khan working group after Prague so I wanted to take this opportunity to thank mammoth thank you for as many years of service thank you very much at IETF ambition of course all the best in his retirement rather than give a long speech I put together a slideshow of pictures that were taken by him and his friends on his many trips to the many different ideas locations but before I run that slideshow I wanted to announce that today after bits and bytes we are getting together for a farewell beer bash for mammoth it\u0027s a surprise for him yes so if you would like to join us in wishing him in his retirement please meet in the lobby at 9:15 we are going to head to a place called locale Hamburg it\u0027s about 10 minutes from here and please make sure you bring your wife along so she can meet all your merry friends after the slideshow bird who was co-chair before me you would like to say a few words so let me just quickly run the slideshow shall I say the words during the slideshow cause this feels like a funeral to me that little grad so Mehmet thank you very much you know I\u0027ve also been co-chair with you and it was great to work with you and you know normally I do a Dutch song but I looked up the words for an English song so that you know it\u0027s not that you can understand what I\u0027m saying but also as a matter of fact so that people can participate and it goes like this I need to read the words for he\u0027s a jolly good fellow for he\u0027s a jolly good fellow for he\u0027s a jolly good fellow that\u0027s all say all of us that\u0027s it "
  },
  {
    "startTime": "00:03:06",
    "text": "that\u0027s and so say all of us and so say all of us for he\u0027s a jolly good fellow for he\u0027s a jolly good fellow for he\u0027s a jolly good say all of us and so say all of us thank you so thank you very much this is indeed a touching moment to leave net converting group after nine and a half years I was doing a lot of wrong and bad things it seems I was unsuccessful I appreciate very much your words it was indeed an honor to serve for net converting group and the ops area I am indeed very happy to have worked together with Netcom folks and especially the authors of the drafts who indeed spent a lot of time very engage in many discussions and helped to publish those documents I especially would like to thank Bert Wyden and dan hromas cano for their guidance and mentorship from the beginning it was indeed a real easy alived having birth as a co-chair so I was just following what he was saying it was indeed a really good start and help from him but I also would like to thank to Boehner for his support all the years and Mahesh in the last two or two-and-a-half years so it is a good time thank you very much to all for your support and getting net Kampf documents done thanks and that we bring the meeting to order okey doke so this is the net conf status slide set can you go back there is a reason why I can is here in parenthesis I will come to this in a second we need a JavaScript and and many takers is apat is actually for everybody please enter your notes there from this session but we have as far as I understand Ignace and maybe Bala I don\u0027t know as main minute takers this would be helpful any JavaScript please okay thanks nada so with this please enter in is a pet whatever you think it should be put into "
  },
  {
    "startTime": "00:06:06",
    "text": "the minutes the link is on the slide and you can find it from the net cunt working group status page also okay this is the new note well the the co-chairs are keen of copying the newest not well into the start of slides any submission to ITF intended by the contributor for publication as all or part of an IETF draft any statement made within the context of IETF is considered as IETF contribution please be aware that all contributions are subject to the rules of RFC 53 78 and the new RFC 81 79 which I didn\u0027t yet read yet but you need to take care of it go ahead okay this is the one slide status since Chicago the working group has been rejected after some long ish discussion but also IES she needed a longer time to approve it now it is available please look at it on the link last call for RFC 6531 alized we will now go to the next step 0 touch and keystore drafts were in workroom class call we will check the status today secure shell TLS and net compress compliant server models are in workgroup last call we might discuss one or the other issue today yang subscription push drafts which are supposed to be ready for working class we will check this Martin raise some issues on the mails but Alex and others addressed them mostly I assume there is one remaining this will be discussed so I am stepping down and now the introduction also which has been made on the main list but officially in the session Kent Watson will be the new co-chair starting after IETF night planned meeting please welcome him together with me okay the agenda you have seen we have a package for items which were or are in vertical glass call go ahead charted items ready to go to work last call this our main leader yang push drafts go ahead we have drafts fitting the Charter they are subject to adopt we need to check whether there is already substantial amount of content available before we can adopt go ahead non chartered items as it is important for net conf and EMA data store architecture it\u0027ll be shortly "
  },
  {
    "startTime": "00:09:06",
    "text": "presented here it has been already presented and discussed in net mode sessions what we need to go through it if somebody has a question it can be asked and we can address those questions you didn\u0027t tim carry any okay if I could ask a quick question on on the status of those elements that are of those drafts that are in the working group last call now is there any chance of any of those drafts being published this year just trying to figure out what what the next half the year is gonna look like in terms of publications it is actually the aim to publish if the vertical glass code is successful if there are issues we need to solve and and if they are substantial and take more time to solve it can be delayed but the aim is to publish as soon as possible okay so these four here that we\u0027re talking about there\u0027s we\u0027re gonna try to get those out this year is that accurate yes the yank push draft set is also in the queue for last call this will be most likely if the issues are solved on time after this meeting in a month time or so yeah let\u0027s see how it goes is there a particular set of traps that yeah the client/server modules in the earth are the are some of the big ones that we were we were trying to track a bit closer so we were just trying to figure out what the next six months Michael I mean the last call is going as far as I know after this meeting still one week time and and yeah please raise the issues during the last call if possible provide a way forward if the issue is accepted and yeah it needs to be solved before publication yeah okay I figured that much but not less so Tim don\u0027t go away so this is like a BBF question sure no no I mean that makes a big difference so okay there is like my personal opinion that if we don\u0027t publish this in six months it would be like a real concern right but now whenever we\u0027ve been dealing with different as do sometimes you have deadlines because they\u0027re meeting at cetera and I would like to know about those I can\u0027t remember these are part of the BBF comma module piece to be honest with your they\u0027re just a Nokia track I just like I said I get old I\u0027m next in line for the retirement package okay there are three go ahead again despite there are three documents "
  },
  {
    "startTime": "00:12:06",
    "text": "accounting Netcom proxy and udp-based publication channel for streaming I hope it will time for them I assume they will be good enough to let them present that\u0027s it so the first presentation is from Kent I think good afternoon working group so this is one presentation for the collection of drafts that are all in last call at the moment so there\u0027s one slide per drafts so if you have any questions or comments on you know draft make sure you ask it on the slide that it\u0027s showing on screen so zero touch is the last call actually completed there were four comments received potentially one technical change and actually I view this is a sort of soft technical change because it doesn\u0027t really change the the the the content of the data significantly or at all really it\u0027s just a matter of structuring of the gang module itself but the issue regards whether or not the top level choice statement is okay in a yang data structure I think a couple of tools piaying and and Andy\u0027s tools flagged as an issue though technically the document being data structure says that it\u0027s okay so long as the top-level node always resolves to a container which it does so I don\u0027t know it is actually currently on unless being discussed but if we want to discuss in the room right now anyone have any comments on this issue ok so again there is a thread on the list right now and I did actually just poke it I think a couple a few days ago so I could pick up on that that\u0027d be great next is the keystore draft again last call completed three comments were received potentially for technical changes are looming I mean so so there are also editorial changes but I mean I\u0027m just calling out the technical changes here because these are the ones that would potentially cause a need for another last call so first is defined a separate module for some algorithms for the algorithm I dented identifiers so currently the acute or drops is has you know base identifiers and then I don\u0027t know about ten sub identifies for various public key algorithms you\u0027re gonna had a comment about potentially moving those out into another gang module I don\u0027t know what we call it an algorithm crypto algorithm identifier module it be nothing but a bunch of identifiers so we could but my concern is that if we were then we\u0027d probably be tempted to you know not just to public identifiers would want to do symmetric key identifiers and other kinds of identifiers and it couldn\u0027t become it "
  },
  {
    "startTime": "00:15:09",
    "text": "could grow right so that\u0027s that\u0027s the concern but I understand it might be the right thing to do I don\u0027t I don\u0027t think Juergen is gonna be in this session so but but maybe someone else has a comment on this all right so this particular item is actually is I replied to uragan comments yesterday last night and so hopefully we\u0027ll pick up on it there next is how should various lists of lists be named I don\u0027t really think we\u0027re gonna be able to discuss this in the room right now there\u0027s a few ideas floating around but just be aware that there\u0027s some strangeness in the naming of the various containers in the draft and again not sort of soft technical change because it won\u0027t dramatically change the structure of the data next is how to normalize the data fields in various lists a similar comment to previous is soft technical change next what type should be used for host keys actually okay so the draft currently says it so something called a one asymmetric key but you know ultimately it\u0027s an ASN that one structure which is kind of uh normal for SSH implementations which are typically textual base this once resolved already what I did actually if I realize the ITF system module also defines SSH house key the same way so I just copied and pasted how it does its definition into the reporting replace to the one asymmetric key here okay so next draft the reducers yes so just to understand next steps for the two drafts which head already workgroup last call so you mean zero-touch is mostly done with an update it can be verified on the mailing list and can go next step yes okay and keystore needs some changes defining a separate moment is kind of big the first poll points the largest and and as I understood you already suggested to have another last call after these changes are prepared or provided depending on the okay it\u0027s depending what we do the first bullet point if we decide to keep the identifier in the current draft then obviously nothing no change there the remaining three changes I all said they were kind of soft technical changes so yes they\u0027re technical changes but they\u0027re not dramatically changing the nature of the data if you have a separate module and rearrange the whole draw yes that\u0027s the first item if we do that a new last call would be needed for talking so go ahead okay okay so the SSH client server draft is currently in last call as are all the other client server drafts so far I think it\u0027s what a week and a half and so far there\u0027s been zero comments posted a little bit concerning but that\u0027s the current state same with "
  },
  {
    "startTime": "00:18:13",
    "text": "TLS client-server 0 comments have been received so far for the Netcom client server draft we\u0027ve had one comment received so far which potentially has one technical change in it actually it\u0027s not a technical change it\u0027s an editorial thing I think but the design considerations section should we remove it altogether or move it to the appendix or fix it currently that design consideration section talks about the motivation for you know for instance why do we do call home and why do we support multiple call home endpoints and you know the justification for all that that whole justification is as it applies to net kampf it this draft it also applies to the restaurant client server drafts so if we keep it then I think we\u0027d want the identical design considerations section to also show up in the restaurant client server draft which then seems redundant but you know I don\u0027t I wouldn\u0027t necessarily recommend moving it out into third draft but perhaps we should any comments on this okay so this actually is being discussed unless this is Tom patches comment so I replied to him already last night and on the rest come client-server draft so far no comments have been received so far and that is my status for this all the straps now the last four are still in last call so we need to wait until deadline and prepare an update if necessary for those which need an update and we will see how decide without me now you\u0027re asking for concerns that concerns me there is not much comment right I keep hearing all the time that need to be rich faster but we don\u0027t provide comments the last call so maybe the thing is that we\u0027ve got a set of documents in last call right now and we have a next set that will be in last call next right so some how can we just stop thinking about our own documents and the people who want to have last call sooner that provides feedback for this set there and and we\u0027re going to help the process here otherwise I mean zero comment what is it mean right so it seemed that we\u0027ve got a discrepancy we want to British fast but we don\u0027t provide feedback if you read it it\u0027s fine just tell it it\u0027s fine so can I have a show of hands of instead of saying all the documents a show of hands on anyone has read any one of these set of documents Oh fairly decent set so I think it would be helpful to indicate that because I think and even if it is saying plus one or the "
  },
  {
    "startTime": "00:21:15",
    "text": "plus one is not the best of opinions but saying that you\u0027ve read it and you have no concerns even that is helpful to the authors so would encourage you guys to on the mailing list the last call is still open say yes you have read the draft and you you believe that the document is either either you have no opinion or you have some opinion okay thank you thanks Alex good afternoon everyone so this visit concerns the yang push the related draft so you see busy the the four graphs which are yeah which are in this package basically here so it\u0027s push updates the subscribing subscription for notifications and then the net con for respective the rest convent HTTP support for the for the transport so one thing perhaps to mention upon decay there was there was a hackathon earlier this weekend busy where which basically showed actually interoperability a yang for Hughes case security use case concerning basically yeah a security gateway for use case discussing the second working group involving different vendors yeah different vendors implementations of this so I just wanted to mention is there\u0027s a there\u0027s a link here to the where you can see busy the details of that in terms of the drafts themselves we\u0027ve had just a few updates actually most of its rank push itself there was one outstanding issue regarding filters concerning busy whether or not it\u0027s a note selector with a match on values and so forth so this video has been resolved clarified basically it\u0027s a note selectors simpler semantics and and the permutation is basically the the concern that was that was essentially addressed to that security considerations have been updated we have also a clear a distinction between a data source of action and the stream subscription that was a little bit books not clearly enough delineated earlier now basically it\u0027s pretty much done but there are a few items which are next which are basically next which are which are this it up here one minor updates will be updated within within a week and the main thing basically what also made mentioned earlier there were speech was "
  },
  {
    "startTime": "00:24:15",
    "text": "raised to are really basic editorial issues which are being addressed but we have actually where we\u0027re getting some text text updates one concern basically how two concerns the context for X powerful that is being defined the second one is a clarification regarding that we don\u0027t want to have replay capability when you have the data store push the third aspect and this is busy the third basically the the the open minor issue but it is a it is a major concerns where they how the stream is designated currently basically this is done to identity and Martin suggestion was why don\u0027t we use string for that so there\u0027s a trade-off between these be the reason basically why identity is chosen right now it\u0027s all that you have a set of the busy each stream has an identity they have a well-defined semantics it is part of the model and defined as part of that you can also if you want to add additional types of streams that you could subscribe to you can add additional identities you can also have build your hierarchy obviously if you have a classification sub categorization of that and if you want to have an ad hoc stream and I think this is busy where Martin\u0027s concerned and essentially come from then you can have something using a custom identity so that\u0027s one stream that is custom and and that would basically any other stream and if it is so desired to actually also distinguish between different custom streams and one could do so or one could potentially also bezzie added conditional custom stream name or something along those lines anyway so that is basically well this believe what we have there on the pro string side is busy just the easy you can be ad hoc to introduce new new string names and you\u0027re not tied to busy having these predefined in the models and there\u0027s a busy one thing where it would be good to get feedback from here and Barash is ready to get you on your lyrics on I am actually put speaking about the other topic replay we have been using a known very similar solution we don\u0027t change updates tones used for configuration mirroring with replay so I am upset don\u0027t really want to take as replay for I agree that for periodic subscription it doesn\u0027t make much sense but for own change notification unchanged push updates I think it is meaningful okay thank you Jason Stern I had a comment about the stream names I kind of like the string idea in a way and I was just thinking like back in I think it\u0027s 50 to 77 I think it was just a string and they just defined a name that was a standard name so we could do a string and reserve some name I guess reserve some names for the standard ones you want to have well-defined semantics in the spec it\u0027s one option well yeah I think that that is busy but wouldn\u0027t be required to have bunch of yeah well defined names yeah "
  },
  {
    "startTime": "00:27:16",
    "text": "are there any other comments yes just one Eric go ahead Eric I guess he left Oh Erika\u0027s online and he was I mean right is this the only issue remaining Alex that I\u0027m aware of yes okay so as the go ahead please stay tuned yeah I\u0027m going Jim from Hawaii Alex I think as a issue which is cast that just no we should confirm each before last call for the go home parameters and for how to state the unchanging and you know how to define the unchanged model for Zeta to issue so what was the second issue sorry the first the first one is basically call home buddy that you save it for the configured subscription maybe when you define the transport clarifying that busy you would use column to establish this subscription air and if you use call home basically to establish the connection is this what you\u0027re referring to yes ii 84 which which except to support as their own changes of solution you know just not with this colorful unchanging you know now is defined that one by one for a duration young and you know for that just now we discussed whether can you use metadata I think I can confirm yeah but I think is a should make clear okay thank you so before a lotta you go on yes I have one message from second okay so I do see it\u0027s a message for you about some pressing big red button only once I turn it okay on that goes otherwise you will accept and really the floor at the same time oh he was just there again military I thought I pressed it once only yeah I guess he it yeah so you might wanna try coming back again or just send the question on jabber alright so so why don\u0027t we in the meantime let\u0027s let\u0027s move to the well to the other drafts so requests here is discussed this may be one quick question to give Eric time to to join but in what case picking our Strang to quickly quickly check is this one an NDA compliant actually no it is not so there it so that is yeah so so busy this it is "
  },
  {
    "startTime": "00:30:18",
    "text": "not an it is not an NDA compliant the question is basically whether an MVA compliance would be needed here because we don\u0027t have interplay between between things which are configured which are non configure tense of every nominee have to have those dependencies so there is basically so we do have currently basically state that if that does define missing which curve which subscriptions are in effect versus beta subscriptions which happen which have been configured so tool to follow the nmda guidelines that would actually require updates okay thank you so in MDA compliance is always needed if you are using something from an MDA you need to be complying well but the the thing is okay you can use it also you know can you you can use it with the nmda architecture but basically the guidelines that you should collapse the that you should collapse a bottle and that you therefore basically optimize the model because you save some mods which you would you save some objects which are no strictly speaking no longer needed that is basically the valve unless we are near to last call yang doctors review will be started and they will take care of an MVA compliance okay Bob Wilson Cisco so just quick comments on that I think you could just name your container without a - - state so I still have a state only container but just remove the - state at the end of it because they knew you had configuration in the future you can change things being conflict true you don\u0027t have conflict true stuff under contain it\u0027s got a - stay to the end so just rename the containing problem okay thanks Lada so again relaying Erik white on Jabbar parabolas previous comment on the replay I am replying to his question on that conf mailing list presumably basically real I should have a notification ID so there is not close duplication in replied items and my opinion is that a new RPC should be added as part of a new draft in conjunction with not notifications to let God answer your question so in so basically angle Ericsson we used notification based on time so when and I don\u0027t know when we say new RPC will that go into these set of drafts or a complete new RFC later there isn\u0027t there\u0027s a new draft that this will be presented actually later in this which concerning notification bundles and that one actually does have the facilities for your for instance link to where you "
  },
  {
    "startTime": "00:33:19",
    "text": "can point actually to the previous message so you can find some detect and so forth if there if there are any updates that you missed and so for picture which should address those concerns but they are not in this draft right is this busy the other this is the notifications to draft I think not but still replay can be done based on time stamps not just on message ID or a notification ID I just want to state that we would okay we need a solution where you can replay at least on change notifications okay alright so there is a result so there is an update anyway and after this meeting once the update is available Mahajan Kent will decide whether it can go to okay I mean based on the available issues and how substantial they are the decision needs to be taking all right okay so let me get get to the other three drafts actually there are hopefully less issues and actually they update their happy to update as well and but the updates have been more minor minor nature\u0027s basically it so the the Netcom subscribe notifications this had essentially a number of text weeks clarification text again regarding we play we etc and the transport routes have just general general general cleanups that we have here actually this is the on the last item this is information not quite correct we had actually conversation before that being regarding the whole working hard working group last call we would actually request to go to working group last call because this issue is actually not specific to the notification drafts is more general issues regarding t RPC an HTTP compatibility happening having where the question was but he can we have G RPC kinds which are automatically compatible with with rest conf and there\u0027s a there\u0027s the issue and there\u0027s an issue concerning the support of the get operation and G are physically but it\u0027s somewhat orthogonal for those lights it sorry there was an update for the slide set where this last pallet has been removed yes okay I mean the dependency with G RPC is not between gr PC and Netcom push drafts it might be a depends between GRC Andres conf but I or the co-chairs would like to hear more on this issue and discuss whether this issue is relevant for us or not so I don\u0027t know very much the details of such a dependence between G RPC and rascal at the moment we would like to hear more on this at some point I mean whoever feels responsible should bring in a few slides or bring it to the main list okay thanks "
  },
  {
    "startTime": "00:36:23",
    "text": "so that\u0027s it please provide an update for the drafts you are mentioning and Netcom co-chairs will guide and we will see whether a last call can be started soon so next is Rob no still Alex on the bundles we seem to be on time good so this is the other draft that that was just mentioned so this basically a new draft but part of the earlier package did they follow on work concerning concerning is any providing the ability to bundle multiple notifications into a single notification having become defined in common headers for that and you\u0027ve seen earlier busy another revision or another version of this of this diagram that shows how the different drafts relate basically with their with the with the transports and etc on the bottom and then be the subscription and the push updates on the on the top so essentially there this this essentially would sit right above the notification is so right above the the transfer updates because this defines basically the message headers they would be food in the in those updates so the purpose is to be able to bundle multiple updates and notification records into a single notification message and essentially define the header parameters and so forth that would go into that and we distinguish between what would go into the record and what would go into the overall button so basically it\u0027s about distinguishing the the record contents from the wrapper that the record is basically ending and busy having the refere be able to bundle multiple notification messages essentially this well at the corpus is the yang model and Betty this defines basically the notification messages first for predict Li a single notification message but we have basic message header with the field that you see here record time type subscription identifier and so forth one item perhaps to point out here is the is the object on the message generator idea it\u0027s a second from the bottom and this basically is to deal with the possibility of distributing subscriptions where you can have different notification or streams that come for that originate from what appointed from different line cards that you want to distinguish where they are from here as touch points with another draft that general and Walker are going to present her later on that he goes action born to monitor those issues anyway so that\u0027s that\u0027s basically the notification message containing a single record and then there is the bundle but you see here so there you would have a bundle notification message but you have busy various information that is valid "
  },
  {
    "startTime": "00:39:24",
    "text": "for the for all records set which are in a bundle from the individual records which wasn\u0027t busy be contained underneath that and that\u0027s okay so next steps to that is to you know basically to reveal revise the data data model there are a few fields that which may be a little bit over specified so we will take a look at that and we want to also basically align this with the concept of having multiple message generators for subscriptions of this is something which is currently alluded to within this so that you can have one subscription you have multiple message streams originally originating from different line cards it seemed like a somewhat orthogonal so maybe this is something that will be pulled out potentially also combined with the we had some conversations with Cameron and Walker regarding the other graphic might actually put it better there so this very some of the ongoing discussions if we should pull it out but anyway so this is where we are that\u0027s that\u0027s what I happen the crush is also busy if the working group is interested in this work alright so let me ask that question so you before I do that tree you believe that the document is ready for work Rupert yes so that\u0027s what we want to request yes okay all right so with the show of hands please indicate whether you believe that that the working group should take up this particular document okay a fairly decent show of hands so I think we will then issue a call on the mailing list or a group adoption Thanks okay next is Rob giving a summary is a bridge between NDMA and Netcom rafts so I\u0027ll try and not bore you to death with this because I mean an MBA you\u0027ve heard a lot about it so I\u0027m in fact this is very quick intro to three draft you about to hear from from Kent and feel presenting them so to tie this together so this will just remind what N\u0026B a is the key points are the operation of state datastore and splitting to the operators are asking device to do this what it\u0027s actually doing and if you look at the implications for that we know the data models are changing has been lots of discussions this week about that and how to modify the data forms to be an NDA compatible we\u0027re talking about potentially deprecating net confident get but not at this stage over time but there needs to be new net confrim risk on four dishes to support operational data stores and this is the picture that I\u0027ve shown in net modern is in the in the draft and you\u0027ll see obviously what we\u0027ve now got here that\u0027s different from what was there before is these new data "
  },
  {
    "startTime": "00:42:24",
    "text": "stores but the intended datastore we\u0027ve got the operational data store and we potentially got dynamic data stores as well I\u0027m sort of a ITRs so we need to add a protocol extensions to net conf the audience handle States was fairly cleanly so there\u0027s extensions there rest Gulf has to have similar extensions to that that\u0027s a bit trickier because they have a sort combined view today so can\u0027t explain how I plan to handle that but the key point I really want to pull out here is that the scope of both of these drafts is trying to do minimal changes and extensions to Netcom from restaurant to support nmda so we\u0027re not trying to go any further we\u0027re trying to get these documents just done as quickly as these pragmatic in in this organization and finally it\u0027s worth mentioning that your library is also going to be updated that\u0027s used by both protocols and yeah and this the solution there effectively works in the same way for both of them so that\u0027s what I had to say I think questions are probably best and directed both to Kent and fellow appropriate thank you no ok so as promised the update to yang library or the proposed up at the yang library so the motivation for wanting to do this is first that existing RFC is don\u0027t provide an ability to suppress all that\u0027s needed RFC 7950 which is yang 1.1 and 80 40 which is rest huh say that all net conf and restaurant servers must support 78 95 which is the yang module library sorry yes this is a question to graph outer and net mod co-chair is it allowed that net comp draft changes or updates yang language RFC shouldn\u0027t it be decided in net not working group ok yes good point I\u0027m not sure exactly how we should move that forward but yes yes and and to be clear the update to 7950 isn\u0027t so much to the yang language itself for the syntax the yang it\u0027s more having to do with the statement that it says yeah you know because 7950 describes the xml encoding then for knockoff and it\u0027s really regarding the statement that it says that the Netcom servers must support gang libraries so that\u0027s that it\u0027s not really - yang language itself it\u0027s just to that statement about the conformance for net comp servers if you actually I don\u0027t mean if you want to pop a stack there\u0027s an even a bigger question about you know the gang next and wanting to factor those things out of yang 1.1 and and you "
  },
  {
    "startTime": "00:45:26",
    "text": "know moving those conformance type statements into some net controlled working group document but at the moment you\u0027re right it is it is a net mod working group document another comment um yeah a Behrman we already have capability urs for candidate writable running and startup so how does this proposal interact with them is if I advertise candidate am I allowed to put something in here that says some configure holes true node is is not supported in candidate no but maybe the potential collision or confusion or any value at all in listing modules that are only used in the conventional data stores so this this might be a better question answer on the next slide but quickly the the one issue might be if you connect to a hello to an Netcom server and it sends a hello and it says it supports the Canada Day store but then you go to the yang library this will the pros proposing library and it says the Canada Day store is not there right that inconsistency but I view that more as a bug on a server that it would even provide that inconsistent data view another coming not a much Kaimuki addressing issues it was raised by by make meant my personal view is that we should antagonize too much over it because so first of all process wise I believe that it\u0027s no rule that says that an RFC as it was worked by one working group cannot be updated but an RFC that was other working group and for all practical purposes is the same people in two different rooms and actually you can just run the last call copies a net most working group and we are fine so that\u0027s my opinion thank you for that balázs Daniel Erickson it is strange that some data stores will be advertised as capabilities well I don\u0027t know if we in intend to have a capability for intended or operational also what does it mean that let\u0027s say I support intended if I just copy over in the running to intended does the is that support already so please make some statement what does it mean to support such a datastore sure okay so first off it again it\u0027s not an exercise a tree diagram but but the proposal is that there\u0027s a list of data stores in each data store has a list of properties and those properties are closely aligned to capabilities so you\u0027ll find property names like writable running and validate and whatnot so so there is this ability also to provide that the comparable support it\u0027s not for Netcom for abundant but for a restaurant that\u0027s necessary there\u0027s no other way to describe these things second thing regarding the support what does it mean I think in part the definition of the intended datastore is defined inside the revised datastore draft so it\u0027s really to be taken up there but also I mean it\u0027s what it means is that there exists a data "
  },
  {
    "startTime": "00:48:27",
    "text": "store a resource that you can get views from it\u0027s read-only resource so it\u0027s a set interesting question yes okay let me continue again with the motivation we\u0027re still on a second bullet point here so these two RFC\u0027s say that 78 95 must be supported regardless if they support nmda or not and so we want to leverage this requirement so that you know this this yang module library 78 95 is already it\u0027s got the hook it\u0027s the must thing that all servers implement but then if you look to 1795 what does it say it says that there\u0027s a mandatory to implement module state tree that a server uses to advertise all the modules it supports this module state tree assumes that all modules are in all data stores there\u0027s no ability to differentiate which modules are where so there\u0027s not actually the case with nmda right some modules may only appear in the operational data store so if you think about foreign since the neck network topo modeled in the ITRs working group where they talk about underlays and overlays it could be that a server only supports presenting the underlays I know the the topology it\u0027s discovered from the network without any ability at all to configure the overlays so here\u0027s an example or a module would only appear or a server might only support it in the operational datastore similarly some modules may only appear in the dynamic datastore if you think about for instance the in iiterest a femoral rib where it\u0027s never expected that it would actually be configured and anywhere else another example of some modules may only appear in running when a server hasn\u0027t yet been coded to support the op state yet night and actually have a slide that touches on this more but it\u0027s sort of a migration a graceful migration phasing strategy that we\u0027re suggesting and lastly there may be variations in features and deviations between these data stores so you know in the running or a conventional data stores traditional features and deviations are there but then if you were to look for instance to a dynamic or operational there actually might be a variation in the features and deep-end deviations that are being in effect at that look at that that data store okay so summary of changes to 7890 pi that are being proposed so first off it\u0027s renaming the document title from gang module library to just yang library this is because the tree diagram is no longer just about modules it also is about data stores and furthermore we want to sort of it\u0027s a hedging bet to in case there\u0027s any future extensions that are desired being a server metadata info this is now providing a place where that a future metadata could be augmented in it deprecates the module state tree because that particular tree assumes all modules are defined in all data stores and we have a comment to the mic please I\u0027m not sure that this change going from yang module library to yang library only "
  },
  {
    "startTime": "00:51:28",
    "text": "is really useful because I believe the semantics is slightly different and this can even more become confusing with the normal term that we use the libraries usually are some software libraries that use we have already libraries that are called yang lip and labia and and so on so perhaps III personally don\u0027t like this term library for this purpose very much if native speakers could come up with something that\u0027s we already have Yin catalog unfortunately used for other purpose so and I I don\u0027t know this yang module library really means that it\u0027s a library of yang mode use Faris yang library I don\u0027t know it I I\u0027m not in favor of doing this change exactly although of course in many cases it\u0027s only used as yang library without the module number two so it\u0027s not the best name but in defense I think everyone refers to this yang library and no one actually probably even knows it\u0027s called the yang immortal library another comment that was gonna be one comment it\u0027s it\u0027s everyone in common parlance calls it yang library there\u0027s really we\u0027re actually helping people because when they go to when they want to find out what yang library is there actually a document called yang library instead of finding a document called yang module librarian wondering where the definition of yang Yang library is I understand your confusion you know I would the developer would look for Lybian a if you said yang library so I understand the confusion but it\u0027s just commonly use my experience is that any person that just comes mu to - yang and read some stuff about that becomes confused especially they are software developers they always ask people what kind of library is this and so okay so this one\u0027s easy to take to the list and resolve there the last sorry okay sorry okay another comment coming anywhere we sold the original module state that\u0027s completely cut and pasted and cut and started over actually says nothing about data stores your additional data structures that point into this say something about datastore so the the idea that we need to deprecate the original thing because it it it doesn\u0027t support this new stuff thats added years laters seems rather extrange to me okay that\u0027s a fair point I can just make your leaf riffs point it the thing that\u0027s already there instead of the cut and paste that\u0027s a fair point um so we did it we kind of went back and forth but in the end we felt like the simplest easiest thing would be to use the existing grouping which is why it does look like an exact copy paste and since we did in the end your we could "
  },
  {
    "startTime": "00:54:28",
    "text": "actually just not deprecated module State and point to it and of course module state has the - Dayton in it to be in MVA compliant we might want to take this opportunity to back out of it but I I understand what you\u0027re saying Robertson Cisco I thought those tear eases we did this one was because at the moment module state redefines that all modules defined in all data stores and we\u0027re changing that doesn\u0027t it okay the the other point was we wanted to actually put an extra layer into the tree so that when you\u0027re accessing this information from a client you could all get it in one request and I think of accessing this existing module state tree that would be more a hassle so certainly the second comment is true going back to the first comment you\u0027re right it doesn\u0027t actually say that but the semantics that are understood when people go to module state if a legacy client were to go and and you know ask for the yang library it would get module state and think that all these models are implemented in all data stores so perhaps by even though it\u0027s the same it\u0027s still a list of modules it\u0027s in a different location and that location doesn\u0027t have the semantics there\u0027s there\u0027s Lexie clients can\u0027t reason it to mean anything other or you know in in the old legacy way so we can take that to the list updates to 7950 and 84-84 t these are actually they are they\u0027re updating these draft in the same way in both cases there\u0027s a section in those drafts where they\u0027re claiming or stating that yang library I\u0027m sorry we\u0027re module state module must be referenced and we just want to change that to instead point to the new path yang library modules module so here\u0027s the proposed treated Graham and um you know again underneath the module list is actually the it\u0027s just using the grouping from the existing yang library so that block of you know descendant nodes should look very familiar to so there is a list of modules and then there\u0027s a list of what we call module sets so each module set is having an ID so this is what restaurant sort of refers to the module set ID or it could be used as that for that purpose and then the module set is pointing to a list of modules so what are the modules inside this module set and then and that\u0027s in there being referenced by ID not not by the two tuple of name and revisions so this actually is a little bit cleaner and then lastly there\u0027s a list of data stores which is pointing and each data store is pointing to which module sets it supports I know that it says actually it\u0027s off or not it\u0027s not mandatory true but it should be mandatory true in addition to which module set that data store is pointing to there\u0027s also a list of properties so this is where we were saying before about how some prep like running datastore you could say it\u0027s writable running it would have that property you see there\u0027s anything else I want to say here yes the last point is that all the conventional data stores would be expected to point to the same module set so you know there\u0027d be datastore entries "
  },
  {
    "startTime": "00:57:30",
    "text": "for running and intended and and Canada startup but all of them would are expected to be pointing to exact same module sets so there\u0027s no inconsistency between the modules that are being supported and specifically with regards to the very features and deviations right okay flex provided so with this servers can state how they support models per datastore NMDA implementations can phase in operational support on a module by module basis I think this is very important as servers go to implement NMDA it would be massive if they suddenly had all modules you know with the applied view you know instrumenting all that back in logic to provide to provide the operational view of all the values but being able to do it on a module module basis have sort of allows for graceful migration and just so we hear about the config false nodes don\u0027t count because config false nodes would always show up in operational so when you say that you\u0027re supporting a module in for instance only in running it doesn\u0027t detract from the fact that config false notes still show up in operational last well point to the nmda implementations may only support the operational view for specific modules server only supports providing oh so we talked about this one before some more flexibility in enables deviations and features per datastore we talked about that as well and of course all conventional data stores would point to the same module set hamster be no inconsistency between them and that was my appreciation or comes so you want to be able to say that\u0027s that medium and even so the foo node is type in 32 in the running game store and in its type string in the candidate datastore and it\u0027s type decimal 64 in in the operational datastore that\u0027s why you want to put deviations per datastore to allow the implementation to have a different data type and in whatever in each data store that seems rather I don\u0027t think it\u0027s specific data type from it\u0027s not a feature in an example but maybe it could find of this is it was actually pushing for this sure because that\u0027s not what the deviations are for deviations are for the for the implementation say I\u0027m breaking the rules I\u0027m doing something wrong you know in the past we\u0027ve been able to just say Oh everybody follows the rules deviations are a way to express that ability to say I\u0027m broken or for somebody else to say he\u0027s broken and and and be able to express that in a programmatic way that an application can take advantage of it\u0027s not saying yes we want to break all the rules because that makes a world in which we don\u0027t want to live but we want to live in a world which where we understand which rules have been broken and and and you know the expectation is you go to your vendor and you say here\u0027s the list of rules you "
  },
  {
    "startTime": "01:00:30",
    "text": "broke go fix them vodka I believe it it could be opportune to integrate scheme amount information into this because for one it\u0027s not clear if we have if we have a mount point in in a module that\u0027s in multiple data stores whether the schema mount is supposed to be applied to all data stores or not I don\u0027t know but maybe a second more important point is that I believe this young library information it\u0027s not only some machine-readable data that that it doesn\u0027t matter whether it\u0027s complicated or all simple I believe this is some kind of metadata that mean we might want to use for other purposes we have earlier today we discussed how to how to validate instance data again some data model so this might be useful to specify in some way before you know it was really just a collection of modules because here we didn\u0027t have any other mechanisms to do it more complicated now we have the adjusters and we have schema mount and I think it\u0027s really important that we think about all this and try to come up with something that makes sense that people can really understand and and use Tim Karen okay I do have a question so by changing this and adding the data stores in the libraries you you made a statement that said you can now provide a migration path where you know some can be old you know use the old way some using the new and could you just expound on that just a little bit further because I\u0027m trying to understand what your thoughts behind of how that might be used for migration sure so the this draft says that a server implements nmda if the operational data store can be the resources available or if the this module is present or if you yeah okay blurs will basically the two things it says but what does it mean that the server supports operational does it suddenly mean that all the modules are present and in operational and we don\u0027t think that is I mean necessarily immediate immediately true you would have the the server a code would have to be modified in order to provide that operational view it the back end logic has to be modified in order to actually look to line cards and hardware to collect the applied state to populate that response we just believe "
  },
  {
    "startTime": "01:03:31",
    "text": "it\u0027s going to take time and so for servers that implement you know dozens or hundreds of modules you know they you could imagine them you know prioritizing those over a year or two or three but it wouldn\u0027t be immediately so so this is what provides them an ability to sort of you know present some applied State in operational well you know over time and more more well as Daniel Erickson I thought that the simplest way of migrating would be just if you get to get data for operational you just copied over the running and whenever you have the code for really noticing the difference is then you modify it is that compliant but this is an option that we could take up if we did do that then I think that the response should somehow have a flag or an attribute indicate that that\u0027s what it did what the server did but maybe that\u0027s the that\u0027s what we did that\u0027s why I think clarification of what support really means is the intent is there I think we all agree with the intent how we satisfy that intent is sort of watch that issue so I\u0027m okay with if we want to consider alternatives they\u0027re not clear on a sin top on the same topic sorry and trying to understand what unda mention is that the ITF yang library does not mention it a store right so if it doesn\u0027t then we don\u0027t need to update it meaning that we could just command if we want to refer to data stores a plashy schema right which means that potentially we don\u0027t need to update 79 50 and 80 40 this is this correct in my response to nd what I was saying because imagine you have a server that exists today and then it\u0027s updated to support in MDA but it\u0027s still we\u0027re not deprecated we\u0027re not removing the existing module state tree and the server wants to continue to support legacy clients so I have a mix of legacy clients plus in India where clients the legacy clients would go to the module estate to get to see all the modules supported presumably those that are in running would be present but but but the server would potentially also support more modules may be for instance and dynamic data stores right but but if those new modules that would also show up in module state then the legacy client would also see them and probably assume that they\u0027re configurable through running even though that\u0027s not the case why I assume because that\u0027s how they\u0027re coded today they they they go to look to list of modules and module state and just as there\u0027s no way there\u0027s no indication or a flag to specify that which data stores that are available in the existing module state that\u0027s right so they have to go to the new module there to discover it the legacy client wouldn\u0027t do that that\u0027s right but what I mean they can do that today so I\u0027m "
  },
  {
    "startTime": "01:06:32",
    "text": "I still do that today anyway right I don\u0027t understand the question right Phil Schafer are you sick then why are you saying deprecated in place make this new thing that people go to and just kind of without explicitly deprecating the existing modules hierarchy yes is there a place where we can argument because I thought it didn\u0027t have a root it module states the top-level container and we want to get away from we\u0027d like to have a separate top-level container that we could target to retrieve all the server metadata info in one response okay okay I agree with what you\u0027re saying so a legacy client sees the new module it says config equals true but it\u0027s really I to RS and it\u0027s not going to be writable and in candidate or running and they will try to write it in candidate or running correct unlikely but it\u0027s scenario but yes I guess it\u0027s safer to not confuse the legacy line so we can continue the discussion on the medes or offline also I\u0027m disposed in the I mean it is covered by the Charter it has been asked for to prepare so it is good that we have it just a question do you want to add something before adoption or do you think it is ready to adopt I think it\u0027s ready to adopt of course the once adopted it becomes a working group document and we can edit it and modified any way we need to so there is nothing substantial missing now okay so go ahead Marsh all right so the usual question for adopting show fans for people who believe that this document needs to be adopted all right sizeable size so I guess we will make the call again on the mailing list okay next is Phil on nmda for net Kampf my presentation is short we want to add in my presentation is short we want to add an NMDA support to net comp we do this through tuner operations get data and edit data which are essentially get config and edit config pulled over with a new target leaf that is of that supports the the new data stores get data supports the origin attribute so that you can figure out an operational "
  },
  {
    "startTime": "01:09:33",
    "text": "data where your data comes from it codifies a hierarchical persistence so that for any parent for any child you you can the the a child\u0027s origin attribute defaults to its parent in addition we augment three operations to add a new datastore leaf and that\u0027s it how\u0027s that for easy Jason\u0027s turn here can you go back to your first slide and just something just triggered me on like on the get should that be a source leaf or a target for the get I believe it\u0027s called target in the existing today forget config we do we make it a target its source okay so maybe that maybe should be source okay and it may be source and I wrote the wrong thing so this might be a question that Andy might ask again but let me prompt him by saying in the last meeting in Chicago the question on what happens to get came up does any thoughts on is get gonna get deprecated is it going to be augmented replaced my failure is to get should be def kept get should be deprecated because it\u0027s poorly defined and there were some people who were feeling it should be kept so I mean if people in the room are in favor of deprecating is fine and we they can see people in favor of keeping get need to speak up mm-hmm anywhere man I don\u0027t have any problems with this draft or the restaurant it\u0027s it\u0027s all fine to me it would just like one clarification in the nmda architecture because this is an identity ref I don\u0027t have to actually support the operational data spore I could support Bob\u0027s operational data store and I want to make sure that for conformance purposes a server must support the real operational they may support additional operational x\u0027 but they must support the real one and I think this is something Balazs also brought up we want to know do you you really support this data store and we\u0027d like to you know see that conformance that that an NMDA device is going to support the real data the real operational and the real and intended the identity ref allows anything that uses the same base and so for conformance like to say these mandatory ones you have to support and you may add your own but you may not you know for conformance versus replace the standard one so the yang library should give you the information on what it device supports and what it won\u0027t I hate putting that kind of mandatory language into it into any definition because it prevents future evolution let\u0027s say let\u0027s say in in in you know in "
  },
  {
    "startTime": "01:12:33",
    "text": "the same way that that the language in the yang spec that binds it to yang library rather than just to a particular RFC rather than just having that information external requires us to update that particular RFC I rather have flexibility and health standards are used even though we only see how they\u0027re used today and and which always remains like that that that one would expect your your Lang Lang library request to return the operational data store and what module sets are are supported by it right now we we only had three standard data stores and and this server implementation is not allowed to make up any new ones okay so that that is useful for the client to know who would like to code to an architecture rather than an implementation and I actually see it the other way it\u0027s limiting in terms of in terms of the implementation because if I want to come up with but I want to have a standard that has interoperability value I don\u0027t want multiple massive flexibility so that\u0027s my opinion could you go to the next slide please maybe I made a mistake the next one down the validate one yeah um the lock/unlock invalidate is per day distorted and this is a this is a clarifying question the validators per data store that\u0027s what that that means there yeah the current the current spec for for lock and unlock has a specific set of data stores that one can lock we want to make that generic by by saying we supply an identity okay so so can I into is validate also for all the data stores or is it just for the intended so you can\u0027t request a validate of intended because intended you already know is valid you can\u0027t have an invalid addendum but it would allow you to to validate running or startup or dynamic or dynamic okay or whatever we come up with in the future that\u0027s what I was I\u0027m focused on dynamic I me then ask my thank you that was the first part of my question and this also allows you to lock and unlock dynamic this is that\u0027s wonderful that\u0027s really great now on the specifics of what you do if you don\u0027t have the standard validate that\u0027s something you define and the data store where\u0027d is that so if you have additional validation let me be really specific for the ITRs ephemeral by the requirements we described there\u0027s some validation you can go from a femoral to config as far as constraint or to ops or a couple other bit but you can\u0027t have something go from config to ephemeral "
  },
  {
    "startTime": "01:15:35",
    "text": "that\u0027s that\u0027s a validation piece that we sent as part of our requirements I\u0027m assuming that\u0027s what that validate does or is it\u0027s supposed to do something else just just this clarification it done how you envision that envision is doing the the validation operation for that particular database if you have rules for your sorry to not database datastore if you have rules for your free datastore that say here are the rules for validity for I to RS datastore then that operation would enforce that would would inform you of validations of those okay well we\u0027re aligned in that thought okay thank you very much Marshall Eriksen I was missing two things from the guarantor get data operation I thought we will be able to filter on origin and I\u0027m definitely missing and I\u0027m needing a filter that allows me to get only the config force data is that intentional or will that be in we don\u0027t have that now we can certainly add it please do some of course origin itself is is a feature so so that would be optional can\u0027t just respond to that question arm I do if we\u0027d Ward add that that\u0027d be a separate RFC I mean how do we do filtering on metadata generic RFC it wouldn\u0027t it\u0027s not necessarily specific to in MDA so does that mean oh so you don\u0027t want to filter on config through config force because I think that was one of the main reasons why we had the separate state three sub trees in originally so the rest cough protocol already has the query parameter for you know content so you can do that in restaurants I\u0027m not I don\u0027t think there\u0027s no reason we should couldn\u0027t add it to but even then that that\u0027s different than filtering and metadata which is what the origin is and state yet state AG golfing force is my main need I just thought and of course even though you\u0027re doing get false you actually all your identifiers will be config true so yeah I just like to echo that I my gut is that some sort of way of getting config true versus config false separate from being interval filter on metadata might be might be useful um back to validate I thought there was still a bit of an open issue we were talking about validation what it means I mean this drat the the nmda or actually the data stores draft talks about templates I know even though we don\u0027t define them we talk about template expansion between in I guess running and intended camera where the expansion happens so and that template expansion can change whether something valid or not right like well since it "
  },
  {
    "startTime": "01:18:37",
    "text": "ends up and intended it has to be valid so I mean so I could take I think it so with templates before expansion sometimes configs are not valid right like relief refs may not be completed etc until that templates expanded right so you could ever running that that is in fact that doesn\u0027t validate and then I would North candidate I\u0027m working you edit the candidate you fill in a template okay yeah so they so Kennedy gets copied running running gets the template mechanism run against it fills in a a complete intended and then that yeah so I can see Oh antenna would be valid but does are we now saying that implementations that have a template mechanism no longer have valid candidates yes we\u0027re running I guess it\u0027s not really related to this draft suppose early accept this draft is the first one that kind of has language around template expansion yeah and that\u0027s that\u0027s part of why we tweezed all these apart so that you can have a template mechanism that affects your candidate are running you know imagine something imagine imagine a template mechanism that fills in you know you haven\u0027t you your your you have a mandatory field and yeah exactly and your and your template mechanism fills it in all the time is false yeah so I mean I can I can I can see how it completes and intend to keep it can become valid am i more worried about now that where is this draft is the datastores draft almost kind of telling us now that implementations of template have invalid candidates which is not really a concept where you really kind of well it\u0027s kind of a concept we face we fudged over so this puts it in in in a more stark contrast that\u0027s why you see it more but like under chinos yeah so would you guys the can return failed to account to see your candidate now someone runs a validate well part of the part of the validate operation as opposed to the so if a client comes in fetch and fetches the candidate configuration they\u0027ll get the raw data yeah that mandatory field will be missing that\u0027s right if you do the validate operation on the on the candidate it will succeed because the template mechanism will be run in place to generate the full configuration well if you\u0027re running validate under candidate the contents the candidate itself is not valid because those aren\u0027t that\u0027s not expanded until it\u0027s in the intended but again the purpose of the validate operation is to is to see you know when this is committed will this be valid we can take this discussion also to the main list or okay you guys can continue discussing later yeah I\u0027m not sure whether it\u0027s fair to put this on the datastores draft but it\u0027s I mean the Davis or drafts the one that brings up this template expansion so we may want to figure out like I said what we want to say about it we left a lot of fuzziness that which we have yeah sorry for interrupting Laden I don\u0027t actually I want to sort of second Jason said I had a very similar comment during the "
  },
  {
    "startTime": "01:21:37",
    "text": "net mod session I think we have to be a little bit careful here because I mean what validate means because yang spec says that some properties have to be satisfied in all data trees and some properties need to be satisfied invalidated trees I can imagine templating mechanisms that break even those properties that are like they types for example you put an asterisk instead of name aside like that so I really think this needs to be clarified what template because it\u0027s its templates are only mentioned in a very and waving way in in the nmda draft as you said basically but in in terms of what what a schema means whether really it can be it can be broken in candidate or in running even that that\u0027s quite an important point I believe we can certainly so I don\u0027t I don\u0027t want at this point to have to specify what it what the with what a templating mechanism is or any sort of standard thing but we can certainly say here are rules templating mechanisms cannot break you cannot break individual datatypes you cannot break Keys you cannot break hierarchy you know you\u0027re all of this for the for the candidate and running have to be in place okay we need to move on please quick last question the yank standard contains the sentence the running-config data store must always be valid if we have this templating managed is this Center still true that is that is not true in a tenet have for many implementations hasn\u0027t been true which is a start maybe if some updated for that yeah are you done yep just the question is there anything substantial you want to add before adoption or is everything you mentioned here or it in draft filtering an origin was not in the draft rules for templating was not in the draft and that was first one there was a thorough knee I personally would be in favor of having those things in the drop before adoption but I let my heart speak well also the question on validate I think there\u0027s quite a bit right so I would second moments opinion that I think we probably might want to put that in before we make a call for it up even in a draft state we would need such text a little bit at least Thank You Kent in the MA for a restaurant so this was again a draft covered by the Charter "
  },
  {
    "startTime": "01:24:38",
    "text": "and it\u0027ll be just adopted once it\u0027s ready okay so introduction this solution is backwards compatible with the existing restaurant servers factors compatibility is achieved by only adding new top-level resources rest comp defined slash data and we\u0027re introducing a new top-level resource thereby leaving all the semantics for all the existing resources alone summary of updates as just mentioned we\u0027re adding a new top of a resource this was /d s I know we try to we shy away from acronyms but after looking at datastore slash datastore many many times it became obvious that it was just gonna be too long and ugly so /d s we also add a new query parameter with origin this is to enable the filtering on on data based on upper origin it also it states that section three five four paragraph three doesn\u0027t apply for all the new des resources this is the one that says that if he--if a particular leaf is being referenced the resources being referenced and its value is the same as the value of the default value then the server must return an even if it\u0027s a server that has the explicit a basic mode we believe that the server should you know be allowed to continue to use as standard with default behavior I even in that case so this actually isn\u0027t specific to nmda in particular but we\u0027re kind of addressing and what we think is an oversight or an issue with the rest contract and we get to do it because we\u0027re defining a new resource so we\u0027re taking advantage of this opportunity again this draft does not deprecated the slash data resource and but maybe it should I mean something that we could right speaking some more about the /data Deus resource datastore is encoded as an identity as was mentioned before so for instance we have the standard identities which are defined inside the revised datastore draft in a net mod working group so they\u0027re all prefixed with the module name ITF - datastore but then other RFC\u0027s can define their own identities so for instance an example des ephemeral draft could define a deus ephemeral datastore and yet they all show up because our identities we did this in particular it because we income in lieu of having a anima defined registry this is much more dynamic and flexible this is why the motivation for using identities instead of just a flat list of strings some conformance verifications so clients can test either if operation will exist you know how do they how does a client know if the restaurant server supports nmda they can they can do a head the head operation HTTP head operation on that URL just to "
  },
  {
    "startTime": "01:27:42",
    "text": "see whether or not the resource exists or if the yang library container exists others being that the new revision so here you could either do a head or a get operation oh by the way I guess in theory you could do a get operation on the on the operational but you would get back all the data which is ridiculous right you would never want to do that that\u0027s why I didn\u0027t show that as an example but if you\u0027re you know the resource is actually targeting the yang library get maybe you know appropriate because in the end one of the very first things a rest client sorry a restaurant client would want would be the yang library so so you know in one step it can boast check to see if the server supports an NDA and get the contents of the yang library so probably preferred would be one of these latter it shows here that you could the client could either check for the yang library in underneath the slash the new top-level resource or the legacy top-level resource right because both would provide the same content but at the end at the bottom here we we prefer the end we\u0027re clients not use the /data resource so that resource could be deprecated more easily someday protocol operations for the most part all protocol operations defined in at44 the slash data resource are apply to the new data source with some exceptions though these exceptions are one dynamic data stores are excluded as each dynamic store datastore definition needs to be reviewed for what protocol operations it\u0027s the force this actually goes to the revised datastore draft where it talks about dynamic data stores and specifically calls out that you know each dynamic store source datastore specifies the protocols and and even if it says it supports neck huh it may not support for and locking in that conf with it like an ephemeral data store there\u0027s no notion of Netcom so so so surely that that name data store excluded from that comment some data stores are read-only by nature the for instance the slash intended and also operational so any attempt to modify those data stores would fail also as mentioned before the paragraph 3 of section 3 5 4 doesn\u0027t apply and of course the new query parameter with origin that\u0027s it what is now actually the meaning of the unified datastore in rest conf isn\u0027t it that it\u0027s now more like a candidate if for example an implementation has the applied datastore so that I can somehow edit this unify datastore but does it or does it mean that it will be immediately the stuff edited will immediately appear in applied if it works this way I mean "
  },
  {
    "startTime": "01:30:45",
    "text": "if it\u0027s applied immediately so the the unified datastore is the slash data resources and in underneath /d s we have a new data source specific resources where they take on the more traditional or dates for specific interpret X and so they\u0027re no longer unified there is no way there is no unified datastore underneath the /d s datastore that said if you.we we were cognizant or concerned about this that you may have an NMDA aware server that you know maybe someday only wanted to present the new resource without presenting slash data and so with the properties remember in the yang library presentation I mentioned each datastore has properties one of the properties I think it was called Auto commit sometimes like this and it was meant it was intent if you read the description statement for it it\u0027s intended to be used for the running datastore and and what it gives you is essentially the unified data store like behavior where you know whether or not the server actually implements startup regardless if you write to running it\u0027s going to auto commit so the there is a incantation of properties that can be used with the new data stores that give you the Universal datastore like nature enter a related question could this be used for introducing a candidate beta store in the restaurant it could be so the the revised datastore draft in the DES sorry the ietf datastore yang module it does define identity for Canada and this draft doesn\u0027t preclude the ability for that identity to appear but at the same time at this moment we\u0027re not defining any operations like copy or commit we could but we were not at this time defining those operations and then also I think if we were to wanting wanting to do that we wouldn\u0027t right because let\u0027s shared candidate I think it\u0027s a known issue that\u0027s the problem and and we prefer doing private candidates so I don\u0027t think we should do that we if we were interested in doing candidate data source at all we should probably think rethink how we do them we actually have an implementation that uses a per-user candidate so that in my view makes a lot of sense so you would like to have some standard mechanism now to query these exactly two questions the first you know I asked at our dress about the entity tag did you get a chance to act as an t about that we not know okay we\u0027ll do that offline then Andy it just it was whether we could use the entity tank for the client priority split prior to us for the dynamic ephemeral so for people that "
  },
  {
    "startTime": "01:33:53",
    "text": "are online Andy just said that we should double-check with HTTP people yeah and we\u0027ll do that thank you Andy that\u0027s helpful that means we probably have to have metadata I would like to not double use things okay second question is Section three dot go back up to the sections at work not Section three dot v dot four is that where you\u0027re defining the roll back I thought it was on a different section don\u0027t know it\u0027s not it\u0027s not here this section is regards what the behavior of the server when a leaf is being targeted and that Leafs values happens to be the same as the default value mm-hmm and and what it says that the value must be returned all the time regardless those could regardless of the server supports explicit mode or not and the reason why this is concern is because if it\u0027s returned all the time then the client doesn\u0027t actually know if it was there if it\u0027s configured or not yep so wherever the rollback is defined part of the requirements were there were you had to have a restriction on the roll back for the ephemeral so I don\u0027t know if we need to add an augment that says we don\u0027t have to do it all the time we could only report the requirements State at all or nothing and you roll back I think didn\u0027t give me that option either we augment it as an option or we go forward that was one of those requirements in different estate okay some I\u0027m trying to think what you mean by robach so with resk off you your it\u0027s pessimistic locking right so we\u0027re using etag or or the if modified and it\u0027s all-or-nothing either the server takes or a doesn\u0027t ok so there\u0027s nothing to go but but but but then there\u0027s a second thing which is a rollback on error right the traditional Netcom and that is addressed in the yang library data store so each data store has a list of properties one of the properties that we\u0027ve defined is called rollback on error which you know has that semantics i\u0027m if you want to have rollback on an error on it it is it is it is the rest comp base functionality so what you\u0027re telling me to reflect back just to confirm rest comforters the thing will block all or roll back if there\u0027s error the rollback at all of this error and the other part if someone wanted to use it they could put it in the egg libraries that would then affect the federal data space and that would be the implementations problem yes just that so i as i was going to ask my question the thing about the rollback on error just triggered another one so rollback on error in net comp has two slightly different meanings depending which data story working in if it\u0027s on the candidate it\u0027s the scope of that edit config because the rollback on error is a attribute of the edit not of the commit versus I guess with unified "
  },
  {
    "startTime": "01:36:54",
    "text": "or running rights it\u0027s intended to apply to the action of actually applying the config or not well there\u0027s two things there\u0027s there\u0027s the error options that can be passed on to an edit config for a Jenny there\u0027s actually the RPC call rollback on air right so if you\u0027re doing a sorry it\u0027s an RPC it\u0027s it\u0027s a it\u0027s in it\u0027s a it\u0027s a flag that\u0027s inside the commit operation so as you\u0027re doing commit you can pass a flag to say whether is that look okay I wasn\u0027t familiar with that part yes so it\u0027s in 6241 I think okay the the the other the other question I had is we talked about library how we\u0027re adding now we we had modules before now we have to add datastore information discussion about rest comp just made me wonder our this is the list of data okay so we have the single new tree that describes data stores and their properties are those properties gonna be different for rest comp versus net comp so if us know that the remember these are views the particles are views onto conceptual data stores the properties are with the conceptual data store yeah which is the protocol so the protocol needs to support the properties but the properties won\u0027t vary by protocol I\u0027m more thinking about an implementation that has different support for different data stores on different types of interfaces so you have a rest comp server and a net comp server on your box you\u0027re not necessarily gonna support the same data stores on both those interfaces are you should and three the dynamic data store are you necessarily gonna offer it on I don\u0027t know if sure if we need to I\u0027m not sure if we should say that when you advertise it necessarily means you have to support it on all possible interfaces that access data stores well if you can come up with a strong use case okay we can take it to the list well I mean the use case use case I can think of is like an implementation who\u0027s maybe gonna implement I char s they may do it over one interface to start not multiple sorry may support over s compa start really calm but again I trust as a dynamic data store and per the revising is for draft each dynamic stays for defines its own semantics like from scratch so whatever you support in the properties you know may not I mean yes so when I describe the datastore in the state tree is fine but that stage that\u0027s the module state or the new library tree is gonna be presumably it\u0027s the same library tree you\u0027re reading through Netcom for s cough so then you advertise the dynamic and that is only one place to advertise the dynamic datastore okay an impression that you support okay I\u0027m sorry I think I now understand your question right so it is in the yang library draft it says that the the response that comes is actually with respect to the server or less responding so so the response could vary by a net "
  },
  {
    "startTime": "01:39:55",
    "text": "confident remember we are over time so I\u0027m sorry a lot that this out so based on the questions I I\u0027m gonna assume that I think we need some more clarifications before we ask for what group adoption mm-hmm actually I thought I was handling all the questions where if there was anything else to ending there are less the quick question were you intending how do you know what\u0027s valid inside of the library ie how do you know which modules might be valid for the dynamic data store is that going to be an I in a registry you know so in case someone says here\u0027s my catalog is this really true are you trained yourself generated or you\u0027re trying to buy registry so this is in the yang library action and so that the the the data store entry for the dinette for the ITRs ephemeral datastore would have a pointer to a module set which is those modules that are supported inside that dynamic day story okay so you\u0027re asking for because I I see questions here and you said you\u0027re taking it to the list that\u0027s why I was we can consider we\u0027re going to continue the discussion unless no matter what I believe the working group should adopt the straps even with questions outstanding we have a milestone for bringing this to last call in the November time frame so all right just a procedural thing as to whether or not we push for adoption adoption now or a week from now okay so a short answer from people that they feel that the document is ready for workgroup adoption okay a fair number all right so before we get off this film if I can ask you to take one more Akshara action item and to close on that whole discussion of what to do with get s ResNet confess concern the get operation okay now we have a slot for nama architecture and guidelines it\u0027ll be just summary actually we had this summary today right so questions can be answered quickly we have less than 10 minutes time all right so does anyone in this show of hands again want Robert to go yet again over guidelines as far as nmda what they need changes they need to make the oil to be NDMA compliant I guess we got that thank you thank you for watch so now my coach he has more time okay now Mahesh will "
  },
  {
    "startTime": "01:42:55",
    "text": "present on the accounting draft these three yeah our I mean we need to go through them and and discuss how can we adopt or whether people are in favor of working on those topics alright so I\u0027m going to talk about the three version of the draft as a quick introduction this draft is essentially addressing the last a in triple-a accounting and defines the yang model forward the changes since the one have been and I know I skipped 0 2 in between but that was because I submitted a division is I change the accounting record a little bit and I\u0027ll talk about that the addition of default deny all China Claire clarify the interaction with Chappell a server and two additions so let\u0027s not in that same order that I had listed there so I added default deny all just like makam does to protect records so you need an explicit rule to be defined if you need access to the accounting record as far as modifications of the record are concerned so I had a start/stop code accounting start/stop code that was used if you wanted to fragment the accounting record as you were transmitting it I dropped that simply because I think that\u0027s going to be implementation dependent depends who you\u0027re communicating with and whether they need that kind of fragmentation so I\u0027m going to leave it up to the implementations that need to fragment an accounting record to add that if it has need be I also added a message ID with session ID simply because there can be multiple messages within a session and if you are trying to key you would need that I there was a question on what happens to any values that you might want to capture as part of the counting record so example and I get is what was requested so if there was a filter parameter that was asked for we will probably want to try to capture it in the accounting records so he\u0027d know what was requested but there is at the same time it tries not to capture the entire response because that would be just huge and especially with get or get data so just to make sure that that\u0027s true and same it-it-it config again values for what was being modified as part of the edit config also added some examples in the appendix 3 examples particularly on edit config get and how "
  },
  {
    "startTime": "01:45:56",
    "text": "to set the maqam rule considering that there is a default deny all what a rule would has to look like to for even a user like root to be able to get access to the accounting record so with the change in record this is what the tree diagram looks like and as mentioned the next step is to ask for workgroup adoption questions any comments questions on this draft who read actually dissolved I saw Andy I mean it has been already presented twice this is the third presentation I I mean what I remember from last meeting there were some people saying in favor others were uneven or uninterested some were saying I remember Kent stating that net converter should start such work later not now but yeah after getting comments from Jason for example I just need to know again the same question who is in favor of working on this topic in net converting group theirs so Jason CERN I apologize I\u0027ve only skimmed through the draft but I just want to confirm my understanding I think it\u0027s not intended to instantiate data in the node this is just like a format you\u0027re proposing it\u0027s like a so you\u0027re putting as a container but it\u0027s just to represent a format of a message that would go on a wire towards a triple-a yeah and I guess that format i I don\u0027t know is it something we know that these triple-a servers will recognize and be able to terminate that format right so one is that there is no format of what an accounting and it could looks like today I mean it\u0027s kind of free format in today so one is to normalize that well there\u0027s tax and radius accounting that probably has some sort of yeah but what standard so I think radius is statement somewhere that is that they don\u0027t care what is in the payload so this is to formalize that payload what is exactly in constitutes an accounting record so I for example I asked the operators what would you like to see in an accounting record and so we went back and forth on trying to figure out what is an important piece of information for them and based on that feedback we came up with and if I define "
  },
  {
    "startTime": "01:48:56",
    "text": "what an accounting okay so I mean there\u0027s one part of it one part of this I think is is particularly useful is that I guess you know defining what\u0027s gonna get captured up in those accounting records you put an instance identifier just to say okay here\u0027s a piece of data and yang tree and here\u0027s a value I guess I\u0027m just a little bit more concerned about whether this format is gonna properly inter work with some of these other servers like you mentioned it\u0027s the payload but you have like start/stop fields and other things but I think are part of the standard actually radius accounting message aren\u0027t they they\u0027re not just part of yes so in case of yeah so radius when you\u0027re communicating with radius and because it at least used to require fragmentation because it had a size limitation you needed that but that\u0027s particular to radius if you\u0027re doing this say just doing a get over net come and doing accounting I could get you won\u0027t need to do the fragmentation so that\u0027s why attract accounting code out of there so I I have to interrupt you this is only a five minute slot you need to be fair to the next two presenters please go ahead can very quick person my only question is is this the right working group for the work and is there another work group like diameter radius working group that that could take a set instead only because I mean I guess from a counting a meaning a triple-a perspective since we were dealing with maqam so let me ask them the question quick because we need to move on so we got the sense of the in other meetings but I have to ask again who thinks this work should be done in that convert of show hands please I did not did you show him I did not see any hand so we need to note this and that come co-chairs need to decide can there be an option to say we might need some more discussion to figure that out first yeah I mean the mailing list is open for discussion I was trying to ask people several times to raise discussion on the mazes so this is also the measure for the interests of the working group okay so let\u0027s go ahead with the next one Netcom proxy so you\u0027ve got five minutes I working group this is Michael from hallway and today I want to introduce update for an account proxy and that is document already present in a staff meeting and I have received similar comments most important thing is we need to address is to show zapped and set "
  },
  {
    "startTime": "01:51:57",
    "text": "this machine ISM need to be done in the application layer are not in the transport layer so it prepares another version to address as a comments either on section to introduce use case and it\u0027s narrows and you can read the document section 1.2 and I also will introduce this scenarios and use guillotine next to side okay this right aisle please remember on time management you\u0027ve got now for me yes yeah okay so first scenario says using a comma or say too many means and every instance ended we can see a picture that shows that after use it Yama\u0027s can actually I met through the published network and within them halfway network the private protocol such as at LCP Iranian through that may have instance and this recipe we will provide the data choose to the knock off too many mental have way instance so yam as we all connect to that operates through sir knock on proxy and the proxy locally and we FM reasons we have management we have within the only half man network the country that we are transported from the proxy to that we are instanced through now come over to ICP and we have for instance will report their ID to the proxies the proxies can report us support can restore amines touch released and then the EMS can using the statically statue management it says there are free in every instance this is the second scenarios if this introduced using the knockin proxy to management is non key element and showing this figure so Yama\u0027s connect who\u0027s a gateway admin destroy public public network and within the updates which network some private protocol such a queue s protocol Iranian services network element and reasons and SH network queue as protocol provides a teacher choose an alcove and notice that this object switch Network natural element may be not some IP devices that so innately cannot be used to report their IP to the an account crunch and to management is none Kidwai element the EMS can connect to the in on k2m and through the nikon proxy and the proxy locating the gateway element within SH network within the owners network and back home now comes later will be transported from the proxy choose an "
  },
  {
    "startTime": "01:54:58",
    "text": "anchor element and non kiev element will repose ID to proxies can restore the means at least then the Yama\u0027s campus on the attach agrees to management is 9kv element okay I need to ask necessary questions just you you can say one or two sentence and we need to go on okay last okay so let this trolley please show hands who thinks net count working group should work on this topic okay so as we I assume much things the same this is not in the current focus I mean in the focus of the current charter we need to come back to this later please update as you already wrote get more commands and update it and especially raise a discussion on the made list to get support okay thank you so this is important before coming back to the next left comm session thank you very much thank you thank you okay the last one is the UDP this might be in the focus of the current charter because it is related to the net Kampf I mean notification set up drafts so please go ahead okay I need to go work faster the udp-based publication channel for human telemetry and there are several young casuality working IETF and our work is based on them the problem we\u0027re going to solve is to collect a large amount of data from devices with maple and line cars there the existing solution consider only one push so er in the main board as you can see from the figure the data cracked here from Lanka\u0027s and aggregated to the main board so the Mabel can easily become the bottleneck especially when management of data so what we are proposing here is the distributed data collection mechanism that can push data directory from nine cars to the tractor and then why UDP mmm the yum push mechanism supports the separation of subscription control and the transport for data stream that means we can use different channel for subscription and publication to existing "
  },
  {
    "startTime": "01:58:01",
    "text": "transports are not confer and the rest come for both of them are tcp-based if we use the TCP based transport for the distributed data correction then the collector will suffer a lot of connections and UDP is lightweight so high frequency and the better transit performance can be achieved which is important for you mental metric and the scenes no connections they need to be maintained the UDP in capsule encapsulation can be easily implemented by hardware that can further improve the performance here\u0027s the brief overview of our solution thus the subscriber send requested to the via the subscription channel and the subscription server welcome host request and distribute the damage to the agent and then the line cards can push data to update to a cracker there are several technical points need to work out firstly we need to define the subscription model and then in interaction between subscreen server and the agent and also the udp-based the message header and also the retransmission procedure if we want to apply or consider the reliable subscription and also congestion control here\u0027s our preliminary design on the UDP telemetry header and you can see the flag indicates the supportive features on reliability or sanitation encryption so and there are also many other fuels defining in the draft and that\u0027s all and the music feedback and cooperation okay as I said this can be seen in the current focus of the Charter Phil no no I\u0027m sure coming first I need to ask some questions but go ahead we don\u0027t have for time for more comments but we should get your so so I support this work this is a stuff that we do in generous as well I think it\u0027s very common you know a lot of work needs to be done on the particulars but something in the space is we\u0027re standardizing in the space is certainly very important thank you okay so who read this draft more than 10 or something nine to ten who thinks udp-based publication channel for streaming telemetry is important for net conversion and we should work on it same amount or even more 10 to 12 okay I am NOT going to count everything it\u0027s more than 10 in any case so people are "
  },
  {
    "startTime": "02:01:05",
    "text": "in so I was not in favor of starting and immediate adoption but I\u0027m Kino asking Netcom co-chairs now I include Ken because you guys need to decide when to go for adoption yeah I think we\u0027ll be taking it to the so you want to start the call for adoption on the list yeah okay so it\u0027ll be done soon somewhere so that\u0027s it actually fill the other option is designed him and getting the folks together on that so you mean this should be done in the so called design team together with other yank push drafts I don\u0027t know so this will be then discussed with the co-chairs and write one last thing all the problem you\u0027re mentioning that\u0027s the one we\u0027ve been going from net flow to IP fix Nick bottom right car so there\u0027s a lot to learn over there okay okay so that\u0027s it for today thank you very much hope to see you in Singapore then I will sit together with you in the room yeah a party in honor of you tonight there are some information available you can get location info from Mahesh we don\u0027t have a slide for this you can add to it so these are together yeah you can send it to me if you want to look at it or just for the summary on on greatest yeah "
  }
]