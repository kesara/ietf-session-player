[
  {
    "startTime": "00:00:14",
    "text": "uh okay we are on top of the hour so i think we can start uh welcome everybody to this co-working group session at itf 112 i am marco tidoka my culture is jaime menetz and we assume people at this meeting have read the documents discussed today to better contribute and be engaged in the discussion the not well applies there's no next slide for that and we're using miteko so the list of attendees is compiled automatically i'll keep a look at the chat and a few people volunteered to help taking notes christian and joran thank you so much again and this is an official itf meeting so the note well displayed in this slide applies it's about ipr patents and so on it's very much about anti-harassment procedures and code of conduct so be nice and professional with one another uh yeah if you want to interact you can raise your hands with a raised hands tool on the top left section of the window the first button from the left and if you can talk in your environment just type my colon followed by your question or comment will relate to the room we've been recording already and end up on youtube in one or two days again blue shield compiled automatically by miteiko so this is the agenda for today and following an introduction uh from the chairs we'll go through uh"
  },
  {
    "startTime": "00:02:00",
    "text": "four working group documents and for uh individual submissions and we start with um href and coral presented by a karsten and christian then we enter into group communication and low score territory we have esco presenting group on this i'll give an update on group of score then we have the key update procedure for oscor on recard christian has updates on cacheable auth score i'll give an update on oscar capable proxies and we conclude with a brand new draft about a co-op option for performance measurement and 10 minutes of flex time also does anyone want to bash this agenda in any way today her known so let's get into the document status and we always have good news uh of the sky was mentioned at the latest interim but just to bring it to the main atf session since july we got published one more rfc 9100 was sentimental versions thank you very much to the authors in the working group for yet another achievement and along the same lines in the past months we got approved two more documents for publication as proposed standard they are also in the rfc editor queue now where they joined tumor documents also waiting there for echo request tag so there shouldn't be any more uh blocking conflicts in this respect and these four documents should be published in the relatively relatively near future we also have two of the four core comp documents in isg processing now uh we got very good reviews from the isg and the authors are now processing the"
  },
  {
    "startTime": "00:04:00",
    "text": "comments uh very well tracked as github issues and discussed in dedicated design team meetings occurring bi-weekly we have uh next one early next week it was are young cyborg and nc and then to mention some post-working groups called uh document uh it's about the other two core conf documents comai and yang library they're a bit in the background now to prioritize the the first two ones i mentioned and once they are done autos can focus a bit more on on these ones too uh there were just some open points left following the shepherd review i remember and finally we have group of score that in fact had the first working group was call earlier this year and the second from last version addressed those comments but this latest version published before the cutoff addressed also some more points remaining open and this is also approaching a second working group last call a few adds up on things to come as also suggested by francesca that shares plan to have an update of the working group milestones uh on the entry page of core in the data tracker and some milestones have been pretty much met in the past months or even years and it's good to add a few new ones to reflect ongoing work and especially documents that at some point next year will be shipped to the isg kirsten yeah they're a question by katyan a couple of hours ago on the mailing list whether we should maybe have another of those core applications meetings and i said maybe not this week because that is already pretty full um but maybe it's actually worth trying to do an interim before one month's time"
  },
  {
    "startTime": "00:06:00",
    "text": "uh so specifically to talk about these applications things because we don't have much apart from coral not much applications oriented stuff on the agenda today yes it might work just quickly thinking on december 1st i just like to synchronize a bit better on that and what to cover with that during this week uh even on gather town i think we can do that okay a second observation so um a couple of interims ago we decided it would be too much work and and really not necessary to recharter and now there is an interesting rumor circulating that the fact that we are not recharging means the working group will be concluded next week i think that's a rumor we all can work to dispel yes sounds very new to me at least okay just wanted to mention it thank you any comment on this first part francesca or anyone by the way on the rumor i have no idea first news i hear as well on the other stuff on the problem details and pops up so i would like to have perhaps before an interim because in an interim it's nicer to present something and show progress so it'd be nice to have first perhaps a work meeting with co-authors so providing that we have it within the month we could have it then the interim also within the month and i agree also with michael maybe we can have something informal on the other yeah sorry yeah that's okay with me we actually we don't really need the the heft of an interim we don't need the two-week wait time and so on and maybe we should just"
  },
  {
    "startTime": "00:08:01",
    "text": "schedule a meeting to to do this that would work better for me at least thank you the other documentation also okay and thinking of uh yeah already scheduled thing um as agreed at the latest interim before the cut-off we are going to have one on december 8th it's a wednesday usual local time and then an actual regular series will resume in january every other wednesdays same local time uh alternating with seabor as usual any more comments anyone okay this actually concludes the uh fair introduction and if there are no more comments we can move on to the first document maybe carsten has a comment no i just wanted to start with the first document you're welcome so are you showing the start slides no i'm showing the slides you can take over now now we are doing the game how to find the slides in the weirdly sorted list okay thank you so i wanted to quickly report on on the work on the draft that is called href but that actually standardizes what cris are and i have 14 slides 11 of which you mostly have seen so i will run through them really quickly but please do stop me um if there is a need to discuss something so just as a quick reminder"
  },
  {
    "startTime": "00:10:01",
    "text": "the the car group really tries to do the the web of things even if we didn't call it that way and um the web of course has three things a hyper references transfer protocol and representation format and moving this to to devices means we did a new transfer protocol we did various representation formats not just a single one um and well we stuck with the hyper references which are there in the weapon that probably was a good thing to to do for the first 10 years or so but maybe we actually want to do a little bit of a cleanup now and see whether we can have hyper references that are a bit easier to to operate on in in various formats so uh we're trying to pair the uri files which of course won't be going away with a concise um form or constraint from the ci so one reason is actually that that ui is why widely really widely implemented most of these implements implementations are really bad and and have mistakes and so on so some features that uris ostensibly have don't actually exist when you look at the the implementations out there so just to remind people what a ui is it has five components a scheme an authority a path a query and a fragment and these are single items like scheme and fragment or they're actually structured in some ways or the the authority is at the end is often a dns name which is structured by dots the path is structured by slashes and in co-op we decided to structure our queries by embassad science because that's the the existing practice in the"
  },
  {
    "startTime": "00:12:01",
    "text": "web and there is also something called ui references which are used together with a base uri to have a short way to to talk about a ui that is close to the context that we are currently in so you can say something like foo and then you take the existing ui the ui you are add and remove the last path segment put in foo and so on so all this all also should be uh standard uh stuff now what does a cr i do it's essentially a representation format for the uri data model which is really interesting because nobody sat down and actually defined the ui data model so we essentially have to reverse engineer that and and one of the contributions that the uh that the ci document in the end will provide is a much clearer expression of the data model underlying eyes so the the draft called ahref defines cris and cri references so these relative things that you use when you have a defined context so this work started a while ago but was started by klaus harker and jim shard had some uh pretty good contributions in this space the format is now sibo uh based and the the short form the abstract content of the draft is in the little piece of cddl here so you you have something that ends with a path a query and a fragment which are all three optional and you have you started with a scheme or authority most cases of which are absolute uris or with a discard number which are relative"
  },
  {
    "startTime": "00:14:02",
    "text": "uh uis so that then there are there is fine print here with optional part numbers and and so on so this was uh moved around for a while and then we with 06 we finally ran into one problem we hadn't solved yet so with urns and similar structures which include the gids which are popular in certain parts of the universe you don't necessarily have a need leading slash so you have to have a way to to indicate that and we found a way to indicate that by putting true in the place where normally an authority would be so this gives us a nice way to essentially have a fully passed uri and the the last thing we essentially changed in know 6 was to pass the host name as well so dzi dot de becomes an array of two elements these are i and d e and there is a port number down there in in the example so we put the port number there as well um so this has uh got a little bit of uh fixing uh so after six they came over seven and then an o8 sorry and we now have something which i think can be called a consistent design we think we are feature complete with respect to rfc 3986 in the form that co-web actually supports and i have a slide on that in a minute we have some initial test vectors um and we need to update those but it's already looking good the implementations seem to work together few updates are probably needed in the implementations as well but by the end of the year we should have implementations that actually"
  },
  {
    "startTime": "00:16:01",
    "text": "uh work together so uh for from my point of view we just need a little bit more implementation and implementer reviews and then a working group last call there are two questions that i think we should think about one is a pretty popular way to treat uh your eyes in in documents like like uh document formats like json ld where you put a uri prefix into one place and then from a different place reference this prefix either explicitly or implicitly via add context and concatenate and give the rest of the ui and concatenate that to the prefix gift and as they are right now cis can only do this in certain places so for instance if we have a scheme and an authority we can [Music] put a path after that and sibo pact can can handle that nicely so we don't have to have any special mechanism we can handle this within sibo pact but it becomes difficult if the uri the prefix ui goes on into a partial path and expects the the referencing site to actually put in the rest of the path and of course the first place where where this becomes very obvious is when you look at w3c web of things thing descriptions because they use a prefix an add context that has two elements of a path name it's under w3.org and then it has two elements ns and td and as for namespace td for thing description and then it it"
  },
  {
    "startTime": "00:18:02",
    "text": "expects that you provide the next path component and that is difficult to do right now zero pack doesn't provide you a form of prefix compression that would enable you to do that so this is an area where we have a functional deficiency and we currently don't have a nice idea how to fix this there are several not so nice ideas how to fix this and this probably needs a little bit more thinking of course we can leave the functional deficiency in place or we can address it so this is one phone in our site and the other phone in our site is that co-app has decided to not support percent encoding in your eyes except for uh the specific case that co-op's own or the the co-op ui's own delimiters are escaped by the percent encoding so i think that that's a reasonable thing because percent encoding means you have to do string scanning and and copying and all that stuff which you don't want to do on a constrained device so when cohab sees a path percent to f slash slash foo then it the the uri to curb conversion converts the percent to f to a slash so that slash is part of the path segment and not the delimiter between path segments and everything works like it should but that doesn't work with w3 c um the ids which we probably have to support in some way and we had a meeting just last friday where we discussed this and we're a little bit unhappy with the situation that"
  },
  {
    "startTime": "00:20:00",
    "text": "we cannot really express all potential dids using cis so yeah we thought a little bit and there is a pretty easy fix which maybe is a little bit easier on the specification side than the implementation side but that's not that bad where all the the text strings that are used in your eyes have an optional form in which they alternate unencoded and percent encoded parts so the the bb percent 3ac that that is up there in the example in the uin example which would be look similar for the idea example that is a colon that actually is not used by the url internal syntax and that may have been registered as double a by someone before so there's no way a generic processor is going to know what the the internal syntax of that uin is so percent encoding may be may be needed uh to escape things that uh need to use the same delimiters that are used for the internal syntax and the idea is to allow instead of the the string which is a a colon bb colon c in the second last line to allow an array there and the array is structured in such a way that the odd numbered elements are actually percent encoded but they don't need to be percent encoded that there's no no need to actually do the percent encoding they can be uh fully processable by the implementation um except that they have the special percent encoding syntax so they would not be used by a string scanner that looks for colon uh"
  },
  {
    "startTime": "00:22:02",
    "text": "delimiters in the unia syntax um so it's a little bit like indefinite length strings that the kind of processing that that you get into except that the the elements actually mean something slightly different and if you are scanning for the limiters you just leave out the odd elements you only scan in the even numbered elements so here we have a proposal and now of course it's it's a matter of of design taste to whether people think this is a proposal we can uh pick up or whether this is just too ugly but not picking it up of course would mean that we don't have a way to do all these cases of percent encoded uris so uh yeah i'm not well i'm happy that we have a proposal that that's a good thing um but uh i'm i'm still not entirely convinced myself but i think if we actually do want to support percent encoding in some form this is the by far the simplest way to do this so everything else when one can come up with is is even uglier so that's that's the two things we may want to think about before we go for a working roblox coil comments just i'd like to point out that the"
  },
  {
    "startTime": "00:24:00",
    "text": "alternative to doing these presenting codings and other things that we might need to do apparently is to just admit uh your eyes in several places where see your eyes are acceptable and it will be something that will need to work through as um also in the in the design team but also solicit input here that this is just the easier thing to do for the general case which means that you have to carry around both the ci and the ui implementation yep where you where whereas do i'm doing these present encoding tricks here and some other um mean that you have to you might have to go into a uri anyway because processing it out of the the separated form if you have semantics in there might be comparatively tricky too i think we should should just keep an open mind towards both solutions while we were exploring them from the chat hank is overall supportive of somehow taking care of the id support you have any preference on the exact solution hank no audio mumbling to you so this is yeah of course it's tricky to uh and encompass them all but my my suspicion my and maybe my hope is that uh when we when we try to support the majority or the host of it um in the end uh the the the of how they're constructed today might be contained a little bit"
  },
  {
    "startTime": "00:26:00",
    "text": "and and then uh the cri could be the uh the guidance how you would do it and and that might might ease the the the the wilderness out there a little bit and calm a little bit down um so so i i think the effort of trying this is volunteered i understand that of course you fall back to uri if nothing fits but in the end maybe that's something we can make go away over time yeah basically the idea in in the ci draft was to actually document which your eyes are not uh supported and there there are still some some very weird uis that are not supported even with this percent encoding fix but i think they don't have much practical relevance why the percent encoding is actually being used even if we know that implementations usually are really bad in this space so they probably won't be correct but they they probably will at least cover like 80 percent of the cases that actually do occur so it would be a drawback if we couldn't support those but it's probably the one place where i would say that might be an optional feature in ci is because you don't actually need it unless you you have weird ui designs or weird server layouts thank you any more comments anyone okay now thank you for today kirsten and next is christian with coral"
  },
  {
    "startTime": "00:28:10",
    "text": "hello um like carsten did i'd like to go through a bit of introduction of what coral does because it's been advised this has been presented as at a full itf meeting coral is a data model and a language that allows us to talk about resources on co-op and other protocols and on how to interact with them in a way that is suitable for constraint devices that is um similar to how cri fits into this um fits into the analogy of of having a constrained device processable forward for um for your eyes here we have a format that covers areas of of metadata and this um before i get into kind of the the concrete um um formulas this could replace or could is similar to um we already have a few users that would would want to use this so the first two documents are problem details and pops up are in this working group a group management that administration is in ace which would be a bit simplified if built on coral sdf might have applications that we're starting exploring and basically anything that so far uses link format could could just as well um build on coral but it's not precisely the same so for example i'm going ahead for link format our c6690 uh this had a lot of string parsing so like we had before about um processing your eye by by bite and looking into what what does this mean semantically um in in coral all the information that in link format is there by"
  },
  {
    "startTime": "00:30:00",
    "text": "possible escaping of strings and and processing parts of your ui out of it this is all expressed in sibo but then again compared to seabor um we have cement we have semantic information there we have properties of resources that are packed with uh that are described by predicates quite similar to rdf which allows us to use several domain languages in the same document so we can for example extend say a pub sub broker that uses terminology originally designed for pubsub with application specific semantics that are ignored by processes that are unaware of that application but can be used to augment functionality so terminology can be reused when building an application on top of coral um but it doesn't um terminology can be reused and sorry last track um and doesn't need to give and there's no need to define a fully custom format based on the old terminology but things can just be mixed in a single document compared to rdf to which this is also similar especially in the data model this is personable processable by constraint devices there's no ui process that you need and it's quite compact so just as we do in co-op with using option numbers here we can express all those predicates that are conceptually your eyes that describe the relation between the things that we describe and the data we described with is included in numbers rather than or is expressed in numbers that encode for uris still all those are similar so rdf and rc6690 link format have"
  },
  {
    "startTime": "00:32:02",
    "text": "now in the latest version defined conversions back and forth at least for for subset without the rdf the exceptions are relatively a minor with link format it's a bit um it needs a bit more thinking um link format can only be converted to coral provided the application uses the semantics that we use in core for the attributes because otherwise the values of the attributes in the link format document are not necessarily well defined or registered anywhere so it's up to the application to define what the attributes mean if the common attributes are used there is a conversion back and forth between coral and rfc 6690 defined at least for the subset of coral that can express 6690 with c-boards relatively straightforward coral can express literals and these literals are just the ones based in zebra because coral is encoded in seawater so you can just use basically any value of the extended data model and put it in there we've been meeting regularly in design team meetings and processed a few focused on a few components that we thought that we better make progress fast one of those was the information model this is now i think almost complete the information model is now twofold there is the very rdf-ish basic information model that just describes what is encoded in the file and there is the additional structure that can be put on that information that describes how this is laid out in the file so that a consumer a constraint device can process this without building the full graph graph and traversing that graph and instead transfer traverse a much easier tree"
  },
  {
    "startTime": "00:34:00",
    "text": "the interaction model didn't change too much this has already been in a very good shape a few versions ago basically describing that there is a user agent similar to a web browser that hops along this this graph or tree but may need to fetch a resource that it that it encounters gets gets information gets a representation of that resource that is typically a choral document again find that there is a link or form that it knows to be able to follow perform that until the program terminates or reaches a state where it waits we've changed a bit compared to the version that was presented a year ago how the dictionary is precisely defined and now seaboro has at least as a as an adopted item the packed seabor format that reserves a few seaboard tags that can be thought of as compressions of larger seaboard items and this this is basically what coral has been doing for a long time but now instead of using integers we use those tags and all the defined semantics for that what is not clear there yet is how we precisely we will set up the doc set up the dictionary based on which the compression happens because it's not like gzip or or said sdg or what or not that ship all the dictionary in the file but the typical situation especially for things like link format ish documents will be that the document format is declared as a as a media type with a parameter has that is all compressed into a single content format number and the consumer seeing that number will know to pre-populate all the dictionary with the typical items like resource type core interface"
  },
  {
    "startTime": "00:36:01",
    "text": "the host's relation and all those things that are common in link format documents we might also want to use basic packet format which is packed seabor's way of saying of defining that dictionary at the front of the file and we might also consider importing a named dictionary that is your state one uri or one uran and the application then loads a dictionary associated with that name into the current state what which of those we take will probably be guided by um [Music] by applications that we try this out on the binary serialization we didn't change a lot this is just something that will also to a later stage when we have a corpus of example cases where we can then evaluate does it make sense to spend a bite here or spend a bite there um but this is best done with a with a with a with a larger basis that will that i'll come to in the next step uh we will will have to obtain there used to be a text serialization that was very similar to turtle um that took up a large portion of the document and we've removed that for the time being um expressing sibor in diagnostic notation of of uh expression curl and the diagnostic notation of zebra is now a bit easier now that there's the edn extended diagnostic notation draft around which allows us to just write cris in in text uri reference representation and occasional examples are also expressed in turtle in those cases where we just need the c of triple semantic and not all the structure that is in the tree shape and we don't need to express whether something is um written down in packed format or just"
  },
  {
    "startTime": "00:38:00",
    "text": "uh just spelled out and there is the topic of how we do queries around this how we modify a document like can we use this with factual with post and how we describe where the data comes from which is related to reification of those statements that is making them into something that we can later talk about having come from this or that authority which is very very important to have but we're currently just chunking it up we're keeping those things in mind um but they are for later phases in the specification and some of those we might also refer to uh through a second stage when the basic model is already done so our next steps are to coordinate with uses of the of the most potential uses of coral and just check whether what we're doing is still aligned with them this is relatively easy for things like problem details where we're already using those documents for example and aligning this with with astf will hopefully give us a bit more insights because we don't precisely know how things like forms are used there at all from that we hope to get a corpus of items that we can then try the binary serializations against and also evaluate how which steps in the dictionary setup we need something that we could probably use working group input on right now is defining the the subset of features that we want to have right away especially when it comes to things like um patching or fetching is this something we should aim for in the in the current iteration or is this something that would be okay to ship um in uh as an update yep uh thanks for your time other questions do you have maybe comments right"
  },
  {
    "startTime": "00:40:08",
    "text": "away nicely no in the chat either audi says this is good stuff indeed okay if there are no other comments or input from christian thank you so much we move on to the next item that's esko for group communication for co-op hi esco we cannot hear you but we can see you okay i just need to unmute so it's okay i don't know yes slides right so maybe you have to remind me again for the slide so look on the top left the second icon from the left so the pdf icon and then you have to select the right one but you can have also preview in the selection window okay and i need to do it right yeah let's see sorry presentation view um that's not the right icon there or just uh yeah okay ask to share slides is that one yes okay it's loading yeah okay i see it at least now yeah it works there you go okay so i'm going to talk about group combis"
  },
  {
    "startTime": "00:42:02",
    "text": "now at version 5 already and let's briefly recap the goal of this document so i'll made it shorter than the last time so we actually have um working on normative successor for rfc 7390 which was experimental and addressed co-op group communication we obsolete that predecessor rc and also make some updates to co-op and observe and the idea was to have a new kind of standard reference document for group communication that also implementers can use the scope we cover various things all around group communication so not just over udp or ip multicast but more in general now as well and also mentioned latest features like observe clockwise and of course security that's also a major part of the draft so we now define a group of score based security besides the unsecured co-op group communication there's also now a bit more extensive definitions of the group types and how they relate to each other and also some guide guidelines for a secure group communication so now i'll go into quick overview of the updates we did for this version so the first thing we did was remove the multi etac option so at some point we introduced the existing e-tec option which seems okay for the purpose and is also simpler because it's already there and it has a simpler structure and to us that seemed enough so we don't need necessarily this this multi etag option"
  },
  {
    "startTime": "00:44:03",
    "text": "the second point was that a whole section was added 1.3 based on review comments to clarify more detail what we actually update or obsolete informal rfc so this is now all kept in one place in this section also in change was made in section two to one so this was about uh yeah how you basically can identify or name an application group within the group awry or more general within the corp request so we put this resolution for this issue also 28 in the new version but it's still pending some working group review and approval of course so now i'll just move to that so this is a little bit of an intermediate where i introduced this issue 28 application group naming so what we have defined in the draft is the application groups that could be [Music] named with any identifier such as a string a number entity number or complete uri if you have such identifier it may be encoded within the group uri so this top form is what we basically recommend it's kind of a usual way to do it so you have here in the authority there's go up co-op group address that's basically the co-op group to which the request is targeted and within the co-op group you can have multiple application groups still and this is group one shown in red here in the path of course you can encode the same information also in other ways so it's also shown how to do that in a query in different ways you can even put it in the host component in that case it becomes a part of the co-op group"
  },
  {
    "startTime": "00:46:00",
    "text": "because the co-op group is composed of the authority including the port you can also put it explicitly in the port number so then the group application groups becomes a number in that case it is also part of the co-op group because the port number is also included in co-op group so there's some overlap in that case between what is the co-op group and what is the application groups but these are always that the receiver can at least identify yeah the group but also the sender can identify it within the uri which is still not yet encoded in carb form this is just the uri as such and the final option we show is that it can also be not in the group uri so you can also say well the sender adds this for example in a co-op option and adds that to the request which is not something that's not part of the group uri but is still in the request you could also have it implicit so that means that the receiver has to figure out what is the application group or there is just one default application group in that case that could be associated to that co-op group so this is the part we clarify a bit more so it was [Music] not so clear from the previous version of the giraffe that that we could actually do all these things and all of this is of course application dependent how you exactly encode this you can do many different things here with co-op okay we'll just continue now with the updates um so three more updates we did was in section two two three we also had another open issue so that was also based on a review by christian about what kinds of group discovery are possible using co-op because we mentioned there is rd group discovery and we mentioned there"
  },
  {
    "startTime": "00:48:00",
    "text": "is also discovery of groups you can do with pure co-op so client-to-server without using an rd it was not so clear what what this really was and what kinds of uh discovery are possible so now we expanded this text here also make some examples of what you can discover with basically cop discovery and link format this is also pending working group review and approval yeah a second important change was the second point on the slides so we have stronger advice on unsecured group communications and are we just saying capitals it's not recommended and this was one of the open issues number 20 do so we hope that's okay for the working group to have it uh as a normative statement still possible to to do it but if you have ways of course to protect it then that is definitely recommended let's see i see your hands coming up yeah i just have a quick question when you say not not recommended does this mean that we are essentially deprecating the use of multicast for discovery i think multicast for discovery was i think mentioned as one of the cases where you uh often you do still need it so uh okay i think marco that's correct right yes we're basically saying it's not recommended if you do otherwise you'd better have very good reasons and analysis of your case and early discovery is one example we gave yeah i think this discovery needs to be qualified a little bit to reduce the potential for reflection attacks so you should say something that if you use it that way"
  },
  {
    "startTime": "00:50:00",
    "text": "make sure that the response is not much larger than the request but of course that also doesn't help if your multicast group is gigantic so yeah it would be interesting to see how how we actually position ourselves to with respect to that usage because it certainly was very popular when when the work on corp began uh on this point carson following jones review we also added in this version more content about the risk of amplification attack so that's discussed now also more than the previous version good thank you okay yeah that's right and yeah then finally uh that's the kind of catch-all improvement so we made some editorial improvements and fixes so there were some fixes in the description of the group relations and the diagrams we expanded explanations of forward and backward security just to make sure that it was clear what that meant in our case also the text that marco mentioned about amplification risk is also more extensive all right so i think these are the main improvements so i want to go back to this top one so number 29 just to give give an overview what this was about so here are some co-op multicast queries that you can do so a multicast request that's basically doing a query on dot well known slash core to discover something and of course all of this is very much application dependent there's no single way to do this it depends how the yeah if and how the groups are encoded as part of resources on servers so"
  },
  {
    "startTime": "00:52:01",
    "text": "in this case we assume that application groups are represented as resources and also these resources in the top example are located within a specific part so slash g something and then that something is the group name so basically this sends a query to the co-op group cg1 which could be uh it could be your uri that's resolved to multicast address it could be just a plain multicast address and port so this goes to all the cg1 members and this these members will basically be queried for uh application groups so and with star we basically have a wild card so any application group name should match here so the idea is that these application group members then will be reporting back their application groups let's see then the second example uh that's sending specifically a multicast to the realm local co-op group of all co-op nodes that's this ff03fd address so there it's querying a specific group so this is encoded in a slash group parent resource and the slash group 1 child resource is in there so it will basically find this specific named group or all including all members of that group who will be responding let's see the third example is to have application groups of a particular type again in the local mesh so that's the ff03 it's multicasting to the realm local address local mesh so this type is now encoded as a resource type rt attribute here in this particular example so the client knows that and we'll just query for anything with that rt"
  },
  {
    "startTime": "00:54:02",
    "text": "to get these these specific types of application groups back and there's one more example of this in the id so this is not exhaustive of course this is just to give some examples of how you can use this for discovery of groups and group members so both for application groups and co-op groups question i see i mean did you have a question yes well more than a question a clarification so all these examples in the presentation are excellent i'm just trying to look for them in the document and i i it would seem it's missing some examples so you could benefit from them yeah yeah i haven't done a thorough review so i apologize for that to begin with but just a comment it's true that they are in kind of in text there uh maybe it would help to also have these uh example uri so these are a bit more specific than what we wrote down i think yeah maybe like this classic request response examples thanks okay yeah that could be uh helpful there to add maybe add this particular example so now we have some text and you could have a yeah request response type thing added to that okay it's definitely something to consider yeah that's fine and let's see uh we can go then i think to the next steps of this so what do we need to do um so i think what will be helpful is some more reviews of the updated parts that we had in the last draft if all that is well we can close these issues maybe extend the examples a little bit more like we just discussed"
  },
  {
    "startTime": "00:56:02",
    "text": "now question is do we need more reviews of the entire document i copy-pasted here the promise from itf 108 so we have here christian and francesca reviews christian actually did a review because i got yeah at least a couple of comments from him so i'm not sure if you you know i wanted to do more than that but okay that's i just wanted to mention that i most likely won't have time to review this before it gets to me um after working group last call sorry about that okay yeah okay no no problem so uh yeah the question was also we can also do this as part of working group last fall that he started and then um this could trigger another review for example i think the previous review comments so there were a lot from john and christian so thanks for that these are now the rest we think at least so that's why we believe the version zero five may be ready for the working group plus call now okay and uh yeah that's it for this time thanks for your attention in case you have some questions let me know just to relay from the chat that christian will have another look too okay very good yeah to see if we uh solve the issues uh in an understandable way basically yeah any more questions comments to esco okay thank you then thanks we came up to the next one which is"
  },
  {
    "startTime": "00:58:12",
    "text": "all right this is an update of the group score document we have submitted version 13 before the cut-off and the version before that was uh indeed a major revision based on comments from working group last call and some follow-up comments uh in comparison to that this version uh comes in with uh much simpler updates and to start we updated the terminology to be aligned with what the addock draft in the lake working group is doing in naming public keys as possible uh credentials uh so now we are referring to ccs uh so it's sufficient to refer to rfc 83 92. uh while roof reading the draft again we noticed there was an oversight when defining the the key derivation of one particular key the group encryption key which is used to derive a key stream for encrypting the signature in group mode so that is fixed now and it was about the exact key size of that key which which is exactly the same key size used by any other symmetric keys for the encryption algorithm in group mode so that's also fixed uh other than this the the major update in this version was instead some more specific text about what is uh mandatory to implement uh there was an issue that john opened about this i think even two years ago and thanks to this update uh it's also resolved now um it boils down to what we can expect constrained devices to support especially when it comes to this signature algorithms and and companion"
  },
  {
    "startTime": "01:00:02",
    "text": "key agreement algorithms for the paywise mode and we are fundamentally taking the same rationale uh used again in the addock document in the late working group adapted to the grupos core case it reads pretty much like this for the sake of the group node we expect non-constrained devices to support uh both the eddsa algorithm and the ecdsa algorithm while we expect constrained devices to support at least one or the other at the end of the day to support as much interoperability as we can recently have um just as a parallel thing the pairwise mode follows the same rational so that we expect non-constrained devices to uh implement both key agreement curves and constrained devices to to support at least one and this can probably be relieved and lived better in the near future as more algorithms are supported also in hardware but it seems a reasonable thing to do for a time being and that was it about the updates to this version and some of the still open issues were actually already addressed in the previous version 12 but we waited anyway considering that we had these small points uh to still close so at the moment uh we are not aware of any other uh open points or issues and we have also updated our implementation for californium aligned now with the latest version 13. so we believe this version 13 is now ready for a second working group last call and independent of that we are also starting producing uh test vectors uh taking as a starting point the ones of the oscar rfc but"
  },
  {
    "startTime": "01:02:02",
    "text": "we expect this ones to be a bit longer uh since we want to cover the group and the pairwise mode possibly and the combination of the different uh signature and key agreement algorithm uh so no need to find an answer to these questions now but but just to to raise the point that at some point we'll need to decide um where the best place should be to have uh test vectors because probably they'll be too long to be included in the main draft uh it can certainly start as material in a working group repo but then we can possibly consider what is happening uh about these vectors in the lake working group meaning it can be a separate informational draft and if so we can think if it is also worth a specific publication but i just want to raise up the point for the time being we are on test vectors and joran hello can you hear me yes yes so how is the thinking on lining this draft and drafting the previous presentation and are we going for independent working group class calls and i mean at some point they will align i suppose since this depends on the on the other so content-wise they are aligned and both mature i believe i think it's easier if they proceed uh to work in group calling parallel uh certainly it's easier i believe for the isg to receive them together okay fair enough and and the other draft was also about to come to its first working group last call or that would be the first one yeah okay i quickly saw francesca in the queue but yeah i just want to say i will move them"
  },
  {
    "startTime": "01:04:01",
    "text": "forward together when they reach me yes yeah that's all from my side we think this is ready for consideration for a second working with us any comments questions okay i seen on the chat either then we can move to the next item oh hi matt no i'm just going to say that maybe we can start the second working group last call already in the main list this week if that's okay what's for me all right uh i think we we can do that um how long would it take to have the test vectors ready by the way do you have any an estimation yeah probably it will take a while before having a full set but i hope by the end of the year we can have a preliminary set realistically right but again they are probably an overkill to be included in this exact document see that as supplementary material and then we can decide later on exactly how to release it in the best way and also could we get already in this uh meeting an estimation of how many people have read the current version maybe not the current but the one that is a major update the previous one um just to you know see some participation there don't know what is the right way to do it but i think a plus one on the channel will be sufficient or if you want to"
  },
  {
    "startTime": "01:06:01",
    "text": "read one of the last three versions yeah or you can join the queue as well just to see how we are we are in the group so i see christian and click card right and also i mean for the second working place called we need more uh eyes on this uh do we have some volunteers already to have a look to the latest version let's go okay for the mini takers please remember to write the names there okay any any other volunteers all right thanks thank you yeah i'm done here so next in the queue is record yes hello let me share the slides loading now yep right yes so i will be presenting this uh work on a key update for os core uh also now called kudos as a short name and these are uh yes let's start with a short recap so first of all what this is about well os core uses and the algorithms for providing security and"
  },
  {
    "startTime": "01:08:00",
    "text": "there is a c4d document which is referenced here in this slide which defines the fact that you need to obey certain limits in terms of key usage when it comes to amount of encryptions and number of failed decryptions and if you reach those limits you should rekey because extensive use of the same key can enable breaking security properties of the aed algorithms so basically this draft has two main parts the first part is the study of these limits and their impact on our score which means among other things that you we we some appropriate limits for our score and yeah for a variety of variety of algorithms we also define counters message processing details and practical steps to take when limits are reached so it's about the limits and yeah what how you should change the message message processing in our score to take this into account which practically means counting uh key usage and we also took into account by the way input from your matson on the april core interim and we also got very recently some further input from him which we will be taking into account the second part is about defining a new method for a keying oscore which we call kudos now we discussed in the previous meeting if it should have an actual name and now we we put the name kudos and this method is loosely inspired by appendix p204s core what you want to do is renew the master secret and master salt and thus to get new send and recipient keys practically rekey in your context and also this method achieves perfect forward secrecy going into some details and updates on the key limits so first of all a recap again on the scale limit so again it's discussed in this e4g document you need to limit key usage for encryption which is counted as the you"
  },
  {
    "startTime": "01:10:00",
    "text": "know a queue parameter or q variable and invalid decryptions which is the v variable so basically what this draft does is defines fixed values for q v and l and from those values they you calculate these c a and i a probabilities which is the confidentiality and integrity advantage um which is basically the probability of breaking these properties of the algorithm so um what you want to see is uh set qvnl and from those calculate acceptably uh acceptable values of cn ca and ia and yeah we also added some text now now i go into some updates from the last um from the last version so what we did we added explicit mentioning of the fact that now when you send an oscore message you have to obey the l value which means you have a practical size limit to the amount of data you may send um since the l is basically the message size per uh the message size in cypher blocks and you should not exceed that and we have some text there specifically on how you can easily calculate that we also now after suggestion on christian have a table showing the values of l not just in cypher blocks but also in actual bytes and continuing on so what we did was we have this table where we show the iinc a probability for a number of algorithms and these are all algorithms um except as128 ccm8 so basically we deal with as128 ccm8 in a separate table and these algorithms are the ones that the c4d document defines formulas for so we set qb and l fixed values from those we calculate the probabilities and in c4d document they mentioned aiming for a probability of c and i a"
  },
  {
    "startTime": "01:12:01",
    "text": "that is to be lower than 2 to the power of minus 50 and as you can see in this table we are within that safe margin so our probabilities are in fact even lower than 2 to the power of minus 50. and so that brings me to this red line here which says that we do intend to increase q and l further because we do seem to have some margin for doing that as we're still way lower than 2 to the power of minus 50 in terms of the probabilities so there's an open question if we should proceed with actually increasing these values and yeah please jump in if you have any comments yeah see christian um listen i'm just i think l would if if we can uh increment l by at least one um one power of two then full messages can fit because one um 1000 1024 byte is enough as a payload but the whole message will be longer so if that's the limit it's kind of impractical because it limits block wise to under fi to 512 byte blocks so you would like l to be like 2 to the power of 11. yes yes that's definitely now these these values are these l is in blocks in cypher blocks then scratch that okay let's catch that yeah we don't present it here but if you check the draft we do have a table now which shows the actual l value in bytes which is then of course depends on the algorithm but check that and we have an actual table showing that information uh yep and so basically here's the table where we deal with aes 128 ccm8 because that's a bit of a special case where um [Music] you need to treat that separately because um here you end up with quite a low or less"
  },
  {
    "startTime": "01:14:00",
    "text": "it's like quite a high ia probability so we chose custom values for as128 ccma to try to optimize uh reasonable values of q e and l because if you have l 2 to the power of 10 here you know you couldn't have a very high q or v so here we have l 2 the power 8. like you see the green arrow where is which is the recommended values that we have currently set alone um and of course here's an open question is it ideal to aim for a ca and ia close to 2 to the power -50 as they mention as a proposed goal in the c4d document that's the current approach we're following yeah then i go into updates on the key update procedure so we did define a new method now for a keynote score which we call kudos key update for score so essentially the client and server exchange to nonsense r1 and r2 and you have this update ctx function that you use to derive new contexts using the nonsense and practically you start with your current context you have one intermediate context and then you end up with your new security context and we also managed to get a number of beneficial properties here as you can see in the list you can initiate by either client or a server it is completed in one round trip and after that you can use the new context only one intermediate security context the id context doesn't change throughout the procedure in contrast to append xp2 it's also robust and secure against the pair rebooting and compatible with prior key establishment using the addock protocol because update c update ctx can actually use the dead.exporter if your original context was built using adhoc and by the way i have this red box here"
  },
  {
    "startTime": "01:16:00",
    "text": "so what we did is also extend the score option [Music] with a new flag bit and a field called id detail where we practically exchange the nuances some other updates we did so first of all we no longer include r1 in the response one for the client initiated rekeying and this is just like how it's done in score appendix p2 because it's simply not needed here as you can correlate the request one you can correlate in response one with request one using the code token so correctly you don't need this extra or one value in the id detail other updates uh we had recommendations now on minimum length of r1 and r2 which are used as nonsense and motivation is similar to what it is written in appendix b2 and as things stand now we recommend minimum eight bytes and here's an open question if this is sufficient or not we also discussed a bit now on observations and our conclusion currently is that you must terminate observations after a keying because uh basically you don't have the cryptographic binding between notifications in some situations and there is possibility to keep these uh by paying a price and the suggested solution we have here is that what you can do is um if you have ongoing observations basically after every keying you need to jump your partial v and the sequence number uh to higher than the maximum partial av for any ongoing uh observation you have and of course the drawback here is you have very big jumps in the partial av which means faster consumption and larger communication overhead so there's also some possibility for more"
  },
  {
    "startTime": "01:18:00",
    "text": "complicated solutions like reserving some pivs in a bitmap but for now our proposal here and plan is to not keep observations after a keying because you can basically reestablish the observations and keeping them seems to have many drawbacks here but again this is an open question and any feedback is welcome yeah we added a use case related to six dish and the i see carsten is in the queue um if the the observations are going away this means that the reaching event is visible on the application layer is is that something that that we should be doing yeah that's uh yeah so basically right then the application would have to probably then take the responsibility to re-establish these observations um yeah that's it's open if if yeah of course that's that's one drawback of terminating observations yeah um so yeah trying to understand how bad this this piv advancing thing actually is yeah so it would be basically like yeah as we stated in the possible solution so right after a keying you would have to check the highest partial av among all your ongoing observations and then when the client starts a new observation you need to jump your your sequence number to that highest value plus one uh so of course it depends on the scenario and and how you know often and much this client is using absurd but in some situations it would mean then big jumps in the partial iv and a lot faster consumption and then larger"
  },
  {
    "startTime": "01:20:01",
    "text": "communication overhead um so yeah it depends i would say also on the on the way that application is acting and how much it's actually observing um so but also by the way one thing we thought about here is i think the way we have to deal with this is that the two peers we basically have to define how this works here in the in the text because the two peers have to agree on what to do and there's not really an opportunity for negotiation or choosing on your own so it should really be stated in the text what to do with observations um so the question is this drawback uh big and you know too big of a drawback and secondly are there any other possible solutions to in such case keeps observations with some smaller drawback so this would definitely be good for further discussions thank you for the feedback i see you're on also um yeah so just question for carson if if if you need to update keys for some other reason uh that are you is your sort of proposal that it should not be visible to just happen or what's what's the mind what what would you prefer would it be that this is controlled or it just happens without without any notice well the the the application probably should have a way to to ask the security component to re-key but what what's in here right now means that each time the security component just just based on the number of messages that have been sent decides to re-key it needs to involve the application doing that"
  },
  {
    "startTime": "01:22:00",
    "text": "and that's a bit inconvenient i mean it's not a disaster but um yeah you have to send all these messages then yeah you want to send yeah i i it's inconvenient and we and definitely a good point for consideration but on the application level is the loss of an observation really that visible i mean the application might be requesting from the from the co-op stack just to get to get a continuously fresh representation of that resource and the co-op stack might do this by polling or by establishing an observation and in such a situation the co-op stack noticing that the observation goes away with a good reason would just re-establish the observation yeah i see a comment in the chat also christian so that's a good point but yeah this is definitely worth i think for discussions and considerations uh yes i'll proceed then so again we added this 60 use case and you can read this i will speed up a bit but you can read about this in the draft but essentially one big benefit of this new procedure is that you preserve the id context which is used as a pledge identifier in 60s 60s or rather you have a binding between the context and the pledge identifier so okay so next point here is that you we now update the rfc8613 which is those core rc saying that this is deprecating and replacing its appendix v2 and yeah just check here if this is okay and if anyone has any feedback if this is okay to do uh we have some more general updates improved the table of content um some editorial improvements formalization with adhok"
  },
  {
    "startTime": "01:24:01",
    "text": "ayana considerations we updated the title now to be called key update for us core kudos and yeah that's also an open question any feedback on the title now the title of course mostly considers the key updates section of the draft and not really the limits so any feedback on that is welcome then we have some next steps uh yeah addressing some open points we have a number of issues on the gitlab repo we need to look at material to save to disk to support rebooting uh applicability uh applicable considerations from score appendix v2 updated security considerations and then we want to further refine the key limits as i mentioned earlier so we do feel that document foundation and the key update protocol are stable and by the way yeah we do have plans to actually implement this in code also both the limits part and the key update procedure so then we wanted to here propose a working group adoption of this draft because we feel that it's it's at that stage now and it's ready for that and yeah that's about it thank you it was a bit over time but thank you for the for listening and thank you for the feedback [Music] so uh when you say workshop adoption in you mean uh within this week or even in this meeting right and and the question also goes to marco which is a co-author i'm just asking because if asked qual for he cannot maybe call the working adoption right now right right okay"
  },
  {
    "startTime": "01:26:00",
    "text": "so right so well actually similar processes before i would like to know first on the chat who has read a current version of the draft other than the authors i assume john and others have and i cannot see the chat okay joran and christian right and uh well then i think i think we could do a a working group adoption on the on the current call and then take it also on the main list let me just i'm trying to use now the show at hands tool to see let me phrase the title here of the question so um something like uh is the first time i used this actually sorry for that who thinks the draft is ready for adoption [Music] right is that a good way to phrase the question by the way who thinks the draft is ready for adoption and then just click yes if you agree and uh i guess you can also uh maybe let you put it in uh race and if i agree just to avoid confusion and you should uh have it on the on the there is a session going on i see seven raised hands so just remember we have 43 it's not bad but we don't worry as much about the numbers because in previous sessions usually like there is a small participation compared with the amount of participants on the call"
  },
  {
    "startTime": "01:28:04",
    "text": "eight nine okay think it's a a good uh show of hands so let's keep it in the minutes that we have got the caption screencap also the number of jabber and reject global differences will be three the number of actual media echo participants is like 28 or 27 or something right correctly just for information they count here is really misleading yes i agree with that because we have so many java participants uh i think uh anyway in the session now i think it's a sufficient number uh for i mean previous documents we have adopted them based on that and anyways we have to confirm on the main list so i'll take care of of uh sending a message to mainly later after this and um thank you yeah thank you thank you richard [Music] okay uh next is christian with cashablascore yep next time i start with video and audio again so that i can speak while i'm looking uh this is an update on casual oscorp which has last been presented during the interims today i'd like to focus not so much on the mechanics which are slowly maturing and basically working but on a topic that came up during the work on the current dash of three draft which is more along the lines of um what does this do precisely in terms of request response binding"
  },
  {
    "startTime": "01:30:01",
    "text": "so what i thought i could do and i'll walk with you through the the steps there is split this into how do we obtain a request response binding north score in general how and in particular how do we obtain requests and response binding when we do not have source authentication for the request and then building on that how do you how do we obtain casablance core responses i originally i was afraid that this would be a very very large change to the document um i tried it anyway and turned out that it hasn't been that large of a change in terms of text in terms of semantics um probably it still is so this is why i'd like to focus on this part one today just to set up the stage in the context um in auscore we get request response binding by repeating the sequence number and the sender id in the response not in actual text but as part of the additional authenticated data so for mismatch to happen there and that's one of those co-op attacks that oscore is designed to prevent the client would need to send a request r1 but the server would need to process a different request r2 that match on those details that are put into the aad but in regular auscore this can't happen because the key is shared between those two parties the server verifies that the request is coming from the client so either the client would need to maliciously um send to requests which doesn't make any sense because the client that wants the rebinding to be there or the server would need to lie and the server can lie anyway because it's the authority on all the things here in grouposcore things look a bit different in grouposcore there is shared key material for the smac for the symmetric um encryption part so any other client"
  },
  {
    "startTime": "01:32:02",
    "text": "other than our particular client c could have done the encryption but there is the signa in in group group mode and in um in group mode and in powers mode there is source authentication because either the there is a shared key used between client server that only those two parties know or there is a signature by the client so again the server can know that the request was originated from the client process that information into the response and then the client can understand that response knowing that the server knew who sent it but this also means that anyone on the wire who might be member of the group cannot trust responses at all unless it also trusts the party that sent the the client the original client see that sends a request which is not generally the case so we are usually assuming that there might be malicious members in the group which may then read responses but they cannot spoof answers from from any other group member so in general um a third party cannot use those at all because it doesn't know whether the client might have sent one request to that trusting to that third party but another to the server and then the server center response and then those don't line up anymore and this is in retrospect the very situation we were finding ourselves in when defining a group of current sorry catchable law score in the first place but it might not be the only situation so how can can we salvage this we can um as done in in the non-traditional responses just uh send the full request in in the response again which we want to avoid because that's"
  },
  {
    "startTime": "01:34:00",
    "text": "with all about saving bytes here so repeating a request is is something to avoid we can send a hash of that request or we could just try to make the hash of that request part of the additional authenticated data that is um that is um verifying that the server you saw that request without actual actually uh transmitting that previous versions of uh cachable oscore had that information in the in a modified external aed currently we're more leaning towards putting that in a class i option and then not necessarily sending that class i option but so one way or another that information needs to wind up in the in the in the aed and thus really it's not really it's not replacing because the parts are still there but practically it's augmenting the request response binding mechanism so now the client even not necessarily the the sorry the receiver of the response even not necessarily trusting that the server was able to perform source authentication on the request can be sure that this response is a val is a response created by the server for this particular request and this deals with all the parts of cachable oscore that are about how these two things match together and some of the even some of the security kind of reviewing the the security caveats that we are having in case of law school one of them just uh goes up to that original just goes into that into that half so the the statement that we don't get freshness with cash of law score is really a more general statement the statement should be that whenever we have we don't rely on the original request response binding but just some additional request response binding we lose"
  },
  {
    "startTime": "01:36:00",
    "text": "freshness and then with deterministic if we use deterministic keys we need additional considerations for how do we arrive at the deterministic request what key material do we use to protect that request and what does that mean for the request privacy but the request response binding is probably a somewhat different topic and that could be even useful for other cases so one example and actually the example that that triggered all this is that group of core used to have an appendix in which it was described that under certain circumstances it might be okay not to verify the signature on a request [Music] now that appendix was lacking a few security considerations and eventually scratched which is a good thing to have happened with with the text that was there but it does leave an open use case that is when you have a group where you're just doing requests and the client does not really have an interest in doing uh doing an asymmetric signature on a request and on sending 64 bytes just to send a request that really any of the other clients could just as well have sent and where the server might not even for its regular operation depend on source authentication for example because any client might be eligible to use that information or just because the request is uh the handler is side effect free i mean it's it's a get there's nothing wrong in um encrypting the response and um and sending it to a party that can read it if it's if it's the authorized party and if not it would just not be able to process it but this this needs very careful considerations for um for request response binding and i think that all"
  },
  {
    "startTime": "01:38:00",
    "text": "the tools that we set up in cash flow score can do this and thus not only benefit cashable oscar by becoming more readable and easier to easier to verify but also open up those new cases so for me the main questions for today are is this a split that you would consider useful and if so is this something that is useful to be in the same document i think so i think it can be meaningfully presented in in a document doing really two parts describing request response binding in absence of source authentication group communication and then building on that doing doing cashable loss core but i'd like to hear your opinions here too you're on go ahead yeah i actually actually don't have a strong opinion here i'm [Music] i'm i'm i'm really happy that that this there is actually a solution to to the this cachable problem and um i think that we should prioritize to make that as simple as possible to to use and whether it's useful in the in other instances is lower priority in my my view okay so but yeah that's my input any more comments or input for christian neither in the chat okay"
  },
  {
    "startTime": "01:40:01",
    "text": "and thank you for the presentation christian and next is oscar capable proxies yes okay so this is a relatively uh new work but not a super new idea it was also presented uh some interim ago earlier this year and to recap you may have a proxy deployed between a client and the server and your use case we have examples may need the security association between the client and the proxy and it may be very convenient that that is based on all score especially if you're already using all score end-to-end between origin client and server and you end up into something that right now it is not defined or even forbidden in your score rfc where oscar is something only for region client and server and definitely you cannot have something like nested oscar where you protect the same message multiple times they started in the draft providing in fact the first use case group comproxy and then it was agreed to just move move this content out of that because it was a more general applicability and to have it in a separate document which is this one as a recap of main use cases also described in the document yeah group comproxy was the first one you want the proxy to identify the client before forwarding a request over multicast to a group of servers uh second one in a sense also related to your communication you may have a server sending multicast notifications to a group of clients all observing the same uh resource and if you have group score end-to-end and the proxy deployed clients are required to take an additional step which means providing an"
  },
  {
    "startTime": "01:42:01",
    "text": "additional ticket request to the proxy to make things work and that exchange among others is better to be protected for instance with oscar again and then another use case comes from the lightweight endpoint specification where um evoscore is used uh the lightweight one client may want to use it also end-to-end with an external application server uh using the lifetime tom server as a co-proxy basically and and here you have allscore both end-to-end with the external application server and uh internal your hope i hope if you want between lightweight and client and server so the contribution of this document is trying to update uh the oscar rfc in defining also intermediaries as possible um oscar endpoint so consuming the oscar option and an oscar layer and then in that meeting also a double triple uh as many as you want layer uh protection on sap message we did have a limit of two layers at most in the previous version of the draft and it was lifted out of feedback we got and yeah we really consider oscar but what you see in the document can be applied right away also for uh group of score so for version zero it was presenting also at an interim we got some early comments from from christian and joran that were uh very constructive and essentially suggesting some more use cases uh lifting the limit of uh two at most protections of the same message and their main feedback was uh that the the message processing description and notation of version zero was way too complicated and uh requires some uh restructuring uh that we did um as to the use cases we mentioned also the case of a cross proxy acting as third party service for the sake of transport indication which is"
  },
  {
    "startTime": "01:44:00",
    "text": "also christian's work a proxy as a traditional big firewall as an entry point of a network uh that is required to uh identify the exact nodes joining the network and then we started to think but requires much more uh elaboration uh a case where you have a long chain of proxies and you really want to hide that position in the network of the client from most of the chain elements and the final origin server building something like let's say on your oscar we have to think more about that though the the core idea is is already defined in the document and coming to the main uh point in in christians and uranus feedback uh instead we got rid of the very complicated notation and two fine-grained message processing step and we came up with a general algorithm that is applicable right away to any endpoint in the chain so a client an intermediary uh or a server now we also say explicitly that we are not defining any explicit signaling of what is happening and we don't need to so basically the the presence possibly in combination of certain cop options is just sufficient for an endpoint to understand exactly what is going on and and what to do and a main deviation from the oscar rfc uh is that an endpoint shouldn't panic anymore if after the encryption uh anal score option is is still there because that just means one more oscar layer to strip and some options uh have to be uh protected or to be treated as class e if you want uh unlike the scorer fc and this includes in fact just corruption and options intended for a proxy so with this in mind it's pretty easy to uh protect the request just applying"
  },
  {
    "startTime": "01:46:01",
    "text": "uh the oscar layers one after the other um typically using as first one the one shared end-to-end between the origin client and the server things get interested and here's the generalized algorithm when you think of an incoming request and based on the new text we have it's really about evaluating which of these conditions apply to a request so it includes proxy options well forward to the next stop towards the server if this is a proxy at all if it doesn't have proxy options and it doesn't have just corruption either delivered to the application if there is any at all uh you are in kc otherwise meaning you don't have proxy options but you have an oscar option and then well use the recipient context pointed by the oscar option decrypt take the result and own it assess which condition applies again and eventually you lend to case a or b for forwarding or delivering to the application omitted in the slides but of course we have also error randomly covered already for responses it is easier for a responder it is really about reapplying the same security layers successfully stripped out from the request but in reverse order and the recipient of the response is expected to find the same security layers it applied but this time in reverse order and at most as many as it as many as it applied in the request so this is what we have now and again the main update was generalizing the procedure to admit right away a long chain of intermediaries more than two layers if you want to and to have a much simpler and easier to read message processing as a general"
  },
  {
    "startTime": "01:48:01",
    "text": "algorithm uh we have still a lot to do we want to add examples uh considering caching but that should be possible to have just using the cachable score proposal that kristen presented before and we want to to elaborate a bit more on a use case where having more than two layers per message is useful and this is perhaps a bit longer term but we want to look into rfc 8824 that define another compression for co-op also for the case when all score is used and maybe not as is but uh we we think that approach can be possibly uh adapted a bit to be used also in the case where a message is protected with multiple layers so that we can at the end of the day reduce the hover and in this case too this is the plan for version zero two but until then uh comments input are very welcome anyone esco is already suggesting a name uh matryoshka probably would think about it we were trivially thinking of oscar and oscar on nested oscar but why not wow unexpected okay we'll seriously consider well you need to code acronym maybe then if you want to do that yes okay i i'm actually done and we entered the flex time just in time for the last presentation thank you all and now it should be about giuseppe hello hi i can hear you okay good"
  },
  {
    "startTime": "01:50:02",
    "text": "uh i have to present by myself right yes if it works for you yeah [Music] perfect okay yeah hello everybody this is a new work about the core performance measurement option that we have proposed as a zero zero version for this meeting yeah first of all let's explain the motivation for this work so [Music] um you you are of course the expert on cope and you know that there are uh two modes reliable mode and reliable mode in case of reliable mode reliability is provided with acknowledgement so because the message is marked as confirmable in this case we can say we can think that if we want to implement some measurement we can use the message id and acknowledge to identify the packets and measure rounded time to verify losses this can be done in case of reliable mode in case of unreliable reliable mode of course this is not possible and this is no easy way to do measurement uh round-trip time losses delay measurement in any case even if we are in reliable mode it is resource consuming to read id sequence numbers store timestamps for each packet so and since we are in a constrained environment with constrained nodes this is not the the best way to implement measurement in constrained environment so we need to find a simplified mechanism"
  },
  {
    "startTime": "01:52:02",
    "text": "uh yeah i go to the next slide so uh it is not new in itf there are already some mechanisms that are also called explicit flow measurement this is a draft that has been just adopted in ippm working group and the techniques described this draft employ a few marking leads inside the header of a packet for loss and delay measurement in particular i want to start with the two uh idea one is the spin lead idea this is already optional in quick protocol so if you go to the rfc 9000 uh there is the one bait that is dedicated for spin beat so distinguished idea is to create a square wave signal using a lead and the length of this signal is equal to rtt so you can understand that in this way this is very straightforward to measure the delay because you only need to measure the the length of a square wave so you don't need to take time stamps so you need to take 10 steps but only for a few packets not for every target on the other end there is also another another technique that is based on square square bit also in this case the idea is not new so it is also known as alternate marking methodology defined in the rfc8321 uh also in this case we are talking about square waves but in this case the square waves is is made of a fixed number of packets in this way these fixed numbers all packets can be recognized between the client and server and you can measure the losses just to give a quick view of these two"
  },
  {
    "startTime": "01:54:01",
    "text": "methodologies so what we are proposing now we are proposing thanks to the extensibility of co-op the draft aims to define a new option carrying these performance measurement bids in particular we can start with spin beat and square beat that are let's say the the most stable solutions because as i said spin bit is already defined for quick while square bit is already defined for ipv6 for srv6 so there are already a lot of proposal about the alternate marketing application so the proposal for this option is quite simple so we introduced two kind of patterns one that include only the square bit the other one include the square beat and spin beat in both cases we have added let's say to complete the byte available we have added some event bits that can be can be used for advanced users who usage of this option uh what are the key points and benefit of this solution so the first one as i said is that no sequence number no ids no sequence number no um time stamping for each packet so there is an easy way to measure ftt and delay and the redundant time that fits well with the requirement of constrained nodes there is also a proposal to improve the square bit mechanisms to find the synergy with spin beat in order to need the methodology simpler but i don't want to to to explain this here because it is it is mentioned we didn't draft and"
  },
  {
    "startTime": "01:56:01",
    "text": "maybe i can we can discuss on the list or i can i can present during the next meeting if the idea is of interest of the working group and once you are able to to to do performance measurement you can also think about possible advanced usage of this methodology for example uh an impact observer that can be a probe a a2ar proxy can use this information to to adjust for example protocol parameters or to decide whether to use reliable or unreliable message transmission based on the conditions of the network so you can also think about this kind of advance and music so yeah the next step the draft is based as aside on well-known methodologies that we had this idea to extend in this kind of environment because this methodology can be easily extended to to constrain the environment and considering that iot machine to machine devices keep growing nowadays the also the performance measurement aspect and something to be considered for an enterprise for a network operator that want to use co-op protocol in in the constrained environment so okay it's just a proposal for discussion so we welcome collaboration quests and comments on this work so that's all thank you christine go ahead um presenting this here you mentioned in the also in the response to previous communication that you would and here again that you would like to use this across proxies so that"
  },
  {
    "startTime": "01:58:03",
    "text": "that the spin bit would kind of be set by the client and then we are not modified by the proxy and then sent to the server or other values of that option could you briefly describe which parties would all need to cooperate here because i understand this to be coming from a telecom background and kind of would would this be implemented on all those devices and which components would then can can you give a concrete example of of which devices interact here in a proxy situation because my impression is that when proxies are involved it could really be linked local and then we could and then things like carson suggested use of mid could come into play but first i think we should understand how you intend this to be used with proxies yes um i i mentioned in in the reply that these there are several way to use this methodology because since it is based on um on the client server so it's kind of applicable to to the session so for example if the proxy is going to start a session on the iphone of the client of course the proxy can implement can be of course the the client in terms of uh in terms of the measurement so it depends on on on the situation that we [Music] uh we are we want to monitor so i think for example i based on your comment i think that we have to further clarify the user scenarios that we have in mind in the next version of the draft in order to say okay if there is a proxy the the way to do measurement is that if there is no proxy client server can"
  },
  {
    "startTime": "02:00:02",
    "text": "implement the measurement so we have to distinguish between the different the different situations so thank you for coming because we can we can surely improve the the next version of the document thank you giuseppe we may have time for one more quick question because we are really on top of the hour any more questions or comments i hear none and seen on the chat so we can adjourn the meeting thank you very much for participating today and for your hard work and join the rest of the itf weeks see you all in other town thank you thanks a lot to the new takers thank you thanks bye so you"
  }
]
