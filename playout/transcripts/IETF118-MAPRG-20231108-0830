[
  {
    "startTime": "00:00:26",
    "text": "Is this There you are. Okay. Good morning, everybody. And also hello to everybody remotely in all kinds of time zones. This is the map RG session at IETF 118. My name is Mia Accuteman. I'm sharing this together with Dave Bunker. Safe, Morning, folks. Okay. Let's move to the next slide and get started. This is the note well. This is an IRTF session, but we have a note while we follow effectively the the same intellectual property rights"
  },
  {
    "startTime": "00:02:01",
    "text": "As in the IETF. Next, This session is recorded. And I think even live street, please be aware of that. If you go to the mic, we wanna say something And also very important, we also have, privacy conduct and the code of conduct. So please make sure you're familiar with that and you follow it. You'd be nice to each other and we have very interesting and nice session and discussion here. Also, something to keep in mind is that this isn't IRTF session, a research group session, and not in, standardization session, in the IETF So there are a couple of differences, in case of Mabrache, what would you today is mainly have some very interesting research presentations a little bit of discussion about this. This slide is mainly giving you all the links to all the information you need have a mailing list, please also sign up to the mailing list. Feel free to use it also. Any kind of information sharing question. There's not much activity. But feel free to do that. And If you're remotely, please use meet echo, And if you are in the room, please also use meet echo to sign in So we have some numbers about how many people are here so we can get the right room next time and we can follow the is please sign in to meet echo And we also use use meet echo for the queue, of course. Again, this session is recorded. Please be aware of that. And hopefully, you know, by now, how to use meet echo to join the queue. And then I have 2 quick slides here, and that is very hard to read, but it's mainly for you to look up in the proceedings. The I am seeing the Internet measurement conference, which 2 weeks ago. And as always, they have a lot of interesting papers. And this time, particular. They had a lot of papers that were looking at IETF Protocols. We did actually invite some of them, so you find some from some IMC talks, which are not on this slide set because they are invited today and give a talk."
  },
  {
    "startTime": "00:04:04",
    "text": "But I also encourage you to look at the, the slides or proceedings of the IMC and look at those talks because and papers, they are really interesting one. For example, about quick congestion control, That talk also was yesterday in working group and transport. There's something about mail, there's about security. And on the next slide, I think there's more stuff about DNS for sure, and BGP and all kinds of things. So please have a look. It's very interesting. And that's, already my intro part, we have a very full agenda today. And that means we will start right away with the first and you can come up here Possible the presentation slides Yeah. Hi. My name is Rollins. Yes. So I'm presenting work of colleagues, so I wasn't involved in that work. I am just a proxy. I have to select this again. Is it that one? Let's see. it is. Okay. So this is actually work, from Michael Oliver and There you know, I'm just a presenter, as I say. So I try to do my best to answer any questions afterwards. So, quick, worst standardized. But it also said that It's meant to be a general purpose transport protocol, not just for let's say transferring web content, right? And so, the Michael actually try to use the flexibility of quick in order to do some experiments. With, different congestion control algorithms. And he found out that"
  },
  {
    "startTime": "00:06:00",
    "text": "actually while using it in a 10 gig, per second test bed, The throughput performance was actually quite poor in that sense that it 1 of the, implementations only delivered a two 100 megabits per second. Which is down there below on that graph on the right hand side. So there is related work, looking at click throughput. I mean, that's the reason, for the industry to use quick so enhanced advantages, right? But, primarily, this work focused on the latency and flow completion times and only a few other, prior work looked at the sustained throughputs in high bandwidth environment. Okay. So, the, the test bed setup is very simple. We have a sender. We have a receiver. All Linux boxes, all the same configuration, And there's a software switch, which is able to emulate, delay with and loss. So that's a very simple basic setup. And so this is not about real world measurements out there. It's a lap. Experiment, more or less. Yeah. So what was evaluated, Michael choose, to have 6 implementations. Because they come with they're perf clients. So they have a traffic generated on board. Which can be used, and the assumption was that implementers know best how to interact with their own implementation, right? And so for comparison, TCP and UDP was, used as benchmarks, so to say, using IPerf 3 and NetPerf. And, no matter whether, TCP or quick"
  },
  {
    "startTime": "00:08:01",
    "text": "our cubic was used as, congestion control algorithms. So, the baseline is UDP here. And as you can see, it's able to more or less saturate the link. So that's what you can expect. To get through so that it's fine. TCP, is also more or less, able to saturate the link. That's also nothing, usual. So this is showing the average throughput of a single flow, right? But the average but, just over 10 runs. Each for 30 seconds. And now quick, at least the implementations that's were under test, showed, a performance difference here. So the fastest implementation was Quinn, showing, some performance gap compared to TCP actually, 8.22 gigabit per second why the others are quite lower. You can see that, for some move fast is, around 2.4 gigabit per second. So there's, ball a huge performance gap. Right? So UDP, as baseline shows that there's no bottleneck, in the sense, in the UDP data path, and since Quick uses UDP, that's important. But As you can see, DCP still outperforms a quick to a large extent, and even the fastest, quick implementation cannot, cope with the you know, available bandwidth in that sense. So, and, TCP performance is also limited by the this 10 gig or test bed, since"
  },
  {
    "startTime": "00:10:00",
    "text": "we did measurements also with 100 gig per second And, there was, Yeah. It was possible for a single flow to to get up to per second, roughly. So this is not the the end for TCP performance so what are potential reasons for the limited throughput that we've seen, for quick for example, MS quick, seems to use only, single core. Which may be then here, the limiting effecterer, So, other implementations use, multiple course. But, this was not shown previous slide, but, we've seen that the switching the course actually degrades group So when when the there's some some switching between the course, there's just a small degradation in the throughput Yeah. So, one factor may be inefficient usage of, CPU resources. So if your implementation is not able to use multiple threats or, of course, at the same time, this may be one reason for limited throughputs. So Since we are, Michael, only compared to TCP and not to TCP with TLS, the question is whether it's a crypto or that is your limiting but that doesn't seem to be the case. So at least 2 or 2 implementations, of the the set language, looked at. You were able to disable the crypto. And as you can see here, for competitive comparison, the throughput is not that much lower in case you're using the crypto, which is the default, right, and if you're turning the crypto off. So business."
  },
  {
    "startTime": "00:12:01",
    "text": "Not explainable by the crypto, What we've seen is also that, in comparison to earlier work, the throughput has been, tremendously improved. By a lot of tractors like 5 or 7 times. For certain implementations and even while asking the implementers of one implementation, they got up from, let's say, 200 meg per second to at least 2 gigabits per second. It was in that sense, also some kind of improvement Yeah. Other things that, were looked at is, example, packet loss. So how do we do the quick implementations reactor packet loss? And, as you can see, for comparison, the black line, buff is, basically, TCP and we have very low packet loss ratios here like, 0.05 percent, actually. And as you can see, all the quick implementations basically are limited in their throughput by seeing packet loss. In comparison, TCP doesn't have that degradation and throughputs. Also packet reordering, on the right hand side, I can see that some implementations probably misinterpret reordered packets as losses because it was insured that say that the feedback, is fast enough there. So there are no time outs. And in that case, you can see that, yeah, several implementations have problems seeing packet reordering."
  },
  {
    "startTime": "00:14:01",
    "text": "So, Yeah. As a conclusion, more or less, the current quick implementations, at least the ones where we looked at. Are not at the same level as of the TCP implementations, And it's not only the the crypto it's probably also Oh, it's internally implemented in the sense that is it, possible to use, multiple course and so on. So Okay. I'm sorry. It was my phone. So I guess my time's up. Okay. That's basically all I have to say. I mean, we didn't study, in-depth, did some in-depth analysis for the reasons, as I said, but, that's, difficult, yeah, to say. So maybe it's also offloading I mean, probably with quick, you have more interactions between user land and corner. And, while you can use more optimized implementations with the GCP stack inside the current As I said, that's all for sustained throughputs. It's not for for getting the wet content, which is oftentimes, foster. With a quick That's all I have. You. Yeah. We have some quick questions, in the queue while I try to fix this line. So so please continue. Michael Turkson, you are talk I mean, you are suggesting to use multiple course for processing this stuff. For TCP was there, It Did the TCP case use multiple cores? I mean, you use a single TCP connect Right. Right. So that goes on a single comment. Yeah. I think so. Yeah. Okay. Thank you."
  },
  {
    "startTime": "00:16:00",
    "text": "Hi. I'm Alan Frindell from None of one of the contributors to move fast. Thank you so much for bringing this work. I think it definitely highlighted to our team that we have not done a good job of instructing people on how to performance test and I don't think our defaults are, representative of how well our implementation can do, and there's a lot of nuance to to tuning quick, but you again for bringing this work, and, it shows that we have some work to do. Yeah. Thanks. Yeah. So we all thought that it's probably Exactly. That thing that you need to have some deep knowledge how to to do it very efficiently, with the perf clients. Right. And so, yeah, you need some guidance there. Yeah. Thank you so much. And we have another talk. Actually, also looking at quick performance of Stack solid take that talk and maybe have some more questions afterwards. Thank you, Roland. There are more questions in the queue. Yes. I'm sorry, but let's have to talk, and then you can receive to your question because it's also about performance. I didn't it's supposed to be there. So you have to tell me the next slide. If you or you can purchase your phone Just see. Alright. So The last talk was a perfect segue and set up for my talk. My talk is basically all with net quick performance. Next slide. I could we can skip the slide. Next slide. Okay. So quick is amazing. that amazing? We don't know. We know that quick is a user space implementation. Is It has faster connection startup and a lot of different benefits. I think the one that is not on the dispute is how much benefit you get from faster connection startup. The other potential, attributes of quick"
  },
  {
    "startTime": "00:18:00",
    "text": "it's not clear how much they actually contribute, to either make it things better terms of speed or throughput. And one of the things we started out to do is basically understand when is quick better than TCP when it's H3 better than H2 I think one of the things that motivated us is this debate where you look online you will notice that there are different benchmark numbers about when is one better than the other. From the last talk, we already saw that even trying to do controlled experiments is challenging because you actually need to be able to get the right parameters, do the right set of tuning, get something in the controlled environment that reflects what is out there in production. So That's the backdrop for our study. Trying to understand y is one faster than another? But more importantly, understanding under what conditions you actually get to see these speedups. Next slide. Oh, So as Alan had mentioned, just getting, source code is not enough. You actually need a bunch optimizations and a bunch of tuning and when you start to look online, you will basically realize that trying to get your quick stack. The performance requires these optimizations. Special configs, a bunch of code. And I wanted to thank a bunch of the people in the ICF quick working group that work with my students try and help us tune things we realized very, very quickly that It's not that easy to tune our service just as performant as matter service. So we gave up trying to do kind of task data from our local development set up and somebody started Can we test what exists online. And the challenge there is if you're trying to test and compare Google's implementation of quick versus matter, versus cloudflare. Their production implementations you don't really understand why you're you're experiencing what you're experiencing. You don't understand what aspect of the performance is due to optimizations, what aspect is due to cultate what aspect is do you"
  },
  {
    "startTime": "00:20:01",
    "text": "to the air code because a lot of this is just a black box that you're observing from external. Next slide. And so a lot of what we ended up doing is we basically try to take a fuzz in approach. To this to this. So we don't know exactly what they're doing. But we could try and play around with with different kinds of clients, different kinds of medical conditions. Different kind of object sizes, basically, we could try and fuzz the different types of interactions that you can have with their servers then we can create queries over the packet traces, the dumps that we get try and reason about why is getting better throughput, what is getting better? Performance than another Oh, so My student, Alex, created, this really amazing framework that would basically spin up different types of clients, different quick clients, or Chrome client and move fast client NGTCP Clyde. Would use, network emulator, TC, to basically play around with the network conditions so we could, like, emulate different loss conditions, a really different bandwidth. And then essentially we work with the IETF to basically get different endpoints of objects of different sizes from several of these companies. And we basically played around with their servers and essentially download it. Server is multiple that that led an object on the server is multiple times and created a large database of packet traces which we then analyze, you look at how did the congested reader evolve? Webpack, different types of packet losses happen. The way latency changed. Did a protocol react? So what we're looking at this, we're not just looking at quick as a whole, we'll also try and look at it's an internal internal congestion control algorithm why try to also look at different handshakes and try to understand how all of these are essentially interacted."
  },
  {
    "startTime": "00:22:03",
    "text": "So I I know I don't have a lot of time, so I was trying to make this very short. We have a paper online somewhere. And we have a bunch of different insights from the study I think the kind of very obvious one is Yes. Where you bring down the handshake cost Life is amazing. But it turns out if you have really large objects, Then the handshake is actually very marginal. So you get a huge you get a benefit from a faster connection establishment but this is not that noticeable where you have large objects. So if you help, you basically could get fast startup, but if you're like doing paints that require bought a large transfers this is not where most of your benefit would be coming from Sawrit, another thing we observed was, you know, we look at the implementations of TLS had a client and server. And some some notion, you probably expect it to be the same suite of protocols, but it's not exactly suite of protocols. That were on all of the clients and the servers. And this actually is created some performance overhead. So certain clients had some expectations on what the server would do. When the servers do not behave in that way, then there's basically that you add to a different set of head shaped protocols, made it faster. They made it slower to essentially set up the connection, are some issues with congestive control, which I'll briefly talk about And then head of line blocking, I think there is a lot of potential there it requires a lot of configuration. And when we were doing the study, did observe a lot of the complexities required to actually realize this where it being, implemented Okay. We have a bunch of these. I feel like I when that studies and compares quick, has these, like, arbitrary hit map style figures. So we have a bunch of these in the paper. Add all of these figures, you will notice that"
  },
  {
    "startTime": "00:24:07",
    "text": "make with you. Alright. At the at the very bottom, it's a combination of the provider and object size you look at this access and is basically what aspect of network we're changing. So object size, for different Providers, And then here, we're changing the latency we have are the latencies we play around with. But there's only so much you can show in a slide. At the top, we're changing less rates. And the blue boxes are basically where h threes are amazing, the red blocks are basically where H2 is, amazing. Where amazing here is just faster. And I love these figures that basically for a single object we have figures in the paper for multiple objects also. Next slide. Okay. So, Sorry. Bonus, So I wanted to focus on one thing here. There are more details in the paper focused on every box. But looking at two boxes here, some company called Google never heard of it. Some company called Facebook. Don't think they exist anymore. You'll notice that if you look at their kind of packet spacing, which is a a lot of what we did we're analyzing the differences, basically, looking at a pack of space and they're trying to into it why from the packet space in protocol is behaving differently. In this scenario, what we're trying to enter it is basically aspects and properties of the congestive control. So I think what I want to kind of highlight here is while this conversation here is largely about quick. Versus CCP in reality what we're really talking about here. Is what congestion control algorithm is being used"
  },
  {
    "startTime": "00:26:00",
    "text": "and how correct is this implementation And I think this is something that we gloss over a lot. When we compare quick. With TCP is that TCP is not a monolith. CCP is a lot of congestive control algorithms. And TCP has this algorithms from the kernel, which is they've been tested over and over and over. If you look back enough, they all had bugs quick, when we did the study, during the pandemic, a lot of these protocols were new. Bugs are being fixed. So I think There's a bit of unfairness when we just say, quick versus TCP. We need to basically say What version of quick what actual congestion control protocol what configuration of that protocol and these are subtleties that we as the researchers highly overlooked and I am also to blame for overlooking much of this. But I don't have that much time to talk about my issues. So here, we're looking at, differences between 2 providers And one of the things we start to notice is that, you know, these are both using BBR and we can we can go into details later about how we try to figure out exact algorithm are being used. I are being used one of the things we noticed in one of the providers is that they had a subtle bug their implementation, and that is essentially what leads to lower performance, we spoke to the amazing developers from that random company to fix the bug, And presto, you know, it improved performance a lot. Next slide. I'm trying to run out of time now. Yes. Okay. Yeah. So giving cover sorry. Okay. Both. So I think my main message here was that just comparing quick and TCP without going into the other need congestion control or the configurations. Hides a lot of the subtleties"
  },
  {
    "startTime": "00:28:00",
    "text": "what I'm trying to do now is redo our paper, but in an as an automated service, and I'm trying to focus now on bugs and tried to find bugs And I was trying to get more people to work with us. I think the reason why this worked out so well last time It's a lot of the quick working group actually helped us set up endpoints of their servers answered a lot of questions, we try to set up our own servers and I think here we're trying to get not just engagement from the quick working group, but also from researchers because we're trying to find ways to model various transport protocols, and create much, much richer fuzzers to essentially ex expose different bugs. And with that, I will yield. Thank you. Yes. Stay here for any kind of because we did reopen the queue again. Jeff, you're next, but if you want to jump in the queue for this talk, please do now. But I already want to think both of the speakers for the work. I think it's very interesting, and I hope you come back with more results. Later. So my question, Jeff Houston is for the 1st presenter. And, Roland, you just can wrap a mic. Right? So let me then ask her. I've noticed in performance measurement that TCP line card segmentation offload has dramatic impact on on the throughput you actually get. Because you send off this humongous packet into the line card, then the line card basically offloads all the rest of the work. With quick, that's not possible. So the real question is in in your measurements. Was that line card segmentation enabled or disabled? Did you actually go to the effort of turning all that off? As far as I know, the offloading was enabled and there were also done some measure measurements about the impact and that the impact wasn't That huge, actually. We've certainly found it's large when you're getting a high packet load. So when you're on very short latency, and you had 2 to 3 milliseconds. So you're frogging the packets really quickly."
  },
  {
    "startTime": "00:30:00",
    "text": "The per packet processing does become interesting on the line cards. It's just a load you're getting rid of. That quick is unable to do so. It's pushing it back into the CPU and the interrupt structure. So, it's certainly an area I think of a little bit more discussion and measurement Thank you. I know. Yeah. So so we did experiments at 100 gig per settings where where this is even more important. And so we are well aware of all the optimizations that might be there. Hi. Brandon. I'm also in a little bit late. Excellent talk. Both of you. Thank you very for them. Theo, I've already sent you a message asking if I can borrow your slides for a lecture at DTH in a couple of the There's something that came out of both of these talks, I think, is super interesting, and I'd like to encourage community to keep looking at things because, like, we're looking right now at a very early stage the performance measurement. If you look back at some of like the forget what they were called, but some of the projects to, like, get TCP up to one gig tell. He's be up to 1st, a 100 meg, 1 gig, 10 gig, etcetera, etcetera. I think it'd be really interesting to do a longitudinal study where we can sort of compare how fast did TCP faster versus how fast is quick and quick getting faster. So, like, know, I think we did a lot of, like, quick uptake measurement, in map RG, like, a long time ago, and everyone's still citing those numbers because they're kind of boring. Like to point out that this work will not get boring that fast. So thank you very much for all of you. Okay. Thank you very much. And we have one more quick presentation. He could come up, please. I used to I'm sorry. What is Okay. So hi, everyone."
  },
  {
    "startTime": "00:32:00",
    "text": "I'm a concept from Aachen University, and this will actually be a tool our talk. So Constantine will be joining me later on. And we hope that handing over control will work. Properly, So this is work, also on the real use world of quick so we've done a large scale web measurements to, assess the use of the spend bit and of ECN real quick. And The second part is also part of our collaboration with You Munich. And, yeah, Okay. So, short overview of our methodology. So we've done obscaler web measurements from our vantage point in Aachen. We have essentially fed in a few top lists as well as, top level domains, I've tried to resolve them, So just to quick overview here. Have tried to establish quick connections then or identify which hosts to support quick. And then finally, we have then of those connections where we had quick connections, in the last use of ECN and of the spin bit. And I will, be presenting the results for spin bit and consulting will then take over for the ECN use results. So Spinmit just a rough overview. It's a mechanism that enables RGT measurements it's quite simple. So the client always spins a bit and the server, just mirrors the bit, and this enables the RGT measurements. And, yeah, while it is a nice mechanism, it's only optional. Only few publicly available sex supported. And hence, we were interested in finding out often it is used in the wild. Second, even though if hosts want to, enable the bid, they are mandated to disable it sometimes. To also allow host to, yeah, decide to not use it and don't face any disadvantages in the wild And, there are different ways for disabling the bit. And hence, we were interested in finding out"
  },
  {
    "startTime": "00:34:04",
    "text": "how the bid is then used in the white. So, for example, if this mandate is used or not. And then finally, even though people could used a bit. The usefulness of it in the wild is unknown. So there have been a few, tests and test settings like the 1, role I'm presented on just, lab settings. We were interested in finding out how it's actually useful in the wild. And the bit or the mechanism also includes application delay. For example, if the client waits until sending out the flipped version of the of the bit. So to check out a measurement, then this could delay the measurements, And the bit is also vulnerable to reordering. So if we take a look at bit up there and just order it with a packet in front, then we'll suddenly have 3 measurements instead of 1 and 2 of those will produce a very short RTT. And does the final part of our measurements were of our study was to find out, how accurate measurements are in the wild. Today I will only focus on first and the third questions, for the second one, I refer you to our paper. So regarding the usage of the the bit in the wild, So, you can see here now our results for ipv4. But the results for ipv6 are in general, similar so of the round half a1000000 quick, quick domains. So the 5000000 topless domains that support quick. We see that 6.9% of them also support the spin bit, while for the dotcom.netend.org remains, we see that 11.1% of the domain supporting quick ops support the spin bit. So in general, in general, the spin bit sees use in the wire. However, we were then interested in also looking at the underlying host support. And there we saw that, this percentage, differences even more"
  },
  {
    "startTime": "00:36:01",
    "text": "pronounced. So we can see that almost half of the, hosts providing the dotcom.netand.org domains, all to all support spin bit while the percentage is much smaller for the top lists. As we kind of thought that there might be some impact or, some some impact regarding the providers, providing those domains and hence also looked at who's actually using those or providing those domains using K SAS AS to ARC. A resolution to a We then, decipher that and found that the 2, top 2 providers of the quick connections that those don't, or only barely use the spinach. Then if we go further down the list of providers, so you can see the absolute numbers in the second column so all already quite smaller numbers of connections. But here, we can see that there's a sustained use of the spin width of more than for more than 50% of their connections. And also for all the other remaining connections, we also see a spend per use of around 50%. So what we gather here is that the spin width, in contrast to the the general trend for quick, which is largely driven by that large providers. That's been a bit more driven by the small to medium providers. Yeah. And with that, I would like to continue with the RTT measurement accuracy And for this, we basically use all the connections that showed some spimbert activity. So where we saw some, some spinning, and here we compared the, RTGs that were produced by the spin bridge with the, with the, ground truth that a quick stack estimated. And we further compared the results for the receive packet order. So basically what at the, the network operator would see, and we then also use the sorted or we further sorted the packets by the ticket numbers to assess the impact of free ordering or rather for that"
  },
  {
    "startTime": "00:38:01",
    "text": "or in that sense, filter out impact of the ordering. On the right hand side, you can now see the, the cases where are the spin with RTG was larger, so we where we saw an overestimation on the left hand side It's another estimation of the spin bit. And in general, due to the mechanism, for example, the application delay included would expect, to see more values on the right hand side. And this, indeed, show true. So we saw quite a lot of overestimation. In particular, the the bar on the right hand side, rightmost side, with more than 200 milliseconds of overestimation. For quite a lot of connections. Interestingly, however, we also see some under, as animation where we're not really sure why that happens. So that definitely deserves a a little bit more attention in the future. However, those are now only absolute numbers, and we also had to look at the relative impact of those results, to, yeah, see how Yeah. The the accuracy really matters in in practice. And for that, we also but you can now see here on the right hand side the, Again, the case is where to spin with RTG is larger on the left hand side where it's where the real RT is larger, and we can see that for around 30% of connections, there's only around 25% difference in the the RTTs So that's quite nice. However, again, we see on the right hand side quite a lot of case with large overestimation. Which, I believe might not be that's great. However, I'm not a network operator, and hence, I don't know how useful this is really in practice. So that would be something we are really interested in finding out useful those measurements really are. With that, I'm now at the end of this first part. So just to takeaways here, trying to hand over control and what we can yeah, gather is that the spin rate, indeed, is used in the wild and that the RGT measurements are"
  },
  {
    "startTime": "00:40:00",
    "text": "kind of an ambiguous picture. So they are often quite accurate. However, there are also often large overestimations. And there's also underestimation, which we didn't x, And, yeah, so this is really something that we intend to look in into more in the future. And now constantine will take over with the second part, I hope Yes. As I said, we also looked into the usage of PCN. Small, recap is that this ECN, basically, and transport connection receivers need to remember about the ECNs in an information to the sender, And this ECN mirroring has been optional before with TCP and is now and must in the quick RFC if these ECN signals are accessible by the Greek taste. But There's also a little ambiguity in the RFC, which says that some pre stacks can also not or can choose to not implement it And so we asked also if there was an OECN with code can be used. For this, again, physical websites via HTTP free and free and then locked the ECN counter And for the top list, we found that 3.3% of the big domains quality CN and forth from that optimize, we found that 5.6% supported this. Given that mirroring should actually be mandatory we see a quiet low score. And when looking into the servers that provide these domains. We found mainly the light speed HTTP server. And there's an, platform CDN instant toss play Google's, which may work in CN. So again, we find a loss powered by hyperscalers and content providers, and this could be due to you two facts. On the one hand, the Greek sticks could simply ignore ECN, of the network mic interface here. So it could happen that router simply take E CN signals like ECT 0, and replace it by not easy to use. So then we not get any getting the wrong information from the quick to expect. And to check for this, we then import some trace book tracing to see the path basically changes PCN. What we found is that there's no visible ECN clearing for 97% of the domains. So for 2% of advanced resource and display clearing, And this has been mainly"
  },
  {
    "startTime": "00:42:03",
    "text": "being impacted by 1 tier 1 ISB. So 98.6 percent of the demands that show it clearing the effect that this one IP and this effect, especially smaller hostess, and especially after do some route changes in December 30 any 2, any 2, any 2, any However, for the content providers, we saw that the missing cannot be roots out due to clearing. So before We saw that this content providers support ECN via TCP. So either the create text or undiscovered middle box, ignore ECM. When looking into the quick stacks of these different providers, so the open source text that they are using. Indeed, many are lacking ECN support. Okay. So now we saw that some domains are mirroring ECN. But this is only half of the story because you then also use ECN with break Click requires an easy end validation approach, and this check whether the first package share some time outs. For example, if you do ECN back only, or whether the wrong code points are made back or whether there are under coded code So for example, whether there's some route flipping and some routers are removing ECN footprints or, if there are tech issues. And what we found is that only 0.2% of the comment documents passed this easy evaluation approach. So 96% of the memory domains fail. And this is mainly due to defects, due to undercounting, or did you re, market and we found the undercounting for the Google AS and the light server, the Google AS, we are not exactly sure why this is happening. Relates work suspects that we will may use some data center TCP internally and maybe quick is now leaking this internally. You see an information out side of the data center, And for the light speed server, we saw that after ticket number space switches, Let's speak to disable CCX sometimes. The remarketing, we again found the tier 1 ISP, which besides clearing ECN also remarks in we write ECTZROCE 1, and we, again, find newest AS. So all in all, we can say that there are multiple challenges for easy and real quick. Several quick stakes do not mirror ECM, some networks elements, we are sickened,"
  },
  {
    "startTime": "00:44:01",
    "text": "and we often see easy and validation to failure to just take a network impairments. This has been all the results from our main advantage point in Athens, but also saw that the usage is limited on a global scale. We never saw more 0.4%. Of the domains passed, ECN validation, and we saw worst results for ipv6. And these results are also interesting beyond free because they can have a significant impact another ECN mechanism for S for S I already said, we see this ECT 0 to ECT 1 remarking. And this can be detrimental for FLS because FOS uses CCT 1 now to distinguish between L4s and traditional traffic but there's also good news. Because we see that there is a trend which is probably to be increasing. We see changes over time, when there's this quick RFC around them, there may be rework or stakes to support ECN in a more wide in a more broad way. We have open tickets with a stack member, and I in touch with the ISP for debugging these images. Okay. So in all, we show up to your 2 last scale, including a quick version of my sentence. Once on the optimal spin bit, which which sees right forward to use in the line. However, shows some measurement accuracy issues, And then we showed you some results on E CN, which is mandatory, but significant limited on a global scale, did you mention many challenges in tech support, 40 network devices. And if you're interested in more details, we have quite plenty more detail she got over a pick Thanks. Thank you. Brian. Hi. Brian Trammel. I you very much for I just complained that, like, we're not looking at quick in the wild much anymore and now excellent paper, and this is pretty much with the spin bit necian, the perfect nerd site for me. I wanna come and talk to you later. I have a lot of things. The the question I wanna ask here is on the spin bit sort of overestimation. Where you're using ICMP. I'm gonna miss it. You were using ICMP ping as the ground truth for RTT or Now we're, basically using the estimations provided by our quick we're using, eco on your side. Oh, okay. We're using"
  },
  {
    "startTime": "00:46:03",
    "text": "So we essentially use the minimum, runtime estimated by quick. And then compare that with these chemicals. Okay. Cool. That's really interesting. Let's talk offline. Thank you very much for the work. This is awesome stuff. And I will lock the queue in a few seconds Martin. Thanks for presenting these results. I'm I'm a bit surprised about the the spin bit measurement results because we had a long discussion about spin that, when we were designing quick. And there was a lot of work done during the hackathons. And and some people evaluated the, the feasibility of the spin width to accurately measure the RTT and the results we got from these groups were Basically, Yasmin Mid is works great and you can accurately, measure the RTT. So there's seems to be, the slight disconnect here. Between those measurements in those measurements and yours. If I may, saw one idea that I have. I I I don't know if how exactly those methods were in the in the hackathon, but what we essentially looked at was the beginning of a connection. So we did those I said, in the web measurements and only request a, like, the first few bytes of, 1st few kilobytes of a of or file. And hence, we might suspect that there would be something in those that HTTP stuff in there so that the at the start of connection there, this application delay actually has more of an impact and an ongoing connection, and this might be something So I don't know if that was the case in in those I'm I'm I'm not sure. It was not involved in those measurements. Just want, just want, autumn might make sense to, sync with those people and see what kind of measurements they did and if they had, like, some, some tricks to get to get a higher confidence there. Yep. So, again, or an additional aspect. So as I said, we"
  },
  {
    "startTime": "00:48:01",
    "text": "try to design this study to measure both the spin width and ECN at the same time. So it not really focused at the spin bit, and they are, for example, also regarding second question that I didn't talk about today. Also some aspects that might be that might look could look better at specifically designing, the measurement study And one more person, Tara. Hello. Thank you, Tara. I was wondering, just for clarification, because you keep saying that the spend date being used where but if I understand correctly, you're measuring domains that have the spin bit enabled. I'm wondering if that translates into it being actually useful and being used. Yeah. So we say a domain has to spin bit enabled if it is used. So we But so in essence, we take a look at the bits that we get back in the in the in the split midfield. And if we see at least 10 and one one Then we say, it might use the spin bit, and we then compute or try to compute the RTT based of it. And if that makes kind of sense, so it's not like, you know, very very low millisecond range, then we deem this number to be enabled. But wouldn't be wouldn't, the definition of it being used being is it being used by people who are measuring RTT rather than it being enabled by domains So you're talking about the if people use it on the way So so we were just looking at the endpoints, and trying to find out if the if the possibility would be there in the network to better the, the, the, the art device in the standard. Okay. Thank you. Thanks. Thanks. Thanks. Thank you. Yes. Actually, I think we have again a problem with the next slides. Or Dave, can you see them?"
  },
  {
    "startTime": "00:50:23",
    "text": "Otherwise, I would actually sorry, Sushi, I would actually like to ask my not to come first. We just switch the auto very quickly. And fix the slides in the meantime. Thank you. No worries. Thank you. Okay. Interesting. This. Thank you. Okay. Hi. I'm Maynard from Teodresen. And today, I'm going to talk about transparent DNS forwards. So, transparent DNS forwarders are often misbehaving CPE devices in ISP Networks, but but How do they work? So, let's consider a simple example we have a client who wants to resolve the domain name, let's say google.com. And he sends its, DNS request us to transplant forward it. The transparent forward and then forwards the request. To its configured because of Resava. And,"
  },
  {
    "startTime": "00:52:01",
    "text": "At this point, as you can see, the transparent forwarded does not rewrite the source IP address of the DNS request. So the recursive resolver more or less does not know of the existence of the transparent forwarder. It only sees that the request supposedly comes from the client. So, it does its resulting drop. And as soon as it, sensing. DNS request, the DNS reply back. To the client decline then, gets a reply from an unexpected source. So that's basically how transparent forwarders work. Should we even care about these devices? First of all, open dinners in general enables amplification it takes. So we have an attacker that sends a spoof DNS request. Which leads to, flooding of the victim with unwanted DNS responses. And we want to lower the impact. So What did we do? We set up our skin campaign measure the complete audience infrastructure. And what you can see is that we, find more DNS open DNS devices than other campaigns And why is that? That is because we include transplant forwarders. And we confirmed, with controlled experiments, these scanning campaigns do not include transparent forward is because they do not detect them. And that is a major problem because back in the days, around about 10 years ago, had about, 20,000,000 open DNS devices. And that, yeah, reduced, the amount decreased rapidly to roughly below 2,000,000 devices but the, amount of transparent forwarders not only remain the same, but, it also increased. So the problem is that the transparent forward is"
  },
  {
    "startTime": "00:54:01",
    "text": "transitioned from an exception to a major part of the audience infrastructure. Accounting for now, roundabout 31%. Why do they miss these transparent followers? It's mostly due to efficiency reasons. The the scans used static queries, and they only evaluate the incoming traffic. That means, that many scanning campaigns just considers a replying source IP address. Which is the IP address of the recursive reservoir and not of the transparent forwarder. So the transparent forwarder remains hidden. Let's have a look at the transparent forward deployment. We highlighted in this plot, the emerging markets. And as you can see, Countries classified as emerging markets are more likely to host more transparent forwarders. And furthermore, count the, different ASs per, country as you can see, in each country, multiple forwarders. So it's not an exception to to a single AS or something, they are distributed all over the world. And, let's have a look at the share of transparent forwarders. So as you can see, compared to other, the audience components. In some countries, the audience consists almost exclusively of transparent forwarders. Yeah. We want to reduce the impact of transparent forwarders. So, we set up our own scanning campaign, which includes all ordinance components, And, We always try to get in contact with, network operators, and yeah. So finally, in April 2020 is free. We succeeded with that. Somehow, a large ISP added filter rules at their resolver. Which eliminated 280,000, transparent forwarders at once."
  },
  {
    "startTime": "00:56:04",
    "text": "So we have a decrease here from 38% of transparent forward us down to 31% so there's still a lot of work to do because we want to, eliminate in the best case, all of them But, yeah, there's this where we need your help. So if you're maybe a vendor and you think you could be shipping transparent forward us, Please talk to us. We would like to understand, your implementations better. So far, we have identified mostly. MicroTech and Cisco devices. Which, which are misconfigured by default. Yeah, and on the other side, which will wrap up, that talk, if you're unintentionally hosting to spend forward us, though you think, your network there, transparent forward is We have, yes, 3 main points where you should update your deployment. So first of all, Of course, these devices, facilitate data simplification attacks, furthermore, they allow attackers to exploit any cost deployments. Which challenges the pop based DDoS mitigation. And, finally your s forward packets that look like spoof IP packets. So attribution is challenging, because these packets are triggered outside your AS. What are the solutions to this problem? First of all, you can update the filter rules at your firewall, or you can update, transparent, forward this individually. Yeah. For more details, you can, find a paper, which is also linked in the slides. And, you can find our weekly scan results, where we publish all open DNS components, under ordiness stuff. Signal dotnet. You very much. There's a question. Yeah. Thanks. It's gonna be. We can you go back we don't ship any forwarders nor do we, run any forwarders, but,"
  },
  {
    "startTime": "00:58:02",
    "text": "go back to the sort of the slide where the the invalid source address is reply arrives at the host, you can set up, like, that on the slide, like, for You mean that one? Yeah. This one. So I'm confused the about how this would Yeah. So so if it gets a reply from the unexpected source I don't understand is like, why would it even take that reply? So because I I I'm familiar with the Android implementation, and we're should reject it because it uses a connected socket, which is drop It it is, it is rejected, but, we can measure measure that, and we can, yeah, we can use these transparent four wallets because we get a valid reply. From them. So what I guess I don't understand is If the client rejects it, why are people doing this at all? It doesn't do anything. Is it just, like, literally a mistake, or Oh, Yeah. It's just, it seems like to be your buck in the, DNS implementation. So there is, Thompson wrong, where the Denest server failed to rewrite the source IP address. So it just gets the, the, the DNS request, and simply 4 was it without any, modification to the, So we've got 3,000,000 of these in the internet. No. No. No. Get, Mhmm. One second. We have, 600,000. Yes. 600,000 is something that doesn't do anything. It, okay. I guess I'm surprised that what might be worth, like, trying contact some of these and saying, Hey, like, why do you do this? Because it doesn't do anything useful. Yeah. Yeah. Let's follow-up offline, actually. Sorry. But, like, we're all here. Let's talk together. Thank you so much. Thank you very much."
  },
  {
    "startTime": "01:00:03",
    "text": "Now we try the online presentation again. Hopefully it works now. Great. I I can see the slides Okay. Perfect, I will hand it over to you. Now you should have control. Right? Yeah. I I think I And we can hear you. We can see you. You can start Perfect. Yep. So hi, everyone. I'm I'm Sadiq. I'm a PhD candidate here at the University of Washington. I'm excited to present some of this work on behalf of all of my collaborators from Northeastern and Cloudflare Research. And, today, we'll be talking about of our work on characterizing, DNS resolver behavior to DNS sec queries, and this is an active paper and and submission. So Over here in this chart, you see, that, this is this from APnick, and it shows the total percentage of DNS resolvers that are used by clients, which are in the same as, as the, as the client, which is shown with the with the blue line, on top, and it also includes user queries to public open DNS with all words such as Google DNS, Cloudflare, and, which is which is over here in the orange line and the other little colored lines at the bottom. But what we see here is an increase in the total number of clients which use resolvers that configured by their ISP. So over 65% of these internet clients use such resolvers. And we also see that there's a decreasing trend the last 100 days of this data that you see here, for people using large public, resolvers. But there are new regulations and policies such as the EU resolver and, others across the world, which have filtering requirements for security and legal purposes. And the deployment of"
  },
  {
    "startTime": "01:02:01",
    "text": "managed DNS resolvers helps these network operators comply with these regulations, and it's possibly the reason why we see an increase in usage and appointment of these resolvers compared to using the larger public ones. So, this makes it important for us to understand their behavior especially around the answers. They return the ability to validate and protect users using, these resolvers, and any other metadata maybe send they may be sending back to their clients. So Well, some of these studies were done in the past. None of them specifically focused the response correctness when, DNS queries are made by the client. And as many of you may be aware, the usage of DNS sec allows resolvers to return cryptographically signed answers for user queries. And these are obtained from the name servers and this guarantees, the authenticity, of the answer and the integrity confirming its correctness and validity. But recursive resolvers can also drop signatures in the answer being sent back to the client and sir, forcing the clients to either perform all of these lookups themselves to validate the that they have. And This kind of brings us to the core part of the question research questions we asked in this effort. First, like, we want to understand to what extent these recursive DNS was always actually provide valid or correct responses to DNS queries. And second, to to what extent are the curse of DNS resolvers actually validating DNS sectors they obtained from from their name servers. And A good way to do this would be to analyze the DNS resolvers within each network. But many of these may be configured to be private and are only accessible to to clients that are within the same AS, to provide DNS resolutions for them But this would be really complex to do without having a lot of individual probes"
  },
  {
    "startTime": "01:04:03",
    "text": "within each of these networks. So as a sort of approximation we try to understand the behavior of these resolvers which are configured to be public. So these are open DNS solvers and in the ipv4 space. So We ran a ZMAb scan for open DNS resolvers queryingforgoogle.com's a record, and that resulted in a total of 9 point 97,000,000, a resolve where IP addresses to which we further sent DNS equities for google.com, which returned, 7.93000000resolvers. And we also obtained at the same time a 101 day snapshot from shadow server. Where the total number of unique IP addresses that were observed were 26.97000000. And, when we probe them for DNS sec, queries at a given time. It comes down to 7,300,000 Resolvewares, which are available and you this bond. But but despite Shadow server actually finding over 2,000,000 or 2a half 1000000 revolvers in their daily scans. Our snapshot finds a lot more, which is almost 7,000,000 revolvers. And it's likely because, of the filtering opt outs, maybe the shadow server, is is respecting because of because they may have received those requests from operators. But we observe in these responses that, over 98% of these resolvers actually return successful responses to to the clients that are making these queries. And, especially when the queries were set with the where the DNS bit of the query was set. And there are very small number of them that return error codes. But Unfortunately, not all successful responses that we receive when, when we analyzed, we're actually correct. And, which means they did not have the correct IP addresses that belonged"
  },
  {
    "startTime": "01:06:02",
    "text": "to Google's advertised IP ranges. But interestingly, there seem to be resolvers out there which aggregate the results from multiple, main servers and provide all the list of results that they may have obtained returning over 10 IP addresses, in the answer for the query, but all of these answers are fully valid, which can see in the blue box, to to the right. But a majority of these resolvers actually return one IP address in the response and over 95% of these responses are actually invalid and do not belong Google's advertised IP range. But this result was quite interesting. So we we analyzed this further and realized that 99% of all invalid answers that we see actually include one of these 4 IP addresses in their answers. And interestingly, these IP addresses are advertised bite, different networks like level 3, Dutch Telecom, Courier Telecom, and Fastweb, but these IP addresses also However, match the fingerprint from previous DNS censorship measurements that that were done about the great firewall and their abilities. So we also observe when looking through census for the the the certificate scans that these IP addresses were present in the common name, field, for for a lot of certificates and all all these certificates are invalid, but these certificates were found in devices such as Frixbox, and also in, some cluster management tools like rancher, which is a Kubernetes cluster management tool in their search in their local certificate authority kind of a project. So, But interestingly, over 95% of these resolvers actually claim to be authoritative in their answers by and set the AA bit in the response that we receive, but 25% of them set the AD bit and claiming to have validated DNS sec responses."
  },
  {
    "startTime": "01:08:03",
    "text": "And this is especially strange given that the resolvers IP addresses again do not actually belong one to Google's advertised IP ranges. So it's not the name servers that we're speaking to, but also the fact that Google doesn't sign its zones using DNSsec. But we also observed the longitudinal data for from 101 days and observe that over 35% these resolvers that we find every day typically tend to be misconfigured. And, this chart shows shows how over time we also see an increase in the number of configurations that are out there. So there's there's a little bit of a trend, over here. Another thing we observed was that DNS resolvers are actually extremely transient on today's internet and only 2.1% of the resolvers this resolved IP addresses that we see on the 1st day of the scan actually are maintained until the 100th day of the scam. And the the p line over here kind of shows the BK and the the pink line shows the cumulative growth over over the 100 days. And This is particularly really interesting because, This means that there are, there are resolvers that are being spun up temporarily, maybe because and maybe con being configured to be public, public, it it it it initially when they when they are, when they're set up, but later event may be fixed and made private. But, also, we we noticed this bimodal distribution where there is a small 1% number of resolvers that actually have very long service up times at the same IP addresses that we've seen them for all for all these 100 days. So We we added we added another query to the mix, and we started, working with"
  },
  {
    "startTime": "01:10:02",
    "text": "a broken DNS sec, dotnet, which is a well known zone that contains a a broken DNSX signature, and these should not actually validate the entire DNS set chain. And when we made these queries, we would have ideally expected that resolvers which are validating send us an appropriate error code, and actually do not respond with the IP address where which is a cloudflayer IP address that we would expect to see. But what we observed was pretty surprising. And even though we had a much smaller number of resolvers that actually responded to this query. 92% of these, actually did answer with correct IP addresses. And when when in fact, we should have expected an error message. So these resolvers are not validating resolvers at this point, and they might pose a significant risk to the clients who rely on their services. So, and But but also the split between the lack of incorrect responses over here, compared to the number of IP addresses and the answers that were incorrect for Google.com queries seem to indicate that either the queries for google.com are treated especially, or it could just be an artifact of reaching to reaching a bunch of sensor, reaching a bunch of machines that have censorship attempts, the over there. But overall, what we do notice is that, there is a growing deployment of these DNS resolvers to comply with local regulations. And eventually, these resolvers becoming in network services makes it really hard for external scanning based mechanism like what we did here, to work in the future. So but also the the transient nature of these DNS resolvers that we notice also makes it harder to kind of study their behavior over time or study what the intent of these resolvers are and why they're, why they're why they exist. And, things like that. But but throwing in the additional possibility for on path middle boxes"
  },
  {
    "startTime": "01:12:03",
    "text": "to actually tamper these responses. I think it makes much it makes an interesting future research effort. But A lot of the clients today actually do not use the DNS sec bit in their queries, and they rely on, using a trusted validating recursive resolver. But is a time for, for us today for the clients to actually enable this bit and send their queries by setting this bit. So thank you so much. That's, I'm I'm open to any questions. I I see 2 of them in the queue. Yes. And I, again, will close the queue in a few seconds. Go ahead. Okay. Hi, Menard from Theodresen. Can you go back to slide 6, please? Yes. I can already start with my question, or it's just something I want to point out. Because, you said, you were a bit surprised about why, Shadow server identifies 2,500,000 open reserves, and, you'll see such a lot of more open reserves. I think that's due to the scanning set up, and I'm pretty sure because we also see, around about 10,000,000 Rizarro. But, we have some restrictions we want to, filter out some responses. So, in the end, We also come up with 2,000,002,500,000 devices. Okay. Thank thank you. Yeah. Jeff Houston. You you borrowed a slide from a panic. But, actually, it comes to a really important kind of issue around this area of measurement. That is not a measure of resolvers. That's a measure of end users and the resolvers that they use. So In this case, we're talking about the behavior of end clients. And effectively which recursive resolve has received those queries and then send them off to authority"
  },
  {
    "startTime": "01:14:00",
    "text": "and then you lead immediately into a different world. I scan IP addresses, I find things that look like resolvers, I analyze their behavior And I suppose the really big question is Are there any end users behind any of those resolvers that you find or is just an aberrant piece of noise that pops up that talks on UDP Port 53, is never seen in any other context And so it's hard to actually understand the import of what you're concluding in the rest of this pack without truly understanding the extent to which you're looking at random noise, or you're looking at systematic behavior inside operational networks that end users rely on. And one of the things you might wanna think about is correlating those IP addresses you find. Against known lists of known resolvers with known clients behind it, understand whether there's any commonality at all, whether you just spend a whole lot of time analyzing noise. Thank you. Yeah. Thank you. I I've I fully agree with that. I think that's one of the biggest concerns that we had is we we find a lot of these resolvers, but are these even do they do they even have clients behind them? And if they do, then there's a risk We don't know if they have clients behind them. So, I I fully agree with you. Thank you. Thank you. And we just go on for the next presentation, Python. We cannot hear you yet. You're muted. It's walking around right now. Yes. Much better. Thank you. Go ahead. Yeah. So Hi, everyone. I'm waiting. And, let's talk about, measuring the, weighing our So this is a joint walkway solution attack. You say Berkeley, ripe, I, j, n, is So RBI support code is a secure VDP, so let's first"
  },
  {
    "startTime": "01:16:03",
    "text": "talk about BD. The BDP is an international looking for code, and it's workspace, announced to build a path to origin. But there's no security, future built in. So theoretically, anyone spanning people for coal. Can't announce any orders at this place. And there are a lot of real world video hijacks that cause weather service outage or 1,000,000 of dollars lost. So Due to these issues, there are many secure BTP protocols introduced and RBKI is the most recent one. It is first introduced in 2008 and recently more than 40% of ISP spaces, verifiable with So how RP Care works? Firstly, reserves owners can create a certificate contains a as numbers and perfect. They owned which is called Raw, and this is where we signed by its IR or directly IRs and storing a public repositories. And all the neighborhoods can't and order networks can fetch all of us from plat public repositories and creates a perfect two AS mapping database. A router receives in BGP announcement, contents, IP Perfect, and IS passed it will take the perfect and origin AS numbers to see if it is in the mapping table. If it's matched, it means that Origin is authorized launch such speed enhancement, which we call off Care Valley. And that's Russia's can accept an announcement. On contrary, if the perfect announced by another AES that is not authorized. The routers will feel find out and we'll label this announcement as RPA invalid and future it. This validation and filtering process is called origin validation, which is our way."
  },
  {
    "startTime": "01:18:00",
    "text": "So in order to understand, how RPI is deployed today, we need to answer two questions. The first is how many network rates run? The second is how many that will deploy are we? So the first question is rather straightforward, and you can find measurement resulting, our IMC 96 paper But the second question is more challenging. Since we can't go to airing that Google operators and ask them how they set up their routers. So there are many successful we'll try to solve these questions measuring the with deployment they usually announce their own experiments perfect and use when he point from some platform to crack data like, Ruth wheels for a control plan. It's premage or ripe analysts for data plan. But due to the small small number of weighted point, These methodologists are limited on courage. So there are some attempts to exchange cars like ethnic to use Google and network So what they did is I see a rental point in Google Google network system SP request to 2 ul. 1 is the undervalued Perfect and order is under invalid However, they only use white and white provided, but Roy is not or unless in case, for example, Sound ASS may not feature in very, as much coming from the customer, so this measurement will have easily impacted. So to make one step further, we want when he point more when deploying to car more ASEs, and we want more opportunity in very perfect to make the measurement to reverse it. So we present and New Master, which is called, that can overcome these 2 changes. The first one are we still use in the well in that perfect measurement. The second one, we use technical IP address that channel."
  },
  {
    "startTime": "01:20:02",
    "text": "For the first challenge using invalid perfect for the measurements. It will be pretty hard to actually obtain 10 many perfect. And it's also not a good idea to wasting IP wafers resources. So our solution is how about you use the invite perfectly well? The question is, are there so many embedded properties we can use So we collect speed tables from disputes and various them with Ross, these two figures shows the result over 2 years. As you can see in the top figure right now, more than 40% are perfect according to raw. And at the bottom figure shows that more than 0.7% of them are are invalid So there are actually plenty of perfect that we can use. So now we solved the first challenge. They remain ening challenges, how to collect more when at point? What do you use a technique called IVA Design channel? Allow us to infer the character between 2 hosts of the past. Which means that if we want to in for the connectivity, between the A and B without directly control whether a r b. So how this IPID side channel works Firstly, IPID is a field in IP header. It's already designed as assist package of fragmentation by assignee and unique identifier for each package different system had different ways to 20 for each IP package. Somehow OS will use a global counter which means IP will increase 1 wherever it's then a new package regardless of the death station IP address. So how can we use IPID to infer the connectivity? Now we have host on the invite, perfect death can be responsible is AB handshake, which we call target. And then a host that have global culture, which we call reflectors."
  },
  {
    "startTime": "01:22:04",
    "text": "We first send same package to reflectors and reflectors were sent back to seeing a package which compares, IP ID in the header. So we know the current value of the country, cometers, then we send the same package to target with spoofing source IP address of a reflectors the target where it says act to the reflector and a reflector were sent by back research. Makes the IP ID increase work. Then we send the same type of game to record IPD. So now we know how IVR design channel works. So can you use this to infer the collectivities? There's 3 scenarios. First is no filtering, which means the traffic, they have to heavily going through talking to 2 refractors. The second the second is inbound filtering which means that traffic from Target cannot reach the reflectors. And then we have mostly the, we cases, which is up on filtering, which means the refractors cannot reach the target. So how can we distinguish these three scenarios in IPRD? So for example, we're saying, test with same package at the same time. I want to see whereas there's a spike in the IVR, the growth rate. If there's no filtering, we can see one spike when the reflectors and res resets to park target. If it if there is inbound filtering, which is the middle fig figure, we can't see any spike because there's low traffic in the ride. So if there's auto filtering, which is the left, left right figure we we also can find a spike when the packet arrives but the thing is that since Target can't get a reset package, it will refresh Miss Act pack it, pack it again. So there will be multiple so in that way, we can distinguish trade scenarios."
  },
  {
    "startTime": "01:24:00",
    "text": "So now we have plenty of embedded perfect in world, and we help terminate when the point using, this channel, there's still one step, which is how can we measure away from the connectivity between email and perfect and a webinar part. So we we introduced our score, which is which is the percentage of socket that all reflectures under the AS cannot read. To represent means that, yes, can cannot reach to any embedded perfect. And 0% means it carriage to all of them. However, it's always core. It's not perfectly matching with our way deployments. If we find one is with 100% our score, it does not directly means that AS is deploying our way. It's itself. But maybe also because all upstreams of that AS do we all be? But this always or can still be used to understand how is that is protected under our way. So we will always start from December one and keep it running until today. Right now, I wish to have covered more than 28 k as this in the more than 200 countries. To cause cross verdicts our results, we may only collect our way status through personal communications, post from the network operators, and we also do a survey with manners for his bench So we only find 3 inconsistency, and we have proved that at least one consistent AES, it's because of an out of date post. Then we want to see how many net will 40 protective is always nowadays. So this figure is the percentage of AS is always defined. 100% ROA score each day. Which means many points on this ASCs cannot reach to any embedded perfect. As we can see, it is increasing from 6% to 12% over the last 2 years. But 12% is still not enough to secure the entire internet. The good news is"
  },
  {
    "startTime": "01:26:02",
    "text": "We for law enforcement and more likely to deploy our way. And right now, 16 of 17 tier 1 ASCs, with very high risk score, since the acute wire do not have a choice. We believe they have deployed our way. So why large networks are more important because deploying our way, we're not only vehicles themselves, but also provide a benefit to their customer peers through in past future. Using our wester refund some real world examples. This figure shows off a school of KPN and a 6 customers KPN's blue line is a lot of SP that we're trying to pulled our way in on March 2002. We still capture 6 customers of KPL and 4 of 4 of them also jumped to 100% of a school. And we also find some cross damage cases where one has deployed away, but it's still wonderful. Because of its options not deploying our way. So for conclusion, we present our VISTA platform to measure the protection of our way. And with 2 years running, we successfully measured our ways to status of more than 28,000 ASIS. So we published all this then and so scoring here and we point out there's a need of future study to distinguish our way deployment and our protection in very large scale. Thanks a lot, and I'm I'm willing to take any questions. Thank you so much. Very nice study. We actually do have 2 minutes for questions. Okay. That gives us more time for questions for the other talks. Thank you so much. Yep. And we move on. Max, you are in the room. Aren't you? Yes. They are."
  },
  {
    "startTime": "01:28:00",
    "text": "Do you wanna request showing now you have to select Okay. So, hello. I'm, Maxim. I'm gonna present joint work with Olivia but not yours. So this talk is based based on a, on an article. We We did together, We're gonna reach to it, but we're gonna start first with a simple question. So I'm living in Belgium, And I'm subscribed to a DSL line to get internet And Today, latency is becoming more important, and I'm I'm looking into improving my latency, my end to end latency, from my home to Google. And the question I'm asking myself is Fi fiber is getting available in Belgium finally. Should I I switch to it? So what does impact end to end latency? So there is the old network I'm connected to Ethernet. There is the access network that was my question, but there is also the hearings of the ASPs. Themselves. So let's look from this, simple 3 panel view, and actually is all settlements just the only things at play. Is is in also address family coming into play. There are several, areas where it could affect the end to end latency. So the early days of V6 deployment, the access networks itself could be different. But What is most possible today is that the pairings using V4 and V6 are largely different. The prings from the ISP to the service providers and the general internet. Could vary from B6andb4. So"
  },
  {
    "startTime": "01:30:01",
    "text": "Today, I'm using V V Six. Because I'm using more than applications and host favor ipv6 using IPable, So we could supplement this kind of The fun the question is is getting funny, but the question becomes, should I switch to fiber or use ipv4? And then what will bring the most difference is changing the access technology or kind of using different pairings will bring the the most results. To answer the question, I use RIP Atlas, and then run queries from the probes connected to my ISP network. In are the results for V6 I took the 10 most, fastest results seems like the 5 first probes are connected to fiber because they have a lower RTT. They're all kind of in the same range and then the 5 lower ones are, connected to GSL. Let's look at v 4 and keep the v 6 results in gray, to compare. Here it seems that changing the address family does also improve latency, and by quite a bit. So where does this address family end to end latency come from? To answer the question, I ran some trace routes. As well from the probes And if we look at the results first, we're looking at the trace route at a very particular point. Just as it exits my ISP network and enters Google, And if we look at the density of the trace hot RDs, we see that in ipv4, there is a slide in in, there is a slight increment in RTT, but not that much. We look at the results in V6, it becomes apparent that the problem was are pairing together. It's causing this difference in RTT. So now let's okay. That was a small anecdotes. Let's hard let's ask smart question now. So Is this common in the internet today. Is there a one that is always better than the other."
  },
  {
    "startTime": "01:32:04",
    "text": "Or are these differences spread? Are they come onto the source do they depend on the destinations and do they change over time? So to answer this question again, I use Rapathlass and I use all the data that is available in rapid trust, from the 10,000 probes, towards all the anchors. So anchors are single machines, so they're well connected. Through the word, And they do HTTP tests over they do a simple get request and response over HTTP over V4 and V6 kind of at the at the same times and periodically throughout the day. If I take all the measurements, And I get a lot of them just looking at the 1st week of June, and then compute the ratios between before NB 6 request completion time, unplug the CDF, we see that it it's a It has a head shape. It is quite centered. So no address family as a global advantage, one thing that we can note is that there is a significant difference in some cases. Let's look more into those differences. I've taken all the measurements as a world, but now let's look per source and destination. Let's isolate per proven anchors. And we have a lot of them in our data. And all of them I've I've roughly more than 300 HDB test because over a week, they do test every 30 minutes. Let's try to classify the differences in request completion time between ipv4 and ipv6. And I performed some statistical tests to try to categories them and found out that actually there there are 3 categories IPV 40s best meaning lower request completion time IPV 6 is best. Or known as strongly better as per my statistical test. And, actually, the categories are quite well spread. There is, a good chance that you will fall into 1 of the 3"
  },
  {
    "startTime": "01:34:02",
    "text": "but ours will those results stable over time, and from a given probe, Like, from my home, is always one better than the other? I just disable the the bad one? To look into the first question, the stability of a time we did some change point analysis on the request completion time over time. And then redid the statistical test on each segments We took, segments of at least 30 points. So that's that covers 10, 15 hours in the day. And found out that there is a difference between the wall classification for a week, and then some segments of the request completion time that got classified differently. And so There are the differences are mostly stable because consistent is the greatest category but there is a significant dynamicity Sometimes you get better latency with 1 address family, but then it changes during the day. It can change this to none is better, but it can also flip regarding the other questions, whether it was coming to the source, no, you just can't not shown on the slide. But you can't just disable one. If you do that, you cannot reach all destinations I mean, all anchors of right with the lowest latency. So at this point, I want to emphasize that latency differences are real, at least from the vintage of Rybatlast, which is quite a bit of quite a big, vantage points. And sometimes they play in favor of ipv6 but sometimes they don't. And so in this day and age of rate, rising importance of latency, and ipv6 transition, it is very important to test IPV 6 for latency as well. And to improve innancy is behaving. IPV 6 has been is behaving in terms of latency. If you wanna improve things now, there's also something that we are looking into our research article, which is finding a new way to select, the address family"
  },
  {
    "startTime": "01:36:01",
    "text": "and using the lessons that we learned from our rap analysis. I'm just gonna go over rapidly what the questions that we are looking into the article so we are looking at two questions. Knowing that the differences happen most And, after the user network, we're asking ourselves all to steer hosts. And we're taking this this, operating point where we're looking at how the DNS resolver running in the I mean, being the boundary between the user network and the And the one could help because it was also the boundary between domain names and IP address and eventually IP address family. So you all know that ipviable prefers ipv6. There is aiable version tree now that is using HTTPS records. And what we could do is when we know that one is better than the other, we could give priority using a GPS records. For experiments, what we did is just use mappedaddresses. It hurt, It is for experiment. It prevents the fallback. But it is good for experiment. Then the next question we looked into the article is how to select the best family we formulated this as a problem, balancing exploration, and exploitation. This is classical of informal learning problems. There is more to read in the paper you're interested in to kind of research work. And I'm gonna jump you to the conclusion to but let some room for questions, if if any. So we're interested more than justice before V6 problem, we're more interested also in ipv6multio when you have multiple ipv6 provider, it is expected that one can reach one destination faster than the other, but not in all not in all cases, not depending on the destination, for instance, and so this kind of techniques could also apply there. That's it. Yeah. Perfect. First question."
  },
  {
    "startTime": "01:38:01",
    "text": "I I'm I'm Simon Linin from Switch. Thanks for the talk. My first reaction to this slide 18, whether RTT changes, on the first Trace Rod hop in the PN Network and the Google's network is that the return cost must be different because even the base even the lowest RTT and the right beige, graph is much higher than for ipv4. So I would assume that ipv6 traffic is routed along a different path back to you. Okay. So, but, of course, this is just, like, minor detail about it. It Interesting. I I think it, it warrants some thought because, the aggregation is different in ipv6 versus ipv4 and Thus, the possibilities of doing traffic engineering are probably different. I, from my experience as an operator, topology is not very different. We have exactly the same path to Google or all the other peers of IP 4 and IP 6. That may have been a difference maybe 10 years ago. Today, I think it's largely congruent but, there may be other differences, hidden here, Okay. Related to Routing practices. So that's, maybe an interesting avenue for research. And, also, I I think your observations are a good argument for doing multipath, but probably, you know that. Yep. Alrighty. Thanks a lot. Thank you. Thank you for the comments. Coming. Tommy Poly from Apples. I just wanted to say thank you for doing this work. I think it's interesting to see yet, like, another way to look at these numbers, Out of curiosity, I was just kind of running some of the metrics we see now for V4 V6. And You know, they're very close, but still, you know, V Six globally does seem to be"
  },
  {
    "startTime": "01:40:01",
    "text": "faster. I think we've shared some of these numbers. Before, particularly at the, like, you know, p90 levels. One of the things we found before also is V 6 support, you know, correlates highly to other better things in your network. Like, other I see. More modern protocol stacks. And can certainly bias things. So I think overall the numbers you know, Wait. In your story over here. What should we choose for stack. Like, you know, let's still do V Six because in general, it's getting you onto a a better version of the Internet. But I also wanted to, point out for the happy eyeballs work, as you mentioned, for V3, I I think it's really interesting what you're bringing up around priority and how the user priority can allow us to potentially indicate the quirks of which works better or not. And the incorporation of priority into the algorithm is still in flux. And so I I'd to see some of this data and some of these experiments, back into that work so I encourage you to write to the list or file issues to help guide that work. Okay. Thank you. The V Six ops list, right, you're talking about? Yes. It's not adopted yet, but, yes, the work we're discussing in basics ops. And tell me just bring some measurements next time here. Yeah. It should. We haven't refresh our measurements in a while. Yeah. Alright. Eric, can you hear also from Apple? Clearly, we think this is cool and care about it. Thank you for doing this. This is really interesting to see. And I think one of the things that is really cool here that we don't have in the data from what we're we're seeing in the wild as it were, is that in our data, we're not doing an AB comparison for exactly that same route. And so part of what we're seeing is that in cases where V Six wins the happy eyeballs race, those situations tend to result in faster, lower latency, faster HTTP requests, and and we see that is consistent across HGP and just, you know, like, the RTT estimate from TCP and and quick."
  },
  {
    "startTime": "01:42:01",
    "text": "So, maybe that is a statement that happy eyeballs is helping and or working, in that the thing that we see being preferred tends to be faster. But faster here is not faster for if we did exactly the same route. Exactly. It's really the rate at which we see V Six winning that race. Tells us whether or not we think v 6 versus v 4 would have been Exactly. Also, the timer for racing could be It's larger than the delays I'm looking during the study. So I'm looking at smaller improvements, that get past the this timer Yeah. And we, like, we will only put b 4 in front of v 6 if it's more than somewhere in the tens of milliseconds faster based on historical data too. So if if you're seeing a very small delta, we'll still try to stick with V Six for non latency reasons. Okay. Thank you for all the comments. Thank you. Yeah. I think there's more than more things to talk offline. You so much for your talk. And we continue with ipv6 Okay. Yeah, welcome. My name is Johannes. And I'm here today as maintainer of the ipv6 hit list. Currently, this is shared work, of the Technical University of Munich with the Max Blank Institute of informatics. What isn't hit list? Why do we need it? Well, basically, the IP six address space is invisible to scan. It's way too large. It's sparsely used only. So to cover it in research as well, we need to find other solutions. And a common approach here is to create hit lists with interesting and responsive addresses. One of the major hit lists here was established by Oliver Casa in 1018. The main idea, it's accumulated hit list, collecting addresses from different sources, So for example, DNS resolutions or trace routes, it applies some filters for ethical reasons and afterwards it probes them on a regular basis for different protocols such as ICMP, TCP on different ports, and Uniper"
  },
  {
    "startTime": "01:44:04",
    "text": "it publishes those results on a regular basis for others to use. So you can apply to this service and use the data for your ipv6 measurements as well. We still maintain this. And we updated this in two steps. 1 published at IMC last year and 1 published at TMA this year. And we asked us several questions first. What is the current status? How did it develop? Second, are alias prefix, so called alias prefixes still relevant or still a problem. What they are, I will talk about in a future slide And at the end, can we improve the service to make better use of Now, let's talk about the history, and some results here. Also apply to the DNS stuff from Suresh. There are some similarities here. In the beginning Oliver Gusser collected roughly 90,000,000 addresses in 2022, we had seen at least 800000000 addresses once 250,000,000 out of those are filtered as they are alias, 4 105,000,000 are filtered as they were not responsive for at least 30 days. So afterwards, we reduce the load on the network and try to improve our skins. However, we see that up to us were responsive at the end. But if we see, if we look at the distribution across protocols, we see that most of them were responsive to DNS at the end. So UDP port 53 probes and in comparison, ICP only 3,000,000 addresses were responsive. So we wanted to look into that why that is the case And at the end, actually, we found those are injections and no real answers. So we probed for DNS with a query for www.google.com. We received more than one answers for most of them. All the address were invalid in our case, to redo addresses definitely not used by Google."
  },
  {
    "startTime": "01:46:00",
    "text": "If we checked another domain, we see all no responses if we check the domain under our control, we saw no query at our name servers, and yeah, all of them came from autonomous systems within China, so crossing the border there, at the network somehow triggered those injections, no real answers seen here. So what we did is we filtered all of them we cleaned the hit list, we cleaned the historic data, So all historic data is clean by now and can be used again. And What we see, we see a steady development of the service. There is a small increase it's not that easily visible with the logarithmic y axis here, but at the end, ICMP is the most responsive protocol with the 3 million addresses in 2022 followed by the GCP protocols, and then UDP 53 and UDP 443. Next question will basically, why is UDP443 so bad here. Why didn't quick pick up more? And this is mostly due to those alias prefixes that are filtered. The idea here is that the ipv6 address space is definitely large enough a single host can use a complete prefix as an alias, for himself or itself, So a single host can use all addresses within that prefix and just respond to them. This is visible. This is definitely the case. And the single such prefix even if it's only a slash 64 is infeasible to scan, and would induce large biases. To filter them, different suggestions in the past where to, well, you can simply generate 16 random addresses within a prefix that all of them are responsive is highly unlikely. So just generate 16 random addresses, they should not respond in theory, not all of them. This is done by the service and all those prefixes are filtered. When we look at them in A lot of them are actually announced by CDNs at the end. So we saw that"
  },
  {
    "startTime": "01:48:00",
    "text": "98% of the address space vastly announces in ipv6 is filtered by us all of them are responsive to some degree. In this case, and are filtered. We also see that a lot of domains resolve to those addresses or to those, prefixes. So for cloud layer, it's similar, 10,000,000 domains are within or hosted within those alias prefixes. And on the other side, we did some fingerprinting there to check whether it single host or multiple hosts, what we found is it's not a single host, but at the end, it's also not a one to one mapping. So we don't find a single host behind each address. There is some sort of load balance saying there is some other stuff happening here. Still all of them are responsive. You can create, hit list with multiple 1,000,000,000 responsive addresses, just taking the address space of fast, your cloud player, is not relevant. I guess and we still filter it in the hit list. But at the end, we suggest, that you use addresses out of those alias prefixes or fully responsive prefixes in your studies when you want to cover ipv6 as well. We published a list of those address prefixes on a regular basis as well. You can use them include CDNs there as well. Finally, we wanted to extend the hit list and we did 2 things here. One is relatively simple. We added category information based on peeringdb a community maintained database where everybody can add some sort of category to their autonomous system, We have this information. You can filter the hit list now based on targets you're interested in if you have this desire most of the hit list is categorized content ISP or NSP would should be expected here. But there are some out of educational or non profit categories as well. Can use that. It's also available for the historic data And finally, we set out to 10th at least. So only 3,000,000 addresses were responsive. They're way more address responsible at PV4."
  },
  {
    "startTime": "01:50:03",
    "text": "How can we improve this? And one common idea here is to apply target generation algorithms, and the idea is that addresses are assigned with a specific policy or idea in mind, and not randomly assigned in most of the cases. And if this policy can be learned, we can guess or generate new addresses and test them afterwards. There are a lot of those out there. We tested them in two steps in the first and in the second study, with different inputs and different configurations, just some examples. Of 4 algorithms here. They generate highly different amount of addresses. So Given 3,000,000 addresses at the beginning, 6 graph generated 125 candidates, and actually 3,800,000 were responsive. So those were new addresses we could add Others like 6 VAC L M only generated 70,000 addresses but still some are responsive. over 2 runs, we generated a lot of addresses. A lot of them are only responsive So a short amount of time, random guesses not usable, but we added everything to the hit list And by now, it looks like this The service is out there. We added new addresses. We remove all those injections. You see at the F in the figure below. That was the 1st study in 2022. We added addresses And then in 23, new addresses again. As I said in the poll, are still responsive but we aren't now at 9,000,000 responsive addresses you can use for your study, If you have any ideas or any input for us, we're happy To talk about that, are there questions? They are none, and we don't have time. But thank you so much for bringing your work and doing the work for the community, and you can talk to Johannes afterwards. So the last presentation, will be an online presentation and will be about, trackers which is actually quite relevant to current work in the IETF."
  },
  {
    "startTime": "01:52:10",
    "text": "Okay. Great. Hello, everyone. My name is Yasar Zaki. This is sort of a joined work, that we did with Mateo Vervello from Nokia Bell Labs in assessing a measurement measuring the the tax performance, basically in wild. So for those of you who do not know how these tags work. These tags are like simple devices. They're meant basically to locate lost items, and they work particularly in the sense that they send, Bluetooth weekends And if, they rely on sort of own bystanders, if there's, like, 1st passing by one of these tags and have a form that is capable of recording these, the tagged location. We'll listen to the beacon, and then we'll report the GPS location of the phone. To either a Apple or Samsung. Servers. And then the, owner would be notified by the location of the tag the the ecosystem differs a little bit in the sense that, for example, for Apple, it's a default sort of reporting. You cannot disable this thing. If your iPhone passes by and find, an air tag, it will record the location. On the other hand, for Samsung, it's only for Galaxy phones, and it's obtained. So by default, many devices do not report. Location of the tags. And that kind of already creates sort of discrepancy between the number of reporting devices in the wild. So recently, there's been a lot of news about these location, tags being used for, sort of, or being misused for stalking And we were curious basically to understand what is the accuracy of these, tags in the wild and whether they already could be used for for stoking. As a result, we basically, our experimental sort of setup was We had advantage point that consisted of a Shiyomi redmi phone, which was not reporting neither of the tags neither the smart tag or the Apple air tag,"
  },
  {
    "startTime": "01:54:00",
    "text": "but we use this the sort of true GPS location of the phone, was recording the GPS location over the phone in all time, and then we had a 3d cover for that phone where we mounted these two tags, the apple air tag and the Samsung air tag, and then we collected, basically, we built crawlers for Find My smart things, these are the apps that both Apple and Samsung uses, to allow the owner to see the location of the tags built crawlers to, store the location, the GPS location of these tags, and we're able to compare basically to the ground truth that the phone was reporting how do we measure accuracy? We say we we kind of vary 2 different things. We say that a tag is, sort of discoverable if it's within a particular radius of the ground truth and we vary this radius, we also look at the time. So how much time does it take until I thought would become discoverable in the in the correct location. So there are 2 always elements that we look. One is time and the other one is the the radius from the true location. I basically span 6 different countries with 20 cities, was about 2, 2 months, and we had about 24,000 different location reports and and and total. And we excluded home locations because home locations are very sort of peculiar. It's mainly stationary. There's a lot of devices that could report So we want it really to remove that. Order not to bias or screw all our results. Here, I'm gonna show you the accuracy of these smart tags on the left side, you see over a 10 meter radius blue, you see the apple, accuracy and in orange, you see the subsequent accuracy we're also sort of interested in seeing the combined effect because what if somebody uses both tags to basically increase the accuracy of stalking to basically being able to pinpoint exactly where the location of the victim is. And as you can see,"
  },
  {
    "startTime": "01:56:00",
    "text": "it's kind of flat out in about 60 percent accuracy. So 6% of the times you're able to find the location of the, victim at least after 2 hours. If you relax, basically, the condition you go to a hundred meters, which is the figure on the left, we are able to increase are you able to see, like, inaccuracy of about 70% interesting is the combined effect is not really sort of, an addition of the 2, there's about 10 or 15% addition in terms of, like, accuracy, but it's not really a combination of, like adding, the 2 2. Additionally, we're also curious to see, like, different effects of mobility a day of time, day of the week, and I'm gonna go very quickly over this because I don't have, that much time, but what you can see basically with stationary and pedestrian, these are like the the the ways where you can get higher accuracy. And then the more you start sort of in in terms of, like, bigger transit, whether it's jogging or in a car or on the transportation, the accuracy basically goes down there's not that big of a difference over the day of the time or the weekend, versus set a weekday. In general. We were also curious to see, like, what's the effect of the population density. So we use Uber hexagonal hierarchical spatial index to segment the location. This was one of the locations that we use is Resinable Derby, where we located, you see these exact and all things, and then we use the contra population density data set to get an estimate of the population size because in reality in the wild, there is no way of knowing how many devices are there. The hypothesis there is that you can kind of correlate the population density with how many devices that are capable of offering. And you can see here on the left side, when your density area there's a high chance, 25% of the cases where you're not gonna be able to get any accuracy. You have about a 0% sort of recording of find discovering where the location is this kind of drops down they in the near density and the high density,"
  },
  {
    "startTime": "01:58:02",
    "text": "to something, that is, a lot lower in in general between sort of the these these density locations. Now given that we don't really know the the ground truth of the devices that we have. We we wanted to set sort of a controlled experiment that we know exactly how many reporting devices are there and we set up a an experiment in in the cafeteria, the university, had our vantage point, in a high flux area, is at the cashier because all the students have to go to the cashier to pay for their meals. And then we, sort of with the help of our IT department, we were able to get estimates exactly on the number of Apple and Samsung devices in the cafeteria. Throughout the day, And here were the results, which was kind of interesting the bar graph shows you the number of devices glue and app, Apple devices and, and orange are the Samsung devices. And as you can see, we had about six times higher number of Apple devices in the cafeteria versus the Samsung, However, what's interesting is the update rates of these tags, the air tag and the smart tag, Both of them, were able to get about 15 updates per hour, regardless of the number of devices that you I mean, Samsung was able to kind of keep up neck and neck with Apple in terms of the update traits, compared to, despite the fact that there's a lot less number of users. Here we bucketize things as well in terms of, like, histogram, of how many devices and and binning them and you see the same thing. Apple, Samsung has a lot aggressive update rate compared to Apple and that could explain as well why in the accuracy figures that we saw earlier both of them have like very similar accuracy, numbers. So we're curious why, this is sort of the case and we measured basically the signal strength versus the distance Here on the left side, you can see the air tag, numbers at a different distances from 0 10 and 20 meters on the right, you have"
  },
  {
    "startTime": "02:00:05",
    "text": "the smart tag, which is the Samsung 1, and what this figure shows you is that Samsung is a lot more aggressive in terms of like sending higher beacon power. And they send also these, beacons more frequently. With which which that they have less amount of, devices around that could report. And as such, that's why kind of, it's able to keep up with, with, Apple AirTag sort of performance However, this comes at an expense of the power, of the battery lifetime between Apple and and Samsung. So in conclusion, these tags could sort of provide good accurate measure in in general and within, like, could have, like, an hour or 2 hour period, but it's still kind of, far from being able to, like, stalk a person like, a high resolution sort of accuracy and more research is, of course, needed to study basically what should be used to combine the misuses of this. We are repeating the experiment this year again. Towards the end of the year by adding more tags like the, tile as well. And we're doing this with with the amigo sort of, framework that that we've built where we have 5 g capable phones if anyone is interested in sort of doing some measurements in the wild with, like, multiple devices across globe, please reach out to us and we're happy to kinda incorporate this as well. Right. That's it. I know that I'm on time but if there are any questions, I'm happy to answer or even offline. Yeah. Thank you so much. Very interesting. And please bring up bring back your updates next time very quickly. We don't have anybody in the queue and we are out of time. So please contact Yasir email if you have any further questions. Thank you, everybody, especially to all the speakers, everybody who contributed, and also Brian Tremmel taking notes, and see you next time."
  }
]
