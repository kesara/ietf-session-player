[
  {
    "startTime": "00:00:28",
    "text": "So There he is. areas, areas, like, I was I was late to the previous session. I was sharing too, and now my whole day is back up. buffer blowing my calendar. a few more attendees as well. Stewart's taking notes. See what? Stewart's taking notes. Oh, very nice. Yes. Yeah. I think so. It seems you should be working. Thanks. And then? So with the chest slides, agenda section that looks like that. So we don't Perfect. Those 3 are first session. Those 3 are second session. That's awesome."
  },
  {
    "startTime": "00:02:01",
    "text": "wanna own a door. Right? Okay. Yes. So have to jump in if I forget anything. Yeah. Yeah. Okay. Let's start. So Welcome to So you DNSST meeting for 117. We've actually got 2 hour long sessions this meeting. So we've got this hour at a break, and then the second hour. We have the usual note well, so Just a reminder that these are our policies. by participating in this meeting, you agree to abide by them. So if you have any IPA, then list you just a really quick trivial question. Are we planning to have a break and grab coffee or are we planning just to work through Where is it? people know what to expect. We're planning to have the break. Okay. Oh, right. So the for for particularly for those joining remotely, the second session will be a different Mito Co link. So in the note world, yes, if you're aware of any IPR, they that has to be disclosed. and you agreed to abide by rules regarding engagement and being nice to people. we take this seriously. So it hasn't generally been a problem. next. So Stewart has kindly volunteered to take"
  },
  {
    "startTime": "00:04:03",
    "text": "minutes, which will be just recording substantive decisions or or actions. if you You should, on your client, have access to the chat. which you can also which is provided by Zulip, available also in Weeteko. And as I said, we've got 2 different me take a link. So the first one of those is for this session. we have some handy links for for information for this working group. the charter, the documents, the mid materials collected for all of this meeting. which is basically the slides and the agenda. And, of course, we have the mailing list, which is where we decide everything. We also have a GitHub organization So that's where we maintain our working group documents. You can find them all there. You can raise issues and pull the requests. If you have another document, you can contact the chairs to. get that set up. So for this first hour, this is what we have on the agenda. So We're gonna give you an update on both SLP and update lease. Ted is going to tell us about the latest on advertising proxy and we'll then go into some discussion of issues that have been found with SLP replication. So, hopefully, that should be quite an interactive session. So please jump in. We've got any ideas. Yeah. This is one of the benefits of meeting that we can we get to debate all this. So for the second hour, which is after the break, We're gonna go and cover TSR and experiences, that have been found in terms of"
  },
  {
    "startTime": "00:06:03",
    "text": "conflicts that can arise and how to manage that. So we need to up with a good solution there. We'll also have some discussion on future work options. So We'd like your input on that. And then the last one is Nate's multicast stream discovery. So Stewart posted on the list about that today. with some thoughts. So that will be good to share and see what you will think of this and whether you're interested. any requests for changes in terms of the agenda for the first or the second now. In that case, So We have 2 documents that we are progressing, So They are both on the ISG tele chat for the 10th August Yeah. 10th August. the colored dots there tell you that the current status of the reviews So we've got one discuss, but most of it as yet to indicate the status, but that's normal. Anything else we wanna say on the statements of those No. No. You'll get the message. So we we vary the input of the ISG, of course. Yeah. Can you -- When is telecast? -- at the for the remote Yeah. So the question was, am I intending to do an update for the telechat? So when what teluchat is this on now? o 20. It's 8:10. I should be able to do an update for that. Yeah. Yeah. I I didn't see anything in Roman's question that seemed like it was gonna be like"
  },
  {
    "startTime": "00:08:01",
    "text": "impossible to deal with. So we are. Thank you. and advertising proxy. So We adopted this a while back. were comments made on the list during adoption. There was a minor update in January this year. The document actually expired 2 weeks ago. but Ted will tell us more in a minute. So that's it for the chairs. The blue sheets are virtual, which means you just need to log in to meet echo, but we have a physical reminder that I'm gonna around the room. So don't pass it to the next person until you've actually signed them. virtually. Yes. Oh, and did you have an 8 cars? Was he on the agenda? Yes. a perfect So on the topic of the draft expiring, I actually did submit a dash 3, but it somehow got lost by the data tracker and it expired. I don't know what happened. Yeah. Yeah. possible that I submitted it, and there were some nip that I didn't notice. And so I thought I was done, but I wasn't. but Anyway, anyway, anyway, I'll be submitting an actual update soon. Okay. So next slide. I'm Ted Levin, by the way. Bye. So we've had some discussion about what exactly should be in this document and We went back and forth between it to just talk about the advert advertising proxy or it should talk about the entire general set of functions that of something that's doing the advertising proxy function might do. And so right now, the draft is still kind of in that second mode. But"
  },
  {
    "startTime": "00:10:01",
    "text": "I think that the general agreement after the last meeting was that we would just have a talk about the advertising proxy function. So I made a little bit of a start on that I'll post an update. probably today or tomorrow with that, that, but I haven't taken all that language out yet. So I'll talk some more about that later sort of the end of the next session. Next slide. So the first thing I wanna talk about is operational experience. This is actually the the advertising proxy exists in a lot of devices that are consumer devices that that are being used specifically, the the use case that I know of where this where this is widely deployed currently, is in thread border routers both from Apple and Open thread, which includes Google and various other producers of thread border routers. And we've had pretty good experiences with generally. There have been a few issues. the main issues have to do with name sort of spurious name conflicts where we get a name conflict that really shouldn't have happened issues with scalability when we have either a lot of border routers in the same home network or or a lot of devices. and also just problems with having more than one MDNS register our responder or whatever you wanna call it. So we'll talk I'll talk a little bit about some of that here, and some of this the 4 presentations are actually informed by this presentation. So some of this stuff, I won't get detail about in this presentation, but we'll be discussing other presentations. Next slide, So So the first issue is that, you know, for redundancy, we wanna or for reliability when these devices which are consumer devices that may be, like, on light switches. So somebody might just"
  },
  {
    "startTime": "00:12:03",
    "text": "flip a light switch and have one of these go away. We want them to survive that experience. We want the the the functionality to survive that experience, and that means that we're gonna wind up having more than one of these devices active at a time, The problem with having more than one of these devices active at a time is that That means that they're all trying to register essentially the same names. and we see name conflicts. We're not supposed to see name conflicts, And we added something called TSR, which I've presented on in the past to try and address some of these conflicts. but that hasn't fully been resolved. So The reasons for these are often a little obscure. We have situations where like, the the process of marshalling a probe message results in some, but not all records be included in a particular probe and then the Receiving MDNS responder, which has the full set of records sees that subset of records and says, oh, well, this is obviously a conflict because These aren't the same. And so we we see a name conflict that's that's just totally a self inflicted wound. And then, you know, we can also see situations where 1 publisher sees a conflict on the host name, and one sees a conflict on the service instance name they both wind up not publishing, which is really bad. And, also, like, when, for some reason, the network renumbers is threadnet sometimes do. We see name conflicts just because there was a sudden change in essentially all of the records register on the network and chaos occurs and some conflicts occur. So next slide. So I talked a little bit about this. Basically, we have a packet fitting algorithm we try to put as much stuff into a packet as possible. And so if a bunch of updates happen at once and a bunch isn't really that many,"
  },
  {
    "startTime": "00:14:01",
    "text": "depends on how many records there are in an update. then we can get into that situation I was describing where we get a conflict because the the information is incomplete in the probe but complete in the auth. database of the recipient of the probe. So Next slide. And then the service instance thing, I'm not sure exactly how this happens I think what happens is that we get into a situation where the TSR record has a time difference of 0 And so that means that we have a conflict, and then we try to a 56762, which basically says sort the records and and the one that's earliest in the sort wins. And because we have multiple records in an SRP update, we wind up seeing like, the host name might might win on one server and the the service system might might one on the other. And then because of the way we've implemented the advertising proxy, the advertising proxy wants to get everything registered or nothing. And so each one of them gets half And then they both withdraw because they got a name conflict, and then we have no registration. So an additional problem here, And this goes to the self inflicted room thing again as you know, because these are these are conflicts that really needn't happen. They aren't actual conflicts. So and I'll I'll talk about this in more detail in the TSR document but I'm just talking about this here because it's relevant to the advertising proxy work So matter, which is a big consumer of thread right now, and also of of DNSSD in general. enforces name uniqueness. Every name is made unique. when the device is onboarded to the network. And so MDNS normally deals with name conflicts by renaming."
  },
  {
    "startTime": "00:16:02",
    "text": "If we got a spurious name conflict where there really isn't a conflict, renaming doesn't work, so we need some other strategy to deal with that. And so our current strategy of either asking the client to renumber, which it won't. or or as to to say choose a different name. or choosing a different name ourselves, neither one of those works. So next slide. By the way, I have a tendency to go through these slides really fast if something that I'm saying here that doesn't make sense to you. the whole point of us having 2 hours is that If somebody has a question, they can ask the question. So please don't feel like you have to wait till the end of this presentation to ask questions or criticize or comment or whatever. Anyway, moving on. So in a stub network, like a thread network or, you know, what we're discussing in Slack. It's the the numb the numbering on the network is associated with a particular stub router. So snack describes this, and and we do pretty much the same thing in thread. whichever stub stub router shows at first picks a ULA address randomly. slash 48. The slash 48. and advertises that on the network. And, you know, we have a strategy for dealing with like, 2 routers arriving at the same time. What do we do and stuff like that? The results of that is that sometimes the network winds up rednumbering. can remember because tub router went away, a stud router went away, or it can remember because because of the sort of conflict the beginning. We And so as a consequence, when that happens, we might have registrations from all of the hosts on the network, suddenly, all of the hosts on the network have different IP addresses. That means we have to do a whole bunch of MDNS updates all at once. and that that the chaos that ensues their tends to produce name conflicts. And, you know, some of the things that I described before, like, the the the the resource record marshaling issue, I think, are responsible for a lot of that."
  },
  {
    "startTime": "00:18:01",
    "text": "So Next slide. So can have a lot of different kinds of conflicts. on You have conflicts between stale data and fresh data, which TSR is supposed to deal with. we can have conflicts because there are actually 2 independent services claiming the same name, That's an actual legitimate conflict as opposed to a self inflicted wound. have the partial conflict I described before. or the host name and the service instance name of them is in conflict, and the other isn't. The other thing we can have is conflicts between name spaces. And I didn't actually draw a slide for this, but Basically, the idea here is if you think about MDNS, MDNS is a namespace that applies to a particular link. So a particular multicast domain. So if you have more than 1 multicast domain, have more than one namespace. Similarly, SRP while it's not a multicast domain is essentially updating a namespace. It's updating a database that contains all the names in the namespace. And so when we're thinking about conflicts, The way that we think about conflicts needs to take into account the fact that we have all of these names cases. What do we do when we have conflicts. Essentially, when we try to represent multiple name spaces in a single namespace, MDNS on a particular link. a disambiguation problem becomes a name conflict problem. And so So part of when we think about conflicts, we need to think about this in terms both of actual conflicts where there are two things that really do have same name, and we need to do something about that versus situations where we simply have ambiguous names. and we might wanna do something different to resolve that issue. So And and, you know, so and as you can see, really only item number 2 here, the conflict"
  },
  {
    "startTime": "00:20:02",
    "text": "a conflict between 2 independent services is actually a conflict. Everything else is something that we wind up treating as a conflict that is not So next slide. So I've been thinking about this for a while, and When I say this is a proposed solution, I do not mean to present this as a fed accomplice. I'm proposing this very much in the sense of a proposal. I think this might work. I'd love to hear people criticize this proposal or suggest ways to improve it. And the proposal is basically to go with that namespace idea. An SRP zone is a different namespace than an MDNS cell. If you have multiple SRP zones, each SRP zone has its own name space, you have multiple links, each MBNS' own is its own namespace. If there are conflicts between left most labels, in in those name spaces, We deal with that in a different way than we would deal with actual conflicts. We don't do the MDNS conflict process. in other words, if you send an SRP update to a zone, then If there isn't a conflicting name in that zone, then we don't report a conflict to the client. because it just doesn't work. So So we're never gonna ask an SRP client to rename because of an MDNS conflict. which is a change from the way we're behaving now. And so I think there are potentially some implications of that we need to think about. But I think it's probably the right thing to do. So the nice thing about this is that it means that SRP replication gets a lot simpler. and SRP gets a lot simpler. Right? We're just replicating a database."
  },
  {
    "startTime": "00:22:02",
    "text": "we have a pretty good strategy for how to replicate the database as long as we don't have these weird and DNS conflicts. And so we can just, like, when SRP client registers, we update the SRPiso. And then replicating or or presenting the SRP zone in MDNS is separate issue. We need to deal with the disambiguation problem there. not in SRP. So and then the question is how do we deal with the conflict on the MD and S side? So And this is a little bit problematic. are a couple of issues here. I mean, one thing that I would love to do is just have some additional identifier, like like you know? So dotlocal is the MBNS zone. Right? So we could say, okay. Anything that's in your SRP zone that has an SRP dataset. I ID of this 64 bit number is that sixty four bit number represented as, let's say, a what do I say? base 64 name, dot Sorry. Name, the the leftmost label, that was registered with SRP, dot, that 64 bit thing, dotlocal. only problem is if you do a query for anything in the dot local, The that is not and this I've only checked this on m d on on Apple platform. So I don't know what happens on Linux. But if you try to do this on apples platforms, food.bar.localproduces a DNS lookup, not an MDNS Right? It's a mystery. I don't know why that is, but that is the case. I checked it in the logs. So So that would be that would be my preference for how solve this problem. But, unfortunately, that won't work. So another proposal that I have. I mean, one one way we could say that is, look, let's just fix that, maybe maybe we should just say, you know,"
  },
  {
    "startTime": "00:24:06",
    "text": "if if you're using a device that's gonna be doing queries of this type, then you need to fix that bug. And that might be a legitimate thing to do. I think we'd have to really think about that knew I'd eventually provoked Stewart into getting up and saying something. This is where I have to sort of dredge back my ancient memory I think the reason the code does that. dates back, 20 years. because when we started using dotlocal for local discovery, A a large company that makes a dominant desktop operating system decided that they would configure all of their customers' active pre surface to use dotlocal and that would sabotage bonjour. And they could have picked literally any other suffix to put on the end without causing trouble, but they day day it. They configured by default and had web pages recommending that every enterprise in the world should use dotlocal as their internal domain I thought we're bitter. And I think that led to us having to do a bunch of workarounds because obviously, people who work at companies with Max won't be able to check their email. and And and I think that's why we decided that While in principle, anything dotlocal is multicast In practice, most host names are a single neighbor singlelabel.local. So anything that's 3 or more labels, was probably destined for the active directory server and not for Lindt Global. Yep. That's not philosophically clean. And it may not even be necessary anymore because"
  },
  {
    "startTime": "00:26:01",
    "text": "After 15 years, who knows how many of those old dot local installations still exists. Noelle. the sabotage was very effective. Yeah. Yeah. Yeah. Okay. So, I mean, another new thing we could do is just say, use something under service.arpa in the same way. Right? And that's that's again assuming that we're willing to change all clients that need this functionality. problem is I don't know that we really wanna do that. So the other alternative is we can say, okay. We're just gonna append something and and this is actually how MD And S renaming now. Right? You just append some number to the end of the name. Like, if you have 2 HomePods, The first one that comes on the network gets the name homepod. second one that comes on an effort gets the name Homepod 2, I think, And so you've got HomePod and HomePod 2, and that's just an automatic renaming process MDNS responder does on on Macs. So we could do something similar to that But we have a slightly different situation because with SRP, we actually have a stable namespace. So it's not the case that we're gonna have, oh, we've got a conflict with the SRP thing, and then we get another conflict, and we need a new number, and we need a new number. We can pick one number that's always the number for that namespace. probably needs to be, like, big enough that it's not just like 1. But some number that's that's always gonna be used for that name space. And we just append that to every name that's registered in the SRP zone. So so SRP you know, particularly if you use the default dot service dot arpa domain name when you're doing a registration SRP is essentially just managing the left most label. And so if you've got like a a PTR record, it's gonna be leftmost label, PTR, leftmostlabel.service namedot transport name,"
  },
  {
    "startTime": "00:28:02",
    "text": "and then you can forget about what's to the right of that. And so we can just rewrite that. and and that gives us the ability to very cleanly merge these name spaces without doing weird conflict detection behavior. So that's my proposed solution. I don't know if that's the right solution. I've also talked in the TSR presentation that's coming later. about how we could improve that but that doesn't solve the the names play namespace conflict issue, you know, when we actually have a legitimate conflict, possibly multiple name spaces. So Jonathan Hui. So Yeah. I mean, I agree with you separating the namespaces with simplify a lot of things, say, would really love to go down that path as well just based on experience -- Yep. -- trying to implement this stuff. I don't know if any of the solutions proposed. Allow us to not require modifications on the clients. Right? So -- Yeah. I mean is matter devices today look for a very specific need. Right? Yep. And so just depending something to the name We'll probably screw them up. Right. So you know, I don't have a solution. I'm just saying I'm Yep. Yep. Yep. Yeah. So, actually, that's a really good point. Thanks for bringing that up. what I'm proposing here is not that we always bend that. but rather that we only append that when we get a conflict. So thing is when we get a con if so if we get a conflict on a matter name, we're just dead anyway. Like, there's nothing we can do about that. So this doesn't make things worse and potentially it provides us with a future proof solution. Right? Another thing we could do is Since we're doing this like, basically, the the use case is probably the most common right now is gonna be matter, actually. the"
  },
  {
    "startTime": "00:30:02",
    "text": "And so we could just say, like, look. We need matter clients to update so that when they get you know, a query for you know food.bar.localorwhatever. whatever we decide that they do an MDNS query. And so then we just use that. And then the left most label winds up just being the the same as it always would have been. And so if matter implemented that correctly, which actually currently doesn't, Oops. we would have our solution. So I think there are a lot of ways to approach this. I I'd kinda like to come up with a solution soon because, you know, the sooner we come up with a solution, the the the quicker we can get our installed base running something that that we trust. because what we have now is a little bit haphazard when we run into a conflict Now the good news is If we can deal with the MDNS conflict problem, which I talk about in the TSR draft, this becomes this this aspect of the problem becomes less of a problem because I think that the problem of conflict between this joint name space is is actually relatively small. example, we should is that just shouldn't ever happen. if we see the same name advertised in MDNS and in the SRP zone, it should be the same matter device. So so in other words, if we don't publish one of them, it should be okay because it should be reachable through the other information Okay. Yeah. It's Jonathan. I'm just trying to think. So There are motor devices that I know are being developed, but support both, red, and Wi Fi. Yep. simultaneously. So I In this separate namespace world, we should also think through, like, what it means for those devices Yeah. Yeah. I mean, one of the one of the things is so so this is a little bit talking out of school, but The the way that that that we do DNSSD lookups for thread devices which I believe is in the current spec. So I can talk about that. Is we look in default that service center"
  },
  {
    "startTime": "00:32:01",
    "text": "So it just does a DNS query to default.service.arpa. default.service.arpa then returns answers that may or may not actually be in default.service. Our So we do have the option of disambiguating name spaces there. assuming that the matter clients actually do the right thing when they get names that are in in default service.arpa. don't know the answer to that question. I would like to think that what they do is they just take the left most label. I don't know if that's true. We should probably figure that out. So, anyway, so so that's kinda kinda where that's going. And I think, you know, I don't know how many people in this room are super interested in solving this problem clear that there are a bunch of us that are in we will put our heads together. But I'd like to I'd like to do that in the context DNSSD and not in one of the other standards bodies because I think this is highly relevant to the work that we're doing in DNS SSD. And if we have this conversation somewhere else. gonna exclude a bunch of people that should be in the conversation. So I may actually propose doing some interim meetings at some point. I know it's a terrifying idea. But that sounds reasonable. And, like, in terms of chartering. We can double check what this feels it makes sense here. So Okay. Okay. And and remember that it's always like, one of the big advantages of bringing things the ETF is that it provides a a form with an IPR policy, which is really important for top press. Right. Right. Yep. next slide. I'm not sure if there's a next slide. There is an excellent Okay. Right. Yeah. Scalability. So One of the other issues we're running into, and, again, I will talk about this some more in the TSR draft is the problem of having registers handle many names and having there be many registrars. I'm not gonna go into a whole lot of depth on this right now. because they go into more later. but but fundamentally, what we're talking about here is, like, you know, you got"
  },
  {
    "startTime": "00:34:00",
    "text": "First of all, if you've got 5 registrars, then every time a reg a a record gets replicated, it's gonna be probed 5 times. And that's a lot of MDNS traffic. Also, whenever an SRP Register r, Synchronizes it's gonna probe every single record. So you're gonna get this massive storm of MDNS. So we need to talk about how to make that how to load balance or whatever. I put this slide here. I actually don't know that it's I the the re the sense in which it applies to advertising proxies just that this is a lesson we've learned from deploying advertising proxies. I don't think the solution is actually gonna be in the advertising proxy draft. So we should probably not go into more depth about that now. Next slide. Yeah. So I guess we are talking about this now. So Yeah. I I've I've noticed that It it feels like when there are multiple registrars advertising the same records that we get behavior that is surprisingly decimal. like, a lot worse than I would expect, including things like probing continues forever. which is really bad So I'm gonna talk about this more in the in in the TSR draft. So I don't think we need to go into quite as much detail as on this slide. But on that's something we need to talk about. And, you know, things like you know, load balancing to primaries or backups or you know, whenever. Like, should, you know, should should backups even do probes at all. Should we have backups not respond to the first packet that gets sent. Like, if there's a query that's sent out,"
  },
  {
    "startTime": "00:36:02",
    "text": "could notice that the query was set out, count the fact that the query was set out, but not reply to it if we're not primary for for the question that it asked. a question that had asked. And then if we See the same question again. then we answer. So Anyway, next slide. So I've actually proposed there'll there'll be some new text in the next So version of the document about handling conflicts. because the current document basically just says that there's an MD and S then we have to like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, you know, take down the name and get it take it out of the SRP zone and all kinds stuff like that. and that I think is just a bad idea. It's very hard to implement. It's very hard to implement in a way that isn't that scales that doesn't, like, just totally fall over when you have a lot of stuff happening at once. The other problem is I was reading over the document. I thought, well, how would I implement this? it doesn't really give clear guidance for implementers. mean, you guys implemented it. So I I I think it's it's it's relatively obvious how to implement it. So the fact that it doesn't give clear guidance is probably not a huge problem, but the problem with vague They're they're not even vague. The state the the descriptions are fairly clear, but but there isn't normative language. There isn't a whole lot of normative language. And the problem with that is that I think you wind up getting weird little edge cases that we haven't documented, and then you find out about them later. So I think it would be worth going through the exercise of actually translating the current descriptive language into more prescriptive, normative language and see what turns up when we do that. then, you know, obviously, the sooner the better because, you know, And We're already we already have this stuff deployed. It would be nice to actually have an RFC. Next slide. We're done. Okay. How are we doing? So Yeah. We're okay. want to Alright? Not possible."
  },
  {
    "startTime": "00:38:01",
    "text": "questions, what I think. Yeah. don't need to pass it along. discussion, There does not appear to be discussion. Right? Next slide or next next slides. Okay. Okay. SRP replication. Next slide. So we we published an OOO after it was adopted. That expires in October. of things that need to go into the next version of this document, so I don't think it's gonna be a problem We've got a lot to update it before October. hope not. Next slide. So the goal of SRP replication is basically like said earlier, we have multiple devices in the network, none of which can be assumed to be powered on at all times. so we wanna make sure that wherever possible we replicate the database so that if one of those devices gets powered down, the database doesn't get lost. of course, they all get powered down, the database still gets lost and we have to recover using SRP. But we'd prefer to avoid that because for example, on a thread network or some other IoT network where we have devices running on battery, the fewer times they have to talk to the SRP server, the better. And the other benefit of replication, particularly for IoT mesh networks. is that it allows it allows it allows it allows the client that's updating to send its update along a shorter path than it otherwise would if if there were if that is just if there was just one server, then then some clients will be very close to that, and others will be quite far from it. So the way the protocol works for those who have an read the document, and I'm not I wanna be really clear that I'm not criticizing here. This is this is new work."
  },
  {
    "startTime": "00:40:00",
    "text": "And so I I do wanna tell people how it's going, how it's working. The the way it works is the the SRP client is the authority. It's essentially an authority for it's little zones. Right? It it it's the names that it's claiming that service instance names that it's claiming There might be more than one of those. And the host name that it's claimed, which, hopefully, there's only one of those. updates are signed. They use 60. authentication is on a first come, first served basis. So once We've gotten an update from a client. We know who it is. further updates from that client have to also be signed with the same key. And the most recent update is authoritative. We assume the client knows what it's talking about. So we had an update from it an hour ago and we just got a new update from it, the new update is correct, the old it is snail. Next slide. So operational experience, I talked about the conflicts. Generally, we we've been happy with how this works at a small scale. less happy as the scale grows. one of the issues that we see so SRP I didn't actually describe this in my brief overview, but SRP it's intended to be completely automated. Right? There's no user intervention. It has to come up and work without anybody doing anything. And in order for that to happen, since we're we It's a distributed protocol. Basically, somebody's gonna come up first or possibly 2 different devices or 5 different devices will come up at the same time and see each other's advertisements. We have an election process for which one actually which advertisement wins. And so During that process, the first the first guy that comes up in principle is starts up in sort of the the I'm gonna wait a little while to see other advertisements. But if I don't see any other advertisements, then I'm just gonna being the SRP server because there's nobody else here. And then other servers that come along after that have synchronized with first one before they start doing the same thing. So"
  },
  {
    "startTime": "00:42:05",
    "text": "The challenge with that is that it there that the way that algorithm works can result in nobody thinking that they're first. When that happens, you don't have SRP service. So you gotta be very careful about not like, allowing little weird corner cases to result in a stall. And that's not simple. So I've been doing a lot of work on on finding those corner cases and fixing them. And, you know so for example, I mean, one of the one of the quick solutions to that was just, like, If it's been x amount of time, and we haven't come up to the normal state, then come up to the normal state. the problem with that is that you can get into places where you might be looping through different guys being primary because of your timing issues or things like that. So as I said, it's it's it's a little challenging. Next slide. So I'll just go over the process of start up a little So the way startup works and this is obviously grossly oversimplified. But we'll assume there's just one pair that shows up first. It advertises via D and SSD. Next slide. a second peer shows up. it sees the advertisement from the 1st beer. it contacts the 1st peer and begins to synchronize. Next slide. Alright. Now a 3rd peer comes up. The 2nd peer is already finished synchronizing. The third one starts to synchronize with the primary. Primary, by the way, is chosen by election, whichever one has the lowest number I think that already specified, but I'm not sure. Next slide. And then another guy comes along the Okay. The third guy is still synchronizing because it hasn't actually synchronized with the second guy. Now the reason that we have to do that is because of what I said before, which is that the the SRP update is authoritative. So"
  },
  {
    "startTime": "00:44:00",
    "text": "when when a particular server gets an update, It replicates it to all the other servers that it knows. that is currently in communication with. So Server 2 came online after server 1, so it might have something So server 3 started synchronizing the server 2 It's not if server Sorry. Server 3 starts synchronizing with server 1. Suppose server 2 gets an SRP update. That update is not gonna be replicated to server 3 by server 2 because It happened after the after server 3 started synchronizing. So server 3 still has to synchronize with server 2 before it starts before it goes into the normal state. Next slide. Okay. Once we're done, we have connections between all of servers. Every server has a connection to every other server. That winds up being n times n -1 over 2. So if you have 5 servers, that amounts to 10 connections. Next slide. And so that's because of the authority strategy. Next slide. And you know, I explained a little bit about this. But, basically, let's say, the upper right Server is server 3. So server 3 gets an SRP update. is gonna send that to all of the other servers. the other servers don't send anything to each other. So there's no, like, cascading replication. It's just bam. We get an update. We update all of our peers who are done. So what that means is that any given SRP update is not gonna produce a lot of traffic. which is great. Next slide. Okay? So You saw that diagram. Like I said, updates only happen when an SRP update happens. So when you add a server, that means you're gonna send"
  },
  {
    "startTime": "00:46:04",
    "text": "the SRP update to yet another server. So that does add some cost, but it scales linearly. The problem is we also need to notice if a server goes down, and that doesn't scale linearly. because every server has a connection to every other server. So that scares that scales by n times n-1over2. And so that means that if you've got 20 servers, you have a 100 connections doing a keepalive you're more often than once a second. which is not a ton of traffic. It's all unicast traffic over TCP. It's not that bad, but it's kind of a lot for a network that's not doing any work at all. So and this hap that's, like, continuous. It's gonna be happening all day and all night. So to it probably don't want that. And so are we actually you know, thread limits SRP servers 5, mostly to conserve space in the threat network data but that turns out to be probably reasonable. In fact, 5 might be too many. because with 5, we still have 7 keep keep lives a minute. That's a lot. It's I mean, I think it's a lot. But that's that's what's required in order to notice that a server has gone down within 90 seconds. So that's not even, like, you know, the RRP kind of a lot. That's just know, what we're doing. Anyway, next slide. So currently, the SRP does not have a limit on the number of connections, which means that if you have 10 SRP servers, you each they're all talking to each other. So the Apple implementation currently limits active peers to 5. And we have our our implementation is not doing what I think it should at this point, but we had to do something. And so what we did is we basically have every peer connect to the peers that are advertising and but only 5 will ever advertise at a time. So So the other peers are just kind of standby."
  },
  {
    "startTime": "00:48:02",
    "text": "And with the standby peers, we scale the traffic to ensure that we notice that one of them has gone away within If if any one of them has gone away, that's kind of okay, but we wanna we wanna we wanna know that one of them is live every 90 seconds, basically. So that if we need a backup server, we have one. So that's that's what all this is about. But the only trouble trouble with that is that right now, the the standby peers are the ones that are responsible for deciding to connect. So if the way that they're gonna notice that an active peer went down is that their connection tool will down, down, And when they notice, they're gonna connect. The good news is that because we're only one of them is gonna is gonna query every 90 seconds. it should we've got 90 seconds to to get that one server back up and add rising, before the next one will happen. But if the next one connects, then we wind up with basically, we don't wanna have a thundering her. We don't wanna have, like, 50 servers suddenly connect because one went down. So that's where we are with that now. Next slide. Next slide. So I I've been thinking about this problem, and Aptan and I have discussed this problem a bit as well. and that happens in a you know, when we were just chatting in a you know, peer to peer. to speak. which is why I the reason I wanna I'm talking about having having a interim meetings is because I'd really like us to talk about this stuff in the working group and not just, like, you know, Eptan and I are sitting around talking and come up with this. But we talked about it a bit, and I've actually thought about it some more since the last time we So guess what? Everything we talked about is slightly out the window. But But it's all basically based on the same thinking, which is So first of all, what I think we should do is"
  },
  {
    "startTime": "00:50:03",
    "text": "First of all, as is currently the case, active peers published in MDS. The numerically lowest partner idea is considered primary. If a peer comes up and it sees an MD And S advertisement, then it's going to register its service, that it's capable of doing. not doing it yet, but it's capable of doing. Via SRP, using DNS over TLS 2, the primary. whichever whichever SRP server that's currently active appears to be primary. it's gonna send a replication session DSO message to start replication. At that point, if there are fewer than 5 peers or whatever limit we decide on, and I'm not even convinced 5 is the right if there are fewer than 5 or fewer than that limit, then when this registration happens, The primary is gonna know that there are fewer than 5, and it's gonna say, okay. Here's your session response. Let's go. we just start synchronizing. On the other hand, If not, then it just sends a retry delay message. And that says disconnect and don't connect for a while. at that point, if nothing changes, that guy is just gonna sit there idle. It's not gonna do anything. we don't need it to. We've already got enough servers. So now we don't have a TCP connection with that server. Nobody is talking to that server. It's just sitting there. But we all the the the the SRP replication because SRP replication is replicating its SRP update, all of the active servers know about it. And so now If if if 1 of the active servers goes down, whatever primary is there after that, can then connect to to to the one of the servers, whichever one seems most appropriate, and start doing SRP with it."
  },
  {
    "startTime": "00:52:02",
    "text": "so, essentially, now the primary is a little bit in charge of who gets added to the network which is a change from what we're doing now. But that allows for there to be sort of somebody who's in control, it allows for a way to hand off control to the next person if whoever's in control down, And We only ever keep 5 or however ever however many. sessions up. We don't keep we don't have to have a 1,000,000 TCP connections the the number of TCP connections does not grow with the number of available servers. So That's that's that's what have come to as as possibly the right way to approach this problem based on experienced thus far. Does anybody have any thoughts on this? So ops in here. So, yeah, Thanks. It's a one thing. Absa absent, can you tilt with my friend, Daniel. Thank you. Is it yeah, I So, I agree. general sense that, like, yes, I think you you see the same problem that you raised that, like, there are yeah, it just scales up quickly. You have too many connections between all and and, like, NSQUAR based he's going up. think you raised another key point, which is on the advertising proxy side. we now expect basically all the partners or peers are also on their sense doing the advertising proxy on the infrastructure level, which as you said, raises all those other conflicts. arm. I like the idea of primary. that the notion of this like, we have on primary, One idea is that can going even further and saying the primary is the guy is basically practically the only guy who's doing advertising the only authoritative SRP server, I think. other guys can act as the devices that can receive a circuit updates from other device they need to send it to the primary to do the advertising proxy side and gets a response that is there an incomplete or not?"
  },
  {
    "startTime": "00:54:01",
    "text": "and send the response back to the original receiver to send to the client or directly send to the car, whichever makes sense. and practically make it like like like like like like like the other guys are not really active like, the primary is the active one. And once it goes down, another guy Is it all the other guys have the same set of data? so that we know everybody has the sense that everybody is ready to become primary primary goes down. But they don't need to really act as a full SRP server in the sense of, like, I received an update. I need to parse it. I need to make sure everything is okay. I need to advertise the policy. No. I delegate, that's also to my primary. I send it to my anyways, I have to send SRP update message, fully encoded, fully basically encrypted as I received from the client to the other partner for it to be to be able to replicate can have the primary do everything and prepare the response that needs to be sent by And everybody else is also ready if the primary goes down, they can basically become elected as the new primary, and they have all the data. And, sort of, let let me go idea, like, that came the background of for this was sort of like the very Like, in threat, we have this notion of a leader in the threat network. it really is divided by the string, the coordination. It's not really not not doing any decision making per se. and every router has the same information as the leader. And when the leader goes on, other guys come another guy can come be elected. I was thinking the same model might make sense here that, like, we have the primary. If it goes down, another guy can become primary, and he can have preference. like, hey. You are a better or reliable device. You have a patchy backup or you are a more yeah, you are you have a priority of how who should be the next leader or the next primary partner. But but basically, it's getting out to say one guy as the main guy, and the primary is the main device that does So I'm not convinced that that's the wrong solution, but I also that also occurred to me, of course. You know, it's it's pretty straightforward. And the reason why I thought maybe that wasn't the right solutions because that then means that there's no load sharing."
  },
  {
    "startTime": "00:56:04",
    "text": "And if the number of MDS queries or the number of MNS services being advertised as small, that's not really a problem. Right? But as the number grows, Being able to load balance seems like it's a nice thing. And it turns out that I don't think load balancing is all that hard, particularly do some of the changes that I'm proposing the TSR doc. The way load balancing would work is basically Everybody, looks at The the So the messages are signed. Right? So when a message is signed, we take the message signature. We do a hash on that. And we hash that with the partner ID, of with our partner ID and with all the other partner IDs. And if ours is lowest, then we're primary. And ours is not lowest, then whichever one is lowest is primary. And what I think that would do is somewhat even of course, it's it's random. Right? But but it would somewhat evenly distribute the load so that now we have You know, each of the SRP replication peers replicating say you've got 5 of them. Well, now 5 now each one of them is replicating 1 5th roughly one fifth of the data. And as the size of the data gets bigger, that starts to become useful. Obviously, when it's the size of the data is 3, It doesn't matter. So I'm not wedded to that. There's there's a lot to be said for simplicity. but but it Certainly, what I've seen in our implementation is that it improve performance. we wind up scanning that list. The the algorithm for scanning the auth record list is an order of n squared algorithm, which, you know, arguably, we should fix. But right now, that's what it is. I don't know what that what the case is in Avahi. may be using a hash table, in which case it wouldn't have the same issue. it"
  },
  {
    "startTime": "00:58:01",
    "text": "but but but but So for us right now, increasing the number of off records dramatically actually really does slow things down. measurably And so And, also, like, we want forgetting for a moment about speed, And, actually, it's really not a speed issue because the the elapsed time isn't large, but the elapsed CPU timers. larger than it needs to be, I think. So so if we can spread it out, I think that reduces the CPU time and and there's some benefit in that. But And, also, it it it It means that if some part of the network is partitioned with respect to multicast, then we don't lose everything. We just lose that part. In some ways, maybe we better lose everything because that's a clearer indication of the user that something's broken. but we can also start looking at strategies to like, you know, for example, with the late response thing that I talk about later, strategies for having the backups also answer if the primary doesn't answer. Right? So So I think there's some benefit to to to spreading a load, but, you know, we can talk about that. So See next slide. One minute. Okay. Uh-oh. to the next Yeah. So let's see. Oh, yeah. One of the issues is we have lots of different devices that can all act as thread order routers in the thread case, but just more generally in the case of stub routers or whatever. Some devices are, like, obviously, the right device to have as the server. Like, if you have a customer edge router that supports SRP updates, should probably be your server because it can do all of this stuff over DNS. It doesn't have to do MD and to do DNSSD. So so that should win. On the other hand, if you don't have one of those and"
  },
  {
    "startTime": "01:00:02",
    "text": "at this point, 0 people have one of those. Then you have options like an Apple TV, which has Ethernet, you know, there there are like, Eero has routers. And I think Google has routers. I don't actually know the products their products not well, but but, basically, that have the same phone again. And those would make sense as, like, you know, these are really good devices to use for for acting as an SRP server. they've got other devices like, you know, speakers like a HomePod mini you know, it's fine to use that as a as a stub router if you don't have an option, But You'd be better off using Apple TV. It's got an Ethernet which you might be using. I think the radio in it is slightly better performing. And then there's devices like I think Nanoleaf makes a light bulb that's a order router and has Wi Fi and it has thread in it. And, you know, They did that because they're trying to sell product, and they might be selling into a home that doesn't have something else that could act as a border router. But, really, if there is anything else that could act as a border router and I say border router here because border routers are all doing SRP and SRP replication. So so That light bulb is probably not the right thing to be your SRP server. if you have a customer edge router that can be your as our p server. Simply put. So being able to have the ability to do prioritization in the SRP protocol seems like it would be a win. And yeah. Anyway, so we're, I guess, at our time limit. So I guess we could continue this after the break. Yep. If there's more I don't know how many slides are left. Okay. But I still think I think some folks have meetings during the break. So how about we take a 30 minute break? get some cookies which are absolutely delicious this year."
  },
  {
    "startTime": "01:02:04",
    "text": "There are no snacks, so don't get any cookies. I apologize for the false hope. And, of course, the ADs tell us the harsh truths. you all in half an hour? and we'll be back for more DNS's team. Ah, yes. And for for the remote folks, I think the the room will be closed, which will kick all of you out of the room. Make sure you click the agenda item for our next session when you join back because it'll be a different link. Oh, I need to go run an errand, not be back in half hour. Yes. desk. screen. Yeah."
  }
]
