[
  {
    "startTime": "00:00:29",
    "text": "Okay. It's 9:30. You are attending the computing in the network. Research Group, and welcome wherever you are. whatever time zone, morning, afternoon, evening. I appreciate all of you who are here in the room because this is the first time that I've attended an IETF in 3 years. So it's the first I've used a lot of these tools in person, so this is great. and I have my co chairs online. Gempey or Jeffrey, and Marie Jose. he's calling in from China, and Marie Jose from Montreal, Canada. So we've got the globe covered. or parts of it. Let's get started. This is just a reminder that the IETF has a whole collection of policies that actually in recent versions of of slides. We have been told that we should really note these very well. that There are various topics regarding patents and code of conduct in particular that are really important. and by participating in the ATF, you agree to follow all these processes and policies, One that is very sensitive, sensitive, of course, is whether or not your IETF contributions or IRTF contributions are"
  },
  {
    "startTime": "00:02:01",
    "text": "patents and that we like to hear about that. early and often. As a participant, if you are participating with audio and video implicitly, all of that. is make public, and you give your consent by participating here. We try to observe privacy policies. And I think most importantly, we really want to create a respect environment, where people are treated respectfully and thought fully and recognize that there's a really diverse audience of participants And if anything should arise where you feel harassed or you notice the someone else is being harassed. There is an ombudsman and process for handling that. and please do not hesitate to approach working group chairs as well. should you need an intermediate area for that. These are the places where many of those policies reside. Should you seek to have more detailed understanding of them. Everything from working group processes you know, I've already mentioned several of these, but also copyright as well. For those of you who are here, you made it here, for those of you online, you've all made it here, but please note if you do not already have the Medecco tool on your computer or personal your your phone. Please do so even if you're in the room because that's how we automatically know that you're here. The blue sheets are now integrated, is also integrated chat into the tool, and we will try to monitor both the chat and the queue for questions. So in person, participants Please use the the tool also not only to sign in, but to join the MyQ and for remote participants please have your audio and video muted. unless you're speaking and headphones were recommended. We would love a notetaker. Is there anybody who'd like to"
  },
  {
    "startTime": "00:04:01",
    "text": "jump in. It's a shared tool, so should you feel inspired, you can always join in. is there somebody who would like to that role officially. If not, we will go back in certain minutes while reviewing the video. This is the mailing list. Please join. We welcome your inputs. and the meet all meeting materials are online and all the slides have been uploaded. please note that there is a difference between the goals of the IETF and the goals of the IRTF that the research task force is really focused on longer term research issues, and it is not about standardizations and specifications per se, which is really in the province of IETF. And and so you'll see that the drafts that we have produced at least at this point in time are all informational drafts. There is also a wonderful primer. for irtf culture and for those attending the broader IETF. Today, we have a full agenda, so we're gonna try to stick to the timeline very strictly. So please do not take it personally. If we say your time is up, your time's up, I will one of us will try to keep you posted if you're running long. But the the way we're gonna run things today are that we're Maurizia Zay, when the co chairs will revisit coin and advances that have happened in the last few years the working group began. a little before COBiz. And then we're going to switch gears and have a couple of research presentations. I believe the first one is Ocesta and Yorg OT will be presenting. this is paper that appeared in"
  },
  {
    "startTime": "00:06:00",
    "text": "Music. What was it? Okay? And And then, secondly, we'll have the share nothing at JI presentation. we're gonna then shift gears into some of the research drafts We have Last time we heard about the use cases, this time we will hear about the analysis of were arching issues that cut a cross all the use cases. At the moment, it's title is use case analysis, but there'll be some discussion about what it actually should be called. We will also have a brief overview of where we stand with terminology draft then we will have a a couple of individual contributions that have not to shoot, but are interesting, quite interesting. And we welcome your discussion, and then what next? the we have quite a few documents. You can see that we have actually, if you go to the data the first three are documents that have actually been adopted by the working group, and there has been some discussion about adopting the directions for computing in the network, draft as well, but that is a that will be a topic for discussion as well today. But those that will be the focus of the the drafts, we have quite a few other related drafts. Again, if you go to the data tracker, some non expired drafts that have been contributed or that still exist as individual IDs. We have quite a collection of expired drafts. And I again, in the final part of our discussions today, we can have a we can review what our policy should be going forward whether these drafts should simply be remain expired or whether we should resurrect some of these and make them current. Without further ado, we're actually ahead. We have a couple"
  },
  {
    "startTime": "00:08:04",
    "text": "of extra moments. So I think I'm gonna switch gears and allow Marie Jose to begin her presentation. I'm going to stop sharing, and Mauricio say, would you like to drive from your side or Okay. you're gonna take it from there. We cannot hear you as yet. Yes. I I know I had to do article. Yeah. the the the right to to to hear me. Okay. So I'm doing this as a some kind of a dual hat as both the cochair of this research group, but also as somebody who's been working a lot and coin and related research in the past 4 years, And and I think it's it's important to see the how the landscape has changed And quite dramatically, to a certain extent, When we started this group, we had Of course, the tofino switches, which was a I call it the the shiny object. which was essentially telling us that there was a way to modify packets while they were in transit and the network. and that we could know, think of all kinds of interesting things that we could do with this And, of course, it was related to a a language that was used to program these switches p 4, And since then, there's a lot of I think data driven and computing driven approaches that have appeared that have nothing to do with Tofino. And when Tofino also was kind of put on the sideline by intel, Somebody told me, well, corn is dead, Well, actually not. because"
  },
  {
    "startTime": "00:10:03",
    "text": "I think in the 4 years that led to almost this is Tofino being being pushed a little bit aside? A a lot of us were asked what is computing in the network? What is the difference between computing and the network and edge computing. And everything else that's data driven in the system. I think I want this presentation, which is way too long. I think I'm just going to go through some yeah. I know you're you're you're smiling. I I look added this morning. And I usually tell my my grad students, though, if you ever hour, don't make more than 5 or 10 5 or 10 slides, and I have 18. And they're very full. But I will go through them, but I think I want this to be some kind of a a stake in the ground and say, you know, this is what happened. I'm going to just maybe glance over a number of things. but I think the title says it. In the last four years, what we had is we had an incredible rise in programmable devices, distributed systems, this and the the incredible rise of AI everywhere in the network and also our the network can help AI and, obviously, digital twins. So These things don't happen in a in a vacuum. And I want to thank three people who have been very useful for me in putting these ideas together and making sure that it was all aligned, David Clark, who's always been my mentor, was at Miller here, which is the probably the most important AI group in in the in Montreal, but also in the world And there was Silverman who was there at the beginning of this group, but also has been continuously supporting us. yes, I know I don't have a lot of time. So The big thing about this slide is 5g is is is"
  },
  {
    "startTime": "00:12:01",
    "text": "you know, passed right now, and 6 g is there, and nobody really knows what 6 g is. But what has been happening in 6g about a month ago, maybe 2. was invited to a 6g meeting online organized by Capgemini, and everybody from Nokia, from friend. University College London for him, everybody, would was saying, okay. It's edgefogandcloud. There's a lot of machine learning, a lot of artificial intelligence, And, essentially, what we're talking about is not just the Internet of Things. almost the Internet of everything. Everything's connected. and webcam, programability bring to that because a lot of people also brought it up. There's also this this next g I I call it the next year. It's this network of network, and this is the big discussion that I had with Dave was It's it's a system of system. It's a network of network This is actually what the Internet was. It was a distributor system. And the pendulum is moving know, from this hyper centralization, which is also because of regional autonomy, because of data governance, there's a lot of economists and federated systems that basically need to work without connectivity to the cloud, Obviously, energy saving, and I know this is sustainability is a big thing for Eve And so Like, the people are gonna say probably later. Today, distributed systems are more suitable, for these multidisciplinary application and for coin. What does it mean for coin? is that there is a lot of things we can do when we have computing for AI in the net work in the network for AI. We need to have modeling and digital twins, where do we get that from the data that is being gathered inside the network We have programmable networks already not just switches, but nicks and all kinds of"
  },
  {
    "startTime": "00:14:03",
    "text": "CPU and DPU and IPUs. and we have the rise of intelligent network nodes, which are like these little entities that can do a lot of things. on their own autonomously if they're in a car or an autonomous vehicle, but also could be doing things if they're in Iran. do things with the other brands around them. So I wanna talk a little bit about each of these things, and, yes, I am very careful with the So this is actually how how the whole thing fits together. I could spend probably half a semester just talking about this. But we're here. We're calling we're actually in an in effect, an intelligent or in artificial intelligence, internet of things. And so we're related to what are we networking? because we wanna know how much kilowatt hour we're taking. We're lagging to applications because we have KPIs. And, you know, because we do functional decomposition and modeling, And these are the axes that I just described. And all of these things are not independent. They are related. They all need to have some form of of of of CPU of some form of programmable device inside a network to work. And I hope that this is actually what coins about. What can we do when we can have things that compute inside the network. So essentially, what we want is distributing the functionality. I don't want to take over v the next presentation this afternoon, eventually, we want to have what I call always from my students the life of a packet. What happens? I was told my students, if you wanna know where to put computing in the network, Think of what happens from a packet. The minute it it leaves your your user device or even your home router, router,"
  },
  {
    "startTime": "00:16:01",
    "text": "all the way to the cloud. A lot of people think this is a one hop. No. Actually, there's a number. talks there. So how do we do that? How do we make sure that we can actually see where that packet is going? throughout each layer in the network. There's also the vertical, is this distribute this distribute decisions. to improve overall efficiencies. And that, I will talk a little bit about twins and multi multi scale AI and machine learning, but, but, We have also to think. what can we do with computing in the network? working on the data. that can help the network work better, being more reliable, and being better suitable for like the IoT. Oh, and I have it not I have a a typo. There shouldn't be a parenthesis here. and and and IOT, I think, is is driving a lot of this thing because now there's pro there's you know, there's programmable devices everywhere And and we we we know that IoT works with container and orchestration with our god. I think I read these things a hundred times, and I still find typos. anywhere. and and we need them to know how to resource share. And resource share in that case is not just sharing information, but made aware of what happens with the power with the the bandwidth with the access to the cloud. Maybe I cannot access the cloud, but my neighbor can. maybe I can actually then establish some kind of connectivity. Programmable knows why, and this is us coin. enhanced data management. I know that they manage I know that data has been quite a a discussion in this group since the beginning But We need to do load management. We need maybe to do compression. Maybe we need data reduction."
  },
  {
    "startTime": "00:18:00",
    "text": "actually, the people from AI can give us a lot of hints there because they are used to send all their day their training data to the to the cloud But that's offline. And for the real time operations, they have they do things wherever the information is needed. And I would like it for this group to be known that this is not only at the edge. This is everywhere where we can actually act on that data. It can be an aggregation point, in everywhere in in what I call the the cloud edge continuum. So this is coin. This is very much what coin can do. And, of course, there's this security thing, and I'm not a security expert, but there's going to be A presentation on security later in this session. So what are the goals? Of course. Of course. want lower latency, higher reliability and sustainability. What would that means? We need to capture store and process. at ton of data. and we need to do that in an intelligent way and actually see where this best way is to do that data processing the right way. who I mentioned before, did his PhD on a not network related, but could be If hold traffic. management system based on just looking at the data of cars in New York City. He had no pre, conceptions. He looked at the data. Can we do the same in the Internet? I think so. another thing that we need to do. is this this the protocols that are needed for for sensors, for data orchestration, there was a draft early in this group. about data discovery, Well, this is extremely important. Where is my data? who owns it? if do I have access to it? and to orchestrate all these systems."
  },
  {
    "startTime": "00:20:00",
    "text": "And, actually, of all these three, this is probably the one that is the worst. because there's more and more hurdles to access data Some data is not open, Some data is protected by Countries, And I'm lucky I'm working with a group in Iowa State right now where we have our own wireless network. So we have access to everything. But this is not the case for it for everyone. And this is actually probably one of the biggest hurdles. And we can come back to that. And and I think Yve, the data discovery draft should address this thing right now. how data is hard to find. So data is the fuel of the 21st century, the goal would be an edge cloud information pipeline. We did the proposal with that. And and 4 years ago, 5 years ago, when this group was starting, it wasn't accepted, but that's fine. And I put the heterogeneous data system with a big h because it is, like I said, a big problem. There's no standards in data. There's no standards in storage. I think data right now is where networking was before the invention of the VIP packet, There's so many different formats, different granularity, different ways of recording the metadata That It means all the time that the datum must be cleaned. This ETL things that you hear about It's not a joke. if you look at a lot of what is called data lakes, which have raw data, this is useless. for decision making for anything. It's just like a bunch of this disconnected. data things. And it's it's true for the edge. It's true for the fog. it's true for it. everywhere including the data centers. and and we need a lot of analysis. So Why do I say partial correlation and lasso penalty models, is a lot of times your data"
  },
  {
    "startTime": "00:22:01",
    "text": "is not in the right shape. to be used. you need to have independent sources. You need to have your data being completely decorrelated. If not, well okay. you can you can do whatever you want, but then your decisions are going to be trivial. and we don't wanna have trivial decisions. We wanna have something that's gonna help the network. I'm not okay. This is another of these pictures where I could probably spend the rest of the day talking, but This is essentially what you do. You have your your sources, you stream them with Kafka or something else, you get into some kind of unified database, database, call it the data warehouse if you want You can treat digital twins, and I'm going to come to that. You have your AI engines and then you you try them. And and The big thing is that this goes into your network supply chain. And that is not Trivial. more and more And I should've not called it a supply chain. I should have called it a supply network. because more and more every element of your network supply chain has pieces of that, and I'll come back to that when I'm talking about independent intelligent nodes. So AI for the network and the network for AI. This has been actually, this is interesting because this was a conversation I had with Holtzman, who was at MIT and and moved to San Francisco just before the start of the the pandemic, and we were saying, okay. So at the time, and I don't know if Laurent is Trevalier is on the on the call, but him and I had had had done a lot at the beginning, and now there's a big thing in in network management on the use of AI. And we had"
  },
  {
    "startTime": "00:24:01",
    "text": "actually talked a lot about this, and What? Henry told me was because he came from the the IoT world, was Sure. AI can help the network. but the network can also help AI. and how can the network help AI. is actually through coin. and this is another big link for us. and And, Dave, I'm going to answer your question later. Can I? And I'll come back to it. It's a good question. And So so Sure. we can have a lot of AI ML who learns about traffic learns about what's happening in the network nodes, but we can also have the smart nodes helping what happens to to, again, to that data, to the functionality we need for AI to work. And and a lot of time, because of delay, you do not have time to to go to the to the cloud for each decision. You have to do a lot of things locally or you have to do things I would say, somehow locally, maybe in Iran. may be in a another I wouldn't call it edge, but, you know, further further down, which is an aggregation point. And I think this is what I told my students about this this life of a packet. of everywhere where there is an aggregation point and think about what you can do there. to help Not all well, to help the system, to help your AI, to help what's happening in terms of the So the the the issues are efficiency. Is this better than doing, sending everything to the to the to the cloud. I think a lot of us think so. And I think it's in terms of efficiency of power consumption, it's also better I could share with people"
  },
  {
    "startTime": "00:26:04",
    "text": "some research has been done here in Montreal on the difference of using the whole the whole chain of network to the cloud and not just what we have as data center in the north here and and how it saves on on energy. Of course, we need physical models, which are very difficult and data Well, then you need a lot of data if you have to learn on your data. And, again, come back to it. Franco Xavier did that the milo, which was he needed an awful lot of data to be learning from clusters and stuff and and the functional distribution. Again, it's the life of a packet, but where is that life? been lived in. So this is actually something very interesting for us because It's It's at the center, of the use of computing. I'm going to go a bit faster. So this is the network AI. I'm I'm not going to I'm okay. Okay. So this is obviously a a a neural network, neural network, We want to have a lot of different elements in that neural network And I would say the federated learning I'm I'm reading Dave's questions. and, obviously, federating the the learning is part of it. And though they've it's not everything. is all computing. it's needs to be related to the networking functionality. And, yes, I think the all network is the all encompassing, which is going through down one path here. So digital twins, for those of you who don't know what it is, I'm going to go fast over this. agile Duan is just a software image of something physical and it can be anything, anything,"
  },
  {
    "startTime": "00:28:01",
    "text": "modeled, It started in the automotive And Aerospace, but it's not it's now been applied to all kinds of things, and I know people who are doing digital twin plants, which is cute, cute, cute, For us, it's not that. For us, it's digital twins and networking, I realized that the cap M and I meeting that I was not the only one. thinking about this, which was cool because it showed that it's it's interesting. There's a lot of work being done right now at Northeastern, and and and Boston about twins of network systems and node, The great thing is that it's all across the stack, You can do twens of physical layer things, networking, network layer things, and application. it is much more powerful than traditional modeling because it's linked into the the actual data from the network, You don't have to make that many assumptions. You can actually in real time, check what's happening. The applications are management, of course. because it allows you to to see what's happening in your network before you can implement your management Processes, It's, of course, related to data and telemetry, actually from the beginning, there was this company even in Montreal with was using COFINA. to do telemetry. But network telemetry is extremely important, and that actually You can use your data your your digital twins there to see how the network is behaving and then through the real time change to actually verify that your date your twins is is doing well. And one that is more important for me is testing."
  },
  {
    "startTime": "00:30:02",
    "text": "So you can actually do hardware testing, because you don't have to send you a real hardware. You could just e email in a way or put your digital twin somewhere on on a drive, and you do your testing. And by the way, I will be hosting a networking channel on digital twinning and networking. I will go very fast. So they'll link to coin because we're coin We've talked about this. The network is a computer board. next generation of software network summarization, new hardware. You don't need the FeNO. You can have a ton of programmable Devices, at every layer of your network, from that the the edge, the fog, and throughout to the data center And you also want to have intelligence at every layer of the network. And it's interesting that the wireless people and Oram the open Ryan people are actually doing work on that. And so it's it's all across all across the stack, and I think that's amazing. There's obviously common requirements. joint optimization, horizontization, cross functional, better integration, This is something that I could spend an awful lot of time, but this is something that been discussing with with Dave Clark was this idea of the decolonization actually distributed decision making, so created these intelligent network nodes. not having just dumb devices talking to one another, but just having but having them having the power to decide do Am I capable of doing all these autonomous decisions or do I need to to connect to other things? So it's a a view it's actually almost well, it's a it's a I think a generalization of containers, but it's also"
  },
  {
    "startTime": "00:32:01",
    "text": "It's a network of of neural network. It's all these network these nodes become nodes and neural network. Obviously, it requires message passing. and again, we're coming back to functional distribution. I think information centric networking has given us an incredible heritage for this. And actually, there's a lot of NSF platforms right now, Looking into this. and I think it includes important concept for for 6gandio and Dave and I call it the Internet and the small is that each one of these nodes is actually it's it's small little Internet. And thank you. Yes. clity cloudization is agification, and, yes, I think it's It's it's so you look at it from different side. So I'm getting to the end here some research corner coin coin and iterative opportunities Obviously, AIMLAR I didn't talk a lot about augmented reality, but it's there. and federate learning and network, it's interesting that there's gonna be a full conference dedicated to this November, So it is here, and it is just not for network management, it is across the whole Edge Cloud, big thing for us to do research. We need we need to have open private and secured data sets especially because of these heterogeneous environments and networks, We need real time message passing, obviously, the function definition The composition and orchestration is extremely important. And I'm a fan of twinning because I come from the modeling world, and event driven is so pass, say, now uni twins. And Yeah. I I'm I'm a big fan. So as a conclusion that I think I'm doing well in terms of time,"
  },
  {
    "startTime": "00:34:02",
    "text": "because there's gonna be time for and I'm sorry to have rushed through this. You have access to it. Actually, I'm going to correct the typos and probably upload it in your version. that, So this is this is what I wanted from this presentation to say that We were not Taffino. We were not Daffino. We were not Just p 4. we were looking at all kinds of programmable devices and how we were going to Make this work. So essentially what coin is promoting in terms of research, and I think you're going to see that from the present defense because I read the other presentations in this in this meeting, is that we are we were looking as a research community for a programmable framework for advanced IoT and and data driven network automation. up want to have edgecloud, and support for advanced network services, and we want to have applications based on and enabling. AI and machine Learning. and and Again, I'm coming back to this idea of the data With digital twinning, and and and an augmented reality. providing the targeted services. So I know there's questions mainly from Dave, but on on very much ready to answer questions. And I will try to answer the first question from Dave. non altrivalled application running inside cloud data sensors are massively distributed. Yes. They are."
  },
  {
    "startTime": "00:36:00",
    "text": "But we're not talking of data centers here anymore, Dave. This is not coin is not about data centers. coin is about everything. from the time a packet leaves your your device until it gets to the data center. I think it was maybe a mistake. that we did, at the beginning, to really concentrate on data centers, And how about nano datacenters? the things that happen in an IoT network. where you do have also there some kind of data center So I I get your point, but I think you probably missed the edification and the decolonization. This is actually to get out of the of that. centralization. and and Again, we don't have to be rise. We just have to ask research question. Maybe everything I presented is completely wrong. frankly I'd like I'd like to be proven around. And I think from my experience working in IoT Systems, that I'm not that wrong. So that's what I'm saying. I don't know if you can see, but we have also someone we have Dean in the queue. Yes. And I I I think I I went through Dave's questions. I I think I I'm against centrally managed, so you know that. So maybe we wanna look at how to do decentralized management and I think you guys in ICN have done a lot of work on this, and I think the distributed people do that well. So It's fine. So Okay. How about over to Steven? I think the big thing is that Yeah. you I don't know what you you think about not true anymore. I I know about GPUs for AIs, engaged for -- Oh, you're looking at the chat window. And the chat window, I think, relatively speaking, Dave and I were just talking about the -- Okay. Well, I -- -- which an idea of data centers versus edge"
  },
  {
    "startTime": "00:38:00",
    "text": "continuum. Yeah. That's what I'm relative agreement. So I'm I'm ready to take the other question. Okay. Great. We have 2 minutes. Ah, okay. The unbooked language. So I'm coming back from the embedded world. And we're a single single system was handling everything within the same a harder. And as more and more distribution is happening, the Embedded systems are sending the data all the way back to the cloud then waiting, and coming back. And then in the industrial systems. There are 3 types of data Some of them that if they are lost, can cause a catastrophic event. Some of them, we've they're lost, creates a problem, but send kit send still can be handled. And the 3rd part of a data 3rd type of a data is, like, if it's around good, but it's not necessarily to arrive. Now the problem there is Not all the data has to be processed, Add a device, but not also in a central location. because there is some data that is important for Some, local neighbors that are working on the same function, but there are also some data that has to be processed with some neighbors that are working on some other functions. And there is a sort of hierarchical Well, I I'll say there is a topology of the data where it has to be processed. But today, we don't have such mechanisms in order to do so. So -- Oh, okay. -- there's a problem Okay. Who's your quest what's your question? because I think I know what you're talking about. -- say it's a general comment. And then there's also problem about the connectivity, Yeah. Be between all those systems, annoying which connectivity is happening. So it's a multifaceted problem. But having distributed computing throughout the network is something debt is stopping some Industrial Automation"
  },
  {
    "startTime": "00:40:01",
    "text": "Going forward? That's one part. And then is also the mobile connectivity in order to connect all of that together. So there there are the 2 problems that are still out there. to be worked on in there. there's some interesting work here. And there's still some of research, I don't see real engineering problems coming out there yet. as as more there's a bunch of research to be done in the area. Okay. So I know I'm out of time, but I'm I'm completely following you because I did that in an industrial setting. You missed also the fact that all sent all sensors have different timestamps, like different granularity, different formats, Yes. Some of them need to be acted on locally. Some of them need to be acted on outside, the system, A lot of sensing systems have very old fashioned outdated computing, the formats are wrong, the connectivity is wrong, everything's wrong. But this is again, this is this is the research, and I think you're completely right. that there is a lot of research necessary And since a lot of it is related to Oh, I like that picture. And And since a lot of this is related to to coin I think it we have a good place to bring these ideas. So so thank you. It's a very good comment. And, again, it shows that you've been working in the trenches in this thing because this is what is happening. Kyle, do you have a I'll let you try to question Yeah. Hi. Colin Perkins. This has to be the most badly arranged microphone queue I've ever seen in this room. Yes. All I can see is the only thing with the room is a very odd room. Yeah. But you for being here today. Right in the middle of the queue. It's fantastic. It's strange. So so -- I'm sorry. I I thought you were going to say that it was the worst arranged presentation you've ever seen. No. No. It's the presentation very orange. But --"
  },
  {
    "startTime": "00:42:04",
    "text": "that the -- I I was I was reading it up. So so thank you, Mary, Jose. I think that was a a really nice review of the area. and a really nice vision for where the group is going. And I think it's important to have a vision. It's important to understand the various use cases and and what we know what to do with the work. I think it's also important that we have a bit of focus. It's important that we, you know, as a as a research group here builds systems and build prototypes and try out some of these ideas. and check the the designs and check to see if the vision is actually be achievable. Well, click. And you know, I I can see you're standing up to to give a talk on on part of that. and I think that there's another couple of talks which which look like they're hitting that space later in the group. So so and and if -- Yeah. It's actually I I I targeted my presentation after I saw what the other presentations were going to be. So What are you suggesting could be the future of this? So what what I was suggesting was that I I I would encourage the the chairs and and the group to to make sure that we we don't just have vision that we also have implementation for testing and duration. -- don't just focus on drafts and standards and low level things that are nearly finished. but trying to hit the space in between. Yeah. And you'll see actually in this session that we have quite a variety from -- And and there is and and that's and that's a really nice today. And and I think that's good. Sure. And I I just wanted to to come up and and courage that that that that mix happens, and we don't just focus -- Yeah. And and I will tell you that What I presented is a subset of something that I proposed somewhere else. which was some kind of a 5 year research, framework. So there's a lot of research in there. no, I don't wanna get into making sure that packets have the same format. I I can't I don't really care about this."
  },
  {
    "startTime": "00:44:02",
    "text": "But I'm really interested in knowing that all these intelligent nodes are connected and can exchange information and create you know, more until I can do it because they grow up. let's take the discussion elsewhere, but thank you for that feedback. very helpful. We're just very tight. tight on time. with our presentations. Yeah. And I then for 4 minutes late, I'm not I'm not I'm not doing that badly. Okay. Well, again, thank you so much, everyone, to giving me this opportunity. And, again, I'm going back to be in the chair now, so I'm not the researcher anymore. Thank you. Okay. Maybe then I'm up. Okay. Let's let's add some running code. So we talked in coin quite a bit about coin is computing in the network, which means that some people think this is programmable switches, other think that this is little devices that are in datacenters REX or whatever, spread out devices at the at theedge, providing all kinds of services, and this is Marie Jose Ole alluded to the fact that that that that the edge and the cloud continuum is important. And so this is some work that we have been doing, looking at devices that aren't necessarily switches, but rather little things larger things, either in all kinds of different locations closer to Be it with an ISP, be it with the mobile operators. be it within cloud data centers would be it. smart infrastructure and buildings or whatever. So I think was also really some discussion in the chat about the heterogeneity of the this is what this boils down to in the end as well. So so The edge can be many different things. For example, constraints more footprint hardware raspberry pay is from a cloud data center perspective, clearly, is a small device we might have even smaller embedded devices that was also just mentioned. These devices may have special properties"
  },
  {
    "startTime": "00:46:02",
    "text": "will have generated computer devices. Some of them may have certain sensors to fulfill certain functions. Others may have dedicated compute units. may use all kinds of different networking infrastructure with all kinds of different proper is time varying properties, sometimes more predictable, sometimes less predictable. They may be in public spaces well connected to somewhere. They may be in private network not so well connected to somewhere. or harder to connect to summer because we have to go to firewalls and all these kind of things. And they open a they operate in different environments, and are operated by different people. We might have cloud providers who look after those, we might have And as I said before, ISP Mobile Operators or all kinds of private entities running their own compute devices or managing their own devices. And this gives us a decent set of heterogeneity along A couple of them engines that all wanna be treated when we think about how to pull these things together to create a computing environment in some kind of distributed fashion. Now, of course, when I wanna run things on these all kinds of these different devices I also need to have a way m to abstract from them. So I would use some kind of virtualization techniques, be it containers, be it also small devices, light more lightweight run times like uni kernels, or then maybe even process virtual machines and the likes. all the way down to EVPF or something maybe. I'm Where does the next hello next slide. What did I do with this thing? I think what you did was you had You're just adding bullets. So there's the next There you go. There we go. Okay. We mentioned already AR, and so this is Typically, edge applications might be trying to do either something very fast."
  },
  {
    "startTime": "00:48:03",
    "text": "So you don't want to have the confusion computation close to where your user device is, or but but in general, you you perform computations that aren't being done on the device themselves either because the device capable doesn't have the necessary data, or it's just the other infrastructure can just do it faster. So simple example would be this little AI process saying pipeline here where we have maybe a packet sequence stream coming from a from a camera that gets into an aggregator that takes all these packets reconstructs complete frames from those then feed them into an object detection mechanism after some preprocessing which is the little red bubble in the middle. that identifies different objects and sends those information onto a tracking service. So you might put all of these things into a into a single application. You might show as we nowadays often do decompose this into multiple microservices like this is indicated here. Then we need to stitch these micro services together by having something processing pipeline or more general a distributed directed acyclic graph deck. What am I -- You have to point over here. Oh, I have to click over there. Is it moving? Oh, Next. -- the battery type. Yep. Do you need me to click I'll just just tell me the -- I I'll I'll just tell you next. And listing out of my hands that I but at least don't don't work with that. So these different microservices decomposing applications doesn't make sense because we may have different scaling properties. Some of these may have very simple functions, that can serve many clients, maybe like the like the simple aggregation and pack processing thing that just does some decoding on the level. on on on on the left hand side, whereas object detection might be might have more power requirements, and therefore, you'd like to replicate this in order to have either different objects detected or run different instances in parallel. and similarly for tracking, you might then have different"
  },
  {
    "startTime": "00:50:00",
    "text": "relationships between the individual between the individual components. Next. In order to make these then then Reality, besides just stitching them together, you need to instantiate them somewhere. Right? We need to have physical nodes that can execute these individual This is individual functions, this individual microservices, and they need to be allocated, you need to have some kind of logic that allocates these functions to these different nodes, potentially taking into count the respective capabilities, the current load, and so on of these different nodes. We all know how this is done in the in in the cloud. We have services like Coronitis, that takes care of these has a control of Many different nodes understanding how they are currently performing and then allocates new tasks assignments to these different nodes in order to run there. Next, This is where the specifics of our agile environment come into place. Well, we have we talked about earlier about the heterogeneity. So we have more diverse devices, usually data center infrastructures are fairly homogeneous. You may have different types of device, but then you may still have thousands them. So scaling and so on is relatively easy Similarly, all data center nodes are usually close together. the microsecond art ping RTT or or even below might be something that you could you you would commonly experience, whereas in the edge not necessarily you may have things that are connected wirelessly. Things may not be in the same physical location. connectivity may not be as guaranteed as in a data center and so on. So we get also network network diversity. And then, of course, we may have different operators who run these things. We have a the greenish, a brownish, and the grayish operator that that is running different that I taking care of different devices. And things like systems like Kubernetes are targeted to operating within a single administrative domain because you wanna be able tightly control each of these individual notes."
  },
  {
    "startTime": "00:52:00",
    "text": "And tie those tightly together, always knowing exactly what is going on on which these notes, which of course also creates sorts of communication overhead. Now that doesn't work as easily as the edge, but In order to cope with these, some of these issues, a bunch of systems have been deep derived from Kubernetes like k3s microkubernetes and so on. in order to take care of these specifics, but that they they don't quite get there yet. And this is next. This is why we took a different approach and thought, well, maybe rather than trying to work with straight from this starting point of a completely functionalized homogeneous platform and then try to teer out some of these things in order to make them more distributed, took the opposite approach and started with a distributed design assumption and build something called orchestra in order to allow accounting for these specifics and diversity diverse elements of of edge computing. Next, So, Okusrat, there's many different things. I just can talk about a few of those here, and there's especially the ones that are relevant when it comes to managing distributed action environments. So the title of the stock where you said it's hierarchical. So we have a hierarchy here. We have a root rate, and we have to have a bunch of clusters. So we are grouping resources into typically physically collocated clusters of things that are then managed together by within some local administrative domains. We have this greenish or yellowish, the brownish and the grayish administrative domain. each of those managed by 1 being 1 cluster managed by 1 entity responsible for this cluster Let's start with this with these little blue nodes down there next Obviously, this first enable need is workers. right, we need to have workers that have a an execution runtime where you can foil container or now it is also micro kernel or something else."
  },
  {
    "startTime": "00:54:01",
    "text": "So, for example, container d using that as a starting point. So this is where our whatever services you'd like to execute get get run. Next, Then we have a node engine. Some of somebody needs to oversee this execution itself, With 2 specific features in there, one is you need to understand what capabilities your node has so that you can actually tell to an orchestration system, what the what the node is fundamentally capable of this includes I don't know, total memory, number, of cores, number of GPUs, what have you. At the same time, you also need to be able to understand what the node is currently actually how loaded the knob the node is currently currently is. So which are this which the services are that are currently running? How many resources are they using and so on next? So that's the node engine that takes of this. Then we also have a network manager because a node as we saw before next. might interact or would interact often with other So any single entity any single function that we might want to instantiate would would interact with another function. beat on the same node, a different node in the same cluster on a different and somebody needs to manage this connectivity. This is what we have the net manager for. and understands which notes need to talk to whom gives gives each applications their individual namespace and then ensures that the different nodes can talk to each other if necessary, creating tunnels across the different clusters for where we have then naturalversal functions, all these things built in. Next, So that's our worker node. Of course, we made our cluster will have multiple node worker node next. And next, These worker nodes then talk to a so called cluster orchestrator, which the thing that has the local oversight we use and MQTT for for communication that a simple pops up based system And each of the workers next"
  },
  {
    "startTime": "00:56:03",
    "text": "reports the individual current usage statistics to the cluster orchestrator. The cluster orchestrator itself has the name, the location, and so on, and then has a bunch of different databases for services the nodes that are existing in the networks, and then all the different statistics that I gathered for the services, for the notes, and for the this is all very nitty gritty detail about what is going on at any given point in time in in any of these nodes. and it has a cluster manager that understands how the over a cluster is loaded that aggregates kind of information. It's a service manager that understands which of these nodes currently executing with services and was able to create, create, new services and then utilize the cluster scheduler to put them into practice of into execution on the different notes. Next? Now we move up one level in our hierarchy. We see all the different clusters. Next, And these different clusters we said before might be operated by different by different entities. And, of course, if I'm running something. I don't want anybody else to look in detail understand in detail what what am I doing inside my own cluster deployment. How am I managing my notes and so on? This is actually nobody else's business. It's a bit like, well, if I'm route if I'm running my own little network, I might expose general routes that I can support support to other entities to other to other ISPs. And so what I wouldn't share in a the intrinsic details of how my internal routing works. And so we do something similar with compute resources here next. Sorry. I'm also typing into the chat window, so my apologies for the delay. No worries. And so these cluster nodes do provide aggregate reports so the the rather than rather than saying, I have 15 different machines and they have all these properties. So we just say, Oh, in total, I can do"
  },
  {
    "startTime": "00:58:01",
    "text": "does. That many CPUs, I have that much memory. I'm my current load situation is 60% and so on. So we have an aggregate reporting that goes up to the route orchestrator. The root orchestrator, as you can see from this figure, has roughly the same components replicated that we also have at the cluster level, except that it manages the entire climate, of such an orchestration system. but it only sees caused granular information. enough to understand, oh, this cluster over there is loaded. this cluster here is empty. if I get a next job, I'd rather push this to the empty cluster, or it might know that oh, this cluster is sitting in San Francisco or it's it's sitting in the Hilton. And, therefore, if I wanna have a fast executing service or fast funding service for somebody who's in the Hilton, I should probably deploy this service on that specific class that because of its location and topology. So other than this, we have roughly the same components. Next, I now wanna specify a service, like, create some kind of, in the end, Jason represented Doctor. documentation that has, for example, says I have an a 1 aggregation unit. I have 2 different object trackers that look for look out for different objects. They feed the respective output of which objects they found. into a tracking system, and that then works and attracts these objects. And for each of my services, I specify a namespace, how the entire application is called and then my respective needs in terms of memory capacity, CPUs, if I need GPUs or what any other special requirements I might I might I might have next. This specification I can push into my root orchestration system on the top left. Next, end, which effectively means that the that this goes into the service and into the system manager next. which will then ensure that the respective code that I'm going to need for will be stored somewhere locally in my respective services database. Next,"
  },
  {
    "startTime": "01:00:00",
    "text": "If I then wanna deploy these, I'm going to issue a deployment request specifies, for example, certain constraints to our route orchestrator next. which means that, 1st of all, there is a deployment request. I'm going fetch the respective code that I have readily available and then ensure that I've I'm going to get this scheduled. at the root scheduler, I don't take the refined grain decision, as I said before, but rather figure out which class us would be available at this point in time to fulfill the requirements for this respective service. Next, And in this case, it's decided that cluster 2 is going to get 2 of the components plus the end is going to get to other components, which also would mean that in the end, this respect plusters would need to see some kind of interconnection happening by this the aforementioned network managers to establish communication between next, While the route orchestrator just picks the cluster, it delegates responsibility to for figuring out how to make this respect of scheduling happen inside the cluster to the cluster scheduler itself. so the cluster schedule then receives this request, receives the respective. In instance, code instances and can then make a deployment decision on where to put these next. When if you could finish up in a couple minutes, Yes. I am. I I am about to do so. So these things get deployed. on the respective nodes next. Hope this. And the cluster scheduler then decides which specific nodes they should be running. The plumbing gets sets up the network plumbing sets up the respective interconnectivity between those and we are done. Should for whatever reason, a cluster figure out because reporting only post grant information to the route orchestrator that a particular service can't be instantiated, then it can actually delegate this responsibility upwards again end, tell the route orchestrator, oh, can't do this right now. Please find a different location, which means that at this this very moment, we don't have to have every second detail insight on what a cluster does, but we can utilize"
  },
  {
    "startTime": "01:02:00",
    "text": "kind of similar rough rough abstractions here. Next, Since I was I'm about to finish up. So we built the whole thing. This is currently a relatively lightweight implementation. It's at at the moment, it it's open source. You can find this from orchestrator@.i0. You can find it on GitHub. It's currently 19,000 lines of coal. When I talked about this 2 weeks ago, it was 18,000 lines of code, so we have an impress a nice I can't promise that this growth is going to continue every every 2 weeks. It's mainly Python for all the orchestration components in Golan for the network, the provision networking components, It is modular so that different scheduling policies can be added did device design itself is more yet to begin with. And it is sufficiently lightweight and reduces the amount of effort that you need to put on the edge devices because, hey. After all, they're supposed to run the services. Next, Performance, the vast bullets, it compares favorably with a bunch of other systems. This is just a sample plot here to show that for a simple cluster deployment with 1 boot orchestrator, 1 cluster orchestrator, and a bunch of workers, the abstraction level between the root orchestrator and the cluster orchestrator works because on the right hand side, you can that as the number of workers increases, other systems that have minute detail of all the individual workers do grow in compute requirements, whereas orchestra doesn't simply because you don't get all these detailed reports all the time. And, also, workers are very lightweight and and support in terms of the CPU overhead. which also allows us to run stuff on deploy code on devices that the other systems can't even do to in the first space. next, And that's it this was a bit high level overview of the buffer of the part above the surface. If you wanna see look below the surface, go to orchestrate. io. Read the code, read the paper, ask us. Thanks."
  },
  {
    "startTime": "01:04:02",
    "text": "Pretty much. Looks like I'm I'm in the queue, so I'll take off my chair hat. One of the discussions or at least comments that appeared in the chat window as you were talking about cluster formation and then cluster of clusters was your reference to MQTT. So this reference to both in interclustercommsandan intruckcluster comms? And are they the same and different? And how are they? And what did you use What are your thoughts about that? -- for for the the the the inter cluster or the the cluster orchestrator to root orchestrators actually rest full API using HTTPS. The interest intra cluster stuff is mqtt to be because then we wanna have fast reaction times with notifications and everything, the rest towards the route orchestrator is very low rate, low frequency. So if we would up we if it would be updating within the cluster maybe every second or faster. We might be doing this every 10 seconds or every minute at the next higher level. if that if that helps answering the question. Sure. Sure. Diego? Did you wanna ask your question? Just say -- I know this is a very hot keen for those of you not in the room, you can see the column there. People show up behind the TV jump. was just no. Collin had some issue with his with his iPhone, so I I I over to I went to him. This a couple of questions. You you show this idea of the different mean, the clusters belonging to different domains, etcetera. Just out of curiosity, I think, in terms of administrative, the orchestrate the main orchestration thing. We're we're where where is expected to live because you -- Oh, there there there is not a single one. you can't you can't build a single system where you would expect a single one. If so if you if you would want to do something in this corner of the city, you might want to do you might want run a an an orchestrator that provides services for a given set of applications for a given part of a city of the region or whatever. And so there is no"
  },
  {
    "startTime": "01:06:04",
    "text": "There is not a single one. This also mean but because you want to utilize resources from different people, there might be exportable or that they you might have different clusters that export their resources Now there is no one to one relationship necessarily, which means that if I have A bunch of street lamps. that can do services for people. I might sign up with multiple things because they be sensing for multiple purposes, and I might be able also or if I have other compute resources, I might be might might be my cluster might, be part of multiple orchestration domains, because I'm not reporting every little nitty gritty detail I could do overprove I could do over subscription. I could share 50% with 1 entity, 30% with the next 140% with 4th one. So so And because there is no way that you could ever possibly agree on where a single thing would live it's designed to be to be allowing operating with multiple routes if you if you so will. You know, without without the the the hassle right now about the multi cloud federated out, etcetera. I mean, are are moving to multi edge, federated edge something that we would be interesting. In the interest of keeping on schedule, I'm going to unfortunately close the queue Oh, Okay? And -- I was looking at those as -- Yeah. No. No. They're But I will suggest that people take things to the the chat window, maybe we can continue there for now. 2nd, very your question about the the networking patterns. Your plan you you're using some kind of overlay? Or -- They have they're constructing a simple overlay, I guess. Vence. That is that is fairly lightweight as we measure it. So the the the performance impact is is is is fairly minimal. But there is a bit of overlay that you need to do this. So we use a lightweight tunneling protocol between the different nodes. the the"
  },
  {
    "startTime": "01:08:01",
    "text": "as a resolution happens I mean, via the route orchestrator, but only four end users, and then the rest is being cached the moment you have been setting up something. And because this is also often being asked if one occurs failures the services continue to run. be the cluster orchestrator or the route orchestrator. They only would only be affected you wanna g fire up a new one that could then gather the the current state of the information again and continue. And this this is why it also wouldn't affect the the ongoing tunnels because they're all running directly peer to peer. Thank you. I'm gonna take let's take this to the list. it's just we have such a crammed schedule. or to the chat right now. Okay? Okay. Thanks. we can continue on chat, and we can continue on chat. Okay. So Sharon, Are you online? Yes. I am. Can you hear me? Yes. Would you like to drive, or shall I? If you can. totally great. Thank you. k. I apologize for my voice. Yes. I do find it. Excellent. Thank you. My voice is a bit sore. I managed to get a call in the middle of the heat wave. I don't know my conditions. Some are not if I'm not clear, please indicate. So this talk is about share nothing at JI. using pipelines, explained through mobility network functions, And as Marie alluded alluded to before, it explores a a reoccurring theme in coin of using logical addressing and overlays to implement computation networks, But this time, we're emphasizing generative AI and using mobile DNS network function as an example to clarify it. So next, please."
  },
  {
    "startTime": "01:10:02",
    "text": "Alright. So the philosophy to the graphical level, We all know that there are synergies between computing and the network clouds. but it pure form Cloudcomputing thinks of the network as a bus sort of like an InfiniBand bus. and access links linking the world to this big computer big, warehouse sized computer. And if you form the network, focuses compute on injunctions, with very rigid and standard logic, which is interoperable and interchangeable. But the synergies between them produce nonpure, but very interesting and very effective share nothing pipelines that have been helpful before. For example, CDN, you can pick I was sharing nothing pipeline where you ask for URL content, you are steered to a cache, If you there's there's a cache hit then you serve the content. If there is a cache miss, then the origin is asked to get the content on gigabit links while the content is delivered over megabit links. and and nobody assumes that the caches share any storage or the caches and the origin search share storage, and this allows CDS to scale immensely well. And we are going to add another such example with share nothing parallel with generative edge AI. Next, please. Alright. So basically, we're looking at sharing, I think, at the share nothing cloud, share nothing pipeline as a network. they do not use the the the cloud as a big computer. Meaning, they do not allocate processes anywhere just anywhere using some kind of EC 2, like operating systems. and they do not assume an s 3 like"
  },
  {
    "startTime": "01:12:01",
    "text": "shared data plan. Instead, the data flow is contextually routed rather than randomly low balance. and Because we do not assume any kind of shared storage or any speed up bus, like, spine leaf in the data center, we are able to geo distribute the computation and the east west capacity, needed to support the service, is in direct proportion to the north south service itself. So if the service itself is a 10, 20, 30, 40 gigabit per second. the east west is not much more than that. Unlike looking at the centralized cloud, there's a big computer where we need the 100 times even more. a multiple high returns capacity east west for any north south service. Next, please. Oops. Yeah. So one one example is NFV service function training, These are clustered programmable functions, still they're operating wire speeds, serving entire network provider is traffic. because, really, the functionality was conceived as an appliance or as a module within a router And the whole pipeline was part of the traffic engineering. So because these functions work and see this way, we're able SSC is able to sort of obstruct the traffic engineering flow and distribute the computation and in full wire speed for the entire network even though it was clustered and it's no longer an in line appliance. Next, please. So we use similar approach in implementing and specifying mobility network functions,"
  },
  {
    "startTime": "01:14:04",
    "text": "in the automotive edge compute consume, specifically geolocation, which is a typical edge use case because it consolidates immediately information for multiple vehicles, for multiple vehicles. Therefore, it cannot be in the cloud because of capacity and cannot be in the car. And the way this share nothing pipeline works is that For example, vehicle spots, a free parking spot. The detection is uploaded to an area agent responsible for that area which consolidates that information because it may be gotten from multiple vehicles, from multiple points of view, localizes and generates notifications for the vehicles looking for parking. So next, please. Okay. So this is based on the the list RFC sets. where we use list in order to assign logical IP addresses to agents, And based on their geospatial jurisdiction, and we use EIDs, and we use also EIDs assigned to the vehicles so that their geo privacy is protected while they are producing and consuming data. Data. So list here allows us to build an ad hoc cloud. Meaning, we you don't know why and when a specific agent who reside in which edge location. This is a DevOps decision based on the density, like to commute or it's in the middle of the night. If it's commute, spread the agents more, less agents per compute location. And if it's a denied, I'll condense. And we don't know when a vehicle is gonna join this cloud or leave this cloud, but the least act here as a sort of, like, layer 3 Kafka."
  },
  {
    "startTime": "01:16:01",
    "text": "fully distributed and terabit a second. and helps us scale notifications. The extensions to lips to list allows us to assign logical addresses to geolocation. And also, sort of streamline a language of data. I know Marie talked about it, sort of streamline the data that if the jurisdictions are ties, Then so are the metadata. They're all mini ties. the jurisdiction is core kilometer, a detection is a square meter. I mean, what I have to say about a square meter. in terms of geolocation. And next, please. Okay. So this was all very nice. And then what happened in the last year is generative AI happened. And the importance of all these architectures became a lot more interest So generative AI is most of you probably a no in a high level and I mean, I'm not expert in the details, but it's basically able to process input language through attentions and generate output language, And it really what it did to our road map is the accelerated, like, many many orders of of of a time, time, What you can do with generative AI is is a lot more powerful than what we can do, for example, with vision AI. For example, with vision AI, if I wanna train on detecting an object like a a cone, a traffic cone or a parking spot. I have to do it for every lighting, for every resolution, for every angle in every country. takes forever. But in generative AI, I'm able to segment images using models that were trained on huge amount of data. and I'm able to label these segments and localize these segments using a"
  },
  {
    "startTime": "01:18:00",
    "text": "generic foundation models that were tuned to this So the the impact own features was his dramatic. Yeah. I'm astounded every day new, n u, n u, we want the network the pipelines to leverage these generative models because no one models does the entire job. you need a sequence of things, and you need to sort of translate from one to another until you get the results that you want, one does one does segmentation, and now one does labeling this are completely different tasks. So we want the pipelines to leverage that amazing technology, but we also want it to mitigate some of its 2 most difficult key issues. issue one is that it's very, very slow. from the time you go into the prompt and the time you go out as a response or a query response, it can take 100 or milliseconds. So it's nowhere near, like, vision AI or definitely not like code. The second thing is that this requires a huge amount of compute capacity. So we want to be able to leverage the network to solve both these problems and engage generative AI in in the mobility network. an example, next slide, please. Okay? Let's take a look at it as an example, which is was futuristic 8 months ago, and it is now a live POC a semantic virtual camp. This is sort of a camera hovering over the city. each one looking at a square kilometer, and they don't really exist these are virtual cameras. The camera is actually fed by images from driving vehicles,"
  },
  {
    "startTime": "01:20:00",
    "text": "And but rather than having just a collection of images just some of the images from different angles and different resolutions in different places. The virtual camera employs a segmentation labeling localization pipeline ending up with a language describing what's going on in that area which is noteworthy. which is worth attention. An event has changed, a situation, a congestion, blockage, a a results of a blockage, and things like that. So this would be possible with the Vision AI but it will take, you know, it will take forever, and it's very, very difficult. With generative AI, we we receive that result immediately from off the shelf foundation models tuned for the those tasks. and it's really extraordinary. we also see that generative AI here plays to the strength of OEM because it's based on a lot of data and what one thing OEMs have is a lot of data because they have millions of moving sensors all over the place. On the other hand, it doesn't it mitigates weakness of OEMs, which is agile, dynamic code. Yes. Sharon, you have less than 4 minutes. Okay. Okay. So I'll I'll run. We also see that the the fleet here solves the time problem. Okay? because the future of the cars on the right is the present of the car on the left So I have enough time to deploy the generative AI pipeline. So next, please. Okay. Here we see that we can combine multiple input sources like satellite drones,"
  },
  {
    "startTime": "01:22:00",
    "text": "something that would be possible with vision AI to do in a reasonable time. and very simple with generative AI. Next, please. Okay. And this is the POC plan for later this year, extending the last year's you have parking, Next, please. Next, please. Next, please. Alright. Now now the second problem is compute resources. And here we see also that the fleet or the connected the collected fleet and the network can help solve that because Obviously, the driving vehicles are very busy with perception and for planning ahead based on generative prompts, but the parked vehicles are completely idle, and we're talking about pet drops, of GPUs going into electric vehicle for because of softly defined vehicles and centralizations of computed vehicles that they have to be there anyway And because of the pipeline, we are able to leverage compute resources where ever they are, even if they're in the far edge, at the vehicle, while it's charging, it could be Ubers or leases or anyone who agreed to this count because it helps the fleet to And because of any way high latency of the compute, it's okay to use the far edge. Next, please. Okay. Next. This is, like, military use. Next, please. I won't get into that. So in summary, we see share nothing pipeline is really a network, and it can really help us leverage the the power of Genuity AI but also mitigate some of it weak weaknesses. Next, the last slightly. As an example, if I were to take a 1,000,000 car active cars, city like London or New York in peak time, over a 1000 square kilometers with a sampling 10% sampling rate of a 100,000 frames per second. that would be her fit 1000 frames per second will create a 100 gigabit The second pressure on the uplinks to a remote centralized cloud,"
  },
  {
    "startTime": "01:24:04",
    "text": "which will require 10 to a 100 terabits a second fabric in order to randomly compute all that. and require a an extraordinary amount of GPUs constrated in one location. On the other hand, if I break it down to this share nothing. at all clouds, I'm I I had end up with reasonable size problems in sort of sort of much more self sustained jurisdictions, sort of living off the land of the GPUs around them, And, like, one gigabit per second, and then I need, like, like, 10 agents, and and I can allocate this dynamically between compute locations which are currently available. That's it. Maybe. Well, thank you very much for the interesting presentation, Do we have some questions the super interesting about I really like the theme across all of these top so far of the clusters of clusters. And I really appreciate the doing the back of the envelope calculation, on what the fabric would need to look like, which, of course, is doesn't exist as yet. Right? So, so, so, so, so, you did talk about you had multiple POCs. So what is your do you have some toolkit that you're using and reusing for all of these. Yeah. These are public ACC POCs. Last year was to find parking, and this year is just gonna describe the location in a language which can then be prompted into models that generate like unit GPT, visualization, things like that. So from a bunch of pixels, to streamline the language of the tiles and from tiles to applications like navigation,"
  },
  {
    "startTime": "01:26:02",
    "text": "The ACC specifications are not a toolkit. The the list stack is open source. I mean, there's the resources for that. maybe Dino or Alberto can point you out to that. using the addressing scheme that's based on the RFC extensions to list, And besides that, just take these foundation models there. There's a new and better one coming up every other day. It's just extraordinary and don't codes. pipeline. Yeah. exactly Okay. Anybody else? Perfect. There's also some relevant chowder going on in the chat that I'd suggest you look at as well about addressing identifiers that seem relevant here as well. Thank you so much. Thank you very much. Okay. I think we already for Jong A, would you like me to share? Let's see. We would you like the terminology or the use case analysis? of Okay. Hello, everyone. This is Jong Hung from. And I'm very happy to take taking over this draft from Ike who have talked great work, great work, about this, And let's like this? The Since I have taken over this drop I I have the tick deep loop over the draft. And during that, I realized that this is really importance and you will be useful document for decoying algae in the future"
  },
  {
    "startTime": "01:28:02",
    "text": "because the it was separated from the use case dropped The use case dropped to how to describe each use cases with the use case, a dream, requirements or opportunities and research questions about it. But on this analysis, is providing the the identify the commonality of the all the use cases. So I think this is very useful work for for coin algae. So during the reviewing the draft, I made the editorial changes. and also added some description about on the categorization on the resource cluster to her a was a couple of things was missing in there. Next slide, please. This is the purpose of the drop which is to analyze the opportunities and research questions and requirements of each use case is provided by the draft of the use case to identify the commonalities So by doing this, we really like to provide the general direction research reaction for the coin So this is very useful work. So but the current title is the on else use case on else, which was part of the under the use case draft. I think at the time, it was a k with the title, but since it has been separated from the use case dropped, I think. We we are proposing to a more appropriate tag to, like, research challenges of computing network, or the opportunities and challenges of computing the network or something else, if you have a better idea, please command or suggesting. Next slide, please."
  },
  {
    "startTime": "01:30:01",
    "text": "This is the current status of the document. The honest part have well, was composed of the resource questions and opportunity and requirement. But still the opportunity and requirements are missing. but the the contents of the research customer was provided through a layered categorization like this picture, Next slide, please. So this is the future plan that as I already mentioned, we like to change the title to the research challenges or opportunities or challenges like that. So by doing that, we like to make this scope of this draft make the broader and more make us generalize So we We try to focus more on opportunity and challenges. So the requirement to get might be the dilutive later on. And to achieve this, we really need the work contributor because We this is very important. this will be a very important document or base document for this group in the future. But we need to collect all the common understanding of the group about the research direction for the coin. we really need a contributor So please send me an email or talk to me if you are willing to become a contributor So Yeah. That's it? Great. k. k. k. k. Let's go to the other draft, which if you have Oh, No. I don't wanna share the screen. I wanna share Slides"
  },
  {
    "startTime": "01:32:00",
    "text": "Okay. This is the Tom Niles for the Again, I haven't taken over from the I could have done a lot of work this. This is also was a part of the use case draft. And there was a number of use cases we use the different technologies. So so when they derive the the budget, all the use case in a 1 dropped. They needed needed to define the time long is So from that, it was started. And then later on, it was separated from the use case drop. it's slide, please? So, again, I only made several chain editorial changes from now and also add a just added one more to knowledge deeper in this revision which is the coin function is a their computational task that can be invoked as part of the program, and it district a it contributes to the over error functionality, of the program by providing a modular reusable univopulation Next slide, please And the the purpose of this drop is the brain clarity to the current understanding of the coin So the old terminology was collect take from the use case draft as well as the direction vertical coin draft, which has not been updated for a long, So I have talked to the who He has some plan for updating but I'm not sure even though he updates these disrupt, then we we'll have a new moral to knowledge for this strapped or not Next slide, please. So For future play, I would like to ask the questions"
  },
  {
    "startTime": "01:34:00",
    "text": "to the group. Actually, this class was issued by the Maria in the in the previous Queen's meeting, that The question is shall we just maintain this stock amount as in the the maintained the current version. And whenever it the new technology comes up, we can add or we can change it if or we need whenever it is needed, and keep going. And until at some point. And then when we think it is ready publish that we can be done it can be done. Or If there's no more comment on this terminology document or if the group is Google thinks that it is ready, to publish it, Alright. are, then We can move to this knowledge drop to the RG law score because The authors really think that of in the current situation, there is an This is just all that we can collab. the technology that there although there are very few anomalies in disease dropped, but but but o Does just We we can't do anything more. It's a very short track. for those of you who haven't looked at it. since -- But it's very helpful. Yeah. Sure. So we need probably more comment commentary. It looks like we do have A question from Dean, Mhmm. Would you like to step to the mic. Excuse me. So on the draft and the terminology, it's good to have one set and saying like this what we are using, so that We have that you don't have over overload the terms if you're speaking. but but there is a with the terminology it's always a living thing. Completely -- So -- Mhmm."
  },
  {
    "startTime": "01:36:01",
    "text": "that's one thing. And then the working group exercise. The research group has to be ready to produce don't know if there is abyss in the Right. In in or in in order to to to update the terminology. hopefully not changing the existing one, just adding new terms to it that are starting, you know to come up. Also, I would prefer to keep the use case draft going forward? Because use cases -- -- case or use case analysis? Well -- The 2. use case, Okay. Yes. The sun no. not rename it. So, like, keep the use case draft and not to look into research and the opportunities is make it as a separate research, like, a separate draft. and and, you know, keep those 2 separates because use case, can be very useful for what is a current problem? And then research and opportunities. You so there are 2 drafts, actually. Okay. There's a use cases draft And then the use case analysis so the so the proposal here is to change the title of the use case disruptions across that line. Okay. So I think we're actually in agreement. Okay. Okay? Yep. Thank you for your comments, and I'm hoping Colin Yu will have a brief comment or clar location about maybe the IRTF process here. for the for terminology and use cases. I mean, in in terms of processes, the research group decides it wishes to publish them has consensus, then it it can request they be considered for publication. What what I came up for, I I mean, having common terminology, I think, is a good fit. Yep. It tends to make a lot of sense if you are building if if if a group is building a system, like we see in a lot of our ETF groups where we're we're we're agreeing"
  },
  {
    "startTime": "01:38:02",
    "text": "how that the the terminology we use to help design a system to avoid talking across purposes. And I think it can help in the research world if if there is evidence that the broader community is adopting that terminology try and all help organize the work. and and and the broader research community. without really evidence that either of those are happening, it's not clear that this month. Well, I think So Would you like to comment? And I and I don't know if I don't know if you've replaced it. because I have an opinion as well. I'll put myself back with you. But but, you know, I I guess one of the things we would be asking before pub publishing a terminology document is, is it getting traction? or is it likely to get traction? will will will of Well, it's like to get the common comment from, like, understanding of the terminology indiscoll. That's a purpose. Right. So that when we use the terminology in the group itself that we're all talking about same things. And I would agree with Dean that although it doesn't need to be published, per se, having an to date terminology draft as the as the area continues to evolve, would be very helpful. I think it would be helpful if it's if it starts to get traction and people start using the terminology. Right. And so the question so it remains to be seen whether if we're producing other and other presentations whether we can codify and accelerate the adoption of these terms. And if people find it useful and intuitive, And indeed and whether the broader research community starts adopting similar terms. Yeah. Thank you. Okay. Thank you very much. Thank you. Where are we? We are now into the final stretch"
  },
  {
    "startTime": "01:40:00",
    "text": "where we have a couple of individual drafts So let me propose that Let's see. Who's next? We have Louis Contreras, the evolution of class, cooperating layered architecture for SDN for Compute And Data Awareness. And, unfortunately, the clicker doesn't work, so I will share your slides. Whoops. Thank you. but I it would help if I clicked on the right button here. All yours. Thank you. So this is Luis Contreras from Telefonica. I will present an update of the the draft, which essentially presents an evolution of the cooperating layer architecture for SDN. And may I request that you try to do this in 5 minutes? I it's on the think that that was what the original schedule. I just wanted to say it out loud. -- do. Thanks. Okay. Next. place. Okay. So as a background, the operating layer architecture for server defined networking was released as an NFC. And it essentially, another in proposal has separation between service and transfer concern. okay that say, separating the control management and resource plan devoted to each of the of those strata So this operation allows independent evolution of of service control and and transport control, And so and and, also, I mean and despite being separated in in requires a need of having a a 15, I think, affect coordination in such a way that the the the overall optimization of the service and the premium of the service, is consistent with the expectations from the user. So what we did proposing this new draft is an augmentation of this architecture, of the class architecture, essentially adding a new Strata. I I need the stratum. They wanted to compute. and also creating a new play or proposing a new play, which we we will be devoted to data So next, please."
  },
  {
    "startTime": "01:42:03",
    "text": "So this is the original class architecture. As you can see, there is this separation of concerns In each of the concerns, we have management and control plane. for the in one case, the voltage to the service itself will be a a to the function service or application, and in the other case, for the transport for the connectivity. And, also, we have the the resource plan essentially trained to to handling the resource applicable to each of the strata, resources for the service, and resources for the connectivity. So next, please. So the first implementation that we are proposing in this draft is to add a new complete stratum, devoted to compute. again, with the management control management plan control planar resource plan. So somehow in what we have seen in decision today. Know that despite the the representation seems to be hierarchical. This is not hierarchical. This was of the comment received in Yokohama. So we will try to figure out how to represent this in I'm more affordable way. But, yes, I mean, the we are stuttering this way just for a peaceful look and feel next, please. So the second addition was this telemetry plane in in the previous version, we call it learning plane. we have now changed the name to telemetry plane. The idea will be it would have this plain devoted to the collection of data that a a little can fit control loop capabilities, artificial intelligence, and all this stuff. So next, please. That will be probably a better represent of that, so avoiding the hierarchical structure. So probably, we will need to to do something like this in in the new version of the draft trained to to put that same level, let's say, the connectivity and the computer start up. anyway, as a summary, you can see here the implementation of the the new concern didn't I didn't computer stratum and also the inclusion of the new telemetry plane in order to complement all the control capabilities. Next, please. So the changes essentially was"
  },
  {
    "startTime": "01:44:00",
    "text": "renaming from the learning plane to telemetry plane, we think that this is much more clear, let's say, the purpose of this plane. So the idea will be as set to collect for each of the strata. relevant data and then allowing the processing of this relevant data in each of the data even though they will be to maintain collaborative interfaces different extra things to chat with the all all for all the planes in in summary, that could be this cooperation at the time of deploying the services. 2nd, we have improved the the description about the potential research aspects of the telemetry plane. the learning plan, so there is more text in in the draft. And finally, we have the included some applicability scenarios. 1st, the cloud is continuous. So considering an an environment, we will have separate the compute capabilities attached to the to the network, even the network being both network and computing from different administrative domains. Here, highlighting aspects that probably could be a ways of of I mean, ideas for researching like yeah, optimizing resources in the in both computer network. guaranteeing services as a loss at the time of doing the services. or even secure transmission of the information between the different planes the different strata, the compute, and the and the connectivity of strata as well. And then the second applicability scenario will be the network application integration with the areas. As today, we have decouple situation where the the services are decoupled the reality of the of the network. the network in a in a broad sense, I mean, connectivity plus compute. So the idea here would be to exploit capabilities of, yeah, in information from the network. also collecting telemetry in order to apply artificial intelligence capabilities and maybe all the aspects, like, maybe using metadata across the compute and and connectivity better. metadata being defined by the service stratom Next, please. And the last one is actually next steps. we acknowledge that we need yet probably to set a little bit more on scope to current energy."
  },
  {
    "startTime": "01:46:03",
    "text": "the purpose of the draft. So we will work in in that line with, yeah, all the use cases supported and terminology and so on so far. We would like to collect feedback on an interest the research group any of these is is essential interesting for the for the research group continue working on that. And, yeah, prepare new version for Nx ATF, and we probably fixing this diagrams and so to make it more in visual, let's say, that that idea that not not having a hierarchical relationship between the compute and connectivity. So that's all from my side. print. Print. you very much. And thank you for being so clear on what your expected next steps are. Maurisha Sayed, You have a question? We are not hearing you. Now I I think I think you now do. I'm I think this is interesting work, but At the same time Question thing. y, Yep. This this Layered architectures are very standard. So I would be interested in and seeing What do you think your approach is different? from other layered architecture that do have computing and aggregation and everything. And I think that would be nice to highlight that in your in your draft because I read it. So maybe you can comment on that. Yep. So, basically, what we see here is the the the interplay between the three dimensions and the connectivity, the computer, and the services. we we think that we need to to keep all the c combined in order to, yeah, to to to do things like a resource optimization, this kind of of aspects. So probably that would be the the the novelty part, I guess. So having"
  },
  {
    "startTime": "01:48:01",
    "text": "tend to work on on this interplay of the three dimensions simultaneously. And not not I mean, the 3 is startup, but also the planes within the 3 years data because they they will be also the instance in the case of the telemetry plane, inter interplay as well the telemetry planes from the 3 different So somehow it's a kind of of a dual in interdependency. at the Strata level, but also at play level. Okay. g, Dan McDonough. So I have an issue with the telemetry plane naming. because telemetry can be part of the data plane and can be part of a control plane. And depending what type of characteristics you want to get, you will choose 1 or the other. So telemetry is always part to one those 2 planes? How how do you see that as a separation? Well, it what we have in mind will be, like, a kind of aggregation of the information from the control and and data plane. it's true, to be honest, that we have the debate on how to name that plane. At the beginning, it was some call learning plane. with the same purpose of quality information from controller plane and data plane, Now we are moving to the elementary frame, but we have yet some doubts would be the precise name. But the the essential idea would be the having a plan for managing and handling the data. with that data, triggering actions later on, that could be actions like control plane or data plane. So that would be the the purpose. It's it's a kind of aggregator of information that could be used for feeding AI control loops. Telemetry give some other meaning. So -- Yeah. -- in this case. -- HIPAA. Diego. Can you quickly speak? Very, very, very fast more recommend towards, and you say, was saying, when asking about this. Just a just a remark that this has we said it was important that we are not trying to make a totally isolated fixed layering approach but the idea that the different layers can interact with with one another. and and"
  },
  {
    "startTime": "01:50:03",
    "text": "possessing because of that, one of the ideas that we have in the QARG, we have this discussing a model of quantum networks based on previous experiences we have had with QKD, etcetera. And for some discussions with Luis, very likely, we will go for trying to apply the same model. I'm trying to portrayed how a quantum network would like would look like in this in this environment. Just as a note, I wanted to to date. Thank you. Thank you. Thank you. Thanks so much. Okay. Our final presentation is about security. It's that an individual draft And let me bring that up. Hello, Pasco. How how much time? Well, if you could fit this in 5 minutes, that would be ideal. Thank you. In fact, if you'd like, I can put a timer up on the screen. So you can see. Okay? Okay. Yeah. There it is. Oh, and you have it there as well. So just too worried about Teekomparis because Teekomparis is not a company. taken Paris as a university in part of a Institute for a technique of Paris. And It's a place where one century ago was invented the World State Communication. by Eduardo Astoria just to So we are mostly comparing. only NIC and then the next item. So the the goal of this draft the the goal of this draft is to It's a term that is to define some security features the about Gong. And so as it is mention in a coin draft. coin works with programmable networks"
  },
  {
    "startTime": "01:52:00",
    "text": "devices. That's to save device connected to network with some computing resources. And such and fast structures could be integrated in the edge computing of IGS Lacing. So a program works with several PMDs, changes in the data over secure communication. So in that context, there is a need for security for is that to say infrastructure, let's say, companies and infrastructures, and such as you need some security fee fee features. But the other part of which is contained in the definition is that the program are a running income system. So it's been because you you run program in this environment. You also could need some security feature for the program itself. So. Next slide, please. Suve. So interesting coin securities, that's to say, and and fast of your security. So corn should rely on the full encrypted communications. So do that, you need authentication and team mechanies that could be based on asymmetric or asymmetric secret. And another important features maybe. is about billing because In communication, billing already exist, you have reduces server, for example, they while I billing the features. But also in in Blockchain, when you use a blockchain such as a the dealing with smart contact and so on. you have something called zip the gas price. So it's mean When you talk about the distributedcomcomputing. You may also think to being Vitchos out. the service would be charges if you need to to charge the services. So Next slide, please. So this is an illustration of what could be infrastructure security."
  },
  {
    "startTime": "01:54:02",
    "text": "So because you you you are going PND are going to manage key They should need a key management system. And after you could have 2 kind of infrastructures, one should be centralized with something called the authenticate authentication that will provide key, through. Brashaki. And this processor could also include physical or secure physical identity, such charity, for pursuing execution environment. or us, security amount. So next slide please. So about conprogrammed security, so you could have some equipment. For example, if your proud compute some transaction for opposite you you need to store a key says says, well, And for example, in another drug, we have something called Internet of securement, which is the idea of putting multi tenant security features in in networks. could be a pandd, could be notification Center, it could be in computing. And again, if you need computing, feeds, how can you evaluate to this this fee. So next slide, please. So identity. We we need an identity for our PND, Because from this identity, no no no no knowledge, you should get information about the computing resources available in this a PND, and the cluster available. plus level is something that are important. Because you you you need the when you delegate the computing to somebody, you need to to know if it's the the program would be executed let's say, in an honest way."
  },
  {
    "startTime": "01:56:01",
    "text": "So you have 2 kind of architecture. You have single tenant. Let's say if the PND will be owed by the same entity, like, in communicate let's say, mobile networks, for example, all operators and all a set of communications features. And multichannel architectures mean you will have some cluster of BND, that will be managed by several better, in fact. Or or several owners. So so so next slide, please. Okay. And just letting me know, you need to I have to assume to test light. and that this is an example with you using symmetric architectures. So it's been you you you you give certificate to every PND And for that, you have a tenant which is a certification authority. And so users that can be a PND what do you want to use? set of PND, he has a certificate, and so it it can it may establish secure channel, and if you want to use 2 Crystal, of a PMD managed by different owners, then as a user could have to to to to to 2 certificate. And this is my life state. Next, please. Thank you. And this is an example with asymmetric architectures, So in Timothy -- This is what you added to the draft. between the last one and this one for those who may not have -- It's then it's mean you have a authentication sound center. And so every every PND and user Oh, some relationship with the authentication center, And so if your user which can be a PND. 1, 2, and directly, we wish a PND. issue re re require for the authentication center. some token. ticket, like, like, like, in Cabos. or token, like, a web token. in order to establish"
  },
  {
    "startTime": "01:58:01",
    "text": "secure communication and have exchanged some services. And, again, if a user or if you want to to use 2, kind of CND. managed by 2 management entity. then you should be really, really require this Credential, from several authentications until And and that's it. Great. Reaches A, you're in the queue. You next? Yes. I think it's interesting that your your system, doesn't have anything coming from the user. It's all coming from the network. And if I follow your all your arrows. the only things that come from user or some kind of dash dash dash dash. I would say from what I know, it should be the opposite. The user could be I should say odds. I I don't know who is with the user of the system. could be a PND. Well, it could be anything, but what I'm saying is that your architecture is very operator centric. and For for lack of of time, maybe we can take that on the list. because this is This is an interesting thing because from my world, this wouldn't work. And as I say, it's rather it's not operator. It's other single tenant or multi tenant. This -- Well, your user is is just a passive agent in this. Yes. But -- And I think I think an a a user should not be passive. Yes. Agree. But the the the the user is a generic term. It doesn't mean that is a user. It means that it's some bosses that"
  },
  {
    "startTime": "02:00:02",
    "text": "communicate Well, then then then maybe you can update maybe you can update your your years draft to to to make sure that we understand who the service But, again, we're out of time. So But I, yeah, it would be great to have an update to clarify this. and also to take further questions and feedback to the list. Okay. Okay? Thank you very much. Risha said, would you like to wrap up? Yeah. I think we we we addressed everything. I think I'm very happy that the the that the view of of the 4 year is actually also shared by most of the people who presented today. I think I will take Collins Comments into action items, to see what we can do to that. We always say we're gonna have an interim, but we always have so much work going in parallel. but maybe yes, maybe end of September, we can think about it And for for for prod, I know we've had a request team team Not overlap with this conference on And if BSDN in in Germany, but you know, IETF. We'll try. Yeah. IETF scheduling is totally complicated. So we may ask not to be on these days. But for people from that conference in Dresden, you know, I think I think of you can always listen to the recordings, and you know, we're we're going to make our best to to do this. So people are starting to leave because of the deadline. So I think we should wrap up and thank everyone for staying with us, especially those of you in time zones where it's well after midnight. And -- I'm so sorry for our people from I know"
  },
  {
    "startTime": "02:02:00",
    "text": "China and is 12 hours ahead of me. is, like, 2:31:30 to 2 AM -- Thank Thank you so very much, Jeff, to be I've been there. Thank you for all the draft. and presentations and especially for the discussion and comments. Let's continue on the mailing list. Thank you so much. And and, yes, I I yeah, I can just say for all the other all the other chairs, we're so happy. We have you guys in our team And we are moving forward, and I think this idea of computing in the network that started with A switch, a switch, is now much more than that. you so much, guys, for all the research. Thanks, everyone. Bye bye now. 5. Bye. 1st. Oh my god."
  }
]
