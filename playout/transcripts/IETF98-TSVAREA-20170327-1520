[
  {
    "startTime": "00:00:11",
    "text": "you mean to turn the whole time I think we\u0027re good I\u0027ll take easy choice order mess up to do what\u0027s right it didn\u0027t seem to make we\u0027re good we were flying very low and we got in here than equal to clock I\u0027m a gallon a meeting that went out 1252 so we were deciding free to the computer chair slides at the Embassy Oh creepy point so we will be starting in just a moment or two [Music] see this yeah so there are she loves you there\u0027s another so let\u0027s go ahead get "
  },
  {
    "startTime": "00:03:17",
    "text": "started yeah maybe in about an hour so I\u0027m still Spitzer a new year and we\u0027re getting ready to start the second tsv area session we are passing around a new set of blue sheet so if you were at the last meeting please sign blue sheets again because that\u0027s per meeting um are we putting up the note well again yes excellent so we will put up the note well again so that everybody can note well that thing said here are ITF contributions yeah we had one session already and we start in the into the next sessions we have a presentation by Tom Herbert and then we have the rest of the time which we probably hopefully might not use completely on the discussion about congressional troll and how to handle congestion control algorithms and conditional to work in the IETF but we start with Tom where is he they\u0027re perfect hi my name is chrome Herbert and I\u0027m going to talk about Express data path or xdp a little bit of background this is a project that we started in a Linux networking stack about a little over a year ago and if you remember at HTF in soil in the open plenary we had that discussion about the denial of service attacks and dying and there was X a cloud fire presentation one of the rationales behind xtp was precisely for dressing denial of service attacks kind of like that and it turns out that CloudFlare actually presented a discussion about a year ago on what they were doing to solve denial of service in their system and basically they were going through a pretty long path and getting really miserable numbers so some of us in the Linux community we started looking at the problem and realized we need a better wife so that\u0027s kind of the background I\u0027ll get a little more into some of the things we\u0027re doing with cloudflare a minute next Laye so agenda today so I\u0027ll go a little bit with history specifically userspace tax which have kind of been the current model to solve a lot of these problems and then we\u0027ll go through the the "
  },
  {
    "startTime": "00:06:17",
    "text": "foundation BPF eb b PF and then present the solution and then how we\u0027re looking forward to extend the solution so usually inflamed works the idea behind these is that there\u0027s a librarian user space and the application can access the networking directly so in this case packets for instance arrived on a Q and a user space application or process processing them directly without any interference or any processing the colonel so this is kind of like a direct data path from the network to the application completely bypasses the colonel it\u0027s safe to use people like that and has found a lot of niches particularly in high-frequency trading and some hpc realms so the question is what why why did we ever bother with this we\u0027ve had colonel networking stacks for years bsd stacks bsd sockets they\u0027ve all been kind of developed the nominal reason is usually given as performance so by having direct access to say network device from user space we can squeeze out some performance but some of the other reasons are safety for instance because of the isolation that the system gives you if we crash an application in user space it doesn\u0027t crash the whole system so that\u0027s kind of a nice feature developing developing in the kernel is difficult in some sense a lot of people are little trepidatious you look at companies like Google and Facebook they have maybe 20 or 30 curl engineers and thousands and thousands of C++ programmers and Java programmers so there\u0027s a little bit of weight towards kind of we know how to do stuff in user space we get more and more into user space that\u0027s somewhat of a trend we\u0027ve been seen and another big one and this is is one that that we\u0027ve known for a while has a lot to do with upgrades reboots things like that if we have to reboot the colonel because of problem or read about the system very invasive takes a long time an upgrade Colonels what have you so that\u0027s kind of a known ongoing problem so on the other hand user space tax frameworks they\u0027re difficult to use for general solutions if you look most of them are usually some niche applications best example is probably the high frequency traders the guys who want to execute trades in microseconds they really don\u0027t need anything except access to the stack maybe do UDP or TCP but it\u0027s very dedicated application so it\u0027s a good use case for this for the general case though implementing say a full tcp/ip stack and user space very difficult to make work and cross all circumstances hybrid path solutions this "
  },
  {
    "startTime": "00:09:18",
    "text": "is kind of what cloudflare was doing originally they were actually taking packets received in the kernel affording them into user space on a raw interface doing the denial of service mitigation they\u0027re basically filtering out which packets to say which packets to fort and then they were sending the package back into the kernel do more processing therefore TCP and then actually go up to the application so this kind of boomerang effect we\u0027ve seen occasionally amazingly they claim this was better than than implementing something in the kernel using TC classifiers nevertheless that\u0027s what they were doing performance numbers if you look at the documentation a lot of the documents about user space tax will say they\u0027re 10 10 x 100x faster than the kernel and those are little jaded be careful about that user space 2 X will be faster but not binding so many orders of magnitude but more practically when we saw em for instance at Facebook and Google when we consider doing this if we have to bring in a separate stack even in user space maintaining two stacks in the same system that\u0027s a little bit of pain that\u0027s one of the kind of big downsides to having user space backs and we still need a kernel stack the other thing is user space and kind of implies more proprietary solutions this can be good in some context for for an application that needs that but if you think in terms of a larger community if we do have a good solution for denial of service if one application needs that is probably true that almost all of them need it so it\u0027s good to have that kind of community behind us the other thing is there are some extra constraints for instance huge pages in DP DK these both help and kind of hurt that so they help and get the best performance but they can actually hurt in some generalization of the problem so what we really need is going to be programmability of policy in the kernel and it\u0027s actually programmability everywhere in networking if you think about it so the trend of SDN for instance is programmable networking program own devices programmable in the kernel programmable from user space so everything\u0027s programmable in some senses is kind of the goal and the colonel it\u0027s a particular issue like i mentioned kernels are typically monolithic hard to update making it nimble for program available t kind of an oxymoron in some sense with some of the goals of the colonel nextly so this is the long long text of what we need but the short answer is we just want something that super programmable super flexible and part of the community so that we can share our solutions and this was kind of the basis for how we started thinking about ex VP Express data path so now we "
  },
  {
    "startTime": "00:12:24",
    "text": "can go back in history a little bit and beginnings of our solution actually go back to nineteen ninety two at Lawrence Berkeley Lab\u0027s van Jacobson and Stephen McCain kind of were the first to see that we need this programmability inside the colonel needs for one small part so from this berkeley packet filters was born we\u0027ve known about this for a long time it\u0027s been heavily used in TCP dump packet filtering trace domes things like that now from the beginning was kind of limited was never really intended at least back then to be kind of an open flexible platform for programmability so was kind of limited in what it could do it had a whole two registers was I think a 16-bit system so the idea was great I have a program ability of networking in the kernel a packet parser that is effectively open-ended but with the limitations and I guess with we needed to develop the rest of the coronal the rest of networking so for about 24 years we didn\u0027t see this hidden gem and some in some sense like so fast forward 2015 and this thing called extended BPF was born alexys solver 12 now Facebook actually had this idea what if we extend BPF added number of registers make it full 64-bit system make a compile easily from C code directly into whatever assembly whatever bytecode we need have a comprehensive instruction set so we needed to add more than just simple header parsing we want 64-bit operation or 64-bit ops atomic operations data structures actual real data structures that we can put ancillary data use those for things like state what have you the other important thing was since we\u0027re running this code kind of loaded into the kernel we need a model of safety so we have this thing called a verifier which prevents code from doing bad things so for instance we can prevent division by 0 or illegal memory accesses things like that even though within its within the colonel so the idea is that this gives us the same sort of isolation same sort of guarantees that user space gives however we\u0027re still running natively in the colonel and sense with a very thin API so effectively we expect that to be almost bare metal inside the colonel so II BPF ended up being pretty successful and right now in the linux stack this is used pretty much all over the place the tracing guys really jumped onto this so we used to have things like DTrace estrace these were kind of difficult we wanted them to be flexible but we also "
  },
  {
    "startTime": "00:15:26",
    "text": "need it some sort of programmability like if i wanted to trace specific events how do i program that so they jumped on top of this and now there\u0027s a whole kind of sub sub project on making BPF and trayce Thompson logging work inside the colonel that\u0027s even outside networking and one of the side effects of that was its became so big that we now hardware support for it and and heading down that path next time so the BPF architecture is pretty elaborate now as I mentioned there\u0027s a lot of use cases at the top even outside of networking so we\u0027re using these for analytics blogging as I mentioned and the good news is we can write this in different languages so the original BPF was pretty much machine code assembly hard to use about about the same time he BPF came out LOL vm added compiler supports now we can write BPF directly and see so that\u0027s a huge win for programmability this goes through various colonel subsystems we have a JIT compiler and also back in compilers into specific platforms and then as i mentioned BPF is running across various forms of hardware so all of this is great it still doesn\u0027t really answer the question of how do we do denial service mitigation at line rate millions of packets per second that\u0027s where express data path came from or at least the motivation so about 2016 we started looking at the denial service problems in particular also load balancing load balancers things like that and we came up with this concept of xkp the idea is to run a BPF program as low as possible in the kernel basically on the device so the program is actually sitting the received part of the program sits on receive cues as packets come in we actually process the raw packet so there\u0027s no concept of metadata and no concept of going through various layers of protocol processing we\u0027re just basically calling a function a BPF program saying here\u0027s the packet here\u0027s the beginning of it hears the end of it may be a little bit of ancillary information that we got from the device but that\u0027s it BPF program go do whatever you need to do so BPF program takes this and it actually comes up with a simple decision either drop the packet accept the packet into the rest of the stack or transmit the packet transmitted the packet usually entails either in popping or pushing encapsulation headers and then sending it back out right now at the same interface but eventually any interface all of this being done basically on the raw packet for various "
  },
  {
    "startTime": "00:18:27",
    "text": "reasons we don\u0027t need to do DMA on maps for instance or in a fancy memory allocation so very fast we get the speed BPF gives us the programmability and the environment gives us kind of that safe context that i was talking about before so the important thing is this xdp data path even though it\u0027s in the kernel works in harmony with the colonel it\u0027s still kind of programmable it\u0027s like so from this we kind of derive the xdp packet processor and pretty much what i just said so at the bottom we have the various input queues for my device they call into the BPF program and the vpf program district turns drop in which case we just discard the packet not a lot of work to do there forward is rewriting the packet prep maybe an address and then sending it back out the same interface and then we can go through the receive stack gr 0 is one thing that we have still have to do that\u0027s a way to Cole ask packets of a single TCP connection into a larger packet for the advantages of processing larger packets in the set and then those packets can go up and then if we receive the packet it just becomes a normal packing in the kernel we just do the normal processing so for instance if we were doing denial service mitigation we could do the mitigation at the lower layer and if the packet exhale acceptable we just send it right into the stack if it\u0027s not accept the wall gets dropped I think so xep has a few important properties so one is it is designed for high performance so there\u0027s a number of techniques that we know about user space tax have been using these and we can put these same sort of techniques inside the colonel on next EP spin polling optimizing cache misses with catch prefetch four or five other things all of this is locked less and we never get into atomic operations in this path as I mentioned its design for programmability so we can add new functionality into the kernel without changing the colonel without a reboot everything can now be done on the fly it is not Colonel bypass in the sense that it\u0027s integrating to the colonel and we can call into kernel functions and use Colonel data structures this will become really important for instance if we want to do statesville tcp and we actually want to see does the current local stack actually have a tcp state so we can access that tcp state from the BPF program to determine if there\u0027s a state associated with it it does not replace the tcp/ip stack this right now is a layer below it we know some ideas for kind of introducing some of these features at a higher layer layer to get some of the benefits but for now its programs running at the lower layer "
  },
  {
    "startTime": "00:21:29",
    "text": "there are some other BPF programs that run in concert with sockets and some of the other aspects of tcp so this is kind of one aspect one application of BPF in the networking back but the point here is this is so low level this is in a sense below the stack kind of a preprocessor for the stack and then the last one is is very important so xdp does not require any specialized hardware we in theory can run xtp on pretty much any device and we\u0027ve added it to i think five or six devices now it\u0027s really helpful if the device has multi q well but that\u0027s the only extent we only property we need so we don\u0027t need ritual functions or any of that stuff now advanced features we do want to interoperate with and use those however fundamentally this is something we believe can sup we can solve problems in the field with existing hardware with software change so it\u0027s kind of an important feature exactly so currently xdp is being deployed for various use cases best way to describe it is layer 2 layer 3 packet processing denial of service mitigation the easiest one is just have a sort of black list of bad IPS and can filter on those very simple sort of lookups more sophisticated denial is heard of protection gets us into some stateful tcp for instance pattern matching can we identify signatures bad packets at some point we may even need more advanced lookup mechanisms regular expressions what have you some of the other applications facebook right now is deploying this for load balancing this is replacing ipv ipv yet PVS as a solution we\u0027re finding has a lot more performance and that\u0027s working out really well we\u0027re also seeing use cases in switching routing tamil termination again at Facebook I was working on ila routing and one of our use cases what\u0027s basically built a la router in the network it\u0027s nothing more than a glorified host routes which in a sense so packet comes in we rewrite 64 bits of the destination address and send the packet on it\u0027s very simple process it was like 20 lines BPF code and we\u0027re able to build a device kind of doing line rate without requiring specialized a second specialized Hardware sets win a few performance numbers so this was a intel xeon with mellanox ml x5 Nick it\u0027s a kind of continuous thing we\u0027ve been getting performance numbers to improve and improve so there\u0027s kind of latest one so for comparison the colonel using TC a traffic control which is kind of the typical way that we would implement "
  },
  {
    "startTime": "00:24:29",
    "text": "this sort of functionality about 3.5 million packets per second on one core xdp doing the same thing gets us up about 16.9 million and xtp transmitted a little more work because we have to rewrite packet we\u0027re still at thirteen point seven million packets per second overall in the system if we allow 24 cores we\u0027re getting 45 million packets per second transmitted I believe that\u0027s the current hardware limitation so it\u0027s scaling pretty well one of the benefits of doing it this way since we don\u0027t have a lot of atomic operations most of these will scale linearly with the number of course the reason we test one cores we need to show performance at that level for certain attacks that are obviously like a temple with hack would be on one core so that\u0027s why we typically measure this number cores and then have the whole system next slide so looking forward right now we\u0027re planning on enabling more drivers currently we have five supported we\u0027ll be adding more the virtualization was actually just recently supported there is hardware support for xgp metronome is the first case of that so basically the idea is all flowed the BPF program so BPF is nothing more than some sort of bytecode when you compile it in theory can run on anything hardware software or what have you so they can download the BPF program and we can run that in hardware let\u0027s see so we need to build out the ecosystem of contributed solutions so the good news going back to cloudflare next week we\u0027re having another conference on Linux networking they are presenting a solution for din Alice or service mitigation for xdp looking forward to that one another important one that we learn from something like VPP and some other initiatives packet batching in terms of processing is important so more packets we can process at once for a particular piece of code the better instruction cache of fits and see what have you we see better performance with that so these are in ways to squeeze more performance out of the system we do have a little bit of a performance gap that we\u0027re still working on with dpd k as I said if not orders of magnitude it\u0027s within a few percentage points and we have some ideas on that one of the big differences between users face and kernel is user space can enable certain instruction sets on the x86 for instance of floating point instructions and string instructions can be used we haven\u0027t really turn those on in the kernel for obvious reasons that they\u0027re kind of difficult if necessary we might we might get into that also we\u0027ve kind of anticipating some TLB translation lookaside buffer issues with huge pages in certain applications so that\u0027s another thing we\u0027re looking at and as I mentioned busy polling and some other "
  },
  {
    "startTime": "00:27:29",
    "text": "techniques or certainly techniques we can also integrate into xdp to get that last bit of performance any questions [Music] thank you very much thank you I think that some interesting stuff that\u0027s going on in the linux kernel and hopefully helps us did you look at more experimentation with new schemes here say yeah next one is Michael he\u0027s coming up and he\u0027s bringing his computer perfect so we now have the rest of our time we can use freely for discussion on congestion control in the IETF because we see a lot of congestion troll were coming up and we want to get feedback from the community what should we do here is something we should target in the I ATF and the I RTF and where and how and so on and say we have two people giving a little bit of background one is Michael who\u0027s also talking here as the chair of ICC RG and then afterwards we have for being giving some more background and then we have time for discussion all right hello everybody I\u0027m bringing my own laptop and plugging the same because there\u0027s an animation i want to show you i have work at the university I teach I do so many presentations but the best ever animation in my whole life is 21 / about to see solely fantastic you\u0027re gonna it\u0027s got it\u0027s amazing you know if time for me taking time for me hits him I might actually show it twice alright so the history of Isis GOG icche was created is about the role of congestion control and ecology and how this whole stuff fits together in the ITF it\u0027s really you know why was I see zhe created why does it exist it was created at some point because there were all these proposals on new congestion controls cubic HTTP compound TCP at that time they were brought to tcp M\u0026T CPM was busy and they needed a place where these people could go and discuss and maybe give up hopefully give up right or maybe continue and then then at some point come back and then they\u0027ve been pre evaluated and I\u0027m here to tell you that this is a good procedure and it\u0027s been actually working so yeah the phrase we need propose propose a place where we can push proposals over the fence and then they can bake there until they\u0027re ready and then this is how we do it so that\u0027s actually we\u0027re with a happy trashcan of the idea we got stuff you know it bakes there and now yeah it\u0027s "
  },
  {
    "startTime": "00:30:31",
    "text": "coming okay here\u0027s a very important aspect to this animation before anything else happens there\u0027s nothing else there right but ii CPM is already grumpy because that\u0027s that\u0027s how they operate so this was ITF 75 2009 mr. junkie they always are right but this is about about a historic procedure we have a proposal coming in in rainbow colors it\u0027s a double u-turn I was proposed to TC p.m. at this ITF and off it went and then that\u0027s what happens you know I mean you spend time this is your G and it came back anyway and then they were happy and that\u0027s pretty much how we operate you know you bring stuff to us we you know it spent some time in trash and up afterwards it\u0027s beautiful and shiny and everything and so in this case it took only three I mean that\u0027s maybe a positive aspect it only took three idf\u0027s or had to come back and then put some more time in TC p.m. and then it was an experimental RFC ICC OG also can publish documents that\u0027s you know something that people don\u0027t seem to be aware of that\u0027s the IRT attract other groups are publishing a lot they can be informational experimental we have only to congestion control in the RFC series and open research issues in congestion control kind of survey stuff why don\u0027t we have any specs of any experimental things only because people don\u0027t seem to care about it or don\u0027t want that it really is an open question I believe that people kind of now I\u0027ve been proposing it actively to people they were like all oh IC AG publication I\u0027d rather go in the corner and shoot myself and I don\u0027t know why that is but it seems to be a really don\u0027t want to do that so one of the questions you know that was that was asked I\u0027m trying to address here is it necessary that I teeth congestion control work is spread around the ITF the way it is I personally think this isn\u0027t an inevitable thing because congestion control as a function is intrinsically tied to the goals that the protocol is trying to solve if you think of MPG cpe your arm participe congestions all is really really clear on what kinds of goals it has and what is trying to achieve which is very different from say that bed or from say the tcp well not so different from tcp okay but from our mcat for instance but has a goal of being delayed based yet into operating with other floors and so on and so forth so I\u0027ll conclude by saying you\u0027re happy trash can is always here to serve you and questions Jahangir who\u0027s also "
  },
  {
    "startTime": "00:33:37",
    "text": "holding the trashcan button shaking there\u0027s a does we just to continue this thread we have right now cubic that\u0027s sitting in TC p.m. and we have a BB r that\u0027s been presented at icc RG a couple of times and if we end up you know getting a draft out of that at some point there is going to be a question of where should it land and the in mind that we know how a quick working group that actually has conditioned controller also as a part of it so going forward that is a real question of where should all of this condition control activity lie should be even in in retrospect now you know maybe take cubic and say well you should you know write it in such a way that it also applies to do PCB maybe we should have a cubic TCB and a cubic a quick draft I don\u0027t know but if you\u0027re describing it without framing it certainly seems to belong in a place that\u0027s not specific to a protocol let bed was a perfect example of this but going forward I think we should think about how to write condition control documents are there separable from the protocol itself and maybe also having ways to express how they are specifically exemplified in those protocols okay yeah so the IETF a long time ago bought this Dogma TCP friend body which is of course an oxymoron because it\u0027s not about TCP at all it\u0027s about Reno we\u0027ve reached the Internet has reached the scale where you should replace those words by not relevant in today\u0027s Internet and the problem is that really belongs in this working group this this research group is if you disband the words TCP friendly what do you replace them with how do you say freedom from congestion collapse how do you say reasonable behavior under resource exhaustion how do there\u0027s a whole bunch of things that TCP friendly embodies the TCP friendly hit has no longer no longer functions to provide and we really need to replace the TCP friendly dog I guess some other safety consideration I guess the probably most accepted definition that we have is in one of these RFC\u0027s that I think I think that the wall about usefulness of best fo traffic from Sally that says it\u0027s about avoiding starvation of other flows so right that is at least you know it\u0027s a lower end so freedom of from congestion collapse and avoiding certain pathological behaviors which include starvation all that might not be the only one it\u0027s a bit extreme to be seem to be a better bar to be striving for but the problem is if you for instance look at at the research community academic research review panels still dismissed protocols that don\u0027t address "
  },
  {
    "startTime": "00:36:39",
    "text": "the issue and making authors spend twenty percent of their page space explaining why TCP friendly where the protocol is adequately TCP friendly is actually doing us all a disservice because it means there\u0027s some very good ideas out there that probably are the feel double in today\u0027s internet which are in fact being banned from being published because they\u0027re not TCP friendly so I wonder you know if this discussion is already going until congestion control issues I mean maybe it\u0027s better to wait for plug-in and put that at the end of that because this is yeah because this is about I mean that was just a procedural introduction but he has a few slides raisings on general questions about commercial maybe we\u0027ll be even better you know for you to moderate the discussion and get this started in that way praveen only has like three more flights which fit very well in the discussion that we just started so let\u0027s look at them and then have the discussion you can stay there will not take very long yeah we want a discussion but I think it should be M thank you you can also just stay in front if there are more questions hi I\u0027m Praveen Michael touched on the operational aspects of congestion control in the IETF I wanted to also bring up the some of the issues that we have been facing as an implementer of TCP condition control and various various other forms of condition control so yeah I don\u0027t have any fancy animations but so this is kind of a recap of where we are in terms of condition control support and various operating systems if any of this is out of date please let me know in a fix up the slides later Android and Chrome OS are doing cubic by default I believe iOS and Mac OS are also doing cubic by default Windows has been doing compound is before the while and on the other front there is a lot of mean of progress in data-centric magician control where you\u0027re dealing with low low our treaties as well as a different forms of workloads like mapreduce data center tcp DC QC n is for our DNA same idea like two RDMA and some other variants of DC tcp which use accurate ease in feedback Then There is obviously bbr which has now I believe based on today morning stock been expanded also to run over van and then timely which is a delay based condition control and there have been like academia proposals in icc RG as well a PCC sprout remy the other kind of interesting thing to notice is that the network operators are trying to solve the problem in the "
  },
  {
    "startTime": "00:39:40",
    "text": "network using active queue management but the end devices have no visibility about this so they cannot for example assume that a given network has a km or not there is no way to detect that so the operating system implementation has to kind of account for the case where there is no a qm quick is coming up there might be cases now we\u0027re popular mobile applications write their own transport so things are getting to a stage where there\u0027s not just one or two congestion control algorithm stood there could be ten congestion control algorithm sharing a bottleneck link and it becomes very difficult to kind of reason about performance or you know how the network would behave the other interesting case is i as in the public cloud where again you have VMs that are running different operating systems so kind of doing different forms of condition control on the same bottleneck links so buffer bloat is real this is like smooth dirty D information from like TCP connections on desktop of an xbox consoles so if you notice obviously I don\u0027t want to imply that correlations links causation here but it does look like on in peak load times the rtd is inflating a lot and this impacts everything from like page load times to us awesomeness in games so this is a real problem we are seeing this data right now so so these are some of the kind of questions that I wanted this room to kind of chime in on and debate so yes a multitude of congestion control algorithms so how do implementers deal with this situation do we test the entire kind of matrix here of every condition control working with another condition control as well as a QM this matrix is kind of blowing up so as an implement of my question is what would be what would be the best way to kind of address that that problem the other question I have is how do we prevent this from becoming an arms race for example when we run tests with cubic and compound sharing the same bottleneck link cubic just kills compound and then it would create a perception problem because people are running speed tests that you know for example windows\u0027s you know slower than android for example right so the question is how do we kind of prevent this from everybody trying to go as fast as they can and kind of causing Buffalo to become worse over time I think Matt was talking about TCP friendly so yeah I have the quiz question now what does differently actually mean bbr is also complicating things a lot more because it\u0027s heuristic based so now you know in presence of bbr how do we define things like RTT friendliness late comer fairness and "
  },
  {
    "startTime": "00:42:44",
    "text": "yeah the final question is what do we do about this is just publishing a set of informational RFC is good enough because as an implementer I feel it\u0027s not good enough because it seems like there is not enough clear guidance on how to we kind of deal with this situation today in today\u0027s networks so yeah I would request people to like could you expand a little bit more on your last point you\u0027re as an implementer you feel that information these aren\u0027t giving enough guidance what would be more useful or what do you feel is lacking ah my question would be how do we kind of define what it means to coexist with other condition at all algorithms and again the TCP friendliness question remains so do does every RFC have to go out there and take into account every popular condition control algorithm and list how it is friendly with it how do we define rtd fairness and late comer fairness and all of that ok so my other comment is really addressed a little bit more towards Michael\u0027s presentation but i think it\u0027s it\u0027s relevant to both um but just going back to the happy trash can which i believe is a metaphor that should live on for a long time I hope Lars gets a chance to you should give him a happy trashcan that\u0027s a good by giving the so the ICC RG when it was created if memory serves one of the the asks from the ITF to the I RTF in creating this research group was to come up with some way of evaluating whether congestion control was safe how to fairly compare different different algorithms because we were there were working group meetings where people would show plots that showed their favorite algorithm was performing really well and then somebody else which you know and they were mutually exclusive and so the ITF community unable to to sort through that said well you guys go figure it out and they sent them off to the happy trash can so I think that that\u0027s still a really valuable role and I think what the discussion and I\u0027m really the comment that Matt me oh good you\u0027re here most Lars\u0027s in life I think that Matt made really good comment which is that our criteria for how we evaluate transport protocols has probably changed over the past couple of decades and we haven\u0027t done a really good job of articulating that I mean some of that falls on the research community but i think that from from this is a community the implementers and operators and engineers and so i think that we can we can speak to the research community and "
  },
  {
    "startTime": "00:45:45",
    "text": "say how how we prioritize or how we think things should be evaluated for for safety or desirability and i think that that i still think that that would fall into the icc argies bucket as opposed to that I etfs bucket because I think that that\u0027s still kind of a research consensus sort of question does also this TCP evaluation sweet that has been around for a very very long time and it has suffered from cycle problems it has been handed over and at some point when David Hayes was in charge of it and while he was working for me I pushed for getting this done and getting it into RFC stated simply percentages here and it was pushed back for being outdated which it is so all we need is a volunteer you know to take that and update it to the current state and that will never happen but I would be very happy if I mean I am soliciting volunteers David black one of the chairs of tissue of G transport air working group which will meet right after this I likely have a trash can like I\u0027m not go go look at websites ECT I can find when the represents the whole variety stuff that comes our way we deal with congestion concerns some which are control some of which may being forward isn\u0027t applicable to for whole pile things that haven\u0027t yet been mentioned here sctp lives in TSU WG anybody wants to play D CCP comes to us this miss controlling you in in the new you UDP usage guidelines UDP and cap as a whole other adventure in and of itself so Mike I tend to agree with your comment earlier that we\u0027re going to string just all over the place and at least speaking from my parochial perspective wearing a teacher of G working group chair hat having ICC RGB a central clearinghouse for congestion control issues that we don\u0027t understand because we haven\u0027t got the expertise in the smokes of people once were cons where might want to work something is incredibly useful to encourage you to continue Ted can I had some thanks um I don\u0027t think the question is do we still need I see crg I think there is no question about it it\u0027s it\u0027s a they do good work oh that\u0027s where the research has come when we get the input from research that we really need my question is rather can we give as the IETF any recommendation what you do because our current recommendation is use Reno which is a little bit outdated our soon future recommendation will be used cubic because there\u0027s a draft out there which soon will will be in our seat and the reason why we think it can be an ietf rfcs because it\u0027s deployed large enough that we know it\u0027s safe but it\u0027s still not like where we are at the state of the art so how can we make our recommendation be given the IETF a little bit more up to date that\u0027s my question huh Laura Secord so um Michael "
  },
  {
    "startTime": "00:48:47",
    "text": "said a bunch of the Oscars it so this slide could have been from 10 years ago um because it\u0027s the exact same questions we asked ourselves when we had Hamilton TCP and cubic and and compound right um and back then we had Sally is still around an active and she basically said we\u0027re going to do this evaluation sweet and there\u0027s a paper on it and then somebody turned into into Anna\u0027s to code and that sweetest is very boring right it\u0027s basically just a bunch of scenarios and the idea was that um four th EF but also for the academic research community if your paper didn\u0027t run through at least these scenarios and shoulders the plots but and they were really really prescriptive that you know this is the parameter this is the metric here\u0027s the range plot this for your thing and compare it against the others you know we you would be automatically rejected or you wouldn\u0027t get face time at the ITF so the idea was really to to provide this sort of arm you know how would you call this today I don\u0027t know test cases or something um it\u0027s very boring work right so thanks for kicking David to do this but that would be one way forward and and today right this better tooling we probably wouldn\u0027t do this to anymore i hope arm so that that\u0027s one way forward um if we believe but then you need to keep adding to it right and and it\u0027s it\u0027s at best it\u0027s something I mean I think it\u0027s like looking at the years I will change an unfeasibly way for well under because I we didn\u0027t be never got you know enough volunteer cycles yeah the other problem with it really is that um it as networks get faster right in this one data center stuff earlier right um forget it right did the the hardware effects are so overriding that you can take you an extra stimulation and you know throw it away and and for the data center case we really don\u0027t have any way to do large-scale anything other than running into the data center and and it\u0027s getting even harder to compare um that the one thing I\u0027m since I\u0027m rambling here the one thing I think that is going to be interesting for this year gee that isn\u0027t all sort of gloom and doom is that it\u0027s quick right because it has um a whole new set of information about the path that is better quote unquote and what you get with TCP and so there\u0027s a lot of new meat for congestion control researchers to play around with and come up with new schemes that are actually you know improving things compared to what you can do with TCP so I think that that\u0027s my hope for for where we\u0027re a lot of research will happen in the future um so that that\u0027s something that\u0027s something I won\u0027t happen in the quick working group I hope no it won\u0027t have an immigrant working history tomorrow I\u0027d like to make a comment on that huffines slide there is that when you design new congestion control algorithm you want to "
  },
  {
    "startTime": "00:51:49",
    "text": "basically provide the best use of the network so you you want to do something like was only be ours basically I aim for being very conservative on how you use the resource and generally be nice and the point was that isn\u0027t was mentioned in the video talk this morning is that there are times where you are competing with algorithms that are not nice at all and that we have like pigs and we\u0027ll just consume all the resources you give them an opportunity and the point with status that means that the nice guys cannot get in the network because it will be known that the refacing pigs and when they face the pigs they lose and that means that you end up having to design your software as having two modes it will are the nice mode and if your detection cut tells you that you have the pig on the network you have to pick mode also called a cubic or whatever you call it right okay and and the problem with that is that it\u0027s very easy to design something like that but is not easy is how you get out of the pig mud when the pig is gone and and we we don\u0027t see that as part of the actual test cases that we have today because we don\u0027t recognize that people have put pigs on the network ya know that the way I look at it is no yeah but they are not always there so you black with no TV no II ok yeah yeah so as I am by point is that we should really have recognized this idea that the state of the network is finally if you want evolution we will have to recognize that we have the nice modern the pig mud and the pig mud is when you\u0027re competing with cubic and you to be a speaker Sarah that\u0027s a yes I mean this is a real problem today when like for example a lead bat or compound on the same link with cubic this is a real problem because being nice when the network just doesn\u0027t work yeah why be nice to tcby be friendly the pic tcp when it won\u0027t return the favor escorting Andrew McGregor from Rome cat yet so there\u0027s one of the things that caused me to stop worrying about the stuff is realizing that in the server a large fraction of the big commercial cases where the money is in the internet today the the issue is boils down to all these things about fairness and a qm and and diffserv and all the stuff it boils down to which of your customers do you want not to be able to run HD TV during "
  },
  {
    "startTime": "00:54:51",
    "text": "primetime and if you build enough capacity where all the customers can get HD TV during primetime nothing else matters and that is this effect that is driven it such as the internet doesn\u0027t care much about congestion control you have bottlenecks at the edges that are at a few megabits per second which are or even tens of mega but even even a gigabit per second which is not which is a small fraction of the core no individual flow can ever cause congestion in the core all of the symptoms of overload our local what you care about is that you have same behavior under local overload and all of the rest of the stuff just doesn\u0027t matter anymore it stopped being important at today\u0027s Internet scale what is important is the fact that the research community and a large number of other people are still solving irrelevant problems problems that that that the solutions to mean that there are solutions the solutions to the problem that they have posed is irrelevant to today\u0027s Internet and we need to fix that one of the things that we don\u0027t have is a good way of defining whether or not a protocol is safe i recently read in a paper a comment that said oh well we\u0027re using tcp so so we know that we have don\u0027t have to worry about congestion collapse no it\u0027s trivial to write applications that will cause congestion collapse unfortunately you can cause congestion collapse with DNS so it is a complicated problem it is a very complicated problem of defining what is a safe protocol and it is really the problem that we care about Nick 11 I wanted to add on on the test sweet stuff so I think the reason another reason why I didn\u0027t proceed further is because it\u0027s actually not that easy I mean might be boring but it\u0027s not that easy I try to implement it and the recommendation there because the idea was to make it as realistic as possible is to use like traffic traces try purchase has that have a lot of short flows and that congestion control doesn\u0027t do anything but short flows it\u0027s mostly slow start so when I was implementing it I didn\u0027t see I couldn\u0027t say anything about it basically and then I came up with my own test cases for my own stuff so it\u0027s actually not that easy it\u0027s not clear what to do there and the other thing I want to point out in this area is that there\u0027s also RM kit which has some test cases as well and I think they took a different approach they took more simple artificial test cases but here the idea was also not to say not to evaluate a congestion control it was more like if you test those tests then you might be safe enough to do further testing on the internet and you might be safe enough for an experimental RFC and then if we see large-scale deployment we can move this to Information Center trick whatever at some point say that was the approach there to allow more experimentation and to make clear that "
  },
  {
    "startTime": "00:57:53",
    "text": "this is an experiment that you shouldn\u0027t like load all your traffic with the stuff you should really try it out and see what\u0027s happening sorry are you saying we should have it we should have another test suite but a simpler one that\u0027s molecular I\u0027m cat 140 CP very unjust WC what I\u0027m saying is we should probably use the status experimental to do experiments we\u0027re sorry we showed you as well the the RFC status experimental to do experiments yeah yeah and make sure that everything thats documented as experimental is an experiment and not a recommendation from the IETF to use it for all your traffic or whatever have heavy RC categories actually mean what they say David black speaking more I think as an individual than a working group chair I\u0027m might be formats comma ballot Matt be the judge be the judge of that he used the word safe and I think he was using it in the context of congestion control algorithms not doing damage to each other I seem to have been spending all my time in a space where safe means traffic that is not congestion control it\u0027s not going to implement cubic or or compound or whatever have you what are the minimum guardrails that happy put in that traffic so it doesn\u0027t damage the stuff that is congestion controlled you\u0027ll see this turning up in a number of things that come out of TF u WG recently a UDP guidelines and circuit breakers drafts percentage now RFC\u0027s in particular and I think that broader question deserves some attention because much is we\u0027re going to work on much as we will work on interesting congestion control algorithms in transport area as all very good and important the some chocolates IETF is looking for some simple rules to not to not screw things up generally and God I was gonna say something else but I\u0027m gonna piggyback on what David just said which is I think this is this is exactly one reason why Reno\u0027s pogo stick around for for quite a while it\u0027s incredibly easy to implement it\u0027s hard to get wrong and it works that\u0027s a pretty good reason I mean I think I think that having having something that\u0027s as simple as Reno to recommend knowing fully well that anybody who\u0027s building our transport protocol is going to experiment the congestion control because there are no interrupt problems as such is I think of a healthy place that\u0027s where we have been for quite a while this is the reason that we have PBR this is the reason we have compound or CT CT and I think that\u0027s not a bad place to be have one basic requirement and then have a bunch of things of course I understand this is a hard problem how do you specify how they coexist I I don\u0027t have "
  },
  {
    "startTime": "01:00:54",
    "text": "any mean it\u0027s Larsa spawning out I remember seeing this is quite a while ago too and it\u0027s it\u0027s hard it\u0027s definitely hard problem the test sweet thing I don\u0027t think scales we can have we can we can come up with a test feedin I\u0027ll tell you now you start working on it now you\u0027re going to get done with it in over three years from now and at that point it\u0027s already going to not be useful it\u0027s that\u0027s the problem through the test suite setup so I don\u0027t know how you solve this problem I don\u0027t think that the suite is it hi this is honest um I\u0027m trying to read him between the lines but math and some other depress saying is and I try to compare it with this sick state in a security environment you have a couple of people who from security context invent cryptographic algorithms you get them in but you obviously don\u0027t want all of those because more agarose mean more problems let alone the analysis of those nobody has time to analyze all sorts of crap that people come up with because the analysis takes much so much more time than crafting something on your own so you have that sort of almost like a DDoS attack luckily you don\u0027t get too many proposals so that\u0027s a that\u0027s a nice things so you try to be come up with a way in a sense of you want to say no but you want to say no in a friendly way by saying oh you have the first to these set of tests which Turner turns out to be difficult to come up if there\u0027s if those deaths in practice however the impression I get is you have to be a really big player to actually really get something adopted because those players then also go ahead and deploy their stuff right away and then tell you after few years we deployed it and it works pretty well um and and that\u0027s how they actually get the work done this is not happening in congestion control and certainly not never never seen that so it was a little bit as theoretical reading in between the line yeah so i think it\u0027s a it\u0027s a little artificial discussion it appears to me so one of the reasons why i speak of a test to check on this is it\u0027s actually trivial to write an application that causes congestion collapse you have a loop with a that does a fork and then the child launches a query and you put asleep in the outer loop or something like that and it\u0027s behaves very nicely under note normal load but if you ever get into the state where the query takes longer than the sleep you get cascading you get compounding load you get regenerative load and its regenerative anything that causes regenerative load causes can there is some part of the state space where it has congestive collapse and and so this is actually very easy to do dns does it because it\u0027s it\u0027s basically stateless and it\u0027s got a "
  },
  {
    "startTime": "01:03:54",
    "text": "two second timer and if you\u0027re in a situation where for instance a root server becomes more than two seconds away everybody\u0027s got two queries in the pipe and the presented load doubles so the consequence of running out of capacity is it latches up and at fifty percent loss rate and it does not recover until the load goes down to below half of the bottleneck rate this is a mess it\u0027s not really fixable in DNS cuz DNS doesn\u0027t have enough stability but it\u0027s really really easy to write perfectly ordinary sounding applications that take a very conservative TCP implementations and still cause congestion collapse and these really need to be viewed as transport issues because its transport dynamics do you have a proposal what we should do it buddy well the research question is is how could you imagine creating a test environment a test suite to look for regenerative load there\u0027s two different things one is is if the presented load rises as the performance goes down and the other is if the overhead rises as a performance goes as your as load goes up and I think of these as being measurement based things because it\u0027s they often very small details in the state machines cause these problems I don\u0027t know if it\u0027s actually a bound problem it made me that once we see the answer Italy oh we\u0027re at an RFC about it here\u0027s a test that everything should be subjected to and that test would replace the TCP friendly language it it may be that there\u0027s always going to be new ways in which you can find creative ways of blowing up things but we don\u0027t know hold on yep hi go ahead hi Sam so one thing that seems to be coming up from this conversation is that is a lot of fun community knowledge about control booth folklore maybe it isn\u0027t being written down and I think it will be really losing life as a community we could find documents in that form so that I was a community example to build on it yeah my brother is missing her go to heal documenting me something so they\u0027ve done a number of efforts through the years to write documents about sort of general advice and they always fail to converge so I mean like we have the UDP guidance document which gives some guidance about like the "
  },
  {
    "startTime": "01:06:55",
    "text": "minimum you have to do about condition at all I have to admit it I don\u0027t remember what it says we just updated yet and that document might say too much actually I mean it\u0027s so even so I mean like the the point about using congestion control is to avoid a congestion columns just by having a adaptive congestion control the ideas that if you see increasing load you would use reduce your load right but if you if you combine this with other mechanisms or you have implementation errors or whatever there\u0027s still something can go wrong um last I guess so I\u0027m one of the authors and I think I speak for all of the office I say if you make us open that thing up again we could but so what it what it says for UDP actually I think captures even met is with overload right it basically says that you should keep an RTT sample if you can and then if you you know keep keep it at most one message outstanding unless you want to do something more complicated like implement and actually regression control algorithm but it also has a default timer when you can\u0027t and I think it\u0027s like three seconds because that\u0027s what TCP uses for the centenary transmit um but it\u0027s not only it\u0027s not only dns that has this thing but they are they already sort of and then the UDP guidelines i think it says that I\u0027m really beginning at that many of those are thoughts apply to any packet based thing right not really UDP specifically but we didn\u0027t want to boil an even larger ocean and then we are already boiling with this document black ACG chair cut cut in front of Jonah to echo Lars\u0027s comment update open etc that UDP guidelines RFC at your own risk there\u0027s lots of sharp pointy spheres that will head your direction see seriously it is a bcp it captures the best current practice as we understand the state of the state of knowledge at this time Lars Lars and and gory and greg did a really good job on him and uh not that the product is very good and I think I\u0027ll wiki page punch at it so I\u0027ll job is done lethargy so I was gonna say that actually piggybacking on this and and and trying to go back to a previous thread about cubic itself cubic is is well it\u0027s it certainly qualifies as BCD not necessary as standard but I don\u0027t want to call it best so maybe you should come up with a categorization called mcp most current practice or something like that deployed current practice there we go one of those "
  },
  {
    "startTime": "01:09:55",
    "text": "classifications would be quite useful I think for us as the ID calls we\u0027ve struggled with this for a while say just because it\u0027s deployed we are not going to sound eyes it sure but let\u0027s talk human tit in some form because that\u0027s what cubic the cubic document is we\u0027re not agreeing that it\u0027s the best thing on the planet we are saying it\u0027s deployed it will be quite useful to have that and I think that actually does go towards you know achieving some of these goals may be so I\u0027ll propose this no condition control work gets standardized they all show up in icc RG there are only RG documents because there is no end to any one of those things it\u0027s a fine world to live in all we need is documentation they don\u0027t need to get sanitized at all not even cubic sorry not even cubic not even kidding why Sunday\u0027s cubic it\u0027s already even like Google\u0027s deployed vbr and others are likely to do other things why are we standardizing a congestion controller so I think the difference is the ietf consensus right and your bird in the message through my gif is this one is safe to use we know this one is safe to use it\u0027s not gimmicks not safe cubix deployed I mean the reason is analyzing it is not because we got together and said that it\u0027s a safe thing to use yeah I got together and said it\u0027s being used by a ton of companies therefore it must be seized yeah it\u0027s we use it for a while in the internet that didn\u0027t melt so much that\u0027s what I called ECB that\u0027s going to deployed current practice this is our new yeah this woman s definition of TCP friendly I just wanted to inject that when Colin was speaking and it came out somewhat garbled in the room we were saying was that there\u0027s a lot of important important focal or knowledge in this conversation that aren\u0027t written down there isn\u0027t written down the community should try to document that in a form that can be published so it can be referenced before I yield the mic to Matt we also have the DC PCB draft thats hanging around an easy p.m. it\u0027s a so that is a real question of you know we sort of all the same transport folks end up you know going to all of these meetings which happens to be the happy accident it just doesn\u0027t mean that the work is well distributed among these things or its I don\u0027t know if it\u0027s if there is as praveen says is that there\u0027s a good process that anybody could follow to figure out exactly where to go for any of this stuff I really think that we should leave condition control where it is which is evolving all the time we should embrace that I don\u0027t think there\u0027s a reason to say that this is the standard that we recommend we will never get anybody to use a standard that we recommend it\u0027s always going to be in the past yeah I can concur very strong with that I\u0027m one of the clues about all of the congestion controls about how well they\u0027re doing is you look at the control frequency I from in years gave a slide some of the one that you Chung showed "
  },
  {
    "startTime": "01:12:55",
    "text": "earlier which you know the pack crack packet loss right to run it some ridiculously high rate well some normal high rate today is point 0 something or another percent but the real issue is for cute for Reno and under those environments that correct loss rate is one loss episode every many tens of minutes it\u0027s like you can\u0027t control a system that way and and the in the controls the the real fundamental issue is the control frequency has to be in the right range and it has to not change as you increase the data rate we understand this now that in order for it to scale right and that means that that the loss rate any I don\u0027t want to I don\u0027t want to go into details the problem with cubic is because of the way the exponential phase goes as the scale gets larger the control frequency doesn\u0027t keep dropping I don\u0027t remember the numbers haven\u0027t worked much with cubic and believe it stays it a few seconds the amount that it overshoots when it overshoots increases and so it slams everybody else on the network and that effect gets worse as the network gets faster and and as a consequence people who are competing with cubic at a big scale networks often get drive by verse losses that are very large that they had nothing to contribute did not contribute to but but I agree with the sentiment that we that congestion control is going to keep changing for a long time and anything that gets the ink dries on is going to be in the rearview mirror I\u0027m Tim Shepherd I\u0027m not sure I really want to disagree with what matter Jonna just said but I one thing is the audience of the RFC\u0027s people who are like diving into the rfcs because they want to figure out how the internet works because maybe they got handed a piece of the project they\u0027re working somewhere and they\u0027re on some project and they\u0027re building something they\u0027re engineers they\u0027re working with engineers and they\u0027re the ones they\u0027re like oh and we\u0027re going to need a TCP implementation and for whatever reasons they can\u0027t just use the Linux kernel or something and just like oh boy we\u0027re in order to show you need right when from scratch and there\u0027s right a TCP a limitation from scratch they probably should put something in there the involving congestion control other than what they can of other than what they might have imagine from reading TF RFC 793 and I I can\u0027t remember what it says about congestion control right and so and so I think it\u0027s probably useful and not such a bad thing for the ITF to have a default pointer that points to some document this says you should do something and I\u0027m not even sure I care which thing that pointer is pointing at at the moment but that\u0027s probably a reasonable thing and it\u0027s probably not so horribly wrong that some person who\u0027s doing from scratch TCP implementation "
  },
  {
    "startTime": "01:15:56",
    "text": "for some reason implements Reno in it and then they run it through a whole bunch of testing and it all seems to work and they deploy it and they start shipping their products then has anything bad happened because they did that I don\u0027t think so I think I mean unless something shows up so maybe you know for that person for somebody in that situation the fact that the IETF is pointing at Reno is probably not such a horrible thing as long as it\u0027s understood that you might have reasons to do something other than Reno if you\u0027re some you know if you\u0027re one of the world\u0027s largest companies and you have lots of data centers maybe you should have staff that think about what the right thing to do is better than doreen oh and that\u0027s fine too is that all ok maybe we\u0027re all everything\u0027s fine give me here you\u0027re up oh yes can you hear me I just make sure mike is working yes ok yeah i synchro actually I want to the presenter to go back one slide I want to say something about the social aspect of this discussion if we can leave this rice come move back one page ok can you go back to 1p yeah the one page number two see what I\u0027m doing I think you prefer you can simply click click on the page number to this right one no that\u0027s three number two yeah this is one score and score historic yeah that\u0027s how many area directors does it take yes I want to bring up one one one issue I think I haven\u0027t heard address I think this is the more like going to a social issue now is not only technology because we\u0027re talking basically the conch you of congestion control and if we look on this this particular slide we see all the competing pick already been deployed by different the platform and behind the each platform there is a company so i think i originally if we back 20 years ago the continued continued discussions that one is is evolving around the environment which everybody has assumed the open Internet everybody\u0027s have a nice behavior so everybody need to be friendly and i think that probably is the PSP for endless that\u0027s the term actually come from that environment but "
  },
  {
    "startTime": "01:18:58",
    "text": "now with our reality now is we are having internet which is essentially the competitive environment there are different players companies they have competing be disinterested their countries can\u0027t you object which is essentially try to make their customer happy and it in such an environment i think i\u0027m talking about currently probably a little matter very easy right now of evil what if we look at a different approach on this whole issue we know there is going to be our pics showing up on our path and they\u0027re going to take a lot of bandwidth available and what if we try to identify which continued control algorithm will be the most robust against around the pigs and that will deliver the user experience under any environment under only a pointer figure environmental any nice environment or image that will work with faith-based Canada he would probably be some CIT who can provide guidance to understand that the the user to have idea their application is always going to deliver some kind of food good service in other words it is the Armory\u0027s just let it run a race and admitted us identify what is the best of the weapon what is the best that they\u0027ve already come out or from that our memories which you will be most robust and hyper consists in the performance thank you I can just add to this point that like we have how many I don\u0027t know how many years of congenital research there like tons of algorithms out there and we see like very limited deployment of new other isms and I think it\u0027s because people just don\u0027t know which one to use and we recently see little bit more deployment because one case is web right you see because the traditional conditional read doesn\u0027t apply here then the other one is you have a big company and you have like a lot of resources and you can just do some experiments but other than that people just really don\u0027t know which one to use if they implement anything on their own if they want to use TCP the only thing they can go to at the moment is New Reno and then at some point they look at their transmissions and their quality of experience metrics whatever that is and they see that it sucks because TC periods not really optimized for today\u0027s network and then they say oh that\u0027s TCP it doesn\u0027t work I have to write any protocol and at the end it\u0027s just they use a different the wrong under control and as long as we don\u0027t give any further guidance then use TCP Reno this will be going on so yeah I don\u0027t know what the guidance is but I would be super happy if you can get if any better guidance than that so I came here to actually it sort of agree with Tim right so I don\u0027t actually think we "
  },
  {
    "startTime": "01:21:59",
    "text": "have a big problem and I don\u0027t take the case that Mayor describes is it\u0027s pretty theoretical but I mean who here writes a new stack and then deploys it on on an amount of systems that may can move the needle anywhere on the global internet almost nobody right that the big stacks are incredibly tightly controlled there\u0027s a lot of engineering going in and I don\u0027t think it\u0027s an arms race either either right because um we have this going back to bittorrent and skype right BitTorrent got hammered because they they basically broke people skype calls right and if if Netflix started interfering with I don\u0027t know what skype or or some Google service right all hell will break loose it so everybody is in everybody\u0027s best interest to avoid that situation right so I think we actually pretty far from her not from an arms race here I do agree that we want to have that what Tim said right we want to have something that people can constrain reading and start learning and you know New Reno is is you know yeah it\u0027s not ideal but it\u0027s also not terrible and it\u0027s sort of easily understandable um the other analogy is going to make right that so transport is very similar to security in terms of IETF right both of those areas about telling other people no no no you can\u0027t send that packet right or not know you can send that indicate so um and and in security right crypto schemes change security protocols changed is you know if you look in TLS right there\u0027s there\u0027s a bunch of cipher suites that we\u0027re deprecating there\u0027s new ones they were putting in based on what we\u0027re understanding it the attackers can do and what the capabilities are and that\u0027s fine right it\u0027s a continuously changing thing and it\u0027s it\u0027s similar in transport right i think our pace of changes is slower but it\u0027s the same thing right we we had no reno and maybe now we have cubic which is better in some cases and worse and others and we\u0027re going to have more specialized ones that that are never going to really run on the internet anyway um and that\u0027s okay right if you have something better we\u0027re going to update the plunger and we\u0027re gonna say look at you know bbr for example like in a couple of years or something it\u0027s okay I don\u0027t think it is a big problem and I don\u0027t think there\u0027s an arms race necessarily happening I want to add to that so but butter bloat is a problem and we know for example that big is worsening it and at this point it\u0027s not an arms race but if you were kind of trying to compete for the same bottleneck link you would have to implement cubed vx6 right so there is a problem so it\u0027s not like there is no problem right so I mean many of these have problems all right Oh make all of the congestion controllers work in it better in some cases and worse than others and you know it\u0027s more about how large the usable envelope is um and that\u0027s okay now can that can be described right and maybe that\u0027s something that that could be an experiment document out of icche I don\u0027t know um but but very few of them have sort of catastrophic failures and and buffer plot is is something that that a "
  },
  {
    "startTime": "01:25:01",
    "text": "congestion controller can\u0027t really do anything about any way or its operating over the path that it\u0027s been given I\u0027m not saying we shouldn\u0027t fix it right we should we definitely shouldn\u0027t up but it I don\u0027t think it\u0027s necessarily a congestion control problem we have five minutes left just listen L this morning I in the ICC RG meeting I try to start a discussion about the round trip time and rates dependency on the marking signal and I think this is a very relevant discussion around this because today it\u0027s not so scalable like Reno the high of the rates the lower the marking signal extremely low becomes extremely though so I think we should open again the discussion about how should a congestion control response and what is best to do have a scalable signal so that we can go forward to it with drop compatibility and marking especially marking compatibility and and and and today there is I think there are possibilities also to make it compatible at a certain rate also for the normal drop compatibility that we have today because okay when it\u0027s it doesn\u0027t work very well we can improve it by cubic improved on Reno I think we can move the scalability to the positive side and make it compatible for normal situations with RINO and cubic but if the round trip times become bigger than the Rays become bigger we can improve and correct actually what\u0027s what\u0027s going wrong so I believe there there is an opportunity and I welcome everybody to join that discussion there\u0027s a lot of mutually conflicting constraints on all the players but I want to point out one that maybe didn\u0027t occur to people if you\u0027re selling advertising for instance and somebody clicks on an ad and the entertainment that they happen to be watching interferes with the ad loading you\u0027ve done a very bad thing to yourself and so that ad comes from somebody else\u0027s server using some unknown technology or i should say the content the click-through goes to somebody else\u0027s server and somebody else\u0027s content and so there\u0027s a very very severe extreme penalty for being too aggressive and although this isn\u0027t apply everywhere it applies in enough places to cause people to be cautious and certainly applies to google to say the least but in fact we do need to have a story especially in the cases where bbr for example does very poorly relative to cubic under some easily demonstrated "
  },
  {
    "startTime": "01:28:01",
    "text": "conditions it\u0027s like what\u0027s the story you tell and part of the story is we\u0027d like cubic to just go away um but we can\u0027t cause that to happen in the transit case we don\u0027t have fairness well guess what we don\u0027t have fairness today it never existed it never will exist it just has different symptoms in different cases we want to avoid starvation we want to avoid other pathologies but like I said earlier I don\u0027t think it it\u0027s it matters less and less than ever did before yeah Colin Cowie has the last word because we\u0027re in the last minute if you want to press a button Spencer hi is this working better this time yes yes good I just wanted to follow up with Lars I think the issue is less an arms race between TCP congestion control and more accidental interactions between TCP and multimedia congestion controls for example I think that\u0027s one area where we don\u0027t have any good understanding of what\u0027s happening thank you yeah very quick um I\u0027ve been trying to understand the internet for a long time longer than I would care to even admit and part of the challenge of I\u0027m trying to understand the internet is by the time I figured it out it has changed it\u0027s not it\u0027s different and I think this you know the whole notion that TCP friendly is sort of difficult to to think about these days because of the fact that the net is just been changing and I\u0027m so I\u0027m actually used to the fact that the net keeps changing by the time I wonder I think I\u0027ve understood it and just like a week and a half ago I watched a video of Jeff Houston a talk he gave I think three or four weeks ago at apricot 2017 and there\u0027s a youtube video it\u0027s hard to find his his talk by searching on YouTube unless you know exactly what to search for and it\u0027s apricot 2017 panel forces shaping the network and that\u0027s on YouTube it\u0027s like a 90 minute video and his talk starts about halfway into that video and the title of his talk is the death of transit and Beyond and what he by transit he means the interconnect like when you if you want to get connected to the Internet you connect with somebody who can offer you transit and he actually told a story that convinced me that in some surprisingly short number of years from now people might not care if they\u0027re if their internet connection actually has transit or not and I probably should stop there and try it instead of trying to explain why that might be true anyway but I found that tie I\u0027ve watched that talk two times and I\u0027ve already recommended it to about a dozen people and I realized that it might be relevant "
  },
  {
    "startTime": "01:31:02",
    "text": "to people who are trying to understand what should we do for congestion control in the future and that if you watch his talk it might reset you because you to stop and wonder what you actually believe anymore about what the internet is so anyway I thought it like his talk because I found it amazing I don\u0027t know if Jeff even in the might be in the room no maybe not anyway saying that was it we have a very very silent mailing list on icc edgy and I keep repeating that people people you know because there\u0027s so much interest and people want to discuss but then they don\u0027t send emails to the list so please oh yeah we\u0027re done basically sorry we\u0027re all right I\u0027m already so thank you very much for the discussion I think this was a very good discussion and I want to go through the minutes and also the java chat and see if there\u0027s anything we could actually write down and con conserve for the future and give some guidance but other than that I don\u0027t see any action points right now I think we just operate as we\u0027re operating right now and thank you for your presentations "
  }
]