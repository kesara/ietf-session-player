[
  {
    "startTime": "00:00:04",
    "text": "they\u0027re so invested they really really are so yeah it\u0027s on its topics how\u0027d it go so okay cool all right yeah I was gonna speak to you about it I\u0027m happy to hold on a little bit the responsibilities are gonna be relatively light and otherwise it may be red tape it\u0027s not going to be a with my travel schedule eyes I don\u0027t like easy because I\u0027m quality that\u0027s true yes yeah and especially right now I know like both last night the night before is like I fell asleep getting ready for bed Robert all right he\u0027s doing better every complication yeah he\u0027s been that good a couple of times like unexpected in fact gorgeous no no a fever that might be an infection they want to keep him there make sure it\u0027s not and yeah then he had something like it\u0027s being back pain they had to go in and be like some touch-up survarium but the good news is apparently and he was feeling all enough to help we have like a regular Friday night my thing in his house no best Friday it wasn\u0027t London it looks like everybody wasn\u0027t over there doing that so those will not hope that she\u0027ll be at least he\u0027s not having pain so fingers crossed I hope this is like what\u0027s good about "
  },
  {
    "startTime": "00:03:30",
    "text": "much like well yeah I mean yeah but I also quick the scene here so right well I can see Nottingham on all night I know yeah leaving I might highlight it for you it\u0027s web time anyway so yeah so I just I\u0027ll even be a feedback then early Chris my 90 responsibilities can\u0027t my w3c responsibilities and currently myself talking to web accessibility standards and and actually building which technology how do we take all of you actually and what a moment JavaScript frameworks and annoying me act but I\u0027m actually taking a bunch loaded I\u0027m the only here w3 o w3 oh well because I\u0027m looking at web or for the Megan or not active in that I\u0027m on the AV so I do my work from that ap um and I need to pay attention five books my ancestral home for my students otherwise it\u0027s gonna be a little bit more um has anyone seen my culture that would be that that would be great I would love that okay yeah I\u0027ve seen on the on the working group roster that it says actual start time I mean they\u0027re doing a nice little survey to see when routes actually start welcome to net BC everyone I\u0027m Natasha co-chair with my I don\u0027t know where my ears I guess we should give him a couple mum has anyone seen him okay he\u0027s here that\u0027s good that\u0027s a good probably a thing I should know right within three hours that\u0027s good that\u0027s good I\u0027m sure he\u0027s on his way right okay I\u0027m agenda sunscreen and where are we okay cool I\u0027ll send around the blue sheets do we have a scribe one known soul who\u0027s offering to otherwise if mo was here I\u0027d quite happily do it myself but until he gets here I can\u0027t yay thank you thank you very much I do we have a jabber scribe thank you "
  },
  {
    "startTime": "00:06:31",
    "text": "very much right okay okay all right I was on my blue sheets awesome right and let\u0027s give him a couple of minutes and then we\u0027ll get kick will kick off and I\u0027ll send you on the agenda on to wanted job ah he appears yeah you\u0027re good yeah I wouldn\u0027t rely on the London Underground oh right okay let me just send this round thingy okay yes welcome everyone to net VC before we kick off or just remind we have a number of people and meet Eko if you are coming to the mic to speak please speak your name clearly and I\u0027m keep your aren\u0027t they keep your questions concise clear speak slowly do as I say not as I do also if you are introducing if you are presenting today please stand in the pink box or favorite pink box please do that this is the note well if you have been here since this morning you\u0027ve probably seen it a few times already if you have a problem with it please let us know as soon as possible and I\u0027m oh is this the new note well great yeah I think I got it wrong last time so okay cool thank you right um administrative tasks I sent around the blue sheets like we have note takers we have Java scrub we yes I mentioned about remote attendees and presenters and also before we begin I\u0027m an outgoing chair unfortunately because my role has changed recently if you are interested in being a chair of the net VC group please let myself all moan no because it\u0027s it\u0027s not a very big talk job actually so if you\u0027re pro like sort of taken your first steps in chairing in IETF please let us know this is a good group to chair I\u0027m very interesting topic cool alright this is our agenda today you should see it on the IETF agenda as "
  },
  {
    "startTime": "00:09:33",
    "text": "well does anybody want to wish to agenda bash or add anything agenda bashing is the same as adding anything great I could see all enthusiasts in excited faces in the room cool milestones and status moe do you want to give an overview of this at all so we are we updated the milestones from last time if you recall we were originally shooting for middle of 17 2017 a lot of these milestones and we are most concerning I guess with the the last three getting an actual a single Kodak candidate spec and reference codebase progressed that\u0027s that\u0027s the most important milestone by far in the working group and so far we still don\u0027t have a single merge code base or candidate so that that\u0027s the milestone that last meeting we agreed to push out to July and I got to be honest that it\u0027s it seems very unlikely and four months Adam just asked you know what\u0027s the likelihood of closing out this milestone in four months I think it\u0027ll be a pretty tall order to get that closed out I\u0027d love to hear some thoughts about what people would like to see is the direction to try to progress that and the the other milestones we\u0027ll finish up the requirements document there\u0027s there\u0027s some few late changes to bring that in sync with other users of this document there\u0027s other industry for that are also using this document and so synchronizing between those two requirements those two sets of requirements is probably important for us but hopefully that synchronization can happen within the next few weeks and we\u0027ll get that hopefully published by April and then the the final one the final milestone for a storage format or container binding hasn\u0027t had any work started on it but that\u0027s really kind of a moot issue if we don\u0027t address the codec spec and reference implementation so we\u0027ll talk a little bit more about those and about what what course the workgroup would like to see to try to resolve that after the presentations and today I\u0027d probably be a short session we we don\u0027t have a dollar update so that was dropped off the agenda if you were looking at the first version so we\u0027ll finish up by five o\u0027clock today at the latest and maybe even a little bit earlier because first two topics let my ID be light great thank you very much London has some great beer so finishing early is good actually do you have another session after this and maybe not hit the weights you ladies you can drink in the session some people do okay all right let\u0027s move on why aren\u0027t you going right "
  },
  {
    "startTime": "00:12:33",
    "text": "the we\u0027ve got Thomas is on the agenda Thomas you\u0027re online aren\u0027t you are you able to present on over-over meet EKKO is he up yeah is there a big red button oh I love this this is the best bit hello it\u0027s my audience and okay oh you\u0027re a little bit distal bit choppy actually I don\u0027t know if it\u0027s your mic try keep talking Thomas this doesn\u0027t work I\u0027ll try one more time that is I\u0027m glad this is not the audio codec working group those chairs should be fired opportunity otherwise we can move on and see if we can get that afterwards and Thomas I don\u0027t know if you can hear me about one other thing you may want to try is just try meeting your video I don\u0027t know if it\u0027s a packet loss issue or a lot baking maybe just try turning off your video see if you can just get audio through you can even hear me here we go all right bye good now yes sorry about that all right um hi I\u0027m Thomas from Ozora I have a very short update on draft ITF nut bc testing this draft has not had any update since the last meeting so this is just really the status current status on next slide yeah not no updates um this is relatively stable at this point um the only thing I thought I\u0027d point out is that we\u0027ve you know I\u0027ve been running you know people continuously using the strap to do testing I\u0027m gonna have an example results on the next slide like we\u0027ve you know for people who you know testing a v1 on this the the current a v1 scores are as follows the only issue has ever come up during using it\u0027s just just for a v1 is that the AV one code base has gotten slower "
  },
  {
    "startTime": "00:15:36",
    "text": "and slower slower so our our a current objective one fast test set is no longer very fast it\u0027s very slow I\u0027m setting this this result actually does not use the very slowest if he wants to be to test with other than that there have been no major issues with it I think last last video we decided to hold this for changes I don\u0027t really have a comment we still neither we have not actually made any changes so I don\u0027t know if we still want to keep on holding it perpetual amendment or not that would be my only question so middle is a question uh as an individual for first of all before the closing the draft topic there\u0027s not a V mana for V MAF on here and I thought there was some discussion a while back about perhaps dropping MSS sim in favor of just assume it was that have a resolved or is that still opened so one thing that potentially changers that Netflix has created a updated version of the math call of I think they call it harmonic weekly math which is this different way of combining two frame scores that I think divergent resolves some issues that the old the math had in regards to the old V math saturating and kind of producing unreasonable numbers at some of the rates we test as the draft has not been updated to you know address that there might be something we\u0027d want to fix before closing it it is the testing infrastructure already have the new vmf computing it doesn\u0027t it is not there\u0027s not okay so it needs to be test infrastructure and draft updates to to get V math correct okay then what about the what about the MSM versus SM issue um the thousand old issue there\u0027s actually I think fast assembly SMS SM the draft has been updated to drop fast SM entirely in favor mssl okay so we\u0027re still looking at both multi scale and and regular s and results together and yes and that\u0027s expected that they produce very different results and measure different things um you know the which is so hot you know clear on which one is better I mean but uh that testing graph basically says if you see a large discrepancies be niche between the two you should you know verify the results with a subjective testing instead to figure out which one is giving you a better answer for your particular tool I think that remains the best advice so after the VMF update what does the group think about closing the document instead of leaving it open if there\u0027s if there\u0027s little chance that it\u0027s gonna change too much more it\u0027s been pretty stable for a while does anyone have an "
  },
  {
    "startTime": "00:18:36",
    "text": "opinion of whether we should close or leave it open my close I mean publish as finished spec Thomas is the editor what\u0027s what\u0027s your view what\u0027s your recommendation um I guess if we\u0027re gonna go fix speed math we can\u0027t close it now so maybe just keep it open till next meeting and then figure it out then um okay there\u0027s pretty much no other unresolved issues as far as I know very very from Mozilla so if I remember correctly the the one thing that we were potentially thinking of adding to this was some tests specifically for rate control and while you know that might Soltan if they\u0027d be nice I don\u0027t think there\u0027s a reason to keep holding the document forever for that nobody\u0027s gonna do it um so I think we can go ahead and publish you could even potentially publish without having a meeting yeah we certainly don\u0027t need to meet just to agree to publish a document so if anyway if I want to feel strongly that we should not publish it I\u0027d say speak up now in the room or speak up on the list because otherwise I think the recommendations from the editor is going to be to go ahead and close this out publish it after correcting the VAF implementation so you Jonathan medic says it\u0027s already have working with last call or do you mean go to organize I don\u0027t think we ever did of working with last call because we didn\u0027t decide to publish it I think we didn\u0027t think we never did in last card because it was not intended to be parked it was intended to keep changing so but yeah we\u0027ll certainly do at least a you know a good long last call because people bother they had their eyes off of it for a while okay so so for the notes will leave the document open now but there\u0027s gonna be another revision right Thomas and we\u0027ll get a revision with the VMF corrected and then we\u0027ll we\u0027ll initiate working group last call after that revision barring any other feedback from the list about keeping it open sounds good to me and then one other comment as an individual not related to the document but just as as as this is a tool to help evaluate results certainly when all the metrics agree it\u0027s pretty it\u0027s a no-brainer that you know the the you "
  },
  {
    "startTime": "00:21:37",
    "text": "know something is uh is consistent and and better but when you say that there was you know divergence of the metrics or not even divergence but you know significant differences between the metrics is it useful to try to categorize that on specific content so we can start getting some intuition about when some of the metrics should be ignored or taken with a grain of salt do we think that it\u0027s possible to get get some insight with that by looking at all the different test clips where they diverge and seeing if there\u0027s a pattern there can we can help the guide further testing say you know this class of content is most likely poor for SM or you know are very good for the VMF is that something worth trying to do um the only that we could do that we would need to actually to compare to subjective results in order to get a good correlation there we have done that but only for I think there\u0027s been a couple do filter tools that people have done the subjective tests on and s put it so we can definitely probably reach a conclusion in regards to the you know types of loop filters how we test it though other tools to be harder because you haven\u0027t done subjective tests some kind of guidance like that meant may be nice for codec developers because you\u0027re just looking at a you know smorgasbord of metrics you know can sometimes be daunting and if they disagree you know it\u0027s a it\u0027s a lot but if we can get some concrete guidance about when they\u0027re likely to disagree and which ones are likely to prevail for certain class of content yeah I\u0027ve also concerned I would\u0027ve actually the current state of the document is with regard to this but you know that considering defining a subset of the metrics you actually defined them all a document but if we get give me a priority to sum or eliminate some that are generally not useful later like book in my picture here at the CB and CR scores are entirely redundant with CIE de and in general the APS our scores are mostly read on it with psnr so they may be useful testing some things but you could drop them off the default one still yeah okay cool thank you very much Thomas we\u0027ll move on to our next agenda item why this isn\u0027t playable it\u0027s here we have our are the xvc video codecs Jonathan you\u0027re here great Oh are we going for four fast Stein are you up for four okay good all right so would you ask for a v1 update and comparisons first are you have a clicker if you want it should work should okay I\u0027m Stan I\u0027m its Cogan so I will be pretty short as "
  },
  {
    "startTime": "00:24:39",
    "text": "well I I don\u0027t have much to update on actually so forth or not much has happened since the last time we met no new tools added the only changes have been some optimizations for this see that the new filter and a few bug fixes and the reason why there hasn\u0027t been much development is that most of the works since last time has been to the a v1 codec which has got more traction but Thor is fairly complete in the sense that it supports I think everything which is in the requirements so what\u0027s missing is really just a few nice to have features like the support for the dollar entry recovery has not yet been completed and also I think the codec like some good tools for screen contents so what we have now is not a complete merge of dollar and four but it\u0027s a fairly simple codec performing pretty similar to h.265 I think but should be less complex and I also have it doesn\u0027t fit the page quite right but okay as for a v1 is frozen in quotation marks I think there\u0027s a quite a lot of updates going on really now small changes but at least no new tools are allowed and as I said there\u0027s a lot going on in the repository but it\u0027s it\u0027s only small changes and still some bug fixes going on last time I gave a comparison I show the history compression history of a v1 over the past few years and also the complexities go through that again with an update I have some numbers showing the improvements of vp9 or actually it\u0027s over what a v1 was like in July 2016 which is roughly the same as vp9 so we have at least gnar why DDR gano 29% and the other metrics are roughly the same so I will show two graphs like "
  },
  {
    "startTime": "00:27:40",
    "text": "I did last time research from are we compressed yet with the objective one fast test set and since the last meeting there\u0027s about five percents BER improvement and the complexity has increased threefold and I only collected numbers for the low delay configuration so this is the compression history starting in July 2016 on the Left been a bit up and down the latest bump in February this year is probably just because I sampled at a bad time but it\u0027s almost 30 percent and that\u0027s that\u0027s what the revised goal was we started out I think like 40 or 50 percent gain but it\u0027s got more realistic and we\u0027ve got 30 percents as for the complex the complexity history it\u0027s you should note that this the y-axis here is logarithmic so it started out at 20 frames like that and now we are well very slow oh you might that\u0027s easier so I I started and runs in our compressed yet almost two weeks ago and they completed this morning and this is using the fast test sequence sets but it has been increasing slightly over the recent months but I think it hasn\u0027t been much a priority to make it practical so I guess the question remains that it is possible to get it practical we\u0027ve done some internal testing and we\u0027re h.265 and a team on overlap there is there is at least the same order of magnitude complexity so so the main difference perhaps between 81 and 86 for complexity wise is that if you throw more CPU at 81 you get more compression but if you throw a lot of CPU that\u0027s h.264 compression will stop you won\u0027t get that wrong that\u0027s much more so that\u0027s everything I had I certainly less than two minutes great thank you any questions slightly provocative "
  },
  {
    "startTime": "00:30:42",
    "text": "question what\u0027s the relationship between Thor and everyone sorry could you repeat that what is the relationship between Thor and a b-1 okay um when the Alliance of open media was formed there was three proposals vv9 for and ala and vp9 sorry vp9 was chosen with some tools from both for and Allah so everyone is basically dp9 with some tools from Thor and Allah and some of the development work that went into everyone has gone back into Thor so you plan to keep Thor as a standalone codec alive as a candidate for here or are we going to robber some baby one at the end I mean that\u0027s everyone knows you\u0027re Thor depends on what this group decides okay gee Thank You Stefan for the important question I think let\u0027s let\u0027s have that question brought up again after the exbc discussion because Nick we need to decide as a group what\u0027s the best way forward they remind everyone the intended output of this group was a single codec and hopefully something that\u0027s practical and implementable by web browsers for WebRTC usage so for real-time interactive two-way encoding and decoding you know as Steiner shows here you know 81 theoretically could probably fit that bill although we don\u0027t have a proposal from from someone representing the Alliance officially to come in here and do that but right now that current practical implementations of 81 or nowhere near that what would you say is the current gap Steiner between making it practical real time speeds and getting compression much much better than h.264 what\u0027s the current gap from it just it was yes to make it bright it\u0027s export it\u0027s pretty fast so but let\u0027s just say absolute speed not relative speed to get a 30 frames per second implementation in a web browser you know at reasonable resolutions that you know 720p you know reasonable not like 4k super agent for everyone currently there aren\u0027t I mean implementations doing that but I know for I guess it\u0027s possible with if you can use a few course on a standard computer so you can graph is frames per minute right for everyone so we\u0027re not even talking about anywhere near where at least order two orders of magnitude off of real time encoding right Luthor you had linear real time encoding yeah it\u0027s certainly possible to get real time encoding winthorpe that we know but for everyone it\u0027s it\u0027s not proven yet that you can do that so it would probably "
  },
  {
    "startTime": "00:33:42",
    "text": "take some time until implementations can be can you actually do that I would also like to mention that I also had a run with I checked that the revision that Jonathan was using as a reference I picked a bad reference apparently in February so I wanted to make sure that it that Jonathan didn\u0027t pick a bad one but and since it\u0027s the second of February which was the reference he he used there\u0027s a 2% game so so the revision he picked seems to be okay yes Jonathan Europe exbc video codec right let me move over the slides bridge is this working fantastic all right here we go okay thank you very much so I will be presenting about the ecstasy video codec my name is Jonathan and I work for Davidian so this is a new proposal to this group and and this is the first time we\u0027re presenting it and I kind of hope it\u0027s not too late to bring in new stuff but what I want to do to emphasize here is that it\u0027s not just a new piece of technology that we\u0027re bringing in it\u0027s also representing a slightly different mindset for for developing and I\u0027ll go into that in the beginning of the presentation so what I will representing today is just briefly about what what is a crisp xvc about the design philosophy very briefly about the technology and then I\u0027ll go into more more details about something that the core restriction flags and how that helps us to do version handling and I\u0027ll describe a bit about that and also a few words about how X you see could be used in real time applications and present some results and then round off with with discussing why we think xvc would be a good candidate for the net we see working group so what is X you see it\u0027s a next-generation video codec which we have developed which was released in his first version last year which we claim that it it brings better compression performance than all other codecs it\u0027s developed by my company and we have made the the source code code publicly available and we have a commercial license for it which covers both the software and the patterns of the codec and we\u0027re also exploring the possibility "
  },
  {
    "startTime": "00:36:42",
    "text": "of defining a royalty-free baseline profile of exbc and we have a well defined framework for how to handle evolution of the codec how to bring in new versions of the codec and I\u0027ll go into more details of that we also have focused very much on making sure that our implementations especially the decoder side is efficient and is is capable of being quickly brought to to practical applications to use it commercially we have a few demos of that and there\u0027s a demo on our webpage where we\u0027re actually running the decoder in JavaScript so that you see that the D current process itself is very lightweight so about the design philosophy for for the XFC codec what when we have developed this codec we have made our focus has been on using the best available technology and not not really caring about the the licensing of patent situation around different tools that we put in into the correct we we only remove stuff or designer on stuff if we really really need to if someone if if there is a request to not use a specific coding tool so that means we can we can use the the very best compression technologies and and get the the best performance and only if if it\u0027s if we run into problems with that we will step down and use some alternative or or rather yeah do some do some design around so here\u0027s a very simple extremely simplified scale that if you have some if you want something which is really high performance you also have to be prepared that there\u0027s a high risk that there are patterns surrounding it or patents on that technology while in the other end of the scale if you want to be really sure that you are using something that is not patent encumbered then you have to be prepared that it\u0027s probably not as good performance so - so that\u0027s the simplified view of the scale and we\u0027d like to see I mean xvc is really on this end of the scale and if you want to be in that end you probably should use something which for which all the patents have expired already so like and pick two video or something and then you can you can speculate or what other codex would end up in this scale yeah depending on their performance and the risk wouldn\u0027t come sidebar and "
  },
  {
    "startTime": "00:39:42",
    "text": "burden when it comes I PR briefly about the technology we have in the first version that we released we isolated 62 different coding tools much of the the common pieces that you see in in modern video codecs is very similar in ecstasy and in in other codecs compared to to achieve EC for example we have a bit more advanced splitting of blocks for prediction and for transform and non square transforms and so on and in the the current version that we are working right now which you haven\u0027t released yet we add quite a bit more of those advanced features so to say that cross component prediction or affine motion prediction or local illumination compensation result and I talked about coding tools like separate pieces of technology separate processing steps that you can isolate from each other and that\u0027s actually also how we have developed and implemented it in the reference software we have this concept of restriction flags so each of these different tools can be turned off via control information in the bit stream itself so we isolate the different pieces of the codec so that you can decide which pieces you want to use and if you if you run into problems with one piece of technology it can be disabled disabled in the bit stream and and that part of the code will no longer be be utilized and so there has to be a fallback method for each of these different tools and each of these different restriction sites and here is a simple example for inter planar prediction if that tool is turned off yeah then we go into this if course and we\u0027ll just replace that prediction mode with DC prediction is that so and we have this for every of these 76 tools we have a fallback solution this is very simple but some of them are a bit more advanced because they relate to signalling and you have to do several modifications in the code to make them work well but this is one of the simplest ones on average each of these different tools bring less than 1% compression performance so turning a few of them off actually doesn\u0027t have to hurt the performance very much so now from the floor mic virtually a question about the selection of the tools that control by the flag did you design those because of what you suspected it may be areas that may be patented or purely "
  },
  {
    "startTime": "00:42:44",
    "text": "based on technology and based on what you thought was the the right breakdown of the components of the codec question what we\u0027ve tried to do is isolate a small pieces as possible which which still practical to implement so so we want to have as many accesses you can have while still not causing too much trouble in the course of say so that\u0027s the reasoning we we want to make every piece so that you can turn off every piece of the code and ideally a small portion as possible is that answering your question yeah that makes sense so it\u0027s not based on IPR or technology it\u0027s basically whatever is practical to separate you separate just to have the most modular stack available yeah so some of these flags might not make very much sense we have one flag which turns off by prediction altogether and in that that is something which are performance and maybe it doesn\u0027t it\u0027s not it\u0027s not a tool which is actually at risk from an eyebrow perspective because that\u0027s very old technology so but but most of them are for small pieces that you relieve you want to be able to turn them off so what I wanted to talk about also is how we use these restriction flags to enable a evolution of the codec and and versioning of the codec so Alba trims all xvc bits dreams contain a indication of an X B C major version and an X ay C minor version where the major version represents new pieces of technology if you increase the the major version that means you have added new tools to the codec while the minor version represent reduction in number tools when you when you say this tool should no longer be available in the codec so so I will I will disable this via the restriction flag and you make the codec smaller so we have separated these two which which makes it possible to control how the the codec evolves over time and it\u0027s actually and it\u0027s the reference software which defines which which versions are valid at any point in time so making this there\u0027s two aspects that we need to take into account it\u0027s the bitstream upgrade if you have legacy bit streams of an X PC version you might at some point if if they are using a tool that you are no longer allowed to use then you have to rewrite them or re-encode them and that\u0027s something if you if you are considering a large service or ott applications you might want to to "
  },
  {
    "startTime": "00:45:44",
    "text": "re-encode your your your bit streams or depending on what the tool is that you have to turn off it might be just rewriting the syntax or if it\u0027s a more central piece you might actually have to do a full of rien coding but on the other hand if you do that you might be able to to get better performance because the codec has evolved since you first created the bitstream so you might be able to compress it more efficiently and then the other side of it is the client upgrade so if a new major version is released with new tools index we see correct then the the clients needs to be updated to support that new version so during a period of time there will be support for two different versions two to two different versions of decoders will be allowed but at some point in time you would want on all the cores to be updated to support the new version and I have I will demonstrate how this is working in practice on this timeline so if we have so so this is one of the reasons why we wanted want the codec to evolve over time is because of the patent situation and because we have seen the how difficult it can be for patent encumbered codecs to become the applied due to the two patent trolls and and unclear licensing situations and so on and so if there is a report of a problem related to a piece of technology and we determined that yeah this this tool can no longer be used in the codec then there is a new minor version released so so quickly after it has been determined that yeah this tool cannot be related used because there is a patent which is in conflict with the license of X you see so then then after the we have made that minor version released we can start exploring what would be a better approach to do something similar or how can you replace this technology with something else so that at some point in time we have to say ok you can no longer be using the the 1.0 definition of the XVI codec because it included technology which will not be which is not possible to license basically but later on you will have a new version with new tools and in that net that version you know you can have new technology which might be related to this specific problem and also additions of other types which makes the codec again better than it was "
  },
  {
    "startTime": "00:48:45",
    "text": "in ecstasy the 1.0 and after still some more time you would say that at this point in time we expect all the coders to support this new version and what that means in terms of from a bits bitstream perspective i\u0027ve tried to draw these different colors to indicate that during which period of time a specific version of a bit stream will be valid you see that the diversion one bit stream is valid until the point where support for 41.0 is removed from the reference decoder but there is an overlap so if you want to do Rhian coding of your bit stream or rewriting over bit streams you can do that during this period and this stream from version 1.1 will be valid over that period and also afterwards when when the two dots here has been released and you see that there is no end to when those bit streams are valid unless there is a problem identified in 1.1 those bit streams will be okay forever kind of when it comes to two I\u0027m using the the version 2.0 bit streams starts to be valid as soon as the new major version of exbc is released looking at it from from a perspective of encoders and decoders you will have overlapping pair which the encourage needs to be upgraded so but I\u0027ve done it with it within quotation mark because this upgrade is a very simple thing this is just disabling something which is which a tool which should no longer be used so this is typically just a configuration thing that you would do on your Ankara to make sure that this specific feature should not be used anymore but then over here you have the upgrade of the decoder and that requires a new installation of a new version of the decoder so that\u0027s pushing out a new new software to the clients but there is all for both of these there\u0027s an overlapping period of time where you can do perform this this action and you can see that you might introduce more more versions of the codec this is kind of the general case where you don\u0027t make the codec smaller you just make it bigger and add new features then you can keep your old bit streams will be a subset of this newer versions so that\u0027s about the version handling I have one slide about XP see in the WebRTC scenario where this is not "
  },
  {
    "startTime": "00:51:49",
    "text": "something we have been working specifically with but but having these restriction flags and this isolated pieces of technology opens up for an opportunity to to kind of determine that the tool set to use based on the capabilities or or the the scenario that you want to use them for so you can you can open up for a negotiation of the tool set for each session to be different and you can also in that in that scenario take into account the the patent situation the aircar situation if you want to do a royalty-free session you can explore what\u0027s the current situation around these tools and you can even do it based on your locality where you are in the world to make to use different tool sets for different sessions when it comes to it comes to results we have been running xvc with the the test conditions and the test sequences from from the testing draft and use this or recompressed yet framework to generate results and we think the results looked really promising we have quite significant gains relative to hmmh EVC but also quite large gains compared to a v1 and i included one figure here as an example that you\u0027ll see X we see being better than the other codecs and then my final slide so we bring X we see here as a candidate for full net we see we believe that it\u0027s well positioned to meet the objectives of this working group that in in sense that it\u0027s a competitive core codec in terms of performance it\u0027s better than everything else we have seen it\u0027s it can be used in in web applications we have demonstrated that the decoder is really lightweight what we haven\u0027t worked with is developing fast speed modes for fast encoding so that\u0027s something that needs to be evaluated more I would say but and and then for the third third bullet here we believe you have a good framework for the the licensing and IPR situation which which would meet this objective and that\u0027s it thank you great thank you very much for that and I will put together presentation that\u0027s great does anybody have any questions "
  },
  {
    "startTime": "00:54:50",
    "text": "a new candidate codec there must be questions Tim you\u0027re not curious - everybody knows look can you go back to your slide on the risk scale slide six yeah anywhere in there is fine so so your contention is that that picking tools that are high performance versus speaking tools that are old and low performance is is one way to mitigate risk and I agree with you that\u0027s one way to mitigate risk right so there are other ways to mitigate risk such as as for example paying a bunch of money to lawyers and doing a bunch of work or getting a bunch of other companies who hold IPR to agree to license it under you know royalty-free terms right so just to be clear here that that you have not done either of those other two things you have solely focused your your strategy on on mitigating risk in the future by saying you can shut things off yes okay so it\u0027s actually worse these guys plan to make some money on it themselves at some point for the high operation points nothing wrong with that but that\u0027s true right yes right so so that is another point that get to a little bit um but I think at least from from our perspective like like I think that would be an OK strategy if I lived in a world where past damages did not exist in the the problem is is that I ship a few hundred million copies of Fox every six weeks and if I\u0027ve been deploying your codec for a few months that\u0027s an awful lot of past images that I\u0027ve racked up if there\u0027s a problem that that we just didn\u0027t know about and hadn\u0027t shut off yet um so so I think in order to actually be widely deployables you need to do some of that other risk mitigation stuff up front yeah well I think one of the reasons why we bring bring this proposal here is because I think a group like this is a good place to to actually explore those those the patent situation and and send out requests for for patent declarations to "
  },
  {
    "startTime": "00:57:53",
    "text": "determine what what is the rule that we the real patent situation well disc this codec and and I agree that that may turn out some problems um but but if I can translate that for you it sounds like what you\u0027re asking is for us to do all the work to guarantee that this is royalty-free um which which you know maybe if this is compelling enough then then that might be worth investing in but but you know that\u0027s that\u0027s a decision I think that the group has to make um I I think the other question is is is you know you\u0027ve talked about the the possibility of there being a royalty-free baseline so have have you actually filed an IPR declaration and and agreed to license any of your technology on a royalty-free basis not sure I understand to this group I don\u0027t know to what extent you\u0027re familiar with a bad policy here right now you\u0027re under no obligation to do anything like this at this point yeah you haven\u0027t shown a draft or anything I mean you\u0027re really early in the process so Tim don\u0027t push it too hard for a newcomer I mean the answer is yes that\u0027d be easy yeah yeah but yeah so I mean but the the the question is is is you know if there are things that that you don\u0027t want to contribute on a royalty-free basis then then we would have to take those out of the codec at least in terms of what I think the working group would be willing to standardize um and essentially like like the only thing I think that that we are interested under the current charter would be that royalty-free baseline um like if you want to hold out those other things like okay but then you have to go reevaluate what\u0027s the performance going to be if we take all of those things out um and I think like having some kind of notion of how much that would affect the performance arm would be important for making decision on on how useful a candidate this is does that make sense yeah I\u0027m just I\u0027m not completely familiar with I mean I haven\u0027t been active in this group but but my understanding was that the focus of this group is not entirely on royalty-free doesn\u0027t have to be only royalty-free which is considered in this group is that so yeah sort of clarify the the then the Charter doesn\u0027t rule out anything that could be royalty bearing but it would have to show good evidence that there is good justification for including that technology so you know if the compression gain was very significant and the royalties were very reasonable maybe that would merit consideration by the group but if the "
  },
  {
    "startTime": "01:00:53",
    "text": "compression gain is minimal and the royalties are in line with you know say for example the situation with other codecs today other leading-edge codecs today then the you know point in the group considering that so we don\u0027t only thing worse than HEV C or even similar to HEV C risk wise if you actually go read the Charter like there there is a section towards the end where it where it says like we should follow the preference specified in bc p 79 to prefer royalty-free technologies um we can\u0027t actually write in a charter that things must be royalty-free because that\u0027s not a determination that the IT can make like essentially that\u0027s only a determination that can be made in a courtroom under under the current legal system in the u.s. at least and probably many other jurisdictions as well right um so so but different members of this group probably have different preferences over what would be an acceptable level of Licensing um you know speaking of someone who distributes copies for free in an open-source manner which means we want other people to be able to distribute copies of our software for free I think the only only acceptable licensing to us would be none but but as Moe mention well no money yes let me rephrase that more accurately but yeah as most of you know other people may have different thresholds over what they they view as acceptable based on their business models and and that\u0027s a debate the working group can have but but there\u0027s some pretty strong preferences already expressed in the Charter well thank you Thomas has been patient on the queue on your line so okay cool Thomas you\u0027re up III this question was actually very happy that you\u0027re with your result slides I think they did a good job in following the testing draft procedure I was actually curious if he had any any comments regarding the testing draft and how he felt that worked for xvc and whether and I was also curious given your very good results if he had any ideas what in xvc gave you the really good results like did you find did you have some special coding tools that no one else did or was it just the fact I decoded this morden in some of its parts yes there was a lot of questions at once yeah I mean when we generated our results we used the the Oracle impressed yet framework which was a very very nice framework to work with we have done testing without other pieces or other coding settings or other test sequences "
  },
  {
    "startTime": "01:03:55",
    "text": "we see some some some quite divergence in some specific sequences stand out to you you will have I mean I report the average numbers for the categories here which were xvc is clearly better than the Navy one for everything except that the screen content but there are individual sequences where everyone could be better than X BC I think there are at least a couple of them and but we also see see a bit of difference depending on which with metric metric we are looking at we try to align as much as possible but I don\u0027t think I don\u0027t have any specific comment on the test conditions it was your first question yeah when it comes to to what is actually bringing the the performance gains I I don\u0027t have enough insight into 281 to to comment on that they compared to - hm or HTV see it\u0027s much easier to it I had this list many of all of these features I would say sorry this list these are features which are not present in HEV C and we had their stuff in in in version 1 as well when it comes to these non square partitioning and non square transforms which is not where there is anything corresponding in HIV C so that\u0027s where we see the the performance benefit okay I\u0027d wonder the question will arise that\u0027s more in the line I is if you do your testing with version 2.0 1.0 so the numbers reported are with all these tools included okay and is that the career the current code based on github version 2.0 or is version one so on our github repository we have the master branch with which represent version 1 and then we have the dev branch which is targeting version 2 but we haven\u0027t released version 2 yet so so that that\u0027s still working progress I cannot work into it in this slide but it\u0027s not really it we haven\u0027t determined exactly which tools will be in there ok hi Ben Schwartz I just wonder I wanted to ask how you imagine mapping what seems like it might be a really highly dynamic codec the changes fairly frequently over time into IETF RFC S "
  },
  {
    "startTime": "01:06:56",
    "text": "which generally don\u0027t change very frequently that\u0027s a very good question I I would think that probably ITF might be more agile than than other standardization organisations in terms of revising and updating specifications but I don\u0027t know how I mean if it becomes too often that you have to revise it is it wouldn\u0027t be good but we don\u0027t envision that we I mean we think that it will be only as rarely as possible that you would have to remove something and making these new additions would also be I mean with a reasonable time frame so that you can get this this update good comfort from the choice yeah I don\u0027t think there would necessarily be a technical problem I mean we have plenty of working groups that have lived for you know probably too long so I don\u0027t see a technical problem with having a rolling rolling technology coming in on a you know monthly or sitting monthly basis the your company\u0027s website lists 60 days as the notification period from a change and also reserves change control to the corporation so you know the 60 days is how quickly we have said that we will guarantee that we can turn off something if there is a detection of the technology that we cannot use we\u0027ll say that we make sure it\u0027s not used within 60 days from our perspective : Jenks one of the most of interesting things about this idea to me is the ability to turn on and off tools and that you got that to work with a fairly you know fairly modern and obvious that obviously complex is correct and I I do wonder if that\u0027s one of the the big takeaways this working group should should take on that idea one of the reasons I like that type of idea is right now I\u0027m involved with lawyers from various companies and arguments about whether a patent is valid or not or whether patent is obvious or not it\u0027s a very different thing some people think oh yeah that was obvious at that time and many other people don\u0027t and this would allow different companies to actually change different settings on whether they think about certain things like that in certain cases so there\u0027s one comment another comment I think that you know patent lawsuits over codecs happen for a lot of different reasons often blocking markets by large players and things like that but often when they are purely to maximize monetary you know somebody\u0027s trying to make a bunch of money for their IPR when it\u0027s just a money case not a sort of strategy case of hand they typically the optimal strategy seems to be for the most part to wait until you know within a few years of the expiry of patent before any of the lawsuits start so often the discovery "
  },
  {
    "startTime": "01:09:57",
    "text": "that there\u0027s a problem in something may be many many years after it bus was what was being used that\u0027s it thanks hi Wallace Chung I\u0027m curious about this process here imagine well how the experience has been with practice in that process because last time I ran into a lawyer or not lawyer but someone who talked about patents his statement was more or less I have happens in your technology full-stop and I asked oh okay which technology with what what is patented which bit is bad that\u0027s your problem so have you actually had experience with resolving a matter like this in 60 days so far I mean we release the first reversion in September last year and we haven\u0027t had any report of any using any technology that would be patented by person thank you I have had some experience in this field maintain situation is not quite as bad as it used to be say 10 years ago in many countries Stephon banger in many countries let\u0027s say having complaints have to be nowadays fairly specific right so to speak more broadly I think the addition of a tool that post standard setting reactively allows you to selectively throw away in the standards domain that one nasty tool which that stupid troll across the street is asserting against the world that\u0027s a useful tool that\u0027s I think what what Holland was going after I went after that to an MPEG about two years ago or so right it\u0027s it\u0027s it\u0027s a useful tool whether it is whether it is technically doable is another question I mean video codec some people will tell you it\u0027s quite hard because of the symmetry effect between those various tools that are sitting there it\u0027s not that easy to compartmentalize however if that such it will variable I think the standards groups would have a tool that would allow them to react in a somewhat backward compatible ways though to to the threat of certain trials and that hopefully would discourage those thoughts so that\u0027s that\u0027s the idea behind behind proposed like this it\u0027s not a bad idea whether it works in practice I don\u0027t know I let I like the idea that\u0027s why I was curious about whether it has been practiced is also though it\u0027s not so easy to overdo that "
  },
  {
    "startTime": "01:12:58",
    "text": "for those for those who were working on video codecs may remember h.263 and it\u0027s well they were they were running out of letters for the various annexes for the optional tools at the end right so and the the the end result was that you know after the fact profiling or not the end result was that that video conferencing system spoke baseline are pretty close to baseline as long as when there was when interoperability was a question at all yeah when when when cross vendor and interoperability was in play because within one vendor everyone that\u0027s what they were one do it does but other than that they were just falling back to baseline for this optional to it\u0027s that you add on top of a patient and this is a dial down mechanism here down mechanism maybe you have a better choice you have it\u0027s mandatory to implement everything except when there\u0027s kind of an industry consensus to switch something off because there\u0027s this one troll which too much money who makes a nuisance out out of himself so yeah there\u0027s there\u0027s an option here it may work whether we can get assuming that this group would want something like that assuming we are not rubber slimming everyone which doesn\u0027t have that and which is frozen bits to him and whatever right then the question is can we get the granularity right on one hand narrow enough to not to disturb the performance too badly of the whole codec on the other head you know brought no broad enough not to disturb the performance and narrow enough that we can indeed rule out when troll without switching of odd functionality yeah so that\u0027s that\u0027s the hard part and that\u0027s the part which 323 which 263 didn\u0027t get right and Schwartz I I thought Collin Jennings was going to say that so I didn\u0027t bother but so Cohen Jennings has been speaking in other working groups recently about new architectures for for video and real-time media and has has reinvigorated I think a discussion about using using runtime pluggable software codecs in a web context or in context where where it might be possible to essentially download the codecs along with the video and if if xvc isn\u0027t a good fit for net bc which is is standardizing a codec it might be still useful for you to participate in in other fora here where that are considering ways to make standardization of codecs less necessary in order to enable the kind of flexibility that you are offering somebody say living standards "
  },
  {
    "startTime": "01:15:58",
    "text": "cool I\u0027d say I guess the best thing to do next is to take some of the comments which are in the room go through the minutes maybe there\u0027s a draft so if people want to continue the discussion you can reference the draft read the draft and reference it and then go and develop from there yeah so talking from a floor mic again I had a few more questions so first about you mentioned something in the draft that you\u0027re in continuous dialogue with patent holders do you have a rough feel for whether or not you\u0027re going to have enough of those on committing on to the royalty-free baseline terms that the performance would be you know at least door like could you have a feel for that yet no well I can what I could say is it\u0027s kind of a chicken-and-egg problem that as long as the feedback that we get is that as long as no one is really using it or there\u0027s no commitment to use it there\u0027s this not so much interest in committing to to make the claims or to define that the licensing terms for it so I mean from the responses that we get so far is many many organizations want to lay low for some time to see where this is going where is what what\u0027s the performance who is going to use it and these kind of questions so I I don\u0027t have much info from a chair point of view I I think we probably would not make a decision to hum on something that we thought would not meet most of the Charter objectives so unless we had some kind of you know rough view of what the likelihood of a performance RF baseline would be then it would be premature to try to adopt something that we that we thought was you know very unlikely to at least meet Thor once you had the RF picture of it figure it out so is it possible that you may be able to go off and and and make some headway on that question before the next meeting or or just in general what timeframe do you think is it months years out before you figure out what the RF baseline would look like or something you can do in a pretty short term month sir by the next meeting but the question is would it really be royalty-free that I mean that\u0027s the the tricky question and I mean we have made some some analysis or of our own to see what would make a good tool set and what would be believed is it\u0027s a good central piece of technology but but to to really have something concrete to say this this is this is round two free I this is that\u0027s very difficult for my company to to to bring I mean okay that\u0027s a fair answer and then the the list of the technologies "
  },
  {
    "startTime": "01:18:58",
    "text": "and tools would you say that currently closely resembles JV et tool sets or hm+ something totally different no it\u0027s a lot of the tools were assemble achieve easy tools and and several of the of these tools which are in addition to HIV see resemble the jvt tools okay so there are some some some additional tools or alternative things but but it\u0027s basic technology is very much on that track okay okay and then the final things merged a small point but maybe double check with Steiner about the blip that happened in a v1 someone was trying to make it faster and by making it faster the compression you know hit you know a horrible cliff so there was a blip that seemed to be about right about the time when you were doing your evaluation so maybe double check on a current on the current get hash maybe from this week okay we took the best results that were available when we when we put together our draft so I think those were not the the blip that sign up I think the dates might have been close enough that you might have gotten hit by the bug yeah I think if you heard Steiner in his presentation he actually went back and checked the commits that they used oh you did it\u0027s it\u0027s two percent worse than the current two head of tip but but not not 2015 that huge spike okay good now sorry I missed that Steiner thanks thanks for verifying that okay so that you know okay Brent\u0027s point I think this is interesting for different reasons not just for the core technology part of it even if the core technology may prove to be you know patent encumbered and and by people that are unwilling to license it worldly free some some ideas of this may be useful for whatever candidate in MVC pushes forward and especially the cadence of regular releases may align very well with our Charter to put it in WebRTC because the browser\u0027s all have this dynamic cadence you know roughly six weeks to you know twelve weeks if that if that could be a cadence that you know publishes on and there\u0027s a reasonable overlap of versions that can be maintained in in a browser you know not infinite number of versions and not one version back but at least you know you know one year\u0027s worth of versions if that can be well contained that may be a good fit for the update models that the browser\u0027s use and that would be very interesting to have a dynamic codec that revs that fast and is constantly improving I think that\u0027d be a pretty pretty cool thing to to push forward but the RF question is the big elephant right now is what performance can you get when you get this down to RF great "
  },
  {
    "startTime": "01:21:58",
    "text": "okay any other questions okay thank you so much longer that\u0027s great thank you okay that is the end of our agenda for today does anyone else have anything else they would like to raise brilliant okay if you have not signed the blue sheets please come up to the front to sign the blue sheets if you\u0027re interested in becoming a chair of this group please let the current chairs know myself and Moe or Adam or ad I believe you are free to go so thank you everyone oh wait wait wait wait Thank You Jonathan for being a JavaScript and thank you Matt Miller for scribing for us any way that you want to get the notes to us would be far but thanks guys now you can go we\u0027ve ready primed the etherpad with some pre notes just mm they\u0027re asking the chairs to sign it I\u0027ve never signed the blue sheet Archer supposed to list them teachers assign this cuz I think so I just pronounced up here okay do you mind taking it over to secure them up um I get cool all right let\u0027s do this mysterious everybody got the blue sheets right that was how was it nobody else missing you "
  }
]