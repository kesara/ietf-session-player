[
  {
    "startTime": "00:00:15",
    "text": "i'm done okay okay uh"
  },
  {
    "startTime": "00:02:17",
    "text": "wow thank you every day thank you and so uh this one is"
  },
  {
    "startTime": "00:04:07",
    "text": "okay [Music] all right folks if you can start taking your seats we're going to get started soon [Music] uh [Music] all right let's get started [Music] good afternoon everyone in vienna good morning good evening good middle of the middle of the night everyone at home and welcome to web transport this is our first in-person meeting since we formed the working group in march of 2020 because we had an in-person buff and then we were virtual so now you know how tall some of you are uh and we're looking forward to uh seeing more folks at hopefully the coming itfs we still have a lot of folk remote oh yeah and so my name is david kanazi i'm one of our chairs and our other chair bernard is here remote hello hi bernard um all right the audio seems to be working well um so as a quick reminder this session is being recorded um the usual tips for remote meetings haven't really changed but if things have changed for the in-person folks since we were last in person"
  },
  {
    "startTime": "00:06:02",
    "text": "in particular if you want to join the mic line you need to do it on the online version otherwise you'll just be standing at the mic stretching your legs and you can use either the onsite or the remote version they both work for that [Music] and yeah if you are in the room like please join it as well because that's the only way we populate the blue sheets which is important and then the buttons for meat echo are roughly the same you press the hand to join the cue and then you can unmute once it's your turn please stay muted until that's the case all right here are some links the note well um it's thursday so most of you have probably seen it but some folks might be joining for their first session today so everything we do itf is covered by the notewell and if you haven't read it you really should do that it said that you had to read it to register so hopefully you at least glanced at it in particular anything you say has implications with regards to copyright and patents and also we have the itf has a code of conduct that the chairs will be enforcing it pretty much boils down to be nice which is what everyone has been doing in this working group so far so we're really happy for that let's keep it that way all right um some more links we would like volunteers as usual for a jabra scribe and a note taker do we have volunteers please thank you lucas for jabberscribing who would like to be our notetaker please i'm good [Music] i hope all right thanks bernard will help with"
  },
  {
    "startTime": "00:08:00",
    "text": "that can someone from the room martin could you perhaps help a little bit with bernard since you're close to the mic um do you mind for note taking notes awesome thanks and we're yeah you can collaborate both of you on the usual thing that's accessible from the itf agenda page awesome thanks right our agenda today is me talking for a while that's done then we're gonna have a w3c update from will we are then going to talk about web transport of http 3 and then we'll transfer http 2 and wrap up would anyone like to bash the agenda consider the agenda bashed um all right well let me handle you slide control there you go go ahead okay thank you just an audio check before i launch into the ether and nobody actually hears me we yep we can see and hear you okay thank you good morning from california i'm will law from akamai i'm representing the w3c group looking at developing the api for web transport for browsers i represent yaniva brewery my co-chair so just a quick update for you three slides uh progress since november the 9th last time we reported the status of the spec is now a working drop the latest version is march 11 2022 and there's a link in the slides here you can go to it as chairs we've set a somewhat optimistic timetable for the year um it's outlined here with candidate recommendation coming up awfully quickly um the main blocker here is by july 31st we had the idea for a proposed recommendation which requires at least two independent implementations chrome have been uh in production since m97 however mozilla"
  },
  {
    "startTime": "00:10:00",
    "text": "have not yet signaled a date so july 31st is somewhat challenged at this point and i would encourage uh alternate browser vendors to chrome to please step up here we have a charter valid through september 30th we will extend that charter if we're not able to get the second implementation then by that rate milestones we've adjusted our milestones uh to match the release process so we have a new milestone up now called candidate recommendation it obviously contains the issues that need to be resolved before we can proceed to that stage minimum viable ship the prior one has a few issues remaining let me figure out how to advance the slides so there should be over the slides at the bottom um controls for you where you can just hit next slide i'll just set that one okay thank you so just a real summary of the decisions i'm not going to go into details of all of these some of the resolution establishing a session clients should not be providing credentials and i've got issue links there you can go through and read the details blocking ports on on fetch's bad ports list adding uh smooth rtrt variation to web transport stats with the definitions coming from elsewhere uh packets lost web transport stats uh extra requirements on server cert hashes uh on web transport get stats we've added expired outgoing dropped incoming datagrams lost and decided that stream stats will be an individual getstats method on each stream instance and after some long debate mostly resolved but not yet merged we have some decisions around reliability and fallback um and also some"
  },
  {
    "startTime": "00:12:02",
    "text": "good discussions about the actual state of the constructor argument you know require unreliable versus request reliable for example content security policy has also been a long issue um and there was recently a decision to to break the compatibility with the early version of chrome in the hope that it's better to do it now as there's not that many users and that it would be more stable going forward datagram priority over streams there's been a change the priority algorithm had some some guidance uh and it's replaced with normative language that the user agent should make sure that datagrams get out quickly the last one was very sort of absolute datagrams always go out first and there were some potential issues there current issues under debate uh bike shedding on names for stream stats that should be resolvable connection pooling there's new issues raised around whether it's default on or off and how to post that in the constructor and then lastly priorities between streams is this necessary and also this is a question for our etf is their intent to define a method of stream prioritization that might then filter down to the browser level uh that we could take advantage of that concludes the summary from w3c many of the participants of w3c are on this call so if there any questions uh we can take them now or else back to david thanks will does anyone have any questions for will if so please join the mic line [Music] all right if you have something martin oh wait there's a different martin in the queue i'm confused but okay mt go ahead"
  },
  {
    "startTime": "00:14:03",
    "text": "so um world's asking about priority here which i think is probably one of the more interesting questions that we could uh we could discuss here there's potentially something we could do uh we could take the http priorities thing and just sort of say well there's eight levels and off we go or we could take a leaf out of the webrtc book i think they have four uh does anyone want a bike shed on this one i think it's really coming down to that thanks martin martin this joke never gets old um when you say that two interoperable implementations are needed does this mean that all the features that you just presented need to be implemented in those for example all the studs and the certificate hash and so on uh no is the answer and there's they don't have to be precisely identical in feature set i think there's an intent that they should deliver the majority of the capability but you know chrome is certainly shipped not with these features in and that's acceptable they're getting added post ship so i think the requirement is that we want basic functionality be able to connect be able to establish streams send datagrams that would be that would be evidence of of an implementation i would think the stats could certainly be acceptable if they were not identical thank you alan and for folks once you've put yourself in the virtual line feel free to stand at the mic that's okay allen frindell um with respect to priority i think i i like the http scheme a lot in terms"
  },
  {
    "startTime": "00:16:01",
    "text": "of the eight lanes and i think in terms of different priority schemes we've looked at at least in http over time that this has got the right balance of um flexibility power simplicity uh and creating another one i'm not sure would be really useful so finding a way to adapt that as like a model there's two pieces one would be if if webtransport adopted it would we add signaling to the wire protocol so that the browser side can set the priority on streams that the server is sending and another one is just on the api side can the browser set priorities on the streams that it is sending which is strictly does not require wire format changes it's just between the api and the sending implementation thanks um thanks just double checking i'm not seeing any typing happening in the doc bernard are you taking minutes [Music] a notes file it's been acting a little squirrelly but i do see someone else taking notes there so at least one other person has figured it out well i just added what mt had in the in there is are we all thinking that someone else is taking minutes well i am taking minutes as i said in the notes file notes dash itf webtrans it's been acting weird though okay yeah i'm not seeing updates well as long as you have them we're good all right thanks uh victor you're next i was mostly under the impression that the priorities at least outside of those communicated as the wire by gtp are typically considered the api concern"
  },
  {
    "startTime": "00:18:01",
    "text": "so i personally unless we intend to provide a mechanism for end-to-end signaling i don't think there's there is much in the working group to be done uh since as far as i can see it's mostly an api problem as in it is about how web applications signal priorities to the browser's networks act then not to the peer thanks martin thompson yeah i'm going to agree with victor here i think that if an application wants to do the signaling it can use web transport for that uh that's probably the most obvious thing to do here uh and then it becomes a purely api consideration and i i think to allen's point that extensible prioritization scheme uh scares me uh i think probably for something like web transport the incremental flag would be on always so we can at least make that simplification and then it's just a matter of how many levels of priority or urgency or whatever it is that we have how many levels are appropriate for this particular context and i think that's almost entirely a black shed okay thanks mt and thanks will i think we've cleared the queue thanks for the presentation we're going to move on to our transfer http 3 victor come on up and i will transfer slide control to you sure"
  },
  {
    "startTime": "00:20:02",
    "text": "i see okay one second revokes lines apparently i need to stop and represent show pre-loaded slides share pass to victor all right you can control yours oh and we started from the start sorry since that's the slides all right go ahead victor thanks um i'm vector by the editor for the overview and web transport or http drafts and those are the two drafts that are probably currently further along in the process in the sense that web transport over http has a implementation in chrome and we to some extent now it works and we've interrupt with servers and it's been used by people to interrupt with servers that were written by not us uh but there are still a lot of unresolved issues especially in the overview draft so the overview draft was originally it was written probably the first earliest in web transport process so it is to some extent the least reflective of what's currently is uh and the reason we so far have not paid much attention to it is that we were mostly focusing on getting one transport done however now that web transport over http 2 is moving along uh we need to provide some form of abstraction between what what transport over h3 does and web transport over h2 does uh and"
  },
  {
    "startTime": "00:22:00",
    "text": "that is something i believe we should do in the overview draft uh since uh it is well positioned to do that uh i wrote up a pull request to do that or at least some of that that pull request defines all of the operations that are cited in w3c spec except the w3cs pack currently size either or c9000 or with transport over http draft for those which obviously does not work if we want this to work for perfect free image too uh i encourage everyone to take a closer look at that pull request i don't think there's much to discuss since it's mostly around the text oh so now web transport over http still has a lot of some issues that are not resolved and one of them is [Music] the question of whether we need to support a go away like mechanism for training sessions and the reasoning behind the why we need to send the protocol as opposed we need it in the [Music] uh application and the application can handle it by itself is that uh the go away can be not only sent by the end points it can also be sent by a reverse proxy which is going away or some other element in the chain uh so the proposal is to add a train web transport session uh capsule uh that would can be inserted by anyone"
  },
  {
    "startTime": "00:24:01",
    "text": "on the path and do people have opinions about that oh i know allen probably has opinion speech since he proposed it do people want to say anything it's a microphone [Music] not seeing anyone get up in the room victor i see uh then the question would be alan would you like to write out a pull request for this alan says sure what's he helen says sue okay what are you drawing no no all right uh let the minutes reflect that allen said sure [Music] uh the next issue is okay let's first issue 67 is a very closely related issue but that's different so for regular http requests the way go away packet works is that when we receive a galway we're not allowed to create new requests but the old ones still can finish and we do not currently define therefore what we do with go away and web transport and my current interpretation is that web transport inner streams ignore that limitations but the new web"
  },
  {
    "startTime": "00:26:02",
    "text": "transport sessions cannot be created do folks do people have any opinions on this on what should we do about this or whether we should change this behavior uh alan alan frindell so i think when you think about it in the context of http 2 where all of the web transport streams are not visible at the http session layer you know an http 2 go away would have no impact on whether webtransport endpoint could continue creating new web transport streams within that established http 2 stream and i think we should preserve those semantics for http 3. so if you receive a go away it means no new web transport sessions but you can still create web transport streams if you want to uh martin says in japan he agrees with alan does anyone else has anything to say bernard all right it looks like we have consensus on that issue so to clarify victor you um what are your thoughts do you agree with alan and empty oh yes cool and i'm seeing another one in the chat cool those sounds uh the next issue is issue 33 closing everything when connect stream closes so our current behavior is whenever connect stream closes"
  },
  {
    "startTime": "00:28:00",
    "text": "uh everything in the session is considered to be closed and we automatically just reset everything and reject all for future datagrams uh at some point alan suggested that we might want to keep the uh internal streams open even when the sessions is closed and when we discussed this a year ago the conclusion was to revisit this once we have more implementation experience and my current impression after having more implementation experience is that they have having the semantics where the control stream uh automatically terminates everything on session uh is much easier to follow and much easier to manage in terms of resource life cycle and also if we allow that i am not sure how that would look for http 2 uh where everything is on a single stream so i think we should keep the current semantics where uh closing the connect stream automatically determines everything uh do people agree or disagree uh eric i think i would generally agree with having killing the connect stream kill everything else it seems like if you don't do that what you're effectively saying is that's kind of an implicit a couple slides ago the go away that we wanted for a web transport session and so it seems safer to make that explicit go away if you wanted those semantics and then still have your control stream around if you needed control messages rather than having the control stream disappear and trying to assign some sort of meaning to that magnus"
  },
  {
    "startTime": "00:30:02",
    "text": "magnus westland erickson um is this unit is it per direction or by directly both directions that this occurs that you close are currently closing in any direction results in it being considered uh terminated so it is a similar to how connection close behaves in quick okay yeah i mean it's depending on what protocols you're doing in some cases it might be that you actually want to finish saying it's the assumption what what if you're relying on the higher level protocol built on top of web trans or evapotrans itself to do the close which kind of semantics you want so it's a bit yeah well the higher level protocol does not necessarily always close if it's a pooled web transport session that just means that the one session disappears but http free connection remains open yeah yeah i don't know sorry about that oh martin uh you're muted martin this is the risk of having five multiple different mutes on my end so if we're thinking about the http 2 version of this one if you close the connect stream you also have just shut off your ability to send any more stream data of any type so um i think you're also prevented from receiving it in a sense uh probably not"
  },
  {
    "startTime": "00:32:02",
    "text": "um so i tend to think that once we've closed the streams then everything else is effectively orphaned at that point uh i'm under impressions that we oh davis can assay uh hey victor speaking as an individual contributor well rather as a mask enthusiast here in the http datagrams draft that we have over in mask when you close the connect stream you can no longer send datagrams so my personal take would be it just makes everything easier if when you close the connect stream everything's done i'm under impression that we're all at this point agreeing that closing the connect stream uh effectively closes everything on the session which i think is the current text of the draft i need to make sure that this is very clear on this uh but that sounds like we are good to just keep it as is uh i forgot to end it that's fine but please state your name yeah magnus pestle um one question i have is about receiving the data that if you're basically sending the rest of the data you have and then you hit close is the risk that the receiver will get the close before the date the other like data is sent and then you reject it and that's i will be a little bit careful about that particular use case well what happens in that situation so in there is in web transport a uh sorry"
  },
  {
    "startTime": "00:34:01",
    "text": "i'll guess i'll just jump in victor if that's okay with you a like closed session capsule that is the like graceful close so that's what you would do yeah and so this is about and then once all that is done then you can close the stream okay yeah to clarify there are effectively three ways you can close the connect stream you can close it by sending close session capsule which gives you the session which gives you the status code you can close it by just sending thin which is equivalent to sending an empty capsule and you can reset and this issue roughly covers all three of those cases uh but yeah in general since the metaphor i'm using is connection close and quick and quick when you close connection you no longer care about any data that's being sent on the connection except possibly for transmitting the close itself the next issue and i think the last issue the one we i think deferred many meetings ago and nothing happened so i'm bringing it here is how do we handle health service header uh the current answer is uh we de facto do not do anything without service header and there are two modes in which web transport can operate it can operate in a pooled mode and it can operate by having a dedicated connection and in case of and this is complicated that to some extent"
  },
  {
    "startTime": "00:36:04",
    "text": "oh for pooled we have to be cognizant of what regular http 3 does uh and one of the tricky one of the first questions i have before even discussing this issue is whether this belongs in the iatf draft or or doubly first t draft or this belongs in fetch uh martin okay so there's the easy part of this one is that if you get not serviced in your response you still connection available and carry on with the requests that you have open on that connection and the web transport session will be just another one of those requests running on that connection so in terms of the effect on a transport session it should be nothing it may affect what the browser does with future requests but that's um that's not something that word transfer necessarily has to worry concern itself with the um the sort of broader questions about how to transport and interact with connections with all of this i would just say look there's an alt service biz going on in http working group at the moment and let them sort it out and when i save them i mean it's mike and myself probably but um we'll see how how that works out for us all i see does does this mean we can so far close as they show on our side that's essentially what i was saying yes uh all right uh that that makes things easier for me uh thank you martin"
  },
  {
    "startTime": "00:38:01",
    "text": "uh and uh i sang this and my slides so thank you victor uh eric come on up all right so this is still an hdb3 topic as we go um but we're splitting some of this across h3 and then we'll also talk about h2 so let's talk about pooling we have decided a while back that we would like to support pooling and that means that we want to be able to have more than one web transport session within a given http 3 connection and i've got a little side note here on the side that this is really not that much of an issue for h2 because each session is fully encapsulated within one connect stream and h2 is already perfectly capable of having more than one connect stream and so a lot of the layering there uh works perfectly well and is not a huge problem um but within h3 we've kind of got these native h3 streams that we're also using for web transport and so we need to figure out how do we make more than one web transport session work there's also another consideration which applies to both versions of http which is that we want web transport to be able to coexist with regular http requests going over that h3 connection not just web transport sessions so coming back to the pretty picture that we looked at last time um this is kind of visually what we think we're doing and we had this up in the context of flow control for h2 but essentially within http 2"
  },
  {
    "startTime": "00:40:01",
    "text": "every web transport session is within a connect stream and you can have multiple of those pooling done h3 is a little bit more interesting because you have web transport sessions where here we've kind of broadened this box and this is all a little bit abstract so we don't need to get too into the details of this diagram but what we're really trying to convey is that we've got these different native http 3 streams that are serving different purposes on behalf of that web transport session and so if you want to have multiple web transport sessions those are by definition going to end up using multiple actual h3 streams um for the different web transport streams that go within those web transport sessions turns out we've actually got a bunch of different issues on github for this and there's been some interest expressed across each of them which is nice that we're consistent to essentially provide resource limits and not do a whole lot else so the proposal that i'm going to make with these slides for discussion is that we do the necessary things in order to keep any individual web transport session from uh essentially unfairly dominating the h3 connection and call it a day and explicitly that means we're not going to do anything to try to preclude future enhancements to this so as we discover other places where we need to do things um it would be great to do those as extensions but we're not going to put any time and energy into them right now and that would close all four of these different github issues which is a non-trivial chunk of what is remaining across the set of web transport repositories when we say resource limits we're effectively talking about flow"
  },
  {
    "startTime": "00:42:00",
    "text": "control in its various forms and when we look at that for h3 h3 is really inheriting that from quick but there already exists a way within a stream to limit the amount of data going by and there's also a way to limit the number of streams that you can open and so some of this is already taken care of and in the same way that h3 inherits things from quick web transport can potentially inherit them from h3 what's left though is some sort of control on the maximum count of sessions that you can make right so if i wanted to open a million web transport sessions within my h3 uh connection that might not leave any room for non-web transport requests to go on there and also the count of streams for each session for the same reason because the streams are native h3 streams those are kind of entangled we do have a note in both h3 and also actually in h2 right now that says that we don't actually go out of our way to provide any limit on the number of web transport sessions uh that you can open and that instead there's two different ways that the server or the the receiver of that can just say nope this isn't happening and i just reject it so we could choose to say that is sufficient to qualify as a resource limit and that's what we've done up until now but we could also potentially i think without too much additional complexity uh take it a little bit further so one way to do that and this part is strictly optional especially in light of the previous slide is to take our current switch for something like settings enable web transport and change it to be a setting like web transport max sessions or bike shed on"
  },
  {
    "startTime": "00:44:01",
    "text": "the name at any point but effectively if you set that to one that has the nice property of saying i'm allowing one web transport session within this h3 connection and if at that point you know there's not really any pooling if you set it to more than one now you have as many as you've allowed and everybody's in good shape the other places where we generally talk about flow control kind of follow this almost tiered layers on which you have limits for different resources that can be consumed and this is very very similar in shape for the way that we did this within http2 it's also very similar to how it looks in quick and so uh in the spirit of maintaining similarity with that making something that parallels that very closely for web transport seems appreciably nice the layers that we have there are this total session count which we've just been talking about limiting the number of streams within a web transport session and then we have essentially session level flow control for total data and then stream level flow control within each stream and this is very much the same pattern that we have elsewhere the way that we generally do that for example within quick is we have a max streams frame that adjusts the limit on the number of streams and then we have max data and max stream data and of course all of these have their associated blocked frames as well to help with debugging and other issues like that we have this nice property here in which because web transport over h3 is already using native h3 streams those have a flow control limit on them already from quick which is that mastering data frame so we don't actually need a new one there that is already present"
  },
  {
    "startTime": "00:46:02",
    "text": "and if we do the maxweb transport sessions setting then we don't need any other limit for total session count that takes care of that and so that leaves us with limiting the number of streams within a web transport session and limiting the total data within that web transport session and it would not be particularly hard to do that with a web transport max streams capsule and a web transport max data capsule if we go back to our pretty picture from before i've added some vaguely shaped annotations to kind of try to help visualize what this means this may or may not be meaningful if it is not no worries so the first one is down at the bottom there we've got our setting for mac sessions and i've put that around the fun little dot dot dot that indicates that you have more than one web transport session and then within that web transport session you've got a limit on the maximum number of streams that you're willing to devote to that web transport session as opposed to a different one this is to basically allow you to portion out the uh stream limit that you have for the total h3 connection and then within that web transport session across all of the streams for that session you can do the same thing for the number of bytes so you've got count of streams plus you've got bytes this is going to be interesting and we're about to have a discussion in a little bit about capsules for htv2 but it's worth noting that this looks exactly like the two capsules that h2 is already defining which are coincidentally called web transport max streams and web transport max data along with obviously their blocked variants so"
  },
  {
    "startTime": "00:48:00",
    "text": "potentially if we've already defined those and we've already got semantics for what they do and those semantics match what we want we could just use those in h3 as well and at that point we bring in those two things we've answered as much of the pooling question as we're planning on answering you've got associate the the uh necessary resource limits in order to make sure that different web transports can coexist on an h3 connection along with other actual http requests everybody plays nice and we are happy martin having connection level flow control on the web transport level sounds like sorry martin talk a bit closer to the mic we can't hear you having connection level flow control on the web transport layer sounds like a pretty severe layer violation since the web transport layer would now need to reach into quick and ask how much data is buffered for any of those streams well quick is all about layer violation so maybe we okay with that but i was just wondering if you have any thoughts on that i don't have a strong negative aversion to keeping track within a web transport session of the amount of data that you're allowed to send and having a limit be communicated from the other end about that but if there's it could also be that i'm not correctly internalizing how painful that would actually be see david is not standing up yeah well speaking as an individual participant can someone perhaps kind of clarify because in my limited understanding this can be done all at the web transport"
  },
  {
    "startTime": "00:50:02",
    "text": "layer without having to reach down into quick or violate layering so can you maybe explain that a bit better yes so of course you can read the data out of the stream and then have all your buffers at the web transport layer so i guess my my question is um how do we account for uh if they are missing missing packets or missing frames on a stream so in quick flow control then the maximum offset that you have received would count for flow control do we then say like we don't care about this or we let the quick layer handle this and we have like this totally separate flow control on um on the web transport layer i i haven't fully thought this through because we are now reusing the the quick flow control from this stream on the web transport layer if there's any mismatch there i would need some more time for that thanks yeah my and as individual precipitate like naive understanding was that this was a completely separate flow control but again not uh let's let's let mt jump in yeah so so martin's right this is basically i i think it's near impossible to do this without major major architectural contortions i suspect that probably the right thing to do here is is not deal with the session level um data limits and simply say that you have a quick level flow control acting on on the bytes having a limit to the number of streams that you dedicate to web transporters is still useful it would be nice if we could stop web transform transport from eating up all the bites and starving out the other things but i don't see a way to do that easily"
  },
  {
    "startTime": "00:52:01",
    "text": "and it makes me nervous so just to make sure i'm parsing what you're saying correctly with this slide here the first two items make sense and i think those are all kind of fairly intuitive and because we're able to punt the last one to just being quick's actual flow control that saves us from having that be particularly painful but it's the one right above it that is exceptionally painful yeah i mean you i don't want to force a web transport implementation to suck all the bytes out of the streams in order to get that that limit enforced knowing that that enforcement is going to be imperfect at that point and i certainly don't want to have it with its tentacles deeply embedded in the quick stack in order to get the values that the quick stack is using so i i think i'm i'm all for saying just okay maybe we we just don't do that one i think that seems reasonable i see martin is also yeah mac streams might also be a little bit difficult because you don't have stream ids on the web transport layer and so you would need some kind of stream counting that synchronized between the endpoints just to be able to keep track of them and know what's there yeah in terms of not doing the max data side of things i don't see a huge loss if we're not doing that right i mean the the reason we have a lot of those tiers is to allow people to be more restrictive um and therefore allow any individual stream to use that entire limit without having to commit to using these you know to being willing to devote the resources for the sum of all of the streams um but"
  },
  {
    "startTime": "00:54:01",
    "text": "it's entirely possible that you can tackle that elsewhere anyway so many new buttons i'm sorry about that um the mac streams one i think would be an absolute number right so you would say i'm dedicating 20 streams to each web transport session uh we probably have to have some sort of minimum commitment for the purposes of like this you must provide this many streams um but that doesn't mean that you're gonna get that many streams and so if you have enough but you can just count how many streams you've got at the moment there's a little bit of trickiness in terms of the accounting but i think we can make that work and of course if this if the quick layer isn't able to issue new streams faster then you won't be able to get up to this limit but that's okay and for what it's worth i think that's probably a good thing in the same way that max data gives you you know a way to say my i'm not willing to take the sum of all of them in total i want something smaller than that you could always say at the quick layer i'm only willing to take this many streams but each web transport session is able to use all of them which does not then commit you to the sum of all of your web transport sessions in stream count so what you thinking about sharing that pool of streams between sessions or is this the first stream one a procession one no it's just the the fact that quick has a limit on the max number of streams is essentially the shared limit yes yeah that's right so at some level that's almost a feature more than a pain point"
  },
  {
    "startTime": "00:56:01",
    "text": "okay i i think we we're going to want to try this out to be sure though for sure alan from dell uh so i think i i hear the like concern that max data is hard maybe i don't know i want to say impossible but much harder than stream so i think maybe what makes sense is we we separate this out and kind of table max data for now but i i'm left a little uncomfortable thinking that like one session can cannibalize quick's entire flow control and leave other sessions blocked um and so be good to have a solution but you know how complicated it needs to be is it worth implementing maybe not um so i but i think i'm okay for the to move forward here like let's just look at streams for now and maybe keep an issue open to revisit data or have somebody sketch it out like what would it really take um jumping in as chair here um that what i would propose as chair assuming folks agree is um well starting first with like limiting the total session count via settings my understanding from the room that i i'm hearing consensus that that's good if anyone disagrees please come to the mic line now okay like limiting max streams it sounds like this might be hard but we want to try so i'm getting consensus let's write a pr to do this and have someone implement it and [Music] see from there if you disagree please jump up to the mic i'm getting a sense that max data"
  },
  {
    "startTime": "00:58:00",
    "text": "most folks think that it's too hard but alan thinks it might be possible i personally as well as chair i i don't love the idea of keeping an issue open with no clear resolution points so what i would suggest would be to close the issue and if someone has a design that they've implemented that they they want to bring to the working group to say i figured out how to do this please take a look can we put this in the spec they could reopen a new issue at that point uh alan would that be an accessible resolution for you and for everyone else by the way if you disagree please jump up i'll just repeat what i said in the chat which is i'm not any more sure than anybody else that it's possible but i am interested in spending some more time or having also the group spend more time exploring is it possible without bending over backwards or doing something that's not worth implementing uh so in terms of i don't real i'm personally like to keep the issues open until they're really resolved but if i mean otherwise it might get lost but i'll let other people decide how we want to handle administratively my my take is if there's something that no one has an intuition on how to solve keeping it open like doesn't i don't see a what leads us to closing it and so i'd rather have like us fail like if no one proposes anything the answer is gonna be we're not doing it and so i i would put the burden on whoever and you know we're at a point on this document where anyone can file an issue so if anyone comes up with a proposal then they open an issue that would be my personal preference but if what if you really really disagree i think we can keep it open but no it's okay i mean go ahead and close it if that's what we want to do i will probably forget about it we will all forget about it probably sometime after we ship it"
  },
  {
    "startTime": "01:00:00",
    "text": "somebody will report a very complicated bug where we've hit this issue and complain and then we'll kick ourselves for not having spent more time on it okay all right well so i'm getting uh plus one from the to what i was proposing on the chat i'm gonna go with this then um all right i'll add some text to uh these issues um eric if you wanna keep going sounds good all right so basically we'll get that in without max data get some experience see how it feels and we can go from there all right skipping ahead that brings us to h2 and this ought to actually be fairly quick so since 112 we've landed a bunch of the changes that we talked about then however a bunch of the changes that we talked about then the resolution was wait for the http datagram design team in mask which has now concluded so that is fantastic we are unblocked on that um [Music] the remaining outcome from that that we need to decide what to do with is capsules so that's going to be where i suspect we'll spend the bulk of our time today before we do that there is one other issue that i think martin filed that is something i wanted to bring up so we all talk about it a little bit and see if there's any intuition from anyone that that would prevent what seems like the right answer to me essentially this is talking about sending frames before you actually get the response back from that initial connect message so today there's text in the document that says that intel your connect gets back a response saying you know 200 you're good to go this is now a web transport session"
  },
  {
    "startTime": "01:02:03",
    "text": "frames at all you're stuck for that entire rtt and i think martin very rightly points out that's basically just delaying things arbitrarily any frames that you're allowed to send by the flow control limits et cetera et cetera so any frame that would otherwise be legitimate why should we prevent you from sending them and so the proposal here is allow sending them don't force people to have extra rtts or you know trips as opposed to round trips and let everything get going a little bit sooner if you want to wait nothing stops you from waiting that can be a nice simple easy way to implement it but if somebody wants to put in the effort to correctly you know send frames to allow things to get going sooner especially since web transport is focused on allowing the server to also initiate web transport streams if one of the first things that you're planning on doing is having the server open a number of streams then having it be able to send those frames uh without waiting for the connect response to get to the client and then for frames to get back from the client to the server so that it can then open some streams seems really very pleasing so if anybody is strongly opposed to that or sees a reason why we should keep that restriction around uh now would be the time to speak because it seems great to let things get going and not have extra rtts uh be there when we don't need them excellent we will take silence as an indication of no major problems"
  },
  {
    "startTime": "01:04:02",
    "text": "that brings us to the fun part of these slides for some values of fun which is capsules so the datagram design team is now complete thank you david and a bunch of other fine folk who participated there and helped get that out in time so that we could talk about this here previously in h2 we defined tlvs for every web transport frame and now that capsules are a thing we could potentially be registering them instead of in our registry of places where we keep our list of web transport frames to use with h2 we could register them in a different registry where we keep the list of capsules and so that brings up the somewhat obvious question of great so what do we get if we use capsules and this is our current list of frames we arrived at this a couple of meetings ago it's basically the minimal subset of things that we need in order to send on the connect stream so that web transport works if we use capsules there exists this cool capsule already called datagram and we could potentially just use that instead of having it be wt datagram the definition is the same the wire format is the same the semantics are essentially the same with a little bit of caveat that we'll talk about a little bit later the other benefit here is if we were planning on using any of these other ones like wtmax streams and wt streams blocked from h3 having them be capsules that you can just use from h2 you can use it from h3 everybody's happy that works out pretty nicely we've just talked about removing web transport max data from that list"
  },
  {
    "startTime": "01:06:02",
    "text": "so the reuse here on the later part of this slide was previously going to be web transport mac streams along with the blocked variant as well as web transport max data along with the blocked variant that would now just be web transport max streams but the point is the same if there's anything from h2 that we need to reuse in h3 we can just use it it's great it's happy we're in a shared registry already anyway no problems there the thing that i think we want to discuss is whether the end to endianness of these capsules is going to be an issue is that a good thing is that a bad thing and so my understanding is that the tlvs that we were previously defining for your web transport over h2 frames were not end to end they would be consumed by whoever's terminating that particular h2 connection and if you imagine a scenario where not everybody wants to implement or be supporting web transport over h2 but we do think that we need it maybe on the actual connection over you know kind of the last mile of the internet to whatever client is connecting because it may not have access to quick on some percentage of networks you could imagine that an intermediary would allow a client to fall back to web transport over h2 but still want to speak web transport over h3 to whatever upstream or origin that it's going to be talking to and it would be doing that for all of its clients so it would be talking h3 upstream all the time and it would usually be talking h3 downstream to the client most of the time but sometimes some of the clients are going to need to fall back to h2 and we want that to work if we define these as capsules in theory they are supposed to transit"
  },
  {
    "startTime": "01:08:00",
    "text": "that intermediary and go all the way to the end and so the question here is web transport over h3 is currently defining a whole bunch of capsules if web transport over h2 is defining a different set of capsules we've just talked about reusing one or two of them but in general it's a not super overlapping set what is the expectation here like when my when i'm an intermediary going from web transport over h2 to web transport over h3 like does every web transport speaking end point need to be able to talk all of those because somebody in the chain might have been using h2 or might have been using h3 so that brings us to our discussion slide and i will move back one here since i don't think having discussion on the screen is super helpful thoughts and feelings alan is in the queue alan frindell so just to um repeat what i just put in the chat which is that the datagram capsule it's end to end as a concept but it's not transmitted at each hop as a capsule right when you go through a hop and it came in as a capsule but the other end supports a native datagram concept then it's the intermediary is allowed to sort of convert and so some of the capsules i don't even know if you want to go back to the slide that has them all uh that might make sense um where you know for example probably like reset stream or stop sending can make sense in that context i'm not so sure about the ones that are trying to control resource limits because those resource limits are very much hot by hop um so i probably have more to say on this"
  },
  {
    "startTime": "01:10:01",
    "text": "but i'll stop there yeah i think that matches kind of my initial intuition which is at the very least the flow control ones seem usefully hot by hot although end-to-end flow control across web transport session is an interesting concept but i don't know that it was the original intent of having those resource limits and and allowing an intermediary to share that across multiple uh people potentially who are sharing the same upstream connection martin so i think mark nottingham's review of donna graham's draft that asked the question is capsules premature something or other generalization abstraction um and i think it's this here that sort of highlights the challenges of using capsules for anything because i think you can simply look at the datagram thing in isolation and go yeah so that makes a very bit of sense we can pull those out and we can forward those however we choose but then i'm looking at the stuff you're doing with streams here and thinking if there are connection level uh constraints on what we can do with streams the amount of data we can put on the number that we can have i don't know how to intermediate those and take stuff that's on an h2 stream and then lift it into native h3 concepts and then back into an h2 stream again at some other point i suspect that the right answer here is to not do any of it because this is just too hard and maybe it's just because it's two in the morning and i'm feeling rather stupid but i can't conceive of a way if that isn't massively complicated and also likely to fail regardless of"
  },
  {
    "startTime": "01:12:01",
    "text": "what we do and how clever we are so i that i'm quite happy with the current design honestly i think there's something in the sense that um as alan pointed out like there is a way and datagram is actually an example of that where in the http datagrams document it says you know each capsule type can define the translation that it should do potentially when running over different transports and the only capsule that is defined also defines one of those to say datagram is you know potentially broken out into being a native datagram so like that concept is appealing and it's maybe possible to do that for each of these things but i'm not really sure that that gets us where we want to be anyway david schnazzy um speaking as an individual so i've been thinking about this capsule thing for a while over in mask land and kind of saw this come up and was like trying to because i i agree with martin is it i mean that's what's made mask interesting is depending on how you stare at it everything looks completely different and the i think what it comes down to is are is web transport of http 3 the same protocol as web transport over http 2 and 1. or are they different protocols and by protocol i made http upgrade token perhaps um and and where this gets really interesting is let's say you have a scenario where you have two h2 hops and an intermediary in the middle you would like capsules are great there everything"
  },
  {
    "startTime": "01:14:02",
    "text": "you want everything to be end to end all the intermediary does is to shove the stream back and forth and everything just works um but if you have a case where you have h3 and then h2 kind of conceptually in my mind it's you no longer have an intermediary in the middle you have a server that terminates the client's hd web transfer http 3 connection and then it turns around and becomes a web transport client over http 2 to uh to the back end for lack of a better word and at that point the capsules are end to end from the client to the first node and from the first node to the second node uh in that you have three ends or four to this protocol as opposed to two um that kind of makes it make sense to me this feels consistent it actually like i i we had another discussion of like they do should they have different http upgrade tokens it kind of pushes me in the yes bucket for that a little bit but that makes it all of this kind of self-consistent and kind of clean and reasonable to me um empty does this make any sense to you that is amazingly slow that makes sense i i think that the sort of minimization that comes if you're if you've got that sort of intermediary there's no edge away through h3 or vice versa and you're doing the web transport thing then that intermediary needs to understand web transport pure and simple up and down back and forth yep there's still there are still problems that we don't have solved in that scenario though and that's where it starts to get really interesting"
  },
  {
    "startTime": "01:16:00",
    "text": "um if the first hop has no constraints on on creating streams but the second hop does and you you have a stream come in at the intermediary and the intermediary cannot create the outgoing stream what is it going to do yeah exactly the sort of thinking that i'm going through right how the hell do you deal with that and the answer is i don't know um really that that's the best that i've got right now um and just to make sure i'm understanding you right well first off i i agree like that kind of back well it's a form of back pressure just uh 17 different like degrees of back pressure um 50 shades of back pressure if you will um the thinking about it though that like this issue at hand whether we're using capsules or something or any possible framing for this we still have to solve that right like unfortunately if you have an intermediary no matter what we have that problem if if it stays in the stream and can stay in the stream then it's not a problem if it if it has to be lifted out and the constraints are different then then that's not going to be the case so if you if your intermediary was just tunneling the effectively bite string back and forth then then that would be fine i think it wouldn't be constrained in any way and the the constraints on the number of streams that you have would be fully end to end at that point it's only funny when you start talking about protocol translations and other things and so in that case if we're keeping everything in the stream then that sounds like a textbook use case for capsules"
  },
  {
    "startTime": "01:18:01",
    "text": "except that it's not buying us anything at that point apart from a whole bunch of redundant length fields i'll jump back in line for that one yeah same right uh alan you're next in queue alan fendell i was when you talked about that stream problem going through intermediaries it reminded me that we have had that problem with i'm going to say the p word everyone cover yours if you don't want to hear it push server push um we had that problem where the one hop thought it could create new pushes and it got to a proxy that no longer had available streams so and it's yeah it's kind of a hard problem and um in the in the h2 version of web transport it's very nice because all the stream creation is completely end to end and are all the stream limits that we have not yet actually defined yet but plan to um or i guess they're here uh will the end to end but yeah whenever you're going between protocols as martin says here there's problems hello lucas buddy speaking for myself um yeah i made a comment in the jabra i think like my understanding of what martin said about the stream impedance between creation on one side of an intermediary and the other seems just to be something that exists already today um to me anyway um if maybe maybe if there is a problem we could solve this with contexts and capsules would let us do that if we really wanted to so i don't i don't see a humongous issue here myself that is different than the problem we already have today so i don't have a huge challenge with that the thing that i think"
  },
  {
    "startTime": "01:20:00",
    "text": "comes up here which is interesting is when you're looking at h3 and you're looking at h2 and then we start talking about what you do with upgrade tokens and that sort of thing with h1 once upon a time we had a conversation and i think this echoes a lot of what martin just said in the in the chat which is we talked about kind of do we define these things that we're sending over the connect stream and capsules have this cute way to say you know it's a datagram when you're on a transport that supports datagrams split it out to be a datagram we could make the webtransport stream capsule um do the same thing and say when you're on h3 split it out when you're on h2 don't um but we we have not currently done the exercise of making h3 and h2 use a single list of these things but there's significant overlap for a bunch of them and we're talking about reusing a bunch of them where appropriate is this just a signal that where appropriate is actually all of them and once you start talking about intermediaries or if you start to david's example earlier like what if i go h2 to h3 to h2 right or what if i'm even going h2 to h2 and we were just talking about how flow control gets propagated there so there's definitely a line here where it's more complicated and your intermediary's got to be smart enough to know what it's doing and like i'm totally fine saying that there's some there's a lot of these cases that cross that line and if you weren't smart enough to do it you shouldn't have been doing it so stop but it does almost seem like it's worth doing a little bit of the exercise to see for the parts that are over h3 maybe those are just defined in the same way and and defined to be broken out for h3 and h2 is just those being smushed together which is the list that we have on screen right now"
  },
  {
    "startTime": "01:22:03",
    "text": "thanks uh david in the queue again as an individual um i agree with eric here just wanted to reply to um martin's uh point like what is this bias and adding redundant lengths so i'm looking at the h2 draft and it like because you know we have these frames and i think it's reasonable that we might want to add a future frame for an extension later these are self-describing and so we have a sequence of tlvs on the data stream which starts to look like something i know um so like we don't have any redundant links these are literally wire format compatible with capsules the only question is are the do we share types are is this one eye on a registry or two in the registries and for the reason that um eric mentioned earlier having some of them be the same everywhere and some of them be popped out in quick for there i think that really meshes with mt's original point of we're defining a generic web transfer of http and then when it's over h3 you can put it on steroids it can do extra cool stuff so like this is starting to make a lot of sense to me personally go ahead empty [Music] you don't get any real performance so you might you might attempt at one point to um get true datagrams your data and drew quick streams for you and then further down the chain the case i was going to say there's probably a simpler way of building this and it may not be as pleasant um but don't do that is actually a pretty good"
  },
  {
    "startTime": "01:24:02",
    "text": "strategy and if your intermediary doesn't need to do uh it doesn't advertise support for http 3 version of web transport if it cannot guarantee end-to-end um quick and http 3 all the way i'm losing my brain it's really good at this time in the morning um uh if i can't guarantee and to win then it doesn't doesn't offer it and the fallback here which is the h2 one which by the way probably works on http one one as well adequately um then that's your fallback the um the way in which we get streams lifted up into the quick streams is would require a whole lot more unification between the designs of the quick and sorry the http2 and the http3 versions of this thanks eric yeah i'm kind of wondering if it's worth doing that effort it's tempting to you know down for an hour or three and see how far we get with it it may be that we get there we're like great this is actually not hard these things weren't really that different the current h3 document is basically defining a capsule that has a way to be broken out for h3 by default already or it could be that we get our way into that and say no this is terrible they're separate for a reason they should stay separate um but there does seem something reasonably attractive about having them share the same thing and then web transport over h3 is broken out and we just choose to not define a way to break them out into native h2 streams because we don't want to deal with that and then we're all happy and it's done the only other point that i do want to make"
  },
  {
    "startTime": "01:26:00",
    "text": "david when you say that using a capsule is a choice to put this tlv in a different iana registry i think that's absolutely true but when we're looking at the other things that we get from that i think the is our litmus test for should this be a capsule should it be end to end or not um and if not what is the litmus test because i think which in a registry do we want it in is i mean that's nice and i'm happy like i don't care what i enter registry we put things in um but it seems as though if we're like great this is nice we can put it in a different iona registry and now we don't have to make a new one how pleasing that is pleasing but if that comes with this extra end-to-end property that is actively harmful it seems like the litmus test here is the third item on this slide not the first item on the slide yeah fair enough i agree with that [Music] [Music] it's looking like we've drained the queue on this one and having seen the slides we don't have anything after this um so clearly we don't have an obvious answer definite consensus on this one but it's worth like i like eric's plan of trying it out and then showing like having an empty look at like a pr from eric when it empty has had a decent night's sleep and see if he likes it or not would be a good next step and of course to have the entire working group take a look or contribute to that um [Music] how does does that sound like a good path forward here so we can make progress i see eric in the queue go ahead yeah i'm certainly getting to do that and you know happy to do that with other folks if we want to sit down together and and"
  },
  {
    "startTime": "01:28:00",
    "text": "talk about it on a call somewhere and and brainstorm a little bit while we do it um but yeah the other thing just in case uh victor wasn't looking at the chat i know we talked about this a little bit before but i wanted to make sure if any reaction there since a lot of what we're just discussing kind of has impacts for how h3 looks i know this had come up in the past if there's anything that we want to think about there that we might be missing i would be good to to hear about that at some point as well doesn't have to be right now but worth thinking about thanks and i i think having like uh i'm just a call sounds like a good idea with like the most passionate folks to move this uh forward i'm debating whether i want to use the term design team or not i'm realizing that the terminology doesn't matter too much do fox have thoughts all right alan says if it it it does walk like a design team in quack like a design team after all okay we shall be forming a design team so the i will send an email to the list uh asking for volunteers um eric since you've been kind of leading this and offering to do the pr would you be willing to lead this design team sure indeed thank you so we'll send an email to the list and if you are interested in joining please reach out to the chairs and then we'll have the design team figure this out for us and in an ideal world um if that that would happen in the near future we could have an interim to"
  },
  {
    "startTime": "01:30:00",
    "text": "discuss it um similar to what we've been doing over the last three months at a very different working group um bernard any thoughts i'm starting to think like i'm rambling and we can probably end the meeting early at this point pretty soon no i think i think uh we've hopefully we've hopefully overcome the note-taking problem you should see the notes in the uh coming up very soon um and it sounds like we've got our next steps uh indeed yeah i do see the minutes now thanks for clearing that out um i do see jonathan linux in the cube come on over uh yeah jonathan linux um this is uh luke curley who said he missed the initial priorities discussion that raised an interesting point in the chat since we seem to run out of everything else to talk about that he has a mock style scenario where he wants to be able to prioritize new things he sent over old things he sent which is hard to do with a small thick set of priorities so um i guess i know the priorities api is not in scope of this working group per se but it's something to uh people who are designing mechanisms like that to keep in mind um like whether that means i mean it sounds like his implementation just has you know uh uh 64 for priority and just names his priorities after his stream ids um that's one possibility or somebody could have some way of dynamically adjusting priorities of older stuff or something like that but just something to keep in mind okay speaking as chair since we do have another 30 minutes it makes sense to use this in person time so i'm okay with that and i see luke in the queue as well so let's just go through the queue um alan go ahead"
  },
  {
    "startTime": "01:32:00",
    "text": "alan frindell well i was going to raise a different point before we jumped into priorities but go ahead that's i'll just talk about the priorities and then make my other point too so i think in terms of luke's scenario you don't if you really all you want is to switch you know your mode from fifo to lifo maybe the easiest way to do that is have a single message that goes over your web transport application protocol that says please use you know please tell the scheduler on the other side to do everything and reverse remedy order and then you're rather than juggling your eight lanes or whatever so that may be a possibility that could work the other point i wanted to make was something that came up right at the end of the media over quick session um that had me thinking about its intersection with web transport which is in media over quick you know i think we recognize that we can get quite a bit of you know value out of breaking videos up in a certain way and sending them over quick streams but that if you want to push latency really low and i'm not going to get into discussion about what is low latency um but if you want to go as fast as possible at some point you are you need better interaction with um or closer closer ties with congestion control loss recovery um re-transmission policies introspection from the like lowest level of the transport in terms of um you know information it might have that you want to feed back into the application a really tight coupling and i personally have not been very involved in the w3c um api definition for web transport so and i saw there's like web transport stats which maybe have some of that feedback but i i wonder if as mock is starting to get going and some people have dreams of someday building that in web transport you know are we going to think about a you know sort of v2 version of the api that's got you know more sophistication there thanks"
  },
  {
    "startTime": "01:34:00",
    "text": "okay thanks that sounds useful i and this topic has definitely been the biggest queue size we've seen in this working group so just to be clear this is no not the mach buff um but we can we can please so yeah we can talk about this but let's scope it to the implications to web transport and on your other point yeah i encourage you to join the w3c um sessions for there i i forget the details about membership in the w3c so double check those but it's we've had good conversations around those things and from folks who really care would be useful and um okay uh luke is next in queue hi my name mentioned um yeah just uh just to summarize the priority stuff um it's something that right now it's fine because i'm sitting i i have a server and the server you can prioritize quick streams um but it is something that you eventually need browser support for is the idea that um we want to fully use the connection and we want the connection to degrade cleanly when uh when the window when when uh it's out of bandwidth effectively uh so yeah the gist of just the warp is um uh nearer segments and audio or higher priority it's kind of like a lefo although it gets more complicated than that uh to some some people saying you could just switch the mode uh stuff like control messages still need to be on a higher priority stream and whatnot um but i agree this is something that's not really the transport doesn't need to support um but we do need a way for uh for end points to tell other endpoints like this is the party of the stream like i created the stream could you please write back to it with this priority is effectively uh what we're looking for and that's that can be accomplished with like an"
  },
  {
    "startTime": "01:36:00",
    "text": "http header um like luke is the the extensible private party header um yeah a little bit of signaling that's probably thanks that makes sense victor uh oh i think you wanted to encourage people who have opinions to comment on the w3c issue which is issue 62 and i plus this link in chat thank you victor empty so i get it was a little unclear from the description there but um it seems like this is mostly something that the application can can do for itself because you're you're having the client tell the server to prioritize in a particular way which i i guess means that you're your uh your signaling can be worked out on on your own terms there's nothing there's nothing here that that's possible because anything is possible once you give someone the ability to send bytes to another one um i was wondering whether there was an end requirement from the browser side from their application so you're getting multiple streams on the browser and trying to send on them is is there any requirement that that those big life in in terms of the sending behavior of the browser that would be more interesting in the context of this one because that's not something that you can [Music] change obviously you sort of control your servers maybe control what the browsers do thanks and"
  },
  {
    "startTime": "01:38:00",
    "text": "yeah jump jumping in ass nope sorry go okay just jumping in as chair yeah i think that's a great place to kind of draw the line for like you know our goal for web transport has been to ship something that that works at least for for now and in terms of scoping it like if something can be applica solved at the application layer kind of our philosophy has been to just keep it there for now and perhaps at some point if we realize every other application is doing the same thing like for example we talked about messaging framing then we might consider adding it to like web transport v2 whatever um but yeah that seems like to be a good um a good demarcation um for for what we consider in scope just some thoughts uh lucas go ahead hello lucas pardo speaking um yeah like obviously this topic is super exciting for me um i i i just i'm not that worried because like the the extensive priorities draft is just one kind of signal and it's intended to be used in one way in and in practicality it should should definitely be combined with all the other information any sender is using to schedule how stream data or datagram data is being sent some might ignore it some some might use it some might use it more strongly than other inputs it seems like web transport has all the pieces there to let us add extra signals or to provide some guidance on how those signals could be used slightly differently um i think the browser world in particular has a lot of constraints that others don't um in the quick world we we say you know we should provide an ability for applications to control the prioritization of stream data as an example and then we leave that completely undefined some people hate that but um it works okay for now and we'll we'll use these things and we'll figure some more stuff out um and maybe in a v2 or some other additional document we can"
  },
  {
    "startTime": "01:40:01",
    "text": "address this um my main concern is are we are we blocking anyone from being able to extend or innovate or address the use case that they want and i don't think that's true so i think we're in a decent state and i'm more than happy to continue following the discussion and seeing what we can do to make things better thanks lucas hey colin uh so i just wanted to jump all the way back to the priority issue and i i do think that it'd be very desirable to have some way to be able to deal with um you know saying the sequence that the the web transport is going to be sending things out of the browser that seems pretty important for a lot of use cases particularly the ones luke was talking about but i don't think that just i think that'll for a lot of them just saying um a lifo doesn't necessarily work uh even in the the case uh the video case i think you run into problems where it works it makes it a little bit better on semi-congested networks but as soon as things get a little bit the next level of worse that lifo mechanism totally fails without an actual prioritization scheme so i think it'd be well worth us talking a little bit about something more flexible that meant a broader set of use cases for for how to do prioritization thanks cohen bernard there uh so i'm talking with my itf web transport hat off but um i will just say that in terms of w3c work we now have a functioning version of web transport api as well as web codecs in chromium um and so some of these mock scenarios you can actually build them in a fairly little amount of code but one thing i'd like to make clear is that um as i see it i i believe and maybe allen"
  },
  {
    "startTime": "01:42:01",
    "text": "can correct me if he's tried this that this the video upload scenario should be implementable using the web transport api as it exists today um the downstream scenario is a little bit more complicated but the key thing to understand here is in that scenario as as has been described the server can do things like priority assuming it's available without the web transport client having to support that so at least within the api scope so far it doesn't seem to me like it seems like the current api can at least accommodate those two scenarios the the downstream and the video upload of course we're uh very willing to understand if there are issues doing that and i would urge people to actually try it out um so you know we we very s consciously didn't try to go down to the lowest levels in the api like you originally thought people need access to the ack info or the guts of quick bandwidth estimation and after some debate we said not really that's turning out to be an issue for rtp directly over quick but i hopefully not for the for most of the mock use cases and the stats that are there to make clear to people they're not so you can implement congestion control uh they're just there really to to give you a sense of how well your app is performing um but i urge people to try the apis out and see if there are any issues magnus westland is the in the priority here is it question if you actually need to have dynamic or as they update the priority value for a transmission so say it is that part of the problem here really i wonder if it's not um are you assuming that you have so small data blocks that you're sending one single datagram and that's the only thing you need or uh i just out of curiosity who are you"
  },
  {
    "startTime": "01:44:00",
    "text": "asking the question because um obviously yeah yeah yeah so you're looking at me but i can't answer i don't know no i ask in the working group i think about if it's well luke's next thing queue maybe he can attempt to answer because i know he's thought about this quite a bit yeah so i wanted to also answer bernard uh so we're doing it we're actually right now twitch is um using web transport i think it's like one percent of chrome users right now um we're not using web codecs for using msc still but i have a player that uses it um and yeah we are doing prioritization on the server side uh it is currently the current design to answer the previous question it is a fixed when the stream is created we know the priority it doesn't need to change although in theory you could make a slightly better design if it's allowed to change uh it really just depends there's no right answer for prioritization so it's nice to have that functionality but it's not required either i think for bernard the main concern is and this is a w3c thing is just if you want to switch the direction if you want to uh to create media at the browser and push it uh like via warf or whatever uh to to the server uh you can't control the priority then and that's something that we're thinking about um like how could you do ingest and send newer media for older media so you can just starve and eventually drop the only media that would be interesting to see your results and concerns about about that as you said it's the client sending that's the real problem here alan from dell yeah i think just responding to what bernard had said when he was up last time just that um i'm thinking about the same scenario that that luke is talking about but also probably not going to be in the early scoping of mock so probably maybe not so urgent but i just got the sense that"
  },
  {
    "startTime": "01:46:00",
    "text": "there were people in the room who have very low latency low latency uh use cases for ascending video and um and probably weren't going to be happy with like the default quick congestion control that the browser was providing so again it could be a long-term thing but i think in the in the for the shorter medium term that we might want to do there it's probably going to be fine thanks alan bernard um to follow up on what alan said alan i'd be very interested if uh in particular if you're encountering any issues with bbrv one in your video upload scenario i don't know if you uh i don't know if your quick implementation you were using did the rv1 or i don't think that same problem would occur with new reno but uh sorry luke i just jumped the queue to respond to bernard which is i just want to be clear that i personally and i'm not sure that meta either does have any code that does this right now it was just based on sort of the comments and discussion that happened on wednesday so if i have running code and i have problems i'll let you know okay go ahead luke just to reply to that um i i've implemented bbr um v1 um there's i also tried v2 but there's some bugs i haven't fixed those yet um most of our concerns with it are theoretical at this point we're not really pushing latency low enough that the probe rtt phase is an issue um uh and you're right like reno and cubic they don't really have the same latency concerns but they also kind of congest a lot easier um i think eventually it would be nice to have more control over congestion control in web transport but i think that's probably asking a lot i'll be honest i think that's probably asking too much for browsers to say like let me control the fine grained when every package should be sent um but i think that's up for discussion and certainly a w3c thing um how much control should an"
  },
  {
    "startTime": "01:48:02",
    "text": "application have over when individual packets are sent which is good i mean i i think that maybe the level that you might want to consider at some point is that there's some sort of hints that can be passed on down about the congestion control that's used at the quick layer but we're probably not anywhere near that point yet um but it probably at some point we want to do we probably will be at that point and we might want to think about whether you're going to directly control what what quick uses or whether you're just going to pass down some hints about the attributes of what you wish it what about your desired performance and i i don't have i don't really care as long as there's some way to influence it uh just to respond to cullen that's kind of what's been proposed in the api it would be some hint in the constructor that would tell you what kind of cc you want yeah exactly i mean i don't see what that seems adequate for what i can imagine right now is what's there right now so like that's why i'm raising the issues it seems fine that was a nice screen frame freeze uh victor uh yeah i think both variants both perfect for congestion control are possible my my idea was the simplest one is like would have an enum which would let you switch between something like bbr and something like i don't know goog cc uh or any other real-time appropriate uh algorithm but it is not entirely infeasible to hand off suggestion control to javascript i've seen at least a research project that they uh i don't know if they ever published a paper but they had"
  },
  {
    "startTime": "01:50:00",
    "text": "a working prototype of to move congestion control out of linux kernel to user space and that was a roughly like cross process can out of process construction control so this is definitely possible and is even something that people have done before and the question would be is it really worth of engineering investment at that point yeah i guess to victor's question it's probably worth discussing at some point what uh how you could figure that out i mean i guess the first step would be for people to do their own thing and see what works and what doesn't work and you know have some evidence right now i think you know victor there's some people working on scream um so uh i think they've had some results that suggest that something like google cc or scream might be worthwhile but there's also the issue you know particularly if you're doing weird things like pooling like what would happen there it may not be that simple all right it appears we have drained the queue [Music] unless folks have other things i think and we're getting closer to time i might i think we're going to wrap this up um thanks everyone for coming thanks to the presenters for taking the time to make slides and to present i think we had a very productive session today we got closer to resolving quite a few issues and we're starting a brand new design team to address a thorny one i think we're making good progress to getting these documents uh close to something very stable which is great um"
  },
  {
    "startTime": "01:52:01",
    "text": "thanks everyone for that thanks everyone for keeping always making sure this working group is a fun and polite pleasant place to work so thanks and uh we'll see you on the mailing list and potentially at an interim assuming the design team reaches something in the near future thanks and bye [Music] hmm um okay yes"
  }
]
