[
  {
    "startTime": "00:00:08",
    "text": "Yeah. Okay. Alright. Good morning, everybody. I have checked the time And It now being 9:30 local. think we should probably start the session. So Good morning, everyone. Welcome to AETF 117 DTN work in group session. So this is your chance to realize you are in the wrong room. Uh-huh. Yes. This is the session title working group. Yes. Yes. That's right. I have a hard problem with a gold check t shirt. They just take it to the mail I have a hard problem Can I yeah. There we go. So as with all the IT working groups, this is covered under the note well. I hope by Wednesday of an IETF session, you would have read this at least once. exceeds my responsibility as a chair, to draw your attention to this and make sure you understand the content and what it requires of you and what you can expect in return from the IETF in terms of good behavior, ensuring that as a participant, you are not feeling harassed"
  },
  {
    "startTime": "00:02:03",
    "text": "coerced in any way, but equally, your understanding where you stand in terms of IPR. will not go through this for ages and ages. Please go and look at the documents if you are at all unsure And this is the note really well, which is new for this session, which is very much talking about making sure we have a positive environment and the people are not feel feeling harassed or aggressive actively attacked on their opinions, etcetera. We are blessed to have a member of the Ombudsman's team in the back of the room. So we will be under strict examination, but in general, DTM has not really had these problems, so long may that continue? but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but The facilities are there if you want it. Equally, meeting tips. So please, in this post COVID days, with Miteshco. Please make sure you are signed in using the Miteshco in person tool, if you are in person, if you are remotely participating, I'm assuming you can hear me through the BTEC tool. If you're going through some other audio video system just for attendance please make sure you sign in through the me tech at all. Again, we will be operating a queue Unified between remote and on-site participants if you wish to join the queue, please use the on-site tool to click the raise hand button And therefore, it just makes everyone's job a little bit easier Again, remote participants please try and keep your mic muted and to save a little bit on the bandwidth but we don't really need to see you listening and noting along. pleasant, though it is, it it makes it a little bit distracting for everyone else. So please keep your video off as well. SQL presenting. bit. Bit. Excuse me. Otherwise, these are"
  },
  {
    "startTime": "00:04:01",
    "text": "of links were in the middle of the week. So I I think that we're all familiar with where to find the agenda and need echo and other information. Specifically, for DTN, This is our agenda. I wanna walk through it briefly and then just take a moment to see if there's thing that we wanna So sort of agenda bash. we want an update of the URI scheme. which Rick had just posted a a minor grammatical update just recently. And then we wanna look at the network management work starting with an architecture by Sarah Heiner and then the data model with Brian Cipos. Following on, Brian Cipos has some updates to the COCI security context, which is currently in working group last call, and then a discussion EID patterns. And I I think we've seen even some mailing list traffic in the past 2 days related to that. Mark has a a yang model for BP, and then Eric has a discussion of a CLA for BP directly over Ethernet. And then we have two areas that we want to get working group discussion and feedback on, one of them is an adoption discussion around putting applications over and, in particular, HTTP and SMTP, And then lastly, a discussion on how we would allocate allocators in the IPN URI scheme, which was something that had quite a bit of mailing list traffic. I think we can get through all of that in in roughly 2 hours. If we do, we'll have some open mic and and some Before you go on, are there any concerns or questions about the agenda or any agenda bashing. Okay. So then, lastly, we do need someone who will agree to take minutes? Is there someone who would come in and volunteer to to have point on the minutes, although we do ask that everyone join the shared notepad because the more people we have taking note the better notes we will have by the end of the meeting. I know that Adam's online. Adam, can we rely on you for minutes? I I just heard. We're already done it. Okay. I've"
  },
  {
    "startTime": "00:06:03",
    "text": "We have a lot of work in our in our charter for the working group. We had put together some initial milestones when we looked at this. They started with July 2022, aspirationally finishing in July 24, which which we are about halfway through. And This is how we would score ourselves right now. there is good work going on in in the delay tunnel management architecture, and there's good going on in the naming, addressing, and architecture, particularly around IPN URI's game. And there's also work going on into the network management protocols, and that is what we're talking about, for example, mostly today. But there are a larger number of things. in front of the working group for which we need energy and and drafts once we get through this initial network management and and IPN naming and addressing, in particular, process signaling, bundled, bundle encapsulation, quality of service, or otherwise flow or data labeling. that comes in in neighborhood and peer discovering and eventually also key management So this is simply something we wanted to put in front of the working group to say that we do have a charter. We do have items and milestones, and we are trying to make sure that as this initial work gets through, we are adopting and putting working energy into the other things that are in our milestones. in in the in the in the Vanev, let's look at and get through some of the work that we have. We do have 2 documents currently in working group last call. 1 is the BP Sarkozy security context. We're gonna have a presentation on that later. The other is a smaller document. which is an entry into an administration records type registry. That one is currently in working group last call. We have not seen any sort of significant concerns. So we wanted to just take a short moment at the beginning and asked if anyone has any concerns, or differences related to this particular document, because otherwise, it will successfully go through working group last call."
  },
  {
    "startTime": "00:08:03",
    "text": "Okay. Noted. And then we will expect coming out of today that things like the IPN URI scheme and the network management architecture would be going into working group last call. So if you have not read those documents, please do. please catch up on them and prepare for a review of that as we go forward. That's what we have. Alright. So our next one is to go to Rick, for pen scheme update. my So good. Good morning, everyone. the open scheme are updates. I thought I would try and just run through the major differences between the IETF 116 drafted the document, which was o 1, all the way through to where we are now, which is o 6. Next slide, please, all the flicker. Let me some No. Mhmm. I can click Sorry. Excuse the technical difficulties. Okay. So the substantive changes between Where we were at? One one sec. Sorry. Drafto 3206. Really, the substantive changes have been around the definition of services. and the registry put in place in this document 4 services and what they mean, what we mean by services, and how we manage them. The other major fire registry advice to designated experts. So The conversation on the mailing list proceeded well. There was lots of discussion around this. to the point where the technical detail advice for the designated experts is pretty clear and noncontentious. the governance aspects around how one hands out fairly these allocator"
  },
  {
    "startTime": "00:10:01",
    "text": "flies to an organization, definition organization, etcetera, etcetera. has become the sticking point. And to that point, I'm not going cover it now. We're putting a dedicated slot in the agenda, as I just said. to cover that in particular. So I'm not gonna talk about it now. Next slide, please. So services. The o three draft talks to the service in terms of a type of service as in a a particular go through it. Instead of considering a service as some kind of generic service type, let me or some kind of protocol identifier as in, like, a TCP port number is each GTP, you would expect that protocol to be on that port. The way we use services in IPN naming is the identifier of some logical functional unit irrelevant of what that protocol is. It is some subprocessing unit on a node that requires a unique endpoint so that it can be identified. And that is different from what was written in the o one to o 0028 3 drafts, and having reached consensus on the mailing list, that is what we now Scribe, which I believe is in line with current practice, the expectations of the working group I'm letting up on just read that definition at the bottom. Anyway, Next slide, please. Given that definition of a service, we then have to go on define what do we mean by a well known service? and after some discussion about what well known means, we broke it down into a criteria of The purpose of a well known service is so that there can be a reasonable default assumed by a deployment so that you don't have to configure the same numbers for things which are always found to be on those service numbers. So it's about promoting that familiarity, that default in managing the"
  },
  {
    "startTime": "00:12:01",
    "text": "operational overhead of deploying these systems. So given that, a well known service does have to be truly well known. So it has publicly specified and have wide adoption. if you have a service which may be very important to your individual deployment, your individual mission, your individual environment, it only makes sense to you. You don't need to go and register it with Ayana. but That's a waste of Ayana's time, and It's also it populates the registry with things that no one else is ever going to use. So the point of the registry and the onus on you to go and register something is if what you are registering is properly universally well known and has wide adoption or is designed for wide adoption. So we hopefully reduces the the work involved with organizations who are developing their own services for their own special needs don't have to go and register these things. So we clarify what we mean. as a well known service, and then we clarify how the registry is used. So it's By registering an entry in the service numbers registry, you all saying, unless you receive configuration otherwise, it is safe to assume that service x will be on service number y. It may not be, and you should handle that case. may be a different service on that that service number, and you should handle that as well. but it is a reasonable assumption to make that it's probably there, and that is the best that you can do with this registry. and I see Scott Verde jumping to the mic. Scott Burley, I I think the direction here is is right. I'm concerned about interoperability. that if you that if it says Fuzzy as this that interoperability errors will be introduced"
  },
  {
    "startTime": "00:14:03",
    "text": "regardless. I I think it's a sort of thing where you you do need to nail some of these things down. or I agree there is a danger of or or there will be operational problems? intra profitability here. and this is quite a loose generalization, but I I would suggest this is analogous to TCP port numbers, where yeah, https is on 443. unless you specify otherwise, but equally, I don't there is nothing guaranteeing that the thing I'm talking to on TCP Port 443. is a a TLS speaker. So I can guess and go to it, but there must be something within my protocol that then has choice to establish that communication path and with BP, it's a bit gentler than that. that understands, oh, no. This isn't. what I was expecting and falls back. So I don't think we can universally force you must use service number 1 for DTNMA. and you must never use service number 1 for anything else. I can't personally, I can't see that being, a, enforceable, and be I think it would be ignored. think there's I wanna use 1 because I don't run DTNMA within my environment. I I wanna use my magic service number. That's fine. scupperly again. absolutely, you can't you you can't really enforce it. But the but then almost everything you know, protocol specification can't really enforce. Right? It's it's -- Yes. So let's try not to. Right. So what I'm I guess, suggesting is that it'd be desirable here these provisions to have the same force and intent as as as the shells. in a protocol specification. And That's that's really what I'm we'll come onto that, and I agree. Okay. arguing for here is that this be sort of"
  },
  {
    "startTime": "00:16:00",
    "text": "shall ish. Yep. better. Go ahead. Great. I'll go to Montelia here. I have a a marginal question. Do do we really need this specified now And the reason I'm asking is and we at the risk at us over a specific find, you know, something that might not be as required right now. and creating, you know, interoperability issues because of that. Again, I I agree with the sentiment of what you say. We have the CBHE service numbers registry already. So there's prior art. that as part of the IPN update, we need to address the existence of that prior art to say, do we do given there's a CBHE service numbers? Registry, what does that mean in BPP 7 what I think the document is doing correctly, you're saying, here is a registry. this is what the registry is for. This is how one should use it but not actually suggesting any there is nothing there are no initial values in that registry. at all. So I'm hopefully not enforcing anyone to do anything in particular yet. but providing the facility to have such a thing in place for when it becomes necessary. And I think DTNMA may be the 1st consumer this. I move on to the next slide? And, hopefully, that will address Can do have Mark in the queue. some some of this. We go ahead, Mark. Sorry. I can't. Actually, can I do the next slide and then then take some questions. So the Next slide. very quick, notional CBHE service members registry. will continue to exist. It is untouched. We are not updating the CBHE service numbers registry. A service from a c v a CVAG perspective was a subtly different concept,"
  },
  {
    "startTime": "00:18:01",
    "text": "there are a bunch of allocations which made sense for BPV 6 make much less sense with this new concept of a of a service number. mashing the two together was gonna make everything harder It's easier to leave that registry in place. The existing allocations are significantly fine for bbb6. moving on. So the new registry is called the IPM scheme URI, well known service numbers for PPV 7 because that's such an easy thing to say. and that makes it explicit. that this is for bundled protocol version 7 use of IPM URI. If you wanna use IPN URI for something apart from bundle protocol version 7, you're probably gonna need to go and define your own service number registry. So we're we're scaling back the scope of this registry to BPV 7 only. because kind of going towards Scott's point about interoperability. And after much discussion, the policy the allocation policy on that registry is designed to share the number space evenly between private use so that operators of deployments can go away and use a whole bank of the numbers to their heart's content for their own purposes. And various controlled values with various and I'll get on to it different levels of restriction on how one achieves an allocation because an important consideration is the seaborne coding of an unsigned integer, which is what a service number is. ranges from naught bytes to 4 bytes. depending on what the value is So the very the numbers in the range 1 to 24 are far more desirable if you care about encoding length than the numbers above 2 to the 16. So, therefore, we can't say, make all the big numbers private use and keep all the small numbers for specification required. because we are penalizing people who don't want to use the specified services into using"
  },
  {
    "startTime": "00:20:05",
    "text": "poor poor poor to compress number values. Hence, we're doing a a cut 5050 across each of the bike links. Next slide, please. I have tried to diagram this in a different way from the draft to try and express the fact that with seaborne encoding of unsigned integers, you get an encoding link from 0 to 4 bytes. Ocatez, sorry, ATF. NORt is a funny one because the tag, the the the type error is actually comes in that first leading bite giving you four bits. So it's it's kind of a naught extra architects, what we've said is values 1 to 24, which is the note by decoding. They're free to use. They're they're private one is gonna specify any stand approach a couple The one opt out in coding 25 to 127 is still private use. There are systems with their own service numbers, we can't get them to to, you know, constantly trample over them or withdraw withdraw that. but having these short service numbers is is very useful. And so we're reserving 128 to 255. So the top half of that one offsetting coding for standards action. So that is harshest and the highest bar of adoption. So that's standard tracker RFC. Not experimental, not by RTF. it's an IETF. It's gonna hold that space. That's ours. However, for the 2 byte, to architect and coding length, half of them, private use. Great. Go. Go. That's all there for you. You have lots of different services. You can do clever things with femoral service numbers on demand, etcetera. but we're gonna keep the high half of that 2 octet encoding, because a specification required. and these track back exactly to texting, and I can't remember the RFC number now, but the the see on how to write iona specific how to get these wordings right, these 8126. 8126."
  },
  {
    "startTime": "00:22:00",
    "text": "we go. And specification required means you need to publicly document your specification. It doesn't need to be our ETF. It can be any other SEO. And I'm I'm not going to quote the text explicitly off my head because I can't remember it. but it is a much lower bar of entry. it still maintains that. open specification widely adopted criteria we said for putting entries in here. And anything of the 34 encoding free for all, we're never gonna specify anything they're probably used. think that's the last slide. is the last slide, and we have Mark and then Alberto in the queue. that not lose it. Well, it's it's a comment about previous discussion, but again. Again, again, still loads, I guess. like, in the early days of Internet, you know, we You don't necessarily know what with with that then. So It it's a Therefore, we may force for standard actions or in our c for a protocol and and at the end, never nobody deployed it or someone deployed it for you know, sometime and something better came up. And so there's kind of a in perfect, you know, allocations here. We just need to make sure that the actual number spaces will manage, but that's it. So because we can find examples of, you know, all cases and You know, there's nothing you can do with I completely agree. I think a just enough pragmatic approach given we don't know what the future holds is what we have to do here, we could be far too tricked. and I think that would be a mistake. Alberta,"
  },
  {
    "startTime": "00:24:04",
    "text": "Alberto, can you go back to the slide? or gold moment. 20 supply. Which slide? 5? The next one. the service phase 3 example. Next last one. last time. And that's why. Yeah. So Rick, I'm curious about don't. is is the intention that the b pv7 registry would starting point would be the existing bpv6 register Nope. And now where is that? Because The for two reasons, the existing CDHE registry was very much talking it it had aligns the concept of the protocol that will be used will run on this service number with the definition of service. and we're being a little bit more verbose in our definition and trying to be a little stricter. I don't wanna say better, but because I know the author sat behind 3 rows back. But retrospect, we we think we can soften the definition of what it means and, hence, carrying the values across may not make sense. Also, the CBHE register was built for BPV Six which is SDNV, which has different encoding characteristics. So having CCSDS, and I'm not pointing a finger at them, pointing a finger at them for for anything malicious, have a reservation of a 100 to 254 in the CBHE verse service numbers. perfectly perfectly valid. That is a huge chunk of the very precious numbers in bpv7, with as far as I am aware, and I don't see into CCSDS documents, I can see no public specification for these. So I don't know what service number 191 in the CBHE registry is, and i. e. I can't implement it."
  },
  {
    "startTime": "00:26:01",
    "text": "independently. That is not a criticism of CCSTS because at the time, they followed the policy exactly as described correctly, But, with BPV 7 with this slightly altered version and slightly different policy, it transferring directly across does not make sense. Okay. I I thanks for the clarification, Rick, and just wanna data, a little bit of disagreement here just based on the fact that you know, space applications are an antenna for that matter. as a registry. an organization. We're gonna be -- Mhmm. -- driving a lot of what get standardized or the fact that's standardized by usage. because of from the space agencies, So introducing weighting changes to me are you know, it's a it's a huge deal because Yeah. At at the end of the day, what we will be is incrementing cost of operations just by ensuring that, you know, service numbers means 1 year and 2 there. I agree with the point that you made on on the the system allocation might be too large, but that's something to discuss. you know, probably that we can discuss with them. Yeah. So I'm gonna keep my answer quick, quick, this is not a breaking change for two reasons. 1, that allocate that registry was for PPP 6, and I know people said, oh, I'll just assume it keeps going for BPP 7. that is semantically incorrect, but it doesn't matter. But if you actually look at the allocation for private use of the 1 byte encoding, it covers that CCSDS allocation. So they can continue to use those numbers quite legitimately they're just now private and a different deployment by a different a different deployment by a different may use those same numbers and CCSTS need to be aware of that, but I don't believe the existing deployment by CCSDS are designed to interoperate with other BPV 7 deployments out there at the service level."
  },
  {
    "startTime": "00:28:03",
    "text": "So it's a safe assumption. I understand there is pain in change, but we the change is necessary, and we have attempted to minimize the impact. so just to jump in, 2 things, a a comment, and then the time. The comment is if if you look at the CBHE service registry for Ayanna, there is a single allocation for CFDP. Yes. And if you look at the Santa allocation, which is delegated from Ayanna, there are only 2 registrations, which are also for CFDP, but using a different number, for the CFPP sender and receiver. and I I'm not sure if those are actually the operational service numbers being used for CFDP. So when when we look at when we look at the actual number of service numbers and well known services. there are 3, and they are all CFDP. So we think that the overall impact of this kind of change based on what is registered today is small. But my second comment is we are out of we are well past time on this, so please let us take it to the mailing list to continue the discussion. Take the mailing list. Thank you. Thank you. Alright. Next up, we have Sarah Hiner, who is going to be presenting on the DTN Management Architecture update update update update update update update update update update update update update update update update update update update update update update update update update update update update update update update update Sir, are you are you here, and can you hear us? Yes. Are you able to hear me? Yes. Great. So like Ed said, I'm gonna speak to some of the dates that have been made to the DTNMA since I last spoke at the once team meeting. Next slide. So the DTNMA is now on version 6. and the updates that were made to the document are to reflect the comments that we received from some reviews. So thank you to folks who took that time, including the feedback"
  },
  {
    "startTime": "00:30:00",
    "text": "from the ops area. And these changes are clarifying they're not changing the architecture itself they add clarification around primarily decisions that were made when designing the DTNMA and they're focused primarily on the autonomy engine. and the rules that we use for management. Next slide. So getting into those changes, First, we made some updates to the discussion of the existing network management approaches. because the DTNMA is a new approach, to provide autonomy asynchronous Management, and address the need for concise and coatings. we need to discuss what already exists. lessons learned from those approaches what could be used from those approaches. And then where we do need to depart from those approaches to provide the combination of those 3 characteristics. So an example that I pulled onto the slide here was that we updated our SNMP overview to explain these of event notifications, Their purpose is to decrease reaction time, not to provide autonomy at least in SNMP, and that's something that the DTMA requires. Next slide. We also made changes to hold the DTNMA to its intent to serve as just a architecture document. So any details that model have been moved into I believe the app education management model, an operational data model which is documented in the ADM draft that Brian's going to be speaking to next."
  },
  {
    "startTime": "00:32:02",
    "text": "Next slide. So now getting into the bulk of these updates. We added some clarification around the DTNMA agent autonomy engine. We're now describing that as a policy execution engine. which was recommended so that we can emphasize the use of pre shared policy. in the form of rules which is received by the agent from the managing device. And also address what a manager can you're in that overall autonomy model. So we show the conceptual difference between the manager configuring the local rule database then the configuration of the autonomy engine itself. So, for example, a manager might be configuring the rule database by defining a new rule removing a rule but then can configure the autonomy engine by enabling that rule as executed. Next slide. then to address the observation that using the term command based management feels a bit general and could be interpreted multiple ways we're now using the term role based management, trying to be more specific here, but the behavior that was defined is still unchanged. So so this is still a rule based system that allows the agent to issue commands on itself for the purpose of both self management using that local autonomy engine, and then also to execute commands received from remote manage Next slide."
  },
  {
    "startTime": "00:34:05",
    "text": "We're also asked to consider the event condition action format for those autonomy rules, since since since rules, rules, could be used to react to an event and adopting that format would allow rules to be grouped by events. So the approach that we took here was to expand the discussion of the capability is provided by these autonomy rules. In all cases, the DTNMA is still remaining at that familiar stimulus response system, where that rule definition includes the stimulus, which is built from 1 or more predicate logic expressions. and then that preconfigured response which defines the actual procedure that's being run when the stimulus occurs. But to but bring in that beneficial grouping on events without a discussion around expanding abilities within that rule stimulus. So the stimulus can include a common condition that's shared across roles, And there's an example in the lower blue box on my slide. and this is allowing bulk evaluation of rules and it's also providing an important efficiency as rule sets start to grow. Next slide. So digging in a little bit further to potentially large rule sets We also updated the language around the autonomy model characteristics sticks, and how they address the challenges of managing those large rule sets. the interest of in time, I'm just gonna run through a couple of these. So first characteristic is strong typing."
  },
  {
    "startTime": "00:36:00",
    "text": "2nd is asic like dependencies. So we want to address the potential for those more complex rules stimuli to require combining predicates. if that occurs, we could be creating circular dependencies. we need implementations to include a mechanism to prevent that from occurring so that we can have those more complex rules stimuli that allow us to group rules in that beneficial way. And then, also, an indication of if data is fresh is a key characteristic and that's going to help prevent the agent from incorrectly inferring some sort of operational state. as out of that information that it is monitoring. And the next slide which is continuing with those characteristics. We've included some thoughts on the need for prioritization, and that's providing flexibility for specifying autonomy model objects, objects, while allowing less objects overall to be defined which again helps with the management of those larger rule sets. and then also speak to configurable cardinality and control based update. as important characteristics with those control based updates allowing the state of the managed device to be changed. using controls. And then last slide, So the last update to highlight is around conflict detection in rule sets. Since the DTNMA allows multiple managers to configure an agent"
  },
  {
    "startTime": "00:38:00",
    "text": "school conflicts have to be both detected and prevented. So the DTNMA is now making the recommendation that implementation include a conflict resolution mechanism and I've added a couple of the examples that we give a discussion of some of those potential options like using time or some item in the managed state data. to decide which manager input would be selected over another. if a conflicting rule set is identified, but we're not prescribing a single solution here. to continue with that trend of providing flexibility in the DTNMA wherever it's possible. so that we aren't creating a brittle management system and levying some sort of constraint on the system that it doesn't need to necessarily accommodate So with that, Just wanna thank everyone again for their feedback. taking the time to read through the document. and then formally request consideration for the DTNMA to move into working group last call. So we we can we can handle the request for working group last call on the mailing list. We will we will put that out but but Now while we are here together, does anyone have any concerns, or observations on the work so far. Okay. Excellent. you very much, and thank you, Sarah. Thank you. Next up, we have Brian Cipos, who will be covering three topics for us. The first one being the data model that is conformant to the"
  },
  {
    "startTime": "00:40:02",
    "text": "management architecture update. Let's see. Alright. So what I'd like to do is to cover All the major topics give an overview of what's what's knew what's not new, And then it comes in questions afterwards. So next slide, please. the Earlier drafts of this thing had called this asynchronous management and and had used mother names, in between and Some of these names have carried over to implementations. And what the goal here is to keep as much of what already exists in the same logical form, and some cases in the same Form to implementations, focusing on keeping stability in the agents and the overall goal of the whole thing is to enable the the management architecture of the DTM to be as easy straightforward to use as existing legacy management systems. So the the goal of all of these updates and changes is to make it more familiar, more expressive, And overall, more difficult to cause disruptions or cause problems. in the managed agent. Next slide, please. So, one big thing in terms of usability, and expressivity on the data model side of things, was to do with typing. the earlier ADM typing was quite narrow, and specific and didn't allow an individual application to Taylor what but could be specified in its ADM to its specific needs."
  },
  {
    "startTime": "00:42:03",
    "text": "And so there was a lot of rigidity, and there was a lot of forced narrowing of what you could do And then what that led to was a a fragility of the management tools because if the only thing you can express in a ADM is that it's a same integer, 32 bit integer, But in fact, there's a lot of restrictions on it or there's other metadata associated with it The manager tools wouldn't be aware of that, and so it couldn't enhance what is shown to a user. It couldn't embellish the data with restrictions or with units or with any kind of additional information. One of the thing that was missing from the earlier data model Was the concept of how to handle failures in a way that's compatible with the whole architecture. And the concept here is that this is not a synchronous system. So if I request a change, Or if I request a piece of information, and the agent can't comply with that, what do I do? So one of the type changes is to reuse at the notion of an undefined type, and undefined we'll talk about it later, but it lives a little bit outside of the normal data model in can't say undefined as expected, But according to the the model now undefined is a thing that an agent can give you. If you ask for a thing and the agent can't produce it, rather than failing with no, explanation, the agent, will tell you You asked for x, the value is undefined. So at least you can continue on with the normal processing And then Part of this is to strengthen rules for implicit and explicit type conversions. and create a way for the ADM to construct type Unions and and add lots of embellishing metadata to to these types. so that this is something that an agent doesn't care about. An agent does what it does with the data, but this is all to make the management side"
  },
  {
    "startTime": "00:44:00",
    "text": "easier. the processing models for macros, expressions, and and port templates have been Strengthened, especially in the flow of activities so that if there are failures, failures exactly when failures are processed, how they're handled, and things like that. the the processing model is is stronger now than it was before. this is something that right now it's a draft. We'd welcome some trial implementations, and feedback of existing implementations on this stuff because this isn't the right answer, but the idea is that it's a a common and and known processing model model. Let's see here. the there were some feedback that came from possible implementations of this thing that the autonomy capability was in a sense, scaring people away from acting on the non autonomy aspects of the management, architecture. So we're separating the autonomy capabilities from the core of what it means to be the agent. and we'll talk about it later on but it's done in a way that is part of the the model itself. So it's a way for an agent to say, support autonomy, use those controls on me. versus an Asian 2 people say, I don't support autonomy. You can't do that. And then overall, the goal of this thing is to simplify and where possible, Minify the actual messaging things. So so where the type is unambiguous. we provide a mechanism to align type information where the type needs to be there. we make requirements about where it belongs. So it's it's strengthening up requirements and allowing some optimizations that in the past, were possible but not specified. Next slide, please. So some of these slides I'm gonna go"
  },
  {
    "startTime": "00:46:01",
    "text": "pretty quickly through we can go back to them a bit later. But There were a bunch of types. There were a bunch of because the the previous type model was very rigid and fixed, and hard related to the encoding of the values. There are a bunch of types that are highlighted in yellow here that aren't really types in terms of encoding, but they are types in terms of how you handle the data. And the new model is to separate those two things so that a literal type, is about encoding, and it's about processing. and a semantic type is about how is the data handled. separately, so that 5555 there can be interrelationships between the two but they are they're separate concepts in Next slide, please. I'm not gonna go through all of these details on the slide, but The concept is that we're strengthening up the the type hierarchy and we're using a notion that comes also from yangdatamodeling and other systems like that, where we have this notion of a typedef. An application can define a typedef. And one option of a typedef is to define a a union So in the past where there were there were need for some playing around with multiple forms of different operators to do the same kind of thing on different sort of data Now we can define a numeric type, which is a union of the values that can be numbers and then we can define operators to say, or or controls that say, this parameter is a numeric type. and we'll explain how the operator or how the control differentiates on what it does on these different things. but the type system cannot enforce the fact that Numeric doesn't mean anything, means one of these very specific primitive plates. similar things apply to the EDD's controls. and And other places where the types are used is it it's strengthening up what is allowed to be in this value. And the other thing that's"
  },
  {
    "startTime": "00:48:02",
    "text": "that's supported by eventually, we'll talk about the syntax, is this notion of an anonymous type it's it follows the same definition model as Yang does currently, which is If you make a typedef, it's great. You can reuse it. You can share it. But sometimes you just need to say, this parameter is an integer, but it's gotta be between 5 and and 8, And an anonymous type lets you do that in a simplified way. Next slide, please. I talked about this a little bit earlier. difference between a literal type and a semantic type There's details on the slide, but the the basic concept is at the bottom line of each of these things, which is A literal type is something that the agent cares about. It's something that affects data handling. It's something that implementations have to change. So the literal types are something that's tied to implementation, and semantic types are things that A person very much cares about and the manager would do well to care about but the implementation of the agent doesn't need to really know or care about them. So the agent is gonna do what it's gonna do, and it's gonna have its own logic of a value is supposed to be within a certain range. or this can be text or this can be a number, but the the manager is the one that that is is is enforcing this in a general purpose way so that We're not writing manager plug ins for each application data model the manager handles this data symmetrically across all ADMs. So the manager understands what a numeric range is. The manager understands what a text pattern is, and things like that. Next slide, please. we talked a little bit about undefined earlier that it's it's something that slots in with this pretty well because we're talking about using seaborne and seaborne diagnostic connotation."
  },
  {
    "startTime": "00:50:01",
    "text": "to represent these things. So we get this basically for free by our encoding method but we define semantics on this now, that that that that explains to an agent when would you use this value and explains to a manager When must you? handle this value. And then separate from the undefined type and value. is again, again, because it's seaborne we're getting this without extra handling cost, a null type and a null value. to allow us to express things like optionals, and we can get into I can talk about details and specific ADMs, but the idea here is that Some data models, genuinely have optional parameters. or reports can have optional contents. whereas undefined is supposed to represent more of a failure and off normal situation. a null pipe that you can union in and say, it's a number where it's null. No is a positive indicator that I'm not giving you a value. It's it's null. I'm not giving you the other thing. And, also, all of these Alright. new notions of semantic types and all this stuff. It doesn't impose on any of the existing data models. It's something that In some cases can enhance or or embellish what's already there. but none of what we're doing here is going to break somebody's logical data model There might be some pointers to existing ADMs to say, well, Would you want to use a typing in in this place, or would you want to put some units on these numbers that that are valuable to know foot range restrictions, but we're not imposing these things on on authors. Next slide, please. One thing that came up as an issue in Understanding and and implementation of earlier processing is to do with time types and that earlier"
  },
  {
    "startTime": "00:52:03",
    "text": "Handling of time types was related to the type system itself. And so one of the changes here, is to separate absolute times from relative times. These are now 2 completely separate literal types. But in cases where a model, would like to handle both of these things because there are cases where in the past models could accept either absolute time or relative time, the semantic type Notion now lets us say, for a specific piece of a specific model, This parameter can be a absolute time, or a relative time. but what it also allows us for certain places to say, you have to give me an absolute time. I don't I can't process a relative time. I don't wanna look at a relative time. So it allows us Similar to I was talking to earlier, to to nail down very specific aspects so that a manager is gonna be doing the right thing. You can validate things for how an agent is gonna handle things. And then the other change related to times is It's shown here in a a cddl expression. but we want the ability to have a compressible form of narrow precision times, In the past, time was handled with a least precision of one second. One second is good for some applications, but it's very limiting for other applications. So this, says, that time is something that can use an existing the the fractional representation is something that's carried over It was already part of Seabore Extended typing, we're just reusing the same structure. Now There's also text representation of these things. So part of this enhancement part of why these are literal types is that As an embellishment on the management side, We want to not have to make people rely on external tools"
  },
  {
    "startTime": "00:54:03",
    "text": "to do conversions between numeric times and human friendly text times. So these are part of the literal types, The so that the tools themselves know what a time is and know how to reference it against a specific epoch. and a note at the bottom. There's batch, the fact that there's now fine grain precision of time means that there's value in having a an agent be able to expose its notion of what kind of time precision can it handle? The the concept here is that I can always represent my times down to, in this case, an nanosecond, resolution, doesn't mean an agent is gonna be able to enforce rules an nanosecond time, or they're going to be able to act on them in other ways. down to that precision. but they have to handle them down to the name a second. slide, please. Next The next few slides are gonna Talk details about processing. But Generally, the idea is that nailing down specifics that in the past. We're not fully specified what that meant was that different agents were free to do things in different ways. And in a normal situation, that wouldn't make a difference. But This is talking about say, in a macro execution. if one control or one aspect of the macro fails, How is the nation supposed to handle that? supposed to stop? Is it supposed to continue? So now that all is specified, And this is something that's open to discussion. there's a explicit specification here. Alright. It's not a right answer, but it is a concrete answer. And then there's also some requirements about what an agent is allowed to do to say things like to for a manager's point of view, if you tell me certain things in certain ways,"
  },
  {
    "startTime": "00:56:01",
    "text": "am I allowed to do things in parallel, or do I have to do things in sequence? Those are all now specified. Next slide, please. Expressions are handled similarly. that there's now a staged processing expression so that If there are references and expression, those get handled first. So we don't get halfway through processing expression to find the invalid reference, things like that. and The expressions are now strongly typed. So in the past, there was a specification that said, Your expression ought to have these things in it. only these things. Now with the semantic type syntax, there's a way to say, Enforced in the machine An expression is a very specific thing the machine will tell you on the manager's side, did you try to do something that you should not be allowed to do? versus letting it go all the way to the agent, only for the agent to fail out. Next slide, please. And there's similar updates to the report template processing. for the same thing is that report templates have to have specific content that's now enforceable through semantic types. on the manager's side as well as on the agent's side. And there's a change of how this thing is processed so that a report template now is not a separate top level object type, report template is a piece of data, that that that if you put it in an EDD it functions the same as it used to, but you can also put a report template a variable, in a variable, And you can also put or or template in any other place where the semantic type value we'll get and we've we've pulled together, the report and the table so that The concept is that It's more it it's fewer top level object types, so it simplifies the data model. but it allows expressive restiveness that"
  },
  {
    "startTime": "00:58:01",
    "text": "We restrict where these things ought to be used in certain ways. but it lets an application potentially do some interesting things that might not be advised, but it lets the application, figure out what is best for its purposes. Next slide, please. And then separate from the handling of those different types, We put concrete requirements and value production. There's a whole section in the document about this. to say, costs, variables and and external definedataEDDs. They have common characteristics. They have common processing. There's certain order of things. If you give it a bad object name, if you give it a bad parameter, They're handled in similar ways and in concrete ways now. And what this part of this change is to allow parameters, under certain circumstances, for constants and variables that are handled to allow report templates and expressions and macros to be themselves very much So it's it's simplifying the object model but expanding the expressivity of what can be in the object model. Next slide. I mentioned this a little bit earlier, but separating autonomy from core behavior that in the data model itself means that I can implement an agent without autonomy, in a way that doesn't have any time based processing at all. So I can build a completely deterministic agent, a very, very simple agent that doesn't know or care about time per se, but it implements the full Notion of what it means to be a DTNMA agent. and I think that's has a lot of power to it because there are certain circumstances where we want to be able to exercise the full autonomy level But there's other circumstances where in a constrained system,"
  },
  {
    "startTime": "01:00:03",
    "text": "I wanna be able to talk to it with a standard protocol, and this gives us the ability to do that. without requiring even a real notion of a clock on the agent. and things like reports can be produced on a fixed relative timer that has nothing to do with autonomy. Next slide, please. gonna go. quickly through these next ones too that What this allows is as part of the the ARI changes, It also allows things to be more condensed in terms of the messaging, We're keeping the concept of messages, message groups, and all this stuff. but it's now reduced to to An execution message that goes from manager to agent and a report message that goes from agent to manager. and there's there's there's complexity in those messages, but it's it's simplified things down significantly from how it was before. and reports now as previously defined contain a bunch of different information that serves different purposes, but it's all the same format. And this is similar to what was done with the the typing system between the literal types and semantic types is We want the encoding to be as simple as possible but We want the applications to allowed to use this to do complex things. Next slide, please. One thing that still needs to have a wider review is that we're now encoding ADMs as a profile of the yang modeling language. So the way this operates is that we inherit a bunch of really useful behavior from Yang. we inherit the contents of modules and some modules in revision history. prefixes and imports, features for doing conditional implementations, all of the life cycle things related to status and references"
  },
  {
    "startTime": "01:02:00",
    "text": "subscriptions, subscriptions, subscriptions We wanna reuse those things. And we want people who's familiar with Yang, be able to understand what's happening here. But then we're prohibiting and replacing other existing yang behavior in a way that does not change the syntax of yang. This is all within the letter of the law, of a yangmodule So we are not using Constructs which are specifically associated with Netcoff, We're not using actions, notifications, and RPCs We're not using isolated data nodes, or the concept of data stores, and because of this, we're saying, this is a separate ecosystem of yangmodules. We're using the syntax, But but we're never gonna import a legacy yang model into an ADM module, we're never gonna import from an ADM module into a gang legacy yang module. So because there are separate ecosystems, then we are not We're not concerned about the people who are managing the existing yang ecosystem do not need to be worried that this is going to start causing conflicts and causing confusions. Next slide, please. as part of inheriting the Yang syntax, we're also talking about inheriting some Processing and semantics of Yang, of how are modules allowed to be implemented on agents. power modules allowed to change across revisions, And there's actually some interesting work currently going through in the net modgroup that clarifies and puts versions and compatibility information into Yangmodules, And we would love to take advantage of all that stuff. Anything that's to do with the module level processing, we can reuse wholesale and get good benefit out of it. Next slide, please. And then before we go, we we do have Lou in the queue."
  },
  {
    "startTime": "01:04:04",
    "text": "pile, Hi, Lou Berger. I was I was I'm a little confused by these two slides of what you are what you're trying to avoid with not taking Yang wholesale. it sounds to me that you're actually saying you don't wanna use Yang transport mechanisms or encodings. but yang is okay. and you also said you wanna but like, not bringing along baggage of yang. And I I I so don't know if you're gonna talk about that more a little bit or talk about why you know, what what you're really trying to accomplish, I I get actually the transport, an encoding thing, wanting something completely separate from NetComm Rascom -- Mhmm. -- maybe seaboard seaboard, maybe GMI. there's a lot of things out there, but you want another one? Okay. quite as soon as you start throwing away other parts of the language, you lose the benefit of reuse of tooling. And it means you have to get all your own tooling. So I'd love to hear a little bit more about that. you wanna take it to the list, that's okay too. Yeah. Well, I'll say briefly that What we're doing does not give you incompatible ability of tooling, but what it does is, as you said, means that there are existing secondary behaviors that you can do on yang modules, like produce a object tree, or produce, you know, information about the module. that if you run those on these modules, not gonna get anything. It's not gonna break the tools. But like you said, It's not the same use cases for those tools. I think I need to go read more because I don't understand. y? Okay. That's And the the the fundamental thing is we we need a syntax for these ADM modules. We need a syntax that we're not trying to reinvent the wheel of how do you create structured text. Right? but identifier names, curly brackets, nested definition structure We'd"
  },
  {
    "startTime": "01:06:00",
    "text": "we we we could have, and in the past, did with a JSON structure define fully what a custom encoding of an NDM. The goal here is to say, We don't wanna reinvent text formatting syntax. Yang is great. And don't wanna reinvent how to version a module. Yang already defines that and does it well. don't wanna reinvent. the concept of metadata and documentation internal documentation and import revision, you know, cross references We don't wanna reinvent all that stuff. we want to use both the syntax and the semantics. someone who's familiar with dealing with gang and net cost, can use this directly. it's using the same syntax in the semantics. What we're saying is We don't have any of the things that are tied to NetConference which are the data node definitions, the RPCs, and and things like that. Yeah. Yang NetComm isn't relatively one word. There's Yang, then there's NetComm. and they do not need to be linked at all. And yes, historically in that that that you know, the early days of Yang, they were completely late. Late. When I say -- Yeah. -- today -- Today they're not. And Personally, I never wanna use NetComm. When I say NetComm, what I mean is I do wanna use GAAP. something that is structured in a a hierarchical way that's representable in the way that NetComm Freshs, that's it. We don't We're not representing those objects. We don't handling those semantics we're not using those those syntax. But it's a legal thing to do perspective of our modules will validate ganglinters. So we'll talk about it later, but -- Don't you want from Yang? And I'm not saying what don't you want from NetComm. I'm saying what don't you want?"
  },
  {
    "startTime": "01:08:00",
    "text": "What are you not bringing on? And and chairs if you'd say this is too much of a rattle. I accept that. we don't we don't model and structure data the same way as before. And one of the fundamental I I mentioned it at the beginning, but one of the fundamental chain things of things have things have all of these changes, is that These changes don't affect the adm or the AMP agents the agents, have constructs called controls and EDDs and variables constants, constants, None of this, it all affects what is fundamental to the DTNMA and the agents. So In order to keep those constructs, to keep those structures, that's why there's a disconnect between So so to -- -- offline. That's great. Thank you. So so, again, I again, it it would be good to have these kind of conversations also on the mailing list. There we have had a couple of discussions related with some young experts. And the the real question is, To what extent does this represent a a domain specific language that we're trying to map? and whether it's better to have metadata for modules, and then the rest be a hard cut off to a domain specific language versus trying to intermix some things that we take and some things that we do not because that may cause more confusion in how we would be building modules. but but that If if you have some time after the working group, it would be a good sidebar conversation. and then something that we should represent on the mailing list. Thank you. So if you skip ahead to and then just an interest on what we can add maybe another 10 minutes for the Well, skip it. We can skip this talking about the remainder of the year. Yanged Yang and ABM specific details. Next slide, lifecycles annotations, The whole point is that Yang and the yang ecosystem do things very well and have evolved over time"
  },
  {
    "startTime": "01:10:03",
    "text": "to do things in a way that people are familiar with, and to do things that people need to do. And so the original goal was Wait. these are very useful constructs. We want these constructs when we looked at it a little further was Why don't we also use the syntax and not just the constructs. because We don't want to have to fully reinvent tooling for these things. So next slide, please. I think this is the end, but the idea is that There still are some open issues and things that are worth discussing. related to what belongs in an agent core ADM are things should additional things go in there? should things be split out separately? things like that. the the in ADM typing still needs to be worked out. this was something that we The current draft carries over this notion of typing and and grouping syntax from Yang But because Yang is based on modeling a 2 level structure were leaf types, are one thing, and complex, types are another thing. But that's not how the ADMs are structured not how the type system works. And so going forward, probably what's gonna happen is we're we're not gonna try to reused any of the syntax of the piping So Nobody will be confused about what is a a a leaf or list in the context of an ADM. not gonna use that syntax. We're gonna use different syntax that's specific to ADMs and then dotdot part of the what needs to be worked at is just terminology reports, Now kind of represent there's a message related to reporting. There's a content of a message related to reporting There's an entry, and there's an activity called reporting. and they they represent different things, and maybe we need some more detailed terminology to differentiate these things"
  },
  {
    "startTime": "01:12:03",
    "text": "Right now, what's in there should be consistent and complete. but but reading through their it could be confusing, and we wanna avoid confusion. And the one other thing that is did it. not been updated so far. is a concrete representation meaning a protocol is actually used between the agent and manager. It was touched upon earlier, and there is there's this notion that it's simple that there are reports that go one way and there's execution that goes the other way. but it's not been written down. And it it It's something that needs to be written down just so that it's a realizable thing. But the idea is that All of these other things are in support of making the notion of what is report message and what is an execution message to be ultimately, a collection of ARIs. That's the goal all this thing is that the AI syntax is the fundamental thing that's about the exchange information and the configuration of things. So ARIs are something that a human looks at. ARIs is the thing that machine handles. And I believe it's the last one. I just talked about the arrows. So just document editing is one thing that needs to happen now. and giving feedback on the contents as they are the simplified management model, the simplified ATM object model one step is to translate existing ADMs into Yangmodules, this is something that could be done relatively mechanically. but it's a question of is it worth trying to automate some of the stuff just copy and paste that And then the other thing is a trial implementation. We have existing agent implementations One of them comes out of NASA Ion Project and there are some others that are in in use or in development currently. But we'd like to"
  },
  {
    "startTime": "01:14:03",
    "text": "Take some of these tools and make these extensions and see see how it works in practice. The goal of all of these changes to the ARIs, to the ADMs and all that is to simplify. the goal is to make it so that It's less error prone to the implementation. It's less error prone to somebody using it. It's so as error prone to somebody trying to diagnose things. that's overall what these changes represent. I think that's the end. Okay. I I I will simply just also add, please, if you've not looked through. This update is a large and beneficial update. and and large. perhaps, perhaps, 90 pages larger. The actual changes are are significant portion of the document. Alright. Thanks. that content is to add specification where it was it was it was loose or missing the 4th So so critical technical review and commentary on the mailing list is requested And now to talk again about Koci, Francipos. Okay. Next slide, please. So at the previous working group, the Jose context was adopted as a a working group. draft, draft, draft, and the idea is that we are building on existing work in the default security context, which was very narrow and focused on symmetric key encryption, for that specific purpose of having a context that's simple and directly usable. But it's limited in the sense that We are not in any way able to operate in a PKI security environment right now. a standard way. And so the idea here is that And this was something that was indicated by the the sector when reviewing the BPSec originally that You need somehow to operate in a PKA environment because that's what the the the world uses. And because we don't wanna reinvent the wheel here, the Cozy Working Group, already defines a message format"
  },
  {
    "startTime": "01:16:00",
    "text": "that is made for this kind of isolated asynchronous. form of of messaging. So what this does is it it uses Cozay directly. It just says, Cose defines all these details about how to build and process, security messages. and we put them into a a security context. Next slide, please. and what this does is that it it provides a profile of cose to say, how do you interoperate this? It provides an embedding of co say to say, where do these things belong in the b p second environment? and this Now is something that is should be directly implement. We'll in a a VPsec Okay. agent. and The last changes to the draft were to expand the notion of AAD scope, the additional authenticated data, originally, And in the default security context, there was a notion of AED could be the bundled primary block, is its identity, and the metadata about the block containing the thing that you're integrity signing or encrypting, That's expanded now so that a, d, scope can can the any other block in the bundle So this can do something, like, for example, I can sign my payload but say, Also, I'm including these three other blocks that have to travel with the payload. If you touch these 3 other blocks, the payload will fail to verify. So it's it's enabling and and just do a single signature or a single Mac. operation operation, So it enables this more expressive content. And it's necessary in certain use cases where you're doing things like encrypting the payload but requiring additional blocks to be able to interpret the payload."
  },
  {
    "startTime": "01:18:03",
    "text": "Right now, the the document is pending last call. still looking for some any feedback that anybody has. This has been requested to be reviewed by the Jose working group on the mailing list. we have gotten some feedback but none of it is to do with changes in the document itself. And So right now, it's it's in a stable state. Next slide. So we talked about the motivations before, this This document doesn't address policy related to BPSec, just generally, So in a PKI environment. there are a lot of additional policy considerations. We've talked about some of them in the security portion of the document. But it's really up to implementations to to to define simple or complex policies that are that are needed in those situations. And Brian, did you have additional slides for EID patterns? No. There's no additional slides. Okay. Okay. So may maybe in the interest of time we stop here and then we can take the edit patterns for additional comments once there's any No. The the the other sort of question again back to working group is. This document is in working group last call. So while we are here, for those who have read the document, are there any questions or concerns with the document now because otherwise it is going to go through working group last call. successfully unless we see something brought up here or on the mailing list. Okay? type, you very much. I'm you, Brian. Yep. Next up, we have Mark Lache, who is going to be talking about the bundle protocol yang model."
  },
  {
    "startTime": "01:20:16",
    "text": "Good morning. for those local? Next slide. The rationale of this work is to be able to manage BP nodes using network management product such as. netcount and rest count, for example. Use cases are, for example, IP Networks, are deployed on Planetaryate bodies in Moon, for example, you could see the various space agency's architecture documents. Some of those BP nodes will be dual stack and not v4v6 as we know typically, but instead the BP and IP, So they could be manage i over IP locally. BP only nodes, so not on an IP network, can be managed also using risk on over HTP over VP. In that context, you don't need an an IP stack just an http listener or parser on the bundled agent as a service. The good part of this is you enable reuse of all network management tools, and can be augmented with any young work that is being done, for example, contact plans that will be discussed tomorrow at the TVR Working Group. meeting. and it's pretty pretty complimentary to DTMA, not competitive or in any way. Next slide. So the way we did it is to actually look at the data structures of some implementation in try to do everybody."
  },
  {
    "startTime": "01:22:00",
    "text": "and try to be as close as possible, but while remaining generic, and is currently that design or that young model is actually augmented by the contact plan. You could see the address. The third side of the year will be discussed tomorrow. next time, So that's the 3 of you. I'm not going to everything here. pretty straightforward. So I know it has a version endpoint in identifier. as you know, the various Properties of the bundle node as neighbors neighbors with their convergysseers, adapters, identifiers and transport. this is currently not fully complete, but you know, you actually get the structure and will be completed as as we go. You have read, write, and read only. We'd only as mostly querying the current state, and rewrite this 2 Actually, know, change the state Next slide. I think we have on the next one. Yeah. The itself, the convergence layer adapters on the the bundle node itself, and then the storage Then within the storage, there are bundles. Right now, they are read right, but you know, we'll look into comments, but probably not the right thing to do. but that very easy to change. And I think that's about it. Next slide. I got Felix Walter. as an early reviewer. few considerations. If nodes, sports v Six, BP6 and DP7 should it appear as a single node with 2 versions or 2 virtual nodes on one supporting each each version. or we don't care about the p c 6"
  },
  {
    "startTime": "01:24:03",
    "text": "I was thinking of a single node with multiple versions. It's not currently yet reflected in the draft. comments, please. a neighbor as a single CLA or may have multiple CLAs currently modeled as as a So it could be modeled as multiple neighbors or a neighbor with multiple CLAs. Right now, I So that's actually was coming from Felix and model with a single neighbor with multiple series. And think that's the neck the last slide. Looks like yeah. Yeah. So so looking for a working group adoption and comments. So so this is Ed with chair hat off. I'd there had been some additional work on defining managed information for bundle to call. I I believe Ohio University had published a bundle protocol Mib for BPP6. And I think that there is a older BP adm also probably for bundled protocol version 6. at Okay. Ed says there's an EDM for BP6 I've asked Ed, where are those EDM and never get them? Oh, I I can put the link in the chat. But the but the question is for maybe for the for the other Mib, Did you have a chance? Is there a significant is there information in the prior work that is omitted in this work I I don't know. I was looking for that, you know, information. So please provide you know, you know, Sure. Sure. I'll -- Provide links of the especially the minute I I was looking for that information. The other is a with Cher Heron, for a working group, We will post to the mailing list of the the request for adoption, but while we are here, Are there any a questions, concerns, or comments on adopting a bundle protocol"
  },
  {
    "startTime": "01:26:01",
    "text": "7 Yang Module, in the working group. Okay? Okay? Bye you, Mark. Next up, we'll have Eric, who will be talking through VP over Ethernet Hello. I'm Eric Klein. I'll be talking about the Ethernet CLA or just BPOE is what I called it. Next slide, please. So in some in some network architecture work I was doing, I had a situation where I had bundled protocol agents that were directly connected over an incident link. and and some situations over things that basically emulate a virtual link, some virtual private cloud stuff. certain types of RF links. And, also, at the more I thought about it, the more everything eventually grows support Ethernet at some point. or look like it anyway. And it seemed to me that I if I was gonna you okay with a datagram CLA that I didn't need to have IP and UDP in there. Some TCP might be necessary, but I thought I would save that for later. In some of these environments, MTU might not be a concern kind of a function of the workflow, but also general frames are a thing, and some links have sublink layer fragmentation reassembly that they do they support for their ethernet payloademulation. Next slide, please. So the proposal was to just put bundles and Ethernet frames. And I thought, well, we just need a dedicated Ethernet type. and some text around how to multiplex v6andv7. And that is basically what the draft there 0 says, With respect to Ethernet types, I did a little bit of searching to try to find out than anything had been actually reserved. I found some BTN"
  },
  {
    "startTime": "01:28:00",
    "text": "code bases. doing some Ethernet things. with ether types that were not in the IEEE registry. near as I could tell. That being said, the ETF has a established process with the IEEE, RFC 7042, and it's currently undergoing this And mean, the short version is that we basically need to have a protocol that advances to the ISG telechat. in the series. get to that level, then discussions happen between the two bodies. with next slide, please. Now there are some considerations here. It I think the more I thought about it, the more there's basically and certainly it was okay for my use case, but a single sender agent and single receiver agent. because it's basically source Mac, the desk Mac. either that or if you have multiple sort of agents that can all open SOC editor, LL. then they all have to sort of be okay with receiving everything and figuring out what to guard. I don't know. I don't know if there's some if additional source desk discriminators like port numbers would be required, that would necessitate some header. I don't. I don't. take take take take That's necessary. they're in a conversation I had with someone. There was a at recommendation at MTU still be a factor, or maybe there should be some built in level of pregmentation, it would certainly be very easy for me to copy IPV 4, ipv6 fragment header, text, that's an established thing. It works whenever the network allows it to work. is a separate conversation. Broadly speaking, you know, a question of, like, extensibility in general. I know I had some quest discussions with Brian, and he mentioned that maybe a a thing to do would be to pull out the multiplexing texts since 7166 was never updated. for VPD 7. This hasn't really been discussed."
  },
  {
    "startTime": "01:30:02",
    "text": "or in a document that was published. So I think maybe That's the thing that we could do. and we could save extensibility for that document. Next slide, please. So this is an individual draft. And I guess the question is, is any of this worth pursuing? And if so, how fancy should we try to get? Sometimes simple is best. Sometimes simple is too cool. So it's Rick, which I had off I really like this work. I think it's obvious that that there are use cases for this. made a comment about MTU and using a bundled protocol for presentation. I think it would be better if the CL itself did some fragmentation in reassembly. rather than that duplication of primary primary block, as you correctly pointed out. I think And this goes back to a conversation that I know you and Brian have been having, and I've been quite supporting as well. I think there are commonalities for convergence layers, not only in the in the interface they apply to the bundled protocol agent, but also in roles and responsibilities, particularly for unreliable underlying transport such as some kind of fragmentation and reassembly shoulda her occur in the sale. in my opinion. mean, it would be great if that was described somewhere, but actually also implemented for for the sale. But I I like it, and I really support this. Scott. Scott Burley, two things. One is I I think a long time ago, a prototype implementation for anything that convergence layer adapter was developed and then be a thing to look up and see if there's anything useful in it. The other is I think it could be"
  },
  {
    "startTime": "01:32:02",
    "text": "it could be helpful to harmonize this work with The CCSVS link layer protocol convergence layer adapters. because there may be things that link layer CLAAs, have in common that would be good cross checks against 1 another. and and possibly inform on another In particular, I know that there is a segmentation reassembly functionality in the CCSDS stack. I'm not sure where whether it's in packets are in frames, but it but it may very well be in frames, in which case, it It could be something to harmonize this work with with Is that is that using LTP, or is there a separate the blue book or green book number? No. They're separate books for of the CCSTS link there protocols. Probably, though, best one to work with would be something called the USLE. side, space link protocol, it is. And then there's a there are CCSTS, blue books, for that for that for that. and and think there'll be you you you worth being aware of anyway. Alright. Alright. Okay. So addendum to that chair hat on. Rick Tainer. somebody was to put forwards a informational document capturing, you know, what current best practice for current behavior, I would absolutely support its adoption. I think that that is would fit into the Chaucer. Hello? Luberger, I'd like to explore or understand a little bit more of the use case where this is needed You started talking about adding in transport protocol, but know, the whole the whole purpose of what you're saying is let's get rid of the transport protocol so we don't need it. So it seemed a little circular to me there."
  },
  {
    "startTime": "01:34:04",
    "text": "and 24, 28 bites doesn't seem like a whole lot of overhead. So you know, what's the use case here? It's not a whole lot of overhead, but you have to manage and allocate the IP addresses on both sides. communicate to that to both endpoints. and the use case was essentially if you just wanted to broadcasts and things on a on a isotropic omnidirectional antenna, just broadcast messages. to all receivers as a for uses as a control plane protocol. or send it point to point link over some RFF some RF links. between orbiting things. And if you just the control plane just needed to say, this is the MAC address of the other guy. and not worry about having IP addresses even even link local addresses would work, but it seemed like. Kind of, like, why bother? especially especially if I was gonna do Datagram stuff. But I didn't mean to add whole transport protocol, just the sort of fragmentation part. GCP gives you all the retry stuff and all the session. none of that is is is anywhere in any of this BPOE at the moment. U UDP gives you fragmentation reassembly also Yeah. Doesn't seem like a high cost, like local, yeah, you use more bits with v Six, but it's still isn't too expensive. 9. Yes. I don't I I'm That means you're link has to support IP as well. Yeah. I Does that mean your link has a support IP as I didn't hear the last thing. I think everything that I have to worry about does, so it's not really an issue. But -- Sure. I mean, but know, the underlying if if you could if you support Ethernet, you support IP. And, yeah, you don't wanna make your underlying network I'd be aware, because look at the Internet aware. Yeah. Yeah. Yeah. just IP on the endpoints, and then you don't have to do all this extra standardization and then all the tooling related to it. just just get it for free. I mean, if there's a use case where it's too expensive. Oh, okay. that that I I"
  },
  {
    "startTime": "01:36:00",
    "text": "it in theory, sure. And there's plenty of use cases where get expensive. by Is that real here? Is it worth Was it worth doing all this? It seems simple enough to me to not worry about But yeah. Alright. we could and Brian. I do agree with the earlier comments about fragmentation is a valuable thing to have in the convergence layer. and and and it's worth looking into. and and the the other thing about separating the BPV6andb7processing. it relates back to some of the things that Mark was looking at is right now, there isn't really a strong model of what does a dual stack node look like? Is it two nodes? Is it one node? So that would avoid you know, cluttering up their your your transport concern with your you know, multiplexing, Yeah. Yeah. concern. Yeah. think I'll yeah. I I had started a document at your recommendation to pull a bunch of that stuff out. put that in there. And then this could be, in theory, just to document the request and either type at that point. which which should be a one pager. And one more from Felix. It expands. Yes. Scott comment on the CCSCs. protocols got me thinking. So, actually, the fragmentation is what the other packet layer below Even in the space packets or the encapsulation packets, So I'm wondering because fragmentation for me in the network and bundle layers really, really bad because once you have fragmentation, you all these fragments, traveling all along the the network, I was wondering if they are is maybe an option or a need for having a kind of generic fragmentation or layer conversions there, which can run on top of several frame layer protocols that is based in protocols, but also things like ethernet or using GFP. framing procedure, other things like that. So that we could have a kind of really generic"
  },
  {
    "startTime": "01:38:05",
    "text": "fragmentation within the convergence layer adapter itself. think this would make seems to make a lot of sense to me. Do you mean that like, selective acknowledgments and retransmit also needs to be. part of things, you're trying to it was said it was said it would be, like, maybe get their LTP. Because it might be covered by things like LTP already. I I I missed that. the idea was to try to do generic reliability on top of framing c l's, so have a bit of a stack. they We do have 2 more comments. I've I've locked the queue just in the interest of time because we do have 2 more top you get to, but Rick and then Scott. So, Rick, very quickly, chair hat off. one of the reasons, Lou, one of the reasons I support this is with if you're going straight to Ethernet from BP, and this is a bizarre thing to say at the IETF, you don't need an IP stack. That's less code. that's less you have to validate. you can go straight from BP to ethernet. and back up again. Yes, Scott. the idea of sort of generic multi Ninkler, fragmentation reassembly, layer has some appeal. I think that that could be useful, and I think that As Felix was mentioning, the CCSBS. and encapsulation packets or space packets, one or the other. do that. now it might be for the moment. If somebody has a a CCSCS, Red Book. Yeah. Blue Book. You can book whatever number. That would be great. Thank you. you so much."
  },
  {
    "startTime": "01:40:01",
    "text": "So for our final two topics, these were more Okay. working group discussion topics. 1 of them is going over See. Where is it? applications over BP, And then the other one is a a governance discussion around allocator IDs. So for for this one, we have a a single slide that we wanted to talk through. We have 2 independent graphs, graphs, that have come up on how to place application traffic over bundle, one for SMTP, the other HTTP. These talk about how you can construct a payload so that a payload that endpoints would be able to operate in a delayed or disrupted environment, in which the payload would then be presumed to be carried over bundled protocol But otherwise, it is a construction of a payload to be carried over bundled protocol. They're working implementations to demonstrate that that the approach exists. And then there was a mailing list adoption call to say whether we would, as a working group, adopt each of these drafts and publish them within the the DTN WG. There was support for the drafts because finding a way to use both male and HTTP traffic in a delayed and disrupted environment is a a useful thing, and we would like bundle to be used more. And in particular, there's a lot of support for anything that allows us to have more application traffic running over bundles. There was a question on whether this falls within the scope of the DTM working group. because it is not specific to bundle or transport, it is the creation of payloads to be sent over a transport. And then there were additional questions as to whether The technical approach was sort of the the favored approach because it doesn't pull data out of the payload and put it into extension blocks or otherwise, use elements of the bundle protocol"
  },
  {
    "startTime": "01:42:00",
    "text": "itself, So in that, through through a variety of discussion 2 things came up. 1 was there's probably a value in an informational document that says if you are writing applications, to send in a delayed or disrupted environment or generally a bundled protocol environment. Theories are things you should worry about and things that you should do in your application layer. And separately, here are approaches that could be used for things like mail and HTTP. So in this section, my hope is that we could have a a discussion. or at the very least some questions to the working group. either online or or at the mic, The first one is is their interest and support in an informational document that would describe application layer concerns when writing and using and transporting over bundles, a delayed or disrupted environment. I see Mark, their hand up. Not bless you, troublemaker. No no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no or trying to have applications on bundle. applications, configuration, promotional document, Jorge, myself and Brian, we had a email conversation that started, I wrote something along those lines a while ago, all application framework, So I do agree we should have someone something about that. I could start one. with my if Jorge still, you know, have some time, it seems that it's more difficult nowadays with this new position. but but but but but but but but but but but but but but but but but but but but but but but but but but Anyway, anyway. yeah, I agree we should have something about that. of Is that prior"
  },
  {
    "startTime": "01:44:01",
    "text": "drink or after the other 2 is an interesting question. You know, we No. No. there's pros and cons on on getting that after we we have actually worked on real stuff. So you get lesson learneds I've fear a bit that if we do start too early where will be kind of a generic, boring stuff Yep. Yeah. That's Indeed. thank you. Eric, Ericcoin, AD hat off. Yeah. I agree with everything Mark said. I think we should do an application document, it would be good to also have some experience informing that document, but with, like, a generic IESG head on, not 80 for your group. Like, Trying to specify SMTP and HTTP. Somebody might ask about Charter and scope things like that. So that could be once you've already talked to smart ADs, Sir Rick Taylor Cheer hat on. responding. Absolutely considering application considerations. document to be in charter. I think it's an important thing for us to do to say how to use our transport well. 100%. If in order to, as Mark pointed out, to make sure that document actually has value and has been built on some sort of experience of trying to do this properly. we may generate the SMTP and the HTTP document. documents as a side effect of getting that main one right then if we have that document, we can then go argue with the ISG about where we act to the finish it off. but at least we have valuable information as text. So then a semantic discussion. Those also don't necessarily have to be adopted documents. Thank you, Edgar. Scott, Yep. Scott Burley, and then just speaking in support of the idea of having this document. I I think that"
  },
  {
    "startTime": "01:46:01",
    "text": "we already have examples of applications. There's bundle of vertical applications that are being specified within this working with So it certainly makes sense for me to have something that documents general principles for developing those applications. Okay. Brian? I agree with the previous statements, and also related to specifically the HTTP and the email transport doing things in BP means that there's the possibility of using extension blocks and combining that with BPSec, and and solving problems in an much cleaner way than in the past some of the other groups have tried to deal with partial header security in HTTP or different combinations of SMI in in an email world. But without relying on that baggage, being able to do things with with b b sec. in cleaning, And ways that are not necessarily limited to these 2 applications. Thank you, Brian. Alberto, For the for the chairs, it has to do with you know, I I support the, you know, the importance of, you know, continue investigating and defining how applications would could be, you know, get to the network. the question is more about the charter and the priorities. You know, we, you know, we we are working with that charter with very you know, delineated priorities, and there are at least items that are also very important, but didn't make it. to the charters. I'm trying to understand what I would be signing here, whether it is you know, a topping that we should consider or whether this is a topping that we would prioritize the charter."
  },
  {
    "startTime": "01:48:02",
    "text": "Yeah. So so Rick would check out on responding. the working group is open to people working on items within the scope of the charter really in the priority defined by those who have the bandwidth to do the work. So it's it's very not It's not a work schedule. It's a this is the the outer limits of what we consider reasonable to do, as a working group over the forthcoming 2, 3, 4 years. Hopefully, 3, the g like us to keep it fairly well scoped. So the charter is a tool to make sure we don't wander off and do random work out of scope It's not a first we do this, then we do this. we're not that corporate. So from following on from that, my perspective is I would love to see a a general application considerations for how to take your existing application or develop your new location for BP as long as there are people willing to put the work in. because I think we can all sit here and nod sagely and say, This would be lovely. But if atietf 121, still doesn't exist. This is yeah. Are there people willing to work on this is my question? And then, again, in the interest of time, so we do have one more topic that we want to get to that is important that the the last question so what we're hearing based on the comments is that there is is general support for an informational document in this way, The second question is what do we do about the existing personal drafts related to these applications. so the the the sense that I'm having is that we work these in parallel They stay as personal drafts, and we talk with the ISG as to where they are test adopted, whether it's DTNWG, or not I've That is what I have heard."
  },
  {
    "startTime": "01:50:02",
    "text": "as they've gotten more maturity, is there any descent or or comment about that position. Okay. height. Our our last topic is going back to the IPN URI With Stephen has jumped into the queue at the last possible minute, Luckily, we're delayed on it. post in can can you just what was the conclusion you had there on the to to had three questions on around the slide. Yeah. I know that you're right. We didn't actually answer those questions particularly well. the rough consensus we heard from the room based on that discussion was there was support for a generic document for application considerations. but no one arguing for adoption for either of the 2 SMTP or HT personal drafts. but no one was saying these are wildly out of scope right now. So I would suggest that Mark continued to refresh them and align them as application considerations document proceeded Right. And come back with an adoption request. So -- At a later date. Okay. So then I so my comment is, I think, the I think it seems entirely reasonable that the DPN working group consider things like interpersonal messaging, networks where there's a big disruption or noise. Yes. So I I, you know, I think the scoping issue would land in this working group as opposed to do something else there. And I don't know where it fits in terms of any of these documents, but I think before the working group spends effort, let's say, the SMTP or HSP things, I somebody really needs to go and find out what are the requirements for messaging in"
  },
  {
    "startTime": "01:52:01",
    "text": "some lunar scenario or mission scenario or whatever. because they may have enough require something like tunneling SMPP, or they might require Yeah. synchronizing message stores as we did years ago. So I think that kind of figuring out requirements stuff is would be important before the working expense time on different kinds of integrations or applications. Yeah. Agree. Yep. Pete Resnick. So looking at the SMTP document, for instance. Keep in mind that at least for that one, It's not really SMTP. It's a gateway protocol for doing email that happens to come from SMTP BP to something else that's doing SMTP. over So it's not as if you are or or Mark is creating you know, some extension to SMTP that really needs loads of applications area review. It's reusing a little gateway in protocol in an informational document of the ancient times and bringing it to this. So it would be plausible that that would be a specific DTM working group item as opposed to something that's over in the applications area. if as Steven says, it's actually needed for something. Thank you. And and also there are some comments in in the chat related to that as well, but in the interest of time in in our remaining 7 minutes, I do want to spend a little time on the governance discussion. For those who have been working tracking the IPN URI updates, One of the large updates, structural updates was the idea of taking the 64 bit node number space and sort of segmenting it into an upper 32 bits"
  },
  {
    "startTime": "01:54:02",
    "text": "an allocator identifier and a lower 32 bits of node numbers with the idea that allocator and node number together are a uniquely uniquely identify a node with the example of having ipncolon7.3.1, being a a a full IPM URI as opposed to IPM Yep. Yep. 3.1. In that, if we accept the idea of allocators and that we use those allocators as a way of segmenting the management space of node numbers. we need a registry of who allocators are so that we can identify them. And a question has been come up a significant question on the mailing list of What does it mean? to manage a registry of allocators in this space. I I I right now, the plan a for how this is done is that allocator ID registry would be created Ayana would manage that registry. the registry would be expert review. of course, with IETF policies if we needed appeal. If an expert said no to something, there would be an appeal to ISG. and that is the default management position. with the idea that individual allocator IDs could be given out to individual organizations. or ranges of allocator IDs could be given out to either traditional or emerging registrars who could then parcel them out as they see fit, which is a a way that we do this with other things like example, for example, for example, IPV 6 addresses. Alternatively, there is a management approach where we remove expert review. and we request that ISG directly manage this allocator ID space, perhaps with the ISG coming back to experts if they have a particular question. but in all cases, the allocator ID ranges exist in an INA registry, that manages them and gives them out in this particular wet."
  },
  {
    "startTime": "01:56:01",
    "text": "In that, Clearly, the registry concern is this sort of 1st order who gets individual registry IDs, And there are 2 significant questions that have come up. 1 is How do we make sure that these small encodings, because we don't have very many of those are given out in a in a pragmatic manner. And just generally speaking, how do we make sure that the allocation of these ranges are according to a fair and transparent process. that we are avoiding land grabs, that we allow new players in emerging spaces, particularly for space spaces. to to have an even playing field avoiding bias, apply rules consistently, and so on and so forth. So the question that we have to the working group is Is there a significant concern That expert review of the allocator ID registry would be problematic. So can expert review meet the criteria of of transparent and fair process, avoiding land grabs, allowing new players and so on, or do we need to do something different? We are working to get scheduling with the ISG in in mid August, we are putting a request to hold new node number registries in the CBHE node number registry until we sort of understand these policies and how the registries will be updated by the and you are a update But the the question to the working group here is, are there specific concerns about whether or not expert review can meet these criteria. apps and comment here, and follow-up on the mailing list, we will presume that there are not concerns. with this review process. And and as mentioned in the in the chat that the ISG does appoint the designated experts. I see Scott Early in the queue."
  },
  {
    "startTime": "01:58:04",
    "text": "And we will use this topic to run out our time, and we can go a few minutes after because This is important I'll be very brief, Scott Burley. Just wondering, have any concerns been breast, already. this And and and what's the nature of those concerns? there have been concerns expressed on the mailing list. associated with whether or not a small number review individuals may have either a bias or an inconsistent rule set in how allocator ID ranges would be given out So so in that sense, a core consideration is what what how would we constitute this body of experts? That's a wonderful question, because So I'm gonna jump in on this. To the best of my under and I have been talking to Ayana about this and the ISG. the expert review process as part of a a is a technical expertise review. and is not a governance review. So this is is this then breaking new ground? Not necessarily. It's breaking rare ground. So the parallels for other registries where this applies is and somebody else used the very nice phrase, these are deployable numbers. So we're not talking about protocol flag bits. That's that's nice and simple to do. and most of the iona registries are little protocol flag bits. or stethoscope or whatever is fine. But we're now talking about big deployable numbers like IP addresses. Dennis, level, top level, top level, top level, top level, top level, top level, top level, names. that kind of stuff. Stuff people care about and build businesses and spend lots and lots of dollars on. and that puts it into a different class of fair use, accessibility, and so on. And a couple of the most possible way, Geeks who understand the numbers are probably not the right people to go up against big corporate lawyers if a mistake is made."
  },
  {
    "startTime": "02:00:00",
    "text": "I think that's the crux of some of the the concerns. And How how much guidance are we getting then from ISG on how this is been done in the past and rare instances where it has been done in the past. So Cai Hatton, if any of the current or previous ISG members would care to step to the mic to offer some advice. Please join the queue. Mark, I see you're in the queue, and you're in anxiety as our ID, please join the queue. I'm not sure about the fact that expert reviews the right thing, but we're talking about it. So You're right. That's expert review is typically technical. if people care about things that may happen under the carpet, then that's easy to fix in the sense that many of those expert review done for summary, Susan, or actually, and all in a public meeting is, like, think it's definitely you wrote this also on the chat So so there are things that enable, you know, other people to review what's going on if and if an expert you know, is wrong. And, usually, there's more than one expert. So So all this can be can a manage almost okay. of as you said, it's a different kind of registry because it's for deployments instead of protocol you know, numbers and stuff. So and I'm not sure expert reviews the right thing. at Thank you. I've Scott Johnson. see it both. Can you hear me? Yes. Okay. My concern with expert review here is that this place is a a significant amount of power,"
  },
  {
    "startTime": "02:02:01",
    "text": "in the hands of just a couple individuals. Personally, those individuals would need to receive significant guidance from somewhere in order to perform that that function correctly, and I think probably we would bet we would be better off with hierarchical allocation here, which sets it up so that both the default registry remains and is operated by Indiana But as well, we allow the community based stakeholder centric governance models that the things like the RIR system have allowed as well as any potential new RIRs that may be been written and jurisdictional terrestrial law is not appropriate. So so, Scott, with with that, if if the AINA registry allocates large blocks of allocator IDs to RIRs or ICANN or CCSDS or others, Does that alleviate that concern? I believe so. Yes. Right. Mhmm. So Rick would share out on quickly jumping in I I had a nice chat with one of the guys from ICANN yesterday about how that process works. and and Ayanna has confidence in the processes is followed by ICANN and the RIRs. And therefore, although Ayanna holds the registries, and owns the registries, they delegate these large chunks out to ICANN in order to do this work for for the be addressing and and AS numbers and things like that. That model seems to be preferred way to go. I am hearing"
  },
  {
    "startTime": "02:04:01",
    "text": "Does was is that what you're trying to say, Scott? You'd you'd find that would work. Right. Yes. I I I modeled not only would work, but probably many of those institutions with additions would work to fill that role. Yeah. It's it's it sorry, Rick again. The model is what's being discussed here. The the guy from I can politely when I don't know what to do with these numbers, but the model itself is is valid. And then and lastly, I know we're way beyond time, but, Eric, thank you, Scott. And Yeah. Sorry about printing over time. I just wanted to say there there are several things here. have analogs, you can one important thing to say upfront is that we we If we write a document that says, I wanna Please use expert review. We can shortly thereafter write another document. It says, Ayanna, we've changed our mind. because we're not. So You could also write a document that says Ayanna We have the highest possible standard for allocating things, and please don't ever do that again, but spend, a month and a half preallocating the table with everything and anything that we want. Right? Getting getting everybody signed up. isn't like, there's a there's lots of lots of opportunities here. So time. I think the only other thing was that there's a SNMP Privateenterprise numbers, the pen registrations, I mean, that's basically 1st come, 1st serve. think you're limited to one per organization, but you could invent an infinite number of organizations. I suppose if you wanted to try to land grab that system, but it has not suffered any ill fate. So I will add one consideration to that comment, Eric. Sorry, Rick Taylor again. The difference between those enterprise numbers is Again, we have the seaborne coding problem, which means little numbers are more precious than big numbers."
  },
  {
    "startTime": "02:06:01",
    "text": "And that's where the the fairness and the you know, oh, I want 7 because it is much better than 7,000,000 I think the OID encoding Oh, yeah. OID is an ASM. Yeah. It's it's It gets pretty bad. Bye. Thank you all. We we are at the end of this meeting. We will see everyone at 1 18, and we will see you on the mailing list. Have a great day. And on the mailing list, please suggest some tech for that designated expert thing that that section because it can't just be me writing what I think I would really like some text suggestions, please. or like he's stuck and osnick. So I put in that 8 private enterprise number 4 from 8. I did all"
  }
]
