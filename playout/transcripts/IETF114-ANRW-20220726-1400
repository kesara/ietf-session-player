[
  {
    "startTime": "00:01:11",
    "text": "you"
  },
  {
    "startTime": "00:02:01",
    "text": "hello everybody just want to make sure that people in the room can see the slides good number of people in the room lovely to see i will say if you are in the room might as well begin with this one masks are mandatory so i've been asked to explicitly remind people of the masking policy at ietf this year across the board if you are attending in person or required to wear a regionally approved mask so n95 kf-94 and the rest certified equivalent i understand the ietf is is supplying masks as well if anybody needs but please ensure that you are wearing a mask at all times even if you are asking a question of presenters the exception of course being if you are presenting you are not required to wear a mask in person okay a few other admin things let me get started uh from notewell standard stuff so the iutf uh followed the intellectual property rights disclosure rules as listed on this slide and i believe most presentations you will see this so please make sure you're aware that any irtf contribution is that's covered by patents and patent applications you must disclose that fact or not participate in the discussion"
  },
  {
    "startTime": "00:04:00",
    "text": "rtf expects you to file such ipr disclosures in a timely manner i'm hoping you have done so uh in a period measured in days or weeks not months irtf prefers that most liberal licensing terms possible are made available for irtf stream documents and the definitive information is in rfc 5378 moving on for audio and video of course uh irtf routinely makes recordings of online and in-person meetings um available online if you're participating in person and choose not to wear a red do not photograph lanyard then you can sent by default to being recorded okay uh and if you speak on a microphone appear on a panel of course or carry out official duty as a member of the irts then you can send to appearing and recordings of that of you at that time and if you participate online and turn on your camera and or microphone then you consent to appear in such recordings the privacy and code of conduct um as a participant uh any irt of activity you acknowledge that written audio video and photographic recordings of you might be made public personal information you provide to the irtf will be handled in accordance with policy uh yes i'm told i'm breaking up left and right so hold on i don't even unmute is this any better hoping it works let's find out um as a participant or attendee you agree to work respectfully with other participants and please contact the ombuds stream if you have any questions or concerns about this of course the code of conduct is in rfc 7154 an anti-harassment procedure in 7776"
  },
  {
    "startTime": "00:06:02",
    "text": "uh recap of the goals of the irtf fabulous group focused on longer term research issues related to the internet while the parallel organization which is the ietf focuses on shorter term issues of engineering and standards making the irtf conducts research it is not a standards development organization but of course important to help bridge the the two sides so while the irtf can publish informational and experimental documents in the rfc series its primary goal is to promote development of research collaboration and teamwork in exploring research issues related to internet protocols applications architecture and technology and the primer for irtf um primer for ietf participants at irtf of course is rfc 7418 so moving on to the fun stuff welcome to a rw everybody i'm really glad you could make it both those who are in person and those attending remotely it's it's lovely to have people join um i want to make a special thanks to the program committee this year in particular because it was a very competitive season we had four or five premier venues uh all running at about the same time so the people who volunteered for for this committee a particular thanks because everyone i know had multiple commitments around the same time on a personal level uh someone who couldn't be with us is my co-chair uh tj and and to be honest tj deserves so much of the credit for actually putting this together without him i honestly couldn't say where this would be so tj if if you're in the room thank you and if you're not in the room um everybody know that tj did a marvelous job here also thanks to the review task force uh ethan and and johanna they did a marvelous job in just looking over reviews making sure they were in order positive constructive and so on so thank you both for that and of course thanks to the sponsors of"
  },
  {
    "startTime": "00:08:00",
    "text": "a rw specifically which are akamai and comcast so um logistics and links uh the paid the program the paper pdfs and so on are all available online i'm breaking up once again one sec okay and i'm gonna try and wrap this up quickly so i don't continue to break up um i'm presenting the program overview in reverse order so this afternoon we have a set of invited talks to broach a new potential area of investigation for the irtf around formal methods and verification in protocols and development so please come back this afternoon because i anticipate these will be a fabulous set of talks but in this section we have a keynote and then the four papers that were accepted for publication um i want to first introduce uh lucas pardue who has joined from cloudflare and he's going to give this interesting talk called layer four and three quarters i work with lucas on a regular basis and he and i have frequent conversations and the questions that come up are actually very interesting uh the layering model in particular has done very well and we all know for the evolution and sustainability of the internet but i think increasingly in many domains we are starting to see that the the exchange of information or signaling or cooperation between layers is somehow increasingly important or relevant so i saw lucas give this talk a few weeks ago and i know he was really excited to give it at this venue so lucas if you are around i am going to hand off to you as soon as i load the slides one sec i can do a microphone check while we're waiting can you hear me okay"
  },
  {
    "startTime": "00:10:02",
    "text": "i can hear you just fine okay uh welcome slides i'm not seeing lars i hear you're in the room i'm not seeing four and three quarters here lucas do you have your slides handy it might just been a timing issue with getting them up to the data tracker in time for meet echo to upload them see okay sorry audience no okay i think there is a way to and um for the interesting time i probably just do a screen share from my my local machine if that's okay yep i'm gonna hand over control to you okay thanks lucas take it away my sharing ah here we go just bear with me okay"
  },
  {
    "startTime": "00:12:00",
    "text": "i have to do entire screen oh is that working it doesn't appear to be working either yes you can see my cursor but not anything else okay hold on and wait a minute one sec manage meeting 20 um no luck on the sharing on your side no um tracker [Music] uh melbourne could you potentially share if you could just load up the pdf we'll have to do it kind of old school oh actually no no uh lucas i am looking now and i don't see the four and three quarters in the data tracker at all okay that was submitted last night maybe maybe we've just uh crossed paths on that um okay it wasn't approved let me"
  },
  {
    "startTime": "00:14:02",
    "text": "let me email you a copy quickly i might in the worst case actually change the order of these things while i wait um let's do that are the look no there's something so it should be on the way i copy colin in as well just in case uh colin if you're there i'm tempted to switch the order of events and do the keynote as a finisher i i've got the slides now i'll just upload them okay [Music] so [Music] okay the slides are uploaded so if you press refresh in the time series uh keynote here we go okay"
  },
  {
    "startTime": "00:16:00",
    "text": "so except i'm not seeing it in the share list hold up so it's i see it in the data track when i update the meeting notes i see it in the list if i go to share preloaded slides it is not impressed everybody getting i hope you're not the only girl that has difficulty with one second seeing it in the share slides list fabulous okay your show lucas thank you there we go i've got control of these slides excellent um thank you very much for everyone uh for their patience uh thank you to the chairs for inviting me to do this talk i was super excited to come and do this in a venue with a live audience um and to be there in the room unfortunately i i came down being ill so i'm gonna preface this with um this whole"
  },
  {
    "startTime": "00:18:00",
    "text": "shaboodle of delays with uh i'm ill so be kind on me um so i'll be quick because we spent some time trying to get to this point um i'm lucas pardo i'm an engineer at clyde play on the protocols team working on technologies like quick and tls and hp 2 and hp 3 these kinds of layers 7 or downwards protocols and i kind of take the view that the layering model isn't brilliant for for some of the kind of work that my team does um it helps but it's also an impedance somewhere because we're often talking cross layers and problems that exist somewhere in the ether so this talk is kind of a tongue-in-cheek view to communicate to people who want us the kind of problems that we face um so briefly you know just in case you don't get the reference to the talk title um this refers to you know platform nine and three quarters that comes from the harry potter universe um if you came just to get harry potter memes or jokes you'll be pretty disappointed um and mainly because you know today's talk is focusing on layer four and three quarters that's mainly because our layering model doesn't go up tonight it's it's restricted to seven so you know that this is a familiar model that we're used to uh some people might call it a layer cake we know that the cake's a lie um and we should just ignore it at least that's my opinion anyway and the reason is because it's detached from reality um it's a work of fiction you can pretty much do anything with it that you'd like and you'll be fine uh sorry i'll just go back here like uh i went to the next slide and i missed it myself obviously i got a bit of brain fog here um yeah i'll just split back and forth between these two slides what's the difference i've pinned upside down it's fairly noticeable there's like hardly anything that would have changed here um and that's kind of funny because it reminded me of something else actually um some of you might know i'm welsh i"
  },
  {
    "startTime": "00:20:00",
    "text": "grew up in the cardiff the capital of wales um yeah they did the layers in this order reminded me of something and that's cardiff central train station uh and you might say what like why is that the case um and if you look closely it's probably too small to see here but um if you look at the platform numbers um they're in the kind of central column of the the image here um they go from one through to eight um i see uh jonathan in the chat mention layer eight uh we're not gonna talk about that one today but maybe some of the other things that you see if you very quickly look at that image um and i think yeah beyond the the ordering uh that we have here um the interesting fact is well not fact so the interesting observation that i had is that um we could analogize kind of transport or internet protocols in some way to kind of train stations and trains so bear with me for this analogy a bit for a bit like uh i'd say that indicative of communication stacks insofar as like hundreds of years ago people you'll never get to meet had some lofty goals and improving the movement of things from a to b and they made well-designed intentions and design decisions that lay the foundation for the future of generations to come those train tracks uh pretty good structured guides for the movement of things and the vessels that move on those tracks can carry various kinds of loads and they can adapt to the changing needs of consumers but sometimes those plans don't go you know they don't play out their way that they they they'd hoped or that uh the needs change in such a way that the tracks and the foundations aren't quite right um and because train tracks and stations are physical things it can be hard just to rip them out and swap something in that takes time and planning and dedication and maybe like you're stuck with what you designed and that's it um and that comes back to you know our layer cake um"
  },
  {
    "startTime": "00:22:01",
    "text": "sometimes you know we have these layers and they don't really serve much purpose to how people use the internet today uh so so for us for me i just kind of removed that existence of these layers from my mind i could just take layers of layer five out yeah throw it in the bin and and everything's fine and you might say well you can't do that in real life but but actually that's something that cardo central train station already did they did us in the 60s uh well ahead of osi um they were really paving the way to use another pun um but it's not just like using and like or ignoring unused parts what about if you want to extend things beyond the initial design intent uh card central train stations like ahead of us again here uh they needed to add another platform for carrying different kinds of traffic uh so there is actually a platform zero at carter central i used to catch a train there myself um what about other things uh they've also crammed in several lines between platforms i mean like i don't know who like in terms of end users that's supposed to serve but it it says different kinds of traffic um yeah i don't know what that's about i'm sure the train nerds in the room probably have a very well justified explanation for me um you might be laughing you might be very confused by all of this uh but what i'm attempting to illustrate is pretty straightforward that there's only really two and three quarter of layers of things that we need to care about in this room today for this talk and those familiar with harry potter will be familiar with the obvious thing to do in this circumstance that you just put those two things together and you just run up them as fast as you can to see where the boundary will take you so trying to return back to reality somewhat um in my view this these are the only layers that came and this is the boundary between them"
  },
  {
    "startTime": "00:24:02",
    "text": "you might balk at layer 3 being removed um surely you know the internet protocol and the internet are important um i would tend to agree uh a lot of the the the work at cloudflare does is about helping to build it in better internet um and we tried to do that so it's not completely opaque to us or or not there but when it comes to a focus at the internet or you know the web and the way that people and humans interact with that uh i don't really want to spend any mental energy on ip details and that's because end users don't really care they they don't access services via ip the direction of travel is that they they should be becoming well the the world is probably becoming less reliant on client ips anyway we're seeing this in the work in the itf in terms of oblivious protocols um for various different application protocols on the top of them where we we can the traditional use of client ip is a vector for authorization or authentication is kind of not not so good um in terms of privacy and so on so we won't dig into that topic today it's a very rich idea maybe some of the other talks will i i don't know but um yeah this is one of the few reasons i think layer 3 can be ignored safely so i just want to move on from that on to to some of the other reasons i think this is important it's because it's what i spend most of my day job on and the team i work in does i talked about some of the protocols we support on the edge um and that our team is responsible for the servicing and there's other things there like websocket and grpc if you're not familiar with those i won't be going into much much detail on them but they help give a picture that um many people think it's the web and that uh http is a protocol that powers"
  },
  {
    "startTime": "00:26:01",
    "text": "the web but there's other the services that build upon these um they might have user agents or end users but they could be api traffic not necessarily just people browsing page page to page um and the reason that you know a company like cloudflare uh it does what it does it provides intermediation between clients and origins and that can provide speed reliability and improvements but also provides scale to clients from a distributed network but also a scale in a different term to the origins for people who operate these things it could be difficult to keep up with the latest trends in technology so there's a long tale of cheap shared hosting or vps services that have no hopes of kind of getting their stacks to support these latest and greatest technologies and and the by having an edge in front of them you can enable that a bit more quickly um or provide this kind of serverless edge compute platform that that some of you might be familiar with um and and beyond the layer seven protocols i've mentioned there the team's also responsible for a product called spectrum which is more at the layer four things like tcp and udp we do a few layering violations to help provide some added value on top of those um and these are lower violations in the good sense to provide some some value for people um but that we provide some specialized support for other protocols that maybe sit between you know four and seven um you see here on the right hand side ssh rdp in minecraft um which got me me uh interested i don't know much about minecraft at the time um when we took on ownership of the spectrum product um and if you look there's a little asterisk that says minecraft java edition is supported but minecraft bedrock edition is not supported my curious brain made me want to understand the difference there um again that's a whole talk title"
  },
  {
    "startTime": "00:28:02",
    "text": "in its own um if you're not familiar with minecraft you go visit these links it's quite interesting what the differences are um it basically boils down to tcp versus udp but um i i i won't digress here too much um the interesting thing is uh there's wireshark dissectors those previous links have like pages upon pages of schema and definition of the protocol language they're not at the iedf but the the amount of detail there is quite astonishing if you're not familiar um and i was thinking quake was the only uh supported protocol in wireshark but no there are indeed dissectors but anyway that's that's kind of an ambling abstract way of talking about this topic um if i've lost you i'm sorry if if you you you want to tune in again um great we're going to get more into some nitty-gritty details right now and talk more in the specific terms here um so uh talk about http uh this is a layer 7 protocol uh sometimes we forget what the acronyms actually stand for the question would be does does this mean the hypertext transport protocol um sometimes i hear people say that um i will quote mark nottingham from itf 99 who said that roy fielding knows whenever you call it the hypertext transport protocol because it means the hypertext transfer protocol uh we might be splitting hairs to say well what's the difference uh what is a transport protocol um speaking recently with marwin to even discuss you know is udp really a transport protocol or not but again i i don't want to digress to help frame some of this discussion and why i think it matters i want to talk about the view from cloudflare's edge um and then some of the the the kind of traffic profiles that we see from real users contacting our edge on a daily basis um so to get into that first of all we offer something called radar.cloverlight.com there's a website"
  },
  {
    "startTime": "00:30:01",
    "text": "um containing you know up to up-to-date statistics about various things that our ed sees this is just a snippet of uh hp and tls traffic taken from april um so it's a little bit out of date but if you if you were to check that today you'd see some maybe similar numbers maybe different numbers um so to go and take a look um but uh yeah uh just the the high level summary of this view is that um it's quite clear that most of the traffic that our edge sees is encrypted hb2 and hp3 uh 99 percent of the traffic according to this this view at that time was https so that's encrypted um only nine percent of the traffic was hp 1.x so 1.1 we'll call that just for convenience um so when you combine those two figures together uh that's that's a tiny tiny proportion of requests to cloudflare are being made over plain text hb 1.1 i think sometimes when when we think about http in the web this is presumption about how it worked maybe a decade or so ago and that's very different to today so when we're thinking about how to design and scale services um against like an older view of the protocol those assumptions don't hold up anymore and i think well what i will try and illustrate through the next few slides is that the tooling and the methodology we have is maybe not entirely fit for the purpose of of how these layer seven protocols have evolved and are changing and to maybe help illustrate that sometime um it will in some way uh very briefly i'd like to think or encourage people to think about how these protocols look on the wire so the astute amongst you will understand maybe that i've flipped the layer kick upside down now um so from the bottom we have layer seven in terms of this http semantic"
  },
  {
    "startTime": "00:32:00",
    "text": "if you're not familiar with the protocol it's a request response protocol where clients send request messages and server sends response messages back to each other these are kind of an abstract thing they have requirements and constraints but um they don't have a strict serialization that's the duty of the http version itself to describe how those abstract semantics map onto the wire so on the left you know we've got 1.1 where we take this this message and we'll convert it into an ascii serialized form and we put it into a tls record because we only care about encrypted traffic here that would then span multiple tcp segments in the middle column uh weekends have the same hp request response message semantics but instead of ascii these map into the binary frames so we have this splitting up of headers so the kind of the metadata for the message and the the data frame which carries the message content but again those get mapped into a tls record and sent over tcp segments very in a very very similar way on the right hand side is probably the most different aspect here with this is this is quick the quick transport with messages getting mapped into frames very similar to hb2 but before they go down into to anything below them um they get mapped into a quick stream which is a transport primitive um that's available that solves all of the the head of line blocking staffing quick that we're not going to talk about today but those map into quick packets and they get sent in udp datagrams so keeping that in mind that this is how it kind of looks let's look at some statistics very quickly radar offers this high-level global view of traffic trends this is an interactive map so i'd encourage you to go visit and play around maybe look at your country or where you reside"
  },
  {
    "startTime": "00:34:00",
    "text": "interesting things like that dark orange is where the share of hp3 traffic is highest um going to lighter for lowest um i don't know maybe that distribution surprises you at the time i'm sure it's changed today due to traffic trends etc maybe it doesn't let me know in the chat back in may 2021 quick the transport protocol launches our c9000 uh cloudflare has been supporting this for a long time we we did a blog post at the time it's hard to see here but the traffic show at the time was twelve point four percent or call it twelve if we're rounding down um and in the meantime uh hp three draft has finally been published this year uh and we're kind of keen to take a long-term view of the usage just as a headline statistic um in that time of the last year we've seen the traffic global requests sorry the the share of hp3 traffic on a global scale rise from about 12 percent to over 25 percent uh which is you know a doubling which is uh pretty decent i'd say um and but in some countries that's even higher so i do encourage you to go take a look and have a deep dive or visit that blog post where we'll dig into some long-term trends over time um to show maybe the the wax and wane of traffic profiles over the last year i'll summarize them very briefly here but um to look at this what we have is 3 in the blue that you can see shortly after the rfc 9000 came out um hp 3 traffic for us overtook hp 1.1 and it's just continued to grow ever since at the top http hb2 um is has kind of been bumbling along at the same rate uh and one point one is it's still there but it is very small percent uh just to describe these briefly if i didn't cover that these are measured in in requests per second um"
  },
  {
    "startTime": "00:36:00",
    "text": "that our global edge sees for human uh or likely human traffic so we we rate traffic whether we think it's a bot or human and this is solely for human traffic you can filter this down into bot traffic i'll present some slides differently and the profit profiles are quite different um which i find pretty interesting too um if we're going to break those requests down into different user agents we know that support doesn't roll out unilaterally and that different work needs to be done um different priorities etc so just looking at these three browsers chrome firefox and safari we can see you know different kind of roll outs happening there um say chrome is now at around 60 um firefox is kind of similar today or back in may at least but you can see that there's been some gradual kind of ramp up or rollout some of this is affected by uh cloudflare customers who control whether they offer hp3 to the edge but by measuring everything kind of globally and then looking at these more specific details we can get a feel for how the the different user agents are also rolling out support maybe experimental or by default um etc so you can see on the bottom left there safari kind of like in behind slightly chrome with firefox um i understand that you know these numbers might be changing soon as the safari rolls out um support more widespread for hp 3 by default um that will will remain to be same tuned in to see this is how it looks in terms of global requests for time i'll just uh kind of gloss over this one slightly this is how we look in terms of share similarly chrome is pretty dominant though and this reflects just global share of user agents to some extent if we took chrome out of the picture you can see more clearly that the different kinds of changes that are happening for these browsers the blog post goes into"
  },
  {
    "startTime": "00:38:00",
    "text": "more detail for some of the observations here so i won't go into any more detail but i would like to focus on this slide a bit more because this looks a box um which you know uh good bots and bad bots we we might spend a whole session on that one but these typically bots related to search engines or social media entities who like to access the internet to do stuff um so you can see here i mean the headline is that there's hardly any all zero hp3 usage they're very much you know in a 1.1 or 2 world you can look at some of the the public information about how these bots decide what version to pick uh it's pretty opaque and obtuse um i struggle to get any more answers or insight into that i'd love for people to to have a chat if if if anyone knows anyone but aside um yeah it's it's an interesting in respect to hp3 usage is pretty high for users and low for bots so with that all in mind binary framing as i mentioned for hp 2 and hp 3 it was a solution to a problem to let us mitigate some stuff i'm not going to talk about but mainly about performance and head of blind blocking issues that can affect web page loading and it does that job so it's a solution to all our problems well um text based http hb 1.1 or or other older versions uh have a pedigree providing quick some recent examples include this idea of request smuggling an attack that's kind of researched lately i'll go into the details here for brevity but um despite all of the baggage it's pretty well exercised and understood by technically minded folks whoever care to look into this stuff around hp performance and behaviors and quicks etcetera yeah like i said hp 2 and 3 constitute"
  },
  {
    "startTime": "00:40:02",
    "text": "see so how can we best characterize the quotes of those specific hp versions there's a bunch of tools um hd spec h3 spec h2 load um there's chrome net logs there's q logs which is um a specification and that we're we're working on in the quick working group along with some tooling to support that there's probably other things i'm not familiar with maybe you have a favorite um i don't know these aren't so much analyzing the protocol but analyzing conformance or or implementation behavior specific to protocols whether they implement musts or shoulds or maybes etc and how those things interact with each other they're all pretty good useful tools i use them myself but in general i would say in my opinion the general coverage of h2h3 as an internet community seems lacking given that the the widespread support for this protocol when things go wrong um people come to me to ask for help and i'm always happy to give it to them but i i think the scaling of that knowledge and the debacability is something that we could do better and this talk is part of that activity um so i want to like use some examples to illustrate this i don't want to pick on chrome solely but most people might have seen these kinds of error messages hands up if you have um this this often just gets shown or something like this when anything goes wrong so you know if you've seen this message how did you solve it did you just hit reload probably and then it went away but if you look online and you search for these things you hit interesting websites that give you interesting recommendations like flush the speedy pockets which you know i don't really know what that means could just be a typo it could be something else we go from speedy into hb2 maybe you've seen that error again here's another website that recommends turning your antivirus off and on again um haha i i don't know if"
  },
  {
    "startTime": "00:42:02",
    "text": "a bit scary in my mind and then we get to quick the latest and greatest um one of the recommendations like in the top of google search is if you're stuck from all of these different methods that don't actually help anything then contact google customer support i don't know if any of you have ever tried to contact google customer support but um you would have maybe mixed mixed results um i would say but i i actually did i had a problem um i went online um i logged a bug um after speaking to some folks and they responded very quickly and it was great um but unfortunately we all have priorities in our work and so that bug is remained open and sits there um it could potentially help um in in a few different ways but we're all busy all get prioritized i understand so in the meantime we have to look for other solutions or other ways to look at problems and beyond just chrome i want to focus on a specific example of this kind of thing where the spec says what we must and may do um and then what in reality actually happens so this is the idea of of taking a a request message and serializing it to the wire and what the hp3 spec says is that um you must have this full set of pseudo headers which articulate say for the request the method the path um the host and and so on but every request needs a method so this could be get post head various verbs that can be included in there that apply to all versions of http um but if you don't include that method then the request is is malformed so they must be treated as a stream error so that means resetting the stream with a specific error code and that you may serve a response before that happens before resetting the stream uh so what i did is is create a mile form client that did this on"
  },
  {
    "startTime": "00:44:00",
    "text": "purpose this is literally if you think of old school hp like the the first thing that you would see in the request line is the method so it's like the first thing that could ever go wrong and i test that client a hp 3 client um against various implementations that operate on the internet and see what happens so to briefly summarize you know four of these returned a bad request um one returned four or five method not allowed which i thought was quite amusing because there was no method um another's uh center reset stream which is kind of aligned to what the spec says but they sent a different error code um and some just closed the connection outright which is pretty terminal um given that these connections can hold or articulate many requests um but i want to stress that none of this is bad behavior they all kind of rejected it and ultimately in hp that's that's generally good enough um i don't think anyone is there as the protocol police making sure that you know if if you receive the wrong error code you you take action but sometimes these things can can result in systematic errors as they propagate through so it's not always clear if this is okay or if this is bad um maybe as a community we need to to take a look and and see if there's anything more we can do and sometimes this is not sometimes it's the equilibrium that's good enough um something else that happened uh different to that um a while back netflix did some work about hb2 they were looking to implement it and then looked at the protocol and then compared that to implementations and were able to identify a number of attack vectors that could result in denial of service on hb2 implementations um and there's a large community effort um responsible disclosure to kind of create proof of concepts of these attacks and and integrate fixes and the community did a great job of that but um you know if you're writing a h2"
  },
  {
    "startTime": "00:46:01",
    "text": "implementation today is that information is accessible um to you you've come after the whole train of of implementers maybe you've read about these things but you don't understand them how can we make sure that those test cases apply to diverse hb2 implementations still there's code you could run them that's great but is it is it sufficient and the reason this is important i'll just pick on one of these one of the attacks was um a ping flood so this is a hb2 ping frame um you can see here the description is that attacker sends continual pings um that could cause build up and a dos and and the reason this is interesting is because cloudflare implemented mitigations for these h2 dos attacks that were that affected our specific implementation we weren't affected by them all but reflected by some and one was the ping flood um but if we kind of put that on a stack for a moment grpc um is a lady7 protocol that does stuff um and they have this thing called uh pdp estimation and this is a direct quote from from this website that says the idea is simple and powerful every time you receive some data um send out a h2 ping frame and then look at the response and do some calculation um and estimate the bdp which is pretty cool um you know but what what it results in is a ping every data frame which is very much like a pain flood and that introduces a interesting interaction between some implementations and the fun thing is that um rust has a library that implements http 2 called hyper and they have a feature called adaptive window that directly was influenced by the grpc bdp behavior so they we we had a situation where the rust hyper client would speak to the cloudflare edge and uh get closed the dos mitigation would close them because it would detect an attack and that wasn't great but you know we"
  },
  {
    "startTime": "00:48:01",
    "text": "were able to detect these things and understand them by analyzing them in things like wire shop or the tooling and and work with community uh to implement the fix and the fix here was to to not be as aggressive in that pinging um and the act the outcome of that activity is actually it was friendlier to the server the client was friendlier to the servers that it was speaking to um and also the the bdp actually um got better um everything got better so that was a net win um there's a different problem that we had we launched something called speedok cloud player a while back um and in this like uh people test how fast um in terms of throughput their upload or the download was and what their ping and jitter was like um shortly after we launched that we had reports that people on very high speed internet access um were seeing uh kind of deflated upload figures compared to other such speed tests that behaved or acted differently so we did some investigation here and what we found um is that there was a difference between just hp 1.1 and hb2 upload performance so that ruled out anything lower layer both using tcp both using ip everything else pretty similar so the difference there was solely between 1.1 and and two um and throughout his investigation what we found is that it was due to hp2 flow control and this is quite a just a basic thing it was there hidden in plain sight that the server was just not giving enough flow control to the client um causing these deflated figures um so my colleague did some work on this looking at different auto different buffer sizes different window sizes and we settled upon an auto tune solution pretty similar to what tcp does already but at the layer seven um and by deploying this we were able to see like immediate improvements which was a again a very good win but it really makes me question why such a basic problem was there for so long and no one had detected it which makes me wonder you know what are"
  },
  {
    "startTime": "00:50:00",
    "text": "we measuring when we do anything like this are we measuring layer 4 layer 7 the whole thing and what's the methodology to detecting bottlenecks or performance impediments beyond the ideal um like i say here layers below hp will attract uh affect its performance um and but you know even if your lower layers perform well they aren't indicative of upper layer success um and this kind of environment is and the observations we'd seen was our motivation to making a submission to the iab measuring network quality um workshop that happened last year um and then we'll say you know on top of just throughput stuff latency and jitter of fundamental problems the hb protocols are very latency sensitive or responsiveness sensitive and that's why we're super happy that the the work in the ippm on responsiveness and the working conditions has been adopted now and is making good progress about testing the whole stack including hp2 implementations these are the protocols that real people use when they use web services and we need to be able to characterize them and understand where there's bottlenecks or performance improvement possibilities but beyond all of that um to quote harry potter life isn't fair it isn't just streams and requests and responses there's a whole evolution of of things happening at http turning it into more of a substrate it's always been a substrate for things but through through the definition of the hb datagrams and capsule protocol that's been happening in the mass working group and that work is now done just waiting publication um we're able to provide even more um extreme use cases um to use a word so udp 443 could be the future of everything is what i'm trying to indicate here um on top of reliable stream data we have quick datagram frames which allow for the use of unreliable datagrams in http in combination with"
  },
  {
    "startTime": "00:52:00",
    "text": "the capsule protocol we're able to do new things like proxy udp over http which is what mask has been working on um they're now focusing more on proxy ip over http which is interesting too we also have the web transport working group and the media of a quick working group all trying to find new and exciting ways to use these protocols which again makes me even more interested in the kinds of edge cases or specific problems that might occur to them and that we need to empower everybody who might use these protocols to be able to understand what's going wrong not just the developers who are working on them right now or the experts who'll be running those implementations or edge services uh for the next few years um we need to scale this up to end users who have some technical knowledge and and want to use these things uh this is an example that takes it to the extreme of multiple hot proxies that tunnel quick over quick um in terms of layer cake that is even more complicated um and i know i won't go into it now because my brain hurts um but we want to come back to the layer four and three quarters um one moment um this is the conclusion of the talk you'll be happy to hear um the the intersection the venn diagram between layer seven and four would obviously include other layers except that they don't really exist in practice so this is where the quirks are but i i i have many remaining questions i can tell you that there'll be things there that will go wrong and we might fix some but there'll be more it's an unknown maybe to use that horrible um analogy but um what i what i want to focus on is you know how can we get better as an internet community at finding the things we know where they are how do we find them how can we get better at fixing them in a more faster more robust way and how can we get better at documenting"
  },
  {
    "startTime": "00:54:00",
    "text": "them do any of these things need protocol changes i think probably not but we don't know until we find them um yeah uh that's it that's my call to action we need to scale this up beyond just itf people in in this conference venue this week um so with that i'll conclude my talk thank you very much for the invitation it's been great to share my mind with people lucas thank you very much for for giving the talk um this is uh just as entertaining the second time around for me um look i know we're over time but i would like to entertain a couple of questions uh i don't see anybody in the chat queue and i have one or two that i could ask but i'm looking at the room so anybody in the room who wishes to ask and and while people are thinking about it let me let me ask you this so if i having seen this now for the second time and having had a few conversations with you i feel like there's three buckets of things happening there's this notion of measuring issues that have cross-layer components diagnosing problems and signaling somehow between layers do you have a sense i mean from your perspective working on the ground are there is there a priority order to these i think is the first question the second is do you have a sense are we talking about a new set of of standards here that need to be evolved or is it something maybe that the formal methods um and verification community can attack some of these things as we might discuss in this afternoon session those are all excellent questions i would say um the best way i can answer that is that typically the way that most http servers work is that they would log request logs which are very high level view of the kind of the response status code that was sent when a request came in with some information about that maybe some headers the user agent things like this on the client side"
  },
  {
    "startTime": "00:56:01",
    "text": "you just get the result maybe that error code i showed in chrome you can enable additional debug tooling so you could look at a dev tool open that up and maybe capture a half file does give a bit more information about the request headers and response headers that were sent but they don't go into any of the framing like this this eponymous um layer four and three quarters to do that you'd need to maybe enable the chrome netlog which would capture stuff but then you're getting into almost um you know per packet tracing and that is voluminous uh it's large it's it's hard to read through quickly um and when it goes wrong it's often these things are multiple requests all in a single connection so if one request fails it could be due to something unrelated that failed in that connection um due to maybe an implementation bug or an implementation choice that was was defined in the specification that implementations must decide and often they will but they won't document why that decision was made it might be there in the code in the comment um or but you know we're not all experts in every language and nowhere to look for these things so um just just capturing these things is hard it's a hard problem to solve because of the data that's required i know facebook as an example um uses q log in production and does capture a lot of these kind of events in real time but i don't i think that's beyond the capabilities of a lot of deployments so it could be that we need a way to think about how to to do logging on demand or or sampling or tracing of things but again it's it's tricky i don't have that many answers in this i have ways that i know how to work myself but those aren't necessarily applicable to everybody i don't i don't think this is something"
  },
  {
    "startTime": "00:58:00",
    "text": "that can happen in the specifications themselves but maybe you know we do a good job with automated testing say for interrupt in the quick working group but that focuses very much on the transport layer maybe there's more room to integrate some of those tools i talked about earlier into more of an automated kind of view very good okay we seem to have one question on the floor please introduce yourself and then we'll move on thanks hello my name is dobby and i work in the kitchens at gringotts on packets and i do exist and i would love to have been in the fcc room when you attempted to discuss delay and jitter without mentioning packets or lower layers that would have been very entertaining because magic requires packets thank thank you for the um the comment that i i would say please go and see the fcc um uh proposal i can't remember the right name here but you know the these um requests for comments that were made like this was a public consultation i think mom will probably correct me if i'm wrong here but um yeah this is my opinion of things obviously there's a lot more to it than just this view and a lot of my colleagues did a good job of trying to articulate all of the areas related to broadband nutrition lucas thank you very much i think there's george michaelson in the queue i noticed the last minute i'm hoping he can reach out to you to ask a question um if if that question stands but i'd like to move on in the meantime uh let's see looking for the next presenter uh hold on where did you go"
  },
  {
    "startTime": "01:00:03",
    "text": "sofiki islam are you in the room okay sure is yours welcome yeah can you hear me guys hi uh good morning my name is shafiqul and i'm going to present is it really necessary to go beyond a fairness metric for next generation congestion control this is joint work with christian carsten and michael from university of oslo next slide please next slide please oh yeah can i do it from here oh yeah sorry used it that's why yeah sorry for the devil um traditionally the evaluation of fairness and evolution congestion control mechanism encompasses an examination of fairness this is done by calculating throughputs from uh of computing flows from experiments or simulations and this data vector is fed to an equation right here and a singular vector vector jane's fairness index is calculated here n is the total number of flows and x i is the throughput of the if connection"
  },
  {
    "startTime": "01:02:09",
    "text": "so the concept has been used to judge how multiple instances of the same congestion control mechanism interoperated interoperate in heterogeneous and homogeneous set of conditions and this is also used to evaluate whether a new congestion control mechanism is fit for deployment on the internet and this is done by evaluating fairness when a new mechanism competes with the prevalent congestion control mechanisms traditionally renault more recently cubic 15 years ago bob briscoe introduced the term um flow rate fairness in his ccr paper and he argues that fairness should be defined in relation to cost per economic entity not perflow however even after 15 years of publication it is still common to evaluate mechanisms on the basis of flow rate fairness uh what happened could you please share the slides again yeah i still don't have yeah got it yep so the question is does such a fairness test indeed provide a good"
  },
  {
    "startTime": "01:04:01",
    "text": "reasoning about the deployment of a new congestion control mechanism so in a recent hotnet paper where at all suggested to use the harm concept that is how harmful a new congestion control algorithm is to prevent prevalent congestion control algorithms so this they suggested that throughput alone is not should not be used as a performance metric developers should also focus on various uh performance metrics such as delay loss and flow completion time so the harm concept is practical seems practical but its practical merit hasn't been explored and demonstrated because this requires more experiment experimental data than the calculation of jane's fairness index so we provide the first evaluation of using a fairness metric versus using harm with representatives congestion control mechanisms so the outline from now on first i will introduce how to calculate harm and then we'll present the representation that is suitable for comparison of harm and fairness metric and also we'll introduce our measurement setup and results so the paper suggested many ways of calculating harm the one that is fit for deployment from from the high harm paper uh dimension we suggest that if the harm done by a new congestion control algorithm alpha to a widely deployed congestion control algorithm beta is comparable or or less than less than the harm done when beta competes against beta we should consider it acceptable to deploy so based on this we carry out two tests for all our scenarios first uh in the first test we use flow alpha a new congestion mechanism competing with flow beta"
  },
  {
    "startTime": "01:06:00",
    "text": "which is the baseline congestion algorithm in the second test two baseline flows beta1 and beta2 competing with each other and we map uh from a flow to specific measurements as m column flow metric value so we actually presented a linear representation of the flow the equation is available in our paper and it's this still old slide anyway so uh so what we did we ran two experiments where experiment one has this alpha flow and uh versus beta flow and the fairness matrix is calculated from mf is calculated from m alpha and beta and from the experiment to where we have beta 1 and beta 2 flow completing we took m beta from experiment 1 and m beta 2 beta 1 from experiment 2 and calculated the harm metric from here the negative value uh corresponds to alpha causing much more harm to better where the positive values represents better harms alpha when they compete and zero means no harm so our measurement setup has we run experiments uh in uios teacup physical testbed where we varied the link capacity to 10 25 50 75 and 100 megabits per second and the round trip time was valid to 10 20 50 and 100 milliseconds and queue size was set to half the bdp and the full bdp for each bandwidth and delay case we chose four different congestion mechanisms uh based on their level of aggression and the congestion signal they use the first one is renault that that is very traditional and and then we show and that's the answer loss-based mechanism then we chose cubic which is more aggressive than renault and also lost mechanism than bbr which"
  },
  {
    "startTime": "01:08:01",
    "text": "is uh more aggressive than renault and cubic and vegas which is the least aggressive here and also and primarily a delay-based mechanism so results we show in three categories we investigate harm metric for cubic versus renault and cubic versus bbi flows we investigate how the harm metric behave when we run a loss-based flow against a delay-based flow and we share experience whether the deployability of new congestion control mechanisms could be judged by a harm-based approach so first thing we try to identify eliminate scenarios where cubic falls back to linear tcp like growth the heat maps here show the whole parameter space where we found the interesting cases at rtt when rtt is above 50 milliseconds and capacity is greater than 75 megabits per second where uh cubic doesn't fall back to linear tcp like growth uh in the and we cons we call this from now on the high bdp scenario and in case of low bdp scenario we found no difference in the fairness and harm because cubic behaved similar to renault so this is the result of harm versus fairness distribution of cdf of image throughput values for different alpha beta pairs uh it could be seen that the black line cubic does uh moderate harm to renault bbr does significant harm to cubic whereas renault uh harms the most to"
  },
  {
    "startTime": "01:10:00",
    "text": "vegas damn it all right my fault was preparing for the next speaker and i clicked the wrong button apologies safiq here you go yeah noise yeah so this shows uh the harm metric for different alpha versus beta pairs and then we look at uh the relative harm and fairness distribution we normalize the uh mf and mh fairness and harm values for varied alpha cubic and beta renault pairs and here it could be seen that the image throughput and fairness metric it's not like obvious like to see the differences here but if you look at the uh mhrt metric the impact of cubic on renault is uh quite visible when in case of throughput metric it wasn't and then we ran the same test similar test with bbr versus cubic pair the impact of rtt is still there but the uh mfn image there is a pronounced gap in between and this is because fairness is quantifying uh as measurement as the equal capacity sharing lengths in case of renault competing with venno it provides kind of better fairness when cubic competes with cubic it's not and it's kind of cubic is a complex and modal algorithm so that's why there is a gap here and then we run the same test"
  },
  {
    "startTime": "01:12:00",
    "text": "similar test with and look at the normalized mf and image values and for the vegas alpha competing against uh renault better flow and this is across the entire parameter space and the mf values is slight to the left this is because the unfairness issues in vegas because a simple delay-based mechanism has some unfairness issues such as late camera advantage and finally we look at the case study of absolute fairness and harm where we look at two scenarios that are relevant with regards to deployment of new congestion control algorithms in the first scenario we look at the case where the introduction of cubic in the prevalent when renault was dominant and in the second scenario we look at the case where bbr supersedes cubic and then we calculate harm metric from these two values where we take uh m beta from experiment one and uh m alpha from experiment two to calculate hum metric and then we look at the absolute fairness and harm throughput harm comparison between alpha cubic petrino and alpha cubic gamma bbr in cases of high bdp scenarios and we show the raw harm values here and it could be seen that bbr captures more resources than cubic captures for renault than renault in all cases and more specifically it shows that bbr captures 1.6 times more resources for 50 of the cases so to conclude uh we say that"
  },
  {
    "startTime": "01:14:00",
    "text": "an old slide so we we applied the harm concept to data produced from experiments and comp with competing pairs of various tcp variants we looked at four different tcp variants based on their level of aggression as well as different feedback types we presented a linear representation of harm to better assess differences between the metrics we also illustrated the efficacy of harm-based approach using experiment result that we showed in future we plan to investigate the efficacy of harm using other performance metrics such as loss so to conclude we say that the harm-based approach is more useful to assess whether a next-generation congestion control mechanism is safely deployable or not all thank you very much thank you very much does anybody have a quick question they want to ask i'm going to try and recover some time i'm going to ask you just a a quick question here is um so i i i think harm is gaining some traction which is lovely to see do you have any thoughts on on how to evaluate this in the real world in addition to the end so the analytical tools are developing um and and you know at scale small things tend to show up that don't show up at small scale or on paper it's more of an open unfair question i realize that's a very good and good question to understand so i mean like this requires as i mentioned many experiments the data to test it and and harm itself is kinda complex to do it because it's it's not easy to calculate and there are many many many approaches presented in the paper to do it so we chose the one that we found kind of fitting for figuring out if it can be safely deployed or not so if you want to do it in real life experiments we need more more"
  },
  {
    "startTime": "01:16:01",
    "text": "experiments and more data so if we can do it it's it's possible to do it but it's like okay it's it's like extra traffic that you need to impose to compare against thank you thank you very much sephiroth very good uh next up jan yvang fabulous thank you very much for the brilliant presentation i i'd love to see harm get more traction on a personal level there was one more question i want to ask i'm sorry question it's possible sorry no no please go ahead ask it quickly you're here at the mic okay my name is jose from amazon web service so i just would like to have a simple gratification question so when you compare cubic cubic reno and renault and renault do they have the same round three time or do they have different parameter time oh yeah we do the experimental setups or settings like they were like the same when we come compare against right because like each case they will be compared with the same case scenario yeah otherwise like it's going to be unfair right yeah i know but this is because you know rain office around the time drastically something new you know two floats has a different round three time the bundle share is completely different but on the other hand you know in the case of in a cubic uh the condition with growth doesn't affect around the time very much so that's that means it's not very it's really difficult comparison between can you think about comparing cubic and you know for fairness that's my from my point of view specifically i'm going to interrupt here and say that's a fabulous question it's worth thinking about cephacool and um i would encourage you to interact offline okay we'll take it offline thank you very much okay moving on cloud cross layer network outage classification jan i believe you're at the podium fabulous thank you for joining all yours hello let's see hello everybody everybody my name is jan marius evan i'm a phd student at the oslo metropolitan university and i'll be presenting this paper on"
  },
  {
    "startTime": "01:18:00",
    "text": "behalf of my research team at the simula metropolitan so the paper is called the cross layer network outage classification using machine learning which should be more or less self-explanatory we want to classify network outages in an ip network using data collected from various network layers using machine learning methods let's see yeah so the paper focuses on the real world challenges or classifying network outages and especially on providing good feedback to network operations for troubleshooting and to customer support for informing customers about what actually happened so my first first i introduced the testbed and the problem statement and then i'll in talk about the methods and the data we used and finally i'll present the results so the network that we use for this analysis is a global network for quality aware network traffic most of the customers here are video conferencing users who need very high quality video they're better quality than the internet can provide the network is spanning four continents with 11 cities four in europe for three in north america three nations one in australia and these cities are connected via least like lay two services from three different providers for redundancy this is uh talia cogent and equinix providing these later networks the network connections are relatively low cost which also means that the quality is"
  },
  {
    "startTime": "01:20:02",
    "text": "less than what you would expect from the most expensive services yeah so our research shows or before we started researching we knew that the network problems were quite frequent we analyzed two years of data and we found more than seven hundred thousand packet loss incidents this uh for this number every time we during one minute measurement lost a single packet we counted this as a packet loss incidence so even this large number of incidents is actually within well within the service level agreement for these services uh but of course not all of these turn into support cases mostly because of routing protocols that protected against outages but all together during that time period with the operators saw 2855 customer support cases that were related to network quality and we found that out of this only around 35 percent actually received a proper response from customer service regarding what was the actual problem that happened and this this is a far too low number so we [Music] that's why this project was initiated so first we started collecting data we used the snmp polling from all the routers and switches to collect the optical signal strength from the links to the providers and internal links we also used snmp to collect interface error counters and buffer overflow counters and we got access to all the information about software crash logs we actually noticed the 62"
  },
  {
    "startTime": "01:22:00",
    "text": "instances of core dumps in the in the [Music] in the management plane of these routers and switches and surprisingly none of those actually led to any customer complaints we also got access to the configuration change logs which was an early hypothesis that the configuration changes made problems but in the end we found that we needed more data so we set up bfd logging bfd is a binary forwarding bidirectional for the detection protocol quite a lightweight protocol that's implemented on all these routers it's usually implemented on the interface cards so it doesn't stress the cpu in the router and in this case it was actually already implemented and in use for for the higher level protocols it's uh it actually provides a very simple [Music] measurement it's sending packets and it triggers the an event if uh packets are not received and this improves the failover times for for higher protocols like isis um after and we also set up layer 2 packet loss and layer 3 packet loss measurements using udp ping this required quite a lot of virtual machines set up quite high cpu usage compared to the vfd protocols that were very easy where data was very easy to generate easy to process and easy to store in a centralized location yeah and then in addition to this data we got access to the customer complaints data anonymized of course and to the customer support responses to those cases"
  },
  {
    "startTime": "01:24:00",
    "text": "first we created a tool to visualize all the data this data was very useful both for the network operation center and for the customer support but it required a lot of training and a lot of manual pattern recognition to to recognize what data is actually wrong what happened what what was the cause of these customer complaints and this is obviously a case for machine learning pattern recognition so then we initiated a machine learning program to try and use this data to automatically classify outages the first step in this process was to do an in-depth analysis of all the outages or all the customer cases where customers complained about all features and uh we identified the root cause of all the complaints and i identified what sort of data was most useful to uh to finding the root cause uh from this list you can see that multiple or outages in multiple two providers at the same time tops the list and the reason for that is that out there just in a single provider usually didn't cause any customer complaints because the routing protocols are quite good at rerouting in those cases we can also see that most of these outages were caused by layer 2 issues so yeah just a very very quick introduction i guess everybody knows machine learning basic but for completeness basic goal of machine learning is to take a lot of data points and divide them into two groups these the data posts in this uh case are of course multi-dimensional with lots of"
  },
  {
    "startTime": "01:26:00",
    "text": "data input and uh the process is run multiple times to create multiple classes we use the support vector machine method which is basically as you see from the diagram here it transforms the data so that the decision boundary is as big as possible the dotted lines this method is more useful at classifying new data that it hasn't seen before and it's less susceptible to the machine learning problem of overfitting so what we found was that a two-stage machine learning process was the optimal solution for this case in the first stage we identified the most common outages which relate to voltages using bfd data only pft is very easy to to collect as mentioned and it's got very little impact on any equipment or any network and for many real life implementations this might actually be good enough for uh to do even for smaller operators with little resources collecting bfd trap data and then analyzing them would be a good idea and then but of course bfd being a layer to protocol it could not detect all types outages or root causes so then we added the udp ping data both lay two and layer three eurips big data and we managed to do a very quite quite good classification of all the other courses as well and this with this data it's much more it's much harder to collect the data it requires a lot of virtual machines it it requires a lot of cpu processing to actually collect the data while the entire machine learning process to"
  },
  {
    "startTime": "01:28:01",
    "text": "one second to classify a case so on to the results of the classification on the left here we see the first stage classification we used 75 of all the data for training our machine learning systems and we then tested the machine learning system on the remaining 25 and recorded the success rates in classifying those and a perfect classification would be 100 on all the diagonal cells so we see here on the left that it had a very good accuracy where it worked and for the other non-layer two cases it it needed the second step and there we removed all the ones where the first stage was successful and we also removed where the the issue was obviously on the customer side so there were no no errors detected anywhere and then we can come up with the results on the right they're not quite as good but uh and the reason for this is that some of these root causes have um have very similar symptoms like it's very hard for uh by just looking at the symptoms only to to see the difference between equipment maintenance and equipment failure for instance so to summarize machine learning proved to be very well suited to classifying outages in this network and using bfds and mp traps to log the layer 2 events was also very useful but we needed the udp packet measurements to classify problems in the other layers"
  },
  {
    "startTime": "01:30:00",
    "text": "then any questions free on so i i will note um very quickly so that we can move on um being machine learning in in a network capacity it's still some proving there's a bit of a proving ground here um people are always uncomfortable with what they don't fully understand uh and in the networking space this is only more important of course right because i think we all recognize that this is sort of a crucial service that we run um do you have a sense of where this might go wrong yeah well um yeah that question is uh the nice things about our setup here is that it has very little impact on the actual network there's no automatic decisions made based on machine learning there's very little configuration changes to the equipment all these protocols all the events created by the protocols were already in use they were used by by the isis protocol the excellent protocols to make decisions about fast rerouting the only thing we needed to do was to actually send snmp traps when something happened and that's a fair point to me sorry go ahead yeah uh and also the main outputs for this was information for the network operations center and the customer support centers to to provide manual help in troubleshooting things and to inform customers when somebody calls in and you you can tell them immediately yes we see the problem we know more or less what it is that's a much much better customer"
  },
  {
    "startTime": "01:32:01",
    "text": "experience than if you say uh yeah we don't know what happened uh we'll we'll let you know once we know something that's uh not a very good answer fair point to make thank you very much jan uh ator are you in the room or remote i believe that's you just passed by the mic you have control thank you yeah here can you all hear me yep yep oh good all right okay um so hi good morning everyone thank you all for coming my name is aitor martin i come from the university of stamanger in norway and i'm here to present our work titled on the suitability of bbr congestion control for quick over geos.com networks so i'm going to start with a little introduction on the topic so the first thing we want to say is that geosynchronous satellite communication networks are becoming interesting um items to provide um satellite broadband connectivity in many 5d and 60 use cases that use satellites for backhauling purposes or hybrid schemes etc and in parallel we're witnessing how um the transport layer is um going through some breakthrough and we're seeing quick being standardized and deployed and developed further and we're also seeing how modern conjunction control is also being worked on uh mainly with bbr condition control so in this um with this ecosystem here we're seeing how there is more and more um quick traffic on the web and we expect more quick traffic going through uh satellite networks so this is where we stand"
  },
  {
    "startTime": "01:34:02",
    "text": "so why is this important here um so um there are some challenges that satellite networks introduce for the transport layer and we have mainly uh long propagation delay we have propagation errors as well and we also have likely bandwidth asymmetry on the link and well tcp traffic is usually optimized for satellite links using the performance enhancing proxies that you might know called peps however unless some network-assisted solution is introduced such as mask um quick cannot use pipes because quick headers are encrypted so um the problem here is that many studies have looked into tcp with pep um and it has been shown that tcp with peps really greatly outperforms quick right now even with quick fast handshake and quick really falls behind so there's been raising interest into looking into improving quick for this satcom usgate so in this context we're going to look into the general performance of bbr over satcom um and with quick of course and we're also going to look into aspects such as fairness and we're also going to look into these items that i mentioned like packet loss bank with symmetry and also how the quick implementation choice makes a difference here so now for some background these are the three main properties that i mentioned about the satellite links and we have a long round trip time that implies a longer protocol feedback which affects the transport layer and on many different aspects and it is also implies a higher bandwidth delay product which implies also larger buffers"
  },
  {
    "startTime": "01:36:01",
    "text": "then we also have propagation errors and um finally the the bandwidth asymmetry that i mentioned this bandwidth asymmetry can be especially problematic if the uplink gets congested and axe cannot reach the sender smoothly this can really limit the for forward throughput performance so so there have been a lot of proposals on this area to try to improve quick and i'm not going to go into them but i want to make clear that we're focused on looking into bbr so um just quickly talk about bbr bbr what bbr does is it measures the bottlenock bandwidth and round trip time on the link in order to find an optimal sending rate that tries to maximize the use of the link while keeping the the path rt as low as possible um and well bbr have been shown to do really well in many use cases um when compared to kubic especially when there is losses on the path etc however there are some issues that have been found with bbr that are related to mainly fairness it has been shown that bbr flows can be quite unfair against each other and also pbr flows can really be quite aggressive towards loss-based condition control such as cubic um as well as rtt unfairness so uh with after this there was an update to bvr and bbr2 uh that tried to fix these issues tried to make bbr a little less aggressive while keeping performance high and this was made by making bbr react to to packet loss and explicit congestion notification and among other improvements and there have been"
  },
  {
    "startTime": "01:38:00",
    "text": "a wide range of research papers into this looking into both vr versions and comparing different condition control and but we saw that there was no paper looking into the latest version of pbr with quick and oversat so this is where we stepped in with our research so um now we're going to show you the experimental setup that we built um yeah so this here is what we built we built a testbed based on teacup um we have two pairs of hosts to generate quick traffic and we have a router that performs a link emulation and um the experiments are controlled using uh we call the tcap controller um the t-cap flat platform developed by kaya allows to design experiments and automate tests really easily and we took it and extend it extended it to be able to work with quick and to extract statistics as well as q log trace data and to emulate the satellite link we use netem and we set up two different scenarios with the satellite scenario which is our main focus and the terrestrial scenario so and we're using a 600 milliseconds rtt for the satellite link and for a terrestrial link 100 milliseconds and we have a 20 megabits per second downlink bandwidth and as you can see for the satellite scenario we also contemplate the asymmetric possibility of having 20 on the dow link and two on the app link and we also look into different bot buffer sizes and the packet loss ratio values as well um we use two quick implementations um these are"
  },
  {
    "startTime": "01:40:00",
    "text": "ngtcp2 which we chose because it is the one that allows us to to experiment with vbr2 with the latest version and we also use picoquick and which has been shown in some studies that it performs quite well over satcom so this is why we also wanted to look into this so yeah that's it so now to jump to our results um we set up four different experimental scenarios um the first of this is the single flow book download um here we start the download of a very large file from the client and he we let the download run for two minutes and then we measure output on the link and um yeah and this is what we saw so first of all here we show you our good put results um with on the satellite and terrestrial scenario and for different buffer sizes um with the two implementations that um i talked about so clearly um as expected we see performance being lower in the satellite scenario and we also see how pico quick is doing significantly better than any tcp 2 in both use cases but especially in the satellite use case [Music] if we look into transition control differences we also see how for ng tcp 2 um kubik is gaining some advantage for higher buffer sizes and while bbr versions remain stable along buffer sizes and we can see how bbr2 is uh giving us slightly lower good put values which can be expected due to the its less aggressive nature so then we introduced some packet laws"
  },
  {
    "startTime": "01:42:00",
    "text": "and to see their impact and mainly we saw two things we first saw how ngtcp 2 with cubic was losing a lot of performance especially in the long rtt satellite scenario and uh also when we increased the packet loss even further to one percent to one percent um we saw bbr2 actually falling behind bbr1 a lot and we only see the cds in the satellite scenario so we believe that this mixture of long rdt and high pocket loss is problematic for for vbr2 here so finally for this scenario we also looked into introducing some traffic on the uplink in order to see how the problem of the asymmetric bandwidth could have an impact here and we saw we tried different combinations of congestion control algorithms and we saw that while ngtcp p2 drops in performance a lot as you can see here on this good results that we when you use this asymmetric bandwidth setup and we lose a lot of performance and on the other hand when we use picoclick we use pico quick here and we are maintaining a really good performance even when there's congestion on the on the uplink and so we looked into this and we saw that picoclique is actually um using a different act policy where it tries to send less acts like acknowledgement packets and it tries to optimize this for the satellite link so this is why we stress the relevance of these strategies"
  },
  {
    "startTime": "01:44:01",
    "text": "so then we also looked into fairness of simultaneous flows running down different flows at the same time and for this we use dense furnace index to measure how fair parallel flows are against each other so this uh this is what we saw we saw cubic um maintaining really good furnace uh even with 64 parallel flows however our research our results showed that a bbr2 and bbr1 flows competing against each other um against themselves sorry i'm drop a lot in fairness when we increase the number of flows um especially with bbr1 we do see that bbr2 does a bit better than but still the the furnace um values here are quite not good when as compared to kuvik here then we also compared condition control algorithms against each other with parallel flows using different condition control algorithms and um here what we mainly want to address is that um bbr2 with cubic is behaving quite fairly which is showing that even with the long rtt um bbr2 and has managed to improve these furnish issues that bbr1 had towards cubic however if you look at the blue box plots on the right that show bbr1 against kubic you see quite worse results um i'm going to go a little bit faster um so for the latecomer we also looked into the latecomer issue where latecomer flows struggle to join the link and to they are either too aggressive or they fall behind"
  },
  {
    "startTime": "01:46:01",
    "text": "so we set up a four flow scenario where different flow starts at different times and they all run for for three minutes and yeah this is what we saw so um here we show you the good put over time over five minutes of four different flows that start at four at 0 40 80 and 120 seconds and we also show you the rtt measured there in the graph and as you can see cubic light commas are quite slow and because of the satellite rtt they take really long to converge to a fair share um however as you will see on the next slide the picture changes a bit when we use pbr so when we when we use vbr here um we see how the latecomers join the link faster and they get a fair share faster than with cubic and um as you can see here um it's also pretty notable that with bbr2 these latecomers are less aggressive well bbr one latecomers actually overtake the previous flows and they make previous flows drop in performance quite a lot however we did see an issue here with vbr2 where bbr2 after the rest of the flows and it fails to recover the available bandwidth um and we have seen that this is a consequence of the long rpt because we repeated these experiments with lower rdt and this doesn't happen so we believe this might be either some problem with vbr2 for long rdts or perhaps some implementation issues with ngtcp 2 and dbr2"
  },
  {
    "startTime": "01:48:00",
    "text": "so yeah i'm going to jump to the discussion there was one last scenario to look into um if you're interested into it you can read the paper or we can talk about it later but i want to be a bit quicker here so um to to give a general idea of what we can conclude from this study um we have seen that bbr can provide better performance under los links um and while bbr1 is the one that's giving us the best performance um bbr2 is doing a great job at improving fairness um towards kubic for better coexistence on the internet and better behavior however there are some problems that bbr2 has to face for these satellite networks um and yeah we believe this needs to be further looked into we also saw uh really uh how this problem with bandwidth asymmetry and axe arc frames on the return path can be a problem because we were seeing any tcp to give having problems to deal with this and uh we this is why we stress the need to look into act policies and satellite optimized act policies um that try to negotiate um strategies with that send less acts and try to avoid these these issues so to summarize even though bbr seems like a good candidate for some reasons there is some some stuff to work into here um and as i said bandwidth asymmetry is proving to be a great challenge um for satellite links and um we also we're also surprised to see pico quick um doing so well with their"
  },
  {
    "startTime": "01:50:01",
    "text": "satellite optimizations and we believe um looking into these strategies that they use might be clue for quick overs outcome um in the future thank you this is great well presented and thank you for accelerating towards the end and covering some time um i think what i would like to do is uh just move on but encourage you please to check the chat channel because i think you've had some good engagement and a couple of thoughts of questions um there are certainly a couple of things in here that are that are counter-intuitive and and i think the um observation about act patterns is is particularly novel so if people in the quick working group are not in the room you might want to reach out to them lucas for example who just gave the keynote uh and and might have something to offer there okay okay thank you thank you very much this was great thank you moving on to our last short presentation uh let's see nushin are you in the room um yes hi control is yours take it away uh yes yeah hi everyone thanks for joining this presentation i'm lucie nervous i'm a phd student at the university of alberta and uh for this project i worked with my supervisor dr paulu and the project i started priority ever forward or correction for http um the thing is that when the http client asks for a web page there is a list of web resources that need to be downloaded but not all resources are the same for example we have images that can be rendered incrementally at the client side"
  },
  {
    "startTime": "01:52:00",
    "text": "so we can use the multi-streaming or multiplexing of a quick to avoid the headline blocking problem uh at an application layer in the case of paco loss but there are also other types of web resources like html css and javascript that need to be downloaded completely before getting used so we call them high priority resources and uh in this project we have the idea of using forward or connection for these high priority resources over http 3e because in the case of packet loss the client needs to wait for the retransmission of the last parts as a function of rtt but by using further collection we can recover sooner and we can reduce page load time hopefully the thing is that to implement this idea we need the priority of different resources at the server side or the sender side and we previously had a complicated dependency tree prioritization scheme for http but because of its complexity it hasn't been widely used but recently with the extensible prioritization scheme the client can set the priority scores of web resources so the server would know which ones are higher priority and then we can use forward or collection for them and actually for the forward or collection part over quick it's not a new thing a quick have folder connection but for our data initially and because of the overhead of order correction it was dropped but the idea here is that we can use them only for higher priority resources and for the paper that we submitted to this journal we have the numbers um over udp-based there's a transfer of udt protocol and we use two-dimensional x or baseboard or collection but after we got positive reviews from the workshop we were encouraged to implement this id over quick and we have chosen ngtcp2 as one of the open source implementations out there because it"
  },
  {
    "startTime": "01:54:00",
    "text": "includes the recent extensions to quick like the extensible prioritization scheme that we need for this project and to further simplify the task we pre-computed the repair data and wrote them in a file at the server side so in the scenario that we want to use for the collection for hybrid resources the client need to ask for that as a web resource as well to be able to use them for recovering last quick frames and uh for the portal collection part we use the open fetch open source library that has the implementation of rich solomon and ldpc encoders and decoders and we use we use an emulated testbed over emulab and we use netemtc tool to add delay and packet loss um to the link here are some early results um of uh setting um 100 milliseconds for the rtt and 10 percent 40 packet loss and these two diagrams show the arrival order of quick frames at http client without forward or correction and with forward location as a workload we had a very simple work load up only one high priority resource uh the green one and then we computed the repair data for this high priority resource we got a file 1.8 kilobyte yellow file and then we also had another web resource the low priority incremental resource the pink one with 26.5 kilobytes and the upper diagram showed what would happen um at the client sand when we do not use forward or correction for the high priority resource um we have uh the quick frames for the high priority the green bond and because we have packet loss in the network one of the quick uh frames the green one uh got lost so we need to uh wait for 103 milliseconds uh to receive the retransmission of that uh last quick"
  },
  {
    "startTime": "01:56:01",
    "text": "frames and we also could finish the whole job at uh 320 millisecond but with forward or correction the client needs to ask for the high priority resource the repair data and also the ping resource and the thing is that we could recover the um two last frames of the green high priority resource at seven milliseconds using two redundant data quick frames so uh while if we wanted to wait for the transmission we need to wait for 109 millisecond and we could finish the whole job um at 312. and the improvement is 7 milliseconds versus 103 milliseconds without forward lock action and as a summary uh we said that we can use resource prioritization as the server side to use a forward or character only for high priority resources uh and this way we can reduce page load time and uh as the future work we are trying to improve our implementation real quick and we need to study the condition control impacts of using forward or collection thank you very much for listening and i would be happy if there's any questions or suggestions thank you very much nasheen any questions at the mic i think the the only question i would raise here is the is the 10 packet loss yeah the only reason that we chose this high our person is to see the impact on this small size low uh high priority resource because we had only seven quick frames here and we wanted to make sure that at least one of them got lost but of course if we have a large file we can reduce it um to for example one percent and you could see the impact that's okay yeah great thank you very much uh only saying so because it's hot it's i don't know that anyone is seen"
  },
  {
    "startTime": "01:58:01",
    "text": "really at that high pack but i i get i see the reasoning thank you very much for the presentation thank you um let me check the chat channel quickly uh i think we are in the clear thanks nasheen and thanks everybody for attending please return around uh 3 p.m where we have an excellent set of talks waiting to be given something new for the irtf thanks everybody for joining see you soon you"
  }
]
