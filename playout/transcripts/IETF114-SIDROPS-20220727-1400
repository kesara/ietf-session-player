[
  {
    "startTime": "00:02:26",
    "text": "away like this people can't hear you if you do this and you're like people can't hear you keep the mic in front of your mouth please also closer like i wasn't doing over there so this doesn't really help it's got to be upgraded like like this you can just do like you know all right it's working okay so back down to the seat oh and feel free to take the mic out of the stand okay as we were as i was saying be nice to other people imagine that they're trying to do the same thing you are which is make the internet better please don't make us have to say this again okay and then after that warren has some intro stuff unless somebody else has other agenda items they'd like to hop to the mic maybe it's not working again well i mean audio is working video i can do we have to dance now george to keep the da okay okay audio video we have both the a and the v"
  },
  {
    "startTime": "00:04:01",
    "text": "my god you're tall there we go so hopefully audio and video continue to work for a little bit actually hopefully they continue to work for the whole thing um so hi everyone i'm warren kumari i'm the op sadie and have been 80 for this working group for what feels like a very long time um i am still enjoying being ops 80 but my term is up in march and i will probably run again but what i would really really like is for a bunch of other people to run um and so if anybody's interested in knowing what the ops id role is like what the time investment is what the fun parts are what the less fun parts are please come along and talk to me and i'm happy to provide background chat about what it's actually like etc um as i say i've been doing it for a while and i've been doing it for a while because it's actually kind of fun um some parts more so than others but yep as i say come hunt me down let's have a chat i'm happy to to provide any background etc and um natalie does it still seem like audio is working yay okay oh my presentation got kicked yep so all this stuff saw warren too far okay i think the next person up is igor who was here oh still here excellent um i think you don't have a phone with you so i can do the slide click it clicks oh can you can you just drive the slides and that's yeah exactly i can do that as soon as i find the slides it's pa well i'll introduce myself first while the slides are coming up and say here"
  },
  {
    "startTime": "00:06:01",
    "text": "so i'm igor lubachev uh this is something that dawg and myself have been working on sri rama's in india but he is virtually with us so this is something that a problem space that's typically not been worked inside the wraps and maybe that's why it's still a problem so we're trying to bring it to this group because we think that cider apps have some very unique and interesting techniques that can help so next slide so the problem is source address validation on the internet or basically how do we stop bad people from spoofing ap addresses and doing bad things um problem is not new uh been around well in been around forever uh in 2000 we even had the draft that says that's bad should fix it we're in 2022 still working on this problem the best we've got state of the art is look at bgp messages insure some information from it uh the algorithms that we have don't work other than in theory because real networks i mean the algorithms require no route filtering no real traffic engineering that can happen uh well that's not real world so the feasible path of rpf has been with us from 2004 um yeah two years ago there's been an improvement uh 8704 which basically said hey let's look at those bgp update messages one more time there is a little bit more information we can infer this is origin as number so if you find on the interface some route advertised by that origin is uh look at all your bgp announcements and any route advertising any prefix advertisement that originates"
  },
  {
    "startTime": "00:08:01",
    "text": "should probably also be okay and there is also one small paragraphs in the in the draft like really an afterthought and talking to the author of it it really is an afterthought that says you could also look at raw information and maybe augment uh your data set uh with that info but anyway uh it's much better but still has a bunch of problems and here we are today next so this is a simple very simple example why 8704 is not working you have as1 which is a customer of as2 as2 is multi-homed and it propagates both propagates customer route to both providers but its own route it only wants to propagate to as3 for reasons if s4 is doing source address validation s4 will not see any prefix advertised with as2's prefix 2 on its customer interface and therefore it will not accept packets from prefix 2. so 22 years later people are really trying to fix it why are we still having a problem and we contend that it's the root cause of it is that we're trying to ensure data plane forward information from bgp signals that was not designed for it uh so it just doesn't have enough signal the next idea is well we have other sources of authoritative information so for example raw and we're talking about aspa and somebody also suggested uh earlier this week what about signed iir data and actually why not so um again that"
  },
  {
    "startTime": "00:10:00",
    "text": "all the signals that were not designed for source address validation so all sort of hacks but that's what we do in the internet we try to do hacks that work um so that's why we're here and trying to see what we can do with nasa but next before we go a little bit further uh next slide um just uh first of all i apologize for this slide it's a little bit it's got a lot more graphics that were used at the atf i just pulled it from an internal presentation but it's basically a real problem that i'm very familiar with because it's a cdn that's trying to serve traffic on any cast ap address and the anycast well of course any cast home pop will look at the incoming request and decides there is an hp that's better suited to serve the traffic from so it will tunnel some sort of ap and ap thing uh the packets to the edge and the edge wants to reply directly and that's where the bulk of the transfer is happening so the only thing that stating the detour through the anycast address is the headers and the acts the heavy duty stuff is going direct pretty neat infrastructure next so this is a more typical picture of the same thing um doesn't have to be any cast it's like any sort of dsr direct server return prefix four has a customer who is conducting uh service at prefix three which is advertised which happens to land in as1 which tunnels the packet to s two the age and there it needs to s2 wants to complete the connection but"
  },
  {
    "startTime": "00:12:01",
    "text": "it needs to source packets from the prefix three so that the end user sees normal connection happening and the question is will s9 this provider allow such packets and dsr is a obviously case well it's an important case cdn i just described and so is mobile roaming and some gaming and security products what not many of the services use the sro wants to use dsr next so here is a crux of our proposal we call it bar self uh bar serve so bar for bgp we're still using bgp messages we need to get get information from there but we also augment it with aspen raw uh it is strictly an improvement on 8704 even in the way it just processes bgp messages uh so 8704 just looked at the origin as number and bar server is looking at the entire asp and we'll see how it does it but it basically gets more signal from existing messages but it also augments this information with aspen raw um the good thing that we think it's actually valuable is that it requires no new protocol no new changes to existing protocols oh that's nice um can we get the slides back at some point and um thank you and the the fact that it requires none of that means that uh it's actually good for uh for adoption because the very very first network that deploys something like that will immediately see benefits so that's that's what you're going to see is that early adopters actually get value next"
  },
  {
    "startTime": "00:14:00",
    "text": "this is a little busy slides many circles but the point here is only simple one that the protocol works on both peer links and customer links exactly the same way next now this is a high-level description of the protocol this is a simplified version in the utopian world where everybody has adopted aspen raw and we have the entire internet there i mean of course it's way way in the future but um basically the way it works is that you have two phases phase number one use ask the information to discover customer call so all the customers of all of your customers and their customers transitively and once you have the customer code which is the list of as numbers look at raw information and find the prefixes that they've advertised that they own those are the prefixes that are allowed in the interface next so let's get back to the real world thank you um the real world is very very similar it's still exactly those two phases find the customer cone and once you did find the prefixes that those customers own to find the customer code what you do is you look at aspa when available and also look at bgp as path and those can be received from anywhere uh your cast this customer interface other customer interface transits even provider um and look at the asp consider every single as number in there and the previous one is the customer of the of the of the next one so"
  },
  {
    "startTime": "00:16:00",
    "text": "look at the uh look at asps look at aspa do it do it iteratively until you can discover no nothing new that's your customer code and then look at raw and also look at just like 8704 prefix is announced through bgp and of all all the prefixes announcer bgp where origin is in your customer column and that's your list for validation next so next so very busy slide the point here is not to read it but in detail but this is just an example where you have a bunch of different things happening you have some prefixes that have raw some that don't some prefixes that have asp some that don't there is a lot of um traffic engineering map no exports happening things like that um next and let's the point is just to illustrate quickly how bar serve works with something like this so next slide so first you will start with the only s number we know which is the as number on the other side of the interface that we are looking at uh so that's as3 and well there is nothing in aspas that shows as3 as a provider fine but there is a bunch of bgp prefixes we have that have as3 so collect everybody before it and that's the new as numbers you discovered next path the next uh iteration repeat now we found something in aspen and and aspess repeat okay we found nothing new we have a customer calling list"
  },
  {
    "startTime": "00:18:01",
    "text": "next for each one in the customer cone look at raw and look at bgp announcements of which one is the the one that features that s number as origin as you merge it two lists together and you have your subfilter two more okay that's your sub list uh at the end next all right if you go back at the example we had before that 8704 couldn't deal with bars have deals with it trivially in fact it discovers that as2 is part of a customer cone in a trivial way it doesn't have to be directly connected it could be like an another network between as2 and s4 doesn't matter it will discover it and then it will see that there is a route received from the pier that shows as2 as the origin is and prefix 2 will be accepted wow we have a good um all right so imagine my dsr slide while it's been found now um what can barsap help with dsr well it's actually pretty simple all you need is that the cdn owns both edge uh both the hs number and the anycast home cs number cdn owns uh all the prefixes prefix um next uh two more times yeah good and one good so cd yeah good thank you cdn owns prefix one prefix two and prefix three so all it needs to do is needs to publish raw"
  },
  {
    "startTime": "00:20:01",
    "text": "that says s2 owns authorized to advertise prefix 2 and prefix 3. now s2 the h1 will never want to actually advertise prefix 3 but that's ok doesn't matter it owns it so it publishes raw and therefore as9 when it does bar save will find from raw that s2 owns prefix 3 and it will allow next um asba obviously can also help with route leaks i mean that's what it was designed for uh it also helps barcelona in the same way that when it's trying to infer information from bgp so in this case we have uh as2 that's leaked route from its peer and the the red one and somehow the trout made it to as4 now the trout should really be rejected for for uh for forwarding because after says that it's leaked but it made it to s4 somehow um barcelov when it before it tries to ensure information from bgpas path would check that hey uh i'm thinking of adding s 8 as a customer of as2 because it comes after s2 but s8 actually has a aspa entry and s2 is not its transit so it will reject adding this s8 however we say that that it will reject but it can still ensure that as2 is a customer of s9"
  },
  {
    "startTime": "00:22:00",
    "text": "no reason to do that and in fact if it's needed it will never be actually needed in this case but um as5 is a customer of s8 um so basically aspar validation also helps ourselves to construct the correct list next and actually i want to do one more slide just one one more that will be my last slide just to show that this problem is important and this problem is not being solved on the internet in 2015 we've uh looked at atacama we've looked at several thousands of our pubs where we are and to see how many of them do any sort of source address validation and we found that about 15 did and just before coming here i pulled the stats and then seven years later about 15 of the networks do source address validation so i don't think it's purely economics it's mostly like it actually is not working too well the algorithms that we have and maybe uh they and we really expect that improved algorithms with data produced by cider ups will actually move the needle thank you questions speaking as a working group member i think it's a great presentation two points i will make is that it'll be great to see how this solution solves the problem for ibgp cases while you have depicted ebgp cases"
  },
  {
    "startTime": "00:24:00",
    "text": "most of the problems um will probably also require you to have a control within ebgp it's quite possible in the scenarios that across ibgp your your uh a solution may not be honored or may not be enforced but what is critical is if we can sort of zoom into ibgp cases that would be phenomenal yeah thank you thank you that's um it is i think it's very valuable to also look at ibcp one of the things that we want to do is that on the internet is that okay if one network will not enforce it maybe the next network should have a chance uh to still do it that said obviously all this serve filtering best done as close to as close as possible but that's a good point okay since the chair seems to be distracted jeff has uh i worked with sri ram and doug on 8704 i'm going to offer two observations for you that feed on the stuff from 8704. the the core enhancement from 8704 is one part you can add stuff to your source address validation from additional bhp data that's not being used immediately for forwarding so being able to see it from other sources is great you know the presentation you're giving is an excellent example of that we talked about rose as one example in 8704 i'm glad to see this is going forward but the second thing ties into the slide you're displaying here about the economics i was like why haven't we seen more of this stuff it's one part the tooling for adding 8704 is not out there but the bigger one is source address validation in hardware is predicated on burning fib resources to do the extra lookup to see if you know that you can actually do this sort"
  },
  {
    "startTime": "00:26:01",
    "text": "of this validation it's cheaper than doing firewalling so from that perspective it's a wonderful thing but it's still an additional cost in your fib in cases where you can uh have sev covered by what you're using for forwarding you basically get it for free just at the cost of additional forwarding lookups yes every other thing that we're looking at here and as you start expanding these use cases you're looking at effectively you know doubling or tripling the size of your fib to be able to implement this functionality so part of what you're fighting against is the economic cost of something that's there for security that isn't actually selling moving bits around it's actually to help you stop building bits so thank you that's definitely i mean that's part of the point i'm making here that economics is definitely a driver probably more for some networks than the others some networks would actually benefit and i mean we do see that 15 of the network chose to implement something so there is some value there so but anything we do is important that yes it's as economical as possible especially for the smaller networks because that's where the source address validation is done best and especially for the first movers hi ben madison work online um so there's three separate things firstly just to kind of continue from the point that jeff was making um certainly all of that's true but one caveat to that is at least speaking for the network that i operate we typically run out of face plate interfaces long before we run out of packets per second so that that that that interaction with the hardware is not necessarily a deal breaker even for reasonably large networks that are kind of a similar sort of a shape um the second point i wanted to make is"
  },
  {
    "startTime": "00:28:00",
    "text": "mostly to just reiterate what i mentioned in savnet which is that using um rpki objects in this way kind of breaks the the um the fail open semantics that they have in their current use case and i think we need to think quite hard about that um i think there are other use cases where we want something that's kind of like a sticky rpki object i think that's going to be required if we ever want to replace things like the irr so i think that there's other work other useful work that would need the same sort of thing um and then the third thing is your example of pruning the customer cone using the aspa i think is problematic semantically um because i can imagine a scenario where a customer wants to use a transit link purely in the outbound direction and never intends to advertise any inbound reachability over it and therefore doesn't include that adjacency in their aspa but expects the return traffic to the outbound traffic to continue working and and using it to prune the the source address validation filter in that way breaks that assumption and i think that's going to be it would be if we wanted to do that i think we'd have to be very careful about how we document it so that that doesn't end up being a nasty surprise for a knock somewhere in the world at three in the morning um because it's such a corner case but it's a valid corner case um kind of all of those cumulatively leave me kind of feeling like if we want to be using rpki objects for source address validation i think i would prefer looking at defining new objects with those precise semantics rather than trying to kind of shoehorn the stuff we've got already into this hole i think it's a worthwhile thing doing potentially and i'd be happy to you know spend cycles and trying to get it done but i i don't love the trying to reuse"
  },
  {
    "startTime": "00:30:00",
    "text": "the existing object stuff okay thank you so let me uh try to remember a few comments so the comment about implementation yes uh you're absolutely right um and basically like i said in subnet i mean the devil is an implementation and yes if you have a temporary loss of cash your implementation should expect that this can happen and not start to deny a lot more as a last comment absolutely having a purpose-built signal is much less of a hack than using another signal that was not built for the purpose so it's i see it as a trade-off between doing one more thing that's new versus using things that already exist now aspa doesn't really exist so it's an opportunity um so i agree thank you i think cohen has a question as well he's the last person with questions so yeah yeah i wanted to um also uh continue on by point that then medicine made um university of atlanta by the way um regarding what there are what you expect the failure condition uh to be because we've seen that rpg publication points don't meet this uh even 100 uptime availability every at all times there's probably some replication point out there that isn't quite working as it shoots so you are not retrieving the roast or in the future objects that from there um and what i've heard from you or what i've understood that would mean that suddenly a lot of traffic is getting blocked which indeed goes against the feel open nature of the rpki um what does she how do you and you didn't answer to bad medicine that you to take care of what when your cash"
  },
  {
    "startTime": "00:32:00",
    "text": "goes but what if just part of your cash goes right so that's the same uh comment about the implementation detail is that uh how can it see i mean the first thing that comes to mind is uh you cash uh your information and you assume if something disappeared it's still valid for like 24 hours or 48 hours and only if it's still not there 48 hours later you remove it because you think maybe it's for a reason i mean that's just the first thing that comes to mind maybe other people will come up with some more clever heuristics okay all right jeff and then alexander yeah hi jeff houston um in looking through this what i understand you're trying to do is to take the simple case of source address validation which is a stub network where you've got enforced symmetric traffic because there's only one way out it's a stub and you're trying to synthesize that form of stub by taking an arbitrary subnet of networks and saying well what's the total amount they can sort of advertise in that arbitrary subnet which is a closed not not using available connectivity tool that doesn't care and in my head is this lingering doubt that when you try and assemble this sort of closed connected what you call customer cone which is not quite correct it's a cone of connectivity and saying well all the possible source addresses are now known because of rov etc"
  },
  {
    "startTime": "00:34:02",
    "text": "your implicit assumption is that the policy directionality is the same as isomorphic to straight connectivity and somewhere in my head is a niggling doubt that i don't think that's the case and if that is the case there's a problem there and i haven't heard you kind of reassure me that that's not true or directly address this issue that policy-based connectivity is isomorphic to just rate connectivity right so you're absolutely right and that's what and in fact everything that has been done since 2000 is looking at the signals and bgp is exactly the same thing that has directionality and trying to enforce the reverse direction and aspa is just another signal like that and that is the information that we do have and we don't have the other one right now and so the goal is what can we do given what we have because that's the only thing we can do and so well okay this is the only thing we can do immediately we can build something we can build something new absolutely but then when we build something new it has to have the property that earlier it's cheap for early adopters and early adopters get immediate value when they're the first network to adopt it otherwise you get problems and i know i mean ipv6 kind of comes to mind but let's not go there um so but in your essence absolutely right but we think that getting sba information and raw information will enhance the state of the art we have and get it better will it get it perfect no"
  },
  {
    "startTime": "00:36:03",
    "text": "prefixes coming from this bounded set of networks will create filters that are too enthusiastic that there will be valid presentations of source addresses that aren't in the list that's created the problem with going down this path is that the operator push back where an otherwise perfectly valid packet gets discarded because of some automated tool then becomes an operational cost and that's the underlying concern when i review this work going you're starting from a small set that's constrained rather than a large set that's maybe overly liberal but at least it encompasses all of that connectivity bound and you know there's more work to do here obviously but you know it's an interesting approach but it just struck me that policy and connectivity don't quite align the way you'd like it to for this work exactly so the goal what this thing is doing on top of what exists is that it's trying to expand the number of prefixes that it will find to put into your more permissive list so it's trying to make the list more permissive and will it get it perfect no and there was a suggestion how about you create a new sba record that's specifically for it that's maybe we could explore that um as far as the network goes so that was like okay somebody put sba just because they know that they will never advertise to a particular provider they never listed them in aspa um maybe now they will uh because they they think that this is uh but maybe it's a bene maybe it's a bad idea because it goes against the original purpose of sba"
  },
  {
    "startTime": "00:38:00",
    "text": "so that that's why i i think it should be explored uh another sp record type i suppose we agree this is not ready yet but it's an interesting path to take well it's the first time the brain is here so and if there are people interested in working on it that it will get better thank you hey one quick question patel again speaking as a working group member um you talk about the customer cone and the relationships there but isn't this problem wider than that can you elaborate meaning the attack can also happen from appearing yes as well yes yes yes uh so when you're talking when i said the algorithm works both on peering interfaces and customer interfaces when you're looking at a peer interface you're trying to discover their customer code got it but it's quite possible that aisp has not um turned on this and you're simply peering to that isp and you still have an attack or and this attack has can be generated from a service provider itself right right so this is not defending against incoming packets that coming from your service provider into your network this is filtering packets coming from your customers or or your peers i mean unless your service provider is giving you a full table in which case you can really treat them as um sorry ben madison again um i think the i think the failure mode that cares kind of hinting at is when you have a combination of a"
  },
  {
    "startTime": "00:40:01",
    "text": "regionalized peering and a partial transit service offered by that peer or or even to one of your own customers in a different region then that kind of breaks this kind of closure of um of of the the allowed to originate traffic relation and you end up not discovering potentially valid sources even under the expanded algorithm because those paths don't show up um i think that's but i don't think that's got anything to do with the rpki related stuff here i think that's a fundamental problem with the kind of algorithm assumptions there are two things there one is if those networks actually bother to create spa records they would list themselves as providers so it would discover it and also in a particular location where somebody is a customer you would likely receive bgp messages from them pointing to that interface i may need to think a little more about it but that's yeah so there's there's a gap there because of the fact that you don't have to talk about lateral peerings in aspa records that the paths don't show up in bgp because of policy the um adjacencies don't show up in the aspa because they're just pairings and as a result that gets left out of the cone i'm pretty sure that there is a i'm pretty sure that there is a gap there that we wouldn't catch um it only happens in the partial transit case but it's i think it would have we can look at it i think it's an interesting case that needs to be explored thank you thanks we have tom next oh okay you can hear me yes"
  },
  {
    "startTime": "00:42:02",
    "text": "all right thanks uh so to recap briefly on this um this document defines a new signed object called a trust anchor key object or attack object and the idea is that it's used to signal to relying parties that the ta key or the root ca certificate urls are going to change so the main goal here is simplifying key rollover if we wanted to roll a key today we would have to take our nutella file uh distribute it to all the different vendors wait for people to upgrade their clients if the if they're upgrading them the upgrade process would have to involve uh getting that new tail into place and so on so there are a few steps involved um and a few things that can go wrong uh or clients might have a custom da update process maybe operating system packages or something like that so whereas if we have a process like this then it's all in banned um and even the people who aren't updating the trust store automatically they still get a signal that there's a change and they need to do something so the more confidence we have in this process uh the easier it is to do key rollover and that helps with hsm vendor lock-in which is the main goal here not being stuck on one hsm indefinitely and the secondary goal is the ability to update um root ca certificate urls so that just gives us a bit more flexibility around deployment uh so this was last presented at itf 111 and one of the key feedback items was to look at other approaches to ta rollover and just see whether they might be relevant here so one document that came up out of that was 40 to 10 certificate management protocol which has uh as part of its ta transition process"
  },
  {
    "startTime": "00:44:01",
    "text": "uh this root ca key update section the idea there is that a ta operator when it's transitioning to a new trust anchor will sign the old one with the new one and the new one with the old one and then make those certificates available via a repository that clients can where clients can retrieve them so that if the client trusts the old trust anchor then they can validate stuff signed with the new trust anchor and vice versa but there are a couple of things here which are different from rpqr the first one is that ta distribution is out of band um clients might be using the old one or the new one whereas with scientel ta distribution is in band and the other thing is that with cmp or at least in that context clients might receive certificates from other sources whereas with rpco it's all in the repositories so it's not clear that this model is uh applicable in the uppercut space uh the next document is 8649 hash of root key certificate extension which was mentioned as part of the discussion after 111. the idea there is that you include the hash of the upcoming ta key in the ta certificate that you distribute so that when a client sees that new ta certificate it can it can compare the key with the hash and uh know that this is um the new certificate is using the expected key basically tim commented uh on this at the time on the list uh one issue is that rp's may not ignore that extension which could be a problem another one is that if the new ta certificate replaces the old one then there's no way to transition from previous tal data once uh the certificate has been replaced um which is not ideal uh it's good to be"
  },
  {
    "startTime": "00:46:00",
    "text": "able it's good to have that transition available another thing is that 86.9 involves a ta certificate issued ahead of time that is presumably stable whereas rpki because of the indirection of the tau supports arbitrary reassurance of that certificate so if a model like this were to be adopted there'd need to be additional guidance about what to do when the value changes and so on uh web pki um it doesn't appear that anybody does rollover here uh it looks like root ca operators just issue new root ca certificates and rely on cross certification so not dissimilar to the the cmp type approach and the last document found here was 5011 automated updates of dns sections so one of the key things out of from that document is this idea of an acceptance timer a client sets an acceptance timer when it sees a new key and it needs to continue seeing that new key for a period of time before it updates its trust store with that new key starts relying on that new key so that model has been adopted in the scientel document now um the acceptance time period is 30 days which is just arbitrary and the idea is that um that will help with some of the concerns that were raised around temporary key compromise um if as in if an attacker has access to the key even for a short period of time they can transition everybody to a new key and a new publication point are controlled exclusively by the attacker which is um yeah and then it's game over so this will help with that something else that came up with i7 was the use of the term revoked the attack object in i7 had this revoked flag and when that was set uh it was a signal"
  },
  {
    "startTime": "00:48:02",
    "text": "to the clients to move to the successor key the problem is that it's not really revoked in the sense that that term is used in um in other contexts uh because you still have to use that attack object to get to the new key so it's just not the right term to use so to address that the um the attack object no longer has a revoked flag just has a different model and uh there's some advice in the document for tas to reuse previous tier certificate urls for new keys once they've stopped maintaining the previous key the idea with that is that if an attacker gets access to a previous talent key and publication point a client that connects to that will see a different certificate with a different key and then all things being equal we'll think uh we'll go and get new tail data and everything will be fine it won't be possible for the attacker to exploit that client a couple of other changes the attack object now includes a reference to the predecessor key this is just a belt and suspenders type thing making sure that each each publication point is operating in its expect in an expected capacity um yep so that the success key actually knows that it's operating as a successor for a specific predecessor there's some discussion around the use of tac objects as a substitute for tau data and after the cut off there were some further updates around security and i suppose threat model type updates but they didn't make the deadline so they'll go into the next update uh another thing that came up the last time this was presented was looking at the currency of validators to see when it might be possible to rely on something like this in practice"
  },
  {
    "startTime": "00:50:01",
    "text": "so this is a graph of the validators that we see at openings rrdp service um but probably every rrdp server sees something similar each of the data points on the x-axis is from the last day of that month the validators we saw in that day uh on the y-axis we have the asn's and their sensor got from the ip address information just by looking at bgp uh so the clients are rather the relying parties that provide version information um those are route nader octo rpqi rpk approver the right validator and fort so each of those is taken into account for one of the time figures in the legend on the right uh then there's rpko client which doesn't provide version information unfortunately so it has a separate section and then there are other unknown clients uh the unknown clients actually exclude traffic that is coming from browsers so uh yeah it's software that appears to be connecting to the repo well is connecting to repositories for some reason but it's not immediately apparent what that reason is so looking at this pessimistically uh if all those rpq client instances are version 7.0 which was the first release of rpq client that had support for rdp then it could be as much as 50 of the validator population that's more than 12 months old uh more optimistically it's still 15 to 20 so it's a substantial number of validators that uh are fairly old and then there's the unknown uh clients to consider as well so that needs more looking into but in short uh if we want to rely on tack objects in practice then after the rp code rp code updates are done there needs to be a fairly concerted effort around getting people onto versions that support those objects"
  },
  {
    "startTime": "00:52:04",
    "text": "okay uh so next steps um obviously feedback would be good uh once that feedback is addressed we will move to updating the prototype code and then we'll go from there and that's it thanks do we have any questions for tom going once going twice okay thank you tom next up is i think tim or yes tim you still stand on the cross all right can everybody hear me yeah i hear myself so probably yes oh good lord there we go yeah well speaking oh that that helps okay um well let's move on to the next slide i'm going to tell a short story at least that's the idea um i wanted to talk about hosted rpi services delegated i wanted to talk about delegated cas and repositories and then zoom in on a particular aspect of it so jumping ahead of it i'll get there i want to look at how do we migrate from one repository to another actually um but let's start at the start so the different models that we see today the by far the most common model is what is here in the in the in the top"
  },
  {
    "startTime": "00:54:00",
    "text": "left which is where um you have a parent and a bunch of child cas in one system provided by a an rar and are usually publishing all their content in a repository that is operated by that same organization parent or organization now you can also run dedicated cas and then you have a choice of where you publish at least sometimes you do you can run your own repository for certain uh a number of arias provide a service where a member of said arya i can publish at them and we don't see this just yet but it has been mentioned a few times in the past the way the separation between the publication protocol and the delegation protocol if you will the provisioning protocol is organized allowed for essentially third-party content providers to also provide a service where people could publish so i remember people saying let's just publish at google amazon cloudflare i don't know whoever i'm not taking any sites but that was an idea behind it moving on next slide please yep so um using a provided repository well what we have found is that um in brazil in particular the rpki uptake is all done through well people running their own system because they don't have an option to use a hosted servers um so yeah maybe if they did they would use it on the other hand we see quite a lot of uptake more than i personally expected and i think that's mainly because if you don't have the concern about publishing hosting a repository yourself that takes away a lot of the pain of of"
  },
  {
    "startTime": "00:56:00",
    "text": "doing this and especially it takes away a lot of the pain of you know providing accurate good uptime for https but more importantly maybe even for rsync because organizations are not always ready to do that kind of thing whereas if you just run an aca on your side and it goes down for whatever reason if you bring it off in time for it to republish before manifest and serials would expire then it is a lot more forgiving than operating something 24 7. so i think this is really an enabler for people to do this kind of thing okay now the question i wanted to get to is suppose i'm doing this and suppose i set up my own repository for example but now i want to move on to another repository how do i go about that is that even possible and we have actually implemented something i think it needs improvement and it's based on key roles an existing standard that we do have so next slide i'll try to briefly discuss how key roles work maybe it go how are we for time should i be quick or well there's a lot of arrows here and most of the arrows are actually missing but it's to give an idea right before any any key role the situation is you get a certificate from your parent you publish a manifest and a serial and then you have a bunch of objects rawas etc and you may publish ca certificates um for grandchildren um they're all in in in one repository right so then the next phase is that you would next slide please you generate a new key pair you ask for a new certificate and you publish a manifest and a serial but nothing else all the actual content is still under your old key and then you enter a 24 7 24 hour staging period"
  },
  {
    "startTime": "00:58:01",
    "text": "next slide then the time comes to activate your new key and what happens then uh in the current uh key role algorithm process or thing is that you republish everything under your new key so the roas i mean and delegated uh certificates for for grandchildren etc um and you remove them from the list of objects that you publish under your old key so there you just have a manifesto crl left over you can publish this in one go using a multi-element publication query and this means that the relying parties will also see it as one delta so they will see this as an atomic operation more or less well more this is true for rdp if you set up rsync with the right incantations this is also true for rsync um yeah and then finally you will remove the old key so you will ask the parent to revoke the certificate for your old key and you can get rid of the manifest in the serial and this can be done immediately after the previous one so it may not even be visible as two separate steps to everybody looking now if we apply this to migrating repositories what we've done perhaps naively is just you know let's just use a new repository for the new key and we follow the steps as though we're a normal key role so we have a yeah here in the off green color is the new repository a new key we created new certificate we got we published a manifesto we don't always wait 24 hours to be honest but we could the next step is undertalk manually and that's just like any other system um"
  },
  {
    "startTime": "01:00:00",
    "text": "you know we activate a new key meaning that we publish everything in the new location and and this i think we should change we also immediately remove everything from the old key because that's how the normal key role works except in this case you wouldn't see this as one atomic update because fetches are happening in two different places so you might see no objects you might see both objects at the same time yeah or you might just see this which would be the happy case of course um so a question is and i think maybe asking the question is ask answering it is that you probably want to keep objects in both locations for some time before you know you you remove them in the old location um next please then yeah the final step is easy once that's all done you remove the alter you ask for a revocation of your certificate and you remove the the objects associated with it so that part should be relatively easy um one thing to realize as well um i don't think any of the relying party tools uh treat aia other than informational so they the back pointers let's say the authority information access pointers uris that are included in objects they point back to where a certificate is published now if that changes here then that might flag some things um or maybe we need to look at the provisioning protocol 649.6492 that essentially has text okay when a child asks a parent like what are my resource entitlements essentially they get a response of these are your entire resources this validity time that may be applicable and by the way this is the uri where your object might be published now if that changes maybe that should also trigger that cas republish things the downside of that is that you would see quite a lot of objects we published at"
  },
  {
    "startTime": "01:02:00",
    "text": "that time but i'm not sure that that is actually a problem last night i guess yeah so the question would be really like okay i think we do need to support this kind of thing um i see at least one way that we could improve it but the more important question that i would have to the group is do you agree that we should perhaps document this and try to standardize how one would go about this this process i'm done yes please questions from anybody else sorry this is chris morrow as a regular person yes please standardize release document very well the migration process all right and uh i'll send one to the list soonish next up alexander i think might be here no no he's not here okay well hold on i think i don't know do you want to do that okay i'll try to share my screen or i'll try to use my select um so i'm gonna briefly check it okay yeah uh i will try to share my printing because it's a outdated version of my slides it's my fault uh so if you can give me a chance to share"
  },
  {
    "startTime": "01:04:00",
    "text": "my screen yes [Music] seems to be taking its sweet time there like that do you want to there you go okay i hope you can see it so uh once again hello everyone uh my name is alexander asimov and today i'm going to present a long awaited update on asp documents let's start with the profile document so in the version 7 and before the spa object was defined as customer is with its provider list it was expected that nasp will create two objects one for ipv4 oxy and another one for ipv6 for this in the latest version [Music] in the latest version this scheme was updated now asp object carries not a list of providers but a list of lists where each item contains a list of providers and may contain address family as far as i understand the idea was to give a way to create a single object for both ipv6 and ipv4 policies"
  },
  {
    "startTime": "01:06:00",
    "text": "it's true that a significant number of networks uh have the same set of providers in ipv4 and ipv6 and the process seems to be converging towards this state still there are networks who have different policies in different address families so it presumes the opportunity to have different policies in ipv4 and ipv6 while given a way to put it all in one asp object uh in my view the same goal may have been achieved with less changes just making the address family option still even these lightweight change is getting into the same trap it's the the level of router we will have different representation of ace pay record for different address families i can expect the click support to navigate separately through the set of ipv4 and ipv6 records it's a the level of rtr protocol will also have different asp pdu for ipv4 and ipv6 for me the last sp object update may break expected behavior by changing the object format during the processing and may also complicate debugging uh i checked the thread uh this morning and uh if i'm not mistaken we have a perfect split through although the new format have intercooked testing on its side i have a nasty feeling that profile"
  },
  {
    "startTime": "01:08:00",
    "text": "document and the new rtr document are running in different directions and running surprisingly fast and if we don't converge uh the community will know who to play so let's try to discuss this topic before jumping through the second part how are we going to address this format change with uh 8210 these in the rfc editor queen please advice maybe ben won't go to the mic then madison work online um look i think i've probably said better i think i probably said everything that i need to on the mailing list already but to reiterate i think it's less work for the rp if there are fewer objects i think that the overwhelmingly more common case is that networks have the same or very close to the same set of transits in both address families um and i think that for i think one of the things that i tried to point out in a recent email is that i think that for an operator that's used to a user interface where it presents this kind of a mental model where the base assumption is that both the dress family is the same topologically um the day that that operator then needs to go and read one of these objects to see what's actually being transmitted on the wire i think it's much less surprising if it doesn't diverge too much from that mental model um i think the other thing to point out is that there's quite a lot that you know"
  },
  {
    "startTime": "01:10:00",
    "text": "there's been a lot of running a lot of progress in in new implementations over the last few weeks and all of those are based on the version 8 profile um i think rolling that back is quite a lot of work for you know a fair number of people um so i'm quite strongly in favor of the version age range uh i have a question for you what are you suggesting to do with rtr specification that is fortunately or unfortunately in the rfc quick that at the moment aspires understand uh was cooked for the previous version of the object we can make it to work with the new one but it will be may have a surprising specifics so i i don't think that there's i mean certainly the two for the formats in the rtr protocol and the the asn.1 do diverge um i don't feel like that's a huge problem um i think that the the i think that the the the overwhelming um the overwhelming consideration for the rtr protocol is to make things as convenient as possible for routers to use it in policy decisions and i think the existing format is you know mostly fairly well suited to that whether we do this translation when it arrives at the router or when it's being processed by the rp i don't feel like that's it's it it's not a non-issue but i think that the the profile change can co-exist with the existing um rtr spec pretty comfortably but don't you think that i'm sorry i'm keeping you at the mic but you are the person to blame for this quick progress no kidding"
  },
  {
    "startTime": "01:12:01",
    "text": "my question is that okay we can split to we can say different policies at the level of uh rpk cash i think that's what you are suggesting yeah that is what i'm suggesting but don't you think that the debugging can become really complicated when you will be using three at your router asking for please show them give me the information about what base days are for selected address family and after that you will you may have problems by building corresponding asp objects in the distributed of the other database yeah so i think you're right um but i think it is i think it's inevitable that there's a the the the kind of structure to use in kind of provisioning tools and config management tools is inevitably different from what is convenient for the router to store in its internal data structures and so that translation has to happen somewhere and my feeling is that probably the best place for that translation to happen is the rp because computer's cheap there and my feeling is it's the least surprising place for it to occur because you're going from one protocol to another and you're expecting there to be aggregation and things like taking unions and potentially multiple objects and you're expecting there to be structural process going on there anyway did that make sense [Music] i still have a feeling because i'm at the same time while you are i'll keep keeping you at the mic i'm reading the chat uh and i still have a feeling that it's a group"
  },
  {
    "startTime": "01:14:01",
    "text": "of side errors we're not converging because i see that randy bush is still opposing the exchange and he's authoring the rtr i don't know how to make everybody happy about asp object style maybe we should ask the chairs to be more involved in the process i think i should sit down and give other people a chance to offer their opinions then thank you man um this is the concrete ncc uh we have an internal implement volume and height is okay now yeah okay this cockpit ncc we have an internal implementation of the version 8 profile and i kind of elaborate here on why i prefer the version 7 profile in hindsight um in the end the user interface that people will present may or may not align very closely to the objects that people create um it's all about making sure that the right objects are created and that there's no confusion in these objects themselves and what we realized after implementing this what was that we could create a lot of edge cases in the content of the version 8 profile where the content semantically overlaps and you need to take a union there within the object and uh covering this with a proper uh a proper set of test objects was just very hard or test cases and that's the main reason i prefer the the version seven object even though i really like the idea of having a single signed object per as"
  },
  {
    "startTime": "01:16:01",
    "text": "but i'm just afraid that covering all these cases where v4 v6 overlap or not uh could lead to yeah interesting edge cases and implementations okay i'll take it as a point for the whole version of a speed or sbo would you go folk [Music] i failed to really track the mailing list for a couple of months and i'm sorry i did not raise my voice earlier but look at the proposed change of the profile uh well okay uh a very superficial look at it says o7 looks much less complex than o8 straight forward and i guess some of the complications that tees was mentioning is related to the more complex data structure on the other hand my understanding is that actually a more complex data structure is not used in any significant way to express more functionality kind of yes there is yeah there is a possibility to add a third and the fourth afi and there is the complexity that tees was pointing to and well okay kind of i uh just from that"
  },
  {
    "startTime": "01:18:00",
    "text": "i'm quite certainly uh not happy about moving to a uh 08 [Music] in my first understanding i was expecting that the work on 8210 bis would not have to be redone to fit the which is adding more complexity to the whole system because if you have different presentation of the in data structures for essentially the same content uh kind of that means there are there is translation there is there is there there are translations necessary and that's more complexity in the damned system than if you just can't straight copy um [Music] i'm for for the question whether we are actually delaying uh creation of the operational system um uh i would think well okay if uh uh a rerun of 8210 bis is required i would strictly oppose the idea moving on on eight on o8 um [Music] i'm very unhappy about added complexity i don't see real good reason for it and for those who are saying well okay lower complexity for the user interface which actually is another presentation"
  },
  {
    "startTime": "01:20:02",
    "text": "of the same of the same content well okay uh ten years back we would have had a situation where the afis usually would not align very well and if they align at this point in time this does not mean that it's going to stay this way so kind of the argument well okay we are making people happier uh for for just this time this is just a temporary argument and i don't really buy it but kind of my highest concern is that i do not want to see development of the operational system being delayed but complexity is is going to have cost and should be avoided and not unnecessarily introduced randy bush first [Music] [Music] your audio is randy you have a really bad voice synthesizer do you want to try one more time"
  },
  {
    "startTime": "01:22:04",
    "text": "okay sorry randy okay you pop back into the mic queue when you're ready uh but i think tim and then warren and ben yeah so uh hi yeah tim gonzalez um i guess i'm partly to blame for all of this because i think this discussion started last year sometime november december um and um i think it started with a desire to use a blast space even and this afi limit actually came to be as a an additional thought in the process so the first proposal that i did then was that we would have a single as object with two distinct lists for each address family then the address family limit was introduced as a way to compress that even further and then the idea came to be that this might actually reflect better what people want to do all in all this can express exactly the same kind of data as you know you can express now with the with this o7 protocol so in that sense it is really a matter of preference and i think it's something that we can you know keep on discussing until you know well how did they say the cows come home and i want a second what um rudiger said i would really hate for that discussion to delay deployment and experience with uh with aspa so if it comes to that then i am quite willing to change my implementation to follow whatever profile is in the end acceptable so that's what i wanted to have said to comment on the data format versus"
  },
  {
    "startTime": "01:24:01",
    "text": "8210 this i think there's prior art there i mean if you look at roas you can have multiple prefixes in a single robot object you don't get this structure in your router you actually have to validate multiple robo objects and make a union of everything and then and then that is what gets sent to the router and similarly whatever the profile is this translation can happen at different levels it can happen in the rp as it is currently done for rawas already and yeah it can also happen in the ui where obviously i can present users with an interface that allows them to provide a common list and then my software can work out how to make two distinct lists of that it's trivial for me as well to do that so yeah i don't know if any of this is bringing it closer to a solution but i guess my main message is that you know i just want this to work try for randy one two three one two three any better good better oh is that a bugger feature first of all what's presented to the user and the gui is arbitrary in either of these schemes you can present separate or joined in the gui makes no difference what is on the wire is almost never seen by the operator a few x 509 geeks actually look to the garbage on the wire when i want to see what's being published i don't look in the repository i don't look on the wire i look in my router because that's where"
  },
  {
    "startTime": "01:26:00",
    "text": "the rubber meets the road okay what is in the router is going to separate v4 and v6 because that's what happens in routers thank you steve dearing and bob hinden the 8210 bis change while technically procedurally possible is not what we want to do because we want to keep the burden of any hacks north of the router the whole purpose of 8210 is to get the load minimize the load on the router and the router chooses choose up six and four separately lastly like it or not v4 and v6 topologies are not congruent we wish they were we've wished they were for 20 years they're not yet this is especially seen in asia but it occurs here today all this air any operator on me in this meeting who actually uses multi-protocol bgp so that they have v4 and v6 in a single configuration configured session with their peer or is it like all the rest of us that we have separate sessions for v4 and v6 it's not pretty it's just reality thank you thank you and warren kumari relaying a um comment from rob stillmore yeah i can't read and look at the same time so i'm extreme oh thank you ridika i'm extremely uncomfortable with requiring"
  },
  {
    "startTime": "01:28:00",
    "text": "transit on different atheists to be in the same path when he damn well know that sometimes they are not maybe i misunderstood the question chaor is looking confused i can read that again i am extremely uncomfortable with requiring transit on different afi's to be on the same path when we damn well know that sometimes they are not maybe i misunderstood the question i'm not getting it if somebody can help me please go ahead it sounds like a lot of this conversation should be on the mailing list probably it would be easier to reason about it that way also just remember please of course code of contact on the mailing list applies uh ben just the last thing i mean mostly in response to what rudiger was saying is better yeah um i don't think that we're arguing here about more or less less complexity in the system as a whole i think we're mostly talking about where that complexity should be that should be dealt with um [Music] i don't think that any of this need to be a showstopper and my priority in all of this really is to try and get some running code out the door and into production sooner rather than later because i think this is this is a technology that we have wanted for a long time and you know we we should prioritize trying to get it deployable as soon as possible rather than arguing about the details okay there's still three people in alexander go ah there are uh three people in the uh in the queen okay okay um"
  },
  {
    "startTime": "01:30:01",
    "text": "there's two warren needs to take his hand down if he's not going to say something i see i say sorry same for tim nope okay tim's all done good it's all in you alex go so um i'm not uh so the person to announce that we have reached consensus here uh hopefully i believe that getting back in the form of zero seven can simplify the process if i want to move faster maybe it's the best way especially taking into account the comments of one of the implementers who was saying that there are a huge amount of tests and the format is getting complicated uh i will i'm not one of the implementers so i can't comment further but my personal view is that uh zero seven is simple and so it's uh can fly faster than zero eight okay uh but nevertheless let's keep this discussion in demand at least i will try to summarize it uh after the meeting ends and as i have only 15 minutes left let's move forward to the second document and it's about verification and it's also suffered a lot of changes the more changes were devoted to generalization upstream and downstream procedures for this purpose we will define new index types first one is invalid index which is a minimal index for which is with index i have no not authorized next is to be its provider we also defined a reversed embedded index that is calculated the same way but for the"
  },
  {
    "startTime": "01:32:00",
    "text": "reverse highest part let's see how these indexes help to detect problems for prefixes that are received by ear provider route server route server client the rule is very simple the inverted index defines the length of the first upstream segment and in the case of a correct path it should be equal to the length of this path from this we can get a simple rule if invalid index is less the length of ice path it's roughly ah it's important to know that uh leak detection at ice by route server and transfer a third client is not a special case anymore in the profile document document we added that if a route server is not transparent it must be added in the list of providers with this all parties at the eyes are entitled to use upstream verification procedure that we discussed just above now detection of problem flips that are coming from private the correct downstream path may contain the upstream segment and downstream segment the invalid and reverse invalid indexes define their length respectively so to detect roughly we need to check that the zoom of invalid indexes is less than highest path length for me it looks fairly simple now let's discuss the unknowns in the previous versions of the document the unknown path was defined as the path but that has a common system"
  },
  {
    "startTime": "01:34:01",
    "text": "that don't have psp record with comments from sri ram and yara the definition was transformed to the next one the unknown path is the path they may have been leaked and it proved uh also that the detection of unknown paths is in this definition is very similar to the detection of fractix we again define two indexes the unknown index uh defines the first is in the upstream segment that doesn't have asp record so it's not inverted it doesn't just doesn't exist we also define a reverse unknown index for the reverse i spot so the path may be leaked if there is enough space for leak to happen in case of routes received by providers peers or at eyes it means that unknown index should be less than highest path length and a very similar family equation we get for routes received from providers if the sum of unknown indexes is less than ice path length we can't guarantee that the prefix was not leaked now the other rules for provided piers and types if the invalid index is less than i spot length it's a arrival if the unknown index is less uh than its path length it's unknown otherwise it's weather for the downstream uh it's a nearly this the same thing so uh if the zoo of invalid indexes is less than ice pathways it's invaded if uh the zoom of unknown indices is less of the ice pattern it's unknown otherwise"
  },
  {
    "startTime": "01:36:02",
    "text": "what surprises so there is an ongoing discussion how we should protest assets it seems that we are converging code on marking routes with i sets as invalid to make it considered consistent this should be applied without reference where i set is placed in the beginning or in the middle of the path the implementation of pfsp logic uh with corresponding unit test the current implementation you can find at the github so what i need i need input on these questions i need volunteers that want to read the document i will need volunteers who want to code the a spay logic to check the specification the mic is yours [Music] chris i go can folks hear me this is sriram i can hear you okay thank you for the nice presentation alexander uh can you please go to slide 9. [Music] right so just want to make a comment uh that uh when you have a provider that has no providers like the tier one uh we have said in the draft that they will register an as0 aspa that is fine i think we have also said that nix nix or the route server as will also register uh nas 0 aspa that's also fine"
  },
  {
    "startTime": "01:38:00",
    "text": "you didn't mention it here so i thought it's worthwhile mentioning those but one more a little bit more tricky thing uh that not not in the draft yet but i think we perhaps should discuss that between you and me at least and include that in the draft and that is about uh if you have a a transit provider who happens to be present at a rs at the rs as a client so it's a tier one a transit provider and happens to be present as a uh at an rs as an rs client uh in that case uh they should register uh naspa with the rsas as a provider just like any other rs client i think that's something that we still need to include in the draft and wondering if you agree with that i wonder why we should emphasize this together because the government just says if you are connected to non-transparent transparent art you should include each other sister number in your set of commanders that's all it doesn't matter if you are t1 okay good yeah so so tier one should not be misled uh into thinking that they just need to register in a zero spa and they are done uh they should be sensitive that if they are present at a route server as a client they should definitely include the route server as uh uh in the aspa uh if you think the draft is clear about that it's okay if not we can talk about it and perhaps put in a word to be sure that people understand that uh if you find that somebody is missing this document please"
  },
  {
    "startTime": "01:40:00",
    "text": "send it back you know i'm trying to do my best to carefully read all the comments and push that into the document uh sure i'll help you uh and just a little mention of the uh the use of the upstream uh for the route server client uh i think we uh i have some examples where that doesn't seem to work correctly but again that is too complex to discuss here uh we will discuss it between yourself and myself and then we can take it to the mailing list if needed i just want to mention that thank you okay if it's too complex maybe you should oh i'm sorry randy bush i j marcus if it's too complex maybe you should take that as a warning um exchange points that put their ass in the path are against spec do not make complexity in 93 other places because of it it's not worth it it's uninteresting and it just creates more complexity reduce complexity please reduce complexity ready thank you very much for the comment i'm doing my best so about the non-transparent access as far as i remember there are two phrases devoted to them and unfortunately i need to stress that they are in the wild the last time i checked i've seen one items that my network is using that was present in the park and i was also surprised to see that hong kong internet exchange is still there i was not looking for more examples"
  },
  {
    "startTime": "01:42:04",
    "text": "yeah madison work online um so it's it's it's certainly true that a transit-free network at a um at a non-transparent ix root server would need to include that root server as one of its providers but that is such a vanishingly unlikely scenario to actually come up in the real world i really wouldn't call it out explicitly in ways prose on it i think that in the interests of simplicity what the document should do is emphasize the fact that a non-transparent internet exchange root server is just a transit provider it just happens to forward on mac addresses and not ip headers but in every in any way that this document cares about it is just a transit provider and should be treated indistinguishably from that case i think for a um uh if we are digging into the details there is only one thing that is different you are speaking about uh rough server behavior because if it is uh transparent as the specifications suggest uh we are not checking that the neighbor at all system is equal to the last account system of the path this is the only exception in its proxy in all our other cases it's just the same so i think we are on the same page here uh this is sriram again um i just want to make a quick comment that it actually"
  },
  {
    "startTime": "01:44:00",
    "text": "turns out that the non-transparent ix is less complex than the uh transparent ix but but both of them can be uh taken care of in the draft without too much complexity the only other comment is that it appears to me that if if as even if the route server is transparent uh it may be worthwhile to register in aspa uh by the client including the even the even the transparent ix in the aspa uh that's something again that we need to think about that carefully and uh there is there is i think utility for that uh according to the goal that we're trying to solve the rapid detection there is no need the origin uh not to register transparent eyes go ahead it's not harmful to register them but still i don't think that the specifications should make such suggestions yeah uh yeah again like it in my mind it helps to it helps to have the uh algorithm at the route server client uh unified hey sriram yeah hey we're gonna get kicked out of the room here so i think all this has to end up on the list so my suggestion is that you start writing to the list right now sure yeah thank you and sasha thank you very much for the presentation you probably also have some stuff to write to the list yeah see everybody in the next place we're going to be which i think is london unless we need to have an intermediate interim meeting to talk about stuff and if we do you should definitely put that"
  },
  {
    "startTime": "01:46:01",
    "text": "on the list thank you thank you you"
  }
]
