[
  {
    "startTime": "00:00:19",
    "text": "Rich should be joining hello um gu the slide should be preloaded yeah yeah I think you can just go ahead and cool I I can wait to do that if you want to folks give it a few minutes and get folks running in my co-chair co chair is here hey"
  },
  {
    "startTime": "00:05:04",
    "text": "Tommy what's sort what sort of issues are you hearing from folks oh Jon was saying get trouble because he's in now so it's good okay great I'm in I think I can hear you all too cool cool welcome J hello folks wonderful to see you all always love to see you same same same I don't come see you Richard but it's wonderful to hear you oh thought I was transmitting video yeah I see Richard J we don't see you actually oh that's hilarious uh this is a multi-party problem isn't it I'm getting a publishing error occurred unable to reach me and I think we just lost audio from you J yeah"
  },
  {
    "startTime": "00:06:00",
    "text": "you could you try with a different browser Maybe okay um should we give J maybe just one minute to reconnect and then start that sounds good to that works for me yeah for looks like we have JN on back hopefully he can hear us you want to kick us off yeah sounds good um I think it'll mostly be Tommy talking but um yeah hey folks so welcome to the interum uh the agenda for today"
  },
  {
    "startTime": "00:08:02",
    "text": "is to discuss chunk omtp um Tommy will be presenting and then the idea is to I guess just see if there's interest um to potentially adopt down the line and then we can um see whether we want to hold a session at the next ITF and take it from there um because last time we had some hesitation about whether this is the right work to pick up so just wanted to make sure that folks were on board with it and yeah um try you want to present sure we'll do okay let me asking to share slides all right there we go okay hello everyone um so yeah previously we discussed this a bit back in uh San Francisco that was before there was actually a draft on this and we're talking about streaming things so uh now there is a draft that's been out for a little while and this is um something that Martin Thompson and myself have co-authored and it's adding a chunked support to OHP all right um so I think the main larger question here for the group is you know are we done with OHP um or you know is there a bit of uh last work we should do before we close the working group and I think the chunk aspect is the main question here um and so you overall like we published RFC 9458 for the main oblivious HP which is fantastic so good job everyone uh"
  },
  {
    "startTime": "00:10:02",
    "text": "authors and working group for getting that reviewed and out the door um but there's like when you're looking at the functionality there there is one Gap that kind of stands out um looking at what is supported at the different layers that have been built for or that are used by optp so normally uh ogtp is using just complete messages so it requires a complete request uh to be read in one go and complete response to be read in one go um and that lines up very well with the uh the surface that binary HTTP exposes for known length messages and of course hpk under the covers can support one shot uh full messages so that works fine but you know hpk does support having multiple messages within a single context and part of the design of binary HTTP was to have the ability to have an indeterminate message and while binary HTP can be used for other um other applications other than OHP I'm not aware of it really being used for any others yet um the indeterminate mode was meant to handle uh cases where you have multiple pieces of a message to be able to handle things like um HTTP uh 100 responses um but with the current definition of omtp without the ability to send in multiple chunks those are impossible to use and so we have these modes of binary HTP uh that are built and around but we can't meaningfully exercise and they're functionalities of just a normal HTTP request and response that we cannot do uh so the thing that would fill in this Gap if we want to do it is chunked OHP and so very simp simply this allows"
  },
  {
    "startTime": "00:12:00",
    "text": "encrypting and decrypting the request and response messages in separate chunks as opposed to requiring that they are encrypt and decrypted all as one so this allows you to use um in determinate mode it requires that you use it uh it takes advantage of the support for multiple messages at the hpk layer but I do want to emphasize like this is still a single HTP request and response transaction uh it's not really changing the content of that exchange it's just uh allowing it to be uh handled in discrete chunks so it's not all in one go all right so I think the main question here is the use cases um like overall I you know totally believe that many many of the OHP use cases uh of things like DNS queries or just doing a simple metrics upload do not need this uh what they have is great and so the chunks uh is for a more limited set of use cases I think it's very very appropriate that it's something that we left until now but I think there are some legitimate use cases here um so let's just walk through those all right um so first uh you can have cases of like database queries uh database lookups that are fetching things that are potentially very large responses but the client doesn't know when it's doing this query um these are cases where uh these responses may be processed incrementally and they don't want to be done all in one go and it may take a long time for that lookup to happen and there's useful work that can be done on the client side to process the data that's looking up that's a very very simple case um and it's also uh pretty similar to the case of generative content uh generative"
  },
  {
    "startTime": "00:14:01",
    "text": "content is uh a hot topic for a lot of people and use cases these days uh with all your chat GPT and other friends going around so when we have content that's being generated by servers on the fly in response to your request um depending on you know the speed of the models or whatever generating that content sometimes they can be quite slow and will be generating bits of those content uh o over time and if anyone's use something like chat jpt Etc um you will see that sometimes it will uh build out responses over time and complete it and so there's meaningful progress that can be displayed or given to a user um sometimes that can be because it's a large response but could just be because it's very slow to generate and so there's useful work the client could do to process or display that response um another case I wanted to bring up which we don't actually bring up in the draft right now um but I think is worth going into uh for some security considerations um or performance considerations is uh being able to allow very large uploads from a client to a server in a request um so as it stands today OHP doesn't have any max size of what's in Flight which is actually quite different from a lot of other layers that you would have so you very reasonably a server that is just using omtp today for DNS or some simple metrics uploads could Implement a relatively low maximum and just uh refuse to process any large requests um however if a OBP server does want to be able to receive some large upload of a file Etc from a client you"
  },
  {
    "startTime": "00:16:03",
    "text": "could be you know many gigabytes or something then uh it it's kind of its only option is to buffer up anything that a client is sending to it um some arbitrarily long message and not even be able to decrypt it not know that this is going to be a valid message at all or that based on the content of the message you know let's say this is a post like it's actually going to a a path or location that the server is going to allow this client to upload to um so it's it' be pretty easy for clients to just send garbage long messages uh either because they don't uh they aren't correctly encrypted or that the content is just uh you know garbage in that regard and so a server that wants to be able to receive large uploads potentially is taking on a lot of risk here um and so so we can imagine that with chunking the server could enforce that it has a uh a particular maximum size of that initial chunk you can read out the headers of where this is going to go it can prove that the client is correctly encrypting the message and then um only keep processing the chunks after that otherwise it could just close the stream uh to the client and send a rejection um and then the last case uh which uh Martin brought up and which is good is being able to support um informational responses in HTTP so HTP uh does have a feature of like a 100 continue where the client can say I expect to get a continue response from the server before I will move on and send the rest of my request uh this is not something that would today be possible with OHP unless you're able to do chunk junking I will"
  },
  {
    "startTime": "00:18:00",
    "text": "point out that this use case uh is a little bit different from the others because it does involve a back and forth um that's inherently part of that exchange the other ones are still just a request in One Direction and a response in the other direction that are not related it's not a bidirectional streaming case this one you do you would have an initial request uh a response that could be 100 continue and then hello I have balloons apparently uh followed by the rest of the uh requests actually coming up um and so we'll go into the details but that does change some of the assumptions you would have about uh privacy of server observability of latency to the client all right so those are the use cases um and I want to take a quick uh I guess maybe before we move on are there any clarifying questions on those use cases just like how they would be used or why they would have an advantage from using chunking Okie doie move on um so I was going to take a quick sidebar uh related to chunk OHP but you know more broadly um you know when is OHP applicable versus doing some other form of proxying um and this is something that was discussed both in San Francisco as well as uh there was a email exchange around on on the list around uh OHP and like could you use it for something like websockets and you know I think in general there seems to be a need to clarify you know when do you use omtp or something else which is like a much more fully capable endtoend TLS stream that's being proxied um so I mean overall I think"
  },
  {
    "startTime": "00:20:02",
    "text": "that you know using chunked OHP uh should only be considered for cases that are already uh using OHP or good candidates for it but need uh this optimization either to uh be able to handle slow data downloads or to prevent clients from uploading too much like it's it's a way to fix some of these cases um it definitely should not be used to replace cases that would be better served by like a mask proxy or any other case where you're proxying an N NLS stream um and I I think there are pros and cons for functionality for privacy for security um that apply in general to omtp as compared to something like a mask proxy or a forward connect proxy uh it separate totally separately from whether or not you use chunks so the chunking aspect of is really just a extra modulation on top of already using OHP uh so this is not fully comprehensive but trying to capture some of the things that have been discussed on list or previously or going have we've gone into in the document um when we compare OHP to like a proxy an ntls stream or mask um one clear difference is that for omtp you do need a cooperating or modified server uh that is explicitly an ogv Gateway whereas um you know mass proxies or just a connect proxy can talk to any arbitrary server so that's just like a basic functionality difference um OHP also is dealing with single HTTP transactions single request response pairs um which is great when you want decoupling at that level uh but doing a proxy TLS stream is much much better if you need to do web browsing where you want multiple requests um that"
  },
  {
    "startTime": "00:22:01",
    "text": "are part of the same session that you do want to be able to correlate um between the server and the client um and then so this next row like going into um essentially more of like the scalability and performance of a proxy omtp uh since it's working at the request level allows connection reuse both between the client and the relays and the relays on the gateways um such that all request from all clients are just streaming over uh either one connection or a connection pool between the relay and the Gateway and they're all mixed together and so it's very efficient and scalable in that regard um anytime we're doing uh like a connect proxy we are having to worry about IP and port allocations for uh streams that are being forwarded to the next top and so it has very different requirements on the proxy infrastructure and then going into some of the Privacy aspects um OHP gets you uh per request response pair decoupling uh without having to add any extra latency um proing a TLS stream uh you know again makes a lot of sense for talking to unmodified servers and cases where you allow multiple HP transactions on that same session but if you wanted to do something you like um doing DNS obliviously uh doing a full anti NLS handshake uh to then uh just send a single request and response that are very small doesn't make a lot of sense another uh privacy benefit that we get from omtp is that on any given request response pair uh the Gateway so the server is unable to measure latency from the client right so it gets a request in it hasn't it doesn't necessarily know when that request was sent uh modul what the"
  },
  {
    "startTime": "00:24:02",
    "text": "client adds as a date header if it's using that feature or not and um then it just sends the response so it isn't able to use this measurement of latency to try to triangulate a user or infer location Etc uh note that if we do have a client that wants to use the 100 continue case with chunks that does change things um but that is something that a client would explicitly be asking to do and so like we have to discuss is that a useful addition to here um whereas in the case of having an anend proxy TLS connection you obviously have a lot of back and forth data um and uh so that does allow servers to measure uh latency there but then there are definitely downsides to some of the security aspects of omdp uh it doesn't inherently have replay protection um and until until the ogtp public keys are rotated you don't have PFS whereas of course TLS does have that inherently so it does make it um certain types of traffic are not things that you would want to use with OHP and that's already the case so you know don't use OHP when you need back and forth streaming of data uh don't use it for cases where your startup latency of the handshake is going to get drowned out because you have a longer live session with many different things going on don't do it if uh the replaying of the request is going to be a dangerous thing to do but for cases where you really care about decoupling individual HTP transactions or cases where you don't want um the servers to be able to measure the latency to the client and cases where you have you know many uh requests from potentially many clients all being jumbled together in one proxy uh so and you want to be able to scale up that proxy easily uh those are pretty good cases for HTP and that's what we've seen so far drive um the adoption"
  },
  {
    "startTime": "00:26:05",
    "text": "there all right um so then jumping into the actual technical bits of omtp I'll just Breeze the chunk omtp I'll just Breeze through these quickly um this is mainly what's in the document so you can just have a reference here really just takes the normal omtp format uh the same request header um and the body except it just adds a varant length before each section of the body um and then you end with a final chunk um and so this does have properties of preserving uh The Ordering of chunks and the number of chunks that we have so we know we still got the full um full message in each Direction and he would uh use some New Media types again breezing through this uh if you look at the hpk requests uh we would use a slightly different uh string uh for the info to denote that this is not just a request it's a chunk request um and and then uh on encoding we add the varant um and we' have instead of an empty aad Mark uh that the final chunk has a different aad so we know that it is indeed the final chunk um on the responses uh pretty similar uh it doesn't need to maintain a counter um this is something you get for free as part of hpk uh but we just essentially need to copy that technique for doing multiple chunks on the response other than that it's quite simple uh so in discussion of this I think there are a lot of interesting questions that still would need to be discussed um things that have been brought up such as having negotiation of are you know do you need to do chunking or not um overall uh negotiation of things for HTP is a"
  },
  {
    "startTime": "00:28:01",
    "text": "bit vague because by default a lot of HTP use cases just kind of a prior know they have this configuration between the client and the server and then they use it um but if we need more Dynamic ways to know that we have to use chunking or not um that would be something that need to be discussed um and you know I think particularly for this case of uh being able to measure the time and trying to enforce that people aren't using this for just you know arbitrary streaming of data uh we could discuss having you know prohibitions on sending further request chunks uh based on what was received in response chunk U but fundamentally I think most of the use cases that would benefit from chunking would uh benefit regardless of uh being able to have the ordering between them and that's all the content I add here um so I think the question is you know do we think this is a useful Gap to fill uh do we want to talk about adopting this or or not so that's good to discussion thanks Tomy I took a a brief scan of our Charter and it seems like I don't think stands out to me as outline this work um but yeah folks have thoughts uh on adoption and interest please go ahead now go ahead Watson uh Watson lad eami I think we should adopt I think this proposal makes a lot of sense to sort of add in the capability which we already have in htdp and I don't think it overlaps too much with mask right"
  },
  {
    "startTime": "00:30:01",
    "text": "e yeah can't say I really agree with that um and the f look the fundamental design principle of HP is to adopt an inferior security posture and um uh an inferior flexibility of TLS at but in order in order to permit um High better performance on single shot applications like like like um like like DNS right and um and and the um and the vment the majority of majority of the propos the use cases you propose are ones which really are not that kind of single shot thing and really not have tight latency requirements but in fact you're like oh no we're trickling out like these like these like tokens from the from the server over like seconds at a time um and so like having having to suck up like a half half a round trip in order to do like the TLs handshake is not a meaningful problem in that case and so like I just like what like I think when you cut out all the use cases that actually do not have this performance property and not in fact benefit from this configuration property you're left a very little um and so um you know uh so I guess like I think when you look back at that I I don't think very very persuasive like thing to do am of complexity and you're trying to rebuild basically a streaming protocol on top of HTTP um in kind of like a goofy way I think I said this basically the same it said the same thing in San Francisco but now that in use cases um in detail I think that's even more true um so like I find like the one like vaguely compelling um but like the token one or like I'm streaming up like like megabytes of data um like and you and you don't want to accept that like you mask that's what mask is for so I mean I I do so I mean right you're saying you know just use for those cases if we want decoupled transactions just burn a new connection for every HP request yeah yeah like you're doing seconds and"
  },
  {
    "startTime": "00:32:00",
    "text": "seconds of latency like like upload the whole thing like I like either like either you are like uploading megabytes over second or you are not and if you are not then put it on one block and if you are then like connection so I I think there are a couple reasons why I I I I think this is still very useful so one um for these cases let's say you know it's the database it's the generative content it's the upload or whatever there are going to be a number of cases where sometimes and you know hopefully oftentimes this is a very fast transaction so you know certainly like if I'm accessing some type of database or doing generative content there are reasons I want to decouple my transactions right and in many of them they would be done in a single flight and occasionally it would have this spillover into okay now it's taking taking a while to generate the next token Etc or um so I think you know reasonably if you have a server that already essentially would just be doing HTTP and then says oh now we have some longer to generate things that happen for 10% uh saying like hey you need to essentially switch protocol Stacks here and do this like either we have the support both modes where they're like I'm sometimes OHP and sometimes I'm a mass connection and I just have the client wouldn't know which ones it's going to be well Ser does not support the mass connection right the server server just operates normal mode just to always do Mass connection on that server the server does not to support the mass connection the clent the client supports the mass connection but the server just like well but the server would be supporting it not as an omtp Gateway but as just like a normal which surely it has to do surely it has to do in any case yes but ALS I mean like when you're"
  },
  {
    "startTime": "00:34:01",
    "text": "think about like the deployment here having to manage you know a hybrid of how the servers are receiving that I think is a bit more comp the client the client also won't necessarily know which one is going to use and so if that's more that's more persuasive but the thing is a web server like I mean can you think I can't think of any application right now that only uses the only supports hdp it doesn't support like straight hdp so it's like I don't like I'm not buying like it's hard I mean so actually I mean like some of the use cases like we're we're using OHP currently for like safe browsing lookups and we've talked about that before um and in that case like we don't have a publicly facing uh normal TS web server like it only accepts it over omtp for some of the servers we talked to but yeah regardless like obviously it has a back end that could do it but I think it's more from the client like the client if it wants to be able to support showing the first first token before the other tokens it needs to now always spend the TL the extra handshake round trip even if for most of the cases it doesn't need that um and and so justifying you know why are we all like spending this extra latency chpt first of all like people like you know I mean like the CH is enormous like um like the first token takes second takes milliseconds to come in so like you know um so I guess like like I think this is like this is like the least persuasive case any of the ones I've heard the CH gpg one um because like I mean like the whole the whole it is gener the tokens as fast as it can they're still trickling in and so like so I like like what what case have you ever used for Mid Journey or chpt where the whole thing action didn't take forever when the whole I what what what's what I've never seen any of these systems right now that have the property that um that like the that that like you would Noti you materially notice the teals ls handshake round trip latency for the first token let alone for the fact that the tokens all trickle in I guess you don't"
  },
  {
    "startTime": "00:36:00",
    "text": "understand like what what what what were you're buying with optimization yeah I mean I think there are definitely some cases where some clients do have a more notable just like local latency that is incurred here but yeah I I get your point um anyway I can let Martin I think Martin Martin jump in I can figure how how to hang up here yeah all right so um I personally don't put a lot of weight behind the uh the use cases that involve long end to end latency I I can sort of understand perhaps why some people might want to have um a very short time to First token in those um scenarios um but I think ultimately when when it's the entire answer that you want a little bit of extra latency on the on the TLs side of things isn't like the end of the world um but I I do think that there are cases that involve uncertain amounts of latency where this becomes interesting so if you have a resource that for the most part produces responses very very quickly but but occasionally has cause to for delay then then this looks looks a little different um but um I I'm more looking at this from the perspective of of being able to take what may be um a slightly larger response not a huge one a slightly larger response or um something like that or in informational responses so the the onxx use cases um I think that greatly Narrows the applicability of what this is um I don't see a concern with having um"
  },
  {
    "startTime": "00:38:03",
    "text": "some potential for overlap with TS I think that was that was Tommy's Point earlier um but I think that we're going to need to be very very careful about applicability if we if we deliver anything like this I think we've made some steps in writing up the draft in that direction but I'm not sure that um we've completed that work so yeah um yeah CH I'm trying to get in there Hi um so I'm I'm kind of new to the conversation here so apologies if I'm uh I'm I'm going to my my my thinking here for this is that the the the stacks that you have to support for these are fundamentally different but I'm talking about mask and uh OHP so to E's point I don't know how much I think about latency in this uh I think a lot more about the type of Stack that you are choosing to build at the client at the relays and so on uh if you're if you're building something and you have to make a decision based on message size at design time of what stack you're going to support that's a very weird uh thing to Pivot on like if you want to decide use a mask relay you should do it because you want effectively a connect server and then you want to like have a b uh relay on the other side of it that's why you do it but if what you're really doing is HTTP messages and then let's say that you built out this whole thing and then suddenly you decide that your message is a large or you have a use case which was traditionally fine with HTTP but suddenly you can't do it on HTP it's a severe limitation and to solve the limitation you have to go all the way back to design and say we going to have to build the whole thing client relay Gateway server the whole thing now use mask instead of HP and"
  },
  {
    "startTime": "00:40:02",
    "text": "that is a severe limitation of hdp at that point so my uh thinking here and I you know correct me if you think differently of course is that this is a um unless there are other fundamental issues this seems like a valuable addition to HP simply because HP is substantially more like weight to build and support having done both mask and hdb relays it is a lot easier to build an HP one and I'd like to be able to support uh being able to send larger messages instead of having to Pivot all the way to using a mask stack cool thank you yeah I agree so so there's a certain amount of miror which TR is saying but I guess I want to observe that the entire principle of HTP is that you have to in fact narrow your application scope carefully analyze analyze your application scope and to determine whether it's like fits with the relatively narrow Contour that ohb supplies um you have to be sure you don't have to do multiple requests for instance otherwise it's kind of pointless um you know you have to be sure that um you know if to sure you don't care pfss if sure you sure you have to have pre you know pre you know pre establishment of negotiation of of the CER SS and so like um and and so some of those some of those problems are like quasi inherent right and some of those problems are not inherent and what what I'm concerned about is um is is exactly pres the kind of mission creep that I think Janna is thinks is a feature which I think is a bug which is like that we do this and the next time somebody says well actually it's really in commun I can't do X Y and Z with omdp then weend hdp again and at the end of the day what we've done is we reproduced like an entire ecosystem of things is basically mask again and we done have HP that would be extremely undesirable and you're shaking your head but I I agree we don't want to do that I I I'm totally"
  },
  {
    "startTime": "00:42:02",
    "text": "on the same page as you but this is like how protocols evolve right and so um so like is this particular thing like the worst thing in the world no and I like shut up after this but like um but like I I think it's like pretty important like understand what the thing is for the thing is not for and um and so I think we're starting to Verge into this is different from the thing this was for and now we're standing the scope and that worries me that's what worries me um so so so maybe you maybe you can lay my fears on that that that front but right so I I mean I I think and and we we can debate this and disagree on this but like going back to like this you know the the the chunking support actually you know was at some point you know a PR in the main draft and like to a large degree like a number of the bits that were built to below this kind of inherently were like yeah we're expecting to have multiple chunks of messages we're expecting to be able to support HB 100 Etc um and I think at that point you kind of complete the box and you say it's done and we don't you know we don't add other things to make it more like a TLS streaming you just say yeah you be you're able to do incremental process 100 100 is kind of like actually you know 100 is actually starting to like pretty much move in that direction right um anything understood understood understood um but again like I having a case where you know I the client like he like sending data that may be small but then occasionally is big and the server wants to say I don't want to have to buffer this whole thing before I respond to you like I I agree we need to switch stacks for that is a big change I agree it's undesirable what I observed it's not the web and so and so the system more is a little more like constrained than the web would be right um this shouldn't be you're not"
  },
  {
    "startTime": "00:44:02",
    "text": "going to be able to write a web browser over o i saying something different different which is like that you like control the client and so like you're not subject to like whatever wins like the web browser has about like what what prot uses right um absolutely uh yeah I mean I guess I guess I um you know I don't I I don't frankly find like this um this argument you're offering about like validating the data as what like um you know um I think your theory is like that the the attacker is like intentionally trying to send you data which is like which is like bogus and you know um and like but it's it's being hidden under you know um you know but and and and but as a consequence you're like oh only able to look at at the very end right um you know like like there's a number I I get like first of all like you actually can incil to crypto you just can't validate that the check sum right um um so you can see if it like is transparently bogus um and second Like You Know Nothing Stops the attacker from like you know producing something which brokus fine the last bite and the last bite is garbage and like you know and and they can do exactly the same thing with with the thing you're talking about where the check on the First on on bites one through a million look great and last thing has a bogus check on and so if what you're say if what you're saying is like that I I barf on a bad on on a bad on a bad authentication tag then this doesn't help at all and if what you're saying is I bar from bad is I'm taking the content incrementally then incrementally decrypted really tag so like I don't really think this actually does what you think it does but again like the road like does what it does do I believe like it allows if I send one chunk that includes my headers and maybe some initial data yeah the server upon reading that if it doesn't like your request could immediately send a response chunk of like five or 400 they could not do otherwise otherwise in order to send the response"
  },
  {
    "startTime": "00:46:02",
    "text": "no you would need to just abort the stream right I guess I is that true um why is that true I I need to double check right I believe you don't have the data you don't have the data to produce to return key do you need the entire message before you can generate the return key I I don't know theal no you don't need the entire message before you could generate a response in that I understand I understand respect into a until you have tag but like you actually possible anyway again I to make a break so again I like I see John's in the queue I said what I meant to say so like if people want to do this like I'm I'll vote no but I'm not gonna like I'm not gonna like Phil bu okay I'm in the qer but I'm also listening I'm I'm just you know this this is a good conversation I think that this is uh for for uh uh are we litigating I I'm just wondering in my mind right are we litigating the right thing here what I mean by that is how applications ultimately end up using this yes you want to tell them how to use it in the applicability But ultimately there has to be a trade-off that has to be very clear uh and that trade-off will dictate what applications ultimately use um if the trade-off is not clear it doesn't so we use HTTP in ways that we shouldn't use HTTP and I will say this now we will use OHP in the ways that we shouldn't use OHP that's going to happen but but well I'm just saying that you know it's it's not it's not something we can enforce uh at the same time if there's a real concern there uh and it's not just a design principle or if it's not like a you know something that we believe shouldn't be done uh if there are real consequences privacy in particular in this case then we should spell them out uh and and argue why oh HTP should not be used in a particular case because of said privacy concerns"
  },
  {
    "startTime": "00:48:00",
    "text": "and I think that should be there in the document I see some of them in the document perhaps there's more um yeah so so I think that's a question of shaping the document to be better not an adoption question so coming back to the question on the table should this be adopted I think this work is worthy of doing and if we are talking about shaping the document to include things in applicability that uh enable users applications developers to make the right decisions about which technology to use maybe it's worth talking about specifically in the applicability about HTP versus mask as an explicit thing like why should you use this and why not mask or when should you use mask and when not this um I don't want to necessarily explode the scope of this document um because you know this is just about this technology and not necessarily comparative but um yeah I mean I I I don't want to get too deep into that that's a question for later but in terms of adoption it's I'm not hearing any fundamental uh disagreements I think this is still valuable uh to do and it should not be used in places where it will be used in the future such things cool you're saying you fundamentally disagree yeah but I mean I said what I was gonna say I'm just I'm just saying like like I'm not persuaded um you know but like we agreement I think no I it's fair I think you you I I what I'm what I'm saying is that um you can't change the way that people use htdp and if all I want to do is introduce the oblivious in there then you're going to fundamentally ask people to reconsider how they use the htb if you limit this well I guess I guess what I am saying I think what I am saying is that like I think we should be I think that for like many of the applications Tommy's indicating we should be steering people towards mask"
  },
  {
    "startTime": "00:50:01",
    "text": "and that by providing features or kind of half mask like we're doing them security disservice by steering them towards by steering toward by VHB for the reasons that are in Tommy's chart and so like again maybe people find persuasive but that's like the case I'm trying to make I mean my my my concern there also is you know I have not yet seen deployments of something like ask where people are doing unique endtoend TLS connections for every HTTP request um you know certainly that's not something that we do in the browser or whatever by default and that also like as I was bringing before like does have implications on proxies that you know if we are taking cases that are not you know not DNS but like similar scale to DNS of how many queries clients are making and saying the egress Mass proxies need to bring up a new new socket for every single one of those queries that's a very different proposition for them and so I think a lot of cases if you're telling people hey for your use case to use mask they're just going to have correlatable uh h&p requests and they're not going to have the decoupling between them and they're going to make a different tradeoff sure this was the argument behind be in the first place right um which I with I guess I guess what what I'm saying is like is that like I think that argument to fall apart when you have things that have like high latency or long running or like a lot of data back and forth the overhead the course overhead Mas on is much lower and so I think like I think like what you're to write applicability statement what you're saying is that this is mostly applicable for things that actually are relatively small um but um and do not involve like with the overhead of of of you know of doing the mask setup is actually um but I mean I think like I agree with you like when you say like you know back and forth and like Long Live sessions like yes 100% do that on mask but like even even if it is a case of I don't like you let's"
  },
  {
    "startTime": "00:52:00",
    "text": "take uh the generative content or any of these cases um where I think there is benefit in having decoupling between these if if we are doing the normal OHP where like we only get anything of the response back like yes maybe even if you say that like yeah I'm going to have to wait a long time to get all the content back and you know it's not useful until then even having the indications of am I even going to get a you know 200 okay back from the server like is it even there um or am I just like showing it like endless Spinners um like just getting something back of like something's happening like is useful and for cases like that where people are trying to build applications if we're going to end up pushing them because they have this case to uh proxy TLS session they're going to end up most likely with more correlation between the requests because that's just how these proxies are built maybe so I'm not yeah yeah I guess I guess I think I suggest like I mean is this schedule for an hour I suggest like I think yeah I think we should just like like like the CH take a Shan just I want to add just one point uh and this is not directly in response uh but I want to reiterate that we uh um from an implementation or deployment standpoint there is a significant difference in the universes between mask and omdp and it's not just as simple as saying we'll switch one out for the other uh I think from a protocol perspective and from from the value that the user gets yes it is correct that they are you know the same value ultimately and if it's bites back and forth but honestly in terms of relaying uh um it is sub substantially cheaper to do this with hdp and you know that's going to see a lot more traction than"
  },
  {
    "startTime": "00:54:01",
    "text": "mask well mask is going to get used for a lot of things but there are lot lot more things that I think that hdp will get used for great all right cool CH thanks um thanks all so yeah I think uh we can do a show of hands um try to do that here asking the question should we adopt this work U does that sound good as a question uh Richard that is uh the ultimate goal here so yep that sounds right cool um I started the show of hands so you should be able to mark your uh answer all right I'm guessing the four people not voting right now two chairs one area yeah maybe two one interloper who's who's not paying attention e I'm guessing you are the do not raise hand yeah okay so we don't need to solicit brother commentary there um all right I mean that seems pretty positive to me um non non-trivial community of Interest here um with concerns from eer noted um I think we can take that and confirm on the list yeah that sounds good to me um I guess yeah I guess depending on how the call goes we can see if um we want to have a session at 119 and Tommy and"
  },
  {
    "startTime": "00:56:01",
    "text": "Martin you can let us know if you want to add something to the agenda um anything else to talk about if not thanks all thanks Tommy Thanks Martin thanks everyone um see you later bye thank you"
  }
]
