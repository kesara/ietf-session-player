[
  {
    "startTime": "00:05:37",
    "text": "Tation So, you know, it's actually one less than this, but same content, just One presentation instead of 2. And Any questions, agenda bashing first? Nope. In that case, without further ado, Sophia will talk to us about the NRO RPKI program."
  },
  {
    "startTime": "00:06:00",
    "text": "And I try and get the correct set of slides up Confirm your selection. Oh, That's right. That's right. Hello, everyone. Good morning. Thanks for coming to the covering, depending on when you landed. I'm Sofia, for those of you who don't know me already, since January, early January this year, I'm the new enroll RPI program manager and since Since I've been gifted with a few more minutes than I originally thought I had, I thought I would introduce myself Because it's been like 10 years since the last time I attended an IETF meeting. And so some of you may remember me from back then, and I'm sure there's lots of new people as well. So until December, I was working in HR for APNIC. So because of that, because of the pandemic, because of becoming a mom, It's been a few years that I've been a bit disconnected from the technical world. And as part of, promoting this new program that I will be talking about today. I wrote, blog art that was published a few days ago through the 5 IR blogs And the blogs have up an author profile And I had to review my bio And I have these kind of philosophical moment and a bit of nostalgia who I used to be when I realized that those buyers were very out of date. One of them said, that I anymore. So looks like the mic might have"
  },
  {
    "startTime": "00:08:00",
    "text": "stopped. I don't know. Try bang on it well, well, well, well, well, Tap on it. Tap makes people Yeah. And here we go. We can take this one, and I will get the echo to bring this here. Okay. See if I can break anything. Oh, Thanks. This would be my work hard for the day. It's quite heavy. Okay. That's fine. I say I wasn't a manager, Cigna? Don't I was used to holding microphones. So I was saying, Tell me, I'm not who I used to be. Those things that I mentioned have now been replaced by amateur nutritionist and expect with expert at dealing with tantrums. And a sleep addict, but most of the timing we drawal. So bear with me. Oh, That's much better. So bear with me because days, I haven't had a very good night's, sleep. I think that's enough about me. What do I use to change life? This one? Okay. So next slide, please. What I will be covering today is a brief introduction to this new, NRO program. What we're trying to achieve and how we are planning to achieve it. Who we are when I talk about us. And, few minutes for questions. If there's if there are any. Next slide, please"
  },
  {
    "startTime": "00:10:01",
    "text": "And I'm talking about Niro assuming that some of you know what it is, but I thought I would include one slide about what the The NRA is the number resource organization that brings together the 5 regional internet registries. contributing To achieve that mission of actively to an open stable secure Internet. Next slide, please. And, why this program is important before I talk about how it was actually created. We are aware that there's currently some diversity in the way that the API system has been implemented by the different There are inconsistencies in the services that are offered, in the way they are offered, there's some inconsistencies that have been identified and documented So, for example, there's a manners document that this some requirements and standards for operators of the RPKI service. And includes this annex that I know is very old, and it's actually outdated. So probably one of the things we need to do is share with Manu's updated information on whether IRRs are compliant or not with those different requirements and standards, but this was just to show an example of inconsistencies that have been identified documented. You can also see in the documentation of Krill, when the description of interacting with parents, there are differences on depending on what the RII that the parent is, yeah, the interactions will be a bit different. So you have the source there for more information. But, basically, we're aware of those challenges that this may be presenting. And Next slide, please. The Anna is aware. And in 2022, the NRO went through a strategic review process, And one of the outcomes of that process was an agreement to work to focus on, more robust coordinated and secure RPKI service And as a consequence, this RPKI program was created. Should I Wait or"
  },
  {
    "startTime": "00:12:00",
    "text": "No. We're good. This new RPKI program was created And as I mentioned, I started this role early January this year, And, As I understand it as I understand it, where we could contribute the most. Is helping with, removing barriers of the obstacles that mostly those organizations, network operators that are interacting with multiple IRRs to create RPIGA objects, maybe experiencing a consequence of the icon inconsistencies that I mentioned. So Next slide, please. What we're trying to achieve the purpose of the program is to provide a more consistent and uniformly secure resilient and reliable RPKI service, but in particular, we've been working on agreeing. What are the more specific outcomes that we could, that working towards. And that's what we have agreed on. So it's basically those entities that may experience difficulties because of inconsistencies because they interact with multiple IRRs. We're hoping they will be able to do so through a single spec interface. We will be also working on improving the transparency of the robustness of the system, enhanced security consistency. That's another area of inconsistency nowadays. And finally, and this presentation is part of that last outcome is Keeping the technical community informed and engaged throughout the program. And also, when there are, concerns raised that we can bond in a coordinated way. Next slide, please. And how we're planning to do that this week? As the steering group, and I will, share with you who the steering group this, has blown from different parts of the world. And, for the first time, we're together in person, most of us, we will be, taking that opportunity to in person, and we will be having some sessions brainstorming. What are the initiatives that we could work on this year."
  },
  {
    "startTime": "00:14:02",
    "text": "And prioritizing based on value versus effort. So do an doing an estimation of what's there for that it would require to execute those initiatives and whether it is worth, that effort if the value that we could offer is high enough. Next slide, please. When I say we, I refer broadly to the product team but, to the program team. Sorry. But in particular, as I mentioned, I will be working directly with that steering group. So the program team consists of the executive council of the NRO. They are the executive sponsors. And I am the program manager, but the stars of the program, Dima, the API experts from the different IRRs So the steering group has representatives from the 5 IRs, and we would probably also be work with, Other subject matter experts from the r I the IRs that also, work in the space of RPGI. Next slide, please. I know that you probably know some of the names here of them are actually in the room if you want to raise your hands Tom, Mark, Brad, I'm just naming the ones that I know are in the room team, Felipe, so I most of them are quite active in ITS, and so I know you know them. Did set the guys that I would be mostly working with. Next slide, please. And, as I mentioned, I studied in this, role early January. And I've been trying to catch up with where things are at because I have been away for a little while and a lot has happened in the last 10 years or since I kind of lost touch of it with the technical world. So one broad question that I would like to put out there for anyone who would like to answer for me, doesn't have to be today in the room, but feel free to reach out to that email. Is what are those main barriers that stacles that we could contribute to that. We could remove or improve in some way through better coordination and collaboration among the RIR"
  },
  {
    "startTime": "00:16:00",
    "text": "But in general, if you have any ideas of initiatives that you think we should consider, also please feel free to reach out to that email address, and we will Listen. Listen. And next slide, please, if you would like to know more about this program, have updated that RPKI page, in the general website with some information about the Epic Air program. At this stage is pretty much the same that I'm sharing today, but we are going to use that page to keep the community updated on what we're working on. So, with time, we will include the specific initiatives, or also if we have specific requests for input from the community. We will also, do it through that page And as I mentioned, there was a blog article. I included the URL for the AP Nick, but the 5 IRs through the blogs posted a blog article, with some information about the program as well. And that's it from me. Thanks, everyone, for your attention. If there's any questions or comments I would be happy to hear from you guys. Job Snyder's, from Fastley. Thank you for stepping up to this job. In my personal experience, the RPKI ecosystem can be a bit stressful to work in. There's lots of opinions. Some backed by academic research, some by operational experience from ISPs, some from operational experience at the RAR level, and and navigating all those, steaks, is I think Quite the job. So I I wish you the best of luck and, Welcome. Thanks. Thanks, Joe. Yeah. Spend the last couple of months catching up with with are trying to understand those, like, big challenges. I'm also trying to understand who the players are, and there's, like, so many and with such a variety of needs and concerns and realities that"
  },
  {
    "startTime": "00:18:03",
    "text": "mean, I was prepared for the job being a bit challenging, so no big surprises there. But, yeah, I will rely on the help and the participation from all of you guys, all the people involved, with, our PKI work, because I'm no expert. Anyone else? Any questions? Any comments? It's good to be back. I was hoping a queue of questions or tomatoes, but have been up to sit there. That's it then. Thanks. Thanks, everyone. I just wanted to say thank you very much. For doing this and also apologies for the technical know, going first is always entertaining. We have I'll send you a month. Hi, Tayo. So now since you've mentioned it, why were you expecting possibly tomatoes? Oh, I don't know. Because I was like, I guess During the recruitment process last year, I was prepare, like, I was I got so many, like, warnings on, like, so that, you know, and I guess I was prepared for the worst. But it's nice to see happy and friendly faces and be positively surprised So, yeah, thanks, everyone. And tell, come to a cider ops meeting, then you'll and So next, we have Naomi. I believe who is going to be presenting on ipv66 extension header comparison. I don't remember the exact title, but it's in the agenda. Interest apparently, it's gonna be a great presentation with disco lights. There we go. So how to implement ipv6 extension hitters, a comparison,"
  },
  {
    "startTime": "00:20:01",
    "text": "Is that Okay. There we go. So I just wanna confirm these are the right slides. Right? Is this the ones you sent? Okay. Cool. And I will give you I've had There we go. As I said, disco. Let's give you this mic, which should work Apparently, yep, Hi. I'm sorry. I'm I'm Delaney Elkins and Chinmai and Umogue are gonna talk about some work that we've been doing for the past couple of years. On, ebpf and extension headers. Sorry. This is the root ops alert that goes off when they do the root ops testing every whatever yeah. Hi, everyone. Yeah. Hi, everyone. I'm Jin Mai. So, we'll be presenting briefly about, performance evaluation of an implementation of, an extension header called PDM. In EVPF and kernel. Yeah. So, this is the overview. We'll be talking about some EBP of concepts. And we'll be talking about, extension header, the premium implementation in EBPF. And we have done performance analysis and the Linux kernel implementation of medium, medium Next slide. Yeah. So, why we chose EVP for our implementation? So, there are 2 other ways that we could have implemented this. So why EVPF over a Linux kernel implementation? So, it it is"
  },
  {
    "startTime": "00:22:00",
    "text": "easier to implement in terms of, development times and There's way less maintenance. Involved. And, also, it's very portable so we can put it across kernel versions without having to worry about, dependencies and any changes made to the kernel. And the BP there is something called the VP of verifier, which, lets us create safe programs that can be run-in the kernel. We don't have to worry about the kernel crashing or anything, make sure that, there are no infinite loops and things like that. Also, the accuracy of timestamp captured is related to a PDM. PDM is performance and diagnostic metrics, and it uses timestamps in order to calculate, network latencies and packet processing latencies. So, the there are EVP of hook points that are executed before the kernel network stack is reached. So, that lets us capture, via arrival time and yeah, via arrival time in a much more accurate way. Also, why EVP of our raw sockets Yeah. So, all we have to do in EBP is we just have to make space in fully crafted packet and then we have to add our extension header Also, we don't have to modify existing user space applications as we are not, we are using the kernel's stack completely. We are just adding an extra program that inserts extension header into each and every packet. Next slide, please Yeah. Coming to TC BPS, So, TC Bpf is, any Bpf hook point in the a control subsystem in the Linux kernel. So, yeah, it's a subset of EBP programs that is attached at the queue disciplines level. So it can be attached to both ingress and ingress So, as opposed to, XTP, which can only be attached in ing ingress And it has bet better packet manual capability than"
  },
  {
    "startTime": "00:24:01",
    "text": "XTP because we have the escape of, which allows us to perform various packet related operations. It's not good for complete packetry rights. So it's better if we just do something like, a minor modification to the packet. Like changing a field or inserting an extension header. Next slide. And this now works. Probably fixed it somehow. Thanks. Yeah. So, coming to the implementation of PDM using TCP So PDium is performance and diagnostic metrics. It's described in RFC 8 to 50. So it's a destination options header. It's used for measuring packet processing latencies, packet processing and network delays. So, we can attach to both ingress and ingress of face, and that is especially important for a PDM. Because we are capturing time stamp in the in the English, and we are using that to calculate certain fields in medium. So, yeah, we've used DPF Helpers which are certain API that is provided, for, packet mangling in EBS. And we use something called EBPF map to store the 5 to post date so we can communicate between the ingress and the egress program using these EBPF maps. Over to Amok? Yeah. Hey, everybody. We benchmark this EVPF implementation with another kernel implementation. So this is heavily dependent on the implementation itself. But I would say this would represent something like, where EBPF hasn't been, like, optimized a lot. Compared to the other implementations. So we checked on 3 things. CPU cycles, network throughput, and packet processing latency, CPUcycles, we simulated network traffic using IPerf,"
  },
  {
    "startTime": "00:26:00",
    "text": "And we would measure it using birth, And, we had a test set up like, with 2 VMs, with, 16 course. And 16 GB RAM each. And, network throughput, we just directly measured the iper throughput that we got. And packet processing latency, we used F Trace. In the current stack to, get the individual run times of each functions, Yep. So the CPU cycles, we got both for ebpf egress and ABPS ingress programs, we have 2 of them. And we can see it performs an order worse than the kernel of We'll talk about what exactly was this. So we we're able to get a frame, the flame graph. And, memory updations were actually taking a huge chunk of that CPU Cycles. So we think it can be optimized talk about it in future work. Then Yeah. So the PDM kernel implementation, that's the egress that we are talking about. So we compare that with the egress program. Yeah. Network throughput. We see that know, without PDM, it is much, you know, we have a higher throughput as we would expect. We're not doing additional processing. And, for PDM Colonel, we see that, you know, there is a slight loss of throughput But compared to EBPF implementation, it does perform slightly better. And EVPF implementation We saw it was using a lot more clock cycles and hence we would expect some amount of you know, affecting to the network throughput, and we see that here. So EBPF implementation performs marginally was, but nothing Too bad."
  },
  {
    "startTime": "00:28:04",
    "text": "Yeah, this is packet processing latency, and how we would implement, like, like, reinterpret this table is the 1st line, the PDM kernel implementation, that is direct measurement of the function in the Linux kernel that we implemented that directly adds the extension here. So, it shows the approximate latency for the function run time. But for EBPF, we are not able to directly use F Trace to get that. So we are using function that then calls the hook point. And so we have measured both with EVPF program attached without the EVP of program attached. And, hence, for each run time of the EBPF program, we subtract both of them and, we see we compare 1.28 microseconds, which is the egress packet processing latency of ebpf with the PDM kernel implementation. And, kernel is around 0.7 microseconds and, ebbs takes around 1.28 microseconds. This was obtained using F Trace, And, that's also heavily dependent on scheduling and the VM scheduling and the other environment you know, in the test bed, the ingress packet processing latency is also around 0.5 microseconds. Yes. And this is what we plan to do program. To find out where the limits are and how well this can be pushed. Because EVPF offers a lot of flexibility that isn't there in kernel. And, we would like to see how well this could actually perform. I what we think is we are doing a bunch of BPF SKB reads, which might be taking up, all of those memory related, CPU cycles that we found using the flame graph"
  },
  {
    "startTime": "00:30:02",
    "text": "we also would want to do how is this EBPF program performed in high performance computing environment, since we were just using consumer grade machines to test this out, And then, we would also like to implement and analyze other extension headers in EVPF. How well it would perform something like hop by hop and some other destination headed, and we'd like to see how is flexible, good enough for these extension headers as well. Yep. That's about it. If there are any questions, we'd like to hear from you or any thoughts Lorenzo. I mean, This actually I mean, this looks like it's fine. You know? Compared to in production environments changing the kernel. Is this generally, like this multi year effort. Right? Do you theory, it's something you can do in 6 weeks, but then you you find out that in practice, you know, like, you qualify a new kernel version and there's regression, so on. I would guess that the inefficiency here is because you have to move stuff around. Right? You run up to the IP header has been written. And so you basically have to shift, Right? I'm not even sure, like, how you do that. Like, how do you, like, do you rewrite the IP header? Because you you've got to, like, create space at the beginning of the packet. And then you go to put your extension header before the payload. And then you have to, like, probably, like, copy the API header. Yeah. Actually, it's made easier using, there's a helper function in the kernel written? Or is it helpful to, like, insert something? Yeah, we can create space immediately after the IP header. See. But you still have to move stuff around because you've done it once the API header is already written. Yes."
  },
  {
    "startTime": "00:32:01",
    "text": "I see. So that helper is probably not very efficient because it and you probably incur the cost of cash misses. Basically, it's Yeah. I wonder if you can do something like use an LD preload thing to, like, create a destination extension header. Like, and and basically cause it to reserve the space, and then you just overwrite it instead of moving it thing around, I don't know if you it's just like a as a hack. You know, you you if in a in a closed environment, it might work. But, yeah, the this generally looks like it's fine. Did you publish the code? Like, is it somewhere where people can look at it? Looks quite interesting. Yeah. That's also a discussion that we'd like to have. We have not yet published it yet. And along with, preferred license, we'd like to do that. So Great. I had a quick question. I'm just going to drive your slides because I can. So in the throughput test, It was, like, 18 gig per second assuming that that's with, like, fairly large package. So was that with and so it makes a traffic yeah, we had a MSS size of 1000 set, Okay. But the MTU was 1500. Okay. Okay. This is because we would have to check each time in EVPF because we are adjusting space. In case we increase it above the MTU. So we didn't wanna do that. 2. Great. Thank you. Great. Any last questions? On CNE Awesome. Awesome. Awesome. Thank you very much. Thank you. Thank you. And next up, We have Mister Jeff Houston, where Is DNS ready for ipv6? And let's discover if DNS is ready for ipv6 66 I gotta Seeing as it's Jeff, probably not."
  },
  {
    "startTime": "00:34:00",
    "text": "Yes. It is. Cool. Thanks. Good morning, all. My name is Jeff Houston. Yes. No. Whatever. Okay. The reason why I'm kind of asking this question is that there is this RFC, which the title and the body just don't seem to gel. So in in this RFC 3901, and it is now twenty years old. Younger than B6, but only just, which actually is titled V Six Transport Operational Guidelines. Where it kinda says Don't drop B4. You know, it really it's saying just keep the foregoing, which and ominously was the guideline in 3901. It's kinda For god's sake, don't drop v 4. Fine. But as a V Six operational transport operational guideline, I suppose that was an accurate piece of advice. But, you know, someone spotted this only last year. And they're kinda going, Really? Really. And so there is a draft out there, momocha Tom Momoka san, what's the female version? Momoka san, he was in Prague saying, look, we need we need to update this. And the update 3901bis recommends, you know, in in normative language, that that There are at least 2 NSs which are dual stack, which if you think about it, is a world ahead of what we were saying 20 years ago, just keep people going. Don't care about 6. This is kinda going, well, you should do something about 6. And and there's even a should there at least should be one v six reachable the authorities. So The kind of thing that I see in that is is sort of if if we are gonna be serious about V Six, you know, let's take it seriously, And it it's it's not saying keep the 4 around. It's it's largely the opposite. You should really be equipping DNS infrastructure, with these 6."
  },
  {
    "startTime": "00:36:00",
    "text": "And indeed, thinking hard about the 6 only. And The assumption Behind this latest draft. Which I'm sure some of you in the room and some of you out there on the net kind of agree with as as a general proposition you know, is it V Six is now mature. I'm like, Christ, 30 years. How much more mature do you want? It's now and it's dotage technologically speaking. And it it it's so well mature that transport for the DNS is one of the easiest tasks in the world. It should be efficient. It should be fast. It should be just as good as me for And you kinda go, really? And some of you might go, oh, yeah. And some of you might go, oh, no. I thought I'd measure it And and and There are kind of three questions, I think, that are relevant to this. And the first is actually a question around We really are relying on happy eyeballs. And we're really relying on an architecture of transition where in a dual stacked world, You always send the first packet in 6. The reason why is that in a large and diverse network, You never know when to turn off 4. There's no clear signal. If you always prefer 6, Always, Then at some point, you might look at your network going, Well, everyone's doing sex. There is no four and it gets turned off automatically by that preference. And so behind happy eyeballs was actually an experience we went through Does anyone remember Deknet Thank you. Does anyone remember how it died? Because all the users at the time prefer to use TCP, you know, and DeckNet just slowly died. There was never a switch off moment Those is no one using it. And the same theory is kind of applying here if you do do happy eyeballs,"
  },
  {
    "startTime": "00:38:01",
    "text": "This V Four will die if everyone's dual stacked. That's just the way happy eyeballs is meant to work. So the question is in the DNS, not the web. This is true. Interesting thing. Or if that's not true, Is there a bias the other way, or is it just random? So that's the first question. Do you have more questions, Lorenzo? Why do you assume that happy eyeballs means that the other thing will die? It's probabilistic. In the presence of an off packet loss, the old thing will always be used. Well, the issue is if there is packet loss in in D6, you need fix that because you'll still have the 4 around. In other words, always packet loss. You pessimist. There's no packet loss. We build perfect network. Yeah. Right. Bite my tongue. Yeah. We're all professionals at least for the 1st 2 minutes. The other thing which is kind of well, okay. Let let's take this one step. Further. Let let's drop dual stack and simply place an authoritative server on V6 only. And let's see in today's internet How many users will get there and actually get an answer? And and thirdly, because Quite frankly, in my view, addresses don't matter anymore. Routing doesn't matter anymore, the entire trust authenticity and security infrastructure of the entire internet is name based. You know, name based certificates, name based everything. And and quite frankly, the web PKI. This is the web PKI. If if we really wanna make this better, we actually need to make that binding tighter. Of of authenticity and names, and the only technology that's available, you know, in this universe currently is dnssec if you want to improve things. So it's a useful question to go Well, if we ever got to that mythical state of using DNSsec and the 6. Does this work? So those are the 3 kind of questions that I had here. And and"
  },
  {
    "startTime": "00:40:03",
    "text": "This has given up on me. You've got batteries anymore. Next slide. Oh, sorry. Back slide. I think it no. There was that was a real slide, wasn't it? That one. Oh, no. How does it well support large UDP next slide? Yeah. Next, you try your button. Okay. The DNS for those who have poked their nose in is bloody odd. It's worse than bloody odd, only folk in this community could have invented it. And the abuse of terms the abuse of terms is stunning. We all talk about resolvers. Nobody knows what a resolver is. Is Google's 888 servers a resolver? Because the common language says, oh, yes. Of course, it's a resolver. It's not. It's a whole family of resolvers. So is it just my platform at home? Well, that's a resolver too. But there's a difference when I wanna talk about resolvers. Because you're trying to talk about resolvers that matter somehow, and that gets really hard because in this day and age of Resolve Farms, of sort of large services where the actual engine is distributed across many platforms, and there's some weird form of synchronization or maybe not between them, to be perfectly frank, We have no idea what a resolver is. So if you wanna talk about the DNS in terms of resolvers, I think you've kind of caught the rocket ship to Pluto immediately. Because you're inventing a reality that the rest of us don't understand because none of us understand resolvers That's the first problem, but there is another problem. We don't understand queries. What the hell is a query? And it sounds kind of silly, but if you think about it, When I ask for name to be resolved, My question doesn't get past to a point of authority for that name. Does not. My question goes to my front end recursive resolver who goes, that's a good question, Jeff."
  },
  {
    "startTime": "00:42:01",
    "text": "Let me hold that to one side, and I'll ask a question on my behalf, no reference to the originator, and go and ask that question. But why ask just one when I can ask 2 or 3 or 10 or a100 And quite frankly, queries just fan out. It's not a single TCP trans action. It's just a massive stuff. So again, when you have queries, you've gotta understand that queries is this sort of amorphous concept that just multiplies like crazy. There is no such thing as a query. There's a lot of them. And the DNS is now basically AI. It's taken a life of its own. And if you pulled out a query from the DNS, logs is he has a query. Why is it here? Nobody knows. Just queries. It's weird. So we try and measure this stuff. And the way we try and measure it, is actually from the edge looking in. So the measurements from the user experience And so when we talk about how many people do this or how many people do that, we're talking about users not the count of resolvers, don't know what a resolver is, and not really the kind of queries, don't know what a query is. And so the way we do this is is Is anyone you're not seeing this slide? No one put their hand up. Good. You know all this shit. Good. Moving on. There is a lot of noise in the DNS. 46%. Of these unique names we generate in the ad system, I'm the only authoritative server at the other end 46% I see 2 or more queries. From a single known seed query that enters the DNS cloud. So the DNS kind of naturally goes, look, 2 queries is better than 1, 3 queries is better than 2, you know, and it just goes on and on and on and on. One query there's, you know, a non negligible amount that actually I see 10 follow-up queries in the first ten seconds. If I'm prepared to back that off for a few years,"
  },
  {
    "startTime": "00:44:02",
    "text": "I will still keep on seeing these unique name queries. I put the time stamp in in the query name amongst other fields, So, a, the DNS never forgets. B, if anyone really wants a massive storage device, just use the DNS. You know, write query, read, listen, it'll come back to you. So there's a lot of noise in the DNS. And quite frankly, you know, forty percent of all queries of 2 or more. Average number of queries, about 1.7. So the DNS is extravagant. It's UDP. Send 2. Send 3. Send 20. What the hell? So now we get into this question, the first one. Fuel stacked, How does it work? Now don't forget I'm measuring users. Not measuring resolvers, not measuring couriers are measuring users. And the way this would kind of work, the authoritative server always answers no tricks. Not making it any harder. And I'm looking at For each unique name, Single seated query coming in, What were the characteristics of the queries I saw at the authoritative? Were they only V4? 1, 2, 3, 10, whatever. 43% of the time Only V4. Were they only V Six? Well, that's only 11% of the time those queries that came in were only coming in on 6. And the rest was a mix. 46%. Now if you kind of thought do they prefer to use V4? They might, but why is it only V4? Interesting. So this is around half a billion queries. Done late last year. So less than half Of all those resolution queries, showed dual stack, even though undual stack Okay. Does it do happy eyeballs? Well, no. Quite frankly,"
  },
  {
    "startTime": "00:46:01",
    "text": "When I get a bunch of addresses that I can query for a name, the DNS implementations that people use for recursive resolvers. Bind unbound or power DNS or anything else. All of them seem to go, here's a bunch of addresses. Pick 1. Pick another. Keep on picking until I find an answer. Don't care which protocol. Mark, is that true? Good. Wow. Wow. Whoops. I want the laser. If bind had a dominant market share, that green column would be at the top. No. It's re no. It's fascinating. I'd derive from that data there is no bias. And if you're telling me Our I only have one server if you're making if you're the recursive, there's only me and you in this this room you know, if the V Six path is completely a wall, then okay. But it's it not V Six pads out that anomalous. Mike, please, I was gonna get was gonna give him your mic, but Mark Andrews. The other the other thing of course is that A request is server. Doesn't know A priori, no. Yeah. So the rancher times when it starts. we'd we'd We choose a random small value Just so. We spread the load around a bit, but we do actually bias IPV 6 service. It is on By a couple of milliseconds. Great. I I, you know, I thought bind was pretty common. I think we all assume that. And if that's the case, the way this experiment work, that does not show in a marked bias."
  },
  {
    "startTime": "00:48:00",
    "text": "21 per no. 20%, 20% 23%. You got a lot of Google out there. You got a lot of quad 8 and quad for and No. quote 1s. No. No. I don't. And Not really. Oddly enough. In these kinds of experiments, Google is not that dominant. Google's around 10 to 15% market share. Cloudflares around 5. The rest you really can't see. So this is a whole lot of ISP resolvers. And there's a long tail. Whatever. But you see what I'm saying? Right? There's no visible preference in the behavior of resolvers I take on board what you said about bind. Now I'm scratching what's left of my head and my hair, you know, it's like, oh, this is weird. But it it is. It is no no solid signal. And this is the next one which is a really hard slide to see and interpret. But I look at the first two queries that I see at the authoritative, and I've got a timer going. There's a packet capture. How long did it take for the second query? Now there are a number of UDP based timers in various implementations. Someone runs a timer of 370 milliseconds, someone at 400 someone at 800, someone at one second, and those are the peaks Those colors which you cannot see varyv64v66, etcetera. And they're not distinguished. It's not that V Six V Four is space to this way and the other ones aren't, the behavior of Resolve seems to be protocol independent in that second query. Please. Yeah. here, Yeah. right? On one hand, you are You have a mix of 2 experiments benchmarking the performance between resolvers and your measurement, perhaps form, OAV4v6, and the knee and and the DNS behavior. You're talking about resolvers don't know what resolvers is. I am benchmarking the out comes between users This is an ad based system. I'm not looking at the infrastructure you you you lucky you get 1 of APNX ads."
  },
  {
    "startTime": "00:50:02",
    "text": "Congratulations. Don't click. I pay more, or Google do. And, I'm looking at the behavior as you see it from you compared to you and you and you and you and you and you. So I'm not benchmarking resolvers. I'm benchmarking The experience of the end users. Right? distinction. Subtle Yeah. But but when you then look at the query logs use you see You see basically paired measurements, from between whatever approaches your DNS server you all everyone gets a unique query name. Lucky you. Yeah. So if I see the same name, I know it's you. Because a different query name is a different user, a different ad. So that's how I string together The sequence of queries from you if you were lucky enough to get an ad, right? And that's the timing that you're seeing here. Yeah. So that does include if the the Bob towards wherever your platform is is 100 milliseconds slower over V Six. That ends up in here as well. I can't see how long it took to get that query to me you know, unfortunately, or maybe fortunately, you don't give me control of your brow maybe you should, but you don't. All I see is a pack that come at my end. I cannot tell you how that query got to me what recursive result. I can tell you what recursive resolve as you end up using. But I don't know delays or anything else. I don't know why But this is the behavior that occurred. Between the first two queries. K. Yep. Cool. Cool. Jared. Jared Mach Akamai. I'm wondering if There's Actually, a behavior difference based on the user agent that you see. So I'm wondering if there's something interesting in the browser libraries and how they especially since a lot of the browser folks seem to think that they know how to do DNS"
  },
  {
    "startTime": "00:52:00",
    "text": "better than the DNS libraries, I reckon on Which the way. I reckon on, you know, Thursday, 5th April, might be right or wrong, by in 1990 or something. Some guy said, let's do browser signatures. In the very next day, someone said I'm going to lie. And sure. The browser signatures are are really noisy. Incredibly noisy about finding what browser am I talking about? So I have not spent time looking at those SIGs. Okay. I was I was curious if it would Yeah. I, yeah, I you'd see the similar, I don't I don't know. It's actually a hard one to correlate. I have sat here and all this work is just for DNS queries, not the follow-up web. Am I Lorenzo. I might, yeah, Lorenzo Foody. Am I sort of My name is Reading Lewis. So is this is just too damn smooth? Right? I I would say scale. Everything is smooth and a log scale. Oh, I see. There's a massive backup. But what actually amazes me and and this thing up here There's an astonishing amount of back to back And it's kind of I sent a query, and I don't even care what happened. I'm sending another. And the DNS is extraordinary extravagant. In those first few milliseconds of just going, I'm impatient. I'm just gonna snow the world with queries. And it's really prevalent. And and the only one that's actually spaces out is if I send V Six and another V Six, It actually spends more time before it sends the second V Six. Is all the others. It kind of So I don't know what behavior that is. They're within 10 milliseconds. It's not even waiting for an answer. It's just going to ask, ask, ask, So okay. So log scale. Yeah. So so that's definitely something I was I hadn't thought of, but, like, then why the peak's so low? The retransmit should be much higher. Right? I always answer."
  },
  {
    "startTime": "00:54:02",
    "text": "And I'm I'm I'm kind of any cast. I'm not brilliantly any cast, but I'm closest to most users and there's no tricks, I always answer. Right. But, you know, like, the peak at 800, right? If there's some common implementation that has a fixed retrans the timer was At 400. There is someone who put their hand up. But that's still kind of low. Right? one second. Yeah. Microsoft was But most of the time I answer it, so it doesn't need to re query. But that's like, oh, I see. I see. Right. I I'm not I'm not withholding answers, and I'm not playing with you. In that respect. yet. That comes Not later. 100 times less than the stuff on the left. Yeah. Yeah. Yeah. So this is all a very fast decay, but what I wanted to show was I The 7th is kinda weird, and I don't understand Some of these recursives just the resolution engine just goes, Funk, and sends out 2 or 3 packets without even thinking. It's gonna geeWiz, I love Packett Send me more. God knows why. And then then you see the retransmit time is going, packet got dropped. Jim, Jennifer. I'm not sure. I'm surprised to maybe I misunderstood this because if you run a wire shark on your laptop. What some browsers do, you get 3 resolvers. You received the ID, CPA, or the NSS, whatever. Right? And you try to resolve a name, your browser will send packets to all of them close the circuit after the first answer and then send ICMP port unreachable to the rest of them. So you will see at least a request for the same name, ideally, in this case. Right? Immediately. No. The stub resolver is normally more polite. But it's different resolvers. Right? Whether this when you've got multiple resolvers in etceteraresolve.com, You will ask one question to one answer generally. There are, you know, varientation. Once you time out, You might ask 2 queries to 2 different."
  },
  {
    "startTime": "00:56:01",
    "text": "You might go 3. It's sort of paced what I've seen anticipate down browsers ask all of them immediately. And then drop answers from a browser It's Is this you're running? Why are you running this browser? you know, this is weird behavior. It kind of it is not a browser. It is DNS mask. It has this option where you can configure it to send either to 1 or to all the resolvers. And so we well, when you configure it, send it to all, it will at all. So if it has 2 resolvers, it'll just spray. And when it sprays across all of them, you'll see some I always thought it was an economic failure that we answered DNS queries for free That's what it does, but you did risk is is is it this extremely popular money. Charge for answers. Well. Yeah. Yeah. Yeah. I mean, there's other implementation, but DNS mask is extremely popular among home gateways. And if it's configured in this way, it'll send them to all all its servers. And so it's not an aggressive retransmit. It's just like, I'm just gonna send you a warning. Yay. Thank you. DNS mask. The DNS community just love it. Okay. Hi. This is afraid. Yeah. Sorry. This is Selena. Now, I okay. Maybe I'm totally not following you, Jeff. It completely well. But, I mean, Seems to me, like, if you're gonna do if you do dna DOA, you know, DNS over HTTPS you're gonna get happy eyeballs. Am I am I missing something? I'm not doing that. This is just UDP. No. measuring. No. This No. No. I know. I know. I know. I know. This is is measuring what if. This is at what is looking if clients are using dough. A client might be doing DOH. Client could do whatever it wants. This of you from the authoritative server coming in from the recursive. But if the client asked twice, the recursive might say, well, the client asked me twice I'm just gonna transmit the second one as well. So, you know, I am looking at the point where I can see And quite frankly, there are very few recursive revolvers that give me all their logs"
  },
  {
    "startTime": "00:58:02",
    "text": "because, you know, why would you do that? And so, yes, this is the view from the back end of the DNS. And even there, the multiplication is, I think, still worrisome. It's just extravagance. So my conclusion is Find notwithstanding. There is no visible evidence of of a happy eyeballs like behavior that comes out of these numbers. For whatever reason, right? And so I'm I'm heartened to see that Brian does it, but it's kinda woah. Is there a reverse bias to years before there seems to be? Because the next set of numbers kinda go there's still an awful lot of folks who go, well, V Four seems to work best, so I'll just use that. Let's have a look at this. By actually now biasing things a bit. This server will only answer in V Six. It's the authoritative server. Recursive needs to get to meet using V Six. There are 45% of folk who only gave me queries in V Four. 35%, Oh, it's 10% less. We'll never query me. If I'm Z6 only. So 51000000 over 5 days in this experiment, 65 percent got me in V Six because that was the only way you got an answer. And the others, 1 third of 1 third of users were then stranded on a V Six only. Some oops. Comparing the 2, that's the dual stack mode where you see 43% in V4 only and no query at all, 35%. When you kinda compare the 2, there seems to be this apparent bias. Now don't forget I always answer. I don't withhold things. I don't choke up. It's just in the second one, you gotta ask him 6 and I'll answer. If you're asking for, there is no answer. It's just silence. Because you could never have got there. The name server does not have a V4 address. There's nothing. So It appears that Now two thirds of the network would see your domain name if it was served from V Six only,"
  },
  {
    "startTime": "01:00:01",
    "text": "from the authority viewpoint and the other one third would not Whoops. and then we add DNS sec. And Because, you know, V Six, packet fragmentation, and DNS Now as we all know, there are only really 2 major changes in the V Six architecture. And one was this thing about the extension header and moving fragmentations out out of the standard header and b 4 into an extension header. And and the second thing is Don't fragment is jammed on. There is no such thing as fragment enable, packet on the fly. Can't be fragmented. You gotta go back to the going, you gotta do it yourself because on the fly ain't gonna happen. Which involves a bit of ICMP pack of 2 bigs and all that kind of stuff. You know all this. So The DNS And DNS sake, Kind of today rely on fragmentation. Because when you ask who uses it, the root zone of the DNS will show you if you just ask for the DNS key resource record, for every single record in the root zone, They're a bunch of folk. Thank you, Slovenia, who say, look, gonna send you 3319 optets of answer. Because you look as if you need it. There are a whole bunch of these which are massive 300 of those entries in the root zone If I take 1500 as the point at which no matter what you're doing, fragmentation will happen. 300 of those GTLDs out of. What is it today that's active? 1000, 3000, 3000, somewhere around there. 300 kind of go then, Essex's gonna work for you. You've really gotta handle something about large packets. Okay? So This is not abstract, and it's not academic. This is kind of if you want DNS tech to work, you're gonna have to do Thank you. I think this is what it's telling you is that DNS SEC is Abstract in academic. I mean, the reason why the reason why it's okay to send back or 1500 is it doesn't need to work because nobody cares."
  },
  {
    "startTime": "01:02:00",
    "text": "I mean, it would be nice if it did work. In practice, you know, packets more than about, I don't know, usually 12:80, Have There is an there is an entire conversation about the comparative qualities of the web PKI and the inability to revoke certificates and the entire mess that is today's secure infrastructure cough, cough, laugh, laugh, laugh, versus what you could do with DNS sec. And I think that's a fine station because in some ways, the crap we've got now in the web PKI is crap. But it's the only crap we have. And and you know, DNS could feasibly do a better but if we keep on going, we're just not gonna use it, I think you're slamming the door a bit early. Think But No. No. I think, for example, I think is you can keep on slamming it. it required by standards not to work because ICMP messages must be rate limited. You're right. It and it's kind of this is a problem. Right? So so no. No. No. Maybe maybe I'm wrong. Yeah. Mark, maybe save what Jared said, let me repeat this because it's worth repeating. Nobody designed this whole internet thing to work together. And, you know, wow, if that's the discovery, welcome to the IEPG. Bak, Andrew C. Up we did actually put API hooks in need of tell the stacks To fragment. At the network MTU back in 1999. Now, if you're Dean is vendor hasn't set that socket option or Linux. Linux. Or Refuses to Do a by default. It hides the way behind a has his death. I'm not posix. Right. So, Mark, in the next few slides, we're going to determine how many folk don't listen to you. Yeah. Okay. We're gonna put a number. That Fragmentation on UDP Doesn't work."
  },
  {
    "startTime": "01:04:00",
    "text": "No. No. Over ipv6, The Server that is responding should be fragmenting for you. There is no time out if you're doing you've originated your name server correctly. So so, again, if if this is a sec. We might lost. We're we're diverting. We might have lost the external can somebody external let us know if they can still hear Looks like the network might have bounced for a sec. Okay. Cool. So if you don't know this, There's a whole bunch of interesting reading that you should do. Fragmentation is a problem in these it just really is. And and so if you're trying to rely on on the DNS and fragmentation, I'm pressing the next button, Warren. Can you just flip it forward Yep. Yep. Right. So can I measure it? The answer is, well, that's really easy, but I'm not going to disobey you. If you set a buffer size to go, don't send me me packets I won't disobey you. I will fragment or whatever as you need. I'll truncate you know, I will obey your instructions. So let's measure it. Next slide. Online ads, same thing, unique name stream, same thing, authoritative server, same thing, but it's a big answer. It's deliberately pushed out. Next, total number of tests, 32,000,000. Founded right. 18,000,000 did not get the answer. you And I know Because I'm doing this in the glue So you kind of set a trap for the DNS going, You need to go and ask this next domain. It's a referral response. On here's the name of the name servers, but I'm not gonna tell you their address. The only way you can ask the next query is if you resolve that name of those name servers, And I'm gonna again send you back a big V Six packet."
  },
  {
    "startTime": "01:06:00",
    "text": "And you can only ask the consequent question, resume what you were doing If you actually successfully received, that big V Six Packer. So were Of that, 32,000,000, 56 percent of users going. Didn't get it. Just abandoned. There was no way I can resolve that name. Now, like I said, I'm not pushing the envelope, I'm not disagreeing. If you give me an eating this buffer size, I will stick to it and truncate. I will go to TCP if that's what you want. But that number is You know, awesome. Awesomely bad. Thank you, Mark. Jeff, Does your name server actually fragment By default at network NTEU. I That's different to I I'm pregnant. I fragment by default, at my interface MTU But And and and that had no standard behavior. We UDP has a socket option to say to fragment at 12:80 regardless of your network MTU. Uniphase m to you. If you're in networking to interface m to you is 1500. It'll fragment at 1280. Have you turned that on in your Name service. No. I'm running standard stuff, Mark. I didn't listen to you. Like the rest of the world, I wasn't listening. I was just pulling it out of the box and running So is This is this have so we've we we So let's let's let's now let's now can do about do this. Let's look at you what this. Because I agree you gotta take this seriously. This is just awesomely bad. Because, you know, v4 to backup V six's failure, is not the long term answer. It just isn't. Right? So we've gotta change this appalling drop rate. And it is appalling. So do you work around it, or you fix the network? Maximum payload in the original DNS spec 512. Right? It was the maximum"
  },
  {
    "startTime": "01:08:04",
    "text": "amount of fragmented packets that could be reassembled by a compliant host. We added extension mechanisms. We allowed, in fact, we said in 6891, Well, fragmentation kind of works everywhere. Let's say we can assemble fragments that go up to 4 1000 and 9 exact kits, and everyone used it. So the issue is if if it's that much bigger, than what you say is the maximum size. I sent the truncate bid, and we return over TCP. Once we run TCP, the answer gets through. So If you can fit it, Fine. If you can't fit it, truncate. V 6, of course, has to go back to source, ICMP, we've said all this. So the issues are These six messages don't always get through ICMP6 messages. They can't be validated. Just messages You have no idea if it was your packet or just someone just sending you ICMP because it's a Thursday. Any cast makes it worse. Because the paths are not necessary symmetric on every hop, and of course, no one likes fragmented packets in security firewalls because and so You've also got this lost response and time outs. You know, in time out ranges times out, you know, 400 milliseconds to a second. So there's all these kinds of issues. So generically, I've been looking at fragmentation drop rates across the entire internet. And and the story is kind of less than good. I got the experiment working right at the@themiddleofthisyear, I'm seeing sort of the global average around 20, 25 percent of fragmented packets from users on the ad to my servers, any cars close to them, around 25% of packets never make it. We'll see. That it varies country by country, implementation by implementation, network by network. The V Six implementations in Australia are relatively clean."
  },
  {
    "startTime": "01:10:01",
    "text": "The server I'm using is against Singapore, but the drop rate's tiny. The V Six inflammation implementations in China impressive. The one in the top left. That's the one that's swinging to I can't even see from here. 50, 60% drop rate. India, good, Japan, not so good. So it's not even the original code, it's kind of how you surround your infrastructure and the kind of packet filters you're using, etcetera, that folk use different infrastructure, and they treat fragmented packets differently. So it's not an internet wide number. It's kind of network by network. Bizarrely enough. So you can't count whether fragmented packets make it or not, And if you're writing DNS software, it's kind of bone fragment, really. Because You just can't. If you're running a DNS, maybe as a server, you just should never send a pack larger than 12:32, irrespective of the stack, irrespective of your kernel settings, just never send a packet larger than 1232. Ryan? Or say to their clients, know, UDP is a mess. It's a nightmare. The web proves that you can do all this stuff over, basically, TLS. Let's just put the DNS over TLS and get rid of this nightmare. UDP was an old idea. Where I vote. Or you take an idea from Davy's song a few years back and just send an additional response together with the deep fragmented one that's truncated. Didn't get the first one one that tells you to use TCP. Oh, that's an extra packet. Didn't we discuss at the start of this session that DNS Mars sends extra packets gratuitously. Let's send another that says, if you got this one first or if you, you know, didn't get the other, just use TCP and get out of my head. Kinda could work. The RFCs actually say this if you pick other RFCs, UDP usage guidelines 2017, What it really says is For god's sake, don't fragment. Just do not. Don't even think you can get away with it."
  },
  {
    "startTime": "01:12:00",
    "text": "Don't so you can fix it. You can fix the entire network and make fragmentation work. Well done. You can change V Six. I'm like, apart from mucking around with of the flow label. What has the ipv6 working group been doing for the last 30 years? So, you know, you can change V Six because it's an option. Just just change this stuff. George, so, Jeff Weef, We've talked about this several times, and the observation I want to make is that 5 to 10 years ago, If you'd stood up and said, let's move DNS to a TCP transport the queue would have been 25 d. Know the administrator saying impossible. And I am not sensing that the DNS community wants pretend, couldn't, use TCP. You know, there is this theory The the only real user of UDP is the DNS. And the only real issue is we can't do source address validation. And if we move the DNS to TCP, we could get rid of UDP from the planet. There is no microphone line. I've still got some slide go, but as long as you're having fun, I'm doing fine. Quick Jeff. Was cute, but incorrect. Fine. We can cook the pig. Just basically changed the application because as we all know now, networks are relevant applications matter. So you actually biased the application to avoid it. So what do you do? The long run, you could just walk away from UDP, and it would not be a bad decision in the long run. But, you know, we're kind of stuck with what we're doing. And realistically, MTU discovery recursive, to to subrecursive and and recursive authority. The overheads too high, for casual UDP It's just you don't amortize the cost of MTU discovery against the consequent packet flows."
  },
  {
    "startTime": "01:14:02",
    "text": "So in some ways, you've got to think about this elsewhere. And and the best way is to actually truncate. Trunkate the thing and going, look, I don't care if it's alright. You either get the answer in the size available that frankmentation, or use TCP. And there's a DNS flag data say that. Is anyone listening? Here are the offered eating a 0 buffer sizes of resolvers that talk to me weighted by query volume. There's an awful lot. 46%. Who are kinda going for and on 6 Someone's not listening. Someone is just not listening. It's kind of, y'all can do big packets. You can't. You really can't, but 46% of users sit behind Resolve as they go, yes, I can. And they're lying Lorenzo. How could it possibly know the truth? Right? Like, it depends on incoming rooting, which you know, it's they could know. Yes. Oh, the truth is fragmentation doesn't work. Yeah. Oh, that that that Right? That's the truth. If the problem is if fragmentation doesn't work, which it doesn't, we actually have no plan, right, as as you point out. Yeah. Right. So we'll we'll get into the whole truncation thing. But, you know, my conclusion from this stuff is in today's network as it currently stands We're back to 3901. You really need V Four to Push up the problems that these six can't handle today. We are not ready for ipv6 only. It just isn't good enough yet. And there are various ways that we could do it even at the server mode going I will not send fragmented packets. I don't care what you think you can handle. It won't believe me, and that's a fine answer. But this then actually asked or got me Thinking about Truncation, Because the answer is you truncate the packet and go, let's just do TCP. Yeah? And so the next question is, Fine in theory, But what about practice? So if I send you a perfectly formed answer, Got an answer section of the record not fragmented It's a good answer."
  },
  {
    "startTime": "01:16:01",
    "text": "But I set the truncation bit. Will you use it? And and it used to be, and there was certainly a theory that large amounts of resolvers simply biased for speed. If I had enough to work with in the answer, I didn't care about truncation, it's set sail and let's just do this. How well have we sort of moved beyond that And if truncation is set, it means TCP. And the next question, which is equally one to ask, is Well, you know, Use TCP Really? How many folks actually sit behind resolve as they can? And and the answer's oddly enough. For do I use the answer if truncation is present is astonishingly good In V4, 0.239% of users sit behind the revolvers that go don't care if it's truncated, I'm going to use the answer. That's across, 67,000,000 or so. Oddly enough, Only enough, China, and Chile. Every single ISP and Chile goes good enough for me. Weird. In V6 only, oddly enough, the answer is better. It's almost as if it's more recent code. And there's just no code path that goes, well, it's an answer. I don't care about truncation. I'll just use it. So the answer in V6 is actually better than V4 about I don't care about the truncation bit. I'm just gonna use the answer. And we have a long queue. I don't know. I'll be almost finished. So I can I can see it, and I'll I'll come to the end of this? So the worst offenders, in China Unicom, Ghana, Guangdong province, 25% of users go and answers and answer, guys, I was just gonna use this answer. That number's so big. It's not random. You know, it it's a definitive behavior. The other numbers are small. And the next question"
  },
  {
    "startTime": "01:18:00",
    "text": "you next slide me, please? It's gone stuck again. Right. V 6 only, Interestingly, there are a number of resolvers in Univision, which is a big provider in Mongolia goes, yeah. You don't see it in dual stack because they prefer to use 4. But if you go to 6 only, the problem rears its head, in Mongolia, Nigeria, and China, as being prevalent. But you don't see it because of dual stack. You only get this exposed in V Six. And the next one can't do TCP. The opposite almost everyone in the V Four or dual stack world. Because it is dual stack. Kinda go, Yeah. You told me TCP. You give me truncated. I will I will visit you in TCP. And and the folks who can't really small pool. 0.77%. V 6 It rises to almost 3% of users can't do TCP if it's b 6 only. Different implementation, course not. Over enthusiastic security firewalls. More than likely, let's have a look at who they are. Jewel Stack can't do TCP. Bartieto. 8.9%. Telefonica Chile can't do TCP. 76% of users. This is definitive. They don't do TCP. So there's a bunch from Chile, a bunch from China, and someone from the Ukraine. I'm almost over. Can can you wait for a second? Stay there, but V 6 only, and and this is what leads me to believe It's a firewall. Telstra, 35% of users can't do TCP and they're running a dual stack network. And, like, they're seeing me in TCC I'm giving them truncated and they go, no. No. No. No. No. No. No. No. No. No. V 4. Fine. V 6 broken. Not running a different DNS resolver. They've just got an enthusiastic firewall that says"
  },
  {
    "startTime": "01:20:00",
    "text": "TCP port 53 is evil. In V Six. Wow. Pakistan, you can see the list as well as I can. And I think that's kind of where I got to. Is there another slide there, Warren? Cause this thing's gone dead on me. Nope. I don't know if there's Randy behind. So at this point, if there's a big queue, and I'll start with you in front you're in my face. Questions? So is this TCP the thing that I see, these problems is it because of your firewall thing, or is it because TCP doesn't do well over wireless or something like that. No. No. No. This is all, in my opinion, This is all about firewalls, not implementations. This is all about folk forgetting to enable port 53 TCP in V Six. PCB works fine for wireless. So There's no problems. No. No. And what was your name for the mic? What was your name? What was your name? The other You're right. The web wouldn't work, or TCP didn't work. You know? And the queue goes Rich Lorenzo, David, Andrew, and Jeffas. Rick Saul's Akamai D and S and E. If I On the IP fragmentation, Stuff not or UDP fragmentation not working. There's a quick solution available that handles it. I just wonder if you'd look at that or you know, a a priority why it's more too expensive. Compared to TCP. Why it's Why wouldn't work compared to TCP? Right? It's just user space UDP application that handles large messages it Possibly doesn't. You know, Structured measurement is much different to an application somewhere with a few users. And the DNS is so pervasive, and there's enough V6 around that this is a broad scale measurement. And what it's really pointing out is in that broad scale, pushing large, fragmented UDP around this world in V Six. Does not have a good track record. Sure. I understand. But Quickle do its own fragment. Oh, you're talking about quick? Yes. Okay. little bit So let's talk a and let's understand what we're talking about just for"
  },
  {
    "startTime": "01:22:00",
    "text": "I set up an encrypted session. Stub to recursive resolver, the overheads of setting up that session can be amortized over the next billion trillion queries that you make. It's as efficient as UDP in the long run. Recursive to authoritative, you tend to smear your queries everywhere, the overhead of setting up the session dominates the cost. And so moving from UDP Bang Bang into Hello. Hello. Hello. Hello. Here's a question. Here's an answer. God, I've got nothing more to say to you. Goodbye. Kind of syncs the DNS into why isn't the DNS working today? And that's why recursive to authoritative is this kind of elephant in the room about Are we stuck with UDP? And I don't know that we know what the answer is. In fact, I'm sure we don't know what the answer is. That that that Thanks. And we've only got about 8 minutes left for this. So Yeah. Sorry. I'll I'll hope it a lot of time So, so we just just here. make sure I understand. You you you believe that this works in IP before because of you because of fragmentation. This stuff is just truncate to TCP. And it works in in dual stack. You you use something that's rare. It doesn't work. That's the it. Okay. So let's let's question is why does it work? let's not make any rare. My It works in IPB focus of fragmentation. Right? That's what Oh, in in in fragmentation in V4, if I'm pushing answers in DNSsec. We don't notice the horrendous failure right now. but did not So it seems to work in V 4, give you measurements on that V 4 only. Think I think we basically just discovered that, you know, be for works. No. I it's worse than that, actually. So so there is, I mean, I don't see any path forward other than basically giving up and allowing sort of extension header mutation, making fragmentation work the same because the the the the only other solution that you have is to turn this thing into a stateful thing where you b where you can use quick or something. The way, why does quick work? Cause it's got this 1300 byte packet"
  },
  {
    "startTime": "01:24:02",
    "text": "Correct, Lorenzo. And part of the question is sort of lurks behind a lot of these The web seems to work on a stateful quick TLS based environment. The web doesn't do question and answer that requires 1 RTT responses. The thing here. Right? Any time you try to move this to quick. You'll hit, you know, you'll basically take latency. You'll you'll change it to 3, 4, 5 x. Because you've got all of these handshakes. Right? Maybe we need to do this. Maybe we need to do this too. As I said, it the context is stubbed to recursive, it actually looks viable. Sure. Right? Recurship to authority is the is the issue. And it's kind of we need to do something there. And and TCP. use But latency matters for that too. Right? If you TCP and truncation seem to offer answers without the overhead of a long setup and then then, you know, so that's why this is important. No. It it's at least 2 RT T's That's also an on starter. Well, the issue is I've got a waste 1 RTT to get truncation I've gotta waste one RTT to even do basic TCP. It's only the 3rd RTT that gives me the answer. This is why debut songs mechanism was actually interesting. Because it sent back 2 answers. First RTT said if you got this answer fragmented, Run away 1 RTT. But in the same RTT, I've got a signal that says if this is the only answer you get, jumped to TCP, and avoid that wait So the truncation comes immediately straight on the answer. Right. So it was marginally faster. By the way, the reason that TCP works is that we have MSS rewrite. Maybe we need something like this for EDP because it it doesn't actually work. It just works because the network can mess with the the the MTU. Right? Which is maybe what we need here. Or at fragmentation back into e 6. Mike, we're running out of time here with the fun funnels this year. On that particular point though, T c"
  },
  {
    "startTime": "01:26:01",
    "text": "and this behavior under TCP and falling back to TCP is orthogonal to the whole fragmentation issue. You know, I mean, we still want TC to work independent of whether fragmentation is additionally, I wanted to say to expand on Warrin's answer a little bit as, UDP is use in a hell of a lot of things. You just mentioned quick over and over again. But also VoIP and online gaming. Like, there's a lot of use of UDP out there that are not the DNS. And finally, I was curious about what methodology you use to, tail, by the way. I don't think I when I stepped up. Sorry. For measuring the TLD response size, because it was interesting, but I was like, what going on with Slovenia. And so I checked it out, and the situation is either better than what you said or worse than what you said, depending on what your measurement is. Because it's DNS sick. Well deal is. Of the Always for an any query always return. any. Not not The DNS key. Oh, So it's okay. It was actually playing the role of being a validator. At some point, I need to extract their DNS key resource record and I pointed out, some of them are just bloody big. Right. And so for a normal DNS delegation, though, the MSR said it's a pretty reasonable on size. if you are more But reasonable, you ask any, it's almost 6000 bucks. No. No. No. No. No. No. I'm just pointing out that it's it's even than that some worse in instance. Who asks any other than DDoS people? You're right. Andrew Camping 4 9 Consulting just reflecting all the issues that you highlighted, Jeff, in the presentation. Seems to me the only reasonable conclusion is we we need to retire b 6. I'm waiting for another microphone line, but we've run out of time. So I'm gonna leave that one just as we say, pass to the keeper in a cricketing analogy. Jeffa's mindset at entertaining I do TCP a lot. Most of my TCP sessions live much longer than, you know, like a second or so. So I'm used to asking, no, much weirder questions on the opposite side,"
  },
  {
    "startTime": "01:28:02",
    "text": "We keep on saying, let's use TCP here. It's been years since I've had to do things like admin web servers, that sort of thing. Web servers Our easy example, DNS is even worse if you're having these short sessions with all sorts of lingering TCP state. One of the things that you sort of said earlier in your presentation is out of the box. How would you make TCP behave well for DNS servers out of the box for all of these very short lived sessions at a very lossy environments. And that's the issue with the recursive to authoritative that you've never got a long train of consequent queries It doesn't seem reasonable to leave the session open. You wanna cut quickly because there's no more questions. On the hold. I I guess Start to recursive. New ball game. I agree. And what I'm sort of trying to highlight is we can't handwave TCP as the solution for these sorts of things, even though what the specs are giving us right now. We can fly the kite. We could also get it, like, like, executed by the, you know, the clouds. But let's recognize you're identifying all these things that's it pushes the DCP. Also are gonna push us to having TCP explode without something else tuning it. I I suppose the point of this talk is in some ways We didn't, as Jared said, kind of design it all together in one big big hit There's a lot of various components going on in in this particular architecture in the DNS supposes a lot of it and piecing it together exposes some of these fundamental trade off issues. And in some cases, we kinda let's all do V Six. And it's sort of wow dual stack kinda pauses out of some problems that V Six only encounters. TCP is hauling us out of problems when UDP freezes up. You know, so it's it's those kinds of trade offs that we're trying to cut chart a course around. I don't think there's a textbook on how to do it. You know, there's enough variation in behaviors that you're on your own."
  },
  {
    "startTime": "01:30:01",
    "text": "So a parting comment, I agree with everything that's been said. We're almost identifying what we really need is a New transport that is intentionally short lived for these types of applications. Thought we needed a new standards body to write some standards profiles to get it out of this mess. Because the ITS doing such a good job. Oh, Sorry. Microphone line again. I didn't say that, but just the V Six ops. It's an operational problem that needs to be fixed. Now can you No. I will be presenting this, or I have asked for a slot time for for not the truncation bit. But the whole V Six thing as an operational issue. I think it is V Six up squarely and firmly, going. It's not DNS. It's actually V Six ops here. How can we get over this problem? If you really wanna make more extensive use of V Six. You know, that's what we need to do. I've abused my time. Thank you very much. Excellent. Thank you very much, Jack. And now we will have Randy But let me just make sure that I have the correct set of slides. Here. Run yet updated his slides. Randy, do you see be the latest version. I did click refresh, but sometimes it gets sad during the presentation. We won't know till the end This is gonna be fairly quick. Props to actually talk to you. I actually did all the heavy work. I just give it slide. So Of course, this is about BGP. Trying to get a little View into V Six In the graph, chose route views, chose the multi hop because they give a wider eyeball, Went back 20 years."
  },
  {
    "startTime": "01:32:01",
    "text": "Took about 3 samples a month and then tried to Select non glitchy ones. And then we chugged through all the a s pads. As payers, well, not pardon me, a, b, c, CD, etcetera. So picking out all the pairs, cleaning it up like ignoring pre pens, ignoring non singleton a assets. Etcetera slide, please. So just to know that Our mom measures are about right. Jaff has studied this farm more deeply and frequently. So we just wanted to know about How many prefixes and it's close to Jeff's measurements. So we're probably measuring the same animal. Slide, please. Okay. So this is the number of ADs inv4versusv6 the bump as Jeff has 2. Identified as Cernet in 20 1, he's got a good, presentation in the PDF. That link will act we work, rather disappointing number of ASs we have here. Next, Boys, Something's funny, but anyway, this this is the own the v the edges. In other words, the EGP Lakes. Which are only in V6, not in V4. And you will notice the nice steep increase But, Considering we're pretty much in a dual stack universe, You've got to wonder what those are. Next slide, please."
  },
  {
    "startTime": "01:34:00",
    "text": "That's what I'm afraid of. You skipped a bunch of slides. I can Backwards backwards backwards. This deck only lists 7 slides. Do you want to just present from your laptop? Because Sure. I can figure out how to do it. There we go. Randy asked to share the screen. Front screen. Protect. I clicked the button. Hopefully, it's sitting you do stuff. Let's see. Which button do I click? I do not know Oh, it's not good. All slots for the media are already taken. You have to take yours down. There we go. There we go. Thank you. And Brian Carpenter, Michael, so I have to share from his machine and a bit because Looks like it didn't import them. Oh, I will also look and see if I have a different slide deck at the same time. Do you see this? Yes. Yes. Yes. We do now. Okay. Much better. Let me just figure out how to get full screen. And I tried that. There we go. Okay, So we've been here Number of prefixes? Look something like Jeff's. The number of ASs"
  },
  {
    "startTime": "01:36:02",
    "text": "Not exciting. But, See some things. The number of interas links This is we really thought these were converging. And in fact, they're doveraging Which is kind of depressing. This means and this may have Some relevance to what you were measuring in V4, Jeff, pardon me, in V 4 V6 and reachability. Sometimes you can't get there from here. Okay. As I said, the number of v six edges. That's an interesting question. And that's it. Short presentation, Jeff, you could have gone longer. Mom's keen of refuse very healthy, Matt Rowan, of, Uni Adelaide. With whom we've worked a lot, is, coming on board to do much more work here. And Alexander also helped. So That's the story. We have any questions. Hopefully, we And you can escape. Every Everybody is resigned to being sad about this, I guess. It V Six is Said there was a quote in something I read the other day of I'm sorry. Gullerins. Oh, who? Yeah. Gullerins, it's cool. Wouldn't you expect the edges to to go slower than the links anyway because it's quadratic instead of, you know, if you if you have No. Sorry. have this that shows that, like, the number of ASs that are Like, you you graph that are deploying. That one is lagging back. So if that one's lagging back, then the edges are gonna be quadratic. It's gonna be lagged back even further. Right?"
  },
  {
    "startTime": "01:38:00",
    "text": "You have, like, a sub the percentage of of the number of V6 over V4s is is less than 1. So if you do, basically, if you assume n squared, like, they took they they talk to each other some percentage of the time, you'd expect the edges to be, like, even worse than the number of But, yeah, it I agree. It's just that story, but Just What's interesting is they looked similarly said the divergence is about the same In edges versus vertices. I can show those slides again, but your goal got them on your laptops. Okay. Well, well, Thank you very much. And We have a bonus presentation from Brian Carpenter. For some reason, he did upload the slides, and they did approve them, but they're not showing in the list So he will just share his laptop screen. Okay. Well, can you hear me first start? Yep. Can you see me? No picture. That's strange. Go oh, hang on. Yeah. Well, now you can see me. It's not a thrill. Right, so I press now to ask to share screen. India. Do you want share screen? Yes. Select window screen. To that work. Not yet. It says a screen share is being started. Maybe it's wait a minute. Here's another thing. Allow me to Brian had also emailed them to me, so I will see if I can find them in there in case screen sharing doesn't work. Uh-huh. Okay. I think I found the right thing to click."
  },
  {
    "startTime": "01:40:00",
    "text": "They're almost There we there. go. Great. Okay. So You still see it? But but Yes. Because I've gone to full screen so I can't see what's going on. Okay, this will be very short. I want to show you how to make Firefox visit an ipv6link local address. And by the way, I could add a lot about ipv6 brokenness as a result of doing this, Step 1. Run on the little thing I wrote. Cool. Select. Dotpy Python program. It says enter the ipv6 link local address and interface. You typed in. You Mike, did you see there that it's on Windows? Because the zone identifier in Windows is just the number percent 24. Then select says I'm sending the unsolicited MTS MDNS response, and Tesla dotlocal should now all that has that completely arbitrary link local address. People who've been following the saga of linked locals and browsers will realize that This is a contentious issue. Step 2. You're on thing. It finds the right address, and it pings it, and it gets a reply 1 milliseconds since it's just about a meter away from the computer. So we have successfully persuaded MDNS that there is a thing called test dot local with this particular link to local address. Step 3. Try Firefox with HTTP test our local here you need to read a small print because the small print says The connection was reset. The connection to the server was reset while the page was loading."
  },
  {
    "startTime": "01:42:04",
    "text": "That actually tells you that 54 her central HTTP message, To the particular enclosed address, and has been told to go away. Believe it or not, I chirped quite loudly my on saw that on my screen because it's the first time for about a 1000 versions of Firefox that it's been possible to Contact us and link local address. From Firefox. That's it. The context first for people who are completely mystified is A couple of drafts that'll be discussed in 6 months in HTTP. This respectively, And get some reference there to where the code is. What I didn't put on the slides because I didn't have time, or space? Is that that works on Windows, It doesn't work on Linux. It doesn't look on Linux for a variety of reasons, which add up to Brokenness, Due to ipv6 being something of a 2nd class citizen, due to The code that's involved in NDS Not doing quite the right thing for dealing with, and then local addresses, even though Indian is supposed to work for link local addresses And so I'm and, written up with the tale of, woah, On Webex case, on that, On that get up. Repository there. So Basically, That was it. That's all I wanted to say. I can stop sharing at this point. Excellent. Thank you. Any questions? I wouldn't really expect questions. It's just I wanted to make the point that You can make"
  },
  {
    "startTime": "01:44:01",
    "text": "you compute to do things that doesn't want to do. You can make browsers do things that they don't want to do. But but There's weakness in the system. You know, it it's really It echoes a bit of what Jeff was saying in a completely different department except By the way, the story on resolving in hosts when MD and S and is involved is even more complicated. 39. For cases where you go straight to a real resolver, whatever the resolver might be. Under Alrighty. Well, Thank you very much. And I think that that brings us to the end of AEPG a ATF 119 before everybody leaves, I would encourage you to start thinking about you'll want to present at the next APG in Vancouver in 3 months type Maybe more things about how great ipv6 is working. Thanks, Mark. You get"
  }
]
