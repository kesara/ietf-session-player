[
  {
    "startTime": "00:00:15",
    "text": "Alright. Well, it's it's time, so we shall begin This is Machine Learning for audio coming. The session is being recorded. As normal, BCP 79. Applies So that everyone here should be aware of. I'm familiar with with the IETF process requirements, particularly that the event, if you're aware of any IETF contributions covered by patents or patent under control by you or your sponsor, you must disclose that fact. We're not participating in discussion. As participant, you should be aware that there will be recordings, audio, video, written on the maybe be public and that personal information will be covered according to the IPF privacy policy. Additionally, the the RC, 715470776. IETF guidelines for conduct and anti harassment. Seniors apply start with you conduct themselves in a professional collaborate manner everyone in the room and should be, the login via the mediocre client in order to get system. If you're not able to get in, there are QR codes, on screen over there as well as the Blue sheet and if you're in room, body on video turned off. Issues of everything that's getting in. Use the yorkers We have already established note ticker sent to the So, Thank you. Thank you, William. Hey, Jonathan."
  },
  {
    "startTime": "00:02:03",
    "text": "I don't Oh, okay. The, So here's the agenda as we've sent out. Do we have any agenda bashing? The agenda can always be Alright. Well, it appears there is no agenda bashing. And so Jean Marc, can you come on, and I'll change the slides over. One moment. Alright, Sean. Mark, you should have control over the slides. Okay. You're still hearing me? Yes. Okay. Great. So I'm Jamak. I'll be presenting an update of the, the redundancy for the opus codec draft? This is a bit of a recap of last time because we sort of ran out of time. So essentially what we're trying to achieve here is to make Opus robust to long bursts of packet loss. And the way we're doing it is through highly compressed redundant audio. We use, machine learning to achieve that. And for the purpose of testing, we've been working with, you know, co, coding 1 second worth of redundancy in every 20 millisecond packet which comes out to, redundancy factor of 50x. And you can see at the bottom how this thing is working overall. So in blue, we have the existing part of OPUS right now. You know, the encoder, the decoder, and in red is what we're adding, So On the encoder side, we're extracting features, So, essentially these are features that are similar to what an ASR would compute."
  },
  {
    "startTime": "00:04:00",
    "text": "And then we encode those features mux them with the main open content and on the decoder side, depending on whether there was a loss or not, we either decode regular OPUS packet or we send the, the bits to a feature decoder. We get our features back and those features are used to synthesize speech using a neural vocoder. So kind of similar to what you would have with the TTS system is just that your, doing that from these acoustic features. The way it works in practice is for efficiency purposes, we have an encoder that works forward in time. So every 20 millisecond the encoder will compute both an initial state that I'll explain and, sort of this these, latent vectors And these vectors actually represent 40 millisecond chunks despite the fact that they're computed every 20, which means they're re they're in themselves redundant, and we don't need to send all of them to be able to cover all all of the audio. And what gets sent to the other side after you get a certain vector and the state from the encoder you include The latest state in the packet that's gonna be send, and every other latent vectors that the DNN produces And so there's an example for 3 consecutive packets that would be sent. And on each of these packets, the decoder If it needs to decode the features, it will work backwards in time. So it will get this initial state, which gives it the information about the latest audio and then it can work"
  },
  {
    "startTime": "00:06:02",
    "text": "backwards through time to figure out all the contents of the packets. And the idea there is both to ensure that the encoder doesn't need to encode every packet fifty times because that would be very expensive computationally. And On the decoder side, it's best to decode backwards in time because the most the frames that is the most likely to be used are the most recent ones. So the ones one second in the past or much less likely to be used. So that's the kind of principle of how all of this works. Now in terms of what's happening on the wire, this is the current proposed format. For the extension. So we propose to use extension code 32, which means it's a extension that is meant to include more than just one bite, obviously. Right now, we use we temporarily use extension ID 126 which is reserved for experimental use. The, So in terms of what the packet itself contains, First, there's an offset which is encoded in 5 bits, the offset is meant to say how we are aligned between the features and what we're going to be playing back because in some cases, you may end up with different alignments and you need the synthesizer to know exactly, where you are. Because we're encoding in chunks of 20 milliseconds, but there could be frames of, know, 2 a half milliseconds used for Kelts, for example, So this sort of keeps track of where we are. So 5 bits there. We have the quantizer select using 4 bits. The idea there is we can change the bit rates depending on how many which loss we have with"
  },
  {
    "startTime": "00:08:04",
    "text": "kind of bit rate we have and so on. So there's 4 bits that signal which quantizer we're using, And in addition to that, we have 3 bits for quantizer slope, and by quantizer's slope, what we mean is that we're able to actually change the quantizer within the redundancy, so that the older frames are encoded at a lower bitrate. And so that slope makes us control how quickly we reduce the quality, when we're going back in time. A slope of 0 would mean everything of the same quality. But a non zero slope means that the older packets are actually, encoded with, lower quality. And way of saving bits without affecting the overall quality too much. So after that we have this initial state that I described, is now entropy coded in the latest implementation. Previously, it was a fixed bit rate that allowed us to And, There's the latent vectors also that are then also entropy coated and, and their entropy coded until the end and the end is when we have fewer than 8 bits remaining in the packet. So that's, that way allows us to have, for example, to decide we're gonna use like a fixed number of bytes for the redundancy. And we just don't signal the end. The decoder knows, okay, I've got less fewer than 8 bits than I stopped decoding there. I think I see there's raised and, no. Yeah. Sorry. Gonna be lazy and not go to the, floor mic, but, there's a individual comment. There any coupling between the Opus, frame sizes and these redundancy, back at frame sizes, or they're totally independent there's no bearing on how Opus is operating."
  },
  {
    "startTime": "00:10:04",
    "text": "This is completely independent in that respect for the rest of Opus, the only link is this offset field, which, tells us exactly how we're aligned in time, But otherwise, the dread format essentially each of these latent vectors, they correspond to 40 milliseconds. So we can encode in chunks in increments of 40 milliseconds redundancy. Does that answer your question? Yep. There's no coupling. In terms of I mean, there is coupling in terms of, you know, the implementation and the order because it needs to you know, switch back and forth between the two like that is not that is not simple, but in terms of the bits themselves, they're not coupled at all. So you can operate a opus encoder or decoder at any frame size, and you can operate the redundancy at any redundant frame size independently. Well, the redundant frame size is always the same. It is multiples. It is like 40 milliseconds and you just include However, many chunks of 40 you want. But that is completely independent. You you could essentially, like take the redundancy for one stream and put it in a different stream that's encoded with a different mode or something And as long as you adjust the offsets, everything would work. So Let's see Okay. So Obviously, one question that comes up when we do machine learning is how we're gonna standardize all this. And we're trying to have, like, this balancing act where we want to make sure"
  },
  {
    "startTime": "00:12:03",
    "text": "the first criterion is all implementations need to be interoperable. You can't have one use one format or 1 DNN and the other one use and completely different and they can't talk to each other. But the idea is also considering that to still leave as much flexibility as possible. And, you know, emphasis on as possible because we can't leave everything flexible, just like and year. Codec, like, you need to specify something. So what we are proposing here is to have essentially a normative specification for the part that converts bits into features. So it was, in my original figure, it was the 1st block of the decoder. So in that part, we can't really avoid standardizing. Essentially would mean that all of the the decoder weights so the DNN that does that would need to be frozen and specified. There's questions on how we're gonna publish that. That will need to resolve. And, then we'll also need the definition of these acoustic features. And Once we have that, we should have something that is perfectly interoperable. And the encoder would be left unspecified. It just needs to be able to talk to that decoder. And similarly, the vocoder, which is actually the most costly part of this entire thing, that is also left unspecified. It just needs to be able to take the features that we standardize and outputs actual speech that corresponds to that. So now a, quick update on the actual implementation of all this."
  },
  {
    "startTime": "00:14:04",
    "text": "So we recently were able to improve the quality from using a newer vocoder which also turns out to reduce the complexity. So our complexity now got reduced from about 10%, CPU for a high loss case that we were, where we were, time at the last meeting down to about 3% CPU right now. And Essentially, that is for both encoding and decoding. Using wideband silk. If we don't do any of their deep redundancy, we're about a 2%, CPU. These are absolute. So complexity got down by, a good amount. And there's also now no costs for the no loss conditions. So if you're not losing any packets, you're not paying for any of this. There was also an issue with the sides of the models themselves. That was increasing the binary size. We got that down from about 17 megabytes last time down to about 4 megabytes now. Still trying to shrink that a bit, but know, I think it's, much better shape now. And, there's a link stool for the, where the code is right now. It's the Opus NG branch and get repo And now there's still a few open questions, we can debate. 1 is should there be a maximum amount of redundancy allowed? So far, we've been kind of testing What? You hold on just one sec? Tim, was your question on the last slide, or are you okay to go on to open questions? If I can match, I'll make my microphone. Yeah, it was on the last slide. There you go. Yeah, so to Terry, very"
  },
  {
    "startTime": "00:16:01",
    "text": "that 4 megabytes, what fraction of that is the bits to features decoder? Oh, yes. That's important question, of the 4 megabytes right now, the decoder that would need to be standardized is about it's about 1 megabytes. Again, ideally that should be a bit smaller. We're not gonna do miracle, but, still trying to shrink that a bit. Okay. The glass is mostly, you know, the vocoder and the encoder. So yeah, in terms of open questions, So we've been testing and discussing, you know, like redundancy of about 1 second It seems to be, a reasonable amount, but you know, technically, we can actually do up to that maybe 10 minutes, which is probably not that useful for real time conversation, but the question is, like, Do we We we We suggest making it essentially open, like, you know, 10 minutes probably won't fit your MTU, but I don't see a reason to say, like, this is the maximum amount permitted in the worst case, the decoder can just ignore what's in there. But you know, it's out there. And in terms of bit rates Right now, we support if you do one second of redundancy, we support bit rates, that would correspond to about, 10 to 100 kilobits per second worth of redundancy. Obviously, if you have more redundancy with be higher. That's basically equivalent to about, a range of about like 200 bits per second to 2 kilobits per second if you, if you look at it in terms of the content of 1 package in itself."
  },
  {
    "startTime": "00:18:02",
    "text": "Yes. Duff And that was my last slide, by the way. Yeah. Jonathan Lennox. This is maybe a somewhat more general question, but I just occurred while I was sitting here, so I thought I'd ask it. I'm concerned you know, as somebody who writes, you know, SFUs, what happens if I've spliced two streams, and now I get the history from is there a way for me to conveniently splice the Redundancy history. From, you know, from one packet, I'll put it back into another if I add them splicing audio streams together, or is that gonna be awful. To parse this stuff at a middle box. Oh, you would so What you're saying is you have 2 conversations that 22 different speakers, and I switch which speaker I'm sending. And I'm so I mean, for whatever reason, I'm putting them into the I'm merging them to a single stream. So maybe you didn't think about this. Yeah, that's quite of an interesting use case. I had not thought about it at all. There may be something doable, but I would need to think of what all the implications are. I definitely think it's a it's a potentially useful one, Okay. And then the other question, just follow-up on on most question. You said that the frame sizes don't matter. So if you're encoding a half milliseconds. Using this. You get a different the pre the pre the 20 the 40 milliseconds before the last 200 a half milliseconds each time or Are there how does that work? You're silent, all of a sudden. Keep on ringing. Jean Marc, if you're trying to Sorry. Yeah. Hello? Okay. Are you hearing me now? Yeah. I never hear you now. Sorry. Well, my phone tried to ring in my headset. Your headset. It's awful."
  },
  {
    "startTime": "00:20:00",
    "text": "Now I was saying that so if you're encoding opposite 2 a half milliseconds, Yep. And does the deeper tendency do the the 40 milliseconds before the last two and a half milliseconds each time. So does that mean it's actually encoding a new Deepgrams every 2 a half milliseconds, but switching which ones are there, or how does that work? So if you were to do 2 a half milliseconds, what it would do is it would still work on these 20 millisecond chunks. So, you know, one for one frame, it would say, okay, I'm computing the, my new initial, my new vectors and all that. And then the next frame, it's like, yeah, I don't have 20 milliseconds more. So it just reuses the last thing, The encoder will still send entire redundancy in each two and a half millisecond packet, or you could you could always the not too, but, it would still do that, but it would not reencode every 2 and a half. It would still you know, run the DNN every 20 milliseconds. Okay. So there'd be sort of like there'd be gap kind of, but Yeah. And that's a reason for this offset thing. Where you know, the next and a half millisecond the offset would shift by 2a half because it's still the same redundancy Okay. Okay. That makes sense. And, yes, you would be missing maybe one frame there, but the there's, like, this is integrated with the PLC, so it would still, you know, it would still work. It's just like, one of the features would be kind of extrapolated Okay. Cool. Thank you. So so just for the notes, on on Jonathan's first point Jean Marc, you wanna you're gonna update, you can do some update about how splicing may work. In the draft? Splicing inspirations. Yeah. I mean, the thing I will first think about whether it can work at all and I'll I'll try to update the the mailing list. It would be cool if we get that to work. I just don't know how to. But maybe there's a way. So I think it's an it's an important consideration because I think people are ready naively do open splicing"
  },
  {
    "startTime": "00:22:02",
    "text": "there are already issues with just splicing opus at arbitrary boundaries. And adding this deeper redundancy is compounding those issues. So if people do it naively, they don't understand the implications you know, they're gonna get art audio artifacts So it would be good to have a a section about splicing to address both the opus issue and now there's deeper tendency issue as well. Yes. Well, I mean, like, to to to be clear, like, if you do it sort of in the dumb way of just forwarding the packet and not asking yourself what's in there. It's going to work. It's just that you know, you're sending the packet from talker a. So if talker if you lost packets from Tucker B, they're gonna be forever lost. They're not gonna be recovered. And that that would work. No artifacts, nothing wrong. The the question I was answering too that I don't know if we can get it to work is can we actually somehow merge the redundancy and get for both talkers a and b. In that case. And that's the one I need to things through. But Nothing's gonna break. It's just like it may not work nearly as well as you would like Are you okay with adding a splice and consideration section to the draft? And address that? Yeah. Yeah. Yeah. You get to dress up at this Okay? Yep. Tim, Yeah. Oops. Mark Harris says the security considerations would highlight the danger of potentially including earlier audio, those attended to be cut out, perhaps confidential information that could be decoded from the dread. Especially if it's 10 Sorry. Say it again? Basically, you know, security considerations should mention that"
  },
  {
    "startTime": "00:24:02",
    "text": "you know, maybe you hadn't intended to have the last 10 minutes of audio being sent to this person that was they were saying something confidential, and they didn't realize that they're audio would be forwarded. So I mean, if If it's in the redundancy, it was already included in the Opus Bank. In the main office. In in the main office packet, but maybe those open packets weren't forwarded to somebody. Maybe this they joined the call. Oh, yeah. Okay. You just can see, like, a joint a late joiner and the speakers didn't realize that the late joiner would have gotten the prejoin messages. Fair enough. Yes. We have Tim again. Yeah, not not to design at the mic or something here, but for your quantizer slope, that's gonna decrease things down to the minimum quantizer. Yep. Yep. Did you think it might be useful to be able to specify a floor that's higher than the minimum. So it drops down to that quality and that doesn't go any lower. I'm open to that. It would eat a few more bits, but it's not fatal either. I would I would welcome feedback on that one, essentially. I don't have, like, strong opinion on the trade off there between, bitrate control the number of bits we transmit. Right. Yeah. I'm not sure I have strong opinions either, but just looking at that. 100 kilobits per second in 10 minutes. It's like whatever slope you put in there, you're gonna get down to almost nothing best. Well, as I said, like the 10 minutes thing is not I'm not sure I see a use case right now. I'm just not sure I want to this allow it just for fun. Right. Right. My my point is, like, there's there's a"
  },
  {
    "startTime": "00:26:01",
    "text": "period of time somewhere in between 1 second 10 minutes where you're always gonna be in the bid rate to the minimum, you might not actually want Yeah. Fair enough. Alright. Thank you. Thanks. Thanks. K. Alright. That that looks like it covers the questions. The only I just missed John, I just missed your question. There's something about Security. That it was, Did it have If you're encoded in a frame size smaller than the for the figure out the box size, then There's So it might be a small gap, but, Okay. So, hi. My name is Jan Buter. I want to give a brief update on speech coding enhancement for OPUS. Yeah. Like this. So are basically 2 aspects to the thing. One is the algorithm development, that's going on. So to develop low complexity speech coding enhancement methods. 1st, site on, without site for info later with and also fully optimize this and integrate this into the it's it's intrally focused. And the second aspect is to standardized the whole thing. And the idea is now not to standardize the specific methods, but rather to standardize requirements that an enhancement message"
  },
  {
    "startTime": "00:28:00",
    "text": "as to satisfy in order to be able to integrated into office. So this way, we have, maintain the possibility of, improving the models in the future. And they asked 3 different Regions for requirements, first one regards the quality of the enhancement method itself, The second one is about, integrating it into the office decoder so that switching still works and nothing breaks. And the third one is interoperability. And I want to focus on quality today. Next slide, please. So first an update about algorithm development. Last time I talked about, lays, the linear adaptive coding enhancer to was the first attempt at improving Open Silk. And now we have a new model, no lace, that's father, you, and it has slightly higher complexity, but also, gives higher qualities, which you can see, we are now, almost very, very close to clean speech quality at 9 kilobits per second. Complexity is around 600 megaflops, which is still low for smartphones off. That that laptop devices. And if you're interested, there is paper, and there is also a demo page where you can listen to samples. Next slide. So now we have basically two methods. That means we can start looking into, quality evaluation. So we actually have something to test and to look at. And the general goal is to make sure that an enhancement method does not degrade. Opposilk And the gold standard for evaluating this is to do a subjective listening test. So 1, like, the one way you have seen the results just now. But this is, 1st of all, very costly. And second of all, you can only test limited a limited amount of data. And the alternative to that is to use objective metrics,"
  },
  {
    "startTime": "00:30:00",
    "text": "However, they also have a drawback mean, they are cheap, but, none of them is perfect and they also tend to not age well. So we always find new ways to degrade a signal and yesterday's, methods often fail to assess quality accurately. Yeah. So next slide, please. So what I did is I tested 4 different different metrics, one that's is past very old and well known, method for evaluating perceptual quality of speech then walk queue, which is a newer version that was design to measure, the quality of, neuro speech collodex because Ask didn't work. 3rd one is modified version of the opus compare tool that's already standardized, And that's uses a very simple psycho acoustic model to give a distortion metric And the fourth one is a very new method called NOMET we'll just have just talking metric based on Neural embedding. So there's a big neural net behind this. Next slide, please. So the first thing I did is if we had some, listening test results, and I ran all these metrics with same models on the same Items, And, just to see how they compare. So You cannot compare values directly, but you can look at the quality ordering of conditions. And this is what you see down here. So Buff is the ordering of the listening test, and then you can see Norman get 2 of them wrong, modified opus compare and walk q get or 3 of them wrong. And PESQ gets even more of them wrong. But all in all, It's not perfect, but it's it's reasonable. And, also a bit surprising that this op office compare perform so well."
  },
  {
    "startTime": "00:32:02",
    "text": "Okay. So that was the first I wanted to show and this Next slide, please. And the second second thing is, So we also want to make sure that it actually detects the quotation and So we want to be able to distinguish good from bad enhancement models And what I did to try this out was to use an almost untrained version of lace and no lace. As an example of bad models and they said no less. So when they are untrained, they are somewhat similar to the identity function. So it's not like they're completely ruined. The signal, you can still understand this. But it is a noticeable degradation. So no complete catastrophe, but noticeable degradation. Next slide, please. And, so here are basically the results, These are now all distortion metrics. That means lower is Echer, And you can see that all four methods have, so to say, a certain margin, that separates good from bad models. The results are I'm not Completely satisfying for all metrics you can see that for PESK and walk queue, Actually, the good models get rated down a bit not much, but a bit, for higher bit rates. And for modified OPUS compare and Nomit, the bad models, are actually seen as an improvement at very lowbitrates. So you can use tight thresholds, like over here, but then you would request, so to say that you So if you would say We want to have, a certain amount of quality improvement, modified opus compare. You can separate them But then you have to say The actual method, has to improve. You cannot say it was stated in the beginning that should not compare, it should not degrade."
  },
  {
    "startTime": "00:34:02",
    "text": "But all in all, it it seems feasible to do this. And the next slide, please. So to sum this up, so of all of these metrics, seen seen capable of separating good from bad models. And so it has based on metric dependent thresholds. Should likely catch issues with enhancement models. And depending on the metric, One might, have to allow a little bit of degradation at higher bitrates. Also regarding the performance of the metrics themselves or normal seems to perform s all in all, but it is it would be quite difficult to standardize. So it's a neural network with 95000000 parameters. And WAPQ and modified OPIS compare. They are very simple methods, modified opus compare even more than WAPQ. So, that that would be quite easy and straightforward to standardize and Ask is already standardized, but it performs worse. So so These are the findings. And in terms of the next five is Thank you. And in terms of of of Next step. So the algorithm development itself, what's currently, ongoing is to really integrate them into Focus, on the Opus Energy branch next generation. Do some size and complexity optimization and also to, investigate noisy speech performance that's already ongoing, and I can already share that these models that are just trained on clean speech, perform, pretty well also on noisy speech. Another thing are looking at is to add outputs extension so for instance, at 12 kilobits, on on decode or invent full band, speech or turn white band speech into full band speech and also to add site information."
  },
  {
    "startTime": "00:36:02",
    "text": "And for standardization, the next steps would be to assemble some Test Spectors or test, like, the one I just, outlined. And to assemble them from open datasets. So the listening test data set was proprietary. 10 spell out some requirements for scheme speech and to noisy speech. Test. And the question is a little bit how strict or Union ought to be and whether we should use one metric or allow multiple metrics or even require multiple metrics maybe even the lower listening test on a subset it's it's still so if anyone has an opinion about this be happy to hear this. Yeah. And that's it then already. On my side. Next slot. Says if if if if Thank you. Any questions about This work? Yep. Go ahead, Tim. Yeah. So Back on the slide with the table of thresholds. So so what that's basically saying is that some of these metrics disagree about Whether These, your your models are actually an improvement at high bitrates? Like, do we know do we know if they're correct or, like, which ones are correct? Of I mean, we don't have ground truth listening test, the only thing I did was to listen to outlier signals, and I did not hear any degradation. So, I tend to believe that the metrics are incorrect, especially past has this"
  },
  {
    "startTime": "00:38:02",
    "text": "kind of nervous breakdown where suddenly it would rate an item way down with a score of, I don't know, 3 or so. And he don't hear. A difference. So that's It's kind of the problem of, the method, aging not well. So I mean, the best thing would of course be to to to also have a listening test at a certain point at these higher bitrates, maybe do a martial art test But at the moment, I'm I'm led to believe that it's, shortcoming of the metric. So in in in the video space, some people have created metrics, which are basically composites of other metrics. Mhmm. Do you see anything like that being useful here or or some of these already composites of something else. No. They are pretty standalone. They're To avoid this divergence problem to avoid this where, you know, you have conflicting signals from different metrics, metrics, if there's a composite metric that takes into account multiple of them. And has rules for how to deal with divergence I mean, if Tweed Banks is we make this mix bitrate, dependent, then it's easy to achieve. From Yeah. Yeah. This is something to look into. It's actually Good suggestion. Thanks. Yeah. My my only other comment is, number. You know, just just to echo feedback I gave you before the meeting. That if you if you look on the previous slide, I believe it was. Oh, maybe it's not in this version, but the this this signal you're comparing to is just the high passed version that you know, that that already has the high pass filter that Opus uses. Internally. Right? Yes. Yeah."
  },
  {
    "startTime": "00:40:02",
    "text": "But that's that's not the only change that the opus encoder makes to the original signal. Right. It also does like bandwidth expansion of the LPCs to try to to enhance performance. And that's not reflected in what you're trying to compare to here. So I think There you'll need to take some care of what what the actual correct reference signal should be. Right? I can easily rerun this with just the clean input. Signal. It's hard to say. I mean, the reason for taking this signal is basically that you get face shifts, from the high pass filter. Right. So some of the metrics might react badly to that. But I will rerun this and, I can check the results. So I will look into it. Yeah. Well, I mean, I'm not sure that that just going back to the the original clean signal is necessarily gonna make the most sense. Right? Like, there's two questions. Right? Like, one one is, are you degrading you know, the original the original input to the to the encoder more than opus in Cove was already degrading it. Maybe going back to the original, if it would answer that question. But the other question is is like if if opus the opus encoder is doing some enhancements to the speech before it encodes it. Are these methods that then get closer to the that original input signal, like undoing those enhancements. Right. Right. When would they be penalized if they didn't undo them? Yes. I actually have to check. So, from what I remember, it was really just the high pass filter, that was applied but I will have to check this carefully to see Whether there's any format enhancements going into the reference. That's"
  },
  {
    "startTime": "00:42:01",
    "text": "That shouldn't be. Wait. I mean, my my question is, like, should there Right. Like, if if if the opus encoder is making additional changes to the signal, at you know, your your evaluation criteria for are you compliant is is you know, are you making an improvement to over what the opus encoder did? Are you then gonna be penalized if you don't undo the things that the opus encoder did? Right. Basically, I I'm trying trying to ensure that we still have some flexibility and encoder design. I think I can answer some of this here. Fair. My okay. Sorry. Sean McValli, I can answer, I think some of this. So, actually, as part of what the encoder does, so the high you know, as, yann mentioned, like, the high pass filter will the phase up. So we tried to remove it from the equation. Everything else should be in there. The form and stuff actually were no longer doing in the latest version, but it used to be done, but it was all about undoing some Like, it will the encoder was doing then with expansion to undo some of the other stuff. It was doing later on. So we want to include all of that. In the comparison. I really think the only exception there should be the the high pass filter just because it you know, it messes with the phase, but it's, otherwise, irrelevant. But I don't think there's an issue there. Okay. Thank you. Great? Thank you. I was going to also answer"
  },
  {
    "startTime": "00:44:02",
    "text": "if there's still time to, most question about the metrics. Where is it? So essentially so video metrics are hard audio metrics, I think, are even harder. And I don't want I don't think we should get into designing new metrics because that's an insanely hard problem. So I think that it's, kind of the idea of what Jan was doing here, which is to just look at what's already out there and pick something that seems reasonable. Soon, like, expects Yeah. Alright, Tim. Do you wanna let go? Sure. Assume you guys can still hear me okay? Yep. You sound good. Alright. It's on Tim Terrybury. This is discussing the extension draft. So just just to refresh people's memory of what you know, in case you don't have RFC 6716 that grams memorized. This is sort of the structure of an opus packet. There are up 4 different framing formats defined in the RFC, the The first three are just for one or two frames with the same or different sizes, But the third one's cleverly named code 3, in includes this extra frame count bite. So you can have an an arbitrary number of frames up to a 20 milliseconds. And in that frame count, byte is also this padding bit, which allows you to add additional padding. So in order to add extensions, we need to use a code 3 packet sounds enable the padding bit and then encode a padding length"
  },
  {
    "startTime": "00:46:02",
    "text": "and the length is just a a simple encoding that you know, represents up to 254 bytes of padding with the 1 byte length and then Adds additional bytes. If you need to have more padding than that. But basically that means in order to turn extensions on, we we spend 2 bytes. 1 for the frame count byte. To use a code 3 packet and 1 for the padding length variable. Then we can add extensions at the end of that packet. Alright? So the actual format of the extensions is is illustrated here. And so they you start with an a a one byte that has an ID. And this l flag, which specifies how the the length is encoded in that changes depending on what you're your ID is so for ID 0, l equals 0 just means treat the entire rest of the packet as padding, than the original definition. So it must be 0. Decoder must ignore it. If l equals 1, then you just get 1 byte of padding. For IDs 1 to 32, l equals 0 means you have no payload, and l equals 1 means you have 1 by the payload. And then ID is 32 to 127. You encode a a length for l equals 1. It's the rest of the packet for l equals 0. So after that, that length, you have the the payload And because, in a code 3 packet, you can have multiple opus frames packed together. We have these separators that separate the extensions for each of the opus frames. And those are encoded using as an extension with ID equals 1."
  },
  {
    "startTime": "00:48:04",
    "text": "Alright. Any questions on that? Great. So let's go over the draft status this is adopted as a working group draft in the last meetings. We published that. And then a a 1 with updates that were discussed in that meeting and and on the list. And so, hopefully, none of this stuff should be a surprise. But we reserved ID 127 for more extensions. Basically kept the the length coding same as ID's 32 to 126. So that someone who doesn't understand those future extensions can just skip it and so far, we're leaving the contents of what the actual payload of of ID 127 is. Just up to a subject of a future draft. If we ever need it. I may be the person in the room who least understands offer answers semantics. But I did try to do some digging and found in in RC 5576. It tells you that that media level format parameters must not be carried over blindly which makes sense to me. It basically means that that If you want to describe what extensions that you support in SDP, you know, you're gonna have to upgrade your SFU to understand extensions. I don't think there's an automatic mechanism we can specify that will just work. Go ahead, Johnson. Yeah. I guess, look at the multiple frames thing made me think, how does that I mean, and maybe this needs to be specified with dread, but how what what frame is the dread associated with the last one. Do you have to we'll see. You can specify it however you want. Right? Like, the the idea is is This is normally not useful to"
  },
  {
    "startTime": "00:50:01",
    "text": "to attack. It's presumably not useful, but but, I mean, my my my point is this, like, You you can Encoded in this extension format. To apply to any particular packet. Right? And and the thing that would make or any particular opus frame in that packet. Now that the thing that would be useful is is you wanna have it on the first frame because you wanna cover all the stuff that you lost before you receive this packet. And it doesn't make sense to have it on the last one and have redundancy four frames that are in the same packet because, presumably, you didn't lose them. But but the point is is is the separators let you make that distinction. And in particular, if you did something completely naive where you just copied over whatever extensions were associated with the packets you received when you were, like, combining multiple Hackets are multiple packets into a single packet with multiple frames. Then it would still all just work. Like, you would know you would know which, which frame the dread was based on on these separators. Does that make sense? Sure. Mark has his hand up. Go ahead, Jean Marc. Very quickly, to complete the answer. One of the goals of this frame separators is that for not for dread, but for some of the extensions we're planning, for example, if you have enhancement extensions, you actually want to transmit all of them that you need to make sure they're actually associated with the right frame and if you were to split them again later, then you again want everything to follow. So that that's why we need this, separator thing."
  },
  {
    "startTime": "00:52:00",
    "text": "Alright. See, so I I also clarified in the draft, that that support for extension ID 0 and 1 does not need to be explicitly signaled via a equals FMPP, And the reason for that is that if you don't support those extensions, you won't be able to parse the rest of them. So it's not useful to be able to say I don't support 01. Also asked, extensions to specify whether or not they can appear times for the same frame in a packet. Now what I think that was one of the previous discussion points I think it's it's okay to allow it. But if an extension doesn't want it, then they need to tell us it's not okay. And then just a few other minor edits for for warding and clarity. So one of the things I noticed in in doing these updates is that we actually have 2 your extension mechanisms? So I talked about the ID equals 0 l equals 0 is makes the rest of the frame into padding again. And the The fact that you must set it to 0 in the encoder and must the decoder must ignore it is basically the same set of rules that lets us add these extensions. So you could extend things recursively. Basically, you get a a a a complete redo of the extension format, should we want one the future, with a 1 byte overhead. To stuff them into the current extensions. The problem with using that as an extension mechanism just for you know, higher extension IDs, as opposed to the the ID equals 127 approach is that you don't have these separator bytes. Because once you signal that you're you're using padding that"
  },
  {
    "startTime": "00:54:02",
    "text": "that eats the rest of the packet. So you can't add a separator later to to switch to a different frame. So the ID equals 127 approach, you know, has a nice defined length. It can be skipped in mix with other extensions. And it can reuse the separator structure so you don't have to signal that twice just to use extra extensions. So I I think it's useful to have both of these I don't think there's a problem with it, but I just wanted to point it out people hadn't thought about it, finally, things that we have not done yet, so I did not split out the INA registration for l equals 0 and l equal and 1 modes for IDs 2 to 31. Basically, I'm not sure there's a a lot of use cases where you would want to have an extension that allows both a 0 byte payload and a 1 byte payload. It seems like they're they're kind of distinct use cases. So if we we don't allow them to be registered separately, then, you know, we, we basically have the amount of extension ID space we have for these. And these are the, you know, Maybe we don't really need 31 of these, but but or 30 of these. I mean, but the idea is is, you know, this this is kind of a limited space. We should think about if we want. Want to do that or not, And it it is something we have to decide upfront, because these will be signaled in the the Eagles f and FMTP signaling. Yes. Go ahead, Jonathan. Other than the separators, do you have any things that you're thinking these basically flagged by extensions will be useful for. Yeah. Yeah. So so, certainly, like, you you might imagine,"
  },
  {
    "startTime": "00:56:02",
    "text": "an enhancement method that only needs to send a very small amount of side information? And if you're just gonna send 8 bits having another 8 bits to signal length, is, you know, a lot more overhead. I'm I'm thinking of something like Bandwidth extension here. But how many of these these zero lungs are you gonna have that? So that's just a flag. It's there or it's not. Yeah. Yeah. I don't know. The answer might be, you know, none. Right? Yeah. That's the hard thing about trying to predict the future. So it's probably not a big deal. But we should make a decision. So I think Mohit suggested switching to a quick variance for the extension IDs. And, initially, I kinda like that idea, because note number 1, it means it you're You're essentially the l flag is gonna get folded into the extension ID to start with. It answered the previous question. But it does reduce the available 1 byte extension IDs from a 118 down to 30, which seemed kind of drastic. So I did not make that change. Just just to clarify, I'd I wasn't I wasn't specifically enamored with quick variance, I just meant in general, you know, there's many variable LinkedIn Coatings that are popular Like, Tim, I'm I'm sure you probably remember everyone's, LE 128. So that you know, Verint, type encodings that may be more compact or meant for smaller spaces then quick variance may be more applicable here. Yeah. I mean, I 4 bit, extensions. A quick minute. Maybe, too big about Sledgehammer."
  },
  {
    "startTime": "00:58:00",
    "text": "Right. And I and I think as as as Jean Marc kind of points out, like, you know, if you're gonna be putting these in a giant switch statement, not gonna have 64, 64 bits worth of them. you're you're Or even 32 that's worth of them. So I I think, like, at the point that we need that many extension ideas, it feels like we're gonna be encoding data in the extension ID which I don't know. Seems weird, but maybe there's a use case for that. But but but but Yeah. If you have if you have another proposal that you think makes more sense than the current one, I think it's worth considering, but But so far, I have not not try to do that. Then finally, we didn't reserve any unsafe extension IDs. ID some somewhere an extension ID where if you don't under it, you throw away all the rest of the extensions. So I don't think we have any clear use cases for that. But that was an idea that that I had put forward is is something that might be useful, but I haven't haven't seen the concrete use case for it yet, so I haven't added Alright. And that's my last slide. Alright. Any further questions? So you already have updates that that are in, that are pending, Tim? For the next version. For a crew draft? Are you waiting for feedback from people to do an update. I'm I'm currently waiting for feedback. I don't have anything else in the queue. Okay. I will point out that that our milestones say that that this is going to the ISG in December, which seems soon. I I didn't come up with that date, but"
  },
  {
    "startTime": "01:00:00",
    "text": "there's not many people physically in the room here, but, for everyone else online, that please let's get some reviews of this document in. It's a short document, easy read, but it'd be really useful to get some some more of your feedback give tips and guidance on what changes maybe needed for the next version. Yeah. I'd I'd definitely like to see, some more feedback before I think it's ready for IETF last call anyway. Alright. Thank you. We're done 6 on the nose, For the rest of the meeting, Thanks for helping everything. I'm glad that you did that time. I was really kind of worried you did. Yeah. He would have the largest Yeah. He had. Yeah. I missed that he had a stop. The middle and then some backups noise behind it. So is that it for you? Are you interested in anything else That was a copy. Oviriums, I'm not even this room."
  },
  {
    "startTime": "01:02:02",
    "text": "find out if I have to do something very easy quick? Yeah. It looks like the, You just the the the"
  }
]
