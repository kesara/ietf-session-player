[
  {
    "startTime": "00:00:43",
    "text": "okay let\u0027s start Andre we\u0027re starting so good morning and welcome to IRT F open I\u0027m the IRT F chair Ellison and I think it\u0027s Wednesday morning so we\u0027re already like really tired and jet-lagged and not getting up sleep so pay attention we have a a new note well in the IETF and the IRT F follows the IDF\u0027s note well so this is our version of it which isn\u0027t very different but just you should be aware but if you haven\u0027t read this new one because it\u0027s just getting rolled out to take a look at it and in addition as you know there\u0027s a photography how the photography consent discussion and the ITF has the IAS James made a statement on this because we actually do have a lot of photography in here because of the NRP I want to make sure people know that that if you don\u0027t want to be photographed the there will be the video but you will you\u0027re you have the right to consent or not and you\u0027ve got your lanyards but also let us know if you have any worries about that okay so I just have a couple of things to say about I acted before we get started this is the the sort of longer-term part of the of the organization and we we definitely have an interest in well focused research that results in things that innovate for the ietf of the idea here is to give people the opportunity to see their research get into action and it\u0027s a time to incubate that and I\u0027m sure you know because you\u0027ve been coming to research groups that we have research groups last time if you were here I talked about how they\u0027re different I\u0027m not going to talk about that this time and that we have an IR s G and their names and all that information is on the IRT F dot org page and I\u0027ve noticed that some people don\u0027t seem to know this so I want to I\u0027ve added it that our tip does not produce standards we\u0027re pre standards so to the extent that RFC\u0027s come out of the IRT F they come out as experimental or informational and they\u0027re meant to say very clearly that this is research and then one reason to have them in the RFC form is that that later they may evolve and migrate into research into results that are used in "
  },
  {
    "startTime": "00:03:45",
    "text": "standardization but not in these research groups so just that\u0027s something and here\u0027s a list of all the research groups the we have this concept of proposed groups and the idea is that it should be quite easy to spend something up because it\u0027s not always easy to tell when what\u0027s new and what\u0027s what\u0027s a good thing to work on so they allow a group to form very readily unlike the working group chartering process but it gets three meetings and then the irst and I decide whether they\u0027ll continue or not so we\u0027ve got a couple of proposed groups in process now the the den group the decentralized internet infrastructure which really should be called deep it subbed in and path aware networking and then we have twelve groups all together counting those two paths aware networking had their third meeting yesterday and we had a hum we do actually Humphrey can because who doesn\u0027t want to hum but the and the agreement by the group by the room was that should go to our G and I believe it will so that\u0027s something another bit of news is that when our recent group gaya which is the Internet access to the internet for all globally is having they\u0027ve organized helped to organize with the IAB the technical plenary tonight so there are three gaya themed talks and they\u0027re moderated by Jan coffin who\u0027s one of the co-chairs of Gaia and then this week everybody\u0027s meeting except the network management group we we do feel that groups should meet with research conferences as well sometimes and they they regularly meet with the noms conference and that\u0027s what they\u0027re doing and then today you\u0027re getting the first look at a potential new group which is the quantum internet research group and the goal here is to have you eventually have to start calling yourself the classical internet people if you\u0027re not working on quantum so we\u0027ll see if you\u0027re convinced of that but we\u0027ll have a remote speaker on that shortly okay so the other thing is I want to advertise the the workshop we have a workshop that tries to bring academic research and and our and our standards in the applied research we do here all together it\u0027s been happening for several years now it takes place during the summer meeting and we have an innovation this year which is that it will take place during one of the IETF days I\u0027ll talk a little bit more about that at the plenary tonight the idea is to try to bring more togetherness between the people who come to talk about applied research and the research groups and working groups themselves "
  },
  {
    "startTime": "00:06:46",
    "text": "and you probably have something you\u0027d like to submit and it\u0027s due by April 20th so please think about submitting it\u0027s really easy to find if you look up in our W 2018 or look at the I RTF page you can find it and the the good thing is that you can either submit a short paper like a couple pages of something that says hot and new or you can submit a previously published paper from the past year so you don\u0027t have to write a brand new deep 12-page research paper you can bring to something that you\u0027ve done somewhere else to this workshop and it was that they\u0027ve been very good and we\u0027re hoping they\u0027ll be it\u0027ll be even better and even more exciting because of the bringing it closer together to the rest of the meeting okay so these are some of the links if you want to get more involved you probably already know I RTF discussed and announced and we have wiki links for all the research groups that you can find that irth orgs a mail we tweet and the agenda for today is I\u0027m almost done and then rod Ben meters going to speak Stephanie has a family issue so she will not be joining us but we are hoping these are the two proponents for that research group and they\u0027ll rod will tell you about how to get more involved in that and then we have our applied network research prize talks and I\u0027m very excited about those now introduce those when we get there so I think we\u0027re going to try to bring rod noise no let me tell you about the NRP I\u0027ll use this slide when when we come back I think no let\u0027s do it now sorry so we have our two speakers coming we are sponsored the the AARP provides a monetary award and travel funding and the sponsors that make that possible are Comcast NBC Universal and the Internet Society so thank you and if you think you\u0027d like to sponsor please let me know get in touch with I saw for myself match for myself because if we get more sponsorships we\u0027ll bring more people back for additional meetings to allow increased engagement and I wanted to say also that we have a wonderful program through the year we have to each ITF we had the largest set of submissions that we\u0027ve ever had and we could definitely have have made many more Awards because we had so many good submissions so I think you\u0027re gonna have a good time with our presentation so and that\u0027s it and that picture is to tell you don\u0027t be like that cat don\u0027t read your mail engage during our meeting and it\u0027s particularly actually for the art research group chairs who who should look around and if you see people being like that cat you probably need to get more discussion going that\u0027s my cat but okay alright so let me see if I can get us to rod now okay so let\u0027s see I have "
  },
  {
    "startTime": "00:09:51",
    "text": "it slides so rod can you speak yes hopefully yes and I think I\u0027m gonna be showing your slides probably yeah I\u0027m told you you\u0027re the one who gets to run the slides okay okay go for it I\u0027ve got your title page slide tell me when you want to change okay great so thank you all for having me and I\u0027m sorry I\u0027m not actually there in person I would love to be in London with you all but in solidarity with the the whether you\u0027re having there we\u0027re actually having snow here in Japan actually in Tokyo - for the first day of spring we planned this presentation Alison asked us to give the presentation based on the email that she and Stephanie I had exchanged over the course of the last Oh what six or eight weeks I guess and I want to tell you a little bit about what a quantum Internet is and what we\u0027re thinking about and ultimately why I RT F should care and why RTI RTF should actually start our working group tell you how you can get involved in what kinds of things that people in the room there can do so as Alison said I\u0027m rod Van Meter I\u0027m from Keio University and the wide project so I actually probably know a significant number of you there in the room although I can\u0027t say he\u0027s there very well and Stephanie Vader is from the Technical University of Delft this was actually originally her idea and she used to work for one of the the I believe for an ISP and the Netherlands before actually getting involved in quantum work so she and I both have a long history with actual networking as well as a quantum stuff so that\u0027s where we are Allison can you go to the next slide please I have okay so the one other person I should probably introduce while we\u0027re looking at this slide here I think somewhere in the room is shota Nagayama shelter can you wave your hand if you\u0027re there somewhere you may be around some youth in that okay so was mine he is actually also a participant in IETF so he\u0027s one of the other people who actually sits right at this border of quantum and classical stuff so after I go offline here in an hour so you want you can track down shota there at the meeting in London and talk to him about it so first off I should say what is a quantum network and I know a lot of "
  },
  {
    "startTime": "00:12:54",
    "text": "people in the room are already familiar with quantum key distribution quantum key distribution is a way to use quantum effects to create a shared sequence of random bits that the physics of the system is supposed to guarantee you are known only to the two parties at the endpoints and then you can use that set of random bits for a key a key for IPSec or kls or what that\u0027s the easiest way to implement qkd is actually using what we call a an unentangled quantum network so it uses single photons but that you don\u0027t actually have to have complicated memories and a complete quantum computer that serves as the equivalent of your router if it works it adds to the longevity of the secrecy of your encrypted information but it works only over a limited distance 100 kilometers or 200 kilometers through fiber but in theory also works over satellite in fact there have been demonstrations of that over the course of the last couple of years it\u0027s sort of weak and multi-hop settings but better for point-to-point it\u0027s a lot easier than in time building entangled networks at the physical implementation level but it\u0027s still not very easy so a lot of you are probably already familiar with the basic idea of qkd you\u0027ve heard us or somebody else talking about it at some point that\u0027s only one of the applications that we actually envision for for networks in the long run but those other kinds of applications that I want to tell you about require what\u0027s called an entangled network I\u0027m not going to go into the details of what\u0027s going on at the physical level but basically it means that that you\u0027re creating a shared quantum state over a distance and so the work of the quantum network then is to actually create that shared quantum state entangled networks are good for a set of applications I\u0027m going to show you those a little bit more in the next slide and in theory using quantum repeaters which are not like classical repeaters a quantum repeater is more like what what in the internet world we would call a router using quantum repeaters we can in theory coupled together systems that are as an arbitrary distance apart within sort of the normal limitations of networking as opposed to the distance over which you lose a single photon which is the limitation in unentangled networks also next slide please okay so on this slide I\u0027ve actually got "
  },
  {
    "startTime": "00:15:56",
    "text": "who of how I think about the applications for entangled quantum networks there are three large bubbles there the purple one is reputed cryptographic functions the blue one is sensor networks and that yellow one is distributed computation you can see sort of on there on the Venn diagram that there are a set of potential applications qkd is in the middle on the right it\u0027s sort of a it sort of sits at the border between sensor networks and distributed cryptographic functions because really what\u0027s actually happening with qkd is it\u0027s behaving as a type of sensor and detecting the presence or absence of an eavesdropper in your communication channel so that\u0027s one potential use another one or two in the distributed cryptographic functions up there you can see at the top in the center are Byzantine agreement and leader election and if you look at Byzantine agreement and secure leader election and quantum key distribution or the point of all three of those reduce our dependency on public key one-way functions and computational complexity as the basis for for security proofs so that\u0027s one potential area for the uses of quantum networks in the lower-right sensor networks besides qkd itself there are two or three other good uses for sensor networks and many more being proposed clocks is one of the important clock synchronization there have been several algorithms proposed for doing this using quantum effects that would in theory gain us synchronization of clocks over a remote distance that add a much higher level of precision than what we can achieve today using GPS or NTP or really classical mekin so that clock actually is a form of reference frame it gives you it tells you what the time is in a couple of different places it allows them to essentially synchronize the the axis of time but distributed quantum states can also in theory be used as a reference frame in the physical sense as well and so people are investigating different kinds of sensors for that and also for what\u0027s called for interferometry which is how large-scale astronomical installations collect light from multiple telescopes or multiple radio antennas and combine them into a single "
  },
  {
    "startTime": "00:18:57",
    "text": "signal so at that level it\u0027s very much a physical level operation that\u0027s going on there and then in the lower-left the third circle the yellow one is distributed computation and this and in this sense is largely like the ARPANET this is an a distributed system in which you actually connect to and use servers over a large distance over a long distance and the idea is that even if you have a computer that isn\u0027t very capable at your site that you can connect to a quantum mainframe somewhere else out there in the world and be able to use that quantum mainframe remotely over the over the quantum network now why is that different from just being able to do what you can do today which is log on to IBM\u0027s website and use a quantum computer that\u0027s sitting in Yorktown high remotely via the web well if you begin with quantum entanglement between your site and the mainframe then it\u0027s possible to do what\u0027s called blind computation and the blind computation is a mechanism for execute asking a server to execute a function for you without giving the server any information about the input data the algorithm that\u0027s being executed or the output data so it\u0027s like gendry\u0027s homomorphic encryption in in terms of its role in the ecosystem although the underlying technology is very different if you\u0027re familiar with homomorphic encryption you can think of it in kind of same use so those are the three broad areas in which I can see uses for a quantum network distributed cryptographic functions sensor networks and distributed computation okay Alison next slide please ok so I mentioned repeat a minute ago quantum repeaters quantum repeaters are the equivalent of routers in the internet and they have four basic tasks which are listed up there on the screen first is to make the basic entanglement the basic quantum state over a single hop over a fiber over a free space link the second task is to be an edge the different kinds of errors there are including loss of photons and inaccuracies in in the operations on your quantum states and the problem the fact that memory has limited lifetim the third task is to extend entanglement "
  },
  {
    "startTime": "00:21:59",
    "text": "across multiple hops so you know the behavior of a network and then the fourth task is to actually be part of that network including dealing with routing and managing resources and security and those sorts of things and it\u0027s in particular those last couple of bullet points that I that I think that are there in the room likely to be able to make a positive contribution George\u0027s next slide please okay so that gave you just three minutes on the the why what is it that we want to accomplish by doing doing this the next thing I want to do is to sort of give you an idea give you a sense of what\u0027s going on in the world and the fact that this is in fact happening on the right hand side of the slide there you could read oh just logos from five startup companies that have been around most of them for a while now but overall in the quantum information industry there are now more than 50 startups many of them that have been created within the last year some of them are hardware companies trying to build quantum computers some of them are software companies trying to figure out how to use the existing quantum computers and the some of them are networking companies that are here on the slide are all huge IT companies as far as I\u0027m aware of none of these companies are actually working on quantum computers yet but that may very well be internal efforts that they\u0027re not actually talking about in public so it\u0027s happening in the startup industry it\u0027s also happening in the big labs IBM Google Intel the Microsoft are getting a lot of attention for their work and there\u0027s also event venture funding that\u0027s out there there\u0027s also of course a lot of support from governments and so there\u0027s an entire complete ecosystem of people who are working to create a quantum information technology industry and I think you can argue that that the fact that industry already exists today so the technology the knowledge and the ecosystem are all reaching this point where we are sitting right at the edge where it where this whole thing is going to blossom over the next several years when you see actually Zeus for now the Quadro repeaters arguably the hardest part of all of this and so it\u0027s been people but while it\u0027s going to be a little while yet before quantum repeaters are out there in the in the wide world as something you people can go and buy off of the shelf that\u0027s what we\u0027re working next slide please so Stephanie\u0027s really the one who ought "
  },
  {
    "startTime": "00:24:59",
    "text": "to be able to talk to you all about this particular slide because the leaders of this particular project at the Technical University of Delft just to come a few hundred kilometers or weary well right now a couple of years ago this was published in October 24 pushing on three years ago since the experiments we\u0027re done the folks at Delft tiny pieces of diamond that were set about a kilometer apart and coerced them to hold qubits quantum bits worth of data in there inside that tiny chip of diamond and also to emit photons and then they take those photons and they route them together as in the diagram at the bottom of the slide there and then they conduct an interference operation between the the photons there in the middle once that\u0027s done then we just want them entanglement over this distance of 1.3 kilometers in the across the the Delft campus so this can be done today an optical fiber over over modest distances it\u0027s experimentally up and running and the lat the group end Delft is arguably the world\u0027s best but there are a lot of people who are working on similar sorts of things next slide please okay so besides just doing this using fibers that are actually in the ground or in buildings in between there\u0027s also been work on doing this using satellites and you may have heard about this and made a tremendous amount of news last summer when China had succeeded in receiving quantum signals on the ground were essentially qkg between two stations on the ground there they\u0027re separated by about a thousand kilometers or so but later they continue to conduct experiments they had already prepared for them there with the satellite and they continue to conduct them and they have actually generated entanglement across that across that kind of distance and they have also more recently used a slightly different approach where the the satellite is actually used as a relay station and so it performs qkd between the satellite and the ground when it flies over China and it stores the classical "
  },
  {
    "startTime": "00:27:59",
    "text": "bits it is created in the process in the memory of the satellite and then when the satellites orbit carries it over Austria then they perform qkd again between the satellite and Austria and then they have created a secure key that\u0027s shared between Austria and China now this was actually in the news quite a bit last summer where or last fall when they talked about it and used the the key that was so generated to encrypt a conversation between China and Vienna so it got a lot of press we can talk about the actual security implications for all of that and whether or not you have to trust the satellite platform and things like that for qkd but when you\u0027re actually using it excuse me for creating entanglement over this kind of distance then those kinds of security arguments are actually very different and so please if you\u0027re already of a particular mind with respect to a qkg in certain senses realize that the arguments are going to be very different with when we\u0027re talking about wide area networks that actually provide entanglement in the end and China\u0027s not alone in doing work on this there have also been experiments from Canada and Singapore and the other picture there on the on the right and the upper part is actually a satellite that when it\u0027s in orbit right now that was put up there by Japan last year as well so satellite work and also ground-based work is going on in this next slide please okay and ultimately there\u0027s work going on all around the world but Europe right now is one of the places where that where there\u0027s a tremendous amount of not individual effort but very focused effort so the in the EU has a quantum internet effort that\u0027s known as the quantum internet alliance there\u0027s a logo for it there on the left and a URL quantum - internet dot team and you can see going back by going to that website you can see the the list of researchers were involved in this Stephanie again is one of the prominent researchers that\u0027s involved I wish you were here to tell you more about it but it\u0027s part of the EU overall 1 billion euro effort to develop quantum technologies and bring them out of the laboratory and into the actual market my place okay so what does it take to "
  },
  {
    "startTime": "00:31:03",
    "text": "actually build one of these networks this slide comes from a paper that I wrote fully a decade ago on protocols for quantum repeaters and you\u0027ll see that there\u0027s there\u0027s sort of a stack many of you in the room probably know Joe touch Joe touch and I or argued about this quite a bit over various meals and and visits to each other the upper layers everything except the bottom is actually a classical control protocol so at the bottom in the purple box you see it says physical entanglement and red letters they\u0027re the only quantum that\u0027s the only part of the entire system that\u0027s actually physically quantum though everything else that\u0027s above it is classical protocols for controlling the behavior of this quantum system in order to go from having entanglement that exists over a single link to entanglement that actually spans end-to-end from your your communication endpoints now the protocol stack that\u0027s above it involves a series of functions which are going to depend on certain key design decisions from your your general architecture from your your quantum repeaters but this is a reasonable example some of it happens over a single hop some of it happens over two hops or four hops along the path chosen through the network and that some of it\u0027s done to end-to-end and that\u0027s where the argument with Joe touch comes in is all right well maybe it\u0027s not really a layered protocol in the sense that the ITF is a customer things it\u0027s actually much more of a distributed computation because all of the individual nodes that are in the path actually participate very actively in the entire process as you are conducting your communications over an extended period of time so it\u0027s it\u0027s a very different approach and the Internet\u0027s fire and forward and forget approach to building a protocol but this is an area where classical protocol design people can make a very strong positive contribution because of the expertise in doing these kinds of things and that\u0027s it yes so Allison perfect timing to switch to the next slide there so we are getting to the the stage of wanting to take all of this and discuss these kind of important problems again any network how you deal with routing and how you deal with connections set up and how you deal with resource management and how you deal "
  },
  {
    "startTime": "00:34:05",
    "text": "with Internet work interoperability and security and all these kinds of issues which are near and dear to the hearts of the people that are there in the room at IRT F and the experimental physicists who are doing the work so far have no idea how any of this stuff is done they don\u0027t know how to go about setting up a connection between between two nodes across the network they don\u0027t know anything about how you do resource management and in a wide area network you know know what multiplexing means in a single channel but not so much across an entire network for example so we need the expertise of the people in the room and we need a way to connect classical networking people to the experimental community that\u0027s beginning to do build these quantum repeated networks so Stephanie proposed two or three months ago that we should create a research group inside a by RDF though I contacted Allison and we\u0027ve been exchanging some email about it and so far the proposed is Qi RG the mailing list is open you are welcome to join the mailing list there were a couple of dozen people on it ready even though we\u0027re just beginning the process of advertising it so please get on the mailing list and Alison next slide please whoops okay this is not the all right the will do will do this I did want to mention briefly as well as we\u0027re finishing up that there are also queue can be oriented standardization efforts both inside of Etsy and I Triple E and of course inside of the IETF we have been talking off and on for eight or ten years about methods for incorporating UK degenerated keys into IPSec and dealing with other outbound out of be and key management mechanisms for key generation mechanisms so there\u0027s work that\u0027s going on on that as well next slide please and the the first task if you join us in the in the queue IRG mailing list is maybe perhaps somebody in the room there can suggest a prettier name than queue by RG which we haven\u0027t even yet agreed on a pronunciation for we\u0027re not sure if it\u0027s a turd or quirk or what it is we understand some of the elements that are going to be in the Charter there\u0027s some of the things that I mentioned on the previous slide a minute ago but there\u0027s of course we\u0027re to do to hammer out the actual content of that and the tentative plan is for qyrg to meet three times a year once at IETF once at some sort of "
  },
  {
    "startTime": "00:37:05",
    "text": "quantum conference either queue crypt which is a qkt focused meeting or the workshop for quantum repeaters and networks which is a a repeater oriented workshop and then to have one meeting be virtual so meet three times a year on that schedule and that\u0027s it let\u0027s say next slide please yep so the we will be having the next of the wrm workshops actually here in Japan in fall of 2019 and if you are on the mailing list of course you will get the announcement you\u0027ll you know ask to join up with us on that and with that I hope that you all are as excited about building a quantum Internet as I am Alison I guess we have time for questions I think there\u0027s time for a few questions if people have any and hopefully you\u0027ll hear them well let\u0027s say your name when you come up here at econo Cosima thanks for a talk I have a one qualifying question in one question about I carrasco network and a contaminate oh so the first name first question is so according to your site page number nine it\u0027s like there is no IP or TCP SciQuest called network stack but but yours your figures say it\u0027s like a that only a bottom layer it\u0027s that quantum network so I think we can there we can it seems like there is a new different network and we are trying to create the quantum network instead of that cross car network so is that my understanding is after we will create a new network is that correct that is a qualify question and also the second question is so if the quantum computer cannot we cannot use IP or those kursk or network stack so in this case like it\u0027s it\u0027s a little bit hard to connect the connector communicate between the classical network and quantum network so do use is current scope charter has some kind of work to achieve the communication between classical network and the quantum network yes I didn\u0027t how are you so the those are good questions so Allison if you can go back to that slide briefly the one the one with the it with the colored stacked by is the yeah the stacks like tell it yeah we are going to have to have a new physical layer so there\u0027s there\u0027s no getting around that that\u0027s going to mean deploying new new devices with new transceivers or the equivalent thereof there is work going on to allow that to "
  },
  {
    "startTime": "00:40:06",
    "text": "multiplex with standard tcp/ip traffic inside the same fiber that\u0027s been demonstrated experimentally so you\u0027ll need new load new nodes but you don\u0027t necessarily need to pull come the completely new fiber in parallel all of those other functions that are on top of that are all new protocols but of course they also need a means to exchange their their classical messages reliably so so when I picture the the the protocols PC and ES I picture those communications actually happening over top of TCP and then the application layer on top of that is the it\u0027s it\u0027s a what\u0027s what\u0027s the equivalent of an HTTP put and get or I for quantum network what is a request from an application to the network itself what what is it that it that that you\u0027re a classical application that\u0027s running on a node that wants to take advantage of these services provided by the quantum Internet how does it make that request what does that contain how do you exchange that and how do you agree on the semantics of all of that so those colored boxes that are on there I think largely will ride on top of TCP in terms of the actual communication that gets done but it\u0027s building a large distributed application on top of it does that help yep okay thanks great one more question West vertical USCIS I you know originally I was thinking that that this was a little bit early in the in thinking but then I decided that I\u0027m wrong there and and we want to get this sort of right from the get-go in terms of Marie\u0027s output of something like this could actually affect you know the development of the hardware and the technique so I think that\u0027s actually that the timing is about perfect so I\u0027m excited to see this go forward I do think that there\u0027s a few other topics you can consider adding to your list of things to explore such as the security requirements for what\u0027s going to be layered on top of it in particular with respect to privacy concerns as an example your satellite is sort of a classic case of a store and forward mechanism where you have to store data at rest well it you know while it travels or interplanetary networks have the same kind of thing excuse my ignorance of quantum stuff I\u0027m actually fairly ignorant and it may show but you know so the quantum routers have a similar property where since you\u0027re essentially breaking communication between in the middle between endpoints then you\u0027re disturbing the privacy that I believe is offered by quantum generally so you know what\u0027s the security implications of that so I think "
  },
  {
    "startTime": "00:43:08",
    "text": "there\u0027s a number of things that need to be considered with respect to what sort of technologies you need above that and the requirements above that do you still need to do you know sent what what sort of encrypted traffic do you need to send above that and what are the properties are the protocols that you\u0027re earning is on top of this whole system that you can either relax because you have quantum or you still need because quantum doesn\u0027t solve them absolutely sign that man up and seven person from a site myself I\u0027d love to have you on board because that\u0027ll give me an excuse to go back I as I\u0026C the game okay one more question shota yeah actually yeah certain I mean speaking actually is not a question just comments so right now the researchers are canta networking requires organization over time neuroses I yeah because some papers started to propose some different terminologies for washing same concept so I needed a I need a place for open discussion for such things as otherwise so you know various papers which use these different terminology will lead to a kind of help so to avoid that we need yeah we need an open discussion so our idea is a best price I believe so and and now we can refactor network timing of this property here and so another thing is so one thing we needed to discuss enjoy our tea is the obstruction of the entanglement and you know the entanglement is a very quantum property and no Kruskal need talking it doesn\u0027t the hub the counterpart for that so we should carefully discuss the obstruction of entanglement and at last yeah so one thing so is a one benefit to talk about continuity mean iltf is that so julio administrate contem network in the future so it will the network engineer so like guys here so yeah to involve such people for discussion of quantum networking is very important from this RT station thanks actually I agree with with all of those comments in particular you know that I\u0027ve had quite a bit of heartburn where the the physicists reuse a term they have picked up from classical engineering and the way they\u0027re using it\u0027s not necessarily wrong it\u0027s just so different from the way we\u0027re used to using it that it\u0027s causing a problem "
  },
  {
    "startTime": "00:46:08",
    "text": "multiplexing is one of them repeater would certainly be another and of course you\u0027re they are they also when they say the physics of this meeting guarantees that it\u0027s secure you know the the the head of every network administrator in the room hits the desk because they know that that theoretical factors of implementations are very different from from the real-world implications and so doing all of this and discussing all of this with the people who understand real networks is the key to a long-lived healthy robust extensible quantum internet architecture and I think the IRT F has a lot of expertise that\u0027s really required yeah we\u0027ve got one more question and then we\u0027ll stop no more questions very cool hi I\u0027m Eric oh um okay I\u0027m not so familiar with this area and this is comment ready to this community and I\u0027m not like I am sure corner computer and computing is important for network engineering and so now on the future um but I think there is not so many people who are familiar with this area so um so if you will walk on this corner computing discussion in this ayat if area um how do how do you get IP network engineers involved in this discussion and also on how do you link on the corner computer guys from outside this community come into this idea community um how how how about your plan for that well the social you know that I\u0027m I was pleased to hear the earlier comment that you the I didn\u0027t catch the name of the guy from ISI that I don\u0027t know it was a who said that you now he sees that maybe now is the right time if we and and it\u0027s a good opportunity for people to be involved to prevent the physicists from going down the wrong path now convincing the physicists that they need to be involved that\u0027s the other half of the equation and we\u0027re working on that one too but there are already hundreds of researchers around the world who are actually working on this and you know we have those two quantum conferences that I mentioned the workshop for quantum repeaters and networks we restrict attendance to that to to a hundred people so that we can have a small conversation but we could have twice that many people and the queue crypt conference is three to five hundred people depending on the location there so there are a lot of people around the world who are already working on the technologies and the goal is to bring them together with with the people who really understand the the way networks behave because I think you need "
  },
  {
    "startTime": "00:49:09",
    "text": "more um you need to enhance more people for another area into these discussions that\u0027s why I absolutely be the start today so we should we should go move on to our next talk but thank you so much rod and audience yeah Matt yeah I just wanted to say I\u0027m using the ether pad for taking notes so you might want to go in there if you asked a question and make sure I didn\u0027t butcher your name or a butcher what you said thank you thank you and the the slides Alison yep which I assume will get uploaded there are some references in there that I didn\u0027t talk about but you can look up those references if you want to know more or contact me and join the research and join the mailing list they are uploaded already so so that people can do that okay so thank you that\u0027s great bye I\u0027m gonna switch off and we\u0027re going to introduce our next speaker now so the that we have the first of our NRP presentations you want to come up to here and let me get your slides up here as well okay and I think do you want to sorry yep this is the ones you gave me late in the night but they are almost thinking about making them larger but I don\u0027t think you can it\u0027s probably about as good oh you hear yes okay um they\u0027re government that\u0027s okay okay yeah yeah you should be able to do that and see if it works okay just tell me what\u0027s there I think it\u0027s not working I think I have the right attachment yes okay all right "
  },
  {
    "startTime": "00:52:29",
    "text": "introduce yourself hi everyone Muskaan I\u0027m gonna present to you our work on performance characterization of a commercial major streaming service my current affiliation is with Akamai but I did this work when I was HDC that at Princeton and interning at Yahoo research so we all have had that experience that remember watching our favorite video and there is a buffering and you end up wondering why and even as the networking community we often wonder what happened and what\u0027s the problem and in this work we have an unprecedented data set - for the first time instrument and study the entire end-to-end video delivery path and with video being the dominant application of the internet with making up more than 70% of the traffic at least in North America during peak hours it\u0027s particularly important for us and we can uncover a wide range of problems for the first time with this data set because we have data from the entire half so I\u0027m gonna walk you through a list of performance problems that we have found them oh you must have touched the registry it changed what Nathan esterday Erica you are it\u0027s writing to improve the same oh I see oops no that\u0027s not good I don\u0027t know why I see what you say maybe we should just sorry how about that okay so here\u0027s the "
  },
  {
    "startTime": "00:55:50",
    "text": "list of problems that we have found don\u0027t worry about reading this because what are we walking through this talk so first let me start by showing you search on on projection systems a yes I can actually okay let\u0027s get her a laptop yeah should we have a research task force for the projectors and displays yeah maybe by the time we have quantum microphones you have that I don\u0027t think it\u0027s the same donk Ozzfest oh is it USB I dunno oh you have AI HDMI directly good okay yes okay yeah you got to restart your time we\u0027re letting her restart her time too okay they also get this alright thank you for your patience everybody all right we\u0027re back in business so here\u0027s the list of performance problems that we found but don\u0027t worry about it I\u0027m gonna walk you through this list through talk but before that let\u0027s look at the system to be instrument it so you can get an idea what could go wrong before we show you what did go wrong so the system is Yahoo\u0027s video streaming system and the way that streaming works very high-level overview it starts by the client which is your player requesting and receiving the manifest and it manifests it\u0027s the list of available bit rates and then there\u0027s usually an adaptive better algorithm on the player that chooses which bitrate to request for and then once it decides the mid rate it sends an HTTP request to the CDN to get that and in this system these HTTP requests are sharing the same TCP connection and it chunk in this system is six seconds once these requests arrive at the city and the city and inspects its local cache if it\u0027s there to the player if it\u0027s not there it\u0027s called a cache miss and you go to the back end to fresh check on it first our Citians in this system the city servers are using Apache traffic server which is a popular HTTP proxy and the caching eviction policies LRU and then finally when these chunks are arrived at the client side they have to pass the clients download and rendering stack "
  },
  {
    "startTime": "00:58:50",
    "text": "before they can be shown to the user and we will go into more details of what these mean later in the talk when we get to the client section so our goal in this study is to identify performance problems that impact video QE and in particular make the users are happy so you know reduce your revenue and if you have only data from the players side for example if you have here a data for example the caveat is that you may be able to detect some of these problems you may be able to say there was a rebuffering but because of the buffer itself you are some of the problems get masked because of the buffer so you may not see an immediate impact on the QE and also if you only have players like information you cannot detect problems that happen in network or the CDN and looking at it from the other side from the perspective of if you have only data from the CDN for example server logs you may be able to detect some of the problems but you will not be able to isolate problems that are within the clients machine so fortunately for us we are looking at this from a contra providers point of view for example Yahoo or Google could do this and what\u0027s unique about them is that they control both sides so this is an in-house CDN and their own player and once you have mu into the entire path of the video delivery then you can find problems everywhere and see exactly what happened for each chunk that you had buffering so our approach relies on three principles one is n - an instrumentation we instrument both sides of this path including the player and is CDN servers and this is important for us to join this together to be able to construct the life of a chunk but and where the instrumentation unit is pair trunk because a chunk is a unit of decision-making in this system for example the bit rate that the replica algorithm chooses may change from chunk to chunk but not within a chunk and whether the file was a cache hit or miss at the CDN also may change in the granola tea or the chunk we did consider subject measurements there are two reasons that we did not go with it one is that it is too expensive and this is a production player CD and this is not an experiment experimental player or CDM and once you start using too much CPU on the player you can have a negative impact on the QE and the users may start seeing the frames may get dropped and things like that but also going sub chunk it and doing sub check measures can sometimes be technology dependent because the way that gm5 handles data arrivals different from flash and we wanted the results to be applicable to all internet video and finally collecting TCP statistics the statistics that we are collecting here they\u0027re sampled from the CDN host kernel there are some limitations here as well "
  },
  {
    "startTime": "01:01:52",
    "text": "this is an operational and large-scale setting so there\u0027s a frequency that we can collect these a prepare this information is not high so what do we mean by this end-to-end parent measurement so the boat lines here are where we are measuring things directly and the dashed lines are aware we cannot instrument things directly so observations based on interference and the life of the charm starts with player sending an HTTP GET request writes at the CDN the CDN has some processing time and we showed that with d CDN if this was a cache miss and back-end needs to be involved to get the first byte then there\u0027s a back in legacy everything that is not measured directly is shown with read and everything that is measured directly in this instrumentation shown with blue and then finally the first byte arrives at the player the time difference between when de should we get request was sent and when the first fight arrived is shown with TFB first by the way and there is also the download second I can see shown with red DDS and similarly when you have the last part of the chunk of writing that\u0027s the time difference between the first and the last is shown with DL you\u0027ll ask why and this is important because this is going to come up for different kind of studies that we do so before I show you our results I want to point out that we are studying QE factors individually in this talk the factors that we choose this there is this is a very rich area of research and they have shown that most important QE factors that impact the users experience our video startup time the buffering rate with your qualities such as betrayed and framerate and we look at these factors individually instead of coming up with the QE score because of two reasons one is depending on the type of content that you have some of these factors may matter more than the others for example for a breaking news video start-up time matters more because the user just wants to see the news whereas when you\u0027re sitting to watch a long movie the start time we do not matter that much or give you a quality matters more and second is the length of the video shorter videos users are usually less patient you want to have the results you want to have so you want to start playing right away whereas with the longer video the user is already patient they have it in their mind that they\u0027re gonna sit and watch to our video so this is the outline of the talk we\u0027ve already gone through the introduction I\u0027m gonna show you the measurement data so that we have and then what are the problems that we found in server-side network or client-side and then we\u0027re going to conclude the talk with takeaways on what can be done about these problems "
  },
  {
    "startTime": "01:05:00",
    "text": "so today\u0027s that comes from Yahoo videos they\u0027re played over a variety of sites for example in news sports finance and like I said this is this is the real player that was used in a variety of sites our dataset comes from 18 days this is a video on demand data of 85 CDN servers across the u.s. were instrumented selected randomly we studied 65 million video sessions and more than half a billion video chunks the users are predominantly in North America over 93% and mostly non mobile users that are not using proxy we\u0027re going to more details in the paper and how we remove users that we think are behind a proxy and the main reason for that is we want the TCP measurements to reflect the path between the server and the client and usually the proxies terminate the TCP connections so it would make our results not accurate in terms of the video streams distribution this is a popular heavy workload with 66% of the requester I mean for the top 10% of the titles and most of these videos are shorter than 100 seconds so let\u0027s dive into the first category problems the server side problems so our measurement in the server side comes from direct measurement so at the player we have such an ideal chunk ID start up time to buffering in video and at the city and we have such a tiny chunk ID similarly we are measuring server latency back in latency and cache Pyramus so because we have this data readily available we can show for example the immediate impact of several latency on startup time there is no interference there it\u0027s just ground truth so for example this is one graph that we can do because we have data from both sides and you can show the impact of several agency instead of time the x-axis on this graph is the several ACC milliseconds the y-axis is the start time of the video in seconds and you can see how it significantly increases with higher server latency so next we are interested in knowing why do I have these servers with these high latency the first issue that we found is the Apache traffic server Vishal timer and cache misses so what happens when an HTTP request arrives at the CDN is that it first inspects the memory and if it\u0027s not there you go to the disk and if it\u0027s not there you go to back-end however you don\u0027t want to overwhelm the back end so when multiple requests come for the same content there\u0027s usually a timer there that stops you from going to the next yearfor for a while to not overwhelm the backend and be fine and we found this miss configuration that was that was still in true between the memory and the disk and it was impacting about 65 percent of the chunks in our study but even more important than that is cache misses we found cache misses in this system increases their relation significantly the median increases by 40 times an "
  },
  {
    "startTime": "01:08:02",
    "text": "average by 10 times when your server side has a cache miss and we also found these extreme cases where several agency was worse than Network and for those sessions they were often caused by cache misses the average cache miss ratio in this data set is 2% for the sessions that had more server latency than network was 40 percent so it is often caused by this significant impact of cache misses on the server latency another interesting thing that we found is that server side problems are persistent it means that once the session starts having server-side problems its persistent it stays and for example like I said the average cache miss ratio is 2% if you look at the conditional probability of the sessions that had a 1 cache miss the cache miss ratio for those sessions goes up 60% that means that cache misses are coming in groups and that\u0027s usually because of unpopularity of the title so once a title is unpopular its chunks are more likely to reside in disk or worse gate work it in the backend and you start having cache misses but every single one of these chunks are going to go through that and in fact we found interesting paradox in the system that it seems like more heavily loaded servers seem to have lower latency but this is a result of the Cashbook is mapping cache focus mapping is you\u0027re trying to have hot caches so survey is sending the request to where it was recently served from which causes your popular content to go to the same servers and your popular contents have better performance because of the reasons Asian nation but you also have more requests for your popular content so there\u0027s this there are these servers that are less have less demands because they\u0027re serving unpopular content but have more performance so the Connie popularity is even dominating a server load in this case so the next category of problems are network performance problems so we\u0027re all familiar with network problems that can manifest themselves in the shape of packet loss reordering high latency variation of latency and so on and they can be transient or persistent it\u0027s important to notice and and separate these two from each other from video QE perspective because video does have the adapter picture algorithm that you may be able to adapt to some transient problems but you can\u0027t avoid bad QE forever when their problems are persistent and the way to fix those is by CDNs or ice fees actually taking corrective actions for example fixing peering so our network measurements come from us instrumenting "
  },
  {
    "startTime": "01:11:02",
    "text": "or host care and also basically the orange box here is showing what the operating system is doing which is there\u0027s this TCP infrastruc and os is collecting we have this information about all the TCP connections including a weighted average of RT T\u0027s called smooth Artie T or SRT T congestion window and packet retransmissions and what we do is the blue box that is we pull this every 500 milliseconds per trunk and store it and then later we use it across chunks and across sessions to see what happened in a network of course there are some challenges in collecting in this way we\u0027re looking at smooth averages of Artie T or s RT T\u0027s instead of individual RT C\u0027s and that\u0027s not sometimes a good idea especially if you are dealing with these long connections for video streaming because the SRC C is not reflecting your RT t during this chunk exactly but what happened in all the previous chunks this network snapshot frequency is also infrequent 500 milliseconds in many cases is more than the RT t of the connection but it comes from operational limitations and how often we have you were allowed to pull this and how much data we were producing in terms of storage overhead and finally because this is it this is it operational at scale we can\u0027t collect packet traces and in the paper be going to more details and how we grapple with these challenges but I\u0027m just going to show you they\u0027re interesting findings so here\u0027s another similar graph that you can do because we know that a network latency and for example the SRT T of the first chunk and we also know the set-up time so if you look at the SRT T of the first chunk that\u0027s the x-axis here and the impacts on the start time of the video the impact is visibly clear and how it impacted their is a significant impact on the video startup time so let\u0027s dig in here and see what causes this network latency to be so high so to do this we are aggregated akiko fixes the client IP prefixes in 2/24 and we looked at them ones that are mostly carrying in a 90th percentile every day so we\u0027re looking at the tail of a tail and among those we found 75% of them to be located far from our CDN so geographical distance or propagation delays the main cost but surprisingly about 25% of them are located in the US and majority of them are actually close to our city ends and but they further investigation and we saw that there there\u0027s also a high latency variation which is a little bit different from a high baseline it\u0027s just the variation and in both cases we find that majority of these prefixes are in enterprise networks not residential networks and what we speculate is happening here is that these enterprise networks are running their middleboxes which are causing high latency and high latency variation "
  },
  {
    "startTime": "01:14:02",
    "text": "despite the fact that they\u0027re so close to the city ends now of course we do not and we do not have in network data to confirm it\u0027s just a speculation of why we see this high latency in only in enterprise networks and not residential ones the second finding is the impact of packet losses this is this graph is generally what we expect to see in terms of how your transmission rates or losses impact we do QE in terms of your buffering rate generally we see that higher loss rate indicates higher rebuffering rate but that\u0027s not all and in fact we saw I\u0027m sorry the timing of the losses are more important than the amount of your loss rate and this is important for us as a measurement community to be aware of the fact that a loss rate measuring loss rate does not necessarily always correlate with QE and the location or the timing of the losses matter sometimes more so in this graph we\u0027re showing you the x-axis the chunk IDs zero being the first chunk and the y-axis is showing the percentage chunks that had rebuffering and the blue graphs are showing that amount so what\u0027s the percentage of the chunks that hadley buffering and it is higher for first chunk and you may think because of the loss so to take that into consideration we calculated the green wore the green plots so those are the conditional probability of percentage of chunks that Hadley buffering given there was a loss at this chunk and you can see that if there is loss the percentage of the repo Franco is higher for everyone but it goes significantly higher for the first channel so this is because of the existence of a buffer in a video streaming fashion so when you have a buffer in a video streaming session it initially it\u0027s not full so it cannot hide these impacts from the user but later on in the video sharing session when you\u0027re on a haier chunk ids then there is enough buffer to hide some of these impacts from the users so here\u0027s an example case he\u0027s a very extreme case so here we\u0027re looking at two sessions red had rebuffering and green did not have any buffering and you\u0027re also looking at the distribution of the last race this is rich transmission rate in chunks and you can see that the red session that happy buffering has actually generally lower lower row three and the green had significantly higher rate transmission rates but because they happened after the first four chunks which is already a buffering twenty four seconds of video because each chunk is six seconds the user actually never finds out about it whereas the red session is very unlucky it has losses in the first truck and right there it has a deep offering so here\u0027s an example of how the location of the losses seem to be even better more than the loss rate "
  },
  {
    "startTime": "01:17:03",
    "text": "unfortunately we found earlier packet losses are also more common so you saw they are more harmful but they are also more comment this is because of the bursty nature of packet loss in the tcp slow-start but we\u0027ll talk about how some servers like pacing can help but this graph is showing you this graph is showing you to arbitration especially a chunk ID you can see this huge difference in chunk IDs zero versus others and finally we found throughput to be a bigger problem then latency for media streaming so here we have defined a performance score which is a direct charge generation in our case it\u0027s six seconds divided by a first black plus last byte and the first byte is a measure of latency if you remember from this graph first plague was the time difference between when - we get request was santander in the first boy drive so it includes the network latency and the serial agency and if you have back-end if you have a cache miss also back in latency and the last point delay is basically a measure of three point because the difference between the first and the last byte of this video so we have divided the chunks in our data set into two sets based on if this performance scores more than 1 or less than 1 when the score is more than 1 that means more than one second of video is delivered per second to this player and from very simple queuing algorithms if you have more than one second video delivered to your player but the user is watching one second of video per second you\u0027re gonna build up buffer and when the score is less than 1 that means less than one second of video is delivered to your buffer your users still watching one second of video per second and that means you\u0027re depleting the buffer expected to have a rebuffering at some point so once we divided our chunks into these two sets we found that the contribution of the last byte delay in the bad trunks was orders of magnitude higher than latency and this shows to us that throughput seems to be having a big contribution in these chunks that do not arrive in time third categorical problems are within the client so what is the client download stack when the chunks are arrived from the network and before they are delivered to our player they go through the download stack at the client which is the neck the OS and the browser before they\u0027re finally handed over to the player and unfortunately for us at scale we cannot observe the download stack like you can see directly because we can\u0027t go and instrument these clients and see what\u0027s happening at their browser or network card so here we\u0027re relying on detecting outliers so here we\u0027re doing some "
  },
  {
    "startTime": "01:20:04",
    "text": "statistical work to see if there is a chunk that seemed to have been buffered at the download stack so if a chunk has been buffered at downloads that I expect it to be delivered late to the player and that means that I\u0027m expecting the first byte to be significantly higher than others so let\u0027s say it\u0027s two Sigma away from my mean but also because it was buffered at the download stack and delivered so late to the player I expect it to be arriving at the machines throughput not at the connections throughput so it\u0027s gonna have very high stint in es throughput from the perspective of the player so let\u0027s say it\u0027s 2 Sigma away from the mean again but it should not have been caused by network and server so it should have similar network and server performance so here\u0027s one example that we found in our data set the graph on the top shows you the latency matrix and look at chung-kai d7 and you can see that it has similar RTT and several metrics server latency but it has a significantly higher first byte delay than the rest of the chunks so what we expect this happening here is that this chunk was buffered at the client down the stack and deliberate late to the player and then the throughput measurements are at the bottom you can see the blue line is connection throughput and we know the connection throughput because we know the condition we know an RTT and the red line is showing the download throughput from the perspective of the player and you can see that the same chunk seems to have a significantly higher instantaneous throughput almost four times more which disconnected is not possible in this connection as we are approaching this so it\u0027s important to be aware of these problems that we can only find with once we have data from both sides because if you don\u0027t have this each side is going to blame the network or the other for these problems and this can be very dangerous in the video streaming world because if your player makes incorrect assumptions about for example agency from their first graph and if the adaptive bitrate algorithm is latency sensitive it can cause undershooting because it may freak out and think the latency had a spike where reality it didn\u0027t or if your player is sensitive to throughput and it\u0027s using the lower graph then it may think the world is so much better than it actually is and it can cause overshooting in it after PA algorithm so it\u0027s important for the player to have some view into the network path that in our system the player didn\u0027t because we only had these TCP measurements at the server so we found these the adonis-like problems they\u0027re transient problems that are that I just described to you you found them to be more common in their first chunk and in debate where we going to more details of how this could be a possible side effect of Flash and we also have persistent down low psych problems the persistent download psych "
  },
  {
    "startTime": "01:23:05",
    "text": "problems are not very common but what\u0027s important about them is once they happen they they often are the most important factor and they are higher than network and server latency and there\u0027s more in-depth conversation about them in the paper if you\u0027re interested in that and finally the last category of problems are the rendering stack performance problems so what is rendering stack so the chunks have finally arrived at the player but they\u0027re not ready to be shown on the screen yet and here we have a rendering stack these chunks need to be D MUX meaning the audio should be separated from the video and then they have to be decoded and finally render and this can be done by CPU or GPU if it is done by GPU it\u0027s called Hardware rendering if it\u0027s done by CV it\u0027s called software rendering now if the CPU is busy and you\u0027re using software rendering the quality may drop which causes high frame drops but also if your video tab is not visible your browser\u0027s do optimizations to reduce your CPU consumption so they intentionally drop frames so to detect these problems but to separate them from browser optimizations we have introduced boolean variable which is this player visible or not and also we\u0027re measuring the drop frames and then for each session we are also collecting what OS and browser it has and there are some interesting findings here first one is that good rendering is actually time consuming we found that the multiplexing decoding and rendering takes time and you have to provision for that if you want to have a good frame rate and so in this graph we\u0027re showing you the download rate of the chunk the average download rate of the chunk and x-axis and what\u0027s the percentage of the drop frames and you can see the drop up to about one and a half second per sec and that\u0027s what we recommend to use as a rule of thumb please in this system for avoiding having higher drop frames next we found kind of a paradox again higher bit rates showing better rendering so higher bit rates put more load on CPU and as I explained to you a software can be expensive so we\u0027re expecting them to have worse frame rate but we actually see them having better rendering framerate and if we need further investigation you can see the numbers in the paper but in summary we found these high higher bit rates coming from connections that had lower RTT variation and lower assurance machinery so they were often requested in better conditions and as a result they had they were able to provide the one and half second per second rule more often and had a lot of extra time to process this and the impact was hidden from the user and finally we found some very unpopular browsers that had a really bad rendering so here we\u0027re looking at chunks that had already good performance so the rate the "
  },
  {
    "startTime": "01:26:05",
    "text": "arrival rate is more than 1/2 second per second and we know the player is visible because we\u0027re filtering on the only these chunks that have visibility so the user is actually watching and we divided the chunks into two major platforms Windows and Mac and this blue guards are just showing you the percentage of the chunks in each platform for each browser and they\u0027re sorted from more popular to this popular and here you\u0027re looking at the percentage of the drop frames in each of these browsers combinations you can see the trend is the opposite and we found so in the paper we further break down the other category which are the least popular each in each one of them and there are some interesting cases there that have really bad rendering despite the fact that we have made sure that ever I\u0027ve already is good and everything else is similar one of the interesting examples are Safari is actually really good at Mac but it\u0027s among one of the worst on Windows and on Windows other section there\u0027s there\u0027s some less popular browsers like Yandex and see monkey that we found that had huge problems in terms of rendering so I walk you through all these problems but let\u0027s see what are the takeaways I mean what can we do about all of these problems that each one of these places so let\u0027s start with the CDN I discussed three problems with you about the CDN one is the impact of cache misses so in this workload we\u0027re looking at popular heavy workloads and I told you we are using the LRU cache eviction policy because of the impact of cache misses that are so high and the workload is popular have you proposed using other policies that are more tuned for these kind of workloads like GD size or perfect all of you in terms of the cache miss persistence we propose prefetching subsequent chunks this is the problem that I discussed with you that once the session starts having cache misses it every one of its chunks is gonna have the cache misses because it was most likely an unpopular title now it\u0027s not very it\u0027s not that simple to just prefetch the subsequent chunks because in many cases the CDN does not know what bit rate is going to be requested in the next chunk but that\u0027s a whole other area of research but it talked more about it if you\u0027re interested for the low latency paradox that I explained to you we propose better load balancing we\u0027re partitioning the popular content if you remember 66% of the load was for the top 10% so even if you balance that better you can get better utilization of the servers the network category of problems disk I discussed for problems with you there there are more takeaways and problems in the paper if you were interested but the first problem was we found some nearby clients with high latency it\u0027s important to know this even "
  },
  {
    "startTime": "01:29:05",
    "text": "as a CDN because sometimes you want to do you want to know what you can do but sometimes you should be aware of what you shouldn\u0027t do so in this case you should avoid over provisioning servers for these near about clients because the problem is not that they\u0027re far from you or of presence and in case of prefixes that have persistent high latency or variation as a content provider the options are limited what they can do about this problem but the least that you can do is adjust adaptive algorithm accordingly for example you can start with the more conservative bit rate or increase the buffer size to handle these legs you see variations better in terms of the earlier packet losses that are more harmful and unfortunately more comment I discussed using server-side pacing and finally the throughput is a major bottleneck we think that\u0027s actually good news for ISPs because it\u0027s easier to fix by establishing better peering points their latency the takeaway is on the client I discussed the download secretary teensy problem with you and we think that\u0027s an important problem that we could only find it because we had data from both sides and we could confirm that this problem is not being caused by the server or the network and I talked about how that can be dangerous for that appetite algorithm we can cause overshooting or undershooting and what we propose here is incorporating some server-side TCP metrics or some awareness of the network path to the player and I have these discussions with the path of our networking folks yesterday and the second problem is that rendering is resource heavy so you should provision for that we propose using 1/2 second per second video rubber raid as a rule of thumb this is particularly important if you are streaming videos that people care about frame rate for example sports that there\u0027s usually that one frame that has whether or not it was a thought and finally rendering quality differs based on OSF browser and I showed you some example browsers that even in good conditions and similar network and server conditions they seem to have worse rendering quality it\u0027s important for a content provider to know this to avoid premature optimizations for example rerouting this client while the problem was that the clients own down the stack is not helpful so it helps Citians a nice piece to know where when the problem is the client to avoid premature optimizations there so in conclusion this in this work we instrumented both sides for the first time the client and the city and server and it allowed us to uncover a wide range of problems for the first time we have data per trunk and per session which allows us to uncover where their problems are persistent or transient and whether they lasted throughout the life of this video streaming session or not and our findings were used to enhance performance in Yahoo and with that I am "
  },
  {
    "startTime": "01:32:06",
    "text": "happy to take questions we\u0027ll take about five minutes of questions hi my name is Stuart Cheshire from Apple thank you for a really interesting presentation as you said most of the traffic on the Internet today streaming video so clearly something people care about and I think we\u0027re all frustrated by seeing that little spinny wheel waiting for it buffering I had one comment you talked about client download stack late nurses and working for Apple that\u0027s the area of this that I\u0027m more involved with I think I may know what\u0027s going on here so I was happy to see Safari scores best so thank you so much no you were talking about in the pipeline the various delays in the CD and the network and you when you\u0027re talking about the client site download stack right ah as far as I know there is no networking API on Windows or Linux or Mac that will just sit on data for no reason and not deliver it I think this is a consequence of the api\u0027s in order delivery of data yes if you miss in a common Network setup now unfortunately we have lots of buffer bloat so you could easily have a two second queue on your cable modem link and you lose one packet and far we transmit fills in that one packet really fast but it\u0027s at the back of a two-second queue so you\u0027ve got all this data arriving piling up in the kernel the sockets API can\u0027t deliver it yes and then the one missing packet arrives and it says here you go here\u0027s 200k all at once and of course that looks like this blip that you\u0027ve had zero throughput for a couple of seconds and then suddenly 200k delivered in 0 nanoseconds is infinity throughput instantaneously so we have definitely seen that with things like the Apple TV products that we get very we get plateaus of no data at all and then suddenly a spike where it apparently all arrives because the one missing packet got that yes I completely agree with you actually we have more data in paper that we confirm what you\u0027re saying because we collected like timestamps at the knick and you collect the timestamps at the browser and it\u0027s usually the API the problem here is that our player sitting on their black box right and we can\u0027t it\u0027s probably the way that they\u0027re handling like and there\u0027s a buffer there and how they\u0027re handling the data delivery that causes these problems but because it\u0027s a black box and it\u0027s like a footnote in the paper "
  },
  {
    "startTime": "01:35:06",
    "text": "that we can only guess this is what\u0027s happening and we confirm it is not at the OS for the browser it seems to be there but we can\u0027t really measure what\u0027s what they\u0027re doing in that API I think you have a good point there with the current api\u0027s you can\u0027t tell if there was just no data or if there\u0027s lots of data with one missing packet exactly yeah good point our West heard occur USC is I am excellent presentation and work very thorough and I really enjoyed it having said that you killed my dream and I\u0027ve had this dream that that with the advent of you know users can go out and find the things that they want and and it would they had helped us discover new things unfortunately sort of your caching results kind of indicate that video streaming services are it\u0027s in their desire to bin everybody into popular titles where you know they they won\u0027t give you as many suggestions sideways of other things that you\u0027re interested in they\u0027re more likely to give you suggestions that everybody else is gonna watch too because it\u0027s cheaper for them give some better performance and thus you know better ratings and that that\u0027s sad but thank you yes that\u0027s exactly that\u0027s true and I think everybody does that hi this is Tom I just wondered whether the system is intended for VOD or live streaming or both thank you so the data set that we use here are from the video on demand the data set that I discussed but the same systems are used like the same CDN and the configurations that we found they were applicable to the live streaming events as well okay thank you all right this is this is from the jabber room on Simon Pietro Romano asked if I be curious to understand whether all of this has been carried out with flash-based clients in such a case is there any updated study with MPEG - yes I think we mentioned ok the codec yeah ok okay I\u0027ll be a symbol for last fall okay come on up do you want to try using your own machine that\u0027s probably safer yeah oh this is yours here we go all right change of topic but not really change of area so you want that in there\u0027s your clicker oh these nice new Macs okay oh there\u0027s no signal up looking better yes okay yeah okay so "
  },
  {
    "startTime": "01:38:11",
    "text": "introduce yourself this is our second day in our piece speaker hello hello everyone good cool okay so good morning everyone my name is Vasco I\u0027m a third year PhD student at the University of Michigan Ann Arbor this work is originally appeared it is the sitcom 2017 last August so today I will be presenting room a new solution to optimize web performance this work is a collaboration with Ravi Mohammed and my advisor Harsha so let\u0027s get started so as you have experience using a mobile phone connected to a cellar network it\u0027s very very common nowadays right we have our used phones to surf the web all the time now I study have shown that this is in fact the case mobile web usage has been increasing over the past few years the year 2016 was the tipping point where mobile web usage has surpassed desktop devices usage to browse the web but despite all of this increase in mobile web usage as you may have speery ensure self-loading many of these pages are actually pretty slow no systems founded it takes almost ten seconds to load the median mobile retail site and on the other hand double clicked founded it takes 14 seconds on average to load a page over a 4G connection so we also confirmed this ourselves using a nexus 6 phone a reasonable high performance phone at the time connected to a good LTE network in the Ann Arbor area to load the mobile optimized popular pages so one thing to note about the results that we found here is that these are heavily optimized popular pages so the numbers that we get here is on the better side of things so this is a bar chart representing the page load times measured in seconds of the Alexa top hundred sites overall and Alexis how 50 news and 50 sports sites the top of the bar chart is the median page load time and the whiskers are 75 and a 25% house at the median page load time for the Alexa a top hundred sites overall it takes five seconds to load the median page this is actually pretty slow considering the fact that some studies have shown that a five second page load times has 25% bounce rate which means that these pages are actually losing some money right and on the other hand if we take a look at the Alexa top 50 News and 50 sports sites things are far worse the median page "
  },
  {
    "startTime": "01:41:12",
    "text": "load time in this case is actually 10 seconds and the reason why things are much worse in this case is because news and sports sites tend to be more complex than the Alexa top 100 sites overall so in this talk I will be first digging into why web pages are slow to gain some intuition as to why that\u0027s the case and then we will use those intuition to improve web performance then I\u0027ll use the intuition with that we get from the first place to explore room our solution to make web page slow faster and the last part would be the implication of workroom now let me take you to into why webpage is slow now let\u0027s take a very simple example let\u0027s say we load one to load a page from a calm and this a calm contains only one image so what would happen when this page gets loaded is the client will send a get request to a dot-com era comes in smack their response find parts that discover the image and then a facial image if you take a look at the network utilization and also the CPU utilization at the client as you can see here the bars with the solid colors are times when these resources are being actively used as you can see here there are no periods that the CPU and the network is being fully utilized and the crux of the problem here is that the client has to parse or execute a resource to discover additional resource to fetch so in some instances CPU it will get blocked by the network waiting for a resource to arrive so that it can start parsing and on the other hand sometimes the network will be idle because it needs to fetch because it\u0027s waiting for the CPU to process some resource so the idea that we want to make page load faster is by making either the CPU or the network to become fully utilized but before we jumping into how we can do that we first have to understand whether the CPU or the network is the main bottleneck here so what we did was we conduct an experiment to find the page load times when either the CPU or the network is a main bottleneck in order to find the page load time when the CPU is the main bottleneck we use the phone connected to a wired network so that we have a near zero latency and also almost like an infinite bandwidth and on the other hand if we want to find the page load times "
  },
  {
    "startTime": "01:44:13",
    "text": "for when the network is the main bottleneck we remove any processing from the page load process as you can see here from the results the page load times when the CPU is the main bottleneck is actually much higher than the page load time with Network is the main bottleneck so in this case the medium page load time is 5 seconds in the case where the CPU is the main bottleneck so that experiment implies that CPU is the main bottleneck in most of the cases but is this actually the case everywhere sure there may be cases where network can be the main bottleneck for example a developing country where they have 3G connection a much slower connection than 4G sure network can be the main bottleneck in that case or on the other end of the spectrum if you\u0027re using a flagship phone maybe your phone is much faster than a network so the network is the main bottleneck but if you consider it a technology trend network is getting faster and faster every year right network will have higher bandwidth and also lower latency but on the other hand CPU is only increasing at a number of course the clock speed itself is not increasing but how is this trend affecting page load times so what we did was we also perform the same experiment but instead of using all horse in the page load time when the net CPU is the main bottleneck we disable one of the horse to see what is the effect of this and what we found was that by disabling one of the horse the distribution of the page load times went and CPU is the main bottleneck it\u0027s very similar to using all the horse and this actually stems from the fact that browser processing is largely serial there\u0027s one main process that keeps doing everything there\u0027s not much leverage in multi-core for multiple horse so the implication here is that the CPU will become the main bottleneck in the long run so just to recap what we found in this first section of the talk so the reason why web pages are slow right now is browsers needs to discover resources from parsing and execution and we know that browsers are largely serial in discovering these resources and performing the page load and with the CPU becoming the main "
  },
  {
    "startTime": "01:47:14",
    "text": "bottleneck in the future this process of discovering resources from parsing and execution will not become any faster so we have to somehow think rethink the way page load should work so our main idea in our project is to somehow have the server to become more proactive during the page load we want servers to eight clients in discovering resources during the page load and that\u0027s the main theme of room so now that we know why paper web pages are slow and gain some intuition I\u0027ll ask you how we can make web pages faster let me walk you through room our solution that uses that intuition to make web facial faster this is a very high-level architecture of room so we have a lion a mobile client a web browser unmodified and now we also have these web servers like what we used to have like right now the clan can start by sending a get request to phu kham foo.com since babba hg responds none of these are modified everything the same as the status quo right now but instead of only sending back HTTP response broom also used SGP to push to push resources down to the client so that client can receive many resources before it needs to actually fetch it I discovered them from parsing our execution but push itself it\u0027s not really enough because HTTP push only allows you to push down resources that the origin owns right so but we know that a lot of these pages contain third-party resources so we are missing out on a lot of resources to make web page load faster so what we also include in addition to HTTP to push is also used some kind of dependency hints resource hints one way to do this resources is to use link rel preload HTTP primitive where the Kleinman see this link rel preload it can start the fetch of that resource as soon as possible now in order for the server\u0027s to push or send these hints back to the client the server has to have some kind of module to discover these resources right so we have these dependency resolution module running at the web servers so these dependency resolution modules are just simply trying to find resources that the client will need during the page load and at the client we have some kind of "
  },
  {
    "startTime": "01:50:14",
    "text": "scheduling mechanism so that the client can use all these resources as effectively as possible now that we know an end to end work for all of room in order to make room our reality we have to answer these two main questions so first how web servers can discover dependencies in the first place and on the other hand how clients can schedule these fetches of resources or use these hints from the server effectively so that it maximize the benefit that it should receive so let\u0027s first turn our attention to the web server let\u0027s consider this strawman approach in discovering resources for the client so the client can send a get request to the origin write this web server foo.com can start a page load a foo.com itself at a route at the web server now because food comes web server is a server it has a much more powerful CPU and also a highly connected network so it can perform this page load much much faster than the client so what foo.com can do is it sends back the response as well as push everything and also give hints everything on everything that it discover during this page load all this sounds pretty intuitive and hopefully it works unfortunately it doesn\u0027t so there are two drawbacks with this approach first as we all know web pages by nature is very dynamic there are a lot of resources that is dynamically generated with some randomized token in the URL and so on so by using everything from a one particular load and hint them to the client or push them to the client it means that we are pushing or hinting something that the user will not use and by that we are first going to waste clients bandwidth next even worse is that by having the client sending fetching these resources we might be delaying some important resources which make the page load slower in some cases now on the other hand as we all know many of these pages contain many personalized resources so in order for food calm in this case to account correctly account for personalization it needs to get a hold of the clients cookies they\u0027re party cookies but we don\u0027t food calm doesn\u0027t have that so food calm will never be able to correctly account for third party personalization so how can we overcome "
  },
  {
    "startTime": "01:53:16",
    "text": "these two drawbacks so what we did was we use an intersection of offline loads to overcome the flux in URLs so at the web server we load a page periodically and then take the intersection of these loads so it means that anything that is randomly generated per load will be filter out by the in because of the intersection but this is not enough what we found was that by only doing this intersection of offline loads we are only able to discover 70% of resources that can be discovered so what we did was we augment online parsing of the HTML on top of intersection of offline loads and what we found was that using these two combined online offline resource discovery we were able to discover most of the resources so now that we have a way to discover it resources at the web servers let\u0027s turn our attention to the client and see how the client can effectively use these resources from the server so let me walk you through the very high-level architecture of room again so room sends a get requests to the web server web server uses sends back a response with the HP to push and also dependency hints uses the dependency resolution module to push and to get resources to push and also hints so one approach for this scheduling would be have to server just push everything that it could from the dependency resolution module and then for hints just use all link preload lingual well plate preload - hint all of them so what essentially this do does is the server will push all the bytes that it could to assert to the client and the client will fetch all hints immediately at the beginning of page load this sounds great as well because we are discovering resources much earlier in the page load and things should work well unfortunately it doesn\u0027t work well either and this is a pretty serious problem because by pushing and fetching everything in the first place at the beginning of the page load this leads to contention in bandwidth and when there\u0027s a contention in bandwidth sometimes the important resources are actually being delayed for example a blocking script or some CSS will get delayed and by and because of "
  },
  {
    "startTime": "01:56:17",
    "text": "those resources getting delayed it has a cascading effect throughout the page load and it can end up hurting the page load process so what we found in our experiment was that by using this approach we don\u0027t see any page load time improvements and even worse sometimes we see degradation in page load times so what we did instead is reprioritize pushes and fetches of resources that can potentially have children for example HTML CSS or JavaScript and one very important detail here is that we have to prop scheduled them or hint them based on the schedule that they will be processed we don\u0027t want to fetch any resources out of order so that because if we fetch things out of order some resource that gets processed earlier might stop waiting after some resources that gets processed later so now let\u0027s take a look at how room scheduler works in action so at the beginning of the page scheduler fetches all HTML Javascript CSS this can both be in the form of HTTP to push or link reload after all of this fetches is done it will start fetching other dependencies such as images fonts on in other words resources that will not have any children and that doesn\u0027t require any processing while these two fetches are going on we also allow the browser to parse the HTML CSS and also execute a JavaScript if you discover any resources during by processing these resources we also allowed them to go out now this red line in the timeline is a very important time in the page load process this red line is the time when all the bytes that needs to be processed at the client is actually local at the client already so what this means is that from that point on when the client wants to process any resource it can start processing that resource without having to wait on the network and from that point on this implies that the CPU can be fully utilized so now that we have the two components of room let me sum up room and then we can see how well room works compared to the current state of the page load so room start by Senate get request to the origin origin sends back there she P response pushes important "
  },
  {
    "startTime": "01:59:18",
    "text": "resources and also didn\u0027t provide hints of other resources room uses room has these dependency resolution module and also we use combined offline online resource discovery on the client side we have the scheduler which we schedule them by prioritizing pushes and fetches of HTML CSS and JavaScript so now that we have an end-to-end working components of room let\u0027s see how well room works so what we found was that brooms dependency resolution is actually very accurate and because of this room was able to speed up page load in many of the cases we have a hub of results in our paper but today I\u0027ll only be talking about how well room works over the stay as quote if you\u0027re interested in other results please refer to the paper before jumping into any numbers let me first tell you how we evaluate room so we used an ex SX phone connected to a 4G LTE network to a web record and replay environment so the reason why we need a web recording replay environment is because room requires server-side changes so ideally we want to use a live experiment but unfortunately getting adoption of all tests would be very very challenging now that we know how we evaluate room let\u0027s take a look at the numbers so using that evaluation setup we evaluate room on the Alexa 250 news and 50 sports sites and this is the bar chart representing the page load times measured in seconds as well the top of the bars are the medians and also the whiskers our 75th and 25th percentile page load times the status quo load is for this set of pages is 10 seconds like we saw earlier so when we enable step2 on all domains we saw that by only doing that we are able to take the median page load time down to 7.5 seconds but if we enable room at all domains we are actually getting doubled of that improvement in page load times and right now the median page load times is actually five seconds down from 10 seconds and this is in fact very very close to the lower bound where we defined a lower bound ratio times as when the page is either network bounded or CPU bounded now as we all know on load event is not the ideal metric to use to get user "
  },
  {
    "startTime": "02:02:18",
    "text": "experience right so we also evaluated room on visual metric so if we evaluate room on the above the fold time which is the time where all the objects appear on the screen and this is the bar chart representing the above the fold time measured in seconds so the status quo load takes 12 seconds to load at the median site and when we use broom broom was able to improve the page that above the fold time from 12 seconds to 8 seconds so that\u0027s a 4 second improvement in above the fold time so now one assumption that we made in all of our evaluations so far is that we assume that everyone adopts room but as we all know adoption is challenging so what we did as well is we evaluate room when room is incrementally deployed so in this example here broom is an enable at all the domains right so what we did was we consider first party domains so when I say first party domains let\u0027s say we\u0027re a SP n com yes PN comm also owns ESP and cbn.com right so we also consider ESPN cbn.com also part of the first party domains because we assume that if they are going to if say ESPN are going to deploy room they might as well deploy room everywhere on their domains so what we found in this setting is that most of of benefits that we saw in the in the case where we enabled room everywhere is actually still achievable from only enabling room at the first party domains and that means that we actually don\u0027t need full adoption to make web page look faster so now that we know how room works and how well it works let\u0027s turn our attention to the implication of room so broom leverages HTTP to push and also link rel preload to enable server-side edit resource discovery right currently every web site wants to use any of these it needs the knowledge of the developer to manually add h2 push or link preload into the responses but room shows that there\u0027s actually a systematic way an automated way that you can actually include these pushes and hints into the response but that actually comes at a cost as you if you recall room requires offline dependency discovery to construct a stable set of resources this "
  },
  {
    "startTime": "02:05:20",
    "text": "actually consumes a lot of CPU cycles and network at the server\u0027s imagine if you have thousands of pages running serving at your web server by doing periodic loads of these pages will be a huge pain so what we think that we could do is maybe the client can help the server discovering these offline dependency resolution so maybe we can crowdsource all these URLs during the page load that the client sees and then send it back to the browser you may thinking we you may be thinking right now that maybe this is violating some privacy issues so let\u0027s take an example so this is a dependency tree off a website in a very naive implementation you can send all these resources as a list back to a.com but this is obviously not good because let\u0027s take a look at z.com slash a dot HTML so that is an ad so anything below that can be personalized or targeted to the user so sending all this to a.com means that we are giving up privacy to of the user so what we could do instead is sending everything in this screen encapsulation and this is in fact enough for a.com to discover offline dependency resolution because a.com if you recall from the offline from the strawman dependency resolution a.com cannot discover personalized resources correctly anyway so by sending anything everything in this screen in caps encapsulation is actually enough and then anything below c-calm can actually be sent to just see comm and this is not also not violating privacy because see Tehama is the one serving the ad so sending you back to see comm would be fine another very important lesson that we found in Froome is that when doing these pushes and fetches of dependencies using link rel preload we shouldn\u0027t be fetching out these resources at the same time as we saw that if we do this we don\u0027t see improvement in patient or times and worse we are seeing degradation right so just to recap what we did in room was we group dependencies in different varieties so higher priorities are resources that require processing and then we perform fetches of these resources based on their priorities so maybe one thing one branch that we could do regarding these person prioritization of preloads is that "
  },
  {
    "startTime": "02:08:22",
    "text": "include some kind of priority to the link rel preload so that the browser knows that oh this preload is actually higher priority than some other reload so that the browser can schedule the load of these resources better in fact this is an on a draft already in the w3c community so I hope to be discussing this more with them but just to recap I think that having a priority would be a great addition so to conclude my talk I present at room an end-to-end solution that fully utilizes either the CPU or the network vrooom was able to do this by decoupling dependency discovery from parsing and execution by leveraging the server to aiding dependency discovery at the client because of this room was able to decrease the median page load times by five seconds for the popular pages and with that I\u0027m happy to answer any questions Alex may offer thanks for interesting presentations really really nice to see that someone actually tries that out in practice other than just an association of it I have a question do we employ any mechanism that you avoid pushing the same resource twice to the same client I\u0027m not talking about link reload preload because the client would decide whether it has that resource already but if you like push the CSS on each and every page that sounds wasteful or are there any technologies employed you mean pushing having push like size 8 a CSS twice in the same yeah so the user loads the home page get you see the CSS pushed then he clicks on a sub page she gets the CSS pushed again uh-huh are you doing anything against that or in this word I am not doing anything to prevent that because we have control of all what we push so we insured we\u0027re not pushing it twice does that answer your question yeah okay can you give us a little bit of an idea on these sites which have been optimized for mobile how many URLs are in a page and how many of those URLs actually changed the page and are not just the spyware so I know the figures for regular web pages and it\u0027s tremendous numbers of URLs I just don\u0027t know if people who are optimizing for mobile are actually giving up some of this overhead which was always understood to be you know people "
  },
  {
    "startTime": "02:11:23",
    "text": "basically we\u0027re designing web pages thinking that CPU and network is unlimited and that\u0027s not the case for a robot all right um so your question is for these set of sites I\u0027m looking at what are the like say the number of resources on top Asian so we\u0027re talking about hundreds of URLs or dozens or just yeah so what I found in these set of sites at least the number of URLs that I found a new set of sites or at least 50 for sure for news and sports sites it\u0027s in the order of hundreds so yes right which means that actually if the people serving these sites really thought of their user and what the experience would be they could cut down on this and maybe you wouldn\u0027t need your solution as much I\u0027m not saying your solutions not a great idea but if people instead with trying to force down our throats lots of advertising and spyware actually thought of the customer and his quality of experience they could solve this problem without needing more technology right yes I agree so since how from hallway and thank you for your talk and worrying system work and you know test set my question is a new test set we have any data reflecting how many website use the domain sharding technologies using what sorry I can tell me don\u0027t whistling yeah okay so number of sites that are using domain sharding most of them are using domain sharding at the time that we used now we perform these experiments hopefully that is less common if we move to sp2 but when I was doing this project like a year two ago I could see that business common okay but because your inside is that the CPU and either the CPU or network is only utilized but using domain sharding it\u0027s kind of a paralyzing parallelism and you can make better use of a CPU either CP or network right so I think that\u0027s a separate issue right now browsers are absolutely designed right now is that there\u0027s one c1 main process that is doing all these tasks sure you can have another process that can do like preload scanning but that\u0027s separate but the main thing that is happening is one in one process so by having that only one process you are able to only execute or deuce doing something waiting on the network on only one process so sometimes they see sometimes the browser will have to wait for some processing to happen in "
  },
  {
    "startTime": "02:14:23",
    "text": "order to utilize the network later in the page load anyway so domain sharding will not help in that case okay thank you hi you\u0027re Vice I\u0027m participating or leading like participating in the priority hints work that you mentioned I think it would be extremely interesting to integrate the concept of the JavaScript scheduler into the browser\u0027s resource scheduling process and in that context the red line you showed where you know which separated the critical resources from the non-critical ones do you think there\u0027s a way from the browser\u0027s perspective to determine that in a deterministic way that doesn\u0027t introduce a gap so trigger those non-critical loads in the ideal time seems like a complex problem how did you solve that I don\u0027t think I have a good answer to that on top of my head and the reason is because like you said it\u0027s very tricky because even though we say that rooms resource discovery is very accurate we are not claiming that we can detect all resources many of resources that say a JavaScript that has some kind of randomized token we still can\u0027t discover those so the CP so from a browsers perspective I think it\u0027s gonna be really tricky to say that oh these are all the resources that it will need to process and then yeah I I\u0027m sure we\u0027ll still need processing but just the the part that I\u0027m interested in is in determining in real time determining the redline determining the fact that everything critical or most critical things finished loading and we can start loading the nonverbals like defining that red line would be extremely interesting yeah I agree Thanks yep Thanks hi Alma Cherica I had a couple of thoughts one of which is did you do any research into seeing how many unnecessary resources actually were downloaded that you had pushed something in advance and then it turned out that the browser didn\u0027t need it I mean one obvious example would be high DPI graphics where you know the browser\u0027s deciding which resolution to download unfortunately I didn\u0027t do any study on that like what are the resources that are push and not being used so one thing to note here is that all of our experiments start from the assumption that none of these sites are doing actually doing any push or anything like that so what we did in our work was that say if in the best scenario if we try to "
  },
  {
    "startTime": "02:17:24",
    "text": "push everything that we know off and then we\u0027ll be used in the page load and also hint everything without having out thee the worry of and using what will be the best case that we could get I mean I guess another way to ask the question is how many additional bytes were downloaded using broom then were that would have been downloaded if the page had just been loaded normally um there shouldn\u0027t be more bytes right so sure there\u0027s certainly some more bytes in the signaling so there there will be more bytes in like saying the hints and those stuff um so I don\u0027t know the exact answer to that but I would think that it should be relatively low compared to the whole size of that HTML or the page and another thing that just comes to mind is this seems like a way that if we had a better way to organize our HTML and JavaScript in the first place so that the browser could identify that critical resources earlier in the parsing then perhaps this server-side trickery wouldn\u0027t be necessary but now I\u0027m telling you it would know so I\u0027ll leave you thanks Matt Mathis I sort of had a continuation of that thought it feels like this is very good work very cool stuff but it feels like you\u0027re optimizing it the wrong layer in the sense that the content providers should be optimizing better ahead of time and they\u0027re not for some reason and I was wondering if you had any speculation about some of the incentives for it things like domain sharding for instance fills me to me is an example of a technology that works the same way that excessive choice in the grocery store works it has the effect of crowding out competitors because you can provide 14 different flavors of chips to use for ten times as much shelf space and what this means is is it underlying somehow underlying some of the people have inappropriate ascent incentives and you\u0027re optimizing away some of their incentives but the real problem is they\u0027re optimizing against you did you look at any of the sort of the causes of why this stuff was done in ways that it was you could fix it in the post-processing so to speak so just to rephrase your question so basically so basically did I so what I\u0027m optimizing is actually Adam had maybe we should be optimizing at a different layer then and HP layer it feels like you\u0027re optimizing something that somebody else optimized towards a different goal I see yeah I guess maybe did when someone tries to optimize something they "
  },
  {
    "startTime": "02:20:24",
    "text": "have to keep in mind that they might be hurting someone else\u0027s performance and so we should be more inclusive between the two all right I think they were the ones being unincluded why do you choose Nexus 6 why not some newer phones so at the time it was a year and a half ago or two years that was the best one that we could get so you started experimenting yeah a while back okay thank you okay let\u0027s think I left the slot in case anyone had any questions or wanting to make any observations but we could also wrap up early we\u0027re gonna give certificates up to motion and and basketball now and honoring their their prizes and there\u0027s no reason why everyone has to watch that but but I\u0027m very grateful to you two for giving these talk I think they were really great and really thorough and inspiring so thank you and thank you oh I are TF open okay we have a observation from Dave Wonka oh I\u0027m hi Alison I wondered if we might spend a few minutes talking about or catching the I heard you have community up on what\u0027s going on or how er NW was formed the the research workshop first I want to say thank you and to the iStock people and they see and people to put putting that together the last time I looked at it we know we didn\u0027t have a committee there was no call and all that stuff so a bunch of people did a bunch of great stuff to get that in order and um in I guess in the spirit of sort of transparency I was wondering if you could tell us or one of them could tell us how the committee was selected what was the reach and diversity goal and did you meet it I see there\u0027s an invited talk that\u0027s one of the people that\u0027s on it\u0027s on the program committee that that\u0027s an unusual thing could you speak to that or who\u0027s responsible forward so either I think neither of the people the program chairs are here so it\u0027s really a better question for them but the and in fact I so the invited talk question I think is an interesting one because there\u0027s quite a bit of variation between committees and workshops as to whether that\u0027s a reasonable thing to do or not it\u0027s of the conflict of interest whether it\u0027s okay for people who are on the committee to also be speakers I think the reason they see that the invited talk so early was to try to "
  },
  {
    "startTime": "02:23:27",
    "text": "give a strong a strong representation of some of the topics in range with respect to the program committee I think it\u0027s possible that that committee could could grow some more so if we if you were to raise a question about its composition to to Sharon and Dave it would be timely okay I mean the overarching issue is I love the way that a nrw is different from the from the academic conferences and I love the idea of running it during IETF because we\u0027re already multitrack yeah ITF so that\u0027s gonna be a huge improvement and the competing idea that was floated last year was having an hour W being a continuing workshop where people could keep like solving the other problem at academia that you keep it\u0027s hurry up and then wait like you hurry up to submit your thing you wait half a year so I love us having it different than you like this but a couple of the things just looking to improve is we\u0027re still inviting now in a or W we\u0027re inviting already published stuff there we already do that for an RP so sure he had a venue to do that and and I think that we should like raise the bar for ourselves to say how did we select the committee and and things like you know that got that kind of stuff but I\u0027m really happy that it\u0027s all in shape already and it\u0027s looking really good the timelines that walk through a workshop in July we\u0027ve got a month to submit and they\u0027ve got two months to form it so it looks really good mostly I\u0027m happy with it if you care about it I think there\u0027s also I mean there\u0027s a hope that there will be a fair number of these kind of lightning talks they\u0027re short and I think it remains to be seen and part of what everybody can do here besides submitting yourself is to encourage your researching friends to to bring up papers I mean if everybody in this room submits something and gets a friend to submit what other amazing choice but I have had in mind that perhaps the committee should should expand with respect to the kind of continuing versus one-off once-a-year thing we\u0027re thinking slowly about the idea of a of a about a bunch of those types of ideas and I\u0027d like I\u0027ll get you involved in that conversation but also people in this room we\u0027re looking to try to increase the amount of applied network research that there is I mean like you\u0027ve heard to beautiful applied network research topics today and and also rods topic but many times there\u0027s a very big gap between academic research and what we could then do something and build in in the real world internet and see deployed so we\u0027re looking to just enhance those relationships in every way we can okay and that you actually hit on exactly the last thing I wanted to say was to remind people to solicit for it one carrot you can use for people one thing that\u0027s nicer about in our W that say IMC is the talks are recorded and we\u0027re having building this huge library of awesome presentations by those people so some of the students that I\u0027ve solicited to come to IETF format Margie or something to say they end up having "
  },
  {
    "startTime": "02:26:27",
    "text": "this talk that\u0027s on their you know web page afterwards where if somebody can see them in actually write stuff I\u0027m also asking IMC to come up to raise the bar for themselves to start doing that you should actually mention that you are one of the two chairs of the map RG Research Group the measurement and analysis for protocols so this is a very parallel case to the yet the rest it\u0027s sort of like a recursive the IRT F contains another IRT yeah that\u0027s just the measurement subset of well yeah it could be Eddie in our death yeah and then oh the last thing is the program committee I it looks really good it\u0027s like a there\u0027s a third women on it maybe there\u0027s looks like a lot of third of the people are ITF participating academics already so it\u0027s looks like somebody did a really good job but we should tell people how we\u0027re doing it okay sounds good okay anything else anyone wants to bring up before we send you off to lunch okay all right well thank you for coming see you see you on our mailing list see you tonight I\u0027m going to give another little bit of an overview tonight hopefully more successfully in the analog Department oh and there should be another blue sheet if someone can provide the other blue sheet that would be helpful "
  }
]