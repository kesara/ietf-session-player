[
  {
    "startTime": "00:00:04",
    "text": "back in not the same room smaller room mr. kiss lucky reminds me that people should fill this blue sheet out the blue sheets out so we make sure to get the right sized room next time it\u0027s searching 20 we\u0027re gonna start the meeting The Cider ops meeting if you\u0027re not here for cider ops you\u0027re in the wrong room for those presenting please make sure to talk into the mic like this hold the mic don\u0027t use the stand if you do if it\u0027s in the standard you\u0027re doing this I can\u0027t see here you either there\u0027s a nice thing in front of the presenters they can read that instead of doing this also very nice we have a yep you can laser point to it like you can see what I\u0027m pointing at I can\u0027t see what I\u0027m pointing it doesn\u0027t work on the LCD it works over here we\u0027re in cider ops in ietf 106 I am Mauro see and that is care we have a clicker that works okay we have a jabber person Natalie or no yes oh okay well let\u0027s see do we have a minutes taker or another jet and other person who\u0027d like to type on the keyboard during the meeting okay perfect we\u0027ll well delay Natalie can take notes minutes they\u0027re called strawberry man is here yes I\u0027ll make my attempt next time tonight hi there oh the other thing for the people that are presenting medico would like you to stand on the pink square Randy bush pink square you\u0027ll see when you get up here cross fine whatever it\u0027s a parallelogram it\u0027s seven parallel lines five of which are transparent okay somebody else will got that I\u0027m sure okay ready Joe it\u0027s coming okay okay okay okay good we\u0027ll just move ahead blue sheets for those who didn\u0027t get them earlier we could pass into the back there\u0027s a note well for the meeting I\u0027m sure that by this day of the meeting you\u0027ve seen this a few times it\u0027s in small print intentionally it doesn\u0027t have all the resources that\u0027s further on okay so today we have six presentations not counting this one hopefully will be done quickly with our part those times are approximate they probably don\u0027t add up to the right time I didn\u0027t pay "
  },
  {
    "startTime": "00:03:05",
    "text": "attention so strat draft statuses there\u0027s some stuff that we have to push into the editors queue out of working group last call there\u0027s at least two things I think here yeah yeah maybe a couple of more the last thing in iest processing is LTI use cases I think that\u0027s stuck for a long time and I don\u0027t exactly remember why it\u0027ll pop out eventually and the things that are probably the ones that need to go into the editors queue are things like OB egress I think somebody\u0027s complained about that I mean sorry I sent a message about that a couple the others here at will as well so we\u0027ll hear about those I\u0027m sure at the mic and afterwards we\u0027ll scrape through the land list and figure it out okay first person up is Tim [Music] almost a square almost [Music] which one is yours called alright so deprecate rsync why well first off as you know our sink is used in the RPI so is another protocol that was invented here RDP so the RPI repository Delta protocol for friends and what I\u0027m proposing here I I made a draft just in my own name it doesn\u0027t have to stay like that I\u0027m actually looking for some people I mentioned to me that they want to co-author as well but I wanted to get out so they get something out quickly so I did that but before going into all of that a quick reminder why why did we invent the Delta protocol there you can talk about this for longer but mainly in order to run an R sync repository reliably it\u0027s not that easy and with RDP you can leverage CDMS so the job becomes a bit more yeah less challenging I would say and also from the relying party point of view most software ends up calling the arcing binary on a system which may have different behaviors on different systems those would be my top reasons why I would want to get rid of it how well this might be controversial so it\u0027s here intentionally because I think we should have discussion about this pragmatically speaking I propose to keep the rsync your eyes even though a repository might not be available at that location and the reason is that if we update the certificate profile this can lead to things be becoming rejected and that "
  },
  {
    "startTime": "00:06:06",
    "text": "will make deployment much harder there is some prior art in an XML where namespaces are HTTP your eyes quite often and their main be anything there I don\u0027t particularly like it I think it\u0027s ugly but on the other hand we do need names for things for debugging and yeah I don\u0027t I\u0027m trying to avoid breaking things and that\u0027s why I\u0027m proposing us so but you know we can have that discussion also we could instruct relying parties to prefer our DP if it\u0027s available and well ultimately phase out our sync but when well there\u0027s a bit of a chicken and egg problem here I didn\u0027t find good pictures of chicken and eggs maybe I didn\u0027t look well enough but essentially this is the situation you have many repositories you have many relying parties and the relying parties talk to all the repositories and repositories will need to support our thing as long as can only drop support for our sink if all the relying parties support the alternative and vice versa our relying parties can only drop support for our sinker and all the repositories support the alternative and I think measurement should turn into that because if you look at the wrote there then Randy wrote a proposal actually that can work but I\u0027m not sure that we have to tie it on to that but to repeat what you said Randy and you can correct me later if I\u0027m wrong is that well you can start from the repository side you can say now you all go support our DP once that\u0027s done and you have the green button right here right so they all supported and you could say to our Peas well some of you support it most of you support it but please now all start supporting it and then you can measure with in recession measurements whether people actually deploy these new versions and at some point you can now say ok now the repositories can phase out parsing but in my view I\u0027m not sure that this should be a heavy segment of the document I think in the eventual document that final document that you probably not be there at all in my view because I think it should just talk about what it updates but of course this is an important discussion to have in my view I think we aim for having everybody move forward in parallel because I don\u0027t see why that cannot happen meaning that we would ask relying party software implementers to it also support RDP even if not all the repositories already have it so but first questions and well this is not "
  },
  {
    "startTime": "00:09:06",
    "text": "even a working group document so that\u0027s also another question that it might go to the list so the floor is yours Jeff Houston there were many mistakes and bad ideas in the original work that developed this and one of them was a sink I actually think that the problem that we got given which was the entire x.509 distributed repository mess and the inability to sort of smash them all together was was was a really terrible problem to get given and opportunistic pool was a solution clutched out of desperation rather than a hard design the problem is actually reliable flooding of information right and we\u0027re doing that by opportunistic grab in N squared mesh and it just seems to be wrong and when you rephrase it as flooding reliable flooding in you actually look at protocols like BGP which is actually a reliable flooding algorithm and if you think that you don\u0027t necessarily have to do routing information and route objects bgp is actually an amazing protocol as many other people have used for many other purposes if I tell you something I told you I never retell you you never have to ask me again I\u0027ve told you it for the life of the session the whole issue around the way it gets flooded is actually not a bad approach and I kind of wonder if we\u0027re going to drop all this stuff about a sink and replacing it with yet another opportunistic grab holding algorithm you have kind of exchanging one model of anchor for another neither of them flow and although I might be accused of going well the perfect shoulds the enemy of the good here I thought we could do better and if the abstract problem really is I want reliable flooding of what\u0027s inside repositories maybe we should look further afield than just replacing the underlying mechanism of this particular protocol and look at an architectural sense and go other alternatives we can piggyback then actually do do reliable flooding so that when I publish in my repository it just flows through everyone else so that\u0027s what I would urge us to think about before simply just going to this particular solution because it doesn\u0027t change a lot thanks yeah can only respond to that because well in my view it\u0027s it\u0027s quite "
  },
  {
    "startTime": "00:12:06",
    "text": "interesting to look at BGP as a plotting protocol for this but I think there will be many things that need to be figured out and it\u0027s going to be a while before that\u0027s a reality at this moment I think supporting our sink is impacting people and it\u0027s low-hanging fruit to do that first of all so in parallel you can look at alternatives at least that\u0027s my my take on it video photo spherical I oppose using BGP for flooding this before before before I retire I don\u0027t know whether some research group will be looking into more optimal solutions than this some problems with arsenic are known the RDP has been developed as a probably working solution we do not have the proof in actual scaled operations I think I think we need some something there some document that explains the roadmap for actually getting forward for the question of how eggs and poultry are bred my take is actually actually the repositories have to move first that does not preclude getting the implementation for clients quite obviously if the road map says the repositories actually should work on providing a reliable service before clients start to rely on them or even start to do large-scale operational testing now and yes kind of a fixed a fixed time schedule I think is not going to work the schedule has to has to be conditioned alized on criteria that say well okay if we achieve that kind of operational status on the repository side we we tell we tell we tell client sites to move somewhere I dima from our Kista open source a relying party project team I like your idea but I\u0027m wondering whether you have extreme nose with repository manager so "
  },
  {
    "startTime": "00:15:07",
    "text": "whether it is it easy for there to support Delta protocol well I no longer work for the ripe NCC but we deployed there uh-huh and there\u0027s some experience there and I think we\u0027re all new software there\u0027s an initial phase of getting it right I think the tricky part of that protocol is generating those files reliably but serving them on the other hand is quite easy but I agree also at what routing a said that we should build a operational experience with running this but some people have been doing that so particularly the ripe NCC and Edenic and others are in the process of developing and employing as well thank you so oil information are you our test bed I think Delta is a watch is walking much better than I think Wendy Bush are kissing i oj I don\u0027t think Jeff went far enough and I and I don\u0027t think Jeff went far enough and I have and I have a t-shirt that says BGP awarding PhD theses since 1994 and I think other bullish males pushed BGP down our throats when we really should have considered idrp more seriously I have also a problem Tim with the chicken and egg because that is not the case there is a sequence that gets you both poultry and dinner and and do you have a problem with the sequence I suggested I mean do you have another sequence which doesn\u0027t drop stuff on the floor oh yes you did the end I think we may actually mean a very similar thing what I mean to say is that I would encourage relying party implementers to already start supporting our DP definitely right and and I don\u0027t want to wait until all the repositories have it right but it doesn\u0027t mean we\u0027ve done that already no I think some still need to implement oh well now haven\u0027t we encouraged all right oh maybe but if we put it in a document so are you suggesting a document that only stops at encouraging them or lays out the whole sequence I\u0027m "
  },
  {
    "startTime": "00:18:07",
    "text": "into mind about including a sequence I think is useful during this discussion and the coming period but if eventually you have a document that updates the existing standard documents I\u0027m not sure that it should still include a road map but you can have a road map whilst you\u0027re talking about this stuff and maybe keep it in a document because it\u0027s a convenient place to put it but once it becomes our of seed and I\u0027m not sure that a road map is going to get outdated I\u0027m afraid but maybe you should take this offline it\u0027s not that I finally oppose it but I don\u0027t want to end up with a document that is going to be out of date the moment it\u0027s published and you have that risk when you include road maps in it I\u0027m perfectly happy with documents that go out of date once they\u0027re successful two minutes left jokes nighters NTT OpenBSD I would be supportive of offering let\u0027s call it a BCP titles using our Seng to distribute our P key is considered harmful and it is good if the ITF puts out a signal to publishers and consumers of the published information that our Seng is not the preferred method to disseminate this data if in an appendix to such a documents a roadmap is included that may be beneficial but at least statements need to be published that we do not like our sync and we need to state why we don\u0027t like our Seng for this purpose Rob lost Arkus and Dragon research labs a couple of comments with a claisen said they were running out of time first of all two people who didn\u0027t quite get Tim\u0027s comment about why we want to keep the arson to our eyes um the problems would definitely basically have us hashes because we very very carefully removed all identifying information from the RPK oh and the only thing that was left as the authorized if you take your eyes away to them what you have is hash it\u0027s lots and lots and lots of hashes which gets kind of confusing pretty quickly on secondly I slightly disagree about the marshalling the data into our DP being difficult actually it\u0027s not yeah we hit limit of this what four years ago um we had to reimplementation zuv our see a code that is because our CA code was in retrospect designed badly it should have been doing basically a transactional model to begin with as soon as you go to a transactional model this is just you generated Delta a for every transaction done it\u0027s trivial there are a few interesting characteristics on the RP side though in particular when you\u0027re dealing with a very very large collection like the ripe ncc collection the validation behavior is different okay if you\u0027ve got I\u0027ll just use this poster children uh throwing rocks of anybody poster children you have a pea "
  },
  {
    "startTime": "00:21:07",
    "text": "neck as the you know holdouts for the least efficient possible way of doing arson and you have Reitman CC as the early adopters for a big one of our DP the one nice thing about the rather strange think the de pinna cause is you can kind of do all the crypto in parallel right fetch it fetch d fetch and the fetch is so slow that by the time time you\u0027re done fetching you\u0027ve also done all the validation with the right model it\u0027s a little stranger than that with the right model you have to wait wait Rance fir and process and suck into the database which unfortunately is slow all of that data before you can start validating any of it really and it maybe there\u0027s some efficient way to do it I haven\u0027t figured out yet but to first approximation you have to eat the whole update apply all the deltas and unfortunately databases tend to be bad about doing this quickly and then you have to do all the validation is this massive chunk so the behavior is a little different I think this protocol is actually pretty much ready whether or not we are ready to deprecate our sink is a different issue but I think this I think this protocol is mature enough to go sorry Rob are you saying to adopt it then or adopt it and just straight during your bless call that I was not actually expressing a straightforward opinion about whether or not we had to just throw our sit got the airlock at this point in time okay should we yeah but I guess I\u0027d need to send out to the list yeah and but thanks for feedback I think that who else and yeah also available for offline discussion about this this week mr. Asimov think this one no no this one no this one no I don\u0027t know one two three four five six maybe they came last minute I didn\u0027t we could do that good work let\u0027s see I don\u0027t know that I got them right here what are they called oh yeah no I didn\u0027t upload those for sure too much going on don\u0027t you is there when you want to start with we could do that yeah let\u0027s maybe we\u0027ll try that of course now I don\u0027t have my chair "
  },
  {
    "startTime": "00:24:08",
    "text": "slides I have no idea who\u0027s next hold on Oh Randy how about Randy mr. Bush here\u0027s our easy to find Europe and we\u0027ll do uh then we\u0027ll do mr. asthma clear okay so back many many years ago we drafted this document and essentially it\u0027s transfer of resources between two registries known asses and IP numbers by registering the rest of this discussion we mean ISPs our IRS NIRS etc we\u0027re not going to transfer between two I Anna\u0027s it should be doable and we kind of have to do it fairly often actually how often do you do transfers Nathalie often she says okay and just to get some terminology there\u0027s the seller and the buyer and the swing point is the I are at the lowest point in the hierarchy which the seller and buyer has a common parent and which is agreed to act as the agent if this one doesn\u0027t agree their parent could be the swing point is what subtly mentioned there and there was actually an internet draft I\u0027m not sure whether it became an i or i\u0027ve seen or not that talked about parents grandparents acting with authority I don\u0027t remember that that it didn\u0027t make it to RFC yeah I remember writing it because of this problem but anyway I\u0027m gonna breeze through this fairly quickly because the point of this exercise is really not transfer okay simple transfer is the seller separates out the resources and tells the swing point the swing point without deleting these resources without the leading this delegation creates and gives them to the buying I are okay the swing point delegates to the buyer and then the swing point withdrawals from the seller when agreed okay and Steve Kent said he\u0027s going to write up something I jokingly called the torn euro protocol which is they tore a euro bill each of them has it and when both of them turned that bill into the swing point when the "
  },
  {
    "startTime": "00:27:09",
    "text": "swing point receives both halves then it can withdraw but until then both delegations are active because we want make oh this is just a side you know hey you can have things where the sellers double indirect you can have things where the buyers double indirect you know who cares there\u0027s a problem or that you want make before break you want both of these active at the same time because possibly circuits are moving possibly other administrative problems are occurring in that transfer and actual operation of the internet is depending on both of these being make before break the point of my story today is the our IRS have are driven by underlying data which I\u0027ll call stats files but you know some of them call them something else some of them have their authoritative data hidden and the stats files merely a realization of it but it makes the difference this is a sit the problem is these files I believe in all the RI ours are one-to-one mappings this resource may only be owned by one are up one I are at a time which means break before me okay now there\u0027s no blame seemed like a good idea at the time okay and the question is how do we help the our IRS get out of this position okay so the our p-a-k-i structure can do it it\u0027s real easy it\u0027s the our IRS have a problem with these one-to-one mappings okay so what we have today is not operationally viable there I was brought to my attention by an LIRR who actually broke okay so where do we go with the transfer document shall we move it forward or not I don\u0027t know and how do we help the are the our IRS become unstuck from this "
  },
  {
    "startTime": "00:30:13",
    "text": "problem and I think that\u0027s my last slide I managed to stay on the degenerate square very good four minutes for questions Oh Dutch Telecom I seem to remember that Steve Kent at some point did draft that allowed some outside of rpki signalling of something like this being in process and first of all criteria for relying parties that wants to check whether the are PKI certificate system actually looks reliable quite obviously include include the criterion that it looks very suspicious if a resource is claimed by two parties in parallel signaling when this is actually intended and authorized is something essentially needed there as far as rir keeping of data it would seem to be fairly straightforward to have to have markers to have a notation on resources this is outgoing and potentially a time limit and it would be nice to have that published so that discriminating relying parties actually can figure out if they see strange things in the classic words of Vince parallelo send code I think uh problem I think there\u0027s a lot of software tied to those stats files is the problem a points of a job Snyder\u0027s ntt a point of clarification I would like to ask you are you talking about transfers inter area or in Tran re era we see the problem today in truck RI are the problem I believe exists inter-rir and so now I mean having been the first victim or actually I wasn\u0027t a victim I "
  },
  {
    "startTime": "00:33:14",
    "text": "was very happy with the experience of transferring between RI ours um it was made before break so since we\u0027re venturing into the political landscape anyhow there may be use cases where it is desirable that the transfer only half completes and the certificates continue to coexist in multiple re ours from a redundancy perspective inter-rir is probably simpler in the current universe due to the perverse political situation yes and I think this work is very much worth considering and progressing through this working group so I\u0027d be interested so yeah we gotta do something actually I know what we have to do I think is helping the our IRS figure out how to not make not break before make moving the transfer document is something we can do at our leisure right yeah no I\u0027ll keep it really sure I think we should talk about this offline I read the document I never get scared of the mechanics that are proposed and I think we can aim for simpler solutions and I\u0027m not yet convinced that you need to signal these things to anybody looking at the rpi maybe you can leverage the provisioning protocol let\u0027s have a talk this document it sits in my memories this document doesn\u0027t suggest that I\u0027m saying I\u0027m not sure that you need the signaling of the intent to have a transfer done in the aapki I and if you do I\u0027m not sure that this is the way to do it but that\u0027s for the sake of time maybe have the discussion not right here right now because I don\u0027t think there\u0027s time for it was it long as quick for an attempt at code I get the nasty idea the RI hours presented us with an applicability statement as an internet draft that pointed to a certain file for disambiguating potential conflicts in the rpki it would be very simple I guess to add in that file another column that says okay this resource is moving somewhere this is a nasty comment and I "
  },
  {
    "startTime": "00:36:18",
    "text": "don\u0027t think I don\u0027t think it is actually going to work but it might be actually a practical way right okay on the break side for example provisioning protocol you ask a question to your parent you say what are my entitlements it tells you what they are it might be less than what you have there\u0027s no grace period for example that\u0027s one thing you might consider that you can communicate to a child that something will be shrunk go clean up that\u0027s one thing you can think of on the side of initially initiating all this there\u0027s a lot of line processes involved as well so I\u0027m not sure that the rpi should be driving this I\u0027m not even sure that anybody else needs to know that a thing is going on but if you do then yes currently the proposal is to create a separate certificate with all the resources that are going to be transferred on it I\u0027m not sure that that\u0027s the way to do it you can also invent a signed object that says these resources are going to be transferred I would think that\u0027s already easier so just you know we can have more discussion but that\u0027s at the beginning of it and a quick question for you about where do we go from here there were lots of comments that you got and job said this is work that this working group can take so do you want to switch your call on this I did I think there\u0027s two bull bullets here and I think what we\u0027re strongly agreed on is the second one which is not pushing a document okay the Straz mob leave we have your documents now although if you\u0027re a telex thing to reload okay max length second one [Music] hello everyone my name is Alexander Asimov I work for Yandex and this will be a rather quick update thanks about a spade rats during this year Randy with fighting with me to make the draft less ambitious to have clear description about the procedures that are present in this draft and these slides is another tenth that\u0027s failed so the cookies not working try this that\u0027s not the right "
  },
  {
    "startTime": "00:39:23",
    "text": "one but oh it\u0027s this thing the other fullscreen oh yeah sorry son take it please is an oversimplified description of the solution without no feelings and unknowns and so on so there is a drafts suggest two procedures first procedure for prefixes that I received from customers and peers and the second procedure for prefixes that I received from providers and for if we are receiving prefixes from the customers and peers the only pair so that may exist in the ice bath is custom to provide a custom to provide a custom to provider as you can see CA here if we detect a single pair that is provided at a customer or at least it\u0027s not accustomed to provide a source sorry for ambiguity of these slides its invalid path the downstream path it may have both upstream path and downstream sub path and but each fit is followed again by an upstream path it\u0027s a route lik so don\u0027t look at this light its listen what about what I\u0027m speaking about it\u0027s very simple and very hard to describe not in gold but with words and slides so there were also questions about deployment process I and it\u0027s very important to bring immediate benefit for the ice be who will be the first one to sign first a spay record and a speed desert because as soon as it signs a spay record it gives a chance to other parties to detect mistakes around leaks so it\u0027s very poor to know islands you know cooperation which just works for malicious activity it\u0027s slightly a harder process but it\u0027s not that hard so you need to create royal records - so it\u0027s a it\u0027s also up to you and your security also relies on your upstream providers if you have happy to be t1 you are not relying to or to anybody so it uses an is period so this the only thing that is needed to protect you from yourself from malicious activity if you are tired - you rely on your upstream providers so the change look I hope - it will not be an additional ambiguity so it is rather simple after the suggestions in the mailing list we changed a spay profile from players that were customer and provider to pairs where we have customer and set of providers to solve Possible\u0027s you musician issues we also added another state for verification procedure which is unspecified so we have valid invalid unspecified it will not be that\u0027s useful "
  },
  {
    "startTime": "00:42:25",
    "text": "at the state of owner adoption but can give your additional debug opportunities at the state of middle adoption or high adoption rate I\u0027ve added some beautiful drawings that shows how the state between valid in validity unknown are changing I\u0027m sure you if this made the draft more readable but I hope you enjoyed it and this enjoyed it and with good first implementation on top of that many things goes to yogini become asif and andre such as ik i hope i pronounced it rightly I know that there is another implementation coming so we\u0027re we\u0027re good here so and now action plan please read the drafts we need your feedback because we need at least another person which will start telling me that\u0027s the way I\u0027m writing text ruining the principle of least astonishment so if you want to at least work to proceed quickly please give us feedback help us to work on the text another on outside we will receive your feedback work on it and we\u0027ll try to finish working on the text until the next ITF meeting rain they also promised that he will work on archaea protocol and so this is the last missing part so we will have three drafts one profile one verification and a data that is supposed to go together to the working group Lascaux so that\u0027s all Randy Randy Bush Jai anarchist Randy promise to do the RPI router protocol when um object was clearly designed and clearly not very significantly changes they all document progresses I\u0027d agree and since we have changed their profile if we have a stable version of object and secondly I really think the draft needs some reading by a couple other people because some of the authors are in disagreement of its clarity yeah hands up for this please read the draft give us feedback it\u0027s very important all right profile drafts in particular that\u0027s what any other one and I left a comment on the list I wanna eat here so this is not a big deal but only the customer is actually needs to be present only and entity certificates in the "
  },
  {
    "startTime": "00:45:25",
    "text": "signed object and I would think it would be good to recommend people that they don\u0027t include anything else because if you do then you run an unnecessary risk because of any of those resources are no longer held by the certificate signing that ASPA object and the whole ASPA object become invalid and i think it\u0027s easy to avoid you can just have the one a yes only and then so yeah yeah thank you for the comment and thank you for your previous comments that you highlighted possible issues with the pair of provided customer I think the CAC the text currently says that the object is valid as long as the customer a s is in the set that is on the EE certificate and I would suggest that you keep that EE certificates small in terms of resources because you only need to have that a s that customer s there you don\u0027t need anything else so I wouldn\u0027t include anything else because if you do that\u0027s just a avoidable risk of things that can break really do you give up yeah okay this is my second set of slides and they are not related to any draft but I believe they can bring variable considerations for this working group so and we will discuss as you see much length the default behavior normally when we are speaking about default behavior or default settings by expecting that these kind of settings are working in most of cases and for a current advocare object roll the default behavior suggested to be drop invalids now rejected methods don\u0027t accept in minutes so when route becomes invalid there are two options first option if the reason is match in the outer own sister numbers and the second option is that the prefix length is bigger than the length specified in the row object so as you have already guessed so we will spend next five minutes discussing I if max length applicable for with the default suggestion that such with such rounds should give it a rejected and it can a move forward I\u0027m sorry some types I included yeah there will be another "
  },
  {
    "startTime": "00:48:27",
    "text": "type us on the next slide so I\u0027m sorry it\u0027s not a thick it\u0027s AfriNIC so I\u0027ve tried to contact all current reuse to find out what are their default settings when people are creating much length because as you know the the hosted our creation of rows is the current so nearly all of them while answering just seems to think they\u0027re looking at the prefixes detective at the moment and creating marks length that is equal to the prefix length look from now but there are okay but there are a couple of problems first if we all took the site of ingress filtering you cannot drop invalid in case of custom sending your air black hole from egress site is just the same so if you are sending a black hole route you cannot just blindly drop invalids and it is related to Rudi girls draft and she is already in line but there is another issue let\u0027s say let\u0027s take a look from the security perspective from the security perspective in a perfect world where is a pacaya is globally deployed we will have raw and I hope will also have a spare objects and to stop both malicious activity and mistakes we don\u0027t need max length and there is another point and it is more crucial at the state because it happens at the state of partial adoption imagine you are the possessor of Network / - 23 and you have created rows with help of some web interface and your supervisor is a good fellow and here already using a rotation procedure but somewhere another guy makes it typos and he hijacks your prefix and it\u0027s going to be propagated and there is it\u0027s not specific and you cannot fight it because you first need to update roles after that you need to call your action provider okay if you are using egress filtering itches a slightly simpler but again it\u0027s it\u0027s doesn\u0027t provide it in a way to fight militias are not malicious but mistakes rapidly so it\u0027s great additional security risks at the states of partial adoption until it\u0027s already here it\u0027s I\u0027ve copied it yesterday from NIST and it says that a half of invalids that are "
  },
  {
    "startTime": "00:51:27",
    "text": "in the world for correct are consistent number and incorrect Marsland there are networks that have already put to this issue into consideration and some of them are sitting max length to the L to the maximum values as it\u0027s possible this way I said yes it turned into to fix this issue because I was thinking about this risk a couple of years ago but recently I\u0027ve learned from several players for some of them have very big names this so they are already hacking on top of apical cache so the increase in max length on their side to accept invalids from their peers and from their customers and of course it is all connected what is set by default in hosted CAS in routing registries I\u0027m not giving you any kind of solution at the moment but I\u0027m asking this working group do you consider this as a problem that should be taken into account if and if you think we should work on this issue let\u0027s discuss possible solutions right again okay there is in my opinion absolutely no question the correct default for brewers is a simple row with max length equal to V traffic\u0027s length that\u0027s the basic that\u0027s the basic rule the max length the max length longer might be something that should be only available in expert mode the user interface after after examination that people have a rough idea what they are doing may enter at you and ask you is a co-author of egress filtering how do you see deployment process of these draft if the default policy and drop-in reddits is not working for the leaves because I\u0027m not I don\u0027t think that the target audience of this draft is dr. chili con I when I first saw this draft I said I was thinking all great so they\u0027re stupid guys will protect themselves from making mistakes but if if it\u0027s not working by default I\u0027m not thinking they will deploy it i I have I have to think about it but kind of I I guess that well okay if roars if well okay if you if you want if you want to have to pass to pass "
  },
  {
    "startTime": "00:54:29",
    "text": "routes that are actually invalid with Nami I don\u0027t well I heard you asked about yesterday when we talked the day before yesterday yeah I could have some more radical thinking about this issue at the moment I believe that if you flip the coin and even much length for the experts instead even the default like is specified for example by some companies that already see the risks it might be flying from the box well if for my own network for your own network it is quite it is quite clear over egress will help me to shortcut stupid misconfigurations that my colleagues are doing for customers for customers a kind of the ingress policy will ensure that I will not accept invalids they are in except for the cases where we have a special agreement and that special agreement quite certainly you have will have implications for special handling on the egress site if there is actually a egress side yeah but again I really regret that not all companies in the poles at Deutsche Telekom and into the okay okay okay okay I will not least each company that is at present in this room but anyway but take into account the idea that there as far as I understand the target audience for egress draft is Leafs and to make it working at least site you make you need to make it as simple as possible the principle keep it simple stupid is the only way to make it and to make it work to make it global or not global even but it is partial doctor not only by but big players because I\u0027m not afraid of you I\u0027m afraid of this jeff has trying to keep this short the max length stuff is obviously advice to the global internet at large this is what the global Internet should be seeing for these things the misconfigurations we tend to see here are often things that you want to have no longers show up for some scoped purpose in many cases now the black holes typically need to go 1a s over same thing is true in many cases of various types of te routes no something that might be a little bit longer than what you\u0027re really good announce to the world at large the two things that can "
  },
  {
    "startTime": "00:57:30",
    "text": "be done here is that the parties that are next to each other that are cooperating can just simply configure something into their system that says that we\u0027re willing to ignore the rpki answer for this because we\u0027ve otherwise done validation we know that this is a valid origin it\u0027s following the paths for my filters and I\u0027m willing to accept it because it\u0027s a little bit longer the other thing that could be done is that maybe provide some sort of hint into the RO objects themselves for these narrow scopes of things that it\u0027s okay if you are here but for your more general case of you\u0027re seeing something much longer and trying to mitigate an attack I think that\u0027s a much harder problem to solve okay thank you for coming yep job Snyder\u0027s ntt it is quite tricky to at this stage of the game fiddle with the semantics of next length because we\u0027re deploying the snowball is rolling down the hill and for better or for worse some things kind of are what they are what they are right now you identified a specific case black holing for deaths I brought up a different approach where we make black hole route requests rely on less specific routes which then can be pumped through the original validation procedure so it may be that for the black hole use case we don\u0027t need to do anything insider ops but rather documents enough away of black hole routing in grow but if there are other use cases that may still allow this conversation to continue but I as you explain things right now I would remove black hole because there are I think more feasible more pragmatic so can i interject guys we\u0027re seriously out of time it\u0027d be great if you take it on the list thank you small points of order meets echo jabber room is not reachable from my laptop anymore so I cannot at this moment scribe thank you did you want this one or the other one first thank you hi folks I\u0027m Deema from zds and I\u0027m also working for mr. open-source project for RP Caroline party so I\u0027m here to report to the working group I can say if it was about the update after this informational document these things is the working group last call and during the working group last call we had got some comments we got some supports and Allah is out we also made update accordingly especially by adding double control to reference "
  },
  {
    "startTime": "01:00:31",
    "text": "IFC 84 16 which you suggested by leaking out and making some wording improvements as such as by oleg komarov ski and then we made a further update because some reference to the draft has been partially of the documents okay at the present we also believe this document is ready to move forward and the request for chess actions so next step I\u0027m ready for actions and I suggest you owe me the last call we just sorry notice ask that you email the list to ask otherwise we will likely forget or at least I will likely forget so just email us I\u0027d like to make last call for this thing please quick quick thank you I don\u0027t see your other slides on this that\u0027s concerning me because I put them on there oh wait sorry yeah okay it\u0027s Dima again I like to take this opportunity to brief implementation of synchronizing rpki in the cache by distributing cache data in JSON over HTTP so before I touch upon the very topic of my presentation lesson review the rpki cache deployment scenarios as discounting obviously attitude when they are ooh another background for the following discussions as far as I comprehend as ARP cache deployments in arif are describing I see a tattoo one arrow - concurrently could be made that a lot older dogs have to set up its own RPC system to do our PTI data synchronization and verification and RB k value than the cash is not necessarily transported around us but between two intermediate italo\u0027s arranged in a hierarchical scheme last year as my team was establishing a third party ERP system is called our pica-x cloud-based system I will reassure that the conclusions that I just made in my last slide was a reasonable and it\u0027s desirable for us to have a tool to distribute our PK value data cache to among different organizations over network the university disease figure the architects "
  },
  {
    "startTime": "01:03:34",
    "text": "implemented ways Avista the open source software is not in service our Praveen cache data to some organizations in China by distributing cache data in JSON format over HTTP to our servers deployment in lead hooks of ISP or ICPs in China so moving along to this slide I am now presenting our consideration of the implementations so Alif all the way of formatting data we prefer to facilitate data paths for this violation and applications other than router origin validation as well as to take advantage of slum to do local control and then we choose HTTP l transport because it\u0027s a widely supported it is with security enhancement to offer interpreter protection and it is independent from protocol than you need so if you are already familiar with our C 84 16 slum you would have finally easy to do RP cache that update by the virtue of slum filters and some assertions which are responsible for deleting and adding cache data items respectively so that\u0027s a reason why we choose JSON as a format to be in alignment we\u0027re still on fire for mate in other word will slim file to describe cache data and it\u0027s update so I\u0027d like to reiterate our motivation to do these so aren\u0027t here either over there why bother to do these on here I designed excuse me for preventing RP cache their cache data for routers so the RTR PDU is bounded to transport which is difficult to as some extensions and some applications well which is not supported RTR cannot read the binary data offered by RT r and we will also like to take the adventure of just you know why API to do business in correlation and local control because the Salam file is in JSON format and so that\u0027s the reason why we\u0027ll do these so here is a desire HTTP header for it\u0027s used in transporting cache data so it\u0027s inevitable for us to have got some had to be her extension to signal what is piggyback in a message body so the we look added we can find inside head that indicates some control information such as operating model time stem version "
  },
  {
    "startTime": "01:06:36",
    "text": "information and a Sigler ship which is the for integrity production and also about a inside body which is actually a slim file so our implementation supports those push and pull model so in pull model the HP clients need to indicate whether it wants who are copy or it just needs an incremental update so that\u0027s a well this is a most important reason of why I talked about this I think it\u0027s necessary to do stand eyes our bouquet manager caches invalidation because sync relation could take place between different organizations and the Randi dedicated reserve receiver match in his provider so and present we have got a crafter bodies coming soon and I hope the well be adopted as a working group item so by the way I will be also sharing our implementation code at a chill hub for you guys who have a give it a try and I also note the CloudFlare has a made us similar efforts yet in a slightly different way anyway we\u0027ll keep an open mind to any inflammation details and anyway if any in Hawai I interested in this walk and can contribute to this let\u0027s work together and I hope it working group will consider whether this work is it is of our efforts to seek deny elation thank you jeff has I\u0027m confused slightly by the deployment scenario is this attempting to populate the catio caches that are supplying inet router to the routers themselves no this mechanism is designed for the the catalyst in correlation among different administrative organizations so that\u0027s a leaves case so this does not actually catch the routers themselves you\u0027ll have a nothing to the way the router thank you we actually ended up with some other format intermediate for the payload the validated payload out of the crypto system and yes such such such a standardized interface we actually have three different validators producing the same stuff quite certainly has some advantages for scaling scaling the feeding of many Reuters and well okay "
  },
  {
    "startTime": "01:09:41",
    "text": "the validated crypto payload as I would call it quite obviously has also other potential very interesting uses so I have one thing to add up as far as I observed the implementation of cauliflower and our our t-rex there are those pride now is private implementation well both working very well but I think the community will benefit if roms are standardized in the face so that\u0027s the motivation thank you yeah go for it Oliver you\u0027re up run fast I just want to interject something in the transfer presentation and make before break issue um I forgot to give credit where dude this issue was raised by Nicki alert I did not invent it it\u0027s Nick hello my name is Alma Boyett I\u0027m from NIST so today I\u0027m talking about the draft be trapeze act like validation signaling which we proposed either last ITF or the ITF before give you an update so since the last time we talked about that we had Burton group adoption call and which was successful but had some interesting discussions the first discussion was that why would we need in why would we need a new attribute for that there\u0027s already the attributed RFC 80 97 out there that signals the orator meditation and it has a long section for reserved and we just could basically pick a big on on the reserved field and at pass validation to that that would work very fine because the unverified stayed for us would have the value 0 and RFC 1897 set sees feared as zero on the sending side and has to be ignored on the receiving side so that would give us some backwards compatibility to anyone who implemented the 1897 furthermore after the draft got accepted there came some discussion about the validating BG speaker another draft out there that deals with origin "
  },
  {
    "startTime": "01:12:41",
    "text": "validation on ebgp and has second part second focus on the operational modes that can be configured and they came the part of why don\u0027t we merge all these work together in one thing so we the whole thing is a little bit more organized so then you cup from the e6 and I we had some conversation about that and said okay the his history basically this this is two sections of ebgp as well as the operational part half of that also applies to what we want to do and the other half the operational mode could be taken out as as a BCP so that was what we decided so far and maybe some input from the working group would be would be good especially in regarding to splitting the validation BGP speak a draft so and then the remaining portion in the validating BGP speaker which would mainly deal with the operational modes and so force would be put into a BCP or informational draft because everything that\u0027s written there is actually pretty good so what does this mean for B to B\u0027s neck validation signaling we we add some additional language or a little bit more clarifying language and regarding to ebgp usage and clarify error handling so the top part is currently RFC 1897 there you have the long block of reserved and at the end we have the validation state and we update that by using the second to the last octet for pass validation and we rename the while relations state for you it into origin validation to clarify which validation state is in which octet furthermore we we at the validation state so a lower part where we have zero for unverified one for valid and two for not valid that are the egypt lee sec states that are defined in at least develop none sell it as the b2b sex states defined in AD 8205 we went ahead and did one further thing we in the 1897 the fields are considered lookup resort and we renamed that into validation state because it makes much more sense you signal the validation states so why not why not calling it like that so "
  },
  {
    "startTime": "01:15:46",
    "text": "regarding ebgp signaling RFC eighty ninety seven actually allows you to be signaling it has some Contessa birding of volunteer situations we are two adjacent guesses under the control of the same administration for example or the other example what is as a warrants iteration is basically side or RPG i which is the link to the b2b to the draft what i explained before from which one was called validating b2b speaker yeah so we propose the change in this wording to say implementations must provide a configuration mechanism to allow the use of the community in both sending and receiving and the implementers should enable the use for this community on ib tree on all ibgp sessions and short by default disabled on ebgp sessions to make it more clear and yeah then error-handling currently 1897 is has an indirect error-handling and they\u0027re mainly deals with if the attribute occurs more than once in the BGP update currently it says disregard all instances except the one with Z largest integer value and that gives a little bit problem if we have not two integer values in there so we say okay if we have more than one instance then [Music] disregard all of them and apply the strategy similar to the attribute disregard so now some thought so that\u0027s that\u0027s changed what we have right now and I upload it\u0027s a document yesterday it\u0027s just take some time read over it now current the current path is to update eighty ninety seven to add these part but the question is where we had so much more wording and also we change or we add other things if it wouldn\u0027t make more sense to obsolete 80 97 rather than update 1897 I think that\u0027s something that should be discussed another thought currently the draft is b2b techno b2b segmentation signaling and we definitely should rename that in path orientation and origin validation signaling because the definitely Dean\u0027s know there\u0027s all those posed questions job Snyder\u0027s NCT "
  },
  {
    "startTime": "01:18:47",
    "text": "unfortunately I am not entirely sure you\u0027re heading in the right direction by merging the allegedly called validating Beach P speaker draft into this I\u0027m not sure if you\u0027re aware of the extreme controversy that that draft has generated and absolutely failed to reach consensus it is very disappointing you see it resurfaced yet again well there are tens of comments unaddressed and with an ITF process it is not the best quality of work so I think when you set out to specify extended communities to signal bgp SEC validation states that is a lot about goal but then perhaps you accidentally stepped into something that doesn\u0027t smell as nice and that may drag down the effort that you set out to originally accomplish in other words if you fade share with a draft that was not well received it will complicate your original goal and I I hear what you say the only part of the merging what we do here is actually that we take the ebgp signaling into the BGP section it\u0027s a transfer for me to be sick that actually makes perfectly sense I want and the other part which we stay completely clear of of the whole operational mode and all this kind of stuff nothing of that is in this draft at all so maybe if you if I can research for us I understand where you\u0027re coming from but the cider ops working group does not get to redefine what transiti means in bgp attributes that is outside our scope and a non-transitive extended community will not in this working group be redefined as well it\u0027s now subtly transitive and it is a mistake that\u0027s in the eight zero nine seven draft that it suggested that or perhaps it could be transitive know that sentence should never have been there that\u0027s a buck in the draft and you will meet opposition from implementers and operators if that part of the church is spray Pence\u0027s [Music] so this this is a technical issue because BGP implementations do not behave that way putting it in a draft will will will not help okay so I would recommend to not proceed with the merch and and proceed with what you originally set out to do they will make the chances of success higher okay thanks John "
  },
  {
    "startTime": "01:21:51",
    "text": "Scudder two things one is personally I would take no offense that if you obsoleted eighty eight ninety seven then just rolled it in that would be fine with me the second thing is on your can you wind back to your error slide yeah that way that looks like a downgrade attack to me because if you pick the numerically highest one that\u0027s sort of the least desirable validation state and that\u0027s going to be less desirable than the lack of signaling of any validation state which is what the bottom box does so that we can take it to the list but that doesn\u0027t seem like a desirable strategy to me I think we should bring it to the list because the only problem is what what I see currently now we have two validation state in there and let\u0027s say for example we have fun attributes it says Origin validation invalid and pass validation valid and the other one says pass validation invalid and origin validation valid so now I know I have the problem do I start cherry-picking the values out of them and takes the highest integer value and I take information of multiple attributes what I don\u0027t know if this is the right way to go as well or which one which one do I know choose - I prefer origin overpass or path of origin so I think there\u0027s definitely some discussion needed that that\u0027s why I said okay the easiest way in this regard to deal with that is if I have these problem what I have to solve somehow if I say okay if I shouldn\u0027t see more than one Ennio that I mean that\u0027s the first thing why why would I see more than one attribute in the in the update so but yeah 90 bushard in Arcis the reason you should is careful reading of the BGP spec says that you also have to check origin as John Scudder said obsolete in 1897 probably is not at all harmful there is actually a subsequent RFC blah blah blah clarifications whose number I cannot quote which throws away a significant part of 1897 and because it was ill-advised 1897 is one of a small number of RFC\u0027s which has my name on it to which I formerly objected in working group last call and an ITF last call Jeff has I own an implementation with a "
  },
  {
    "startTime": "01:24:52",
    "text": "policy engine that has to look at this stuff you do not want to put these things both into the same community the primary reason for this is there\u0027s a lot of if you flip back to the community description a lot of people that have policy out there that is matching against you know basically a scroll going go go go go one more thank you there you go there\u0027s an awful lot of people that have policy that says no match community for three zero zero bunch of zeros and then number one two or three and it is done as an exact match type thing despite the fact that no it says reserved most people know if they\u0027re doing something slightly smarter for match purposes might do regular expressions regular expressions suck for performance so a lot of people don\u0027t do that the consequence here is that if you overload it you know you don\u0027t have a bunch of zeros in the middle in there anymore and you\u0027ll no longer match the thing you know you\u0027ll basically have an incomparable number and you\u0027ll jump into the wrong spot year policy code so your suggestions to keep them separate yes exactly as we had originally yeah it\u0027s an extra as an extra community it\u0027s not a big deal it\u0027d be nice if engines that work this way you\u0027re a victim of what I like to call magic extended communities although the validation procedures you give is an attempt to choose one out of many because it\u0027s magic okay actually if I can quickly into jaga I was myself contemplating whether we do this as a separate community or not but I think Jeffress is a really really good point and you probably want to take this out you know that I think I think that\u0027s all very very well good points and I think there\u0027s some discussion that has to be done on the list and that pretty much also answers your second question whether you want to obsolete our existing RFC or not just leave it as is dr. Montgomery just a quick question I guess for John\u0027s comment as I\u0027m not sure I understand the threat scenario that if you\u0027re in a position to somehow on the wire insert a second one of these communities aren\u0027t you equally in a position just to strip them all and I\u0027m not sure the vulnerability that you don\u0027t already have if that\u0027s your threat III only want to respond to that if I\u0027m not taking away from the rest of the agenda there\u0027s no more agenda last three minutes all yours okay there we go so still John Scudder yeah I don\u0027t want to try to defend it as an attack even though I used that word I guess sort of putting it in more neutral language the "
  },
  {
    "startTime": "01:27:52",
    "text": "their handling that was prepared proposed on that slide is sort of less stringent than what was it that the you\u0027ve changed the semantics you\u0027ve changed the semantics to something that will accept routes that were previously rejected and that made my knee-jerk you know if you subsequently analyze the you know the the case and sort of convinced me that and you know John even though we\u0027re letting in routes that you were previously gonna drop on the floor it\u0027s okay hey okay but I think that you know I think Jeff\u0027s comment kind of moods the whole thing so you know we probably don\u0027t need to to dig into it further so you\u0027re gonna do some more work on this and okay cream make sure I didn\u0027t misunderstand result all right yes yo I think just one no oh there were two so there\u0027ll be we\u0027ll find it and we been a question of job Snyder\u0027s entity a question about procedure the draft ITF cider of validating BGP speaker which is of course anything will validating did not reach consensus what is the current status in the working group see you in Vancouver no I believe the end result of it was the author\u0027s could go back to try and make something sensible out of it or we\u0027re not moving it forward anywhere any further so right now they weren\u0027t group doesn\u0027t care about it it I think is the end result like it\u0027s not part of our game plan so so should we still consider it\u0027s a working group documents I don\u0027t know do we oh I can kill it I think it\u0027s not on the list right yeah wait wait wait it is currently a working group document because we inherited it from cider and we therefore it was before that it was an individual submission so it currently may be a working group documents which yeah okay yeah we can take that on the mailing list yeah that\u0027s what I want but not maybe what the offer sponsor I would say like a please email the list and say you said ba ba ba ba ba do we kill this now or that would help Thank You Jimmy Cooper bye "
  },
  {
    "startTime": "01:31:18",
    "text": "[Music] you "
  }
]