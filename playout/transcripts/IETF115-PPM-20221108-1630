[
  {
    "startTime": "00:00:05",
    "text": "foreign given the short session I think we're uh my clock shows that it's time to start and so I think we're not going to wait um we're just going to Dive Right In so this is privacy preserving measurement at ietf 115. I hope you're in the right room this is your ietf note well as always the ietfs IPR and good behavior policies apply please familiarize yourself with the note well if you are not already familiar with it as this slide notes if you are in fact in London and not in New York City you are going to need to wear a mask if you're in the room thank you for helping us to keep all the on-site participants safe foreign I think these links are probably not as helpful now most importantly the agenda bash this is our proposed agenda for the one hour meeting that we have if anybody would like to suggest a change to the agenda now would be a good time"
  },
  {
    "startTime": "00:02:04",
    "text": "foreign so I'll just repeat that one more time this is our agenda Bash here's our proposed agenda uh if you have questions about the agenda or if you'd like a change to it please come up to the mic or add yourself to the queue foreign not seeing any requests for changes to the agenda so let's get started with the DAP editors uh I'm going to give you a 30 minute timer and you can divide that time as you like among these topics please do leave plenty of time for discussion if you take all the times with your slides you're not going to get any feedback no we did uh I am I've requested to uh screen oh wait I I'm trying to request I wanted to request to share slides sorry okay let's get the technical issues sorted out first I have requested to share slides uh okay let's see you could also run them if you if you prefer first one staff update"
  },
  {
    "startTime": "00:04:08",
    "text": "hey sure if you can hear your keyboard you only want to mute yourself uh okay uh let me see if I can fix this thank you cool uh I see the slides I am I running it no that's okay you ready to go yep all right so this is just going to be a very quick update on recent uh changes to the DAP spec I'm going to talk a little bit about the issues that we want to address in the immediate future and talk a little bit about what's uh like our long-term objectives are um and then I'll just quickly update folks on implementation status so the major change in the the current draft dappo2 is this new notion of query types so folks might remember from the previous version of the spec the uh the way we would pick a reset of reports to aggregate is the collector would specify a time interval and then the aggregators would go find a set of reports that have time stamps that fall on that interval and aggregate those um this is useful for a lot of use cases but uh it doesn't solve every problem that we that we have so we decided we need to generalize this um so there's this new uh parameter of the task configuration called the query type basically what we do now is the"
  },
  {
    "startTime": "00:06:01",
    "text": "aggregators are going to partition reports into what we call buckets and then the collector's query um actually specifies a sequence of buckets and this is what we now call the batch um so you can think of a bucket as for example uh for the old semantics a sequence of non-overlapping time intervals so now we have two query types one that specifies uh this uh preserves this semantics from the previous version we call it time interval and then we have this new one called fixed size where um we don't actually care how the aggregators partition reports all we care about is that they're all roughly the same size so that's what we accomplish with that um so we're anticipating need wanting a lot of flexibility here so as folks start to use the depth spec I encourage you to think about you know whether or not your use case fits and we'll figure out what we can do to accommodate it hopefully by spelling out a new query type uh next slide uh one minor change I wanted to highlight is we now have this notion of task exploration uh basically the consideration here is primarily operational um it's useful for capacity planning uh figuring out how long you have to keep around the various assets that we end up Computing in the DAT protocol um it's useful to know how what is the maximum lifetime that uh a task will be around um so that's what this this new parameter accomplishes next slide on the final major change is um the is to uh HTTP client authentication so we know that we're going to need client author authentication for a couple of channels uh one between the aggregators the leader makes requests to the helper and we want to authenticate that request and similarly The Collector is going to collect uh request Aggregates from the leader and we want to authenticate that channel as well so in dapple one we had spelled it spelled out a a kind of a"
  },
  {
    "startTime": "00:08:01",
    "text": "simple uh dinky uh Bearer token based approach um and we decided in Depot 2 to instead of specifying a concrete scheme we would just spell out requirements for uh the establishing the secure Channel um and the idea here is that we uh we wanted to permit lots of flexibility here for for uh implementations because this is client HTTP client auth can be can be uh tricky uh next slide so um the the plan for the the the next spec Dapo 3 is basically to address some minor uh bugs that we noticed while we were working on the implementations of dappo2 so these are things like um we uh we uh uh at issue 362 uh we ended up making anti-replay requirements uh a bit stricter than in the previous draft and we're wondering if that's actually a regression so there's some discussion happening there uh it'd be great to get people's feedback um and then another issue to highlight is um uh extension processing semantics I uh we haven't actually fully spelled this out and uh as we'll see later we're gonna propose a new extension so I think it's time to figure this out um and then beyond that um Tim is going to talk about the some API rework uh proposal um this is like the only major change that we foresee doing um and uh and then beyond that integrating pop other one is something we want to get to in the near in the medium term future um editorial changes I think there's this presentation in the spec is is wanting a lot uh as as people many pointed out and then beyond that we wanted to start doing all the things that makes a good spec good um experimentation security analysis and so on next slide so very quickly there are two implementations of dappo2 that we're aware of uh one by cloudflare and another by the isrg"
  },
  {
    "startTime": "00:10:00",
    "text": "um we've been working together closely on our implementations um we are we're quite confident that we're at a point where we could really start experimenting so if there's interest please ping the list um also the PPM channel in the ietf slack is quite active so we'd love to see you there and finally I wanted to flag this uh draft that David Cook is working on he's from the isrg um they're uh yeah they're uh it's it's basically spelling out some some some ways to do interrupt testing between implementations and that's been really useful for us um thanks uh I didn't leave much time for questions but if there are some quick questions let's hear them if not uh we can move on to Tim who I believe is next in the agenda okay it'd be great if somebody could uh take over note taking while I'm speaking uh okay same it looks like you're gonna be driving the slides let me pull up notes and all right uh let's dive in so for the last few weeks I've been uh working on a new version of the HTTP API for uh for the distributed aggregation protocol um but thanks uh So currently this exists only as a memo that I've that I've written up in a request on GitHub that sketches out the proposal so there's a lot of work to do before we'll even have a PR that could actually be merged into the protocol text but what I want to do at this stage is socialize this idea to the working group uh and gather feedback but to make sure that we've identified the right problems and that we're headed in the right direction to solve them so I think it's important to do this kind of thing sooner rather than later uh the idea being to minimize disruption to DAP implementations um would have to adopt a new API since as Chris just highlighted there's only two of them known in public"
  },
  {
    "startTime": "00:12:01",
    "text": "hopefully it's cheaper to have them you know to have us update our implementations than uh to do this like months or maybe years down the line right so on this slide we see the the current API surface uh specified in napo2 um so these seven HTTP you know requests um Drive the upload Aggregate and collects our protocols so what we notice uh next slide slide please one thing we noticed right away oh thank you is that um the thing described by the uh the relative path is variably a noun or a verb uh which I think makes it then awkward to that you know to resolve the verbs against the verb that's in the HTTP method so we see this because in fact uh reflected in that um more than half of the uh of the API pass here use post which to me suggests like that they should be speak for I want to do something to the thing at this path but the semantics aren't you know aren't super clear um next slide please crucially post also means that the requests are not item potent uh which makes it unclear for protocol participants how they're supposed to go about recovering from faults uh this is especially a challenge in the aggregate subprotocol since uh that's all about the uh stateful multi-round um execution of the vdaf verification uh algorithm um and finally a problem with the current API layout is that there are some cases where servers have to partially parse a message to extract a value like say the task ID which is needed to then parse the rest of the message because there are cases where you have to look up the task to figure out what vdaf or query type is in use and that informs the structure of the remainder of the message in the request body so that's awkward at best and um risky I think in some cases um okay next slide please so with those problems in mind let's look at what I'm now proposing to do instead so in designing this I started"
  },
  {
    "startTime": "00:14:00",
    "text": "by enumerating what are the resources that this API is managing what are the things um the idea being to let the HTTP methods be the verbs so that's enumerated in this table so the resources ought to be familiar uh from dappo2 it's hpk configurations reports aggregation jobs aggregation shares and collections um so we noticed that the New Paths uh contain much more information uh so in particular the task ID and in some cases the unique identifier for the resource are there in the relative path um and we see that you know most resources are now subordinate to a task uh with the exception that the hpk config because those can be Global so it would be a bit awkward to make it look like everything else um so implementations have to implement the methods listed in the required methods column but unimplementation could you know Implement more methods on a some resource if they choose to or if it makes sense for them uh next slide please so I also tried to pay attention to making better use of put so that we can get item potents um in particular for example if an aggregation job you do a put request to create one since there's a unique identifier in it which allows the the server the helper in this case to sort of disambiguate repeated requests but advancing the state of an aggregation job which is again you know that's the stateful vdap verification algorithm um that does incur a side effect in the helper of advancing it to the next round of verification so that's a post since it has a side effect um next slide please all right uh for some resources the unique identifier for a resource appears in the URI because this is for cases where uh the unique identifier is assigned by the requester so in a report the client assigns the report ID for aggregation jobs uh yeah he's assigned by the leader uh next slide please in other cases it's the message Handler that will assign the unique ID of the resource um so the put is to a resource for aggregate shares plural or collections plural then it's the Handler"
  },
  {
    "startTime": "00:16:01",
    "text": "of the messages responsibility to sort of assigning identifier and construct a URI so for that reason in these cases we don't specify um what the URI for the individual resource is just what methods it supports and what the you know what's and what the semantics of those calls are uh next slide please so despite being significantly different and I hope better um the migration from the uh Depo 2 API of his proposal hopefully ought to be relatively smooth for implementations uh so this table enumerates the old API endpoints uh with the corresponding view ones and there's actually a one-to-one analog in almost every case so the migration hopefully would just consist of adopting new message types um since in some cases the message is no longer need to contain information like a task ID or some other unique identifier as that's been hoisted up into the to the URI um and you know update the path of which a message is handled but the actual message handling code shouldn't change much the exception is handling of aggregate shares which in Depo 2 uh was one synchronous post request to initiate uh from the leader to the helper to initiate the construction of an aggregate chair and then obtain it uh in this proposal I've I've um changed the handling of aggregate chairs to align it with collects sorry the leaders collect uh resource primarily this is for symmetry since the two resources the helper aggregate chair and the leader collection is that they're very much the same uh this also enables the helper to asynchronously compute aggregate chairs since that process can be expensive and take some time okay next slide please um so there's a lot more to all this that we didn't have time to discuss today and a lot more work to do so all of this needs some more analysis to show that we can do error recovery and all the cases that we care about um and there's some open design questions so for one thing does it make sense to align the aggregate chair resource on the helper with the collection resource on the leader further in my opinion The Collection resource right now it's rather awkward I'm not sure it's the right noun that we have um so Chris Patton suggested that"
  },
  {
    "startTime": "00:18:01",
    "text": "we could call this a collection job instead which I think is better but maybe we should think about this as a query right in the sense that the collector is making a query against an aggregate that's been compiled by the aggregators um the other thing is there's some awkwardness in the collect API that we didn't discuss just now because we don't have time um that's there because we're trying to write one API for the collection flow that accommodates both the time interval and fixed size query types that Chris discussed just earlier so I think we should bash out like is this a good goal or should we accept that these two things are fundamentally different and surface two different apis um each better fit to the specific task so I'll close with a call to action here uh which is that um if you've been following all this Gap in PPM work and you find it interesting but you're you know but you don't have a good handle on this Euro knowledge proofs and so on but you do know your puts from your posts please come and help us out with this um so there's a pull request linked here um and uh otherwise we'd love yes we'd love to hear from you there on GitHub in the PPM mailing list on slack wherever all right uh that's it thanks very much Martin Thompson that's a hell of a walk um Martin Thompson um thanks for walking through this Tim can you walk back a slide or two right so I'm singing a bunch of effectively URI templates which is all reasonable uh my question is uh does the client determine any of the things that are in those curly braces on in those urls yes so the client chooses the report ID when it uploads a report um additionally well I'll let Chris and Shan speak to the task provisioning extension later but uh yeah I think"
  },
  {
    "startTime": "00:20:00",
    "text": "they'll explain how the task ID can in that setting have something to do with the client that tends to produce interesting problems um it may be that you're far enough down the down the path that it doesn't matter as much in this context but if you have the ability for clients to for the same for instance for the same task ID have multiple clients they might for instance produce the same report ID or whatever other pieces you have there which creates um potential collisions and other things the usual practice here is to have the client request the creation of a resource and the server to tell it where it is and that means using your you're losing some of your item potency but in exchange you um you get a lot more resilience on the server side against things like clients that might accidentally or maliciously try to get collisions out of the resource identifiers it also gives the server a little bit more control over how it structures identifiers for its own purposes so if the server is in a position where it needs to know how to route some of the um the queries from to each one of these resources if it gets to choose the identifiers then it can do things to um optimize and uh change the way it processes these requests so I would recommend maybe in the cases where you're trying to create a resource you you look at using post with um say a 201 response that contains the location of the resources created as a result you can get back a lot of your item potency by making that create thing do nothing of consequence and then have put to update the content of those resources once you're done if if you particularly care about those sorts of things I am a bit where one of the nice properties of prio hopefully the mic is better now by the way uh one of the nice properties of systems like prio is that"
  },
  {
    "startTime": "00:22:00",
    "text": "the client only speaks once um and I think that's something we should strive to maintain but I do take your point about the risks of clients choosing report IDs as well taken yeah so the other thing is it's totally okay to use post uh and trade it like it has item put in somatics because you know what the somatics of the the operation is and so in the case where you're posting and you didn't get a response from the server and you're reasonably confident it's got some other information that that can be used to prevent a replay or something along those lines then it's totally okay to do another post it's fine you won't necessarily engage all the automated logic um that might be in some Stacks to to do the post again but then again if you're doing it in a browser you might get retries on posts anyway just saying okay let's uh let's keep going Chris do you want to try sharing slides uh again foreign oh yeah here we go so uh we're on to uh differential privacy right all right everyone can see that cool all right um well oh actually let me set my timer real quick okay um so this is actually not going to be that specific this is more about uh the land of PPM in general um we've been talking about uh the composition of differential privacy with protocols like dap um in various different venues and you know we're reasonably confident there's a lot we can do here but to date this working group at least hasn't come up"
  },
  {
    "startTime": "00:24:00",
    "text": "with anything concrete either in the DAP spec the vdap spec or anywhere else in the in the in the PPU working group um what I want to try to pitch today is that we need to start working on a draft that provides some concrete guidelines for integrating specific differential privacy mechanisms with specific PPM protocols so just to get a little at the motivation here um let's ask I think it's the starting point should be like what does dap provide a protocol like that provide and what does it not provide so dap specifically provides NPC style security guarantees where basically you want that the ad The Collector can compute some aggregate of over some measurements without seeing the individual measurements themselves so in our threat model from debt this is certainly like unnecessary property for privacy right um but there's really no reason to think that this is going to be sufficient for every application and kind of the canonical example of this uh Chris Wood brought up at the last ITF I have a link here to the slides you should go check it out um which there's this risk of overexposing a user if say some automated system has uh that measures the client multiple times in in a single batch or across multiple batches over time either way there's this risk of overexposing information about an individual user so um what can we do about this well I think mechanically at the at the level of depth there's there's really not much we can do that's going to be good for all situations um so the way you know an alternative way to approach this is to kind of formalize what do we mean by privacy um and one answer to that question is this notion of differential privacy which has been around for a long time um and I'll give a quick overview of my my own understanding of how DP works you basically imagine you have some"
  },
  {
    "startTime": "00:26:00",
    "text": "randomized query algorithm that um is exposing Aggregates over measurements and the property that we want is the distribution of the output of the aggregate should not depend significantly on any one individual's measurement and we can formalize this by thinking of uh the the the uh the difference in the distribution of the output between two databases that differ in exactly one measurement so it's kind of like a intuitive example of how this would work um if we have some secure method for computing uh the aggregate uh to make it differentially private private what we do basically is sample uh randomly some noise from some appropriate distribution and instead of handing to The Collector the aggregate we hand to the collector the sum of the aggregate with the random noise and intuitively if we perturb the output enough uh then the idea is that we hide the contribution of any one individual measurement so that's a really nice intuitive idea um but uh differential privacy has a lot of subtleties to it um the main kind of consideration when you're applying DP is the Privacy budget so basically the degree of privacy that you get from the system is going to depend on how many queries you allow in the system as well as what is the exact nature of those queries and um you know but despite this complexity I think there is a there's a clear win here to be able to compose uh PPM protocols NPC style security goals like dap or even this uh the security of what security properties we get from something like star uh differential privacy is going to be interesting for a lot of different applications so the question is how um and in all of the discussions I think"
  },
  {
    "startTime": "00:28:00",
    "text": "the the main thing to take away is that uh there's not one like clear there's not one solution that's going to fit every protocol the the the the the the the the the kind of the ideal mechanism or even like the set of suitable mechanisms is going to depend first of all first off on your base protocol so differential privacy is going to look very different but for star versus uh versus dap um and also you have to be very careful about considering the application and uh and the nature of the data that you're collecting so um I think I you know I think there's I there's a lot of open questions here about what would be the precise scope of such a draft so on the one hand it could just be like guidelines like this is this is uh you know uh this these are different DP mechanisms that might apply to a given protocol um one thing that would be really useful something that the ITF is quite good at is you know spelling out algorithms for sampling noise from a given distribution basically take debut random and map it to uh you know a random point in a LaPlace distribution or whatever um and then you know there's like you know like guidelines for enforcing uh privacy budgets I mean as a cryptographer in my analogy is uh like something like safety margins for an aead scheme encryption scheme uh is there an analogous kind of set of guidelines we can develop for differential privacy um and then another idea is you know like something we can do is spell up concrete mechanisms um there are lots there are like you know lots of Prior work on this that that that uh apply more or less directly to protocols that we have already um so yeah I think there's a lot of interesting stuff we can do here I'm kind of just throwing out ideas um I would love if someone in the room is who has some expertise in"
  },
  {
    "startTime": "00:30:01",
    "text": "differential privacy uh had a strong opinion about uh what we should do here uh and with that um we have uh five minutes for questions all right Erica squirrel out um as I say more a comment than a question um we should do nothing here um we have like already a very complicated specification that we're trying to get out the door these are on these are they're independent pieces of work and we should not do this one until the first one is done um is consuming the exact same excuse me the exact same resources so like I think it's I I don't mean to give the impression I don't think it's important I think we should sequence it next but I don't think I think that by trying to do them both like the only thing we should be doing right now is making appropriate changes to adapt to make this possible if necessary and if there are none we should do nothing um I I say and again I don't mean to make light of this problem I think it's a very real problem but I think it's also a problem that is not like very straightforward and so I'd like to like keep like got my eye on the ball thanks Ecker uh Jonathan Holdens uh this this is more I guess related to the first presentation in a way um when you have different query types uh does that break or make differential privacy harder or worse because there's different groupings of queries that are now possible and you know how to consider the interaction between them on how it leaks information yeah so um one thing that we're trying to guarantee in the spec is that regardless of the query type you you have a like a proper partitioning of the reports so no report is going to be used in more than one batch that's something we try to guarantee um however like to your point though there are considerations for differential privacy the main one it would be um the size of the batch so um my understanding and this is like I"
  },
  {
    "startTime": "00:32:01",
    "text": "you know I hope I hope I see Charlie Harrison in the room and I'm calling him out to to correct me here uh my understanding is that we have to um use like basically the the the the the the batch size that we're aiming for to tune our differential privacy parameters um so for fixed size for example we enforce a maximum batch size and that's like one of the considerations there yeah I I can just quickly speak to that like the batch size I think the batch size is important for some of the deployments that we're considering for differential privacy like namely the deployments that are similar to the uh uh covid um uh privacy preserving analytics like that info work where noise is added on the client in that case the total amount of privacy is based on how many clients you're kind of adding together because you're summing a lot of noise from a lot of clients in the central case where the like aggregators themselves are adding noise the batch size is not related to the privacy of the output it's only related to the kind of like relative area that you're going to get so if uh maybe this is just completely mistaken if a record is queried let's say I do a query that says I want to know the average of all things every single record I will know what the average is does that mean I now can no longer do any queries because they can't appear in two queries I think that question is Chris you're on mute you're still on mute can you repeat the question if if I if I make a query that says"
  },
  {
    "startTime": "00:34:00",
    "text": "uh give me the average of all things I want to know every single record what's the average does that mean I can no longer do any more queries because they've all appeared in one report what one at some point you have to start you have to stop like you don't get to do like rolling a rolling average like give me the average of everything so far uh at least in depth what we're requiring you to do is at some point you have to say this is the end of my batch um so yeah the intent is that batches never overlap right I mean this goes back to the discussion we had sir Erica discussion we had last time about drill down right and um and so once again we're going to address that problem at some point um though I understand we've said not address it like at this exact moment and that's another reason why the DP problem would be so difficult I agree uh I think that's it for the DAP editor's time thanks everybody great so uh I see Eric back in the queue um but Chris and Sean this is your uh your time for in-band task provisioning Sean I saw you yeah I'll just ask to share slides um sorry can anyone remind me how do I share slides obviously Chris is passing me control for sharing slides but I don't know where else do I click"
  },
  {
    "startTime": "00:36:04",
    "text": "to try one more time I'm going to reboot your slides try once more foreign there you go um let me turn on my camera as well hi everyone um next I'll talk about the inbound task provision right now it's a individual individual draft for extension to the core depth protocol uh first of all let's talk about the motivations so today the dev protocol doesn't actually Define how a task should be provisioned or configured it basically mentions it will be done out of band um we delete our helpers agree that a particular mechanism to share the parameters for configuration as an provision tasks securely but here we want to introduce a new mechanism for provisioning a task purely through the existing flows especially the upload flow and the aggregator share flow and use the extension mechanism without introducing any extra flows and hopefully this will be useful for many deploy deployments and many return helper diplomas can just provision tasks in the same way without defining any extra candidates now the basic protocol architecture here we introduce a concept called a task author author is basically a logical participant that it defines the task it"
  },
  {
    "startTime": "00:38:00",
    "text": "defines what configurations goes into a task and we assume it has ability to send the task configuration objects which I'll show later what it contains to the clients now in reality also could be implemented by the leader or The Collector so we don't have any more trust to task also then we have on the aggregators so the basic flow is the following the author will set the task config to the clients the clients will verify the task config make sure it makes sense and then when it decides to opt into a task it will send the report as usual but contains task config as part of the extension data and this report will be received by the leader which is going to do its own uh procedures for checking the task configs and verifying tasks makes sense and then share the exact same task config with a helper using the report share struct as defined today in in Dev core protocol so the key Point here is there is no task provisioned in advance until the very first report is received from the reader and also there's no need to Define any Auto Body mechanism between leaders and helpers for Tasker configuration purpose now if the aggregator does not support task proof extension in today's draft we will simply ignore the task config and depress it as usual as in the task has been configured in other mechanisms so the task config object itself is basically the task specific parameters described in the corporate call here we basically group them into different structs based on their purpose there is a query config which includes"
  },
  {
    "startTime": "00:40:01",
    "text": "things like minimum batch size maximum batch size for example for the fixed size query and the redef config contains over the specific configurations like the type of the redef and the buckets in Need for for the normal preo3 histogram and here we because the tasks are the task is created on the fly so um it's necessary to have a mechanism for all parties to derive the same task ID based on the same task configuration so a task ID here is simply created by a shadow 56 hash on the serialized task config object just go into some details of the client side Clan simply receive the task config sets existing type to task proof and then encode the task config in part of extension data which itself is part of report metadata and all the aggregator side you need an helper they both will check the task configs received and run the same task ID generation mechanism and make sure the task ID costing measures the generative task ID um and here often basically means the the provision task is either unrecognized which means the leader or helper needs to provision this task or the config received matches are already configured task uh we haven't mentioned about the collector side but hopefully the clutches are will be very simple it is very much oblivious to the use of task proof um it should have ability to receive task config from task ulcer but after that it simply sends the collected request uh including a task ID you know generated from the same task ID hashing"
  },
  {
    "startTime": "00:42:02",
    "text": "mechanism and here are some links for the draft and the GitHub repo to uh there is one reference implementation in Daphne which is still in progress so the main purpose of this presentation is to show everybody that task proof is useful for many deployments it may not be useful for all deployments but we want to gather more feedbacks and see whether the group wants to adapt it as a working group extension uh that's it so I'll use the many formats for questions please here for squirrel up [Music] um I guess I'm not really persuaded by the problem state um I certainly understand why you might want to have um uh Dynamic configuration but I don't understand why telling for the client is a good idea it seems to me that like basically that the there's a relationship between between the clock between the between the The Collector and the leader and Helper and there should be the collector on the client and like that's the appropriate place that this information to be carried and I don't understand why you're selling for the client but it seems like like not not the design not an actual design okay so if I understand correctly you're asking why we are sending a toss config to the client no I understand why the client needs the test config why are you tunneling it to the client that is why don't you provide the task config directly from The Collector to the leader and helper um that's basically it was a core protocol is suggesting but I think the problem is there isn't a specified mechanic for doing that so if you have a leader helper if your organization manages many of these are Health repairs then you need to make sure either you have a unified mechanism for all these other organizations that working with you as a aggregator or you have to maintain basically different ways for uh Distributing cost configs to to all"
  },
  {
    "startTime": "00:44:02",
    "text": "these other projects you misunderstanding me I'm not opposing having a protocol for for provisioning the test config what I'm saying is that protocol should be two separate protocols one for the well really the only protocols actually needed is the one from The Collector to leader and Helper and that should be standardized and the client should not be part of the picture um you certainly could stand arise from collectors or your helper but I think there are extra benefits when it comes from the client side so in this case the client knows exactly uh how the task will be created so the task the current report will be aggregated in will have exactly the same task config as the client have seen it if you started out just between collector I need a helper so you're basically asking the client to trust whatever the leader helper the leader or uh or task author sending it is exactly the same as the server side used to provision the task yeah are you talking about malice or incompetence um are you talking about malice or incompetence but now let's learn confidence by the by the by the malice or incompetence oh minus yeah um no I'm talking about a cough this okay but like if you want that this is your this is your problem then we should just have checksums on all the configs which we in fact discussed previously um so but like here's the thing the client cannot implement this cannot the task config is not even remotely sufficient for the client to implement this because a client needs instrumentation in the client code to collect the data that goes the testing thing so like the client needs all kinds of garbage and not just the test config and so like that and that needs to be delivered by some other channel the client has all that stuff so like yeah like again like again I'm on board with Dynamic configuration but this is like not the right approach"
  },
  {
    "startTime": "00:46:00",
    "text": "I'm sorry Chris did you want to respond to me or um or uh it's the other garbage what the like probes in the code that like what the data oh yeah sure yeah actual software I mean the thing I understand is like that that like that and Dan isn't like that this is the reason we didn't do this in the first place um was because in order to make this work you've got to like modify the client code and so like I mean even remotely provisioning the client is almost impossible because you got a modify the client code and like the number of clients that can be really Vision with like new code with lack of code update is very small right and so like like that's that's the machine that's gone Nick hmm I will be brief I I think my question was uh I could use the benefits from doing um in band uh provisioning and the draft seems to suggest that the client will decide based on some of that configuration whether this seems like an acceptable task or not um but if we are going to do that I feel like there would need to be more richness in that configuration for the client to make a good decision about what's acceptable or or not that that seems like a set of privacy type position and that's going to need a lot more richness than just batch size yeah so there are some texts we think the client could do for example we introduce the task expiration from a photograph to in depth the client could check whether the cost has expired and is not worth participating anymore um there could be more advanced checks like if we introduce DP whether we do that is another question but if we do introduce DPE we might not want to check out the"
  },
  {
    "startTime": "00:48:00",
    "text": "client side of whether the client is willing to participate in a task with a certain a certain DP guaranteeing um yeah hi uh what I want to say is that I'm speaking as a somebody who implemented a client that reports metrics uh I I appreciate ecker's point that uh it takes code to implement new metrics but there it does seem to me like there is room here for essentially Dynamic reconfiguration of how especially numeric values are reported so if I decide that I've been measuring the average of some value but actually I want to switch and start measuring the histogram of that value do I have to push out new code to all of my clients uh you know do I have to reach all of my clients through some sort of external uh control system or can that actually be done in band through depth that is that that kind of narrow use case I think there could be value here yeah I think there are two different things here this one it's a consort implementation uh and the other is uh the Tasker distribution I think in today's step task distribution is not defined it's very much uh deployment specific or outside of that scope but for the client side I think there are many scopes for the client to do things to make sure the the Privacy guarantees and the transparency it provides for example we would like clients to log all the toxic conflicts it has received so Cloud itself knows what kind of a task it has participated in so you know that's true yes the client will have to do extra work to uh Implement these but uh if you're considering you're already"
  },
  {
    "startTime": "00:50:00",
    "text": "receiving touch config from the server and you might also wants to log them then just putting it into the metadata and send it to the the server side I don't think it's a massive leap from what's already being done sorry it's me again Chris unless are you using the queue Chris or you're gonna say something I mean go ahead no um I mean Ben I mean you're certainly right like it might be nice to reconfigure the client like without having to load new code but like that's not going to be done first of all like this doesn't provide the channel for doing that so you still need a channel and that channel is like not going to be done with like like with high probability that's any news has configs it's going to come with like some whatever remote configuration mechanism your your product already has and like and that that's again a much more Rich mechanism than this thing and so like it just don't I just don't think it's very possible like the task configure not really enough like reconfigure the client but again I guess I just think that's like missing the main point which is that like that you're like not induced to a good reason to Tunnel this data through the client to the the different Helper and it was much more sensible is to like configure that directly downward from and in both locations and then if you want to compare them compare hashes but like it's just I just I don't understand why you like like like like I just think the telling thing is to keep a half so um I'll just quickly say so uh I want to point out first of all that there is much more to the thing that is actually serialized in the extension than just like say the Min batch size um everything that controls the aggregator's behavior um at least uh like in the context of like the interoperable protocol um is is specified in there um so it's important you know for for many reasons for everyone to make a decision as to whether to opt into the task so in particular like you know the"
  },
  {
    "startTime": "00:52:00",
    "text": "the the the vdef that you're going to use like preo three sum or histogram Etc is going to be it's going to be part of that configuration and the second thing I just like taking a step back um we don't ex we're not asking for an architecture change to DAP um the question here is really like is this the scope of a protocol extension um and uh like if is this the sort of thing is this the sort of like Behavior change that uh folks think is is is useful to make um okay that's it thanks yeah I think we're gonna leave it there and move on to Star Siobhan Europe yeah um should I start I think I can control my slides using my phone yeah but Al um Siobhan here um I'll be talking about star which is distributed secret sharing for um threshold aggregation reporting yeah so the main idea is that um we're getting K anonymity for for clients um and they're reporting measurements to an untrusted server um the goals are it should be cheap fast simple and obviously private um that's what we're doing over here um just a very quick I guess overview similar to what I uh did last time is the idea is that the client wants to send a Telemetry value to the server but only wants the server to see this if there are at least K other submissions of the same value by other clients so as an example like a Json you know blob of like City Vancouver you can imagine the client might want to send something like this to the server so the client generates a key by um a deterministic output of um like hashing it's like the main measurement that it has and then it encrypts that using the same key um so so if multiple clients have the same value same measurement then they will get the same key uh and then they all generate the secret share of that key and they send the server the"
  },
  {
    "startTime": "00:54:00",
    "text": "encrypted message and the secret share of the key and on the other side if and only if the server gets care shares it can recover the original key and then you can decrypt the encrypted message this is not a new idea this is um but we are basically using it for privacy preserving measurement it's also really important that there is an uh like a proxy in between um the plan is to use Ohi but you can you know use your favorite um Network for that you can use Tor and there's also this idea for Randomness server so in case the measurement has low entropy uh it's a low interest low entropy space then uh to prevent like attacks from like prevent the server from brute forcing all possible measurements you um you use a randomly server to get the to stop to get the search and the randomness server uses a voprf to so that it doesn't learn the input value but still can operate it can still like provide the randomness um there was some feedback on the list um we've been getting a bunch and there was the idea was a Dos attack using corrupt reports where essentially the client wants to prevent recovery of a single value like a given Telemetry value so it sends a random secret share for a given tag so we worked on this and we addressed it using uh verifiable secret sharing um so we have this idea of a share commitment now that becomes a tag and verifiable secret sharing allows checking if a particular share is valid but like importantly before you do recovery so you're not wasting Cycles this adds a little bit of computation cost uh it's about a big goal of k um in badminton competition so it's moving on um but yeah so I guess I just wanted to give a note on implementation it's shipping in the brave browser we the rest implementation was the original one and we had some we have some bindings for that but there's also a new one that Chris Wood wrote in go uh who's also a co-author now on the draft um and I think that happens a lot more up to date with the raft but yeah just"
  },
  {
    "startTime": "00:56:01",
    "text": "what's new in the newest version is that we specify the verifiable and unverifiable secret sharing uh we're refracting the document to be easier to implement like Chris did a bunch of work on helping us with the defining the cryptographic apis and functions we also have defined this the protocol message types for Ayana and yeah and we also talk about garbage reports which is this idea that the client generates a key from one message but encrypts and sends a different message so in this case the recovery happens correctly but the value will be like garbage um so there's a couple of different ways to address this Ecker pointed out that throwing out the whole battery again this causes simple Doss so we definitely don't want to do that you can do like a majority vote um but like one idea that we had was um you could use blind signatures instead of an oprf um and the idea here is that you would bundle the signature that you get um from the operation of a blind signature to um and then send that in your in your encrypted message and then when the the aggregation server recovers the message it can uh also check the signature verify it against the public key of the randomness server and try to get that back we it's not like it's this we haven't defined this yet in the draft but we at least describe the problem uh but anyway so I guess we're calling it Superstar now but this idea that you can have um you could pick your secret sharing scheme of choice you can picture pick your signature scheme or protocol of choice uh and then they give you like more or less uh protection against client threats so um so I guess we kind of recommending the third option for a lot of implementation like the user um oh sorry the second one the verifiable secret sharing and the regular prf you can prevent the trivial dos attack I think that's really important but if you have if you also want to prevent the bad Cypress attack you can also use um verifiable secret sharing and blind signatures and there's like increasing implementation complexity and cost to this"
  },
  {
    "startTime": "00:58:01",
    "text": "um but yeah I mean just if you wanted to leave some time for questions but it seems to be pretty strong interest in Star and we have been addressing feedback from the working group and it's been improving the document so I think it makes sense to do it within the working group um yeah happy to take questions okay you're up I'm sorry Martin Thompson is first I'm uh I'm reading from the bottom Martin Martin is Martin has first uh first cut um thanks for doing this I think this sort of helps a lot um I am seriously concerned about the computation cost of recovering values once the value of K gets big I just um you you said it was okay I think uh Alex and I and chat have both believed it's K squared um because each each submission requires that you perform a computation across K values in order to uh to generate the confirmation value so every submission contains k um field elements um so that is scary um it would be interesting to see how this sort of compares to some of the alternative like how does this compare to popular in terms of the computation cost involved how does it work in practice as well because I think that if you're talking about a k of 100 doing 10 000 plus ristretto computations is probably okay but if it's ten thousand but yeah it starts looking a little scary yeah I agree I think um some more analysis on the performance aspect of it and how it compares to to that I think especially would be pretty interesting um yeah I don't particularly care as much about the bad cycle text thing but it seems to me like you're going to get uh in the"
  },
  {
    "startTime": "01:00:00",
    "text": "case of 100 you're going to get 100 submissions but and some of them will be good and some of them might be and most of them will probably be good and if most of them aren't good which is the um sort of the point where the verify verifiable secret sharing breaks down anyway you won't be able to you won't be able to recover them if someone wants you just to get garbage anyway so I'm not super concerned about that one but the verifiable secret sharing I would love to find a more efficient version thanks yeah Eric scroll up um support I got to say what Martin said um I think you know the primary value proposition for this work ahead of popular performance and so it was not faster than coupler then it's kind of like hard to justify so I think we need to see some some analysis of that um and I agree that the case where I think is pretty scary um I think maybe you meant it was okay over the exist multiplied by the existing thing which is true um because right now you have to do okay computations and now you have to do okay computations per client so whatever um but it is okay Squad I agree with Martin um um second um I said this list um there's like way too much crypto in here crypto in here crypto um this like this needs to be cut apart in two pieces one piece needs to go to cfrg and one piece needs to happen here so um so would you say like we should block adoption until we do that yeah absolutely so there is a draft in cfrg right now the frost one that has like that also uses and defines verifiable secret sharing um so I I think you know cfrg folks one are interested I think we could refactor that document and like so that you know it can be serve as a pointer to like both documents sure I guess my point is like if this like contains stuff but like like this is gfp a lot it has like you know scalar mold like that is like way way too much for this like the the way this needs to work is that there needs to be a box that is called verifiable secret sharing it is sucked in and that is like consumed the same way as Dash consumes like you know in the same way as you know a TLS consumes x25509 in the same way as like you know"
  },
  {
    "startTime": "01:02:00",
    "text": "um as daf consumes uh um you know um popular and like it's like okay and the reason for this is interest like isn't just like um you know um persnicketing this about like you know the uh about like the purity of IDF it's about like who can do the review and where it has to happen and like you know like I I like skimmed like the VSS thing and like sort of like it's like how much time it takes you to persuade myself this is correct and the purpose of insistency I've really ensure that it's correct foreign splitting the document or at least making it really clear would only help the working group because it's going to get to the back end of working group last call it's going to come to Pub wreck for me and the first question I'm going to have is what's the process by which the crypto was kind of verified did you go to crypto panel and now you're trying to get resources from the crypto panel and if you have that as a work product from cfrg it probably will go faster I guess my sense was that like cfrg already has an adopted document that defines all of this as a sub part of that so it seemed fine to me um just link to it now I have to read the documents and persuade myself as exactly the same algorithms in Frost I mean that's like consider a problem right and if it's already there then like I mean I guess like yeah if like things already exist and you can just refactor that's great I'm just saying like that it has to be like like there needs to be enough crypto a little of crypto in here that like we can persuade we can just read the protocol documents and critical documents um but again I think the threshold question really is like is like this performance question like that's the thing I think our sense for the performance was that it should still be pretty good um but yeah yeah yeah I don't want to deliver the point I haven't looked at the draft in sufficient detail but to me just to be in a very kind of crude metric there's going to be a lot of things that look like kind of crypto if it comes with a draft that says irtf when we go through the even kind of pass me in the IU view it's gonna be like good to go it came from crfg it obviously got reviewed with us it's gonna if it comes from the from"
  },
  {
    "startTime": "01:04:00",
    "text": "the an ITF worker oh wow like let me try to kind of pull this apart okay this one's a reference this one's not and again I mean I think it will go faster makes sense um yeah I'll defer to the chairs about the adoption okay thank you everybody for the Lively and efficient session and uh I think we are all done for this edition of privacy preserving measurement all right thank you all very much foreign"
  }
]
