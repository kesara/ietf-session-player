[
  {
    "startTime": "00:00:22",
    "text": "we'll get started in a couple of minutes you have people time to finish joining all right welcome everyone to the tools team meeting for May uh keep in mind that this session is being recorded and the recording will be posted to YouTube we are um"
  },
  {
    "startTime": "00:02:00",
    "text": "several several weeks have gone by since our last meeting because we skipped the meeting after ietf 116 so we have a lot to cover does anybody have any bashes to the agenda that they've not already added to the uh the notes page and everybody has the notes page in front of them yes not hearing any bashes um first item that I've got is to congratulate Jennifer for joining the LLC as a senior developer so yay um we have Paul Selkirk he's um sharing video yeah just waved his hands joining us from painless security has already landed a PR it's been merged into Maine should be part of this week's release so well on our way good to start so we had a tools team retreat at the end of April right before the beginning of may we met in Montreal we had Lars for one day I bet we spent quite a bit of time talking about how we're going to prepare for the change in the I.T infrastructure how we were handling our basic deployments um the interaction with the CDN how we're going to be doing releases going forward automating our releases into a you know cloud an infrastructure that assumes that we're deploying into into cloud services um and then you can read the rest of the bullet list if you've got any questions about the things that we went into let me know um one note at the end is that we do"
  },
  {
    "startTime": "00:04:00",
    "text": "know we've got a few um strategy paths that were um going to start pursuing and we're going to write those up and put them out for Community feedback over the next few weeks so far I haven't heard anybody say anything I'm assuming audio was working for me but if you said something and I haven't heard it please um raise raise an issue anybody have any questions about the tools yeah thank you we've started making fairly rapid progress on the move of DNS into cloudflare and the revision of DNS sec to use a more modern signing algorithm you can track the link that's in the notes to the card on the road map that shows where we are we've finished irtf.org we are going to start with RSC editor.org later today ietf.org will go last be either later this week or next week unless we run into trouble we also have a large number of domains that we're holding because we don't want someone else to use them but they're not really doing anything at the most they're doing a redirect most of them are just doing nothing um those are in the process of of being moved um we had expected to have them done before this call but have been running into some impedance with that registrar um but I think we're working through it this has been quite an eye-opening"
  },
  {
    "startTime": "00:06:02",
    "text": "experience for me with just how difficult it can be to deal with the uh as they call it losing registrar when you're changing the registrar for a domain and in particular some of the registrars that we've been using are not the same companies that they were when we initially started using them if I have any questions about our changes to DNS all right the next item that we've got to report on and possibly discuss we have our meeting wikis spread out across several Technologies some of them had been in track um as we'll talk about a little bit later on track's been turned off um the uh we do have the flat HTML that we created and we have the meeting Wiki part of the um flat archive that we made for those meeting wikis that we're in track we also have several meeting wikis that were in docu Wiki um sitting under www.ietf.org registration and then modern meeting wikis are in Wiki Js we already have some rfcs that have linked into meeting wikis the RSC that talks about running hackathons in particular has many links into um these older meeting wikis we've been discussing whether or not to continue to try to hold on to these things as not Wiki artifacts but"
  },
  {
    "startTime": "00:08:03",
    "text": "something unrolled that could be attached to the proceedings or if we should just move to not preserving them ourselves but using archivets Service as the archive and I think that is the path that we have um landed on as an operational recommendation so that we're not continuing to carry separate bits we just make sure that the wiki themselves got wrapped up and archive it at some point and we just drop our versions of the wikis at the point that the meanings are finalized anybody have any comments concerns okay do you want to say it go ahead Alexis um I do want to say uh web archiving is an awesome idea I think that is great we should do it regardless um it can be a very finicky kind of process to make sure you actually got every single URL so it's not a it's not a tiny undertaking but um we we just need to make sure at least the when we do it that we are very very thorough so I think you're going to want to give like a month or two of leeway to make sure that we got through everything because the the crawls can run for like a week at a time and then you need to do like QA make sure you've got everything figure it out go run another crawl etc etc so I just want to give you a heads up that that is a relatively uh ponderous process sure all right John do you want to speak a little bit to this item that we've got on our current dmart failure for aliases"
  },
  {
    "startTime": "00:10:05",
    "text": "uh sure um as I'm as I'm sure you've all noticed um when mail gets sent to ITF aliases um it frequently disappears particularly if you're a Gmail and this turns out to be because people send people's organizations have published dmarc policies that say people's project which is very which is very very secure and which means that any mail that's received from them has to have a valid dmarc signature or come from an authorized IP address and it turns out that when mail goes through our our forwarder the headers get reformatted just enough to break the dmarc signatures and since it's coming from our from our forward or rather than from the original Source it doesn't come from the authorized source so the dmarc fails and the mail disappears so it seemed kind of like why was it reorganizing the the why was it reformatting the header so I took a look in the code which is ancient and messy and the problem is essentially that it uses a mind parsing library to parse the entire message and then it de-parses it on the way out and the mime Library thoughtfully reformats the headers to make them look nice um and so it would not be super hard to kind of patch around that and in fact it does that in a couple of places there's one place where it does demarc rewrites and it actually specifically checks to say oh if I didn't do any rewrite just use the original text um and I can pretty much go and it wouldn't be super hard to go back and Patch around the places where it does it in other places would be not totally trivial because sometimes it adds some headers you need to part you need to splice it into the string but I'm reluctant to do it just because the module is such a piece of garbage it's got two thirds of two-thirds of the module are copies of random random libraries that duplicate stuff that's now in the standard python library and there's places where like it reads its own it reads its own source code to look look for the string arguments to put in"
  },
  {
    "startTime": "00:12:01",
    "text": "the help text so kind of depending on how long it's going to take before we want to redo it I mean if it's going to be a year it's probably worth fixing if it's only going to be a month or two I would rather push to rewrite the whole thing in Python 3 because it's going to end up a quarter in the size and more functional oh yeah the end it's the final thing I put in the in the in the in the notes which is even if we fix the dmarc problem there are a significant number of people who only authorize only authenticate with SPF which means that even even if we even if we did nothing whatsoever and pass through the literal bits the dmarc would still fail so we still need to do some sort of hack similar to what we do for rewriting the addresses on mailing lists which is ugly but anyway around it John just to be clear is it for the aliasis like a draft dot all or something like that not for the meaningless right uh sorry it's hard to hear you yeah sorry the issue is only when you are using aliasis like blah blah blah blah dot all yeah well if it doesn't go through no it has the same our mailing lists have the same issue but we have a bit but we slapped the Band-Aid on the on the mailing list that rewrites the from line so this is yeah this is this is yeah the answer to your question is yes this is when when it goes through you know to draft blah blah or to the various role accounts you know it's mailing to the I to the uh yeah thank you so the rewrite that John's talking about is to Python 3 is going to come with mailman 3 right now the the library that um we're talking about is necessarily um Python 2.7 because it integrates with mailman T quite closely which is a python t7 package"
  },
  {
    "startTime": "00:14:02",
    "text": "[Music] um I actually have a way to work around that I I basically like it would be possible just to write a little stub that just just using a fight to just called enough python 27 because all it's all it's integral all it's doing with mailman is pulling out the names of the mailing lists so we okay if we wanted to we could hack it but again uh is it worth it yeah so the mailman 3 transition is um expected to be later this year um you can see where we have it on our roadmap but there is um uncertainty around how we're going to approach it that is expected to resolve with the RFP process that we're about to go through um we want to understand what our um bitters to the rfps are proposing for management before we have a really good feel for the timing of what we're going to do with these modules so um I think John you and I should continue to talk it might be worth doing a smallish fix um I don't know that I that it would make sense to do a lot of work that's not going to be reused when we get to mailman 3 the thing that you hinted at that if we just leave t7 and write the Python 3 version of the um of the full um post-confirm demon um but just leave it where it has um the integration to"
  },
  {
    "startTime": "00:16:03",
    "text": "the current versions of the connections into post confirm I mean sorry into post fix and the current connections just wrapping the current connections into mailman um we might it might be worth pursuing but let's let's continue that conversation offline and see if we can scope how big that effort might be yeah I mean like the Deacon fixed is maybe it is maybe a day to do more or less so yeah we can figure out whether it's worth it anybody have any other questions things they want to discuss people want to volunteer to jump in and write help rewrite the code okay kasara we've got a um IAB website transition effort in progress the uh one of the things that we've been working on is extracting the uh um artifacts that are currently in WordPress and in the word in in the primary website um reflecting IAB minutes and IB statements in the primary website we've got iasg statements um we're going to be moving these into the data tracker we have had um several discussions over the last week about what the modeling for that is going to be that it's pretty well understood what the modeling for the minutes are going to be and what the modeling for the statements are going to be um one of the sets of artifacts that are out there are appeals we're having a discussion about"
  },
  {
    "startTime": "00:18:03",
    "text": "how we're going to model appeals going forward as they have a different structure from other just document types and that there's a uh appeal response and for the correspondence relationship that we need to make sure that's um natural to work with as as we go forward so um there's also quite a bit of work that's been taking place on modifying our set of wrappers our extensions if you would to wagtail to support moving the IAB website onto a wagtail instance that's very similar to our www.itf.org wagtail instance because I already want to talk for just a little bit about the activity that's been going in the testing that's been going on there so um spring load is working on the developments for moving vectors moving IAP to sector website we are going to use multi-site keychain reactive web tale to do this so codebase is going to be a single chord base for both ietf technology and IAB but we will be hosting those two in two different instances because if it if you get that for example in search in documents there can be a spillover from IET a website to"
  },
  {
    "startTime": "00:20:03",
    "text": "the IAB so it's better to have two different instances most of the work is done and there's some bugs that need to be fixed so we are probably be looking at another couple of weeks to get this into production we have meeting today with spring Lord to talk about remain initials awesome you may have any questions comments Gregor um Cindy is there anything that we left out no just thank you guys for all of the work on this all right Jay do you want to talk about the uh trust initiated tools changes okay audio in place great um so um as explained um uh the trust have explained to me their concerns about content licensing for wikis and con uh contribution attribution for Wiki changes proposed on GitHub um the uh I'm reading this out loud but I'm assuming many of you haven't read it the initial view of the truss was that some of our current practices could no longer Contin could continue and so I've proposed some possible Technical Solutions to them which they accept and which I've um started the process of discussing with the community so the first one the one I've already um um the Rays of the community is um content attribution or contribution attribution it should say for wikis um uh their concern is that"
  },
  {
    "startTime": "00:22:01",
    "text": "contributions made via GitHub to the wikis are not sufficiently attributable to fulfill the requirements of the note well and their initial view was that um we should not allow people to edit the wiki via GitHub um I have suggested instead that we have a mechanism whereby um we can as the um uh maintainers of the wikis um see whether you know have an automated process that tells us if that GitHub user is somehow registered in data tracker and um do the um choose whether or not to accept it that way that's um acceptable to the trust but when we've been when I've raised on the list for discussion people have started arguing about the basic premise of whether or not a GitHub um uh a contribution is sufficiently attributable or not to meet the requirements of the note well and in that specific case I'm only the messenger so I'm hoping that the the direct conversations with the trust will take place and the trust will respond to those on list about that um the other one which I haven't yet raised on the list um is about content licensing for wikis um they are concerned that we can't have a blanket ccby um 4.0 license across the wikis because the wikis contain things that our contributions and need to be licensed um under the TLP the the trust legal Provisions um for example excerpts from rscs and other things now the initial view of the trust is any such content should not be permitted but we've actually had that content there for many many years um on the wikis we've just never been particularly clear about what the licensing of The Wiggies was um and so um I propose that instead we build a macro for the wiki that can be used to flag content under a different licensing"
  },
  {
    "startTime": "00:24:01",
    "text": "scheme so we apply one blanket licensing scheme to the wiki whether that's TLP or ccby is to be decided and then anything is an exception to be that is flagged as under that exception um and uh I haven't yet raised out on the list for discussion at all so that's where we are about those if anyone has any particular thoughts or views of those please let me know if anyone disagrees some of the premises then we need to um somehow um have the the trust engaged on that um about those things okay great well I'll continue on that on the list and um the next one and we'll just see where that goes thank you Robert thank you all right we're through the Section that I anticipated we would have discussion time on um a really quick summary of what's in the fyis if we had a spot where we need to stop and discuss just um speak up the uh rfps for the change to the iot infrastructure are being prepared now we have a lot of um mechanism that we are building for our interaction with cloudflare one of these are we are building tools to better manage our access rules the lists of ips that we always allow to access our sites those that we have blocked because of abuse as the UI at cloudflare for doing these operations leaves a lot to be desired um we significantly changed the way IMAP interacts with the data tracker credential base um we haven't received any complaints over the way that that change"
  },
  {
    "startTime": "00:26:01",
    "text": "um went so um for us it's really good because the IMAP server is not using a fork of the data tracker code anymore using a copy of the data tracker code we have that uh tight coupling dependency now separated into a proper API we spent a lot of time at 116 working with the RPC primarily Gene Alice Sandy and I met for several hours working through the uh first set of things that we're going to be doing on the RPC um tooling refresh specifically on the tool around managing the um the RPC workflow um we've got a lot of user stories have come out of that and the beginnings of design and we're now into starting to um go back and forth on the uh um uh what the interaction with the data tracker is going to look like in detail is just starting to get into wireframing um this is the the meat of this and the implementation part of that's going to come behind the Django 4 um upgrade process after we've got Django 4 deployed because the what we're going to build is very definitely going to be um based on The View right framework and that we really want to have django4 underneath this before we start into that the road map has been up for several weeks we've got feedback that it is useful we added quite a bit of content to it while we were at the tools Retreat please review it if you see anything at"
  },
  {
    "startTime": "00:28:01",
    "text": "any time that is confusing missing raise the point on ietf discuss or directly with us and we'll we'll make sure that the roadmap is giving the community a clear view of what we're actually working towards we finished the data tracker postgres migration we had a hiccup with the timestamps that required uh uh pre-intricate hoop jump to fix but we got through that um we're watching performance with the instrumentation that we've got we're not seeing the overall performances significantly different but performance for pages that people were particularly sensitive to have anecdotally improved we're suspecting that much of that observed people observed Improvement is in parts of the stack that are current tools aren't instrumenting so we're looking at changing the our instrumentation to see further up the stack so that we can see what's happening with performance there we'll be talking to um serious open source later this week to schedule the next step in managing our postgres instances the next task is turning on replication between the postgres instances at itfa and itfc right now our failover strategy is that we would take one of the daily dumps and restore but that would mean that if we had to failover we would lose a chunk of a day we'll have um replication set up over the next several days and then we'll"
  },
  {
    "startTime": "00:30:00",
    "text": "evaluate whether or not there is a significant performance impact for having that replication running we're not expecting there to be one but we'll be monitoring that closely and I'll send in a report once we have that data so before the itf-117 everything will be replicated right we get Austin Mai in July I think that we'll have that before um the end of next week okay thank you so we had a period of time um the beginning of May where our asynchronous container processing draft submissions lost the ability to send email this was a unexpected um change in the behavior of Docker on our primary server Docker compose brought the network for the containers up on a Network that was not what had previously been advertised is the set of networks Docker would you know the the addresses that Docker would come up with with networks so it was outside of the firewall allows on itfa to um let the let the doctor reach out to the out even to the in to the mail server uh once we discovered it um the fix was fairly quick we reached out to every um draft submitter whose submission was affected and we know for certain that their submissions are complete so we don't we've not left anyone hanging and we're still discussing what we can do while we are still on ietfa t"
  },
  {
    "startTime": "00:32:00",
    "text": "be more resilient to changes like that should Docker decide to start using different network segment than what we are currently prepared for them to use then we could automatically react instead of having people reporting Brokenness before we move on it I'm reading some of these things in a bit more detail than I intended to I Really intended these fyi's to be things that we would skim through quickly so I'm going to up level we've turned track off we've got some issues with the way that the redirects are working because of the way Pages got moved but we'll get through that it's just um lifting that needs to be done um we had a effort a while back to choose a font set of font families to use consistently across our sites we've landed on that we're expecting the next release of the data tracker to use them um the other web properties will start using them as the we hit um the next release in each of their development Cycles we've um started using static.ietf.org for facilitating static content delivery the next release of the data tracker we'll be using that for fonts in particular um and very soon after that we will be using it for the artifacts that we have that are um always and changing so the internet"
  },
  {
    "startTime": "00:34:00",
    "text": "drafts the um document types that we have where once they're out there they don't change the they'll be served from this site okay development projects the data tracker has seen a bunch of releases we've talked about some of the things that went by already we're in the process of the Django 4 transition we've made it all the way up to jennygo3.t there's a link to our development test server that has the Django Branch running on it please spend some time poking around there and help us find things that are broken the diff crawls are currently not finding any differences between the sites but those don't exercise forms so those of you that are used to doing Secretariat things or iisg things please take some time go poke around on that site and make sure it behaves the way you expect it behaves um so far the kinds of trouble that we've been running into as we've been migrating you know I have um been of the Forum that dependencies um change significantly as Django moved from two into three and into four or were just flat abandoned and as we're moving through these things we are moving on to more modern versions of the dependencies in dealing with the changes in the interface to those dependencies as we're as we're going along the changes to core Django itself have been pre-state forward to deal with um in that part of this process is going fairly quickly Jennifer is there anything else that you want to particularly highlight"
  },
  {
    "startTime": "00:36:06",
    "text": "I think that covered it oh yeah so um what we're planning to do in the short term now is to finish this migration into Django 4 to continue to move these artifacts that we discussed earlier into the data tracker we have a couple of long outstanding problems that we're going to dive into shortly after that we have to repair our reference relationships because the um submission tool was building them on submission from the XML without expanding the um includes so anything that was in the repository in XML with a pronoun and X I include to a reference did not actually get captured as a reference in the related document since in the data tracker um we've got a workaround for that until we get to the um point in the data tracker where we only accept fully expanded documents that's on the roadmap but not as soon as repairing these reference relationships and we're also going to regenerate all of bibexml3 using modern author extraction tools dealing a little bit with some of the questions about initials um I had hoped to wait to do this until we had uh agreement in the community and the change at the um RPC to where we just used author names as blobs instead of paying attention to this first initial last name but that's still looking like it's going to take significant time so we're going to move"
  },
  {
    "startTime": "00:38:01",
    "text": "ahead and make the references less broken than they are now uh we've got a wave of changes we need to make to support nomcom a wave that we need to support the meeting scheduling then of course a wave to support um the meeting itself those will be the remaining focus and in the very short term and as I mentioned getting into fully expanded drafts and then the work to support the RPC tool refresh walk us through the bib XML author tools xmlrc and the went to the website part okay on feedback summer service and there are a few main bugs that one one particular fact that needs to get fixed they have the reboss has fixed that Upstream so we need to get that into the big XML service um I will be going to the open issues for the external service and um layers with the reports to get the main ones fixed any uh questions about bbxml service um so I want to do all the tools um since the last call the Main major development of art tools is to add the way to compare documents"
  },
  {
    "startTime": "00:40:01",
    "text": "without converting them into a text so rfcd rxctive can be used to compare XML files or background files without converting them um and then another change was um the document comparison on Auditors uses rfcd by default um there's a work on the way to switch between RSV depend IDT but I guess most people would be more comparable with rxd any questions about other tools RFC they have two major releases and they particularly the three points Seventeen one seems to be breaking things some people seems to have issues with the installing uh the new version because it's changed the setup files works to Future proof external directly if the one with the easy way to fix that is uh just upgrading the the tools on your system that should fix most of those issues as for upcoming work I am compiling a a GitHub repository all the fonts that"
  },
  {
    "startTime": "00:42:01",
    "text": "Excel taxi uses right now the instructions are to use not a funds not not a funds from Google and the repository that then the link to the uh not funds that they provide on accept RFC is outdated it's not maintained anymore so um in the future this font repository would be the front repository to go and get all the phones required for excellent RFC any anyone have any questions about excellent RFC um on Webmail website I think we touched we talked about the migration to IAB apart from that um a couple of other features that would be in the next release uh type tables which allows you to learn add cable City majors and other content and uh we will have a widget that gets the notepad from the ietf GitHub repository and show it whenever that widget is you so idea behind that is the not available any questions about that that's that's it for me"
  },
  {
    "startTime": "00:44:03",
    "text": "all right Ryan or Eric do you want to add anything to you what's in the notes about mail archiver game catalog so we've had a long running um project that was um cycle starved because we were focusing on off PD on at the IMAP server where we can allow mapping from one username to another to support people who have changed their primary login at the data tracker and move to the point where somebody can use any of the um email addresses that the data tracker knows them by is their username when they log in this doesn't mesh well with the um core implementation of user at the IMAP server so Alexi and Ryan and I have been working towards uh an ability to configure things in the middle so that a user name can be mapped into another username inside imapd so a data tracker username could be mapped into a different IMAP the username the first attempts at making this go didn't succeed but we're about to pick it up and and make another run at it that brings us to um open discussion in any other business does anybody have anything else that they would like to talk about so I miss the first part of the call but I don't know whether you said it orbert there will be you will have two ing liaison from now as well uh should be my twins yep I mean Warren wasn't able to make"
  },
  {
    "startTime": "00:46:01",
    "text": "the call today or did we not just not give him I was expecting him uh no I'm not sure where to include the right information right so no he has the right information for sure yeah you know we can make sure he's in um Mallory normal is also um joining us as the IAB liaison and I think is on the call today so I'm glad that we're getting um all of the um all the help and the the good communication with the the people that are using the tooling the most if no one has anything else we'll let you get back to your day thank you again for the time um you take helping us um steer the way the um the tool development is going and we'll see you online between now and the next call"
  }
]
