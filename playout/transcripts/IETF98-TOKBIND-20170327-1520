[
  {
    "startTime": "00:00:10",
    "text": "good day everyone you found yourself in the token binding work group if you believe that you have boarded the wrong working group meeting please make your way to the exit now the work the blue sheets are being circulated everyone should sign them and up on the screen we have the note well please note it well and we have an agenda so we\u0027re proposing to go off over the work work last call issues review the token bound TLS 1.3 draft on 0 RT t look at the reverse proxies draft and spend no time on the attestation draft other than to remind you that it does exist then you should read it so let\u0027s get started with the last call issues once there\u0027s somebody that wants to add something to the agenda hearing none let\u0027s proceed so um the way we thought we\u0027d do this is to get and the people have been active contributors please discussion to take an active part today and see if we can work out as much as as much as possible of these issues on the there are no slides for the court documents at for good reason we don\u0027t have a lot of stuff to cover other than this space the festive face-to-face discussion these are the issues that chairs could beam from the discussion then that may not be a complete list I\u0027m hope I mean I\u0027m sort of assuming that it\u0027s not going to turn out to be a complete this but it\u0027s it\u0027s a starting point um there is sort of there\u0027s been like two main sets of issues other than the editorial issues and the editorial stuff we propose not to cover at all we can we can deal with that on the list and deal with it as normal as fallacious there\u0027s mint or two mains or well sets of issues one of them has to do with security for the considerations there hasn\u0027t been a lot of support for from making the change that Denise proposed around how to determine essentially a terminal in the area issue around security considerations and we propose actually leave it as as is so that make no change the document and that leads Morton Thompson\u0027s review and I think Morton gets up to the market area exactly the right time so not insulting I put this on the list because another thing discussed yeah we\u0027ve already dealt with them I am expected to raise him in this room the the third one was new though um and that\u0027s the last one yes yes yeah exactly and whether you know if "
  },
  {
    "startTime": "00:03:13",
    "text": "some of these don\u0027t need any more discussion that\u0027s fine um but you know we want to put them up here so everybody can get those sort of a finally go at them hi Eric rajala speaking is about to be a person who has to approve this document on so with respect to Danny\u0027s point on it be great if the cherish could run through the process carefully so that you can tell me that there\u0027s consensus not to do this um and you\u0027ve considered it rather than just sort of leader pass the silence because that way you all right that\u0027s a fair point we actually did ask a question on the list whether there was support for making the change and there wasn\u0027t right we can yes we can absolutely sort of wrap that up without harming the room so I\u0027d like to ask the question whether there for anybody who supports making the change of terminology suggested by Denise to please come now and for anybody who support who thinks we should leave the text as is to please come now all right to me that sounds like pretty clear consensus and since we sort of pre confirm this on the list I think we\u0027ll sort of call this right now all right thank Eric um and I guess the floor is open please settle these issues now this is why we\u0027re here in in physic with physical presence okay let\u0027s start off with the first issue people is there anybody who wants to make the argument that the TLS extensions aren\u0027t justified okay so in that case we can just sort of do a the same sort of the same hum around these issues what do you think that\u0027s you like would you like us to do that Martin okay so so the question is should we the question is to believe the text as is or you know propose a change work towards they change eliminating the use of TLS extensions right so if you believe that the text is good enough as is come now right if you believe that work should be put into the draft to eliminate towards eliminating TLS extensions um now right and the the "
  },
  {
    "startTime": "00:06:16",
    "text": "third question if you if you don\u0027t sort of this doesn\u0027t make sense if the question doesn\u0027t make sense to you or if you need more information to answer this question please come now alright that seems like pretty clear conseils this to be that baby text as is do you want to you know maybe we can get some takers on the last one which is I guess the most recent one to come up and that is the signatures on referred token bindings do we need them as we currently have in the spec or could we drop them for some sense of simplicity does anybody want to talk well do any of the current authors want to give us a brief explanation of why we think why it was thought to be important to sign both the provided and the referred token binding well in the in the pink box hi I\u0027m jerk I\u0027m in the pink box so the idea was so the referring provider bindings are there in a situation where the client is basically asked to go to an identity provider and the identity provider then issues a some sort of assertion or identity token or something like that that is that the client isn\u0027t supposed to hand to a relying party and the identity token assertion is supposed to be bound to a key that the client controls and the key is presumably something that the client doesn\u0027t normally use with the identity provider because the client will use different keys for different websites so normally the identity provider wouldn\u0027t know what the key pairs that the client uses with the relying party but there are provisions in the spec that sort of make exceptions to this rule and allow the client to show the key pair that it\u0027s that it\u0027s normally using with relying party to the identity provider so here this is the keeper that i\u0027m using with the relying party can you place min please mint me a credential that is bound to that key pair because I\u0027m going to use it with your life party and the presumably it\u0027s doing that while strongly authenticating to the identity provider while sending a cookie or a rolloff token or something to the identity provider saying hi I\u0027m the client that you already know I want you to mint me and credential for thee for this other side that I\u0027m about to go to poker night party and 22 strongly a "
  },
  {
    "startTime": "00:09:19",
    "text": "chanukah to the identity provider there\u0027s this the provided token mining in there whether where the client can say look I am the client that you already know he is proof that I\u0027m in possession of this keep hair and he is also my cookie that is bound to that key so you know who I am can you now please mint me this that I\u0027m going to use with the riverline party and to get to the sort of an equivalent level of security between the client and the relying party we wanted to make sure that at that point we can be sure that the client really is in possession of that key pair because it\u0027s going to be a different key pair from the one that the client uses to authenticate to the identity provider and in particular if you think about scenarios like web browsers or something clients can sometimes be tricked to send stuff to servers that they don\u0027t necessarily have much control about with redirects and so forth while we have lots of provisions in place in the spec you know prefixing the header with sec dash and so forth to try to make that impossible we don\u0027t always know exactly what kind of environment the the token mining will be deployed in whether it\u0027s really going to be a web browser whether it\u0027s really going to be in sec free fixed header and so forth so to make sure that clients can be tricked into sending mentioning a key that they don\u0027t really control in this in this message we thought it would be prudent to at this particular layer also make the client actually proof that they possess the ski so we don\u0027t have to worry about other mechanisms sitting on top that try to prevent the same thing so someone Thompson I think I think that\u0027s a that\u0027s the why that I was looking for but there\u0027s another design that achieves the same goal with only one signature and that is used that the key that you have that you\u0027re using with that site that you can you the token binding you\u0027re using with that site any you include under a signature on that key the record talking binding ID that would achieve the same goal right so we used to have an earlier design where we where the key so it sort of cross certify each other is I remember that yeah yeah there\u0027s no there\u0027s no that the attack that you\u0027re looking to to avoid is whether the client is effectively duped into performing what\u0027s effectively an unknown a monkey share where it says please please bind the following key that the client doesn\u0027t control to this and as long as the client is the one generating this message and we can make that we can guarantee that and you can do that with a single signature which is you have a "
  },
  {
    "startTime": "00:12:20",
    "text": "referred token binding which is a token binding for the token binding ID of the client that it\u0027s using this origin which encloses the same signature also covers the token binding idea for the other origin then you have a single signature right if if you can be sure that the client can\u0027t be ah tricked into the do things the same of the same applies yeah yeah yeah well presumably the client would know that it\u0027s doing this and have written the referred token binding does exactly this right so when you have the referred token binding Heather in the redirect you\u0027re effectively tricking the client into sending this information so it\u0027s all automated yeah it\u0027s entirely possible so there is a security difference between the two yes it the way that we have it now you can\u0027t trick the client into signing a token binding that it doesn\u0027t actually have the private key for because it doesn\u0027t have the private key for it in your proposal right messe Miller but in principle if you could trick the client into signing provided data then it would be they the server that receives the provided coke invited wouldn\u0027t be able to tell the difference Jacqueline relative i find that relatively convincing and I\u0027m kind of willing to step back on this one but I would ask that you write down that because that\u0027s not in the draft at the moment there\u0027s a long explanation of why you have this and the conclusion from that is that you don\u0027t need the signature right the actual text that goes through and there\u0027s quite a lengthy section explaining why you have the two things in the referring and and the attack but the particular tack that attacked that you\u0027re talking about but my conclusion from reading that was that you don\u0027t need the signature you have to be have to describe this other attack that you\u0027re talking about which is where the client is due to some way into signing something that it doesn\u0027t own so to somebody else want it put the counter position because but otherwise I can make a couple of other points sort of in favor of Martin\u0027s argument even though i don\u0027t personally agree with it i we may as well get them out on the table bring them up later so that the problem that we have with the two signatures the current state is that there is an edge case where the relying party uses different crypto algorithms that aren\u0027t understood by the server so in principle you could have a referred token binding browser supports both algorithms and the "
  },
  {
    "startTime": "00:15:20",
    "text": "server supports algorithm a client supports algorithm B and the server won\u0027t be able to actually understand that referred token binding that wouldn\u0027t happen in under the case of Martin\u0027s one signature over both token binding IDs but on the other hand the argument is that if you actually are doing Federation between two parties actually coordinating what algorithms you\u0027re supporting is it actually the end of the world so where knowing having two signatures creates some additional deployment complexity because you could have algorithm mismatches is really the most compelling argument for not signing the second one or signing it with with the same key so under a cup of Microsoft my argument against this is that in practice if you have you know rps and idps then the it is reasonable to expect that I dp\u0027s you know that I DP and he is our peace you know that that relate to him actually have a communication channel between them and that they actually you know coordinate deployment of the token binding parameters I think that\u0027s a reasonable thing to expect from them and my other argument against eliminating the signature on the referred bindings is that you know today when we look at the actual redirect mechanism that we\u0027re currently defined we can maybe convince ourselves that there is no way to dupe the client into into into sending a referred binding to a key that the client does not control but from the protocol perspective I would like the token binding a movement implementation to be independent kind of the details of the Federation mechanism so the you know so when we have new Federation mechanism in the future we don\u0027t have to you know to go back and validate that that again the client cannot be duped into signing into sending a referred binding that the client is not support benkei dark I just want a second martin and say please add some text to the document that says you\u0027re making the client actually produce the signature means that it\u0027s not going to sign something for the key it doesn\u0027t have so do the do the authors have any particular objection to adding such explanation given that you probably thought it was self-evident okay the issue is already filed hi I\u0027m Darrell Piper sort of from deck I think I\u0027m following a microsoft guy I was told this sounds you know you can tell a lot about working groups are the kind of languages they use the domain-specific languages they\u0027re inventing the terms "
  },
  {
    "startTime": "00:18:21",
    "text": "they if I feel a need to redefine for example tells you a lot I\u0027m hearing language that sounds a lot like the language that was used once upon a time in gssapi to bind tokens securely with things like a cryptographic algorithm independent protocols and assertions into the API about what levels of protection you want these bound tokens to carrot and I brought up Microsoft because I know they have it and I know I used it in there Ike to authenticate cross-realm authenticate domain controllers Brent e5 and I\u0027m just wondering why this working group seems to be reinventing the wheel I was told it I was cautioned that i would mind pissed people off and that\u0027s certainly not my intent here this room obviously kind of forward people writing drafts which I love to see I just that someone will don\u0027t understand what\u0027s being done here I know I hope someone can explain it Oh take a thought but saying that I think this is more about there is a cultural there\u0027s certainly about cultural inheritance from of camera bindings but it used to be cool to this well done duplication of effort then k doc again just a clarifying question about the previous comment so when you\u0027re talking about the gssapi and the quality protection are you actually explicitly talking about the GSS quality protection values mostly been deprecated in the current usage are you talking about something Oh first off I should admit I\u0027m over in my head here I haven\u0027t been to an ITF in 10 years I\u0027ve been retired I\u0027m just here um I don\u0027t know what\u0027s happened to gssapi in kiddin I haven\u0027t been following them either I will start following them all right um I don\u0027t know if anybody wants to comment on that again I sighs as i said i don\u0027t think there is duplication of effort i think there\u0027s more so there\u0027s it there\u0027s um yes that\u0027s the word on the people um good answer i like it but it seems like i don\u0027t be nice you\u0027re violating the golden rule of trusted systems design keep the TCB small so they\u0027re in simple so that you can understand it right and it turns out that people haven\u0027t been implement or there hasn\u0027t been a lot of support for implementing gssapi for the HTTP layer unfortunately or 4 12 there\u0027s general desire by anyone all of you guys you think everything should run user mode in the browser seems misdirected "
  },
  {
    "startTime": "00:21:23",
    "text": "most of it but that\u0027s premise of this working with business old hope including the scope of what the platforms we actually have so I guess what I\u0027m trying to say is by just inventing yet another protocol yet another way to do it yet another thing we\u0027ve made it very confusing for people to go to the Chinese menu and figure out how to build secure systems so when you just do another one I assuming you get it right in its secure it\u0027s just confusing Oh what I mean I this is an interesting discussion it might be it probably would have been more valuable during the both when this working group was formed then when we\u0027re almost off now okay fair enough so um Martin did you want to say anything about the ET le plus one issue or is that another okay look so I\u0027m just going to do a pro forma jamon that this may make sure that nobody else who sort of thinks that actually that is really important so the the Mortons common balls I guess correct me if I\u0027m wrong that you know the it seems sort of seemed like a problem that we\u0027re baking etle if not one into the HTTP spec and it because what and the question is whether whether the working group should actually try to eliminate that dependency from the from the specification so I\u0027m just going to ask for a harm of people who believe Martin is correct and we should actually try to do work towards eliminating a dependency on ETA because one when the specifications please come now all right simmer of the right to your XID and what was there a proposal for some other thing or is this just a question about whether or no saoirse one right ed exactly evil there was no so some mutton Thompson I think I suggested that you could find these two an origin scope scope to an origin which is host sorry scheme host port alright so the avian and buff so so the appropriate ham would be the halls for support for moving towards scoping to origin rather than scoping to eat a late no that\u0027ll be a reasonable question to ask okay like all right so rephrase so um if you believe that marking we should find out whether or not people actually understand what the two options mean before we well I think one often that I was going to hum for is sort of leave the text assets "
  },
  {
    "startTime": "00:24:23",
    "text": "right and the other is no work towards origin scoping all the negativity puff monsters hi Jeff Hodges um so this is in regards to the HTTPS token bargain bindings that right yeah that\u0027s correct I wanted yeah that could get that in the record okay what does suspects say now the spec says now that that you should I believe scope the scope you\u0027re bound your token binding keys to etl d plus one in order to match the binding the scoping of cookies which is by default and it references RFC 6260 65-62 60 to 65 I get that bathroom just a slight clarification under a puff of Microsoft up there are two levels of things that the specs say the protocol spec says that the the token binding keys should be scoped no larger than the the scope of the tokens in the application protocol and then the HTTPS token binding spec says that the scope is et al D plus 1 right am i understanding of your your question / proposal Martin is that you suggested this be moved to scoping at origin is that the vitae take as long as I correct yeah I tmartn not just to be a little bit clear the current text is slightly more complicated and that has one case that is must be et al v plus one and then there\u0027s another case that should be ET le D plus 1 so it\u0027s four pairs for first party in Federation use cases to find in this specification and used for binding HTTP cookies is must and then if you want to find other things like Oh auth tokens or open ID connect ID tokens that\u0027s the should so Martin Thompson since you have the text in front of you then does that allow you to go wider in scope what does it say that you might want to that\u0027s the map I mean it should their suggest that you could choose to ignore that rule and do something else in which case does that mean that you could do wider scope tokens are going to give the same ID to every single site on the Internet so that\u0027s permitted by the spec okay that\u0027s that\u0027s a no from me so so what I\u0027m looking for is it is it is a maximum size of scope to be very clearly delineated I understand that before when we discuss this the working group had "
  },
  {
    "startTime": "00:27:24",
    "text": "consensus that it would be etl D plus 1 as a maximum and if we\u0027re not there now worshiping Shirley\u0027s fix that Brian Campbell paying it so the etl D plus 1 is is a must Maxim for web browsers be I would it\u0027s come up a few times on the wording is very confusing but the intent as I understand it is that a broader scoping is permitted for other applications which use HTTPS such as Oh off oh that how that shakes out is still a little unclear that that\u0027s the intent to allow that the touch could maybe be improved yeah there\u0027s actually another part that I can read you Daphne ha which says that applications other than web browsers it may use different key pair scoping rules sorry the intent and I do think that text is pretty clear about this is that that for web browsers standard web browsers it\u0027s etl d+ long period it can never go broader than that and certainly that\u0027s housed them implemented I think and drape ahold microsoft just confirming that this this is indeed the case so so originally we were going to say it\u0027s must you know that this is the scope and you cannot be broader than that and then the point was brought up that in some application domains there is inherent knowledge in the client of certain domains should be you know the unlink ability problem does not apply in some domains basically that\u0027s why we kinda made it issued yeah hi Jeff Hodges um one could argue that we may want to document design rationale on this point so Martin let me ask you this if you do believe that there is language to be clarified such that your your sort of issue um you disappear effectively disappears or or is there is there some way for you to sort of formulate your your issue in a and the complex logistic after many concentric so not insulting the reason I raised this issue is it\u0027s an architectural one where we are continuing to build things on top of cookies that a terrible I mean cookies are terrible and now this is another thing that will will span origins undermine the origin isolation that we have allow for all sorts of interesting corner cases and we have to continue to maintain things like the public suffix list and all sorts of other mechanisms that we use to actually determine what a tid til the plus one actually is very poorly defined by the way and very hard to it\u0027s a it\u0027s a huge maintenance hazard so my preference would be that all new "
  },
  {
    "startTime": "00:30:25",
    "text": "specifications actually used origin because we actually understand what that means it aligns with the security boundaries on the web and it\u0027s pretty straightforward but I understand that we have cookies here and will actually want to protect cookies so I appreciate the position that people are taken on this one it\u0027s just that I wanted to make sure that we had a discussion about it and we had a discussion about it I think that\u0027s and when water bears consensus so that\u0027s one one possible outcome of this actually clarifies on language I if there is a national language that could be i think that actually doing is jeff suggested and saying we\u0027re doing this we understand that is bad but its necessary to to protect cookies because cookies are bad and maybe this might cook is somewhat less bad it\u0027s probably something that would be reasonable to put in a document and fix the should can at the same time I suggest that you and your marking that you sort of city you\u0027d go off and a harder then come back with a pull request against the suit of those back to the SUV aspect yeah very very clearly i mean the the core specification is very precise on this one and what Andre said earlier is fine it was the HTTP aspect that it needs the small toy right so so just to clarify the notion browser use of token binding with HTTPS where your binding it to a token which is used across multiple endpoints in the case where you were you using HTTPS and and as you\u0027re driving it through an API and you\u0027re not operating inside a web browser and your understand who you\u0027re talking to and you want to maintain some some sort of consistent identity because you have a token that\u0027s shared across disparate set of servers then fine do whatever you want to do and I think the specification is pretty clear on that one not retain the maii that the Prime is talking about and then it\u0027s gone right so you\u0027re not objecting to that should but you want to make sure that in the web browser case that the scoping is no broader than EPL be +1 right right side I think that we\u0027re all clear on that just and properly explained I think that is is the reason because I think one of the things that browsers may ultimately do here is play with the scoping rules of cookies and understanding that this the scoping rules here I meant to align with the scoping rules of cookies means that when they play with cookies they should also be playing with the scoping rules of these tokens so ideally if you fill the cookie more narrowly the token binding ought to be scope as narrowly and it so happens that we do these sorts of things on a regular basis now and so understanding that would be very helpful right I don\u0027t think that\u0027s powerful version i am very helpful thank you with that I think we are poster what we identified as the list of issues just before you what why do you "
  },
  {
    "startTime": "00:33:27",
    "text": "get to them like um or I just want to ask if anybody has to identify anything else in the law school review that you believe we haven\u0027t covered other than editorial issues yeah raise it a little while ago about the ramifications of allowing for longer EK m values which generated a little bit of discussion that felt like maybe it was tending towards no let\u0027s go back to just having a fixed 32 x 80 km that\u0027s signed over all the time but kind of it\u0027s sort of trailed off and wasn\u0027t um didn\u0027t come to any clear consensus of those aware of so i I\u0027d love to get to an answer one way or the other or not and drape over Microsoft so I would just like to to try to explain the issue again over km right in the history of that so originally in our token bindings pack we said that the ekm will always be 32 bytes for token binding protocol version one you know one once you have a new version you could have a different 2 km but basically it was fixed then an argument was brought up in this working group saying that you know why don\u0027t we make the length of the km dependent on the type of the token binding key right so if you if you add a new algorithm and using the shelter in for example in your signature scheme then you can use a m EK m of a different length that was that was the argument and we did that we said that you know for further currently defined token binding key types we will be using 32 by tpms in the future this may change with a new definition of the token vanity type now it is a valid point that this creates issues for example when there is a tale as Terminator which basically forwards the token binding message and the EK m to some back-end server for verification in this case you know the without knowing without the TLS terminator doing all the possible to combining key types they don\u0027t necessarily have the ability to generate all the necessary KMS to be forwarded to the backend server for variation so so there is this this little bit of a corner case but it\u0027s an interesting kind of issue to discuss and you know in principally in principle our choices are to go back to say you know the token binding 10 you always have a third to buy tqm ekm and if you want something else you know then you need a new token by an inversion or we can say that for example you know the dtls Terminator is going to generate the key ends of various lengths and put them all in that message that is getting forwarded to the backend server for verification so I therapist roller so I apologize for having this the conversation earlier but why would you want a longer ekm for some others energy scheme more bits basically but the the "
  },
  {
    "startTime": "00:36:31",
    "text": "argument was that the signature schemes to find now have a strength that\u0027s somewhat equivalent to being able to sign over 32 bits and that there might be one in the future that in order to bytes not bits that practically the same right um there might be a signature scheme defined as a key parameter type in the future that that needs a longer input into this what\u0027s being signed over in order to achieve its full strength and I I was actually the one that made the argument and so although there was less of an argument in more of it should we maybe consider this um and that I think we\u0027re practically speaking there\u0027s there\u0027s not there aren\u0027t algorithms known or likely to be adopted here that require that um so we are I think the discussion on the list was that the next round of algorithms would be perfectly well matched with the 32 bike yeah that\u0027s amore Masterson at some point beyond the next round of algorithms you might want hypothetically want a larger ekm but that seemed in the discussion on the list to be fair I pathetically extremely hypothetical yeah yeah I\u0027m sky draw that point it\u0027s worth noting that beast in TLS the the underlying entropy is tied to the strike that the hash function which are using for HK DF so unless you\u0027re also using Shaw 3d for you you have 3g gets really but bites whatever on and on so um I\u0027m gonna wear of any reason why you need to go larger than that um if you did you have to tie it to a bigger function which tells you the maximum is 48 our tests on an infra current version to tell us it is a decipher sweep but the but the largest efforts we currently defined has shot 34 so the biggest possible and every block you have and of course then of course you also have to be using what I guess efforts of Mad Max if it\u0027s a match the secret to master the security level I mean do the problem is it like the look that you have to be using I mean no you\u0027d be using shit on your curve p you know nine nine hundred or something because 521 is a fat w 256-bit that\u0027s right right so as I said the next round of algorithms is appropriately Nash too yeah I I\u0027m just I\u0027m trying to think of any situation where you actually have more than turn a few Z\u0027s pits and strength floating around here by the time we actually have to worry about that it\u0027s the quantum cryptography that will have already bitten s in the ass you know in all likelihood so so mum Thompson I have a suggestion here echo mentioned it um we can make the EK m as long as the output of the hash function that\u0027s the Taylor\u0027s paraffin hash and you avoid the problem if you have the entropy then you get the entropy that you have and if you don\u0027t have the "
  },
  {
    "startTime": "00:39:32",
    "text": "entropy then you don\u0027t get pretty much more entropy and then you\u0027re in the situation where you want to go to pee 1342 which is an awesome prime by the way because I just made it up then you then you have to pick a site this week that is actually yeah the best prime unlike to the five 21-1 have to pick a side this week that matches the strength of the thing that you\u0027re looking for and then then we have a hopefully forward safe way of dealing with this one and then we can sort of just not worry about having to issue a new version in their circumstance hopefully actively bad idea not an implausibly bad idea sounds like the best endorsement for any of my ideas ever had them mainly google so the TLS Terminator is now forwarding on the cipher suite it used to the back end so in those furnace so does anyone want to express what the consequence would be so so to be clear that the consequence of this change for people who are negotiating a yes GCM with charge of 56 is nothing and for those people who happen to be using AES 256 with the shell 380 fours and all that and I think that\u0027s probably one of the only and psycho suites that we actually have that\u0027s got the wider hash those people would be affected that would be using a larger he came and so it would be incompatible so people are routinely negotiating those things it would break that is that is a problem that may need it I don\u0027t know what you want to do about it but I mean you could pick a new extension number and reverb or Microsoft so this does not sound like a bad idea there are few a lot exactly there are a few potential like implications from this right so one implication is it would mean that the the token binding protocol specification has this you know it means that you know whoever implements this make if you did this change right whoever implements needs to figure out what cipher suite was negotiated right and request an ATM of that length that\u0027s that\u0027s an extra kind of question for Lee for the TLS layer I think that most the less implementation support this so it can be done in principle another thing is that yeah I guess we could do I think it\u0027s a possible "
  },
  {
    "startTime": "00:42:33",
    "text": "solution yeah just a just a process thing I you know the chairs sort of do it get a quick coupler and we both feel that if we do this it\u0027s sort of a it\u0027s it\u0027s more than just clarification and and you know an editorial and that would probably require us to do a quick but still another love working with law school make sure everybody think it\u0027s worth it because good really know yeah you absolutely do a one week or if that\u0027s cold but still you know we\u0027re gonna we\u0027re gonna do that don\u0027t intend to just actually get it for free it not been kada canyon just noting that this idea generally seems reasonable as we bring it up right now in the abstract and the only concern I would have that\u0027s not obvious right in the room is just as Andre sort of talking about how is the plumbing in a work to actually get the information to where it needs to be so hopefully someone will think about that harder in the next day or two yep you know Adam it\u0027s on the cue enough that up I\u0027m is gonna oh I just wanted to stand up and say I think it\u0027s a bad idea because someone should stand up and say its excess complexity for no particular reason because 256 bits even if you\u0027re worried about collisions like 128 bits 228 different connections necessary generated collision in this space I would say keep it simple for it\u0027s worth I agree that them I mean I\u0027m not going to like pitch a fit here but i agree adam second or third dog agree mother andre before microsoft i also think it would be easier to just you know go back to saying 32 bytes and that\u0027s it or i guess the other thing would be to keep it the way it is but just tell me rhythms to keep using 32 bits and then work you can have whatever outcome you want but it\u0027s at 32 bit EK m high so i think we should do it a few hums here let\u0027s see do a storm an humblest and then people can complain about that and then we can do the actual home so the one question would be if you support going back to 32-bit date pecans right now that\u0027s one change the other change for this version absolutely we\u0027re we\u0027re not committing any you know future versions of the protocol right to anything and you\u0027re allowed to change hi test text with the you know the right amount of arguing so the the other approach would be to you know make no change and clarify that if you actually run into this problem you\u0027d have to change retrospect that\u0027s another option and the third option is to make it the "
  },
  {
    "startTime": "00:45:33",
    "text": "size of the hash function and so that\u0027s a breaking change but he\u0027ll that\u0027s the third option right anybody anybody think of other options other questions we should ask Steve I\u0027m sure you\u0027re going to do it anyway but maybe don\u0027t ask about 32 bits but 32 bytes yeah it\u0027s it\u0027s now it\u0027s it\u0027s it\u0027s now a Freudian thing all right okay so let\u0027s start with go back to 32 580 km if you support making that change please hum now all right next question it is support making no change other than a clarification saying that this sort of is something you will need to the protocol no need to revisit if it if it becomes necessary in the future please hum now all right and third option make the breaking change to make the size of the e km dependent on the size of the hash function is um now all right it\u0027s pretty clear consensus to go back to 32 bit EK m all right but if you find bite so this is my way this is now I this is a brilliant thing it will never go away alright alright does anybody have issues other issues that they want to bring it clarification on that the only thing there\u0027s to change based on what I just looked at is to delete a sentence that find delete one sentence that is arguably non-normative anyways I don\u0027t think we\u0027re saying that that change requires another last call it would have been the changing it to match the prf yeah alright so anybody else nope um issues that we hope to bring up that they is remembered or not seeing anybody rush for the mics and I think in that case we\u0027re ready to do the following if the authors could ship another edition of their document with all of these issues addressed including editorial myths that we have discussed on the list at that point will shape it upstairs and hope for the best alright agenda yeah well no we will already be back "
  },
  {
    "startTime": "00:48:36",
    "text": "there so we got two items on our agenda left one is the 0 RT and nick nick is in the room yes let\u0027s see if I can dig up those slides that looks like it okay so I\u0027m going to review the changes in this draft from the 0 0 of F version and there are also some discussion points raised on the mailing list to cover so I added some more language to the security considerations about the proof of possession or lack thereof and I\u0027ll go into more details of what that what we do or don\u0027t get with the proof of possession with 02 Tito binding also i changed the client to switch from 0 rst exporter to the normal exporter during the connection to satisfy some concerns about using 0 TT exporter and also clarified that when we use the rhd exporter there is a token binding extension with that it\u0027s just an indicator extensions your length data to indicate to the server which exporter is being used so that the server does not have to do a trial verification of that signature so on to the next slide this is an overview of what a client malware attack could look like in TV proto so you have the attacker on the left here does that LS connection to a server the attacker uses their access to the client machine in the center to generate a signature of that EK m and then sends that the token binding with that signature over the connection from the attacker to the server with the bound token stolen from the client the key point here is that once the connection between the attacker and the client has been severed the attacker cannot create any new connections to the server so at the approval of possession in normal token binding is that the whoever sent the token binding message had possession of the private key was able to do private key operation at the time the connection was established so the next slide this is what it looks like in zero RTT the attacker can make several TLS connections to the server and get new section tickets that can be used for zero RTT then and forward requests to the client and generate token binding signatures for future connections using this new session ticket at a later point in time "
  },
  {
    "startTime": "00:51:36",
    "text": "the attacker can now send initiated to the US connection as the rxt connection and that really data include the token binding signature over the early exporter and all this can be pre computed without any input from the server so this is why sometimes learning is there is a lack of proof of possession because after the attacker loses access to the client machine as long as it has a new session ticket i was still valid and went through the stamps previously it can that send a token binding signature that is still valid by about I mean it server can verify it so there\u0027s this sort of lifetime concern of up to seven days and next slide this is just outlining a different possible attack that can be done we still have these seven day or new session ticket lifetime concern here in this case it is the attack are actually replaying the token binding message and client hello from a previous connection in practice this one is probably less likely to be workable just because it needs to replay that exact message and the server is hopefully going to enforce some amount of life time on the ticket age which will be a shorter time period so that\u0027s sort of the overview on the sort of proof of possession concerns on here I think even with the sort of seven day time frame this sort of form of token binding that has these security concerns is still something that is valuable it is especially valuable on the use cases where to client malware is not in front model that\u0027s being considered for example protecting OAuth tokens from XSS there\u0027s no client malware involved in there so only the zero RTT replay case that is covered in the TLS 13 spec is a concern and past that as your oxy token binding is through the similar security so next slide so in terms of which exporter to use their served in three ideas flooded out the original proposed idea was always used 0 RTT exporter another idea was require that at the zero HT exporter is only used in early data in practice this is not really implementable basically it when a client starts preparing a request there\u0027s going to be some time delta between when it starts repairing that request to when it sends it on the wire and sort of in the diagram I\u0027ve sketched out here for request see it starts preparing that request before the early exporter or before the normal exporter is available but then sends it after "
  },
  {
    "startTime": "00:54:37",
    "text": "that\u0027s available so having this hard limit adds a lot of complexity to the server the bed this also creates issues with a fragmentation of TLS stat could decide that it needs to fragment of requests for whatever reason could be something related to the one n minus 1 split for CBC IV randomization or it could exceed the maximum to us record size if it\u0027s a large post request there\u0027s several reasons that fragmentation could cause an issue here and then if the token binding message gets political cross records and pre and post handshake then I don\u0027t think anyone has any explanation for that means so this mechanism for having the client change the exporter as soon as possible I think is more reasonable as soon as possible is kind of vague language but essentially what I mean here is a client sends some requests maybe sends it with the early exporter the server doesn\u0027t like that it used the early exporter for what every incident says this is not secure enough for me I want to see this with the normal exporter and sends back a response that says please try again with the full exporter by the time the client receives that response from the server the handshake will be far enough that that exporter is there and then the client can try again with the full exporter I\u0027m sorry Adam a goo what is that signal to say please try again with the book yes that\u0027s a good point so tell us 1.3 doesn\u0027t have any mechanism say I didn\u0027t like what you sent an early data after it has accepted the early data so this would have to be an application-specific signal for HTTP this signal could be a 307 redirect to keep the method the same and redirect to the same location with possibly an added query parameter to say that it performed this redirect so such a mechanism could be hacked together it\u0027s not the most elegant of mechanisms suppose anger yeah um so why is this different from for instance a server allowing a 0 RT exporter for some amount of time and then and then expiring that later this is basically the same thing I said this is sort of codifying the client rules of when to switch there isn\u0027t a sort of time-based thing on here nor is there a ratchet mechanism explicitly indicated so one could imagine that the timing of two requests is that the client starts both of those requests at the same time and then it flips the order in which it sends them on the wire and ends up sending one with the early exporter right after one with the full exporter so okay I\u0027m just suggesting on the server side you you allow both for some amount of time and you say like like hey this is possible "
  },
  {
    "startTime": "00:57:37",
    "text": "that one request was sent over 0 rd another one was sent over one hour duty just to close to each other with exporter but then later you you wrapped it on the server side like after a certain amount of time yeah that sounds like it could be a reasonable thing for a server implementation to do I don\u0027t have any sort of our duels of the server can be guaranteed at this point in time that it is safe to do that but in practice I would probably work out fine so what is the security consequence of accepting that request with the early exporter in the token binding in the say an access token that\u0027s going to give that that requests their access to some information under the early exporter or are you proposing that that if that request just be thrown away because it doesn\u0027t meet desert security requirement so I think I heard two questions in there what is like sort of these security difference if the server accepts token with the early exporter and then the second one is what should the server do if it receives a request with you really expert if it doesn\u0027t like it well yeah I guess if it doesn\u0027t like it if it I guess we would have to explain that there are different security properties of these mysterio I key requests to whatever\u0027s being the resource server or authorization server or client sounds like so first the question is what is the different is there a meaningful difference in the security properties that would cause one to be vulnerable to meaningful attacks by accepting the 0 RT t day so I think that and that first question I think it\u0027s back to the proof of possession concerns on the previous slide which is that if the server is concerned about client malware attack or is trying to think if the 0 RTT replay from TLS 1.3 is potentially a concern yeah then and that would be reason why the server might decide that it doesn\u0027t like it because that the proof of possession is weaker that was so the API from the TLS from the token binding stack would have to expose for the application that this was a zero RTT token bound token token ID and good luck with it that could be an option for server so under a pop of Microsoft so on the first question I do think that the token "
  },
  {
    "startTime": "01:00:38",
    "text": "binding with zero RTT creates replayable tokens in a way right so they can be either replayed directly with the same message or as you know showing again in the attacks so all of those attacks with token replay become possible if you do 0dt so I do believe that it it involves the degradation of security in the way and it takes away some of the guarantees of the token binding protocol that we have my second point is that trying to ratchet up from the early ekm to the appropriate IAM with proper proof of possession is a pretty complicated mechanism is currently defined but i also think that you know once the server has accepted a weakly bound token from 0 TT the course is kind of out of the barn so trying to secure it after that is a little bit so I\u0027m just not convinced that the complexity is worth it okay like either you accept 0t token binding and you are willing to deal with it you know or you don\u0027t this is where I\u0027m living if you go to the next slide I can make an argument for why the change to the draft of switching exporter\u0027s should actually just stay where we were earlier if always using the rxt exporter so the argument is roughly as follows that if a server is accepting token by name is York City on the same connection then it is fine with the security properties the CRCT exporter for at least some of the requests that server can process and if the server receives these that same request post handshake I it can benefit it\u0027s fine what if it still gets York City exporter with that because it was fine receiving a pre handshake with the 0 st exporter so that request can have 0 should be expert at any point and then the next part of this argument is that a TLS 1.3 server cannot reject early data based on the contents of the early data it needs to decide whether to accept that before looking at it otherwise there can be a deadlock scenario in the handshake so the server in deciding i\u0027m going to accept some early data basically saying i am going to accept all hourly data i\u0027ll accept any requests that i receive in early data and then via the previous steps we\u0027re fine taking the early exporter with it Benkei duck well now that you went through this slide it seems like a fine opportunity for me to say that at least for me having seen this slide that\u0027s sort of saying it\u0027s an argument for the server to not accept 0 RG t data at all but that\u0027s not actually what I got here to say which is more along lines of with the 0 TT data there\u0027s sort of two general cases that could be considered and one of them is in like a well-behaved client that\u0027s using PS KS from session tickets and it\u0027s only using the session ticket once and it\u0027s amazing client hello ones and in that sort of "
  },
  {
    "startTime": "01:03:39",
    "text": "case when you do actually go through and complete the full TLS handshake that sort of retro actively confirms the zero RTT data because the client is well behaved is something going to send it once I\u0027m so epic okay I thought that I got up here and I\u0027ve done bedroom and then the other other cases sort of when the client is not well behaves or when using an external psk and you really don\u0027t have any sort of guarantee about that at all and I\u0027m not sure that we\u0027re really thinking about that this case for all of the discussions were having and certainly through some of the text in the document sure I\u0027ve definitely been ignoring these here PS k it\u0027s a curious here to tell me why I\u0027m wrong well or you could tell me a lot wrong so you going to stand there so um my understanding security property you were attempting to obtain is that I can\u0027t bootstrap temporary access to the token Detroit to the team the signing key on the clients device into permanent into a permanent for truth right that\u0027s these part of the principal\u0027s eyes I temporarily controlled the clients device and I signed a bunch of crap and then like then I lose access right and so the on there what the problem is the reason it doesn\u0027t retro actively bless the CRCT data is that the cinema is that ordinarily thus ensuring cut includes the servers and you replay knots and now this does not as the mere fact so I can basically makes me a token as I want and like and then because I because I complete the handshake with the server on the on it\u0027s only the TLS eng replay you have it\u0027s not that it\u0027s not the injury play from the tip from the token bite his signature and so in or so so that I got up I was gonna say what you said and then I realized halfway through that I think it\u0027s not true because because there\u0027s one or two Tito combining covers covers that server in as you can hear like that level even though you don\u0027t even though the teeth though you\u0027re right to say that on the arm for the TLS data alone the one or gt handshake does can resurrect retract if you confirmed on replay so I think Ben is correct in that completing the telus 1.3 handshake retro actively blesses the application data yes but it does not retro actively bless the exporter signature used for took that\u0027s what I was trying to say yes I think I agree with you and so then the only potential chance of protection that you have is the reuse of the ticket because when the bad actor has actress access to the key material and it\u0027s signing all this random stuff so even auntie replay on that doesn\u0027t help so the scenario on slide for it shows that the attacker generates it\u0027s only session tickets to use so it\u0027s never replaying a new session ticket it is just yet using the on ones that it\u0027s generated using the client as an Oracle to sign to use after the fact yeah i mean my impression is essentially no matter what cryptographic on start to use and what\u0027s it includes data from the server or you have some other strong and reply you can solve this problem yeah I think we\u0027re sort of stuck with at this sort of seven day or "
  },
  {
    "startTime": "01:06:40",
    "text": "whatever the age of the new session ticket time window is for the approval possession for the ZX TK Andre Pavel Microsoft which brings me to the second to my second point that I wanted to point out which is you know if we are if the working group is willing to consider using cocaine binding in 0 TT with all with all the with described so far I would like to have a mechanism for the client and the server independently to to have a way to avoid doing this so like if so that I can build a client that never does 0 to see token binding but still can do 0dt with some servers and tow combining with some servers that\u0027s basically something we need to design into the solution if we are to to go ahead with this yeah so the i think i had mentioned this on the mailing list an original proposal for doing this is that basically in terms of how the processing rules work for the token binding extension on 0 RT t would be that if a client it does not want to do a combination crst in token binding if it has 0 xt a new session ticket that it could use first sort of that\u0027s tanto combining with and passed it doesn\u0027t do z rh t and that\u0027s fine the sort of complication gets into a server that does 0 RT doesn\u0027t do token binding and now is doing both and looks like ever has a question another question but a suggestion and we went to all the trouble of minting all these extension points in the new session ticket and when you could simply made a section point that said I will do is your GTO combining sit ops you in and rather than out yeah is so putting that extension in the new session taken from the server allows the server to and do sort of either type of mining or crst but not both I don\u0027t think this helps in the case of a server that is willing to do both but a client that is not one no no so sorry so I was on just so you think about the second I think I think you probably may need both but i think is i understand it the server\u0027s willing to say so you don\u0027t want this over to be able to say that i will do that i will do is are too deep i don\u0027t want to talk about even 0 TT so right now if we want a server to do either type of my new or z RX t but not both it can do that very easily by looking at the client hello it sees both extensions and picks which everyone at once and all thrown together the complication though is a client that wants to support 0r TT with servers that do not do token binding and token binding with servers that do support it but still interoperate with a server that supports what I\u0027m proposing without having to do this spec and that would rely on the server implementing some "
  },
  {
    "startTime": "01:09:41",
    "text": "parts of negotiation correctly which for a client to rely on that sounds a little washy I think a better mechanism would be to propose another TLS extension just an indicator extension that says the client says I support doing a token binding over 0 RT t and server echoes that back if it supports it and sounds like something that could also be put in the new session ticket maybe just as a reminder yeah that\u0027s what I thought that carousing was that you have a new extension that says I\u0027m willing to do 0r TT with token binding together and it goes in the client hello or and/or the new session ticket depending on your which party supports it ok so the case to think through is a client has a new session ticket from a server that is valid for 0 TT but at that time that sort of Naruto combining and now wants to do token binding right now the server supports tow combining and it wants to not do with it so if this flag isn\u0027t there in the new session ticket the client is still safe to put token binding in its client hello and because that extension ism in the new session ticket the server will not assume that it\u0027s doing this newfangled thing and it will assume that it\u0027s just you know the idea this seems intricate enough which is it down with a piece of paper or funny girls yeah I just wanted to say I would really like it if this world and not yep that is certainly my attention to make this combination via opt-in for both of them so yes i will continue their offline here or on the mailing list or both with those details that i think i have two more slides on yeah so in the spec right now is this replay protection to us extension sort of the let\u0027s make sure that a server won\u0027t accept the same new session ticket twice I think based on the discussion of the threat model and the attacks that an attacker can do that don\u0027t involve actually playing a new session ticket I don\u0027t think this is useful so I\u0027d like to remove this yes please do so yeah um the attacks that you described we defend against them however we defend against them and we don\u0027t need to do any extra signaling in order to do most of those things you can either signaling you can stuff things in session tickets and other Dallas\u0027s ok so i think this extension will be gone on the next draft look slide looks like so upcoming changes um so this first bullet point was fixing up some language around changing exporter\u0027s sounds like based on i like to get a better sense of the consensus of should we stick with switching exporter\u0027s in which case this is a language fix since i got the lingo drawn or should we just go back to doing "
  },
  {
    "startTime": "01:12:44",
    "text": "0hd exporter for the whole time and oh yeah and then the other one was clarifying about what basically the process signals are in 0 xt for that took money extortion yeah I was blessed like you wanted the pole yeah so for the first point about sort of either sticking with the early exporter for the whole connection versus switching the exporter hope you nice to hold the room and also leave an option of let\u0027s think about this more okay so there will be two options one is changing the air sorry could I just add many Google I just button before we do the pole you had a point where you said well the server might want to send a 300 something to say please try again with the full exporter and then you had the slide arguing y0r TT exporter was fine forever yeah that can that argument didn\u0027t convince me that no the case where the server might wonderful exporter is invalid did you expect this argument to destroy that previous suggestion I yeah so this I suppose all of this argument I destroys the thing about a server sending a 307 to say please a fall exporter II I think what it can and this argument is not sort of destroy the 307 argument this argument is sort of looking at this ignoring what the application protocol is and assuming that TLS gives us no mechanism for a server after accepting early data to basically reject the early data yeah the 307 argument is an argument that says with HTTP we do have this mechanism to sort of rejecting basket for your response later so there\u0027s always a danger of building complexity out of the fear that somebody may need something do we have concrete cases I\u0027m looking sort of a Dirk and maybe somewhere cloud flurry there do we have concrete cases where servers would one to three hundred requests if they got it on a zero RTT exporter all right thank you and a I think some of that might be not knowing the answer which is why I think having a third option of let\u0027s think about this more sounds reasonable for the pole I just had a quick question about the intuition behind the client malware model where so is the idea there that your token key "
  },
  {
    "startTime": "01:15:44",
    "text": "would be in a place on the client machine that\u0027s not accessible by malware but does have an API to sign with it yes so the idea is that the key on the malware might be like sort of backed by some hardware module ever but there is some API for signing it that the browser or whatever client software has access to which the client malware can run at same privileges as that software so similarly has access to do signing operations with private and this is the model that browsers are using to implement this I think it depends on who you ask andreypopov microsoft defend definitely this depends on who you ask in IE and age we use stronger protections for the token binding key then for the cookies so malware that can access the cookies cannot necessarily get that since the token by ninky so within but but they do have access to sign because the application you know process is supposed to sign so yep the sort of a chrome view of that is that like if you have malware that can really cut these ominously can read the token binding keys on disk and we\u0027re not really trying to protect against that um it\u0027s about I endure I just wanted to ask for a quick clarification by always using the or gate supported you mean by that even if you have access to the one arc you get supportive you will use the architects porter yes so I mean on a connection where 0 RT t data is accepted by the server and token binding is negotiated 0hd expert it will be used for the whole connection if CR HT data is rejected on the connection or not attempted at all then we\u0027d still use the normal exporter so my question is that uh why isn\u0027t there a third option which is that you if you have access to one RDX or use the one hour to get exported access is 00 but the server can choose how long it is actually valid for like the 0 RT to explore to how long is valid for so for example instead of signing 307 the server could implicitly use its RTP calculation estimate to say like hey I expected one our TV this would have this should have come in the monarchy exported requester come in by now that sounds very similar to the current language of switching exporter\u0027s it adds a complication of these servers now I think sending some signal to the client that says it only is one RCT exporter from here on out it it does allow the server to know I\u0027m not suggesting an explicit signal at all I\u0027m suggesting just that the server ratchets tries zio RT one Arkady both on HTTP message that it gets so it tries to validate against the order today exporter as a valued one RT exporter and then after some amount of time it says like sorry there where did they export not valid anymore like go away like if there\u0027s narrow it sends a 307 which it can do anywhere doesn\u0027t need like token binding support for that um so it can it has it has this you can do this in any case yeah I think that\u0027s roughly what we have here it I don\u0027t "
  },
  {
    "startTime": "01:18:45",
    "text": "know if we would need extra language to say that a server can do that eh I think that\u0027s sort of in these forum adds complexity space or it yeah I\u0027m saying that like well it\u0027s not it\u0027s an argument against saying that always do 0r XD exporter it doesn\u0027t like that\u0027s a third option right so if Okafor humming on a fourth option sorry I mean uh if we\u0027re humming on options and why isn\u0027t that an option to choose from hey I think that option is sufficiently similar to the existing object work we\u0027re going to run out of time for Brian so I think that we proved that we don\u0027t actually have any kind of consensus okay further conversation about this on the list would be useful because as I had a sense not everybody is it I don\u0027t know that we can actually formulate binary binary things on the first one about cooking a quarter there another one that he wanted us to be there was another one on the last slide slide 9 and this was clarifying language around negotiation so I think that\u0027s just an editorial point and can you do that on the list thank you very much okay thank you and our next guest participant will be at the marquee spot on last with 10 months you know you shouldn\u0027t mark the spot mean right all right uh yeah that\u0027s yeah the wrong songs yeah pretty steady six lights yeah I think you can or just bring your laptop off the wolf plugging and we\u0027re running short on time this you have a video "
  },
  {
    "startTime": "01:21:47",
    "text": "adapter i have an hdmi cable the baby thing is you but if somebody has like you needed to some peanuts reply that displayport to begin you slippery here Brian plugin kagu where we are H tiesto combining and feel terminating reverse proxies I\u0027ll go to Mexico so the problem here problem statement is a lot of HTTP is applications are I need to read my own notes you go back oftentimes applications are deployed behind some kind of reverse proxy that actually does the TLS termination um for these applications actually do all the kinds of things you\u0027d want to do with tow combining something needs to be passed from the front end terminating TLS to the back end in the general case anyway there\u0027s a few things you could you could do differently but sort of the general case of supporting this you need to pass some information between the two um and in the absence of some standardized or documented way of doing this everybody\u0027s going to do it differently this is bad for an operability for security and you know opportunity to get things wrong just as an example of it recently working on client mutual client TLS authentication and passing that between front and a back any component and everyone does it differently um and now we\u0027ve been pulling in one product and our other product has to do it differently than that one to match up with the way the first one did it because it was based on a de facto implementation of how Apache did it and it\u0027s just it\u0027s a mess and I\u0027m looking to avoid that kind of mess for these kinds of things going forward with tow combining however that may be and so next slide please um in so we got to a consensus to work on the problem that was as far as we got um because I didn\u0027t have a draft or anything but I took that and ran with it I wrote up a 0-0 draft and basically how this works is there\u0027s a new HTTP header introduced that\u0027s to be passed between the reverse proxy and the backend application called token binding context it\u0027s a base64 URL encoded bite sequence and basically that byte sequence is a concatenation of the token binding protocol protocol version that was negotiated the token binding key parameters that was negotiated which is just one bite and the km value on this is a sufficient amount of information for the back end application actually validate the so combining and the token binding "
  },
  {
    "startTime": "01:24:47",
    "text": "header the sacto combining header including that\u0027s provided and referred it requires some level of trust between the reverse proxy in the back end application um that they\u0027re not sending it to the wrong people are accepting the inbound from somebody else and the reverse proxy also needs to sanitize this header so it can\u0027t be just pooped and sent through from the from you know the actual original client quick example it looks like that you break it out you\u0027ve got the you know the version their draft 13 this came from a live example 2 is the UC PSAP 256 and then the ECAM follows that arm next please and so in real life someone actually implemented this already they have it shown up here the bottom one is the token binding context this is a mod token binding plug-in for Apache by a former colleague of mine Hans I think he is on the line so Thank You Hans he\u0027s got it working um which is nice it\u0027s not too difficult next slide please um but he and some others recently have raised some questions about whether this is really the right approach and my my goal really is just how kind of one way of doing it I\u0027m not married to one or the other but but there\u0027s kind of two general ways to tackle this one is the current one where the backend application validates the tow combining message itself and it does this by being provided this additional data context from the front end on this keeps that took a TTR p the TLS terminated reverse proxy velit of Li lightweight it doesn\u0027t have to do any extra graphic work on the token binding header it doesn\u0027t have to parse the HTTP header pull it apart which is I think a goal that many have stated either explicitly or implicitly for this is to keep that layer as light as possible I think it even one point durkee said that Google\u0027s implementation does validate the the tow combining at the front end and that in retrospect you might consider doing that different is that still true yeah maybe okay so that that\u0027s the kind of the main main goal here um it does introduce maybe some problems reconciling and updating supported he parameter types if you have a lot of different applications behind one reverse proxy and you want to add a new key parameter type all those different applications have to be able to support it in order to make this work it might make that both the management of it and rolling out new ones rather difficult it also potentially is you know what feels like a lot of redundancy in trying to make all these different back-end applications capable of processing and validating the token by name header which is arguably a a little bit difficult um the alternative approach at some level would be to have the reverse proxy validate the token binding and pass back some information about it either the token binding IDs itself the whole message maybe the hash of the message i\u0027m not sure exactly but something basically where the work happens on the front end and some sort "
  },
  {
    "startTime": "01:27:47",
    "text": "of digested version of the hook and bindings get past back to the back end um this is way simpler for applications I think it\u0027s a lot easier to deal with the support key parameter types because they\u0027re all isolated at the front layer you can upgrade that everything works you just pack so combining back to the to the back end the downside of it is it\u0027s definitely less light on the token up too many teas excuse me the TLS terminating reverse proxy arm and that that violates a goal that at least some have definitely said they\u0027re interested in and I\u0027ve had some people go so far as to say that the draft should describe how to do both um which i would really prefer to have just a single way of doing it whatever it is rather than both but it\u0027s been thrown out there as a compromise of sorts so um the current draft is the the first approach that the second one I think is totally valid there\u0027s trade-offs between the two um yeah is it is now a good time yeah hey pachuco Google I wanted us to radiate what you just said that the second approach is much cleaner application and the fish shouldn\u0027t push the complex logic of verification of top and binding to each application and just keep it at the top see okay so we\u0027re going to we\u0027re running out of time so we\u0027ll cut the line what could you say just before after coming yeah hey just just before Tony yeah we could come on that they just after Tony but was that your last life not quite but um so I\u0027m not particularly fussed about whether or not who does the parsing and validation but I am fost about the fact that if the the proxy fails the striptease properly and um you have a really serious problem and so on as I said an email i really be nice if the proxy give you some cryptographic Mecca is some secretory binding did this value so that you can sit the client could not generate a value which would pass through the proxy and be acceptable with application max signature X or X door with a random with random value stop in a random value you something your points were actually on the next okay who\u0027s to raise discussion I\u0027d I\u0027m not sure gonna have time I don\u0027t think we go yeah I agree we don\u0027t have time so the Justin richer and so I\u0027m in favor of the first approach of sort of doing the calculation and passing it back to the applications because that allows the applications to do sort of the full processing of these things and figuring out what they want to do with that information whatnot and I would also like to point out that this does not preclude the gateway from also doing validation of the of the TLS toe combining if it wants to dig into the messages and all of that stuff as well the question then is what can it then pass back to the backend system to say "
  },
  {
    "startTime": "01:30:49",
    "text": "that yes I also did this and this is where I think that what was just brought up is a by Eric was a really good point that if the Gateway if the Terminator is doing something then the client sitting behind it should have some indication that that really did come from the Terminator we see the exact same problem at the application layer with API gateways and all sorts of other messy things that people aren\u0027t you\u0027re absolutely right in your intro people are going to deploy these things anyway so we should get the tools to do this correctly so I would like to see something that mixes in keying material from the gateway or something like that in generating this to prevent injection attacks and lots of other things that we\u0027re already seeing in other similar spaces so the injection attack though is mitigated by sanitizing the headers and while there are other ways of dealing with it if they\u0027re properly done it\u0027s done if there\u0027s other ways again with the I introduce guys at a time and so Tony last word and then I suggest you use the you know fine time go in a huddle somewhere and their respective out Tony nedelin we\u0027ve implemented this different ways within Microsoft and one of the issues that we have is this doesn\u0027t work on untrusted endpoints very well at all so that\u0027s accuse case that just this thing doesn\u0027t work we\u0027re also not seeing any real need for the interoperability aspects of of this mainly what we\u0027re doing is talking to her own back ends and we want to be able to do what we want to do our own way and sometimes we have validation on either side of this thing so we\u0027re not we don\u0027t want to be locked into this particular situation here yet so can you mark Tony down as against interoperability all right that\u0027s it for today come thanks everyone for coming up syros next time obviously it does this doesn\u0027t lock you into anything this is about you know open source and better products and Owen hood so they can hurdle together here\u0027s right yeah possibly I think so yeah oh and by the way it was a chair screw up his mic like I don\u0027t care I was gonna go to curdle but yeah you know Kathy moving over to me anyway but so you get stick me with that case I\u0027ll yes thank you I think critically and early and finally conquered all on my own fire when you cook it out you "
  }
]