[
  {
    "startTime": "00:00:05",
    "text": "and audio video and so on and there's one that is labeled um probably sharing slides or something well i asked to share slides um it says share preloaded slides right so i need to just uh right share preloaded slides and then you can you get a list and then you select your presentation and then you have the control and can can move forward and so on all right wait for it till we've done our chairs introduction and it's 7 30 so we get started anytime okay but if there are any issues let us know and we can also just run them for you okay yeah welcome everybody to ic energy at iatf 113 in vienna special greetings to our friends in the room thanks matthias for being our on-site co-chair today it doesn't look like we will have lots of questions from the room today but let's see so matthias is going to manage that in case there's something to be done um hello thomas i see you in the back okay um yeah i'm the coach and my coach is dave iran and um so um i think you probably have seen this light now on day five of the meeting but yeah so just make sure that you are using the the mid echolite tool if you are in the room and if you're not talking or presenting turn off your mic and video as always um of course this session is recorded um just quickly"
  },
  {
    "startTime": "00:02:01",
    "text": "we still apply the irtf ipr rules so um let us know so in in summary let us know if you there's anything ipr related that you see being discussed here um right so everything is recorded um the irtf is using the itf uh privacy and code of conduct rules and um we also have anti-erasmus procedures so if in case of any issue um we have an ombuds team that you can can contact and quick reminder so we are doing research here and not standards development the best we can do is uh like publishing experimental and specifications as rc's so the idea is in general to enable research and generate insights for the ietf or the general internet community okay today we are supported again by jose thank you very very much really appreciate so my real as you say is taking the notes um in the uh hedge doc um note-taking system i pasted the link into the chat um if you want to follow along or assist in the note-taking you are welcome to do so and this is our agenda so we have a really nice agenda um today um so um we are not going to to read this just asking is there anything else that you want to add or you want to say about the agenda okay great so before we get to edmonds presentation"
  },
  {
    "startTime": "00:04:02",
    "text": "check where we are in the group so we recently had a really nice flow of outputs from ic energy so um being published and we have a few more um that we want to um get out so ccn info um is i think still waiting here for irsg reviews um we're going to talk about flick later um icn lte um basically went through the ihg conflict review and the authors just published a new version and as far as i understand what colin is asking for right now is a statement on the ipr situation so would be great if the authors could really clarify that as soon as possible um to get this published soon and um i see colin joining the queue all right yeah just just quickly on that one the the issue there is that uh there was an api declaration on the individual draft uh before it was adopted as a research group draft which hasn't been right for the the research group draft uh so we're just waiting on clarification whether that's still relevant or or if the draft has changed so if it isn't is it just me or is your audio breaking up let me just save here it's possibly seems to be on your side calling it here is that better yeah i think so yeah uh my my my laptop runs out to cpu with all the video streams and that's the audio uh all i i was saying there was the the"
  },
  {
    "startTime": "00:06:01",
    "text": "ipr issue is that there was um an ipr declaration of the individual draft uh before it was adopted by the research group draft which hasn't then been reflected onto the research group draft so we're just looking for clarification on whether that that still applies uh and if so it needs to be updated for the research group drift yeah okay thanks uh dirk crossan is at the microphone yeah hi um as i mentioned to to colin because he's been pulling the authors and while i have no ipr declaration obviously the draft was initiated when i was with my previous company the problem is as you may recognize given the current situation i can't query with the company directly they won't reply to any email that i send directly so we've tried to figure we need to figure out how to get in touch with in the digital to figure out if there's any ipr that may be pending i can't declare i mean for my side i say no but i don't feel comfortable in doing that 100 so colin maybe i can give you a couple of names and you may ping them they may reply to you if i do that they won't uh i i i i i can't if you think it's necessary that they've already made an ipr declaration for that draft so okay good i didn't realize that good thank you it's really just clarifying okay yeah thanks um for that um and then you may have noticed that there have been new versions of icn ping and i seen traceroute recently so we want to publish these specifications soon as well and so right now they're waiting for the shepherd oops and that's me um so i'll take care of this um very soon and um david do you know the status of the nis architecture considerations draft"
  },
  {
    "startTime": "00:08:08",
    "text": "i will continue at least i can't hear you yeah same here you're muted okay so i think this is um this is actually done um so we're waiting for it to be published so maybe dave can come back later and clarify any questions that may arise okay so with that let's move to our first presentation and ah colin is my audio working yep so for the nrs arch considerations i think it's the rfc editor isn't it yeah right so i am that's what it said um i think it's yeah i think it's done so there's nothing we can do at the moment all right okay so um edmund are you ready um yes um so i've requested to share slides right so um [Music] yes so i do okay i think it's coming up and then i used to really share right"
  },
  {
    "startTime": "00:10:01",
    "text": "yeah yeah looking good then i can i can just operate it from here right yeah you can use your your cursor key something all right all right great thank you uh dirk uh and thank you uh dave for inviting me to present uh today for the itt energy meeting so i'm edmund yey from northeastern university and this talk is about a project that we have been um uh pursuing for the past uh more than a year uh it's called ndn for data intensive science experiments uh indeed to uh for for short um so so uh this is actually a follow-on to a project uh which is called sandy uh which uh started in 2017 and i believe two years ago i made a presentation to icnrg about sandy and indies and of course at that time it was more preliminary so the project concerns data intensive science so the examples that we're considering here are examples such as large hadron collider high energy physics lsst the largest synoptic survey telescope or ska or genomics these types of science applications essentially you have a situation where there is an enormous amount of data being taken either at one or or a number of different locations around the world in the case of large large hadron collider of course the data is being taken in cern and in in geneva and that information then"
  },
  {
    "startTime": "00:12:00",
    "text": "have to be distributed has to be distributed around the world to a couple of different um a couple of hundred different institutions for analysis and these are very big data volumes that have to be distributed and they're distributed distributed for analysis purposes and computation purposes and uh so we we had um started this collaboration almost five years back and uh there exists actually a uh problem uh in this area because um because um the data volumes here for instance for large hadron collider are said to grow um very fast almost 10 times due to high luminosity with what are so called luminosity experiments coming up they're already on the uh in the order of exabytes and is said to grow another 10 times in the next couple of years and even given the considerable resources that lhc network has they are still going to have a lot of problems in handling this data volume and distributed around the world so they actually sought out ndn and particularly our group to work with them to build a new system for them uh as a future system that can distribute this data more effectively for the lhc community so this particular project is funded under the cc star program by the nsf and the team consists of myself who's uh i'm the lead for the project um at um caltech our co-pi is harvey newman who is someone who has been involved with the lhc high energy physics network for many"
  },
  {
    "startTime": "00:14:00",
    "text": "many years uh we also uh have uh from ucla li xia zhang and jason kong uh so alicia of course has been a leader of the ndn team for some for a long time and jason kong is an expert in fpgas uh from ucla we also have a participation from susmit chanegrahi from tennessee tech um and susmit has been working with ndnt for a long time also and he had previously worked in them in the climate area using ndn for climate and of course we're in partnership with lhc uh the genomics collaborators and the ndm project team as at large we're also interested in this particular project on genomic data although that work is still more preliminary because compared to the uh work for high energy physics there is a need to use uh diverse computation storage and networking resources to meet the challenges that have been posed by um by these data intensive science fields and i think during my last talk two years ago i outlined the need to essentially build a system which with an architecture which is more appropriate for the needs of these applications which are very much data centered around data so the traditional architectures which centers on connections and servers and processes are not especially well suited for these applications whereas we believe that ndn is well well suited and what we're doing is to build a data centric ecosystem for providing agile integrated interoperable"
  },
  {
    "startTime": "00:16:00",
    "text": "solutions for heterogeneous data intensive domains that that is sort of the overall goal and indies is an important project in in furthering this goal i want to make sure that i yes so what are the what are the goals of nds in particular it is to deploy and commission the first prototype production-ready indian-based pedo-scale data distribution caching access computation system serving major science programs so it actually has an ambitious goal of really putting in a system that can work for the intensive science based on ndn uh led high energy physics is the leading target use case uh biogenome human genome projects atlas lsst ska are future use cases we want to leverage ndm protocols high throughput forwarding caching methods containerization techniques integrated with sdn methods and fpga acceleration subsystems to deliver lhc data over wide area networks at throughputs approaching 130 gigabits per second so we want to really build a system that delivers data over a real wide area network and a lot of the work actually is is in interfacing with entities such as internet to es net um to actually put this network together and test it uh over uh the geographical i mean it's basically cross country right now in the united states we would like to dramatically decrease download times by using optimized caching uh we're building an enhanced land testbed with high performance ndn data"
  },
  {
    "startTime": "00:18:02",
    "text": "cache servers all right so let me this talk is really about updating the progress of nds what we have what have we done over the last year or so um so i would like to talk about a few things uh that this team has been doing uh the i'd like to discuss the indies deployment architecture and ndnc which is an integration of ndn cxx with ndnd pdk for the purposes of the indies project i'd like to discuss the when testbed and the throughput tests that we've been performing and also the experiments that we've been doing with optimized caching and forwarding i'd like to discuss the congestion control work that is currently ongoing and you know in the ucla group and uh how that's interfacing with caching considerations and also discuss the fpga acceleration work that uh professor kong's group has has been doing at ucla all right so let's first talk about the deployment architecture in ndnc so um here's the basically the deployment architecture and to me this uh looks a little bit uh small actually i hope um you guys are gonna see it uh see it full screen here um so uh the um you can see that uh we have basically on the left here you can look at it as an in uh the consumer side and on the right hand side is the producer side um we have developed um a containerized setup where you see that"
  },
  {
    "startTime": "00:20:01",
    "text": "we're building docker containers which enables us to make this work for various kinds of operating systems and these cm ssw jobs are jobs which are generated uh for within the lhc system so these are jobs which require that require certain calls to certain data sets and xrd law uh the x3d plug-in is essentially a plug-in that interfaces with the between the the calls within the um lhc networking environment and um and then turns these requests into essentially ndn consumers uh and from there uh we have a a set of functions which are implemented using uh extensions of uh of ndn uh cxx and interfacing within the adpdk uh the indian dpdk forwarder through something uh called mmf and we then go through a network which is a high speed network having capacity up to 100 gigabits per second in the right hand side you see essentially the producer side and operating finally interfacing with a number of different services being provided by the lhc system so i hope we can see that but let me describe this a little bit in detail so the caltech group has built something called ndnc which is a lightweight integration of ndncxx with ndndpdk to achieve high throughput performance in scientific applications"
  },
  {
    "startTime": "00:22:02",
    "text": "so you you can see the code there in that link it uses the memo shared memory packet interface to provide prof to provide that provides performance packet high performance packet transmit and receive functions between user application and the the vector pro the vector packet processing uh within indian dpdk um so i'm going to skip some of the details here um the uh so um the the each fake transmitter receive one or many of these blocks in a single burst and it offers pit token support which um is something that is needed by ndndpdk and it uses this ndnc access library to encode and decode level layer 2 layer 3 packets it also has support for congestion window and retransmission either fixed or am aimd this is something that we're still uh playing around with a little bit to see how these congestion windows should be set but this is built in the ndnc system uh future plans for ndnc we like to do extensive benchmarking to understand the current behavior and maximum throughput performance that can be achieved identify guidance possible bottlenecks add multi-threaded support to mammoth and pipelines currently that's that's a single threaded as we mentioned over here and we would like to port the ndn x4d plug-in developed in our previous project to"
  },
  {
    "startTime": "00:24:00",
    "text": "ndnc uh and we want to extend the number of applications to cover some of these services that are provided by the lhc network and the file systems they have okay so let now let me talk about the when test bed and the throughput test that we've been doing so okay here so here's the test bed that we've put put together and and in fact you know putting together this test pad is actually in a major uh time uh major time uh time zone for us um but uh you know it's it's worthwhile because this is the high performance testbed that we wanted to have so it has a number of nodes here basically it involves the participant institutions of so northeastern the node is actually sitting in mgh pcc which is a shared computing facility uh between mit harvard northeastern and bu um there's a note at ucla there's noted tennessee tech there are just multiple machines at caltech and we are also um very happy to have the collaboration of starlight uh in chicago so you see this is a topology essentially it's um a fully connected network here um and with different uh link capacities here we have um a very high capacity links of 100 gigabits per second uh between northeastern all the way to caltech and then some of the links are less smaller capacity 10 gigabits per second but these are you know it says high pretty high performance network and running across the country here and you also see the specifications uh the configurations for the machine sitting on this network"
  },
  {
    "startTime": "00:26:01",
    "text": "so we have quite a bit of um do you allow questions in between or shall we ken go ahead hi hi edmund um hi are these running uh are these all over internet 2 and is it l2 stitching uh yeah is that what they are okay we we put this together with a lot of um collaboration from internet to esnet um and uh and uh yeah this is running at layer two yeah so we have um uh we have these vlans uh the the numbers you see there are vlan numbers which have been provisioned and in fact you know just doing the provisioning the vlans uh sometimes can take months to accomplish as you know if you work on these things but yeah it took about quite a bit of time for us to put this thing together but it is these are dedicated there's a dedicated host at each one of these sites is that right yes so some of those machines um are shared um but some of the i mean so the machines that sitting at the participant institutions they're all dedicated um the machine out uh starlight uh i i think is is shared but it's not heavily used by other applications yeah okay thanks mm-hmm yeah so so this is um uh the network that we're currently experimenting over and uh i i you know it's it's an incredible actually resource that we actually have here because we're able to do real experiments on a wider network basis um it also of course requires a lot of upkeep um for the throughput test i won't go through all the details here but basically we actually wanted to do the throughput"
  },
  {
    "startTime": "00:28:00",
    "text": "test from um on the 100 gigabit link from northeastern to caltech but because of various reasons uh the the vlan from starlight to to caltech was only recently put up so we ended up doing the the throughput test uh basically on that um that that orange uh link here that you see uh but basically what that does is um it sets up a loop at starlight uh between two machines setting a satellite the path actually goes from starlight in chicago to canada and then back to starlight so um the um but but but we do have a consumer and producer on two different machines and the capacity of that link that path is basically 40 gigabits per second according to iperf and you can see the configurations of the machines there we have um two threads for the each consumer application and uh running three forwarding threads launching six consumers simultaneously currently we're using a fixed window size for congestion and uh there are various reasons for that well we have a dedicated path there basically and um we're sending the uh files which are one gigabyte to each these are evenly allocated under three forwarding threads and uh we are caching these these these files at the producer so they can access them quickly and we request them from the the consumer machine so uh what we're getting here and we're really trying to push the throughput on this path here um is for you can see that over a span of six minutes so the average throughput uh over the different consumers has listed it for the total of about 21"
  },
  {
    "startTime": "00:30:01",
    "text": "gigabits per second um that's the highest numbers that we got we're still playing around with the different uh configurations in this in this setup but this is currently the throughput numbers that we've been getting on this is all over a wide area network this is a real uh wan network over continental uh distances and so far i think that's the highest number that we have gotten over the past few years we previously had gotten some numbers more in i think at sc 19 we had 6.7 gigabits per second and at sc 21 we achieved about 14. so um so you know we are definitely pushing that that throughput um and understanding better what kind of um what the system is capable of of course this is you know running uh the consumer producer that that we have uh that we have written and working with integrated with nd dpdk and you can show that uh really ndn is capable of high performance over real wide area networks okay now uh let me talk about caching and forwarding and so this is also a big part of the project is that uh to try to test in in in a real experimental setting uh these caching algorithms that were first developed by my group at northeastern so we're using this testbed in two different here i'm showing two different sets of results one where we have two consumers at ucla and starlight and going through a forwarder at northeastern and then ending with a producer at caltech and another one uh with two forwarders in the middle uh one at northeastern one at tennessee tech and uh we have 30 files which each file"
  },
  {
    "startTime": "00:32:03",
    "text": "is 4 gigabytes and we we generate requests at the consumers according to a zip distribution and we run this caching algorithm which have been developed by my group called vip and we will then compare the performance of that to a case where you do not cache anything or use a kind of lru kind of caching or a an improved version of that which is actually within the indian dpdk implementation called arc so these are a real experiment results here what i'm plotting is the different colors corresponding to the different file indices so file one is the most popular one according to the zip distribution file 30 is the least popular one and they're ordered like this and now the algorithm that we're running the caching algorithm we're running uh they it doesn't know the swift distribution beforehand it actually just adaptively measures these things and adapts the caching pattern adaptively in real time uh without any prior knowledge and so here what we're plotting is basically the cash score which is output by the uh which is put out by the by the vip algorithm and you can see that actually at the forwarder node where the cache is uh the cash score of that most popular um file is actually the highest you can see that they're ordered in the right way and if you if you actually see what is actually cached at the forward or node you see that it's caching exactly what it's supposed to cache so the cache is capable of caching five files and it caches one two three four five um so this um is exactly what it caches um after you know some stabilization uh we run it for a few minutes um and um now what's interesting here is here"
  },
  {
    "startTime": "00:34:02",
    "text": "we plot the result of the compare the performance of the different uh scenarios here on the x-axis is the throughput uh in gigabit's perplex on the y-axis is the delay okay that's actually the download delay so you see that um the numbers for so we plotted the numbers for um what you get at starlight where you get a ucla in the overall average um and the uh the square ones are one for the for vip the um circles are for the arc which is a version of lru and the the triangles are for the case where you know cache so you can see that um vip is actually is actually getting lower delay and also more throughput in general the differences between arc here is not is not big but in the next experiment you will see the box actually gets bigger so you see that um uh the the effect of caching is basically to increase throughput at the same time is decreasing delay because um essentially because you you bring it closer to the consumer to any decreased delay that way but also because you make obviate you make unnecessary transmission further uh transmissions of the request upstream therefore you sort of reduce congestion in the network and you further reduce delay and you also increase throughput so you have both the effects here now and then just a real quick question is this dram caching or do you actually have external i o going on uh yeah this is um uh using uh caching in the um yeah let me so this is caching using um no this is dram caching yeah"
  },
  {
    "startTime": "00:36:02",
    "text": "great thanks yep uh right so um the next experiment uh let me see oh here yeah yeah so in test two remember in test two we are we have um uh two two nodes right two nodes in this so two caching sites and in this case um at the first one you see that uh at the northeastern one so so what you want you expect here um i just want to point this out in the second one where you have the two nodes here you would expect that the first caching point should cache uh the the top five most popular um files and then the next five should be cached over here right that's what you would expect um in fact that is ex that is pretty much what happens um so you can see it the first forwarder you catch the first uh the top five okay in the next uh forwarder you you essentially catch the top at the next five but some minor variations here um now this uh i think has to do with essentially the stochastic variations of of the of the algorithm but but overall you see that it is the it is the files that are that are being the right files are being um cached here and uh and of course the algorithm is doing this completely adaptively it doesn't know this beforehand so the the result here for test two you can see again vip is doing essentially the best everywhere as it did in the previous case but in this case the improvement is even more pronounced it has greater throughput in lower delay than compared with arc and uh to certainly compare with no caching so this shows very clearly"
  },
  {
    "startTime": "00:38:00",
    "text": "that if you are clever about um caching and by the way we have actually not even totally um exploited the full power of vip because vip is actually a joint caching and forwarding algorithm so because here in a topology we really don't have much in in the way of multi-path capability so we're really only leveraging the power of caching and already we're getting a lot of improvement in the performance so um the next step for us is really to test out these topologies over the land uh that has more multicast a multi-path capability and i think we'll get even better results um but it takes time to do that and we'll report that in in the future all right so let me see what i can say i don't know how much time we still have but uh um i want to say briefly about what the work that has been happening congestion control and fpga acceleration so uh leisha's group at ucla has been working on congestion control and uh they've actually been working uh on the impact of caching on congestion control so there are some interesting examples they've been studying so for instance in this particular example you have two consumers c1 c2 that are fetching the same object from the producer p and the objects are segmented however c1 has a higher uh capacity higher speed connection to f1 which is the uh which is a caching point and c2 has a smaller as a slower connection so the expectation is that c1 uh the c2 is going to you know start first okay uh c2 is going to start first and uh so uh you the expectation is that c1 will initially be satisfied by the cache because c2 pulls the uh the the object first and"
  },
  {
    "startTime": "00:40:00",
    "text": "caches it at f1 so c1 will initially be satisfied by the cache and later catch up with c2 and eventually be satisfied by the producer um and uh however what is actually observed um so so so you see that c1 because it has a higher bandwidth is your expectation is that you would first get it from the cache and then later catch up with c2 and be satisfied by the producer right but what actually is happened happens in simulation is that c1 may never catch up with c2 or be satisfied by the producer instead it will be continued to be satisfied by the cache and c2 will be soliciting data in steady state and the reason is that um uh so so here's a plot of what you expect to happen in terms of uh this this um this is the x axis is a time and at y axis is the delay uh that is observed by c one so you would expect that it first gets it from the uh cache uh to f4 f1 and gradually would take over um and then eventually we'll get it from the producer but because um uh what happens is that um so the scheme that i think alicia is working on is you know the congestion is is the control is based on looking at the difference between the interest sending rate and the data receiving rate so that c1 will essentially look at this and interp and interpret um this difference because it you know it's getting it from the uh the the the receiving rate is essentially lower than the sending rate so it interprets this as a congestion signal and actually decreases its request rate so what happens that it actually then never catches up with c2"
  },
  {
    "startTime": "00:42:00",
    "text": "so uh the the observation here is that this is some um a a um somewhat unexpected way in which um the congestion control interacts with the caching so that um they're still exploring this and uh they're interested in looking uh working out congestion control measurements which must be resilient to these rtt uh variations so um what they have done so far is they have developed a multi-path interest forwarding uh a hub they've developed a hop-by-hop congestion control design that uses queuing delay as a control signal which is hot by hop it's still i don't think it's published but this is well under development and this is able to respond to different kinds of bandwidth uh limitations upstream and it uses the queuing delay as a signal which is propagated top by hub to from downstream to control the um the sending rate to the request rate uh of course then the question is how does this interact uh with caching and that that's the question that they're um exploring right now and uh uh we'll report on more details on this later all right so let me say something about fpga acceleration so this is the group of jason kong working on the fpga acceleration fpgas are used in wide variety of applications in machine learning networking in ip long distance prefix matching packet inspection for firewalls and so and so forth why use fpgas they are capable of pipelining tasks each cycle can start up a new iplookup for instance it can do parallel processing multiple interfaces of its"
  },
  {
    "startTime": "00:44:00",
    "text": "own have its own processing block instead of sharing one so there are many benefits the goal is in in here uh if the goal is to use fpga in the forwarder input stage so we're talking about the indian dpdk forwarder uh the where the components of the interest name have to be hashed and there has to be a table lookup uh to to see uh which interest packet should be dispatched to which forwarding thread and so they're essentially you have to do the the the prefix hashing and you have to do the table lookup and those are both computationally expensive and this is what the fpga will be designed to do and we're still in the preliminary stages of this but uh there's been some progress so again looking at the hashing of pre-physics and name components table look for thread dispatching preliminary results show a four times improvement four time improvement over doing this all in the cpu um four times improvement in terms of the throughput but uh the integration with uh ndpdk is still ongoing um and uh so there's actually a multiple stages to this first we will apply uh the fpga acceleration to the uh the ndt which is the dispatch table for the input in the indian dpdk forwarder then we will apply it in a similar way to the combined pit cs table and finally to the fib so um we we are still in the first stage but we're the there's some promising results of of uh having uh some throughput improvement improvement okay so um i think i've run over time so um"
  },
  {
    "startTime": "00:46:00",
    "text": "to give you some summary i mean we're of course it's really a progress report what i have said here um just to give you some results and we're still working hard on that but i think we've achieved quite a bit i mean just having the when uh testbed put together was actually a major achievement as far as we're concerned so the we have a high performance nd's deployment architecture with containerization and integration with nd dpdk using ndnc we've established a high performance ndswan testbed we have managed to obtain a throughput of roughly 21 gigabits per second over this testbed and we've shown that optimized caching forwarding yields significant uh improvements in both delay and throughput over this testbed these are real experiments we have been developing hub by hub congestion control based on cueing delay looking at interactions with caching fpga acceleration of hashing of name prefixes and table lookup for a thread dispatching shows the four times improvement over cpu but we're doing this in cpu and we're working toward the prototype production ready indian system and uh we are seeking on the long term collaboration with other domain sciences networking computer systems communities all right thank you very much great thanks a lot to edmund for it's really interesting presentation let's see some questions so one question that i had was um so i mean this is obviously um you know a lot of work um went um into setting up"
  },
  {
    "startTime": "00:48:00",
    "text": "this testbed and really accelerating this data access for these large data sets and so on um i was wondering so once you are able to do that um often you want to do something with the data so it's like processing the data is that also in scope of the endless pro i think yeah a good question um so it was actually in the proposal there was some um there was a section on in the original proposal it was proposed that we would also look at joint data movement and computation scheduling of these workflows um but due to the then then the budget was cut a little bit so we removed that from the scope of the project for now um obviously that's that's the goal of the ultimate goal of um hopefully a continuation of this project is to not only deal with the data movement which is what we're doing right now but to handle the workflows and that's something that's definitely very very relevant and this is the whole problem in high energy physics application and many of these other applications so i think there's still a lot of work that remains to be done but um i think that the data movement is definitely the first step um okay yeah cool um so we have thomas in the queue hello edmund thanks for this uh very interesting talk a question on your caching analysis i mean you're considering high energy physics or other huge bulk scientific data and you were you were assuming a zip distribution um i was a bit surprised about this i mean i would expect zip with netflix but but is there a use pattern that actually distinguishes between certain certain portions of the data in high energy physics"
  },
  {
    "startTime": "00:50:00",
    "text": "so we actually took uh statistics um using actual uh request data from the um lhc community and in fact there is a similar falloff you know that you would expect in i mean usually you you have data sets which are so called hot data sets which everybody wants to work on you know so there is actually a falloff in the in the popularity um uh and we fit actually the the distribution with this this one it's it's a good approximation um so this is actually based on data that we collected um is that is that your question or is it uh yes it does thanks yeah yeah so uh so it's it's a similar enviro phenomena actually is is true in in physics um what happens is that you know there's there's typically some big questions that everybody is trying to explore right and that requires certain data sets which are maybe the you know the most recent one or some data set which is really relevant to the task at hand and then there's of course a tail and uh yeah okay next question from ken thanks edmund you may uh as you know may know we're we're trying to do some similar we're trying to build on some of your sandy stuff actually and you've probably seen from my master's student but um my question is when you in the three hop thing where you showed the the architecture of the consumer and the producer uh what did you develop a bespoke transport protocol for that or are you using some kind of generic chunking protocol to break up these very large data sets into segments and"
  },
  {
    "startTime": "00:52:01",
    "text": "is it you know is it designed for this application or is it a is it just a generic uh chunking protocol i uh so questions how do we chunk um so you know um yeah i'm trying to find this so i i think uh it's okay i'm so i think these are um so i'm not sure if my student is online right now but maybe he can give you better details on this uh i'm not sure i think yeah he's there you can answer offline if if that's easier it's just okay yeah so i think that chunking is essentially is is is something that is essentially inherent in the application um so um i don't know if you and how are you able to speak quickly or no um i guess he has to ask to be to speak or something right he can just unmute himself okay yes yeah i'm thinking about chunking uh yeah the chunking is implemented uh directly embedded into the file server and the consumer application the how to channel the chunk the content and uh yeah so uh it's basically where to uh do all the things oh yes content that's something that was you know developed by ndnc right um and this and the ndnd ptk group the national development by server okay that's that's what i wanted to know is whether there it's it's part of dbdk or indianc or something like that or was it is it actually built into the"
  },
  {
    "startTime": "00:54:01",
    "text": "application but yeah just i'll just send you an email and ask you that question yeah let's go let's let's talk about it more yeah so thank you inhale thanks ken okay okay one more question and that's the final one by dave well i'm going to abuse my q location asked two questions hopefully they're both real easy ones uh number one and you're 21 gigabits per second are you doing signature validation uh i do not believe so um how we're not we're not doing we're not doing we're not doing signature value of validation right right right so you're not you're not actually exploiting the um security properties of indian uh so what happens is this in the in the i mean that's a good question in the high energy physics application i mean that's something we will be doing um i think that's the the the the sort of the providence aspect is a part of the project but right now we're not doing that and in fact the physics application what happens is that you have a sort of a pretty strict security parameter for people who can access the system you have to be approved and so on and so forth um so they don't actually usually worry about security too much within this physics application but nevertheless we as a part of the project we did propose to um to put to put the signatu you know to to sign the packets and to verify them you know to provide providence so um we're not haven't done that yet okay and then one final quick question which is um 4x out of improvement over cpu for an fpga seems not very impressive do you have any insight into what the bottleneck is"
  },
  {
    "startTime": "00:56:01",
    "text": "yeah um yeah you're right i think it's not i think that these results are still preliminary tell the truth um i think uh jason's group is still working on that um well just just for the future i'm really interested in this subject uh yeah and i've been working on some stuff that's going to be bottlenecked on um on fpga style hashing of um of these uh load distribution to courses so i think it would it's really that works of really broad interest and publishing some stuff on that would be really great well you know i i absolutely i don't want to say anything definitive here because um jason's group is really the expert on that um i we can take this offline and we can i can probably give you more insight um uh talking with their group about but it's ongoing work i i did you shouldn't see this as finalized at a bio by any means network coding the same way was that if you can't exploit the high parallelism by doing you know 20 50 70 packets at a time in the fpga you're just not you know because the clock rate the inherent clock rate of the fpga isn't just all that high right you really need to be able to paralyze things absolutely so so um i i think you're right i think this is not that impressive yet um and uh but uh the the fpga work i i have to say um it it is ramping up it's ramping up the first part of the project was much more focused on these other things that i talked about the caching and setting up the win test bet but i think during the next couple of months we should be seeing uh much more ramping up of the work on fpga and we'll keep you updated"
  },
  {
    "startTime": "00:58:02",
    "text": "great thanks okay thanks um so i'm sure people have more questions so have i but um yeah let's uh defer this to the main list um i'm sure edmund would be happy to answer your questions um thanks again edmund um for being very much all the time yeah for the opportunity hope to see you again soon yeah good thank you okay so uh let's move on um and next would be um i'm gonna one on um data time encoding for ccnx yes thanks to um i dirk ide for vienna i think you can hear me right um i'm asking to share slides yes granted so i can see my slides now looking good all right um yeah thank you thanks again for this uh for this five-minute slot that we that you provided us so this graph is about a compact time representation for ccnx it was actually first presented in 2019 uh i think in singapore by thomas thomasod and then in 2020 virtually by me um so far it hasn't received that much attention on the mailing list and that's why i wanted to also give like a brief recap on the core ideas of this of this draft so the resource constraint environments we have usually like a low bandwidth and high latency access to to wireless links is typically slower than the packet processing within the stack itself and the packet transmission is usually the dominating factor for the energy consumption of these little"
  },
  {
    "startTime": "01:00:00",
    "text": "tiny devices and header compression is actually a solution to to reduce the energy expenditure but it also improves transmission reliability which we also measured and showed during our work on icn open which is now rfc 91 39. so ccnx is making use of two different types of representing time first they have a relative type relative time which is used in the interest for the interest lifetime and there's also absolute time which is used for the signature time expiry time and the required cache time in both message types the relative time is unbounded this means that the tlv supports an unlimited number of bytes that you can use for to represent the offset and the absolute time is limited to 8 bytes to encode the unix time in milliseconds and in this particular draft we are like i mean the idea got in got inspired by um by rfc 5497 which is a time tlv rfc uh for or yes it's a time present or like a dynamic range encoding for money routing style protocols and this is all very similar to the work in ieee 754 which is the floating point standard and the idea is that we encode the exponent and the mantissa of a number into a single byte and using the formulas that you can see at the bottom of the slides we can [Music] calculate a time value which is in seconds then from this one single byte the subnormal form of this formula is"
  },
  {
    "startTime": "01:02:02",
    "text": "needed to solve actually the the underflow issue um because if you only go with the normalized formula then you have a huge jump between zero and the smallest number you can represent so in the last presentation in 2020 we actually presented like multiple configurations for this single byte because you can play around with the mantis and the exponent sizes and come to like different ranges and so also on the mailing list we had this discussion uh two years back and i think this is the configuration that we converge to and this is also now in the draft and this is configuration we can have numbers that range from from zero you can see in the in the on the left side the table there's the subnormal range and then it goes in in seven millisecond increments until the normal range and normalized range and from there on it goes lower smoothly until a very huge number which in this case represents close to four years and check some questions and yeah yeah sure so ken is in the queue sorry i just forgot to leave the queue i don't have a question right now okay then dave is in the queue yes dave i forgot to remove myself okay i'm sorry and then i am okay sorry let's go ahead all right so if there are any questions then please raise the hand um so from here on uh there was this huge question also two years back on how actually we can integrate this compact time in the current ccnx protocol specification and there were like multiple solutions to that the solution that we mark moscow was also"
  },
  {
    "startTime": "01:04:01",
    "text": "giving a lot of input on the main list for that so the solution we went with currently in the draft is that if we for example look at the interest lifetime we say that if we set the length to one then the forwarder has to interpret this number as the compact delta time and if it's like any number greater one then it's the typical ccnx time delta in milliseconds for the recommended cache time is a little bit different i said before that's fixed to eight so it's not actually using other other sizes there so what we said in the in the draft is okay if we use a length of one then we put the compact time in there but you probably saw it's a delta time so it's a time offset and not an absolute time anymore but we think that the regular cache time is not so critical if you lose a little bit of precision there so the forwarder must be able to convert the the time offset to absolute time on return on transmission and receiving the packets so there's obviously uh well the advantage of this uh integration level is that we don't need to allocate a new tlv number at the honor but the disadvantage is that this requires an update to to the our ccnx messages rfc and that if the if there's a forwarder that doesn't know about this change then of course the time is like misinterpreted i think that was also one of the core arguments for hitachi i can see that hitoshi sorry i can see that he's in the in the in the chat at least so maybe he has something to say to that um so an alternative integration would be to actually define a new top level uh tlv so to allocate a number at ayana and then we would have something like"
  },
  {
    "startTime": "01:06:00",
    "text": "interest lifetime compact and this was this would live like next to the interest the typical interest lifetime and of course the advantage is that if the forwarder doesn't know this the tlv it probably just ignores it and takes a default interest lifetime and yeah the disadvantages we allocated number and this um brings us to the end so this yeah within the two years there were like very subtle changes to the draft itself the div to version three was actually just to describe the integration level that i just showed you in the last slide and the div to version four is that we change the formula to streamline it streamline it more to what time tlv was saying and what we chose for iceland open and also there's a pseudocode in the in the draft that specifies how to encode the actual numbers and we had to update this according to the formulas and also we added terminology and enlargement sections yeah you probably saw we need more feedback also on the mailing list especially for the protocol integration so how to integrate these compact times into ccnx and also there's the expiry and signature time which is currently absolute time and there were some ideas floating around a couple of years back whether we can generate offset time offsets of for these tlvs the problem is that they are within the security envelope so it's not that easy to change that on the forwarding but maybe we can also find a solution for them so and then is the question um i mean if there's interest in the group for this particular work then would this be ready for uh rg adoption all right thanks thank for progressing this and for um bringing this back um"
  },
  {
    "startTime": "01:08:01",
    "text": "yeah so i mean we think this is um the right type of document um for a like research group activity um is uh it's kind of a it's a useful um feature um especially in these um high delay environments and it's also it's raising interesting questions on protocol extensibility um so like from a shares perspective i think we would propose adopting this we will reconfirm this on the main list but in general i think this is in the right type of document for that is there any immediate feedback on shanks questions so the protocol integration or the encoding okay because that's what we need of course as a next step then um make a decision in the group by how we want to go about um integrating this especially and um yeah so let's um discuss this on on the list then all right thanks again yep thanks all right so um so we are moving on with um the updates on ping and traceroute um so spiros can't make it today so dave is going to present these things so hi um spiros couldn't make it he did the slide so i'm just going to quickly go through things on ping and traceroute so just to remind people these are these are instrumentation and management tools for nbn and ccn style icn protocols and they're analogous to the similar capabilities we have in ping and trace route in the ip world although because the architecture of the underlying"
  },
  {
    "startTime": "01:10:01",
    "text": "protocols is different the capabilities of these um these protocols are similarly somewhat different in terms of how they support multipath and how they can support the existence of caching in intermediate nodes so ping gives you the reachability of names both in from producers and on path caches and we have again packet formats for both of our popular ndn underlying protocols ditto for trace route um this is a multi-path capable trace route uh just as we've seen in the ip world with um with tunnel trace uh types of things but this is now built in on day one uh and similarly we have um protocol encodings for both uh our popular icn protocols so the current status is that these drafts have been around for quite a while they've been implemented uh in a number of experimental settings uh we completed uh last call in january uh didn't get a lot of feedback but we got some very good set of comments from june child she um on the draft and both have been since updated uh by the authors so we had one issue that came up on last call which had to do with just how to integrate uh path steering which is used in the multi-path case for both ping and trace route uh into the into the uh encoding method methodology of this um and uh decided based on some discussion that it belongs in the base protocol as opposed to some intermediate link mapping protocol uh because although it's not an end-to-end uh capability it is modified hop by hop but the actual process on each hop is is"
  },
  {
    "startTime": "01:12:03",
    "text": "independent of the link type so the changes in the in the current version very quickly very few packet format was updated for the latest version of the ndn formats uh we moved this the tlv that carries path steering out saw after the signature tlv to make sure that it's excluded from the security envelope since it is modified hop by hop and um made use of the new capabilities in ndn to have types name components and then didn't have that until fairly recently so we use type name components both in the ccnx and the ndn variants of the protocols to indicate that ping and trace route require special processing uh by intermediate nodes so the next step is basically uh the authors think the drafts are ready for irsg review and uh dirk has been is going to have to be the documents that shepard since your other co-chair as a co-author of the draft and i'm done and i'll take questions okay i think we're done yes yeah i will take care of it um if there are no questions let's move on with the past doing oh one quick thing just for colin i'll confirm this uh with the mail to you but these two documents are are essentially a pair so the same irsg person that will probably want to review both uh since they're just two two piece of the pod"
  },
  {
    "startTime": "01:14:08",
    "text": "colin said make sense okay i guess i'm up for the next one as well dirk you want to introduce it um yes so the next one um is a refresher on also work that has been done earlier and that's path steering so um an interesting capability um that we can introduce to icn so make path selection available to um consumers and um so um we think it's it's quite um relevant for like the multi-pass discussions that uh keep coming up and like also deficiencies that other protocols have in this direction and so that's why we thought it's a good idea to look at this again and make people aware so yeah dave go ahead is everybody else seeing the uh messed up conversion slide on this stuff yes that's not on purpose okay so uh well we do have a capability of encrypting the path labels so maybe that's just the encrypted version it's supposed to say path steering or refresher all right so this has been around for a while um and we haven't had a lot of strong motivation to move it forward quickly until recently um and the reason the motivation has gone up of course is because we're progressing uh ping and trace route both"
  },
  {
    "startTime": "01:16:02",
    "text": "of which uh need this in order to be maximally useful as instrumentation tools in a multi-path forwarding environment because you want to be able to make sure that when you're measuring for example rtts that when there's a multi-path environment you get individual rtts for each of the sub-pads that might be traversed and similarly for trace route you want to be able to explore for multiple paths to destinations and be able to report on them so i'll quickly go through this uh a lot of this is material you may have seen before uh but we haven't really talked about it in a while so it's probably worth taking a little bit of time to go through it again so um so the underlying problem statement is that icn communication is inherently multi-path and multi-destination so we don't today have a mechanism for consumers to direct traffic interest traffic onto specific paths um we do have forwarding strategies in icn that could spray interest onto various paths but consumers have a hard time interpreting uh whether there are any failures or performance glitches when those when multiple paths are being used so um we need the ability for troubleshooting and performance tools to get this path visibility in order to find problems and do simple measurements and we discovered the same thing in the ip world uh when we tried to do things like mpls and tunnel trace where there are multiple underlying pads and uh and hence a lot of work was done sort of like late in the um evolution of the ip protocol suite to add these capabilities and as a result um it was quite substantially messy so we're hoping that by incorporating these"
  },
  {
    "startTime": "01:18:01",
    "text": "capabilities early in the evolution of the icn architectures uh that we'll be able to do a much cleaner job so what we'd like to be able to do is discover and monitor and troubleshoot a multi-path network connectivity environment and accurately accurately be able to measure the performance of a specific network path when there are multiple paths similarly we have a number of already published and implemented multi-path congestion control algorithms some of which not all of which but some of which work by estimating and counting the number of available paths identifying pads and actually allocating traffic to each path explicitly so we have two um that have been uh published over the last couple of years one called merck a multi-path rate-based congestion control algorithm and smek a sub-path window-based multi-path congestion control algorithm both of which would be able to properly exploit an explicit path steering capability so the design of this um actually goes back to a paper published around 19 2018 on path steering and also path switching which does optimized forwarding pads using past steering so the design question is how do you label the paths and over the last few years we've looked at a number of possibilities you can use bloom filters which are probabilistically pretty good we looked at what are called pairing functions which have minimal overhead in terms of storage but requires some multi-multi-dimensional math in order to compute them and don't deal with long paths very well"
  },
  {
    "startTime": "01:20:01",
    "text": "we looked at label stacks similar to mpos label stacking uh which um has some nice history behind it they're known to work reasonably well but require being able to vary the size of the the data structure as you traverse the network with the push and pop operations we chose to use fixed size labels simply because we expect um path length to be not un unreasonably long um and uh the processing of these to be being really really fast so uh the second issue is how do you discover uh paths and how do you steer packets on the paths that are discovered so one of the nice properties that makes this way easier than in the ip world is that we have symmetric forwarding symmetric routing so that returned packets returning or returning over the same path that the interest uh is uh is forwarded over so the interest contains a path label marked in discovery it's forwarded via the um least named prevex match in the fib and the content uh in a data message carries the path label that's been computed on the way up uh back towards the source of the packet and then as a subsequent interest can take this path label that was obtained from an earlier return data packet not mark discovery mode and forwarded uh via the the fib and this explicit next hop selection so we don't bypass fib look up here we simply use the path label to select the particular outgoing face among the possible faces that are already in the fit i say that because the original paper also had this optimization of being able to align the path the"
  },
  {
    "startTime": "01:22:01",
    "text": "uh the actual fib look up so we can reliably measure path rtts we can iteratively discover multiple network pads uh congestion control can discover and distribute load across pads and although this hasn't been proven in anything implemented yet um if you believe you might be getting a content poisoning attack across one of these pads and you have multiple paths that could bypass a poison cache consumers potentially can mitigate the effect of a content poisoning attack and we can also potentially build traffic engineering solutions if path labels can be distributed via sdn style route distribution so some of the interesting issues are how you deal with route updates uh so you have to invalidate uh pads that have been discovered so we have a an interest return in ccnx style mac for carrying an invalid path label which can uh invalidate it you could silently forward the interest through any available next top if you don't get a match and we can actually control this behavior uh by either forcing the error or allowing you to fall back or redo discovery the next obvious problem is how you deal with route updates we assign new next hop labels every time the fib changes and on the reverse path that we drop the data or the knack so that you can invalidate the path so the last piece of this which is the only piece that's really changed recently is how we do the packet encoding so on ccnx we add a new error code so that you can report broken path labels there's a next hop"
  },
  {
    "startTime": "01:24:00",
    "text": "header that carries the path label and the sub tlvs that go with that in order to complete the encoding on ndn um ndn hasn't adopted this but we have a proposal in the spec for how one would how much we believe should integrate this into the ndn packet encoding which is to have a new packet tlv called path label with essentially the same semantics as we've defined for ccnx there's just a picture of what it looks like now there's some security considerations here they're all in the document you can look at them um clearly consumers could do probing and maliciously mister packets um but you in order to be able to guess a correct next hop on a different path uh you have to sort of send about two to the 12th interests for for this to work um so uh we have a number of mitigations whoops in the spec in order to deal with that and i just unshared my uh slides let me put them back up again there we go back to security considerations so a second possibility is cash pollution by consumers and producers colluding uh to inject off path and bogus object so cache entries have to be annotated uh once you add this capability with the corresponding path label and only use that cache entry to satisfy interest with a matching path label and that cache entries should not evict entries for the same object with no path label or a different path label uh to to mitigate this uh potential cash pollution attack and i'm done"
  },
  {
    "startTime": "01:26:01",
    "text": "so um the i think the issue at hand which we'll talk about when we're done the rest of the the agenda is whether uh is perhaps we should do an rg adoption call for this work at this point so thanks very much okay sorry i forgot to respond okay i'm gonna say the words you should never say at a itf meeting but um i have a question but i haven't read the draft recently um so what is it that the uh could you put the the picture of the of the encoding up again because i didn't quite digest that um okay so it's a 12-bit next hop label and the idea is that it's completely up to the next hop what the what to put in there it can be a random uh correct okay correct it just gets recorded in the fib as an identifier for that face in that fib entry okay and there's a link and i mean it could be at the at the as level or it could be at the node level or or whatever right or are you even the the intent is that uh since you want to be able to use this for diagnostic purposes um as much as possible the intent is that it would be it would be every hop would be identified okay thank you oh and uh please read the draft we're interested in your comments i will other questions"
  },
  {
    "startTime": "01:28:06",
    "text": "yeah so i mean um we think this is a quite a powerful tool um that gives consumers um many ways to influence the way they want to work with the network and um it's designed in a way that is using soft state and so on so it's something that other protocols cannot really do and so that's why i think it's it's a really nice um also illustration of how icn could work or could be leveraged yeah i mean this is way less messy than doing similar things in the ip world because of the symmetric routing properties of our icn protocols right okay should we move on okay so i'm going to the next one so one of the reasons we put up past steering on the agenda was actually that um so when dave and i were discussing um how to redesign reflexive forwarding parsley was one of the candidates but in the end we decided not to use it so this is about reflexive forwarding for ccnax and ndn and um so let me just talk a bit about um the motivation so um i mean there are many demonstrated scenarios where you know i see an interest data is just fine very useful so like admins you know data science environments but also iot and multimedia streaming and so on"
  },
  {
    "startTime": "01:30:02",
    "text": "but there are also other scenarios where it's not quite sufficient so when you would think about how would we for example do web over icn so something like restful communication um so i'm going to explain why that's a problem or things like remote method invocation or like maybe phoning home scenarios in the iot world or peer state synchronization so often in this scenario you need something like the ability to push data uh to somewhere and um or you want to have it like it's like a sequence of um say um interactions with like restful semantics and so you need to establish some state and pass some parameters for every request and so on and so these are of course relevant uh use cases and so i mean in the past research has tried to somehow um you know realize them sometimes with say maybe if i can say so maybe like some some hacky approaches and so the goal for our work here was to enable these scenarios in a way that doesn't completely contradict say all the icn paradigms that we enjoy so much so um having no source addresses um flow balance and so on so we we think that so this scheme here could be a foundation for these scenarios and possibly others as well and so restful icn i think that's something that would be needed in say anything that has to do with web in the future for example so what's the problem uh for for these kinds of uh interactions so take web for example uh when you when you do a restful communication you um set up a connection to some server you often transfer many input parameters um authentication authorization some"
  },
  {
    "startTime": "01:32:01",
    "text": "kind of tokens and so on and quite often you you do this in with every request so something like cookies and so on and so if you look into how like web requests look like today um so um the the header fields and sometimes also the the size of the body um is really quite large and um so you wouldn't put this into an interest if you can avoid it so that wouldn't really make sense and another example is remote method indication so for distributed computing another use case where i see n gets more relevant so you actually think about how would i include my authentication information how would i include like potentially large data sets that are needed for some kind of computation on the server side and so on and then yeah you you so the way that you would typically do this is enable some multi-way handshakes so for example for rmi you would maybe like want to fetch the arguments somehow if you want to do it in an icn compatible way and maybe then perform some authorization and um then other scenarios where you could um uh utilize multi-way handshakes when you do iot and you have like phone home scenarios so sometimes you like have something to tell to your home base um but um yeah you you don't want um like the say we say cloud-based server to pull you all the time but you want to notify the server and then you want to serve so to fetch the data from you so there you would utilize multi-way handshakes or for any type of peer state synchronization where you need at least three handwag handshakes to do this reliably"
  },
  {
    "startTime": "01:34:02",
    "text": "and so the question is okay how how would what we deal with all these scenarios in icn and um okay we have seen some say uh proposals where people just stuff in additional parameters into interest messages why is this not a good idea well when we want to like employ congestion control based on say the data object size and rate it could be detrimental to um you know just send uncontrolled um interest was very huge message sizes also um well if you think about these rmi scenario so like client server communication if you just you know push a lot of data to the server and that the server needs to analyze to make a decision whether this is a valid request or not well this opens the door for say well-known computational overload attacks and you wouldn't want to do this you want to give the server say more control in what he wants to accept and and what not also of course if you think about pending interest on forwarders that would mean extra state and if interest are too big that they get fragmented that's also highly undesirable and okay you could say if you want to do multi handshakes okay let's just you know include some kind of icn name into the interest that the prusa can then use to query additional data so the issue with that is um so suddenly you would require consumers to reveal something like a source address or a source name which um well normally we wouldn't have to do in icn and i think we it's quite a desirable feature for many reasons so you could say maybe you do anonymity but"
  },
  {
    "startTime": "01:36:00",
    "text": "also uh consumer mobility uh becomes more difficult right when and when we don't have source addresses it's very easy to move the consumer but like having a routable address that our name that the server needs to know makes this way harder and so it also uh would probably result in quite complicated state machines if you have to deal with like multiple exchanges in parallel uh and so on and so which reflects the forwarding we try to overcome these issues and basically make it possible for a producer or a server to to ask for additional data that it needs to perform some transaction perhaps um but we don't want to do it in a way that the suddenly the server has to know some kind of routable prefix or like stable name for the for the consumer and um so what we want to do is basically allow the server to send a what we call reflexive interest back to the consumer leveraging the state in the forwarding system that an initial interest created so you send an interest to a server somehow this allows the server to implicitly get back to you but it doesn't need to have a um routable and stable source address for the for the consumer and also we want to couple the state that is needed for these say interim interests data exchanges in the opposite direction to the overall state of the general interest data exchange and yeah this is how it maybe it's"
  },
  {
    "startTime": "01:38:00",
    "text": "better to explain it with a picture um this is um how you could ex explain the general operation so you a consumer would send an initial interest and um so this has the like a usual the usual icn name to identify the say named object on a server but has additional information um so a reflexive name prefix that we then use to get back to the consumer so let's just assume that this interest arrives at the producer and so it enables the producer to maybe check some initial parameters and then decide um whether it wants to continue with this interaction and fetch additional data and this could be several interactions not only one and then make maybe step boys decisions how to move on what else is needed and so on so rnp is this reflexive name prefix that we want to communicate and then so once the say reflexive data has arrived this would enable the producer to kind of com do the computation or complete the transaction and then uh return a regular data object d1 here in in the end and i see thomas in the queue yeah please go ahead so if i look if hello if i look at this picture then i would expect that any consumer is able to send to a single producer such a such a request with a reflexive name prefix which means it looks for me to me like a very very nice dos attack vector isn't it so if you have a if you have a botnet you send uh"
  },
  {
    "startTime": "01:40:00",
    "text": "for many places you send this rnp x1 and r and x1 is actually your your victim and then your your um yeah it's pretty well in the game that's something that you uh we have to avoid of course um so maybe let me continue with the explanation and then i think it becomes clear um so in the previous version that we talked about some meetings back the system worked in a way that we actually expected forwarders to install like a fip entry in a say separate database that would then later allow them to forward these reflexive interest back to the consumer and so this kind of works you can do it this way and so it's the the say forwarding information here is still bound to this um say outer interest data exchange and so when for example um the like d1 data object gets sent back then the forwarders would also remove the the this uh reflexive fib information um but of course it has the disadvantage that you have to maintain this extra data structure and manipulate um the the forward information base uh and so on and that did so we we thought about okay how could we maybe make this um a bit more elegant um a bit more if it's maybe easier to implement as well and so the current version was actually inspired by the pit token approach that the high speed ndn dpdk forwarder used"
  },
  {
    "startTime": "01:42:01",
    "text": "this was published in a paper at icn 2020 and this is the system that edmund talked about earlier so in um high speed four borders um you have additional uh interesting challenges um so you often have um charlotte pits because you have multi multi-core forwarding and you want to give the forwarders um an efficient way um to map and an incoming data object to the um like to the correct picked instance in a charted system and um so we thought um okay pit tokens which has been used for that um is probably a useful feature anyway so for high-speed forwarding it's it's needed for these multi-core sharded pit systems and let's leverage that and so what we have done here is um so we defined like two um two tokens so one in the forward direction that we say use in the initial interest we call forward direction pit token fpt and then another one for the reverse direction that is then um yeah kind of leveraging the this uh pit state um that gets established uh in the in the forwarding direction first so maybe uh let's look at the picture um as well so here um we have um the consumer sending the i1 interest so with a regular prefix and also with a reflexive name prefix so that's a prefix that um the consumer chooses and um so it was like um like like significant uniqueness"
  },
  {
    "startTime": "01:44:01",
    "text": "and and so when a forwarder gets a interest was a reflexive name prefix it's supposed to create this forward pit token and install pit state for that so we we we have our regular pit uh entry uh maybe with an additional um good additional field i mean the different ways to really implement this and um so this contains the information about the reflective name prefix and then when we get the i2 in interest later we can then use the the same token id um to locate the the pit entry and then uh make a decision where to like forward the interest to so that means the the producer is receiving these interests and is then kind of mirroring this forward pit token into this reverse pit token field and then sends the reflexive interest back to the original consumer and then the the forwarders would be able to just use this pit look up and and make the decision um where to forward the interest to thomas um this looks to me as if you assume that the forward is on your on your reflexive way back so this is with interest two are actually have seen interest one absolutely but this is absolutely this isn't obvious to me because i mean if you if you're forward from from the producer interest one using the prefix x1 then you're using the flips in between and the hips are forward"
  },
  {
    "startTime": "01:46:00",
    "text": "directing and they are not they need not be the same i mean they they need to be the mirror of the other fips on the on the on the on the way uh uh to the producer so it's so metric but that's what i'm saying right on the reflexive interest going back sorry say again dave that's why we don't use the fib on the reflex of interest going back we map directly onto the pit entry of the original interest okay so the incoming face to forward the reflex of interest yeah so this absolutely uh relies on symmetric forwarding and if you don't have it then this wouldn't work ken but now so now you are using the pit to forward interests is that right it's the same pit or it's not a sep or it's a different i'm trying to understand the exact difference between this and what you had before in ripe uh rice right so previously um we we actually had to install say routing information in the in the forwarders and here i mean the different ways to implement it but um here basically um the forwarders would see that there is this rpt field and then this would enable them to to look up um the token uh in in their pit okay so so there's a different processing path when an interest arrives it has one of these uh forwarding path tokens right so um of course this would also require a modified forwarder behavior but we think these changes are more benign than you know"
  },
  {
    "startTime": "01:48:01",
    "text": "manipulating the rotting state and maintaining separate tables for that right i guess i'm just reacting to the to calling it a pit entry because it's it's really forwarding an interest it's not a pending uh in some sense it is a pending interest but is it i mean you see this as the the same data structure is that right um well so this i2 interest here that goes back to the consumer um i mean that's first of all it's a regular interest um like um except that the [Music] the name that we are asking for um you know is this re reflexive name prefix that we received in in the initial i1 interest and so if it's like mechanically it's it's a regular interest it just has an additional field rpt field and so this enables the forwarder to determine okay where do i have to send this to um but um we don't need to yeah disclose any um say globally routable name or anything so this is just like the rnp is just a label essentially that that the consumer generates thanks right so um so these these um pit tokens are um typically hot by hop um extensions or about features in the end or the ccnx um so one question is okay how do these names actually look like so um i didn't really explain it well but um so the idea is that you would communicate a prefix and so under this prefix um"
  },
  {
    "startTime": "01:50:00",
    "text": "the consumer could like provide different like a set of information like a cookie username or something it would be different different data objects and the draft also explains a bit more about this like you you could use manifests if you have like larger data sets and so on um but the prefix itself um well it would be a new name component in ccnx and ndn and it's essentially just a random 128 bit number so we were referring the uuid rscs how to generate it and so it's nothing that has any significance outside of this interaction and and this is also why thomas's earlier questions this this wouldn't enable you to do any kind of reflection attacks to non-related or third-party entities in the network and um yeah so again so you could use this as a prefix and then construct um like different types of of names or maybe uh just a manifest that then um refers to additional objects and so on so this could also be used for like like several interactions if you think about um say something like web interaction where you have multiple uh maybe parameters that are needed for the server to process your request and yeah so as we mentioned so there is of course new node behavior required so details are in the draft for consumers producers and and forwarders and um but so we haven't really implemented it yet but um we think that the forwarder modifications um well are actually um not as invasive"
  },
  {
    "startTime": "01:52:00",
    "text": "as as we had it for the like previous version so you you need this pit token generation when you receive this interest as the first forwarder on the pass and when you when you see an interest with this reflective name prefix and so we think this is maybe a good a good approach if you want to cater for both like high performance um forwarders and just you know standard software-based one so it's relatively easy tournament but it still wouldn't um you know screw up performance in these high-performance scenarios yeah i'm not sure we have to go through these specifications so but what we provided encodings for both cc and x um and ndn so is slightly different because they use different mechanisms and maybe let's let's talk a bit about what this could enable um so we talked about remote entertainment invocation restful and and data pull from from sensors and for rmi we had this previous work that we called rise remote messenger application for icn and so there we would um yeah also you use this system because we would also have to send some request parameters when we want to initiate um a remote method invocation and um just quickly um the way that it would be used there would be that you send your initial interest um with the reactive name prefix and um then the the server kind of fetches the input arguments um say one by one and so in this system then we would have an"
  },
  {
    "startTime": "01:54:01",
    "text": "additional interaction you would kind of return the data object here with a handle that allows you to fetch the computation reset result later so because um well this we should support long lasting computations and so on and this is a typical example how we think this would be used so often you have additional things that you need to communicate so um this um fictitious and forwarding scheme could just be one element in say a more evolved communication pattern um i have to wrap up quickly and i talked about restful already and just quickly for this iot phoning home scenario so assume you have some asynchronously generated data and you want to get it somewhere so you could say use the i1 interest to kind of notify your data sync or database or whatever and then this would trigger this reflexive interest and then fetch the actual data item okay um let's maybe jump over this um you can so this so security was kind of one of the concerns here um so um there's some extended discussion uh in the draft and what i think we want to convey is that while we think this could be say a key element for making icn fit for future web kind of systems um what is also related here is key exchange when you want to have something like encrypted communication name privacy and and these things you also have to convey parameters and you also often"
  },
  {
    "startTime": "01:56:02",
    "text": "have to convey something like a cookie that kind of maybe links to your your your session and so on so that's pop that's the scheme that we think would be useful in say many scenarios that you would maybe use this forwarding scheme to establish some state later and then when you have something like a session continuation um you could like use a cookie in the interest in the say what we call i want interest to just uh you know refer to that previous state and so different protocols need this or different different interaction styles need this so web key exchange or say secure communication and so on so that's something that could be like a general feature that we want to yeah specify uh maybe as an act as a next step okay let me wrap this up um we have time for one question yeah christian hello christine you mentioned uh the problem of client or consumer ability in the in the context of previous work do you have any plans of addressing this here because as i understand this now also doesn't work with the mobile client but maybe the client could kind of in later interests send information that would allow the producer to use the same prefix different prefixes with this with the same suffixes like establishing something like a namespace yeah so you're right i mean say if we have a moving consumer um so ins so within this uh whole interaction that would also be a problem uh of course um but um so what we would you could say here is that um so between reflexive uh say forwarding uh interactions um yeah you could still have the usual mobility because you wouldn't disclose any stable name and so"
  },
  {
    "startTime": "01:58:03",
    "text": "the prefix you um the the record effects of prefix would be generated by the consumer for for every interaction and so um yeah so there it's it's not as bad as using these um stable or globally routable names thanks for the question okay um yeah i would be really interested to get uh get more feedback please give it to us on the main list and let me bring up our chance lights again so dave do you want to talk to this i'll continue uh okay got my audio on so uh just a couple things to wrap up we're just about out of time but i just want to sort of give people a feel for um what's happening in the next period before we uh get all together again so um we did some progress on flick a few months ago and it stalled again um we really would like a lot of things are dependent on having a manifest capability in the architecture so i guess we're just sort of asking hey is there anybody out there who can potentially help us here uh to try and unstick flick and get it to our g last call there's still technical work to be done it isn't just you know dotting eyes and crossing t's here so that's number one issue um um question by ken on flick i think yeah can yeah i'm it's not a question i'm just saying uh i'm in the process of going through it again it's it's much improved but i want to be able to give substantive comments i will do that uh i"
  },
  {
    "startTime": "02:00:02",
    "text": "will bump that up in priority and try to get something to the list soon that would be great thank you um we're going to take ping and traceroute irsg reviews since last call successfully closed and we have an updated spec um dirk is going to do an rg last adoption call on path steering uh and please give your feedback on on the mailing list as to whether this is appropriate um and then two quick things um are our sort of new work coming in is at a level lower than we would like our engagement has obviously been hurt by all the covet situation and a variety of things but we are not running out of capacity to deal with interesting new research work that people would like to bring to the group for discussion and potentially um work among the participants in icnrg so please think about bringing your work in we may have some interesting possibilities for doing joint work with the computing in the network group since icn protocols do actually do a lot of computing and intermediaries because of the complexities of forwarding and potentially some of the work we're doing on distributed computing fits um in icn as well as with that and a general question is we will plan the meet at itf 114 in philadelphia in july but uh if we have enough stuff going on uh we're interested in your views as to whether we should have another interim meeting uh between now and uh late july so that's it i'm done uh i think dirk and i both thank everybody for for your time uh we hope to have this face-to-face in philly um and uh please remember we have a annual icn conference coming up in september paper registration deadline is"
  },
  {
    "startTime": "02:02:02",
    "text": "a couple months away so you have still time to work on your papers um it's our premier research venue for work in the area so keeping that vibrant and interesting uh is of course interesting to this whole community so thanks thanks everybody thank you and yeah see you on the main list hopefully um check out the discussion on uh low latency video distribution i think it's quite interesting what's going on there at the moment great thanks again everybody bye okay and a special thanks to our on-site chair matthias and our notetaker mario say bye is"
  },
  {
    "startTime": "02:04:01",
    "text": "doing that again"
  }
]
