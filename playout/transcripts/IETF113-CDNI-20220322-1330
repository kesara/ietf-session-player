[
  {
    "startTime": "00:00:07",
    "text": "hello hi kevin hello hello the room is all yours yes pick your own seat all right i think we are pretty much on time here it's good to see folks in the room but uh thanks everyone for coming uh this is iatf 113 the cdni session um uh we'll go ahead and get started uh sanji next slide please all right this is the notewell everyone should be familiar with it your participation is governed by the the uh documents listed here um so you agree by participating to uh follow all of these guidelines i'm sure everyone's seen it and read it next slide please um this is a hybrid meeting so just to some reminders for folks um there's some folks in the room some uh joining remotely um some tips on keeping connecting and making sure that you are in the blue sheets and when you're not presenting please turn off your video and audio just to help us save on the bandwidth um next slide please we will be using meat echo for all of the um queuing even if you're in the room so make sure that you are on the tool and you're logged in uh next slide please uh for remote participants you are here"
  },
  {
    "startTime": "00:02:01",
    "text": "with us on echo so um hopefully you know how to use the tool use the queue use your video and audio blue sheets are being tracked automatically through meet echo so no need to sign in if you're on site but you do need to sign in to meet echo next slide please uh sanjay go for it this is the most important slide you see the only reason if you're in in austria you've got to visit the state opera and i was lucky to do that on sunday evening so i thought i'll just share that with you nice very nice sanjay okay back to business um this is cd and i uh do we have a volunteer to be a jabra scribe i'm going to take minutes um and everyone please log in to me that go to make sure that we have a accurate account for blue sheets anyone in the room to volunteer for jabber scribe i think francesca is saying she'll she'll do it thank you i don't think there was much there was much action so okay thank you next slide please um this is just a reminder of our milestones last time we did re-adopt the triggers and we adopted the footprints so there's a new milestone there for the for the footprint draft we'll be talking about that i'm still on our list is finishing up uri signing and phil's going to give us an update on that i don't see phil online yet but hopefully he will he'll be here um acme star that's the https delegation uh fred is not feeling well so sanjay's going to take that update for us and then uh nier is here to talk about the the triggers and the footprints"
  },
  {
    "startTime": "00:04:02",
    "text": "and then we have unpacked agenda beyond that next slide please um chris lemmons can talk to us about the cta wave stuff um we also have uh the other half of the https delegation that we split out last time um so kristoff is going to talk about that and then we'll have updates on the metadata and capacity advertisements from alfonso and andrew and hopefully we'll get all that in in the next two hours and do a quick wrap-up and i 30 seconds under my five minutes so let's go ahead and get started is phil here i don't see him online all right phil will be here um let's uh do we want to just go straight to chris or do we want to let near go i'd like to keep the cta wave and uri signing pieces together so um near do you want to go ahead and go two why not awesome thank you hold on hold on a second let's see if i can bring the next slides so nearly going with your slides first the cdna footprint extensions yes it's not showing up okay would you like me to show me the screen that i can also share screen if it's possible yeah you can uh but i should be able to bring this up here"
  },
  {
    "startTime": "00:06:03",
    "text": "okay it does but it doesn't bring in the okay let's see stop share slide okay this should work yeah okay so let's start uh hi everybody i'm neil sofer from quilt in the previous meeting and the meeting with we discussed uh an internet of additional footwind types and we like to would like to get to the let's call the volume group let's call for this internet draft um do you move the slides or yeah i'm i should have hmm why don't you go ahead and um looks like i not letting me okay here sharing an idiot [Music] approval i need some one of you to approve my request it looks like phil just joined do we want to go ahead and let phil give his update um is your screen working yes okay"
  },
  {
    "startTime": "00:08:05",
    "text": "do you see my screen no so i might have permission issues yeah i gave the permission no it seems like mech i need to approve it somewhat so um i'll try loading this slide let's let phil go ahead since he's here now um and we'll go back to our regularly scheduled agenda cool phil you're up we cannot hear you phil immediately lots of technical difficulties today i don't see you here muted phil see if on your end if you can reconnect right i it just shows i don't see any video i mean yeah i don't see the option for audio there"
  },
  {
    "startTime": "00:10:00",
    "text": "okay phil you have to do the unmute your audio if i'm i guess that's probably the obvious thing you may have already done that can't hear you yeah did you do that allow your browser and all that when you first started you shaking head you did yeah still no voice phil okay maybe he's reconnecting yes near do you want to go ahead i have your slides up go okay so a quick recap and rfc806 define the footprint types and the object protein types uh ipv4 or ipv6 idioms asn country code the footing object is actually a an object that allows to define uh a bit of phototyping the values that the client can match to thank you uh and rfc8008 allows to use this footment on a footmate object uh to specify the k on which a footprint each capability uh is supported"
  },
  {
    "startTime": "00:12:01",
    "text": "so in the previous meetings in the previous meeting we suggested two new footman types expressed the first is the a is a 31131662 a code which is a the subdivision code for example states in the us and which this footing type publicly gives us a better granularity compared to country code next and the second and second fifty type was the footprint union that allows the that introduced a did additive semantics between different footprint types and that's it the internet draft with several comments and thank you kevin for start the shepherd review on the on the draft and there's no more action items i'm aware of so and and nothing new to to bring up here if you this slides actually point to the to the left draft why would they be happy if we can proceed to the less goal of this uh thank you yeah i think this one's pretty straightforward uh it's just registering two new um footprint types in the iona registry we've talked about it for a little while now it's i don't think there's been any contention about it i went ahead and did my shepard pre-review as is our custom um i think once assuming that all those were addressed we're probably in pretty good shape i don't know if anyone has any objections to us moving this forward to a working group last call"
  },
  {
    "startTime": "00:14:00",
    "text": "if so please speak now we will of course take it to the list after the meeting but we want to have a quick discussion here to make sure everyone i think there's pretty much agreement that we should just move this thing forward uh andrew supports it good um all right so if there's nothing else on this one um phil are you back to having audio looks like i see the icon for audio and video no no i can't hear you gonna try one more thing maybe change your browser all right then we'll go ahead and load up near's other presentation how's that sound since he's already here okay and the second draft we discussed and the previous time was uh the second edition for the controller interface and a quick reminder the controller interface allows an absolute uh to manage and content and met is content or metadata within the dance incident operations like proposition or invalidation uh we suggested a new uh version for this rfc uh early sanjay and myself uh next in this new version and we already discussed this added functionality which"
  },
  {
    "startTime": "00:16:02",
    "text": "is additional information in the air propagation a triggers extensibility uh for example time policy that allows us uh allows the absence of the end to indicate that he wants the a preposition be done at a specific time and also define two additional content selection methods which are the content regresses and content playlists next and following the discussion which we reached the point with we redefine the trigger object so this is the original version of the trigger object it has the trigger type for example preposition it has the cdn path the list of cdns that actually instruct the operation and it allows us to do some circuit breaking and a set of properties that are publicly used for target selection okay the trigger should be applied on method on des metadata urls or those content patterns etc etc next so the trigger v2 object first of all edit was added with the trigger extension list which is a list of generic items that we can register more and more item as we go items as we go and in next and was also added with two properties for project selection for content selection which is the contact context excels and playlists uh next please thanks er so"
  },
  {
    "startTime": "00:18:02",
    "text": "the point we got to is okay we ask what what could happen if we want another method for content selection uh and then is it a separate property we will need to define the trigger of which we and so we said okay why not do the same trick use the list of generic objects for the metadata and content selection as well so next slide so what we practically want to do in this change is to take all the green properties that are used for target selection to say on what on which elements the trigger should be it should operate and put it in a list of a generic trigger specs okay any questions so far two three that's good next okay so we suggest a generic spec of the object which is specified targets to execute the trigger on okay it's the generic object we may have several types for example we already defined the types matching they're all the types from arc 807 which are url spec or uri patterns as well as content playlists okay and in the future we may add more and more types and the spec is a sp is specified for a specific trigger subject we define a new term which is called the trigger subject which is either metadata or content and maybe in the future where we'll have something else i have nothing in mind but it can be done"
  },
  {
    "startTime": "00:20:00",
    "text": "next so if we look at the spec object structure it is the spec subject the type of the spec and their values so this specific example it says a match metadata with the following url okay we have metadata that that is chosen you are using url spec with this specific url do i have a pointer here somehow okay never mind uh okay a specific that match the specific url questions we'll see an example in a minute okay one last change before uh i'm showing the example next please the in the original rfc uh there was a temp trigger type which which is preposition or invalidation or purge it's actually specified the operation the trigger stands for and i find it more appropriate to call it action okay so i would like to have a win to rename this field and to call to call it a action instead of a type okay so let's look at the example yes so this is the trigger v2 object first of all it says okay the action is preposition and we would like to preposition uh this following list we would like to preposition metadata with this single url as well as content"
  },
  {
    "startTime": "00:22:00",
    "text": "with that matches those two urls okay and so previously this would have been two separate trigger requests right you know there's it was a single request there is a metadata urls and metadata and content url so uh it was it was still in a single request uh it was just you you had to define the properties over and over again so if we have uh you need to define metadata url property and content url property and and change the trigger version is every time you define it so i see a queue here i see a question from rajiv rajiv you're going to go ahead hi guys uh hope you can hear me uh yes yes i have a question here you're talking about uh metadata and content pre-positioning uh typically when we do a content pre-position we are also pre-positioning the metadata with it but in this particular case you know you have a case where you're pre-positioning metadata for a parent part and content for file console that's the url type have any ability to do recursive or wildcard or is that only going to be on the specific url i'm sorry i i actually didn't the line was a bit disrupted so i wasn't able to hear properly so so i'm i'm just wondering uh when we are doing a content pre-position we are typically doing both the content and the metadata right the metadata when we do a metadata"
  },
  {
    "startTime": "00:24:01",
    "text": "pre-position we are only doing the metadata and not the content itself but in this particular example you have used a parent part for the metadata and uh child paths for the content so i just wanted to know how that interaction works and whether when we are doing metadata you know does the url spec trigger type have the ability to do something recursive or is it implicit or is it explicit in that it will only do the specific listed urls and you know nothing below that um actually i i would actually need to get back to you with it with this uh [Music] for i think that if for metadata if you ask for abc you'll get the metadata for abc but that will also have pointers to everything that's below it and it may be implementation dependent on whether you choose to pull all the rest at that particular time okay so so you're saying that it's quite dependent on the exact metadata definition of that particular object right because yeah so if there are pointers in the metadata would this uh the downstream cdn be expected to recurse through that and you know get all the metadata for the objects that it's pointing to that's a good question again i don't know that we specify that i don't know that we we prescribe yeah so i i i suspect probably specifying that either explicitly make it a requirement or you know if you want to keep the behavior identical between the metadata subject and the content subject we clearly say that any any recursion has"
  },
  {
    "startTime": "00:26:01",
    "text": "to be explicitly requested by adding the tile urls the downstream cdn wouldn't implicitly recurse the reason i'm bringing up this question is because of this specific example where we have metadata and uh content preposition happening on you know different levels within the hierarchy yeah i would think it would be odd for the downstream to pull content for which it did not have metadata now whether there should be an implicit pull of metadata based off the content preposition is an interesting question all right again it it's probably something for discussion uh on the list maybe yeah i i admit i i failed to follow the the details of this this remark um but as i understand it's not related to the structure of the the trigger or it's related to the specific example uh yeah example and it's it's kind of related to the expected behavior of the downstream cdn when dealing with uh you know a pre-position request yeah but it's a a similar issue would have could have existed with the previous version of the rfc with the 807 where you already can define c8 metadata urls and quantity urls together it's not a new it's it's probably a use case that becomes more visible because of the new structure you're using right yes because of the way it's kind of separated now it may it forces us to think this way and it's it's more visible now it probably wasn't explicitly visible in the earlier structure yes and you should somehow relate to that rajiv can you also post your question in the mailing list i think just so that we can make sure that i will i will do that"
  },
  {
    "startTime": "00:28:01",
    "text": "we're separating the structure part of it from the actual implementation piece of it okay okay thanks any other question comment okay so let's proceed i wait let's do the example sorry well so i want you to okay if you keep this example in mind okay we will relate to it later on we have here a trigger of people for preposition of a metadata with a single url and content with two different urls okay let's move on let's move on okay so similarly to the trigger v2 and we needed a object we needed to redefine the error object actually practically defined a lv2 object without all properties related to content selection to metadata like metadata urls et cetera and we added a generic twig spec list and we took when we defined the status v2 object which holds the arrow v2 object as well so if we go to the next example this is an example for an aov2 object which in this case says that the air of type project that we encountered an error of type project and it's important to note that the error is a relevant is related to the lead sorry yeah the error the error is related is relating the spec listed here okay so it there the only the spec"
  },
  {
    "startTime": "00:30:03",
    "text": "relevant to the error are listed in the spec list okay so if you remember the example we showed before of the trigger we had a a metadata aspect and a content spec here we have only the content spec so the metadata we had no no error for the metadata and if you remember the url list in the twitter add two urls and here we have only a single url because the error only relates to this url the other with the other url everything went fine okay so this is the semantic of the error it's similar it is similar to the semantics of the l the original error with the previous structure and one point however i find a bit of an issue here is it seems like a bit of a of a a bit of a hassle to pass this arrow by the absence of the end okay to understand which object exactly uh was no they relates to it's it's maybe some swing comparison or something like that and i would like to suggest next a a bit of an adjustment to the structure okay so if we add some placeholders in the list it may allow us to uh to to keep the a position or to to take the trigger and the spec list from the trigger and uh add placeholders for saying okay for this object everything went ahead is not relating to everything went well so in this example they uh"
  },
  {
    "startTime": "00:32:00",
    "text": "there is a spec list and the first video is an empty spec which we respectfully means okay they met the data spec we had back then everything was okay with it there is nothing it does not relate to this aspect as well as if you if you are going into the uh spec itself that the failing specs there is a again an empty object in the url list meaning okay this url was okay the o does not apply to it and this may help the and better understand the error so this is just a suggestion the uh does not contain this change already and i would be happy to your opinion about it so we have uh q here uh rajiv you want to go first and then kevin uh comments on this one obviously is the fact that you're using your first example in the previous delay uh you know there is the two levels of ambiguity which makes men one obviously that you've already identified the fact that uh you know you are only listing those specific effects where you have an error and uh so in this case you're nesting at the level of spec for showing the specs that have an error but what you're actually doing is you're actually populating it with a partial spike when compared to the preposition request because the request spec had two urls and here you have a single url so that is one question the second one is you know how do you expect uh you know this error to be dealt with in cases where the error has happened at a url that may not be part of the original"
  },
  {
    "startTime": "00:34:01",
    "text": "stick so like uh so this obviously would not work with the spec type of urls but say there's a spec type of playlist which you have added okay so part of that would need would mean that downstream cdn passes that playlist and brings all the uh dependent resources of that playlist into uh you know the pre-position queue and populates them and say for example on a playlist there's a subtitle file which is giving me a 404. the rest of the cop the playlist is fine i'm getting a 200 on the playlist i'm getting at 200 on the av segments i got a 404 on the subtitle segment now that subtitle url is something that i where would i put that in in this structure because that is something which is not explicitly mentioned in my incoming request right so so we'd have to be a little more uh you know intelligent in how we write this structure in order to allow for these kinds of errors to also be surfaced and if we come to the next slide there my uh you know point would basically be that uh you already have a status object right is there any reason why we are using an explicit error object rather than just rolling the errors into a generic status object and that generic status object is designed to exactly mirror the pre-position requests [Laughter] mapping between everything what succeeded what failed without us"
  },
  {
    "startTime": "00:36:00",
    "text": "having to you know put in hacks like you know trying to maintain position or without necessarily having the upstream have to maintain a lot of state about what it sent so that it can match up when the response comes in i've just a couple of two thoughts over there and so i again wasn't able to hear you well i heard what i heard was the first element at the first part of the uh what you were saying i'm sorry near your voice is breaking up badly is it just me or yeah i think maybe uh rajiv if you can turn your video off and see if that helps if you can turn your video off okay because i was able to hear your uh the first issue you presented and i think it's something we need to think of the ability to say okay this playlist not only this the the entire playlist fail but only part of it uh and it's adds a significant complexity in the for the definition of this api i believe uh and need i think it we need to think about it um and the second i i was not able to you were breaking out from at my place at least what rajiv is saying is that instead of using why don't we create a generic object instead of an error object but i think that's that really is part of these the rfc already the the error and it's not a generic object in that sense so i'm not quite sure if if i got his full question right i was basically trying to say that instead of having a separate error object which only tries to communicate the uh specific errors why don't we roll the functionality of"
  },
  {
    "startTime": "00:38:00",
    "text": "reporting the error into a generic status object so the status object is designed to as closely as possible mirror the structure of the incoming preposition object the request so that everything every spec that's in the preposition object is available in the status object and each spec in each component of that spec has either a status okay or or a status error and if there's a status error we have you know sub fields and sub objects inside that more you know granularly describe what error was encountered the point the advantage of doing it that way is that the upstream cdn doesn't really need to maintain a lot of state and especially because some of these transactions may be quite long-lived transactions so if if a upstream cdn is saying hey here's a set of ten episodes playlist that i want you to pre pre-warm or pre-position okay it it may take 20 30 minutes for the downstream cdn to complete that operation before it can send the status back saying that hey this particular trigger has now been completed right so do you really want the upstream cd and trying to hold on to state for that long so that it can match up with an error message or if the complete status message comes back it basically mirrors everything that was in the original pre-position request so the upstream cdn doesn't have to have a record of what it said the status message is fully uh self-contained yeah first of all the status object itself also holds the trigger uh the trigger v2 object okay also yeah exactly it's already it's the the trigger object only the part of the status object it's it resides in the object and the two appears both of them appears in the start in the status object"
  },
  {
    "startTime": "00:40:00",
    "text": "uh so this is covered what is not covered you you suggested also that you would we would have an arrow at the in a different object per spec so for this that we have the status of this block is like that and the arrow for this package and like this is the error of this path and this is the arrows of this deck and i believe that in we can achieve that particularly with if if the opposition wants to achieve that it can actually break the request to several twitters so it may have a trigger for the fair spec and a separate trigger for the second spec and that's it so yeah so so my point here is even with a single spec you know that happens if it's a single playlist um it may be it may be a playlist say for example a movie which has a quality ladder with six different qualities so which means that's still going to take some amount of time for the downstream cdm to process before it can respond so that's still a certain amount of state that the upstream cdn needs to maintain to be able to correlate the errors so by joining the error object as a part of the status uh in the state testing as a part of the trigger object itself you know uh basically the response from the downstream cdn is the trigger object with it is embedded inside it so the upstream does not have to depend on any state the status response has everything in it yes how does this advert i think we should take it offline yeah i'm going to inject myself as chair and say that um rajiv i think it's a good discussion if you have actually uh a proposal for for an object you know that might also help move the thing forward but if we can take this to the list and have that discussion that would be great um we're running a little behind schedule"
  },
  {
    "startTime": "00:42:00",
    "text": "so um near we could we could try and push through the rest of this thank you everyone okay let's uh proceed and skip let's keep this one next yeah thanks and the last important thing i want to discuss here is the the photon capability objects relates to this control interface and so the downstream sedan may support some of the action it may support pre-position and invalidation but not support period and but it may support those actions but for different subjects for example pre-position in in validation and purge are supported for metadata but only for content only preposition is supported so for example and we would like to be able to state that and we would also like to say for each such action which extensions are supported for example a a pre-position of content support time policy but purge does not support time policy and also what is the target selection methods that are allowed okay so for metadata for people's version of metadata you can you may use playlist but for purge you cannot use playlist why i don't know okay but next okay so what we try to do is define a structure that says that for a specific action on a specific subject the list of support supported specs are this list and the list of supported"
  },
  {
    "startTime": "00:44:00",
    "text": "supporters extensions is another list and of course everything is subject to the footprint that the capability is specified to so let's look at the object and an example okay so the the capability is built from a list of objects each object states the action the subject the supported specs and the supported extensions so in in this example uh we support only preposition of content and invalidation of content okay no operation of metadata is available no purge is available okay and the specific preposition of content can be done using your specs or ccid specs and it supports time policy however invalidation does not support a time policy it doesn't support any extension because the extensions list is empty or not appear it doesn't appear um and then that's it and do you have any questions for this i think there's been good discussion here i think everyone please go and read the draft and we'll have we'll continue the discussion on the list i think this is good there have been a lot of um good updates to triggers and i think we're making a lot of progress on making it better so i'm excited by the discussion if there's no other questions thank you and we'll give phil another shot at this and did you have anything else thanks"
  },
  {
    "startTime": "00:46:03",
    "text": "kevin you want me to see if i can bring the slides up in the meantime while we get um phil still get back on uh phil still there new browser and i don't see the mic icon what i see is that your mic is muted oh maybe we we come back at the end of the of the representations so you want to go ahead and do frederick's um yeah so you probably have to drive that yep i'll just stand up here https delegation where is it we go okay so i'm just voicing um fred here he's he could not make the um the meeting today as he was not feeling"
  },
  {
    "startTime": "00:48:00",
    "text": "well um so the the revision of the draft that uh that is put out uh in the mailing list and also in a in the really the the main change in this version eight really is that it has been pretty much cleaned up and has been very simple very much simplified so what we really see is that this document is now aligned with rfc 9115 and so basically what 9115 prescribes is a pre cdn conversation that can happen between a downstream cdn and upstream cdn where the downstream will create an account into this entity and it will exchange the certificate information etc and all of that so once all of that is done really the the only um only part that is required in this rfc is to be able to identify that um what is the location from where um the downstream cdn will go and fetch the certificate information and all of the conversation has already happened before it so so there's no uh exchange of information other than um that the the https um draft this draft will basically essentially uh only go out and make the request to pull the certificate from the upstream cdn and that's what the the change is really for this particular draft so as you see here the example acme delegations and and then the the url that has been already identified on which it will go and retrieve the request from so that really is the change um you want to go to the next slide and then we still have to talk through about uh security and privacy the the sense is that since this draft"
  },
  {
    "startTime": "00:50:02",
    "text": "is not adding any other exposing any other metadata so there's really not any additional security questions that come into play here and the rfc 9115 and rfc 8006 already have addressed independently of this draft their security concerns that uh ensure that the metadata is preserved or within the rfc 9115 that there's no other security leak so i think this document may by itself may not have anything that exposes the security and privacy concerns but i think we still want to just talk through that preferably in the mailing list if you can review the draft and see if there are any questions or concerns that this draft should address with respect to security and with respect to privacy so i think what from the author's standpoint the request is that if folks can review this draft this version 8 and respond in the mailing list for any questions you have or you may have and based on that based on the progress then maybe we can ask for the working group last call for version eight i think that um you know it's it's much simplified now which is good um uh i will go ahead and do the shepard pre-review on on the new version and hopefully everyone else can also go out and review it it's it should be pretty straightforward now um to move forward with it i think we took out a lot of the the other stuff about metadata and the harder stuff so right yeah okay all right uh any anyone have any"
  },
  {
    "startTime": "00:52:02",
    "text": "thoughts or questions otherwise please review the draft i will review the draft uh officially and we'll try and move this forward by next ietf all right sounds good thank you um is kristoff around can you hear me yes okay so i'm christopher wordpeak um yes i'm going to present an update on this draft here uh on delegated credentials subsets so the start of this draft is actually the result result of a split of the previous draft that just has been presented so the previous draft is about delegated certificates and this one here is about delegated credential which is ongoing work in the tls working group and so this split has already been discussed in the in this group here in cdni and um yeah and uh yeah i would like to ask for adoption of this uh draft within the cdi cdna working group uh the next line so currently what is in the draft it's uh that two objects which are defined two mi objects one is a conf delegated credential it only contains a url and the idea is that the downstream cdn when fetch or when accessing this url can download another object and this other object is this delegated credential objects which contains the delegated credential as defined in the"
  },
  {
    "startTime": "00:54:03",
    "text": "subset internet draft and also key material so that's yeah so it's quite simple those two objects uh one which allows to just provide an url in the other which transports all the uh cryptographic material let's say so next line so there are a couple of things to do in the for this draft uh so the first one is so is about aligning those two drafts so the one on dedicated certificates and one on dedicated credential just about the advertisement so that we have an fci object which is a bit common in which would allow to announce the what is supported by the downstream cdni and it could also allow to advertise additional parameters there are sections about privacy and security that needs to be added and then but the main thing is about the there's still an issue about this delegated credential object which is not really an mi object as as uh [Music] as in the spirit of cdni so i have a slide on this in the next slide and and then there are also things like um currently in the current proposal the public private key is generated by the upstream cdn and maybe this is not something that we always want to do we should also support uh the case where the downstream cdn is generating this key and then asks for the delegated credential for to the upstream cdn okay next slide so yeah the open issue is that uh well this delegated credential objects is not really an mi object in the spirit of the rfc 8006"
  },
  {
    "startTime": "00:56:00",
    "text": "because it's uh it's not um well it's not fetched as a normal mi object this is fetched via this url which is defined in the configuration delegated credential objects but if we uh remove delegated credential from this draft well there's no point of uh of just keeping the dedicated credential of the conf delegated credential objects and so they're different object uh options on how to solve this issue i mean the issue is really about uh how can we fetch the delegated credential how the downstream cdn can fetch the delegated credentials so there are one could be to rely on an fci object which allows for the downstream cdn to announce the number of uh delegated credential it needs and then the upstream cdn pushes the delegated credential vsmi object but the problem here is that it's not really dynamic that we have to keep in mind that dedicated credentials have a very short validity times so we have to renew them regularly and it's kind of the downstream cd and each time it sees that a delegated credential is expiring it has to fetch a new one so we need a really dynamic mechanism and the other option would be to well specify a dedicated interface which allowed to fetch delegated credentials so that either it's an interface somewhere in cdni or sva that needs to be defined maybe the trigger interface i don't know or something something else some dedicated to that or by proposing an extension so option b in the acme working group um which details how to fetch the delegated credentials so this is something which is mentioned in the subserts"
  },
  {
    "startTime": "00:58:00",
    "text": "draft but i don't know if the acme walking group would exact accept that knowing that the idea the interest of this delegated credential is that you don't need any ca anymore so it's not really in the scope of the acme protocol so and yeah i mean i wanted to really present all the options and the this open issue and have feedback maybe on the mailing list maybe today on the discussion and to see how to move forward thank you kristoff i i know i owe you a response on the mailing list as well i haven't responded that email yet um i apologize for that but uh sanjay is in the queue sanjay yeah um one quick question christoph um i think the i'm not sure that this draft will really go into the acme working group um because it really refers to the subserts which is the work done in the tls working group so i think it would be better to to sort of if you're making use of the uh subserts then keep the focus clear that it really um any reliance it has is focused solely on that um that rfc um the second point is that i i noticed that the the final version of uh that draft in tls group has been submitted for rfc though there's a slight change and i don't know if materially that matters but they're talking about that subsearch document will apply to dtls instead of the tls um it may materially not matter but i just wanted to call that out also enough not sure if you paid that attention to that part no i haven't i will have a look to to understand the the difference okay uh i'm gonna put myself in the list"
  },
  {
    "startTime": "01:00:03",
    "text": "or in the queue i think um you know i i brought up the this it's not really um a metadata thing um i still need to think about it more i don't know that it makes sense to create an interface just for this maybe we can make an exception but it is something separate i am concerned about the security parameters of we're now going to be retrieving key material and we need to be much more conscious of how we're doing that and that feels like a security area not in you know not an art problem but and we may get a lot of pushback on that if we try and specify something like that in here um we can specify the format excuse me right all you're really doing is specifying the format and not who's going to set up this server to to provide it but it's a it's a it's a fine line right now we're gonna have to say what what version of tls are you gonna use to pull this and is that good enough and um i don't know that we want to go down that road but i think everyone should take a look at the draft and if you have thoughts or comments on where we should take it please please respond to the list send your comments there and we can try and take this forward now as chair we wanted to have a discussion quickly on whether or not we can go ahead and adopt this draft this was a split out from an existing working group draft um i i don't see a problem with us taking on this work since we already had the work and and we probably should move this work forward but as a matter of protocol we wanted to um you know bring it up and say hey does is the working group still interested in going down this road i assume we are but if anyone has any um objections please feel free to voice them now we will of course go to the list and send out an actual adoption email and uh solicit feedback but"
  },
  {
    "startTime": "01:02:01",
    "text": "if there are no objections here we will go ahead and start that process all right i think we're i think we're good kristoff anything else you wanted to add well that's all on my side thank you so much thank you um we're gonna give phil another shot nope no idea okay so [Music] do we want to hear the access token or we want to wait until after if um well okay let's let yeah let's let chris go let's go ahead and let chris go and give phil some another shot it all right my audio working yes well yes fantastic and video too all right and you got my slides up excellent yes i'm going to try to move a little bit quickly through this that we have time for at least a couple of questions uh this is gonna be the common access token uh next slide yep and uh we're gonna start out i wanna talk about who's doing this uh what all we're doing and how that relates to the uri signing that is uh uh being worked on in this group and uh why this group might care next slide so this work is coming out of the cta wave project and the primary use case is for streaming media although the token itself is a little bit more general"
  },
  {
    "startTime": "01:04:02",
    "text": "that covers all of the existing usages in the industry from all of the uh uh all of the cdn specific tokens that are in use today next slide so uh talk a little bit about how this is different from the uri signing that this group is doing uh this uh the common access token is cwt based um so it's it's going to be um smaller terser and is a little bit faster to parse for cdns there are more musts for the intermediaries that receive these tokens and so the idea is to kind of draw a very very firm fence around what needs to be supported in order to claim common access token support and provide provide strong guarantees so that issuers can issue tokens knowing that any intermediary can process it obviously there's going to be no built-in support for delegation it's not part of a larger cdni or other interconnect effort it is just a token uh but it does generally have more claims with greater complex complexity next slide all right we have uh uh just like the just like the uri setting token uh it has encrypted claims in order to protect the privacy of the uh end user um but unlike it instead of putting it as a base64 string it uses the cozy object directly this saves processing time and is uh and saves a lot of space avoids repeated base64y"
  },
  {
    "startTime": "01:06:00",
    "text": "unfortunately this means you can't use the subclaim because that's a string and this needs to be a cozy object we'll get onto more of that in a minute next slide so it also has some additional claims the first few are really trying to draw a box around any request information that intermediaries or providers are likely to use in order to determine what content is uh returned so the http method is uh has a claim on it so that you can specify that if your token is valid for get but not put and not post for example and the uh alpn is actually an extension of the fact that you can adjust based on scheme and so there are tokens that claim https but not http using the regular expression on the uri we wanted to extend that to the alpn more generally and then there are a number of headers including an arbitrary number of unspecified headers that uh servers can use to return uh different differing information uh they are of course well advised to use the very header appropriately here but um yes having a regular expression on arbitrary headers um we have some geography claims um that we're still working out the details on uh we have a claim for the tls public key this allows you to use uh for the tokens something similar to what oauth does with mtls self-signed certs uh wherein you use if you use a"
  },
  {
    "startTime": "01:08:00",
    "text": "certificate even a self-signed certificate to acquire an authorization token by whatever means you provide credentials to get authorization tokens um the issuer can bind your public key to the token it gives you and the intermediary can guarantee that the certificate you presented matches the token so that if your token is stolen then it can't be used without your private key as well we have nestable compositions for and or and nor and that drastically increases the flexibility of the token and increases the language that issuers can use and lastly we have actions that modify the rejections so there are situations where when a token is rejected uh you the issuer wants a very specific status code uh and some headers to be returned and uh we're still workshopping exactly what this is going to look like but the idea is that this will allow issuers to uh define these rejections directly next slide okay another difference is that uh we have some strong types on these claims so for example the critical claim here is an array of claim numbers uh claim numbers and uh strings uh encrypted claims are of a cozy encrypt type directly network claims are going to be uh it's going to be a seabor array of rfc 9164 tags uh other types uh other claims are going to have appropriate types this will reduce the amount of parsing of strings and error handling that has to happen uh post uh well-formed validation next slide so let's talk about why we in this group might care about it or we might not"
  },
  {
    "startTime": "01:10:01",
    "text": "um so like the alpn method and headers those are just general um uh uri uh signing claims they're applicable to any sort of a uh any sort of a token like this um compositions encrypted subject critical claim those aren't even specific to um uri signing they are literally just generic uh claims um these are potentially generally useful maybe it's useful to try to define them generally in a broader sense next slide uh there's overlapping utility all uri signing tokens can be represented as cats here and any successor token is likely to use uh at least some of these i don't see a whole lot of energy for any sort of successor token um to the wonderful and delightful token that this group has that satisfies the needs but if there ever were it's likely that such a token would overlap heavily with at least some of these claims next slide so there's no real takeaway here uh this is no action items this is uh noodles for your noodle machine and some public domain cats um and i wanted to leave at least a few minutes of time for questions um about uh what this token is and what it's doing kevin so um i guess how far along are is the development of this and do you see um adoption of it and do you think that it will it will go anywhere or whether or not"
  },
  {
    "startTime": "01:12:01",
    "text": "you know uri signing token just it will it also go somewhere or will it fall by the wayside because this thing is more i guess does this add a whole bunch of stuff that people want or does it just add a whole bunch of stuff it definitely adds it it definitely adds a bunch of stuff that people want and it adds some stuff and which category which thing uh is in uh depends based on the use cases um so like there are some there are some folks with zero interest in any of the tls stuff um and there are folks that are like no no that that enables my very specific use case and so um there is there's a lot of drive towards adoption on this and that's why this work is happening in the cta um that there's a group of people there that represent a variety of intermediaries and providers that uh yeah um uh uh that have a lot of energy towards this and they're really pushing for something that will be genuinely implemented across a wide variety of intermediaries and so yeah the the goal is to codify the feature set that people are already doing so that they don't so that nobody has to give up features by switching tokens right and do you think that this is something um we should liaise with or is it just be aware if we choose to take forward uri signing if we don't ever do anything else with urs signing then they can go and do what they want yeah i i think that some sort of a liaison probably makes sense there's a lot of overlap in uh participation already which is which is a valuable thing i think there is um"
  },
  {
    "startTime": "01:14:03",
    "text": "if this uh if the cat takes off and uri signing token does not for whatever reason it may at some point be valuable to define an interface layer between the uri signing token and the cat that allows people to use the cat for uh cdna delegation right we may want a metadata at least or something to support it right right um and now if we if if we ever want to take on that work there are other questions we have to ask um and a lot of the the musts around this were intentionally left out of the uri signing draft because there was not a desire to force all of the intermediaries to implement all of these features in order to do cdni um right and and the goal here is to create uh you know genuine interoperability and may is the may is the bane of an interoperable system right so uh if we ever do that there is a there is definitely going to be some um some questions an issuer can express anything that they need to express in a cat but there are more requirements on an intermediary uh supporting a cat than there would be on supporting a uri signing token got it sanjay yeah um so chris i before you were put on the agenda i did see that there was some support in the mailing list about you know you talking about it so i'm glad you did that um and you clarified some of the questions that kevin was asking um moving forward you also mentioned about implementation so that's an important thing um so i'm i'm wondering um if you want to keep us sort of apprised"
  },
  {
    "startTime": "01:16:01",
    "text": "maybe you know in july when we meet to see you know where you are you know other companies are if there's any implementation and and you know maybe if there's anything that you made use of out of the uri signing so you know we'd like to hear more about it and see you know where you are at yeah um i can certainly i can certainly keep the group updated um and yeah there's there's no surprises um the the the structure is very similar they're coots versus jutes it's you know it's the same roots at the core there are only a few obvious ways to express these sorts of things okay thanks for the chance i just wanted to say that uh to the earlier question there is definitely very very heavy interest from content providers and the cdn in the ecosystem of content delivery to support a common scheme right now the ecosystem is extremely fragmented and cad has the benefits of flexibility with these logical claims that can be put together with ants and ours but also the conciseness of that which helps the performance which is extremely critical for the cdn vendors thank you anyone else have any comments on this phil are you back thank you thank you thanks chris say get phil can you hear me yes oh yes yes i can hear you finally it took you what hour and for 45 hour and 15 minutes meet echo thinks it has something to do with the udp session initiation but oh"
  },
  {
    "startTime": "01:18:00",
    "text": "this is amazing i can i can see all kinds of little uh wave things going on over there all right sorry um so my my thoughts on this um i love standards things that are generic that can be used by more people i think that's why we change uri signing to be jwt instead of all the query params stuff that it was in the beginning um if something like this comes along that that fills that gap better than uri signing i'm all for it i don't i think you are a signing with something that came along because there was nothing else that did it but if there's a more generic solution for it i like it and i think the um the puns are going to work out the memes are going to work out a lot better for cat than they will for uri signing excellent [Music] oh funny andrew i got it all right phil do you want to talk about your signing real quickly sure um so uh the the issues that ben had with it there was one minor clarification he wanted to see which i added and he was happy with the other thing the other point was having to do with delegated shared keys going from from going from csp to ucdn he was fine with it was from ucdn to dcdn that he did not like and i wrote to the mailing list and nobody really replied and we decided to just go ahead and and correct it so what i did was remove the the explanation of how to do it and like there were a couple places where it was like oh and you can do this here and i just removed that part but i did not remove the concept entirely because i did not want somebody to rediscover this and say oh this is a great and clever idea i've come up with and let me implement this and instead i put um that they should not do this um"
  },
  {
    "startTime": "01:20:00",
    "text": "ben is still concerned that that's in there at all um and that it's should not versus must not i guess i i'm i was torn on this i thought about this for a while before i went with should not um and mostly because i felt like we had we had no recourse like if we say you must not do this um or else what like they can do it in the background it's it's already out of band there's nothing that it's breaking there's no way to detect it or stop it so i just said you should not do it i think he wants it to be must not i'm super open to to that being a better route i just didn't know if i had a solid argument to make so it please give me the the grounds to stand on and i will gladly make that change i see francesco on the mic here so francesca hi phil um about the should not uh something that should be noted is that it's not enough to say should not and um and let the reader decide that what what level this is like how um you know required or recommended it is to not do this or it there needs to be text around um and i haven't reviewed the text so like yeah i'm not sure at the moment if there is enough text about that but it needs to be clear what the consequences are of implementing this and in in um like ideally should not means that um there are like the the writer foresee cases where this is allowed and this could be like corner cases or you know situations in which you cannot do differently than actually using this mechanism and that's what the should and should not is for it's for allowing these corner cases and describing the content context and"
  },
  {
    "startTime": "01:22:01",
    "text": "like maybe examples in which this is acceptable um and yeah this is uh we see this a lot in in isu evaluation where we considered is it must or should really and i think it applies here as well um so maybe that's that could clarify a bit uh ben's point about why um this should maybe is not enough to him um i don't know if it helps at all but like i see the point of you know not wanting it to remove it completely and then someone reinventing it um it really depends on on the actual text that is around this should not in my opinion how strong it is and how well-described it is okay yeah i i don't i don't see any like valid use cases other than people who implement are saying no i really want to use shared key and i really want to use this and and i think we've said a ton of stuff about not doing it and why not to do it there's things in the security consideration i can always add more i'm always open to that i really was on the fence somebody jokingly sent me i forget what what rc it is now but it was an april 1st rfc that had really should not and yeah and and i really i i kind of like you know really should not is what i wanted to put there but i was like oh no this is you know obviously a joke but it i was really on the fence and i just i guess maybe i'm maybe i'm just misinterpreting the should versus must more as uh plain english versus what the rfc state and so maybe maybe you must not as is the right thing and i can i can make that change i just i i discussed it with some people um some experienced ietfers and they they"
  },
  {
    "startTime": "01:24:02",
    "text": "didn't even have a definitive answer one of them said start with must and see where you go and the other one said start with should and see where it goes so i can just i can just change it to must or must not rather just to be clear yes and and if the working group has an opinion that's that's very valid like um to have this conversation and decide and have a consensus call by the chairs that is going to you know um motivate putting in their must or not master should there that would help so if i'm also like i will have to look at the text again too to have a more definitive opinion but yeah i think our original i mean the whole time it's been there are people who do this right we know that they shouldn't do this but there are people who do this and we don't want we didn't want to you know completely lock them out but and maybe it's okay to say must not and they can still go and do it there's nothing preventing them from going and doing it right um but it i guess the stance of the ietf should be to do something secure and not allow them to do something that's unsafe right like that could be um described in the document without normative language and say like we are aware that exists be like there is exist and this is not uh this is not recommender or like yeah yeah there's a lot of this is not recommended language in it already i think it's i think ben really wants me to put must not like just do not do this it is not it is outside of the specification if you do which i think is fair i think it's fair"
  },
  {
    "startTime": "01:26:01",
    "text": "any other comments francesca has one more comment i think yeah sorry so so from a process point of view um i just wanted to say that uh ben will be stepping down the isg tomorrow so his blocking discuss will disappear but i hope that we can still you know get his input and um even though it's not going to be like actually blocking the document i hope that we can still get his yeah like informal approval of some sort um but yeah we should get to a consensus in the working group and then move forward from my perspective i i definitely respect his opinion and would like to have him on board with it so i can i i can totally agree with that and he's been very helpful in finding issues with the document in general that that we fixed up so i'm okay with going must not um phil um you know i'll i'll leave it to you to decide um i'll go and review the text again everyone else please also if you have some cycles review the text again and if you have thoughts or feelings on it and we can really really try and get this one finished up and out the door once and for all thank you phil thank you for the perseverance in getting us your audio we appreciate it yeah sorry for wasting so much time but glad it finally got through thank you phil all right all right we've got we've got 30 minutes and we've got two more topics to cover here yeah"
  },
  {
    "startTime": "01:28:01",
    "text": "but i think we have enough time so um i think next alfonso goes alphonse are you good yeah i'll bring up your slides just give me a second here okay thank you you can try and fit it in 15 minutes to give andrew time that would be great yeah i know yeah i would try sorry we lost the order here so um there is capacity there it is okay thanks yeah i will try to go quick on the first part of the slide deck because it's a recap of something that we have explained oh glenn that was let's understand what the the person that was presenting this in the last meeting so please next slide yeah next please yeah so just why we present in this um this draft to the to to this group um [Music] as all of you probably know um in the streaming video alliance there is a the open catching working group where is gathering different perspective of the streaming video business where you have the content providers you have commercials at the end so you have sales providers and that they all they have the common interest of uh bringing the best quality of experience for video delivering this video services to end users um so we working on trying this effort of having the best quality of experience um this is why we work together um to create this inter try to get integrations more efficient and try to get some standardization of how the different processes of the video delivery goes from the content provider to the end users"
  },
  {
    "startTime": "01:30:00",
    "text": "all through all these different roles or techniques technologies um et cetera so please next so at the first let's see the sva open catching working group was paying attention to the cni interfaces and metadata uh definitions of the different rfcs like a80607 and 8008 um but there was a finding that it was probably needed um to cover some gaps that uh trying to put all this together for the this video service deliveries um the interfaces where metadata interfaces were not enough uh to cover all those needs from content providers from the cdns or for the service providers this is why we were working on this extending the metadata that come from this group to try to to cover all the needs from the different members of different roles in the video delivery trying to define um useful methods to make the configurations try to automate the configurations to putting this in place with our current implementations and deployments in in real life and extending with more advanced configuration publishing capabilities um that are required by content providers trying things like publishing versioning of configurations and things like that please next yeah well this is very very quick this is a representation of the of the metadata mobile model of the 806 um and yes i will be very quick uh we haven't touched any of these many structure of the metadata model but"
  },
  {
    "startTime": "01:32:00",
    "text": "we live for uh our proposed extensions in the genetic met as generic metadata objects so with taking leverage of this infinite extensibility that the interface permits on the metadata is next yeah so at the end all what you have in in the orange background are the different categories that we are adding or we are trying to add with this extension to the metadata model from the cdni we will come in in a minute for that this next so as a summary uh i think yeah we can go later on this past peace sanjay yeah one of the things that includes this draft is what we call it processing stages this is a way to permit um to intervene in the different processes that happens when the user agent is requesting a content to um a cdn or a damson cdm sorry how that process will go to an abstinence cdn origin to get the response back from the upstream and that response goes to the user ryan by the damson cdn these processes that we he identified for the steps from the client requests ordinance requests or any response on client response we defining a way to intervene on those processes so downstream cdn is able to make transformations on different parts of the of the request for instance modifying the the headers that goes to the origin when try to get the the the object that is requested or we are able we could be able to modify a header in the response to the client and and any other transformation that could be required for the different use cases that are required for this traffic delegation yes next"
  },
  {
    "startTime": "01:34:04",
    "text": "yeah well um this is a i'm going to pay time here now it's just a representation of the processing stages model all these leave in a generic metadata object that is defined defined in the in the draft so please next and this was an example of the host index representing the generic metadata mi processing stages and in this example you can see that we use an expression match as is defined in the in the draft as a metadata expression language where for a condition in this case that is the response from the origin is 200 then we will modify the cache policy for that specific object so this is an example of what we we intend to do with this next yeah and and again well there is a definition of the expression language with the different operators variables or even built-in functions that could be used um for for all these modifications this next in terms of the capacity capabilities interface um we have in the these drafts have been defined uh a few new objects that are required to the thompson cdn be able to announce to the upstream the different capabilities regarding these new extensions for the metadata model that could be required for the abstinence to know so for instance uh it's important and it could be important for the absent to know that the processing stages is available or not or even it could be important uh to ha to the amsterdam to know that a subset of the processing stages is the dams incident is capable for for instance uh it should be important to know the amps incident to know if the spectral language is supported in the"
  },
  {
    "startTime": "01:36:01",
    "text": "dumps incident or not maybe it's not but maybe is able to do some processing stages replacement without expression language so um there are some conditions here that the fci interface could not only announce the capability of a complete object but of a part of an object applicant yeah [Music] yeah i will not stop here this is something that will come probably next so it's not changing in this draft version regarding this but we just for you to know the sva and the opencasting working group is working on try to extend the metadata model with new features regarding an advanced publishing method that will meet the content provider a better management of the configurations um so probably this is something that will come in in the following meetings please next i'm going to insert myself in the queue here yeah um so i i see that you have uploaded the um the new revision of the draft right yep so based on and i think you have addressed some of the comments that were posted from the from the last meeting in the draft i see that yeah okay okay next please yeah so well that we are version two what's changed from version one to version two one of the comments that came from the last meeting is that uh they could be good to have a categorization of the different generic metadata objects that we are including with this extension uh that was a very fair point um so we we have made this categorization uh we have separated different metadata objects"
  },
  {
    "startTime": "01:38:00",
    "text": "what these sections like cash control or metadata related could catch properties management here's the list of the different objects in this category the origin access metadata related to the acquisition of the content client access control in this version of the draft there is no generic metadata object uh yet but we did because this is something that we in the open catching working group are working on different options different objects for this client access control uh but they're not finished yet so we haven't included in the draft uh but i think it would we think we were good to having in the in the as a category here even and is empty now but we we will have different objects in the next version of the draft edge control genetic metadata to inform processing of responses downstream so it's a way to to the abstinence to condition uh the way the thompson cdn could respond to a reversal agent request in the edge uh so this could include something like the i will see we will see that in a moment for the crossover in policy for instance there were some doubts in the last meeting uh so the upstream is able to somehow set the damp since the end to independently of information that are that came from the absence indian origin to enforce some functionality to the end user request okay processing stages as a complete category for itself and in general metadata is where we include other objects that are not we haven't found a category of the others categories to where to put in there okay yeah we have other examples to every genetic metadata object because it was something that in the version one i think they weren't"
  },
  {
    "startTime": "01:40:01",
    "text": "examples we have include some minor changes in the metadata expression language um that there is no change in the in the built-in operators or variables uh parts but it's just about the in the syntax in the in the object in the json object um there is more detailed description for processing stages uh we have removed from the draft the mi requested capacity limits there is something that goes in the other draft that andrew will be presenting in a moment uh so we decided to put this out of this draft as the capacity um inside a draft have their own definition of mi object that will be extending the 806 and we added corrections from the previous version that was detected in the revision from the members please next so i will just go directly if you for the sake of the time to to one of some of the main doubts that were in the in the previous meetings uh so i i'm open to to discuss any other questions if you want um here i i have asked in the presentation um in the first column the section of the version one and the section four the section two so version two sorry uh because we go up we have categorized all the objects then section numbering has completely changed uh so yeah for the sake that you can find the reference of the of the previous questions on the last meeting um so i i will i will go to the section 232 the third line there um about aloe compress and this there was a concern about this object from kevin um and yeah there was a very fair point of the doubt here the thing is that uh we have found that this object allo compress that we still have it in the in the"
  },
  {
    "startTime": "01:42:01",
    "text": "version 200 with that name that name does not reflect correctly the function that is expected for a dumb abstinence cdn to use this object the idea of this object of the functionality is that the upstream cdn enforces the dumpster cdn to compress a response to the end user if the user has sent the access encoding header correctly but independently if the origin has sent the response to the downstream uncompressed so it's like delegating the compression of the an object in the damson cdn in the response to the to the end user independently of what the origin is doing okay uh probably this will um ha made the abstinent to not dedicate resources to the compression and delegate that to the dumpster in cdn okay so uh for sure the name allo compress is confusing um uh we are working on changing the name so we still don't have a a final name with this but something like edge compress or first compress will be more suitable for the use case that is especially for this object uh so probably will be changing the next version of the draft um for other the next section 211 the cash policy and there is another one in the next slide that this is the negative catch policy this is something similar related to um to what the origin is doing uh of the in the upstream and what the upstream cdn wants the dumpster incident to do all these cash cash policies are related to cash control headers in the responses typically the origin could send a cash control that is the same that should be sent to the end user right but the thing is that there are use cases where the upstream wants the dumbstream to have a different behavior regarding the cache"
  },
  {
    "startTime": "01:44:03",
    "text": "in this case this is regarding the time to leave of the objects in the dumps in cdn cuts so independently of what is sent to the end user in the response to the request the upstream needs to say the downstream to change the way that it's behave the cuts so there are some examples um that that we have found in in real life in in our production services deployed is like um for instance an upstream wants to have a catch control zero or no cash no store for the local catching the end user but they don't want to receive a huge amount of requests from the dumpster so they prefer to maintain in a double stream a ttl of let's say one minute while the end user received a no catch no store why why damstring could do that because at the same time the hamstring using the cni or any other method available is able to make content management of the downstream so they don't need that the damsen continues to revalidate the object because if they need to change the object they can use the content management to do that and that way the downstream block or stop the huge amount of requests from the end users and is not disturbing the origin server so this catch control policies or the negative patch controllers policies is intended for that so a way to change the behavior of the dams in terms of the ttl of the objects without changing what is intended for the industrial request so please next uh sanjay yeah this section um this is two three one one is regarding the gross origin policy um the doubt from here from kevin was"
  },
  {
    "startTime": "01:46:01",
    "text": "in the description was not very clear we have tried to change the description to have a more clear understanding of the reason of this the course the cross-origin policies that this came from from the http fetch standard the the definition of what comes in the origin header in a request when there is a fetch in the browser or in a player in a client that is making the request the origin header include is included in that request and there is not the service um url it's not what is defined in the host in the course only in a host match because the upstream one is delegating a host match is delegating the service of the request for a video content for instance or for another object but the origin header is related to the url of the you know how the client access that content in the in a video platform or in a web page so in an example you could the client could access the video platform.example.com but the host match is videodelivery.example.com so the request goes to videodelivery.sample.com that would be the host match that is delegated but the origin header included by the browser or the player could be videoplatform.example.com so in that case we cannot relate the course with the host match because it's not going to be that it's going to be where the content is published publishing on internet so why it's the schema and the domain name and port is because that is how it's related in the http fetcher standard well we have tried to explain this better so hopefully the multi are now so whatever you can find later we can we can discuss can we wrap up quickly um yeah i will try to finish yeah yes next um"
  },
  {
    "startTime": "01:48:02",
    "text": "i'm trying to find something more important uh catch policies is explaining before yeah some correction yes please next yeah there are some questions about including something in the registry for different options one was for the private fetchers um is my point here is that private features is something that is more a point to point so from one specific abstinence to one specific downstream cdn because our private features probably from the dumpster so i don't think it's needed to have a private a public registry for for those private features because of that because i think they are not generic and by definition there was another comment regarding and the traffic type this is a generic um this is a video specific configuration and we're talking about generic probably generic we can discuss later if you want by the mailing list i think that's a good point there isn't here an explanation but we can discuss so this is next yeah and that's it very quick i'm sorry uh for the sake of the time but please i'm i'm more than welcome to to use the mailing list to for whatever requirement that you want um and in the last slide please sanjay that is where we find that in this draft person is good enough to to plant uh to have this adopted as a working group draft uh all the realistic concerns maybe we can use the middle east to to discuss around this um thank you what i would suggest is that if you can put those uh questions that kevin had and the answers you have in the mailing list i think that would you know at least create a record there and yeah you know that have similar questions maybe answered it in in that manner and then we can um"
  },
  {
    "startTime": "01:50:01",
    "text": "take up your question of uh adopting this into the mailing list kevin what do you think i think um thank you for thank you for the updates i haven't had a chance to go over the full draft the the the groupings are great i think that the other thing then is do we want to break it up into multiple drafts right now it's a 90-page draft and there's a lot of stuff in there and there are things we could probably accelerate some of the metadata are probably easier to push through um if we do them as separate pieces um some of the stuff that's more fleshed out we should consider that i think um versus the the mono draft which is which is very hard and especially with the um the stages which i think are interesting i think that's good stuff but that is going to take a lot longer i think than just pushing through some of the metadata that's helpful for you guys okay okay okay we we will discuss it then and with the other co-authors so what what what would come plan to do with breaking out in different drafts okay thank you alfonso thank you great job all right last but not least andrew you have the remainder of the time saving the worst um i guess for the last yeah yeah i'm gonna do this actually a lot of the back story and just kind of go right into the changes so this is the version of the draft we're considering it's the capacity insights extension version 2. next slide please as part of this the this the draft has actually been simplified a bit we've cut out one of the main concepts which was allowing an upstream to ask the downstream to reconsider capacity limits uh we weren't necessarily confident in the approach we were taking so we decided to strike it out"
  },
  {
    "startTime": "01:52:01",
    "text": "just to make it a little more clear and concise uh next slide please so the component that was related to that which was struck out is the mi requested capacity limits object once again we just didn't feel we had a a good enough grasp on how it would be used effectively to um communicate or allow the upstream to ask the downstream to reconsider adjustments uh next slide please this it represents one of the two extensions we're looking to make to the fcvi uh capacity limits payloads uh the this is the capacity limits it is the main driver for allowing a downstream to communicate to an upstream of utilization goals they should be targeting next slide please this is the uh the delta of the json structure from the last draft on the left you'll see that there were some hard-coded sections of total limits and host limits we've decided to try and generize that slightly by moving those over into generic limits objects and then identify or adopting a new subscope to those limits so you can see here on the right hand side there is a scope object which is of type published host with several values this allows um a more granular scoping of a capacity within a cdni footprint sometimes that we found that that wasn't necessarily um granular enough to meet certain needs so we decided to allow within that footprint a subscoping and this is the vehicle that we've identified for that next slide please uh this object hasn't changed since the last version of the draft this is a an object which is meant to"
  },
  {
    "startTime": "01:54:01",
    "text": "advertise telemetry source capabilities and this is relevant in that the capacity limits were advertising we've we felt that there was a strong correlation with correlating that to a uh a telemetry source so there was no ambiguity between what capacity advertisement were specifying and how to measure the utilization against that limit um so this this is really a foundation for further work which we have yet to um put forward but we we felt it was very relevant in this case and wanted to lay the foundation for it here next slide please uh once again this is a diagram from the last presentation just kind of goes through the over overview of what we how we anticipate this to work really the the upstream would be polling the downstream to ask for the capabilities of and within those capabilities we would include capacity limits the upstream would then also be looking to consume telemetry data that related to the specific capacity limits combine those two data points together to make sure that there is sufficient um sufficient capacity in order to delegate requests to the downstream next slide please um once again this is the the diagram which allow or shows how the downstream would reflect changes to capabilities to the upstream relies upon a callback mechanism and a subscription service handled by the upstream in the downstream uh next slide please uh once again this is just a call out to the fact that we had removed the mi requested capacity limits object from this uh version of the draft and next slide please and that is it all right i made it in five minutes all right good job andrew"
  },
  {
    "startTime": "01:56:01",
    "text": "thank you um any any comments um so i encourage everyone i haven't had a chance to look at the updated draft myself uh everyone please go out and read the updated metadata and capacity drafts and send comments to the list please um i think we should we we should we should see um you know if we can get some more comments out there and have more people discuss the draft and then we can talk about whether um there's you know appetite in the working group to to adopt this work and take it on um any other final comments andrew no that was it okay thanks so much um we have three minutes left to wrap up uh sanjay anything you want to say uh my comment is i hope to see everybody in person at ipf114 um it's good to see people back in the rooms and i am happy thank you for everyone who came and presented thank you for you know carrying with us through some technical difficulty um we got through it all and i appreciate all the hard work from all of our authors sanjay yeah yeah i i i echo your comments and i think there's there's good progress and i would like to see you know andrew you know sharpen up this draft so that you know we can bring it back here and and take a pull on it and likewise um glenn and team um everybody has done a lot of work on the metadata so uh alfonso you know you you have the action item here to try and move that draft um maybe you know as um suggested that maybe break it up into smaller pieces so i think that might be able to we might be able to kind of look at it easily uh and move forward um with those"
  },
  {
    "startTime": "01:58:02",
    "text": "with those drafts um so yeah i hope to see everybody in person uh the next one is in july almost end of july third week of july in philadelphia so maybe easier for some not so easy for others but um i know francesca would not be able to join us there she is she's going to be on on maternity leave then um wish her all the best but uh keep up the work here because there's a lot of stuff going on so a lot more to come i guess in the next itf um but before we end we've got about a minute left francesca anything you want to add she says no she's so she's all good i think thumbs up okay all right thanks everyone see you in philadelphia take care all right thank you everybody is that crystal at the back hello hey chris how are you all right good to see you nice people through masks i figured he must be here somewhere how are you all right good oh finally we get this meat indeed"
  },
  {
    "startTime": "02:00:07",
    "text": "i know so you're right yeah yeah i don't know it's just tuesday and it feels like already a long week yeah or something was that when did you arrive i got here on saturday yeah i i did but i'm just like feeling against something today you know but i you know i'm good you know just you know having this myself not stay up late in the night you"
  }
]
