[
  {
    "startTime": "00:00:08",
    "text": "damn chris i should have stayed back for the cookie exactly only we'd known had i known warren was going to bring cookies all right i think when cookie time is done we're ready to get started and i believe warren is uh running the presentation so i won't do any of the presentation stuff stop the thing at so i'm doing the slides we said okay just waiting for the thing to reload and side erupts i'm assuming that the first set of slides that you would like presented is going to be the chairs ones because that seems to make sense i just realized that i've always done this being not in the chair role so warren you are that audible oh no really i thought i saw them no"
  },
  {
    "startTime": "00:02:06",
    "text": "um does this forward and backward let me check if my forward and back works it does not but that picky does good alrighty the chairs the slides are loaded i don't know if you're going to do the intro or i can yeah okay so we're getting started we have a relatively full agenda this is cider opposite ietf113 either you're in the room and you're in the right room or you're virtually on the on the medico in which case you made it to the right room virtually um if you think you're not insider ops you should go to the other room but you could also also stay and listen uh i'm chris cayer's on virtual thing too natalie's at the desk with warren who's running all the slides for us and john's gutter says perhaps the in-room chairs need to make sure they're very close to the mic okay next slide that work actually before let me also check the room mics because the batteries keep dying okay great okay uh the next slide is the note well people can download i should have said all the slide decks we have should be in the meeting materials site appropriately i think there's one update to this chair slides to put in but we'll do that at the end of the meeting uh miniko says they had to lower the front mic because of feedback oh good okay um i think we don't really have any agenda bashing to do we do have um a short chat from allison menken the uh ietf ombudsman who wants to come and talk to us about code of conduct and how we should be paying attention to whether the code of context says instead of not paying attention which we have a"
  },
  {
    "startTime": "00:04:00",
    "text": "tendency to do so allison yeah yo from the ombuds team um that was the yo from beowulf so we've had a bad um season this this season with cider ops uh i'm authorized to say we're we're normally extremely careful about about confidentiality but i wanted to say that we've agreed to let you know that we've had quite a few cases of people who feel concerned and uncomfortable with the kind of discussions that go on insider ops and the ticket to doing the right thing here is to just come back to the code of conduct over and over again which says be neutral be equal be be respectful to each other um if you are saying something about somebody using the word you you're probably doing it wrong um and understand that the problems that you're solving are very difficult so it has to be there's there's a need for more respect and more care in disagreeing because the disagreements may be complex but the problems that have come up from the ombuds team that we've seen have been both problems of long people that are well established and you would expect to be comfortable and know how to deal with problems but also problems of people who are new and not not well established are not sure that they can participate because they don't know how to work in the environment so we wanted to make it clear that without paying enough attention to the code of conduct it's probably very difficult to solve the problems that you have which are really difficult problems it's also probably difficult to get new people involved who may have solutions that you haven't thought of so this is my pitch and i'd like"
  },
  {
    "startTime": "00:06:02",
    "text": "come and speak to you um and i've spoken uh several times now with the the chairs about this too to go back and reread the code of conduct if you have any doubts about something you're writing in an email go back and reread the code of conduct um if somebody objects to something you say please think about your code of conduct and pull back from the way you've said it please try to lessen our caseload from cidrops so i don't i can't really answer any questions because of the confidentiality but that's the message and i think warren or the chairs may want to add something but if not that's the pitch warren anything you want to add yep i'd like to add something yeah i mean i must admit that well let me turn my volume up here um yeah i've never had code of conduct discussions with the umbud's team in any of my other working groups um so you know i would like to continue not having code of conduct discussions with the ombuds team um i'm really glad that people are passionate about this stuff um and we have some strong personalities just sometimes the passion kind of overflows from let's make this thing better into like let's beat up the artwork and one of the things which i think is happening is in many cases a lot of the people participating know the other people and think that they understand how their message will be heard and even if that's true people who aren't in that discussion see the sort of sniping and get concerned that they might also be attacked if they start participating so you know while chris and i can call each other you know i can call chris's mommy fat and he can say i'm ugly and we're both okay with that um"
  },
  {
    "startTime": "00:08:00",
    "text": "especially if you've seen his mama it drives other people away and so you know we need to keep that sort of thing in mind my god i'm glad that i'm not interested near where chris is because when i get back there's going to be a beat down i'm sure but yeah i mean really we need to be a lot more careful with this um if you're writing a reply both think about you know whether it's technically accurate and also whether you could word it in a way that comes off slightly less you know aggressive slack jackass actually um so warren not slightly less so let's let's try not to pull our punches too much not don't be aggressive be be only as aggressive as necessary only as as as strong as necessary not aggressive and i know that some people will disagree with that but but the problem solving um the the judgment about how aggressive has been over the top for a while now so i'd like us to to let the strong personality say you've been talking about you've been aggressive in ways that are you know too much of your mom is fat and now we have to have none of that now now let's let's let's pull back further than you expect you you need to and i think you'll find and i think the chairs will find that this will make for the problem for the the working group to go better um i hope that's true okay yeah sorry that's what i yeah that's what i was trying to say and also you know we should if you are disagreeing with something it should be the idea that you're disagreeing with not the person if you're disagreeing with sort of the person or the person's personality you've already lost the argument but yeah we have lars in the queue so lars go ahead oh there you are in person lars lima from that note"
  },
  {
    "startTime": "00:10:01",
    "text": "i would like to add one other recommendation that i saved myself a couple of times and that is when you feel upset about something that someone has written and you write an answer do that but don't send it let it sit there for a couple of hours yep go back have a cup of coffee read it again and you may find that you might may want to change a few words here and there and change the entire tone of the message that has helped me every time i've broken that role role is sort of broken that rule i've i've had i've had to to you know back up and and apologize and and uh sets miserable and every time i follow the rule i've been very glad glad afterwards so at least make the attempt yeah yeah also you know if the main discussions are between two or three people you know and there is more than two or three emails in a day you might want to sort of stop and take a step back and be like is this really the discussion we should be having now if it's you know you're missing a comma it might be better if you had a dash here that's great if it's your idea is the worst one i've ever seen that's not okay i think we've all all got this under all understand what's going on um and also thank you very much alison and the ombuds team for you know coming along and doing this and also apologies to everyone that this was needed and okay i'm gonna go but um i wish you everybody luck and i i'm gonna we'll we'll be continuing to monitor um the cases so please uh do us do us a favor and keep reading the code of conduct and thinking about how to be good to each other thank you and also if anybody is feeling stressed or you know attacked or something i will admit i don't always keep up with all the mail feel free to mention it to the chairs and feel free to mention it to me you know if stuff's not going well"
  },
  {
    "startTime": "00:12:00",
    "text": "just speak up and let's get this under control and i believe that the first set is mr yobe with a long title which natalie will read out because i'm trying to find the slide update on resource shine checklist and rpki client where which slide deck is that the second one i think well meet echo decided to just close the tab at me oh no don't if you want to start introducing yourself just in case somebody doesn't know who you are my name is job snyders i work for fastly contribute to open bsd in the last two years i started running hiking kung fu i'm waiting for data tracker to load the tab closed itself which was awesome did you try disabling ipv6 no actually i'm not luckily because i would have felt like an idiot if i were rejoining request browser permissions complete click share slides i only see sign tell verification hey how is that even possible i will have to share from my thing instead from my tab sorry that this is all taking a while luckily that's in no way embarrassing like we've never used a computer before do you want to share my screen why not try what was wrong with uh slide sharing the pre-uploaded ones"
  },
  {
    "startTime": "00:14:01",
    "text": "they are pre-uploaded but for some reason my machine they decided to not show them again they are so they are in the meeting materials they all are but they don't show up in uh the chair truly thingy do they show up for you in the chat then they meet echoview would you mind doing the magic to make the pre-uploaded slides populate into the share thing again if you're on the thing they are in the chat and of course my browser had also updated what and i can see them i said i can see them now they've all populated they have now populated in the thing yeah so which set was it the name rsc yeah there you go goodness me that took a while apologies all right i have a timer as well how much time do i have ten perfect uh research resource science checklist uh this has worked together with ben madison and tom harrison and there are many other contributors that helped us develop the current specification next slide please so what is an rsc rc is an idea that is essentially a conceptual successor to an earlier idea called rta but it is a simplification of rta you can find references in the draft to the prior work what rc offers us is the means to use the rpki to"
  },
  {
    "startTime": "00:16:00",
    "text": "[Music] sign arbitrary digital objects in the form of a hash you can sign so-called with resources rlcs are distributed outside of the rpki publication system so their existence does not in any way impedes the the current ecosystem used for routing objects and they could be useful in bring your own ip address or bring your own as scenarios in context of for instance peering to be or cloud providers you have to imagine that the likes of say fastly or amazon when there's a bring your own ip request that indeed it's useful to have the resource holder create a row app but the web user signing up to us we don't know if they were actually the ones that created the row app and we don't really care who they are but we do care if they are able to sign with the appropriate private keys the current running code state is there's multiple implementations there there's uh two signers uh one created by ben madison one created by tom harrison uh and on the validation site there also is multiple implementations that demonstrate how these objects are to be decoded and validated next slide please the last update on this was at iatf111 we through the iana early allocation procedure received a code point to foster interoperability testing uh by now that code point has been renewed and the code point has been added to openness openssl 3.0 which is has been"
  },
  {
    "startTime": "00:18:02",
    "text": "released a few months ago and starting in libra ssl 3.4.0 the code point is also available but these code points uh in in the cryptographic libraries are uh nice but uh all rpe implementations know that they also have to uh declare the code points in their own uh rp software because you cannot rely on the library being new enough to support code points like this unfortunately next slide please the rsc files have the dot sig extension sig short for signature there is example files available on github that you can use for for testing your own encoder or decoder this should be fairly straightforward as rsc files are sort of a mixture between uh the rfc3779 extension as it exists in the ca certificates and you can take some inspiration from manifest handling so if you glue those two together you you easily end up with an rc uh capable parser next slide please this document would be really nice if other interested parties contribute the expectation of of the implementation report is that per normative term term in the draft people indicate whether they implemented it or not and if not why not so the why not could be that you wrote a signer implementation which doesn't concern itself with actual validation so it's all very uh context dependent on which of these uh fields you'd implement which of the what values you'd fill in"
  },
  {
    "startTime": "00:20:01",
    "text": "in this table but the advantage of filling in reports like these is that we get a good understanding of whether all normative terms in a certain document are covered by at least two implementations which means that two individual human beings were able to decipher the internet draft and convert it into code and this greatly helps with the shepard write up and also the isg review in subsequent steps so if you're interested to work on rrc please also take a look at this wiki page and and fill it in as you deem fit um sorry yeah next slide the request to the big five the likes of aaron a p nick ripe afraid eclectic is to consider implementing rsc support in the hosted environments in the rpki dashboard or web portal or whatever you call it exactly all of the rers have different user interfaces which i consider a positive thing because that is a good indication of diversity and allows us to learn from each other so in yellow is is an example that maybe is applicable in context of aaron's email interface and and the the lower right could be a web form where you for instance fill in the resource with which you want to sign the hashes over which you want to generate the signature optionally a file name and the validity of the rsc and then what you should get back out of the system is a sick file that you then can either email or upload through a web form or put on a usb stick or whatever the transportation means are"
  },
  {
    "startTime": "00:22:01",
    "text": "transportation happens outside the global repository system and of course you should make a tool to revoke the ee served in the rrc so that if any future point in time you you want to withdraw the the attestation uh there's an ability to do so next slide please um the spec has been stable for quite some time between version 5 and version 6 we essentially only bumped the the version i think there's uh sufficient running code to to see that it works and is uh implementable so with that i would actually want to wrap this project up and request working group last call and we have two minutes left for questions comments so with that i hand it back to the circus master jeff yo quick operational question since i haven't been following this i think this is a wonderful idea uh i don't operate one of the caches that collect all this stuff and do all the nasty crypto to verify it is there any long-term concern about uh adding a large number of objects to the rpki system and impacts on the various applications that use it jeff thank you for the question it is very important to note that rsc files are distributed outside the global rpk repository system to illustrate what that means exactly roas route origin authorizations or crls or"
  },
  {
    "startTime": "00:24:00",
    "text": "manifest files are distributed inside the global repository system so if you use rsync or rdp those are the files you you you pull into the system but rsc files are not distributed through that means they are distributed in a one-to-one fashion so i could generate an rsc file email it to you and the global participants in this ecosystem would never ever know that i generated one and send it to you so there is no burden on the global system answers my question thank you thank you and i'm next in the queue um what would be helpful is at some point somebody explains to me the relationship between this and side ups rpki has no identity it feels like they're closely related i'm going to have to explain it when it goes to isg evil and rudiger will do your question question but then we'll cut the queue right after yeah you want to answer the question no do that oh okay somebody can explain it i mean if you want to take over my job you will be shorter um the relationship has been noted in the rsc draft itself the rc draft references the no identity draft and the rsc draft explains that rc files cannot be used to confirm identity all it does is it confirms that somebody has possession of the private keys and uh the resources with which they signed are subordinate to the the certificate authority okay so from my perspective there is uh no conflict whatsoever um i think everybody is on the same page and uh and it's it's explained in the draft itself uh very short yeah uh referencing back to jeff's question about what is the load on the general"
  },
  {
    "startTime": "00:26:02",
    "text": "distribution mechanism uh i was surprised by your poi your request for revocation tools which quite obviously uh will have a need on the distribution system and i but that's that's actually a surprising point yes the the only load on the global system is if you revoke an rsc uh the serial is appended to to this the crl of that that ca uh so per rsc that you're revoking you're you're adding a few bytes to a crl but then again the cr rsc files could be short lived uh where you you don't want to revoke this is something we'll we'll have to figure out in in the wild this is a tool we're putting it in the toolbox and we'll we'll see how it works out okay thank you and we are close on time so let's get on to the next one which might also have been uh what's the file name for this one is uh discard origin authorization and i will put you down for nine minutes even though we said 10 because we wasted the first bit me losing things okay thank you sir not miss slides anyway i'm ben madison i work for work online communications and hopefully we'll have slides soon we've just discovered it'd be useful to have the title not just the file name and the file share anyway so doa was the acronym that we came up with before we had quite agreed on what doa stood for so that is still subject to change"
  },
  {
    "startTime": "00:28:01",
    "text": "but this is another piece of work between myself job and mikhail who's not here today although maybe online um next slide please so a very brief bit of background in case anyone is unfamiliar with the the practice um out in the wild of the internet in order to try and mitigate the effect of distributed dos attacks it's fairly common practice for operators to need to ask someone closer to the source of the attack to discard traffic on their behalf because otherwise by the time it's on net for them it's already overwhelmed links or devices on the path and generally this is done by adding a special purpose bgp community to a to a bgp announcement and that community signals a request for the recipient of that route to drop the match traffic on the floor rather than forward it towards the the announcer of the prefix um and this mechanism is is the thing that we're trying to make a little bit more robust and secure through through the use of doas next slide please warren okay so today um so today as i say there's this fairly common practice that's been around for a while which is inter-domain rtbh signaling and over the last maybe three four years or so it's become increasingly common for operators to run policies which involve dropping anything that has a rov validation status of invalid on the floor at every ingress to their bgp topology and these don't play nicely um excellent the fundamental problem is that by announcing an rtbh route for a victim of a ddos attack you're essentially completing the attack for the attacker"
  },
  {
    "startTime": "00:30:02",
    "text": "the victim in order to mitigate collateral damage and as a result you want to keep the you want to keep as much granularity in that approach as possible and so usually these rtbh routes take the form of host length prefixes um now um origin validation on the other hand has this concept of protecting against two types of misorigination the first and most obvious being the wrong as number doing the origination the second and often more useful being preventing prefixes longer than some upper bound from being announced on the on the open internet and there's a conflict here because in order to um in order to have host length or very long prefixes propagate throughout the internet one would need to create rowers that permit very long prefixes to be to receive a valid status in so doing you effectively remove that second type of protection that rov gives you because you're you're essentially opening the door for any sub prefix hijack to occur for any any uh address space for which you want to be able to use this mechanism um the two common workarounds for this that exist today um some people force users of this kind of a service to create rows with those very long max length values which essentially turns off that second type of check and is not a good idea um secondly and this is what probably the majority of people do is they have a kind of a a carve out at the beginning of their routing policy which says if this is carrying the rtbh signal then i'm going to exempt it from the origin validation related policy upfront next please as i say the first approach is is problematic because it really breaks one"
  },
  {
    "startTime": "00:32:00",
    "text": "of the fundamental things that origin validation gives you and is if anyone is doing that today then i would suggest that they stop as soon as possible next please the second is as i say the the the kind of the lesser of two evils today but it's really really quite problematic um firstly the the fallout from abuse of an rtbh signaling service can be quite severe um especially where i sit in the topology as a transit provider um my customers are obvious are very often competitors of one another and there is a very very straightforward um path to one customer black holing the prefixes of another customer using this kind of mechanism which is almost impossible for me to defend against today without a mechanism like this um next please and there's other kind of you know side issues as well um whereas a rower has a has has meaning in a global kind of default free zone wide scope um rtbh signals are typically used very very locally you're not expecting to advertise one of these host length black hole routes and have everyone on the internet starting to drop traffic um the intention is that at most that signal is intended to propagate one or two as hops away from you um and rowers simply don't mean that and have no way of communicating that kind of a scoping the other problem is that we have no either implemented or even proposed mechanism to provide secure attribution of who's added a community to a root so in a situation where the origin is more than one hop away from you it's entirely unclear as a receiver whether it was the origin or their transit or their transits transit that decided this traffic should be falling on the floor and that has"
  },
  {
    "startTime": "00:34:00",
    "text": "substantial contractual implications as you can imagine um and this is that that first point is made a little harder actually because there's a there's a there's a well-known community um that's defined in in several triple nine which which uh which um which is a value that anybody can use to indicate this uh the the semantics without having to carve out a special community from their namespace but what the old practice before this came along gave you was at least some indication of who the origin is trying to talk to because if they if they're using a community that i've allocated from my name space then i can interpret that as it being a request to me whereas if there's this general purpose um signal then it's difficult to know out if it's propagating kind of beyond the the neighborhood of the origin it's difficult to know who should listen to that and just a note you've only got around two minutes so okay sorry i'll try and speed up okay so the idea is essentially to follow most of the logic that exists today in origin validation um but to add um add some heuristics to the object that we create that allows for receivers to know whether or not they should act on it um the um let me skip forwards maybe one in the interest of time it's a um 6488 style signed object um very similar in structure to a rower it follows the rower um procedure of the prefix holder signing the the signed data in the cms and the distribution mechanism is very similar as well it will require an extension to rtr and it will be pro validated and processed on an rp and that rp will be responsible for for sending data to the to the router for"
  },
  {
    "startTime": "00:36:00",
    "text": "use in routing policies next this is what the content looks like next there's a version filled next um the ip address blocks is subtly different um to what you find in a rower instead of having prefix and max length you have prefix and length range reason being as i say you probably only want sorry let's have a minute um you you don't want if you if you're if your prefix is a a 16 you may want to accept rtbh roots for a 31 and a 32 but probably not everything in the middle so you have a range the origin asid has exactly the same semantics as it does in aurora next please the peer asides is is there to provide that idea of scoping so um only if you have received the actual bgp announcement from one of the as numbers listed in peer asides you expected to act on it um next and finally there's a set of communities that the receiver of a bgp announcement can cross-reference against and that tells that tells the receiver whether what they think is an rtbh signal community is in fact being intended that way by the originator of the route um now i use the term heuristics rather than you know than rather than kind of specific criteria because none of this gives you absolute certainty that this this path that you're seeing in bgp absolutely is a black hole we don't have the mechanics to do that today either in bgp or in the rpki but it gives you a very a very very much stronger hint than um than today's mechanisms do and at its very worst it is at least as good as"
  },
  {
    "startTime": "00:38:01",
    "text": "rower coverage for the equivalent unicast prefixes are so it's it's a it's it's not a perfect solution but i don't believe that we have a perfect solution available to us today next please you're gonna have to go faster yep um i'm gonna skip over bgp reprocessing because otherwise that's going to take a while but the general idea is that you have a parallel um a parallel uh status which kind which mirrors in some way the the origin validation status but the two are orthogonal to each other so it's entirely it's entirely legitimate to have a route that fails to match a door but matches a row in which case it's probably a unicast route and the reverse is true as well being origin validation invalid and being a match for a doer probably means that it's a host length rtbh route and so the idea is that those can be ships in the night and as long as you are looking at the doa status before the origin validation status you can implement the same kind of check if it's an rtbh route up front before you look at the orientation of origin validation status logic just like operators are doing today so it's a it's a fairly easy drop into current policies um next please this is kind of what i'm describing there where you are you're checking the doa state up front and using it as a black hole root if it matches and only then you're going on to your normal kind of unicast uh routing policy checks next please so this is fairly early stages still i think the the idea is mostly fairly well formed i don't think that there's going likely to be much change to the underlying idea um the document itself needs a fair amount of work um one of the questions that we had for the working group is whether we think that this should stay as one document or whether it should be split out into parts kind of like the the origin validation series was um where we"
  },
  {
    "startTime": "00:40:02",
    "text": "defined the the object itself in one document the validation process and another and the rtr extensions in another still i'm not sure i have a strong opinion on that so i'm i'm hoping for some feedback there and i'd also like to know from the working group whether we think that this is a good candidate for adoption at this stage or whether the working group would like to see it mature more as an individual draft and then talk about it sometime down the line okay so we have jeff in the queue and if you can keep your answer short thanks yes so i have not read your draft my question to you is is this intended the address rtbh for anything in the adjacent a.s so that's the purpose of the pras ids field the default behavior is that this will not allow um transit for rtbh routes um but if you add one of your transit providers to that list of pras ids that's a signal to the receiver that you have authorized that transit um and that it should be matched and accepted so that that is supported okay thank you do you remember what is slidex called we're just looking for ignis i think you're up and we're just looking forward to something something yes scalability okay that's the one thank you and you we originally said 15 if you can do it in 12 or you know that would be hugely appreciated if that's fine hello the ignorance mcdonald's exactly tonight let's uh look a little bit into some practical experimentation with bgp sec so if i operate uh an exchange and if i would prefer to see what happens if the current global routing system is bgp second and next"
  },
  {
    "startTime": "00:42:01",
    "text": "slide please so this is an experiment this is a simulated uh playground based on uh real-world data based on the real-world data for gathering the above absolute numbers and the related relation relative numbers of all sorts of identifiers paths lengths distribution of prefixes and so on this this was done as an instrumented uh implementation for focusing with the measurement of uh the performance and mostly into the right i like this i will try to mostly for measuring the performance and trying to get some data why things work in the way they work and why we are getting such uh performance measurements this is a limited domain isolated environment which uses uh not necessarily what bgp spec specifies right now in the rfc it was implemented as uh friendly to the environment in the sense that using vendor-specific code points uh so there are no hijacks and every negative impacts and the main concern or the main goal of this is to try to find out uh how this would work if we move to bgp second turn and why it doesn't work as it is expected next slide please and if you can speak even closer to the mic right i'll try next slide please so if we uh use play in bgp uh there's a top of 400 neighbors uh having some some having full views some having less than full views uh with the distribution taken uh based on uh the public available route collectors uh so 400 neighbors feeding"
  },
  {
    "startTime": "00:44:00",
    "text": "into the route server and then uh no no policy um just best path selection everything is fed back bgp does that in a minute and a half for bgp set for exactly the same topology it takes over half an hour that's not necessarily the nicest result let's look why next slide please uh this doesn't run in a vacuum it runs on uh specific hardware platforms and uh those put uh certain limitations on to how things operate there it's one thing to write an abstract code for uh illustratory purposes the other thing is to write a code which runs on real hardware and your uh contemporary uh compute platform has plenty of raw compute capacity as such uh it might have plenty of memory capacity but not necessary memory bandwidth and memory latency is certainly an aspect to keep an eye on uh vectorization and uh smd uh wide operations are a general trend and the increase in scalar platform capacity is single low single digit percent whereas uh the increase in width of the computation is in orders uh sometimes orders of magnitude and yes your fancy shiny platform can still run 16-bit 16-bit code from 40 years ago that just does not mean that it is the right thing to do sadly many textbooks still continue to think that that is a reality next slide please uh if we look at the inners of a bgp sec uh two steps uh receive things hash the incoming parts get the message to be signed assign it in this case for receive its verify and then do"
  },
  {
    "startTime": "00:46:01",
    "text": "the rest of processing uh chatu is hardware friendly and in general that's a light uh computational light operation uh uh operates on the fixed blocks of four bytes and does a rather light arithmetical operation shifts bit back and forth and so on the problem is that it touches memory and touching memory is expensive if you can avoid touching memory uh without a real need you better do that signature generation it involves much heavier computational operations large uh integer uh realization plus there is also multiplication and uh computational it's more expensive but it doesn't need to touch the memory therefore uh overall the larger the longer the assigned path is the more time you spend on calculating the hash and even before that fighting with the memory layout and only then uh you do signing and verification in this case next slide please what is this vectorization thing about uh that's a simple idea that you have one set of instructions but you operate on multiple streams of independent data at the same time this is a perfect fit into uh hashing multiple uh secure path segments while calculating and verifying multiple signatures at the same time the operations for each chat you block they are the same uh just the data difference so take uh the full received secure path uh elements feed them sequentially into different lanes uh for processing and run uh shad to transform uh on them in parallel works definitely definitely works fine provided that you can get your data uh into the layout that is friendly for this then once you have calculated the"
  },
  {
    "startTime": "00:48:01",
    "text": "hashes uh feed them into your elliptical stuff and on the output you get the answer valid not valid um overall the latency is marginally higher for this uh you need to do some additional work and some instructions are not directly uh one-to-one mapped as in the scalar world but from the overall throughput perspective you have uh performance increase uh proportional to the width width of the vector lanes next slide please that's an interesting font over here let's try that again all right um so the problem uh with uh not the font but with the uh more important things is uh that the format of the secure path uh message on the wire is completely not the one as is expected for uh the hashing function to operate on there are two components in the secure uh path um the path part and the signature path the total length of the signature for the hope is 100 bytes but that's six plus 94. none of those two they are divisible by four and if you try to uh use the capabilities of your underlying platform which is able to fetch only at uh four or eight by granularity you cannot use this uh you can force it but the end result is that you will lose far more than uh the potential uh gain out of all of this therefore uh the receive side problem for bgp sec is that it has data on the wire which directly contradicts to uh performance to have being able to have a performant implementation let's move to the next slide and see what the font we get there oh the real one"
  },
  {
    "startTime": "00:50:03",
    "text": "for the transmit side um gpsex signs uh also the target s number that's a good thing uh the not so good thing is how exactly that is done so if i advertise the same path in the same prefix to a multitude of neighbors uh the stable part is the path itself and the prefix what changes is the target test number the first the initial first four bytes which are at the beginning of the block to be hashed that means that starting from the first round of sh2 processing the result will be different and basically uh we'll end up needing to redo all the calculation just just for nothing if instead that target s number wet back the stable part can be pre-compute and then intermediate state cached and uh the longer the uh the path length is the higher uh the savings in this case would be uh the other aspects are exactly the same uh signature generation is computational expensive it doesn't touch memory therefore it's not off the problem next slide please back to the experiments so uh do some uh fixing here and there rearrange some fields uh do some other protocol level changes and uh call it magic and this can uh move into the order of five minutes for uh the same uh test environment overall next slide so does this mean that bgp sec is fundamentally broken no it's not fundamentally broken uh everything is fine with the security aspects uh just that uh the current approaches to the wire format and some protocol mechanics uh does not uh"
  },
  {
    "startTime": "00:52:01",
    "text": "correlate well with uh performance implementing those things in a performant way and it also is a little bit disconnected from the realities of the current compute platforms next slide please uh all right uh that's actually strong encryption even for me i don't remember what was written there [Music] so i think those were the questions about those questions which you wanted to ask but you didn't uh manage to run to the mic so is this purely an implementation aspect can a smart compiler fix all of this for me we are talking about the data dependencies not control flow dependencies uh smart compiler is good at doing rather trivial things for example lot of vectorizing the round of chatu is mostly feasible you need just to provide a little bit help to the compiler but certainly the compiler cannot do anything about on the wire data layout formats and uh overall uh the data architecture of your application so if we use a fancy programming language will this solve a problem automatically well if you implement your bgp second implementation in uh in javascript i'm not certain i can read it from from that far thank you so much ah what can be done then uh i will return to that thank you jared so if i if i rely only on the language aspect no you can make things worse but not necessarily better uh so what can be done of trying to fix uh the bgp sec here first thing the protocol is versioned now one if you could try moving to the next slide maybe it will work no it doesn't so i will have to repeat that so bgp sec is versioned now right now we have uh uh mostly global uh deployment"
  },
  {
    "startTime": "00:54:03",
    "text": "of zero uh instances of bgp sec version zero uh and if we if we uh define a new wire format which changes uh a few of the things first is how um why a message is uh laid out on the wire that it is one uh consecutive block and uh not intermixed uh fragments the second thing uh the algorithm identifiers right now they define only the actual algorithms they possibly could either also identify the format of the message to be hashed and possibly processed in other ways that would allow for a certain degree of freedom of experimentation further and making that in a more or less forward compatible way as much as we would need that questions so that was discussed previously next slide please and let's see what we get in discussion right so um that's a feedback of trying to uh experiment of what it would take uh for bgp sec to be deployed and and used uh the question is do we as a community care about that and if we care uh how we try to address this and that's that's it from my side and while we're checking if there are any questions i'd just like to thank ignis for having kept the shot thank you thanks there must be at least some questions if not we can also take them on list keep it short and one from sriram sriram for your head oh okay question uh ignis um we've"
  },
  {
    "startTime": "00:56:00",
    "text": "we did some studies with like caching um the signatures that have been all already verified so during the signature verification on the update you can cache segments of the aspart the signatures that you have verified and next time the same update or another update that has a common aspar segment with the previous one you can make use of the cache so that's another way of improving the performance perhaps you have thought of it uh yes i do this actually is contrary to uh the recommended practices of using elliptical signatures you can do this only if your random number is stable and that leaks your key that's not the right thing to do at all cashing is certainly possible and uh rearranging a little bit a few things here and there you can cash and that's a whole point however signature signing and verification for as part longer than uh in this particular instance four or five hops becomes less computationally expensive than calculating the hash and that is the problem so you are not limited by the performance of the elliptic curve as such you are limited by the overall performance of memory system okay thank you joe snyder's fastly you ask do we care i can indicate just like an iepg i do care and i do think that now is a good time to start work on this i think version zero will give us valuable operational uh feedback on how it works in the world provided that bgp sec router key publication becomes easily accessible to operators and from there migrating to a performance enhanced version"
  },
  {
    "startTime": "00:58:00",
    "text": "seems a very logical and organic way to further the development of this protocol so awesome presentation thank you for sharing your insights and yes let's do it all right thank you thank you so i'm done thanks and next week [Music] thank you warden um so this is uh this talk is about aspa verification procedures um we we have considered some enhancements and also route server there has been a very good um productive creative discussion on the mailing list um i'm very thankful to nick hilliard um for for offering several uh very constructive comments i've had i've had discussions about this uh in the past with alexander and also thanks to ben jacobs chunwon and jeff and others for participating in the discussions on the list next slide please so um so we'll first look at uh like summarize uh the working group discussions on the list and based on that uh we will look at a solution for the route server issue there has been prior work where we identified a shortcoming in the aspa downstream procedure that was presented a year ago at ietf 110 and based on that the there was a working group consensus around that uh to to update uh the algorithms to overcome that shortcoming so in today later on i will describe the updated algorithms as as they stand today"
  },
  {
    "startTime": "01:00:01",
    "text": "that includes the above fix from ietf 110 it includes the route server being properly accommodated it takes care of some necessary special and corner cases and it's it's ready for uh the algorithm description is ready for updating the aspa draft next slide so i'm showing here a couple of urls for the working group discussion threads on next slide so just few basics about the route server rs is a route server in this picture as2 it's an ixp uh rs clients are as1 and as3 uh so in the control plane uh the transparent rs would in would insert its asn in the path and that is common uh and the non-transparent rs it would not insert it as and in the um it inserts its asm in the path the transparent one doesn't um and but the non-transparent rs is a is rarity or abnormal or even an abnormality as you will see we are not focusing uh the solution on the non-transparent rs instead we are focusing the solution on transparent artists uh in the data plane the route server passes the next hop attribute unmodified to its rs clients so the data plane connection between rs clients is a direct connection rs client r2 rs is essentially like a customer to provide a relationship and the and the relationship between rs clients as1 and s3 in this example uh is effectively a lateral carrying relationship next slide please um so for aspa-based route league detection uh involving the route servers uh we solved the problem for transparent rs and it just so happens that the"
  },
  {
    "startTime": "01:02:02",
    "text": "solution for the non-transparent rs comes with it at no extra effort no extra effort involved next slide please so this is an example of like how uh like it's a preview of the of the general solution with an example uh the rs client essentially uh we recommend that it should include uh the rsasn in its aspa so in this topology the update propagates from as1 to as4 from left to right uh and along the way at rs at as2 it passes through an rs as4 doesn't know that there is an rs in the path and it doesn't need to know it uh the asps take care of uh take care of that uh so the asp is that for for as4 to validate the update and determine whether it is a route leak or not uh the asps that should be in place are the are the three that i show in the middle uh as1 attests as2 as a provider as3 attests as2 as a provider so both rs clients essentially attest the rs as a provider um and in addition to that uh the rs itself assigns a as0 aspa and this recommendation is already in the draft uh so it with this uh with this set of aspas whether as3 receives a route uh with with whether it receives the update with the rsas included or not in both cases whether in other words whether it is transparent rs or non-transparent rs in both cases the as4 is able to uh correctly validate uh and tell in this case it's that it's a leak so by by having this set of rs asps in place the"
  },
  {
    "startTime": "01:04:02",
    "text": "the correct detection and the correct operation of the validation procedure is possible next slide please so the solution description is that each rs client registers aspa including the rsasn in the spas spas stands for set of provider asses in theory it is sufficient that each rs client has an aspa just including the asns of its providers other than the rs uh however some rs clients may not have any provider so in that case it it's good to have a general recommendation uh that the rs clients should include the uh rsasn uh in the aspa and additionally including the rsasn in the as in the spas has diagnostic value for troubleshooting etc next slide so another thing that is currently in the draft in section 5.3 is that the rs client detects whether the rs it is connected to is transparent or non-transparent and based on that it applies either the upstream algorithm or the downstream algorithm however nick hilliard uh offered this a good suggestion uh based on which we don't have to do that that complication can be avoided and the way it is done is that the validating as uh if it is if it has the rs client role uh it determines whether the most recently added asn uh in the as path equals the sender's as number in this case the sender is the route server if that is not so that is a confirmation that the rs is transparent in the in that case the rs client can simply add the rsasn to the as path"
  },
  {
    "startTime": "01:06:02",
    "text": "for aspa verification purposes only and then the downstream verification procedure can be applied so there is no need to make a choice at the rs client uh between the upstream procedure or the downstream procedure so with this modification or simplification uh the uh the section 5.3 in the draft can be potentially deleted next slide please so now quickly um the refined enhanced aspa upstream and downstream verification procedures including the enhancements including the uh the fix from ietf 110 as well as next slide please um as well as some special cases pertaining to presence of air set that's already in the draft but some special cases that have to do with this with the length of the update etc uh one can simplify uh i mean one can additionally enhance the procedures so that's been done and i have shared with alexander a few times and we didn't get a chance to spend too much time on it but we we did alexander did look at it and uh i have some feedback from him so in terms of the description uh this uh this is complete and it includes a number of corner uh cases that are necessary uh it includes the treatment of the route server as we just discussed a few slides ago uh next slide please so downstream procedure is described in these two slides uh and and it it kind of takes care of various uh corner cases special cases the route server as i said um and we have uh uh we have confidence that that this"
  },
  {
    "startTime": "01:08:00",
    "text": "works several people uh back in one year ago people reviewed it and we have confidence that this is a good way to uh to update the algorithm next slide please so similarly the upstream has also been enhanced including the special cases um again i will not go through this step step wise obviously but the description is complete and we can take one more look at it and potentially it's a good candidate uh to at least in uh in in its uh um the form i mean the wording may be different but essentially all the necessary ingredients for the procedures are are described here so it should work pretty good to update the draft next slide please so at least we have been working on implementation of everything a bgp rov bgp sec and we have those implementations available in bgp srx and included in that is also the aspa procedures uh that are described here except for the route server related parts which are new um so this is the reference to to our implementation uh there is also a data that is available for for tests data sets are available for testing so that concludes my presentation thank you happy to take questions excellent thank you we have been and alexander and thank you again for doing it in a short time and thanks to everyone for keeping question short hi sharon um i'm a little confused by the the read server handling um warren would you mind going back to the the example topology um there is no there's uh the one further on where it had as4 in it as well"
  },
  {
    "startTime": "01:10:02",
    "text": "that's the one um i i'm i'm a little unclear still as to what we thought was broken prior to this update um there is no difference from the perspective of as4 um if the if the rs is a transparent one it can be ignored altogether and if it's a non-transparent one then it is indistinguishable from a transit provider um all that needs to happen in order for as4 to correctly detect this as a leak is as1 needs to have created some aspa with any contents as long as it doesn't have as3 in it um and that's what the previous that's what the previous version of the algorithm said um i don't i find the the additional corner case is a complication rather than the simplification and i i don't see the logic personally yeah if you uh thank you uh for the question us if you focus on uh like here we are looking at as3 from s s or as4 from s four point four points of view uh points of view uh what you said is correct uh it doesn't need to know about the presence of the rs whether it is transparent or not uh all that it needs are the asps that that the two rs clients should have uh with any uh asn whatever uh any provider that it may show in the aspa but if you look at it from the point of view of as3 when a as3 rs client is evaluating a s then um it then it helps for it to uh see that as1 has registered an aspa including the rs asn in it um and that is uh that is one reason to uh so it is not necessary uh to include the uh rsasn in the aspa like you said um uh"
  },
  {
    "startTime": "01:12:00",
    "text": "because we are already assuming that the that the uh non-transparent uh rs is is a rarity um however uh if in cases when they are when the uh when the rs is non-transparent or even otherwise it helps the rs client so as4 it doesn't matter but for for the rs client as3 it matters to some extent to have uh the uh the asn of the rs uh in the aspa thanks that i i understand the explanation i think it's important to realize that as3 knows that it's speaking to a root server um i don't think that i don't think that having corner cases in the protocol helps anyone here i think it's more complication and the validation that as3 applies can take into account its local knowledge i don't think that i think this makes the validation procedure harder to understand rather than easier let me try to jump and ask the question for sure the problem occurs if in a slightly different topology imagine on this drawing that is4 is a custom and it receives a prefix from i3 and in this case if is1 and ic3 haven't signed the ic2 as its provider it will treat such a route as a router so one hope away from the non-transparent uh transparent internet exchange point we cannot distinguish if it is a round league or if it is not transparent internet exchange point that's the problem and alexandra are you talking about the case where the ixp root server is non-transparent there"
  },
  {
    "startTime": "01:14:00",
    "text": "yeah yes yes yes and a slightly different drawing where ice4 is a customer of ice i understand so i think in that case i think the in that case we embrace the fact that the root server is a transit provider it is a transparent it's just not giving anyone a full table and i think we should erase that rather than trying to make them artificially into different things yeah yeah i totally agree with you and so in this direction i had a plan to change the document i'm sorry for interrupting no problem thank you alexander um i just want to add to what you already said um uh i think you you said it correctly that for the non-transparent case uh if it may be extremely rare but just in case the route servers inserts is as number and for example as4 is the customer as you said in order to be able to detect route leaks it it is it is not a corner case it is it is it becomes useful for the for that rs client to include the rsasn in the aspa i think you agree with that okay thank you i suspect there'll be some more discussion on this on the list yeah um i'm sorry lauren um yeah i would request ben to uh put a message on the site on the list so we can we can understand if we misunderstood him thank you and ben says he'll put it on the list okay excellent thank you and we seem to be managing to recover some of our time but we will still try and make sure that we get through things so everybody has a fair fair shot at the mic and i believe next we have"
  },
  {
    "startTime": "01:16:02",
    "text": "yo rpki and ct and discovery and i think this is 15 as well all right i'm back joe snyder's and i'm here to steer up some stuff and cause us a lot of work in the next five years um this is an attempt to kick off a what i would say is quite large project amongst our fairly small community but i do believe it is worthwhile and in this in this deck i'll try to explain why next slide please so if we look at the rpki although the standards are 10 plus years it's only in recent years that we've really grown through some some teething pain um i think we're i mean this this image is is a bit of a joke uh but i've we've come a long way in the last two years i know that almost all rars are now testing with multiple validator implementations before pushing code to production some rers even move to a 24 7 support model i think rp implementers have uh taken a much closer look at the original intentions of of the design behind the rpki data structures uh and and also for instance on the bgp router side a few uh terrible bugs were uncovered and subsequently fixed so you know we we learn as we do but there is more next slide please because if i look at our sibling the web pki they are years ahead of what we are doing and i think they're really cool and i would like to copy some of their ideas and apply them to the rpki for mutual benefits next slide please"
  },
  {
    "startTime": "01:18:05",
    "text": "all right let me go to my backup copy of the slides here we go so this slide shows some differences between rpki and web pki um in the web pki if you want to be a root ca synonymous to to the trust anchor operators in the rpki um there's a number of things that that happen in that ecosystem that we sort of skipped over or are now slowly catching up to uh in the rpi ecosystem so for instance if you want to operate a web pki ca route you have to jump through many hoops to to produce audit reports and share those other reports with others for review you might be operating your route for multiple years without any subscribers so the analogy to the rpki would be without any subordinate inr holders uh before you are included in commonly accepted trust stores um and and that's that's kind of the the paperwork side of things i would say because an audit report and then a compliance uh report uh or or running for years in a sort of dry mode uh is is an administrative action but to help justify the trust in an uh in a roots in in the web pki there's an additional technology that is used that is certificate transparency so other reports are for the the"
  },
  {
    "startTime": "01:20:00",
    "text": "encounter so to speak certificate transparency is there for computer geeks to verify uh if what is happening under that root actually happens next slide please this is not getting any better so this slide uh contains uh a pointer to the last ripe routing working group where uh martin hutchinson from google gave an a very high level introduction to what certificate transparency is what the benefits are and some pointers to how not just the web pki but but also other pki infrastructures are life reaching certificate transparency so reviewing that 20-minute video uh is is worth your time next slide please nice so in order to kick off a certificate transparency project in context of rpki uh we need to map certain con concepts from the the web pki to the rpki and use that to to develop our own procedures to make extensions to to our own software so in the certificate transparency world there is a role called the believer and the believer is an entity that takes an attestation made by a claimant and and verifies that according to the cryptographic procedures as described in the specifications so in the web pki world the believer is the web browser and in the rpk world the believer is the relying party cash implementation so the likes of rpki client fort or rootinator verifiers are entities that have a stake in the ecosystem and in the web pki this could be the owner of a domain name say"
  },
  {
    "startTime": "01:22:02",
    "text": "fastly.com uh in the rpki this is the the resource holder so the holder of an as number or or a set of ip addresses and the verifiers want to know what certificates were issued covering my resource or my domain name which means they can verify whether it's cas exist which which hold those uh resources uh as subordinate uh whether they are all under control of the verifier but also security researchers use can be can assume the role of verifiers to to compare what they see in the wild versus what what should exist then finally the claimant is the entity making attestations about who who issue certificates so in the web pki we call them cas but in the rpki it's it's basically the trust anchor operator plus the cas that belong to the trust anchor operator so in the rpk you you have the the trust anchor itself which is usually the offline hsn and a few intermediate certificates that that hold the power for zero slash zero in both v4 and v6 and then the the trust arc jumps towards the the cas that are heavily constrained by the 3779 extensions so uh in in the claimant for the purpose of this presentation is kind of the the top of the the trust stream if you follow this link you can read more about the use of these words next slide please"
  },
  {
    "startTime": "01:24:00",
    "text": "now uh a very natural question is why not use existing publication mechanisms like rsync or rdp and we have to realize that both rsync and rdp have been optimized for a very specific purpose which is to bring the current set of objects as fast as possible to a verifier to for instance the rp implementations so an example of why rdp might not be complete is uh you you can although everybody knows that manifest numbers monotonically increase between rdp deltas you can observe in some situations gaps in in that numbering scheme because if if a user adds a rowa and deletes a roa in a matter of seconds the the subsequent overt event overtakes the earlier event and it's not worth sending out uh the the earlier uh manifest because it has been overtaken by events and the purpose of certificate transparency in this regard is to provide full insight into all ca certs that have been issued which makes it a bit of a heavier machine and therefore i think it's very good to to have separate mechanisms uh one targeted to get files or objects very fast to the verifiers um to the believers apologies and then a secondary system that is designed to to inform verifiers about all actions that transpired next slide please so the benefit to the community is that if we set up some kind of global rpki certificate transparency system"
  },
  {
    "startTime": "01:26:01",
    "text": "we get auditable logs of all actions that rars took and this means that resource holders such as myself uh can inspect exactly which cryptographic entities at what point in time had received what entitlements so in the case of for instance rsc it's it's really nice to to understand at what point in time uh could anyone other than myself and my my parents issue rscs signed with my resources or if there was some kind of administrative issue where for one reason or another my my res my rpki entitlements were revoked when exactly was my ability to uh sign with those resources reinstate it and i think that the having this type of maximum granularity or full detail like the highest resolution image that we can get of the issuance process of of the rers will positively raise the bar in the ecosystem so aspiring to implement certificate transparency forces people to take a careful look at their issuance process and document and understand how exactly their procedures work and through that process we may see some organizations optimize their process so i i think it's good to have uh something on the horizon that that yes is difficult is is it's not cheap it will involve many man hours or people hours but the end result is i think a healthier ecosystem that is worthy of the trust of the believers i don't think it's i think it's it's not sufficient to say hey we are an rer you can trust our brands"
  },
  {
    "startTime": "01:28:01",
    "text": "we we are trustworthy because we we engage with the community we listen to you all of this is true but in addition to that i want to be audited to be able to audit those claims and and see how what the health status of a a trust anchor operator uh truly is uh because that provides the grounds to to engage in conversation uh and make retrospectives on on incidents and and talk about why did this happen and what can we do to prevent this next time in the same way i see that ct in in the web pki has brought numerous improvements uh and and far more uh uh operation of of the cas in the web pki to the point that now uh ct and the web pki are so intertwined uh that that web pki is is fairly functional these days so um next slide please slides broken uh but it's here if you want to see it in the big screen you can see my slide here so the to-do list and keep in mind this is a multi-year project so if you happen to be an rer and you look at your budget and you're like there's no way i can pull this off that's okay there's always next year or the year after this is not a project we should rush through this is a project where we should carefully take our time to establish what exactly it is we want to implement so for the initial scope of certificate transparency i propose that we attempt to keep track of the cas in the ecosystem so we leave roas gbrs bgp set keys aspa objects out of scope those are ee certificates i think if we can get to the point where ca certificates are carefully tracked uh that would also already be a big step"
  },
  {
    "startTime": "01:30:00",
    "text": "forward now who does this concern as i mentioned i think the rars are are the ones that the prime candidates to provide a high level of transparency uh through ct log operators some entities will need to set up [Music] surfaces that can absorb information coming from the rers and publish that in immutable logs and then of course verifiers and that could be anybody with a stake or interest so somewhat out of scope is delegated rpki or even rp implementations because rp implementations are just believers and then the second part of my slide is a call to to look for interested people step one would be to offer an internet draft that kind of maps out a plan about what cte is and how it applies to rpki we need to find people that are willing to host logs because the claimant should be a different the lock operator should be a different entity than the claimant we need to find people that are willing to send pre-certificates to the locks and ideally find people who have hands-on experience with ct and can help our community understand what to do and what pitfalls to avoid so if that the microphone uh queue i guess is open yep and we only have five questions sorry five minutes for questions so people can keep them short one minute each okay uh this is the um yeah i like the idea of ct and i see a lot of value of i've seen a lot of value in applying certificate transparency when uh you apply this to any certificates like the resource sign checklist that you mentioned because it's very hard to"
  },
  {
    "startTime": "01:32:01",
    "text": "observe all the objects and the nobles actually published and unless you have certificate transparency on identity certificates you cannot show a very important attack in the rpi which is which i think is the original objects from the view that you present to somebody and that would be critical that ees are included initially but my main question is if i may respond to that okay um [Music] it would be very cool if eeserts can be part of uh the ct lock infrastructure and i'm not pro precluding or x sorry i'm not excluding that that path but to to reduce the scope and and get somewhere i think it's great to start with c a's and if we can get that working we can maybe add more to it oh okay i don't see much benefit in removing the code part where for a ca certificate you submit it to the log incorporate the rsc and for you certificate you don't but that's uh we probably actually have to prototype this to see how it works out but if you want to if you want the rp's to check certificate and transparency they will need to check the the attestations that are in the ca certificates this means that when you want to create a ca certificate you need to get enough responses from qualified logs at least in the web context and that implies that that log availability causes an upper bound on ca availability um and more brittleness in the rpi scares me a lot being an actual ca operator of a real world instance that has a lot of impact so how do you well how do you think about this risk so as you can see clearly i consider rp implementations at this"
  },
  {
    "startTime": "01:34:01",
    "text": "point in time out of scope because rp's are believers and and they just absorb rdp and rsync and do what they do today separately will create verifiers maybe based on existing rp code bases and those would absorb the logs and maybe use monitoring alerts or or but an rp in the rpki context is a believer not a verifier so sorry we are all going to have to be okay we need to clarify there's lots of clarifications that need to take place uh i don't consider myself a ct expert either so we'll need to educate each other and yeah that means lots of talking uh um in web pki the end game of significant transparency as these as i understand is that if a ca really misbehaves we just pull it from our trust and we no longer trust the ca how these what do you see is the end game for rpi because i believe there is no real no real alternative to the rars currently if an aria are misbehaves yes i will remove them from my thruster okay and instead i'll tell your goal with uh certificate transparency to see if an rer misbehaves and then to pull them from from your thruster the first goal is to engage with rudiger the first goal is to engage with the ror and confirm with them hey i saw this incident can you provide me with an rfo and then an rfo appears and hopefully everybody learns but if the same type of mistakes repeats over and over or if there are systematic issues it could motivate some operators to to perhaps temporarily or perhaps permanently no longer"
  },
  {
    "startTime": "01:36:00",
    "text": "sees using a specific trust anchor so the goal of transparency is is in part to to be able to to hold people or organizations accountable but distrusting a route is is then that's the end of the process that is the death of the universe so the the goal of ct is to avoid to get to that state because we can learn uh from what appears in the logs okay so it's just logging and knowing that it happens but that's it right then or the first step towards solving a problem is knowing there was a problem okay thank you thank you and next in the queue we have russ hello russell um i have uh real problems with this work my concern is that the rpki unlike the web pki was constructed so that the ca is authoritative for the resources that it issues the web pki on the other hand all of the roots are able to do anything with any aspect of the name space so when we started uh working on this the ieb suggested that ayanna run a slash zero and the rir's be subordinate and then to accommodate uh easier transfers among the rirs each of them became equal roots for slash zero i argue that you don't need this if you go back to the first model"
  },
  {
    "startTime": "01:38:01",
    "text": "thank you for your comment russ i think i should clarify and probably repeat this at many subsequent presentations in the rpk ecosystem there are 22 000 cas the ones to which i think certificate transparency applies are the rers that have the 0 certificates and their intermediate operational certificates and the moment the trust arc bounces towards say an lar who indeed are heavily constrained by rfc 3779 extensions i they can only shoot themselves in the foot so i want to know which other lars are are able to sign with my resources but beyond that i'm less interested so i think in practice in the rpk ecosystem ct will apply to maybe 15 25 cas uh but the cas that i myself as as resource solder run uh are are not of interest to the cte project and we're really going to keep the questions and answers a lot shorter it's been way more than 15 and we're okay well two two views one is following rus some of the basics of web pki and vrpki are really different for some parts of your arguments i had the bells ringing for example that you should keep in mind that ca and identity are not really the same thing um so kind of harking back to successful treating web pki"
  },
  {
    "startTime": "01:40:03",
    "text": "is not necessarily the most valid thing to do uh nevertheless the activity of establishing tracking mechanisms and monitoring for what is in the p in the uh in the rpki and figuring out what info what information support should be generated like your question i am the holder of resources and for a long time for a long time i have been thinking we need to establish some monitoring and things like that including uh making it easy for resource holders to get an independent signaling of what the global view of their resources is so for many of the details i'm i think the web pki is track is directing you to bad tracks the idea of we want to monitor this um and figure out a lot of details what is necessary and useful there that is valid word uh work and uh well okay that's that's that so i don't want to dismiss this effort overall but i don't think it is headed in the right direction right now um couple of things i'm not sure either of them are really questions um so the first is i think that i think there's a there's we need to do better as a"
  },
  {
    "startTime": "01:42:01",
    "text": "kind of as a group about distinguishing when we're talking about publication events versus signing events because often that you know obviously they happen very close together in time and usually by related parties this is about for me about seeing signing events which are not visible through any theoretical version of the publication system not just the one we have now i think we probably want something like this for the publication system as well but there's separate problems um and i think teeth for example the problem that you were pointing to that that's an important thing that i want to be able to see but that's not a signing event that i'm looking for that's a publication event that i'm looking for um the second thing that i think it's important to point out is i i i have a very good working relationship with my local rir i know the individuals involved i trust that they are doing things with the right intents and they're not trying to you know act with any malfeasance but that's not necessarily where the chain of the chain of trust needs to stop i also need to be able to demonstrate to some third party when the rpki eventually causes some substantial outage for one of my customers when it will you know it's just a matter of how and when and what that looks like i need to be able to stand up in front of those people and demonstrate why it was reasonable for me to trust this system in the first place and having things like some version of ct having the ability to demonstrate that the publication system was sound all of that sort of thing allows me to make that argument better and allows me to use this in a more robust fashion thanks um so email me if you're interested in this type of work and this is super super early stages no direction has yet been set other than i want more transparency in this ecosystem thank you excellent thank you very much and now we have our last presentation"
  },
  {
    "startTime": "01:44:01",
    "text": "and yes thank you so much [Music] yeah i will i think we are a bit short on time so i will try to keep it short um so my name is kung i want to talk a bit about the rpki of the beaten happy path we know how the rpki works when everything works because it currently does however there are a couple of edge cases that can currently occur with the current standards which would probably be caused some issues and i want to discuss five of them uh and basically ask your input about what you think should happen in these cases uh next slide please uh the first one is is partial rpki data the id is now this is a fictional example however you can find similar examples in real life uh we have a sca0 at the top and it has three children ca1 which has a roller for 1.106 16. we have a ca2 for that has that for the slash 8 another one for this 16 and cr3 for a slash oh nca4 no ca3 4 24. the problem is that if you send for some reason ca3 for example becomes unavailable then you have a rower for the slash 8 and you don't have it for the slash 16 that is a subset of the slash slash 8. and now you get a valid that becomes invalid which is uh contradictory to the fill open situation that we normally have with the rpg and the question of course then is well what do you do do you then include the data from co2 do you uh use the cached version if you have it"
  },
  {
    "startTime": "01:46:01",
    "text": "from ca3 do you uh and what do you do with date from ca4 if that's the child of ca3 or do you just say okay if you're doing this then you're so stupid then this is your own fault if anything bad happens and then the question that of course uh also uh you get is um when do you actually then when should the relying party actually report ready to rtr that's it has tried to get all the data and uh perhaps failed for safer next slide please um now a couple of ways that you could have uh situations where you get don't get all the data uh look i can delegate my resources and i can do that as however i want so if i am e in this case i can create nine children that are all different publication points and different locations and they can also delegate their resources however they like so like they can delegate it to nine children again and we can do this for eight layers uh we have some limits to the depth of the chain this is uh based mainly implementations uh this is not but not in the specification however if just already with the limitations that we currently have i can create in the end five million uh publication points just by creating something that is nine wide and eight deep and and the question that i have then is is what should erp software and operator do in this case and should the ca prevent that from happening or uh what should the ca actually do and and also if you think that you notice something like this but it's actually just an honest structure that is looks strange how do we then deal with false positives next slide please um there's also something about file systems i mean this is all runs on machines that are"
  },
  {
    "startTime": "01:48:01",
    "text": "actually that that just have actual disks uh especially with rsync we first get the data and then we do something with the data now we have some ways to exclude files we don't like and those are being used but we can create many folders and the thing is that we can even do this uh well not just the publication points can do this but any uh hop between us and the publication point can for example modify the reply that we get from rsync to add a lot of folders in between them i i've shown an example structure that you could use and that means that if you were to apply that to the right ncc publication point uh from two months ago then you would get about 18 billion folders which is probably more folders than your file system is able to uh manage and in the current implementations what we see is that uh there is no way to restrict this in any way shape or form which then means that your file system says okay i cannot handle this anymore and uh whatever happens is then left to what your rp implementation does and the operating system but then really doesn't result in you having actually uh data that you can use next slide please i and this is also very simple but we we have robots and we can create rawa so i i was just calculating okay how many vrps can i create from my rows so i mean let's say that i have a 48 which is not that's uh out of the realm of possibilities then i can create a 2 to the power 81 give or take prefix from that because i can split everything up into everything which is not useful in any way but i can and then i can uh allow that for 2 to the power 423 asms which means that you get a very"
  },
  {
    "startTime": "01:50:00",
    "text": "large number of possible vrps that your router is probably not going to handle however rp's accept this in principle rtr accepts this in principle so at which stage should you actually go okay probably not don't need all this data we should probably leave something out should router do that or do should rt do that or your lying party should do that or the publication point or the ca next slide please so and then the last thing is if something like this happens um and i just do this because i like they're very very much this like you then i can do this and only target you okay if we do then get significant transparency this becomes a lot more difficult for everything that requires science objects but the rsync one that can also be done as a man in the middle so and you would stay out of the significant transparency part so my question is okay how can i effectively stop this and if i cannot stop this how can i report it to someone else that can stop it and how can i prove to this person that i or this organization that actually this is happening and i'm not just making this up because i want to get someone else taken down that is actually the one that that that that i actually dislike i mean there are malicious parties out in the world i would like rather like that there weren't but they just are and so the question for that is uh how do when can i prove who the perpetrator was and is it even possible and and how can a ca know that what they are doing actually is viewed by me as something that is malicious next slide please so those are the five points that i wanted to discuss and uh my question to you is um should these problems or if there are problems be dealt with and if so how um"
  },
  {
    "startTime": "01:52:03",
    "text": "who should solve what and if they are going to be solved should that be done in a proactive manner or in a reactive manner so i would like to open the floor and order the microphones do your comments thank you jobs first job schneiders fastly open bsd um i i don't want to go as a slight first slide to to offer potential solutions or answers but for instance i think that publish in parent really is a technique that that helps the whole ecosystem because one of the fears is that a sibling ca of yours can do something that somehow knocks you out and for instance in the partial rpi data example you you listed that if ca3 has an issue ca4 disappears a lot of those scenarios are alleviated if published in parent is used and and i think for this reason and other reasons we as ecosystems should really strive to to get rid of the well not get rid of the delegated aspect but encourage everybody that the default setting is publishing your parent because it makes life easier and if you publish in the parents the parents can out of bands so on on the layer uh where where the child is is sending it to the parents api uh apply some restrictions like with email in the smtp protocol it's not encoded that i can only send you up to 10 megabytes but if i try to send you a 10 megabyte email your email server will say oh it's too large i i'm not eating this another metal surfer might happily accept it and this is local policy so each parent repository can apply local policy as is appropriate for for that context that environment"
  },
  {
    "startTime": "01:54:02",
    "text": "without signaling such limits through the rpki so the parents i think can not only help rp's and and help help against partial rpi data but also keep their children in check in a way that is lightweight dynamically adaptable uh and and does not encumber the the broader ecosystem um but i can monologue here for hours uh i yield the mic to other people yes thank you yup uh i think you make a good point however that that in the implies that you get a first class citizen namely or the first class uh publishing parent and a second class delegate and i'm not i it is a solution but i think that is that is a consequence of that uh solution that needs to be at least we people need to be aware of right yeah um tim russell's um my name was on the first page of the slide act um one thing i wanted to say first is i think this is an example of a number of issues that may occur and a general question about how we should deal with them secondly to be a bit more specific uh so i my feeling is that there are things to be discussed with regards to these suggestions that yop just made i think there may be work there but the current reality is that parent case can only be reactive they can stop a delegated ca when problems occur but i think we should look into more proactive measures and if we want to do things with repositories that implies that we need to look at the publication protocol that also implies that we may want to think about what trusted repositories are and what not so i think those are all very interesting interesting things to think about but currently uh just to repeat i think the"
  },
  {
    "startTime": "01:56:00",
    "text": "reality is that we can only be reactive thank you tim first small side note thinking about of bad characteristics of arsenic just ignore arsenic has been identified as a dangerous spot and is being replaced for the uh voluminetr volumetric attacks that you were uh going um well okay i'm i'm uh i i think i think they will blow up uh much earlier than they hit the routers um but uh well okay the thing that uh you really should be checking is your first slide where you only told the roars that certain cas are supposed to publish and you did not show what sets the cas were holding and the the trouble that you constructed depended on the unusual idea that the delegation of resources was not hierarchic and a kind of overlapping between siblings and yes the monitoring and tracking system uh quite certainly should show that and yes uh the policies that this should not be"
  },
  {
    "startTime": "01:58:02",
    "text": "hap this should not be done when running your registry uh have not been that obviously and formally erased but they are actually i think very well understood thank you whitaker yes i want to point out that this was based on a real life example which is currently in the uh primarily a p naked and identic relationship where this happens uh it's not yet it doesn't happen a lot but it happens in some places so that's why that that sounds that sounds like to rehark to russ uh russ remarks about having a single or multiple routes and not having very clear and explicit formal policies about how the how the resources managed under the overlapping routes actually are defined and strict thank you um i'll try and keep it as short as possible i think your your your first dog uh your first example may well look for curd in the wild but i think it's an example of you know people will be able to shoot themselves in the head using whatever technology we give them um and that's you know that that example is an accident waiting to happen i think for the rest of i think for the rest of this i think that it's an important problem and um we do need to be clear about what the action we take it i'm not i'm not convinced that the any of the action that we take against the um the the kind of the potential doses that exist should be changes to a protocol um what i do think that we need to do is being much clearer um on in particular how rp's are dealing with placing limits on their willingness"
  },
  {
    "startTime": "02:00:00",
    "text": "to traverse trees and lots of directories and lots of objects and so on and so forth and it's not necessarily the case that they need to implement the same protections or make them as conv configurable as one another but i think that there would be value both for people potentially doing new implementations or for information sharing between implementations or users of those implementations if there was some collaboration between rpm implementers to document what recognized attack vectors there are and how they each deal with them and the strategies that have been taken in the pla the trade-offs you know and i think that wouldn't be you know anything more than an informational document but i think it would be a a useful reference point um but i don't think that we i don't i don't think that having constructing a system that is impossible to abuse or that is is capable of you know not breaking under any you know absurdly overloaded circumstances available to us because essentially the limit is your memory if you don't have anything written in the software um yeah thank you jared motch uh job q's coolest um one of the things i observed uh in listening to this is how much it reminds me of the early days of usnet news which was a case where you would have these files and they would get transmitted over this protocol and you would write a whole bunch of directories and files out to the file system and one of the companies that decided they wanted to build commercial software to run a usnet new server they actually figured out that leveraging the underlying operating system was actually inefficient and while i know the original implementations of much of this used things like rsync as the method to go and transfer the data the data is still just data and the files are just files and that doesn't mean that an implementation shouldn't perhaps look at okay do we"
  },
  {
    "startTime": "02:02:00",
    "text": "abstract this out and store it in our own internal data store so you know what one of the companies did is they went and implemented a solution whereby which all that data was still there but instead of them having to go out and making all the file system calls and dealing with all of the slowness of interacting with the kernel and operating system and hardware they actually implemented a lot of that internally in something that was optimized for that use case and i suspect that that historical perspective may be useful in the context of this which is we have a lot of potential traversal stuff a lot of potential file creation and stuff and as well as expiry policies which are very similar to but not exactly identical to that use case and the solution space may lie very similar to that software which then became very dominant uh even though it was commercial and there were other you know open source softwares to compete uh as a result and so i i think here that that that is very likely the case uh you know that we should be looking beyond the individual file systems and looking at how that actually data store is held in uh you know internally within the software and uh versus that so i'm not sure what your thoughts are on that well i think you are i agree and and for rdp a lot of implementations already do that but the rsync protocol makes that more difficult to achieve that same so i believe that there are implementations that do that for rdp but do not do that for uh rsync at the moment because rsync and rsync is still requirement so you can always downgrade to rsync and then execute same attack okay and our last one thank you very much we managed to recover some time so we had his this initial reports about these issues and we resolved part of these attack attack"
  },
  {
    "startTime": "02:04:00",
    "text": "attack factors which is a really good step because it got a lot more complicated to do a denial of service attack on all validators worldwide however you also showed that if you want to attack a specific instance you can still do that because it's really hard to set these limits in a way that's in the recursive case which still needs to be quite wide because some cas are quite wide you cannot abuse it so in my opinion we need some work on that in this working group so that at least relying party instances can detect it when it when the administrative domain changes when traversing the tree because uh for some entities it may be logical that they have an extremely large repository and for the for a non-rar it's probably likely that their logical entity has an order of magnitude less objects in there and yeah i think we should continue investigating this issue thank you peace thanks i think that was exciting i want to congratulate warren for being the first one that has managed to voluntarily get a group of engineers to accept cookies so thank you so so thank you everyone and also thank you to the speakers and you know questioners for keeping things short um we did lose some time in the beginning but we're going along nicely and also thanks everyone for you know we're going to try to keep things civil code of conduct etc again you know passion is great um poking each other is not so thank you everyone um and button oh we can still be means chris yeah he's an area actually i mean yeah we should yeah we can joke it chris"
  },
  {
    "startTime": "02:06:00",
    "text": "um okay so [Laughter] i keep forgetting that i'm coming home tomorrow and you live nearby and if you all don't hear from me on the list for a few weeks somebody please come find me okay um thank you everybody thanks to the speakers it seems to be something"
  }
]
