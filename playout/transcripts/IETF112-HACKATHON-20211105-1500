[
  {
    "startTime": "00:00:21",
    "text": "hello everyone um i guess congratulations on making it uh to the uh the closing of the hackathon hopefully it's been a productive you know week for you um probably a bit of a tiring week as well but uh but if you accomplish some some good things then that that makes it all worthwhile and uh i know this is this is my favorite part especially when we're in the um online and in this virtual format because uh you know usually i like to go around at a different table see what people are working on kind of learn that as the hackathon's going on that's a little more challenging to do in this format but when we come together for the closing it's really really great to see what everyone was able to do during the course of the week and some of you i touched face with in gather and elsewhere but some of you i haven't really even had a chance to do that so really looking forward to to all the presentations and let's see we are just about the top of the hour um first of all is my audio coming through okay this is a different computer for me on a hotel network or on a hotel network works great okay great thanks um and actually i'm traveling for a conference and things are starting to pick up a little bit looks like there's a chance our next hackathon will be in person kind of fingers crossed that we get good news on that we'll see how things go so um yeah i'm going to go ahead and get started this is uh being recorded so um"
  },
  {
    "startTime": "00:02:00",
    "text": "in case you miss any parts of it but i figure yeah best to jump in i know it's it could be a very inconvenient time for some of you so we'll uh we'll get going on that okay uh just a quick reminder you probably all know this but this is more for people who are tuning in perhaps just to hear these results that a lot of you accomplished the the whole goal with the hackathon is really to speed up and improve the work that we're doing in the ietf to to work on um implementing standards as we're defining them to get more running code related to those standards that not only helps make the standard better but makes it a lot easier for developers to go and adopt the standard and implement and add it add support for it into whatever it is they're working on and it's also just a great networking opportunity too to come together to have people from from different backgrounds with different skill sets who all have a shared um interest in these standards and but maybe working on different aspects of it the implementation of it as opposed to the definition of it or perhaps a bit of both and uh and it's really a great time to also engage with um what i would say are often not your are typical ietf community members people who who come from maybe from universities from other sdos from open source organizations who are aware of and interested in the ietf but maybe it's not something they follow on a day-to-day basis but this is a great time for them to come in and work really closely with with people from the ietf uh we operate under the note well hopefully you're all familiar with this but uh if not you know it's it's really good to take a a look at this it covers our processes and and how we work you can see the uh bcps there at the bottom uh you know i think code of contact is we see a lot of emphasis on this it's"
  },
  {
    "startTime": "00:04:00",
    "text": "important to look at that the copyright ip you know disclosures all that type of thing good to be aware of what the rules and what it means to be participating in the itf so so good to take a look at that if you haven't already suggest you do it asap um the agenda as i said we've made it to the closing um and after just i'm not gonna spend too much time going through slides we're going to spend the bulk of the time hearing what all of you have accomplished i have a screenshot there on the right uh that i took maybe about an hour ago and uh don't panic if you've updated it like if you've added yourself to that list more recently and you're not in that slide we're not going to go based off the slide we're going to go based off what's actually currently on that wiki page or i was just messaging with alex in the background he uploaded all the slides um all the presentations that have been put in the um our project repository let me go forward a bit uh if you uploaded your your presentation to this repository those have been imported into me techo so we can actually run through them um from me teca as opposed to you having to share your screen you can drive it from me techo so um uh barry if you're up for giving that a try when we get to that point we can try going through and doing it that way uh so far it's been working great for me so i think you may find that an easier way to go through the presentations as well uh but i'm getting ahead of myself a little bit i also wanted to share a reminder about the presentations and hopefully you were all here for the um the kickoff and uh these presentations are meant to be just you know very brief presentations really uh just covering"
  },
  {
    "startTime": "00:06:01",
    "text": "what the problem was you were trying to solve what it was you accomplished and then just some interesting highlights about what you learned things you're going to take back into your working group and perhaps some interesting collaboration with other sdos or open source organizations or kind of you know other people outside of the ietf working group or even if you collaborated with another ietf working group that maybe isn't as deeply involved with um the standards that you're working on all that stuff's just uh you know i think it's very interesting to hear so now if you can do all that in five minutes uh that's fantastic and just this is meant to be a conversation starter so don't feel like you need to cover all the nitty-gritty details really just give us that the high level that would be fantastic and we already covered that and so with that we're going to switch to just walking through the presentations and [Music] barry i'm happy to uh turn it over to you and uh we didn't get a chance to discuss this before barry but uh do you want to just try driving it from the uploaded presentations into me techo yeah i can i can do that or each person can write their own using the uh yeah the slides let's let's try that so um i'll call you out and use the um the share preloaded slides uh option which is the little piece of paper with the corner uh dog ear click that and we'll allow you to share the presentation and it will you know pick your own presentation and run your own slides if you have trouble with that let me know and i will run your slides for you so why don't we go down in what appears to be alphabetical order in the wiki um so and rather in the in the github uh"
  },
  {
    "startTime": "00:08:01",
    "text": "so we'll start with bmwg and the next one will be the dns header people so bmwg whoever is doing that requests requests sharing pre-loaded slides is there somebody from the bmw group who wants to talk you can just unmute yourself and there we go uh there you go okay can everyone see my screen your screen go ahead okay so i will be doing our projects in his hackathon 112 is a containerized infrastructure very much green so the main goal of our project is in continuously hard carton theory is to figure out control networking performance in various crystal oxygen and the result will be contribute to our draft consideration for by measuring network performance in container right infrastructures the two main features in this graph implement in container right culture with various network models and verifying performance impacts depending on configurations testing now in the hackathon 112 our main i mean a project is to do the very much"
  },
  {
    "startTime": "00:10:02",
    "text": "parking lot rate in multiple scenario with different acceleration technologies for 2v3 exactly bvp and obs dbdk and we consider the luma proceduration for so to see how it will impact the performance of cp switch so inside the head curtain we the scenario we consider is a multiple scenario so in our one one one ietr package so we consider the single signal boss scenario three only consider the multiples so basically the traffic between chinese from the generator to the toolbox and v-switch and it's about case mantas cni and use the spacing hybrid used to create multiple interface of vnf and we use a layer 2 folding application at the plugin inside the port for the luma alignment we consider inside the multiple scenario we have 6 scenario it can be split into two parts the first part will be the physique same with the luma node and the net nfc and the second be the v6 different window rotate the nsa and we denote the cl cf1 in the port that received the copying package from t zn and then change it to the cl2 cl2 will be the poster chain with the packet back into the traffic generator and it's a similar way both to cnf upgrade in different loop placement operating shipping incoming packet cnf and visibly considered so and we don't consider both choosing that in different node with 6k but it will degrade performance but we have observed in signal plus scenario this is our configuration for our testbed and this is a design basically we have our master node in the worker node to deploy the port and we have the traffic signature to transmit the packet to them so this is a result that we have"
  },
  {
    "startTime": "00:12:02",
    "text": "achieved throughout our ideal hackathon the thing that we learn from this result is the first thing that in multiple scenario bpp are performed obs and the second is the result that we observed from the luma alignment the first thing is the alignment of district and x and the element of visual and snake is almost the same it plays on different notes when we slightly dig the performance in a higher packet size and the second observation is cnf and d switch we also set a separate inplacement of cnf in significantly degree performance by 10 to 50 percent and finally we is a receiving bracket cf1 this week we uh observe the reverse the reverse result for obvious dpdk and vpp with oft bdk cmd and this week in the same note we have higher performance meanwhile in vip cnf and district in different node it has higher performance so for the future we plan to travel through the cf on basic and numeral alignment results and we consider multiple vc case where we will deploy one vision in each one node and we will plan to discuss our utility interesting result with other creative community so to wrap up our presentation so this is our team member and we cooperate with the i2 nsf and fpua team from the sktu team thank you for listening thank you tran are there any questions for tran okay hearing none uh thanks again tran uh good good job um"
  },
  {
    "startTime": "00:14:01",
    "text": "next one up is the dns editor uh willamah yes uh so i'm i'm going to try this slide sharing thing yep let's see uh and the next uh next step will be i2nsf so get on get ready yep your slides are up go for it okay yes so um yeah so the the dns hackathon was actually just a single project which i did with uh someone else uh tonka park my colleague and uh so it's not not really denas but either and uh so what's this about so um roy irons has a draft which has been adopted not so long ago at a dns working group which builds upon extended dns errors extended dns errors is a mechanism in which a resolver or authoritative nameserver can send back a detailed information about error that occurs to the carrier for example a resolver can send back to the step resolver or a normal user that denysec didn't validate because the signatures expired or there was a ds record with no signatures in in the zone or that kind of things so the dns air reporting draft builds upon that but instead of informing the clients what went wrong it informs the authoritative servers"
  },
  {
    "startTime": "00:16:01",
    "text": "that we're serving the broken songs about the error and uh so it's actually extended dns error reporting and therefore i uh so i i know that roy argens who's on this draft and also met larson they don't like the funny acronym either but i i do like it but and therefore i called it either so this was discussed one and a half week ago during a dinosaur interview meeting and there was what was discussed there the the mechanism consisted that or meant that resolver would send a edns option to the authoritative saying hey do you have a reporting agent for me where i can send my errors and it was noted during the interview the interim meeting that this breaks some authoritative resolvers so as a alternative it was suggested that instead of the resolver requesting the edns option the authoritative could also just send the eds option and solicit it and it was suggested that resolvers would be much more resilient to that than authoritative towards unknown options so i proposed that it would be a nice thing to measure at the upcoming hackathon and some people agreed as you can see in these meeting notes minutes now two hackathons ago we also implemented the reporting agent reporting in nsd our authoritative name server but this time we went a different way"
  },
  {
    "startTime": "00:18:01",
    "text": "especially because the authoritative would send it insulated unsolicited anyway we made a program to do this in a beefy bpf and if you haven't heard about this yet it's not the old berkeley packet filter from tcpdump but it's you know it's very exciting and happening you can run programs in the linux kernel or very close to the network card or even on the network card hardware and what i like about it too we have been playing it with with bpf and at in our matlabs is that you can augment existing uh name server installations so we made something which is named server agnostic but you don't have to anticipate it beforehand if you have this name server and you want to have this feature added that it can report its reporting agents then you just load this bpf program and it will work but if i work with bind with not with nsd doesn't matter so that's it's pretty cool here's the git repository there's all all sorts of dns experiments in there [Music] and here you see example uh so i i have this server either dot lobs dot nl and it's running the either domain and i have not anticipated the dness error reporting yet but to do to enable it i just have to do this clone the repository initialize the submodule go to the directory with the option make load and i'm done and you can see here the result"
  },
  {
    "startTime": "00:20:00",
    "text": "from a dick query that it is indeed reporting the reported agent in an edns option report.netnetlabs.now and here is a little piece of the c code of the bpf program you can see how to configure it with the reporting agent and what option code to use so i think that's really cool and we also did ripe atlas measurements to see if resources would break if they get this option unsolicited so i also created a server with the domain that does not send this option which is called meter dot internet labs orton l and here's the vipe outlast measurement that targets this server and then also the the server rich doesn't option i did a very simple measurement so ripe atlas has 11 000 props currently i think but for the hackathon now i did it with just a thousand uh doing it with all of our losses a little bit more work but it was good to have this experiment anyway i think so 962 probes anticipated the program that processes the results is there in a link to and what we learned was that 99 of the resolvers don't have an issue with this option and that the remaining 1 is also a little bit unclear because actually we had more answers on the domain that did send option than on the domain which did not send option which was the baseline measurement so"
  },
  {
    "startTime": "00:22:01",
    "text": "but i think so from initial guess that support is okay but of course you know there needs to be we need to be do like large-scale measurements to to be sure and be certain that this can be done for dns or reporting without causing problems another thing uh what we learned and more that's more discussed is that this is like dmarc right it gives operators of authority of domains dns operators serving authority domains authoritatively confidence to deploy dnsec because they get feedback from what is happening on the internet if something breaks etc so it might be interesting to develop a new draft at the rtf and which would do driver and dns right that you can try the nsx but if validation breaks the answer will go to the clients anyway but our error will be reported to a error reporting agent so that might be some interesting future work and that's it we just worked with the two of us on this because you know everybody is busy and it's it works better on in a live meeting but that was nice to do anyway and i think that maybe we present this a bit more elaborately at dinosaur next week so that's it if anyone has any questions then i'd be happy to answer them thanks welcome welcome yeah i mean um yeah thanks for the presentation i found uh i mean the the potential of helping with the kind of dns sac deployment problem"
  },
  {
    "startTime": "00:24:01",
    "text": "is as i understand it is i mean it sounds really uh interesting and an encouraging thing to to try out um also i was uh it was interesting to hear you bring up uh you were talking about ebpf too right at the beginning yeah that's prevalent i think in the networking space edpf gets uh used a bit and now i'm hearing it used a lot for dealing with security um things that just i don't know it's it seems like a really neat uh kind of toolkit no way to to be able to use so uh interesting to see you using it the way you did um it makes me think i need to look into ebpf a bit more because it seems pretty pretty powerful pretty flexible yeah absolutely yeah yeah we sort of made a toolkit so this this uses parts of that toolkit to to implement this this option and it's cool yeah because you know it's also a feat a feature can be implemented independent of what you're winning and it's very fast very suitable for dealing with uh you know of service attacks and those kind of things cool thanks thanks for sharing thank you other questions or comments okay okay thanks again willem and next up is uh i2nsf followed by ipwave both of them are paul and your screen request is approved go for it hello uh can you hear me yes okay so hello uh this is uh john this is i2 nsf hackathon project report so this is a poster for this project basically uh this hackathon project case"
  },
  {
    "startTime": "00:26:02",
    "text": "we we don't have your hunger slides up yet okay can you see my slide no oh no it says my screenshot has been started but uh oh okay okay okay okay okay there you go okay good okay okay uh my name is tim perjung this is i2 nsf hackathon project report but this is a poster for itunes f project basically this hackathon we want to implement distributed network auditing system for itunes f framework using hyperledger fabric as a distributed network auditing system so um as you can see this is i2 nsf framework so i turn as a user giving the fallacy and then secure controller translate into level policy for the nsf security enforcement this hackathon project case item accept the analyzer collects the monitoring data from nsf using a monitoring interface and then that monitoring data are stored into hyperledger fabric as a distributed database after that secure controller has a web server and it display the monitoring data into web so this hackathon project we use the distributed database based on"
  },
  {
    "startTime": "00:28:00",
    "text": "the hyperlaser uh fabric so you can see itunes of analyzer collect mounting data from nsf amb and then using nested a5 that information will be delivered to hyperledger the organization node this is this should distribute to the database node and and then using order and confirm that um stories and then the data information delivered to control law okay and then it can display that motion data into web so this is at the push of nsf modding data to distribute the data based using rest api basically i2 nsf delivered this data into hyperledger after that this distributed database receives that nsf modding data from i2 and sf analyzer so you can see this is a json format so um right hand side so we display the monitoring data okay so this is the system resources such as memory disk okay something like that and then right hand side is the network uh traffic so this incoming traffic is over that some threshold means ddos attack okay so let's take time we use the centralized database with the mysql this time we replaced mysql with the hyperledger as the distributed database so what we learn the usage of distributed database can tackle the possibility of data templing"
  },
  {
    "startTime": "00:30:02",
    "text": "such as a sprite supply chain attack and also the distributed database system [Music] also denies the failure of single point because we are using distributed databases like the blockchain overall the distributed database system can improve the security and reliability of our itunes framework so next step this time we just stored the i2 nsf nsf multi-data but next time we try to store more data and information such as security policy and nsf capabilities this is open source project at github you can get the source code from this link so this is one on two source code and also we have the demonstration video clip you can click and you can see our demonstration so this is the member information we work for hackathon with the sunshield university bmwg working group and ipo web team this is my team members so also this time you also work for a hackathon project so korean uh gathered in the busan west in joseon hotel in busan we work for hakkason bmwg um i2rsf and ipwave team so this is the sponsor information so appendix explains how to install opelate our distributed database system for itunes app thank you for your listening questions or comments"
  },
  {
    "startTime": "00:32:04",
    "text": "thank you paul questions for paul on i2nsf before he starts ipwave okay okay let's start if you wave okay so can i start if your wave report a very can okay please do it okay yes but it takes a minute to unmute so it got cut off okay sorry okay may i start ip wave report you can see my slides right okay okay uh this is the ipo wave hackathon project report so this is the hackathon project poster basically this time we implemented the ip wave context over navigation protocol is called the cmp so robot car and the web server can communicate with each other so we using ipweb cmp beaker information option type is delivered over a world wide web double 3c pi ss feeker information service specification we used we deliver information using that standardization format also we demonstrated the text last time we used a wave such as a tsrc so european folks they are"
  },
  {
    "startTime": "00:34:00",
    "text": "popularly using um simply takes a cellular feature to everything so so in this case we use the simulation based information was done so i said this ipwave hackathon case we had two parts first one is the simulation second one is robot car based implementation so what got done so we demonstrated our ip wave context of a navigation protocol message can be delivered over cb2x using simulation also we implemented the ip wave draft and w3c standard it is called the pis s vehicle information service specification deliver figure signal or sensing information to web server so the first part simulation case we use the sumo for vehicle mobility simulation and the lower part is the omf plus with the cv2x module to simulate the car uh communication based on uh cbtx okay so this figure shows our information implementation of omni plus plus so we implement the peaker structure to support the cv2x also on this is ipv6 stack especially um the icm 5006 is for context over navigation protocol okay so we used this stack so last time uh we used elto.11 waiver okay so at the eleven ocb"
  },
  {
    "startTime": "00:36:02",
    "text": "uh stack so we used on this kind of mustache this time uh we demonstrated the the feasibility of cb tracks the cbu text is a standard for cpp so in future i believe ipwave can take advantage of wave stack and see if we to access stack for vehicular communication so this is a remote server and the vehicle so using ip wave vehicle mobility information message can be delivered over w3c standard such as a free iss figure information service specification also a general pre-access figure signal specification deliver vehicle sensor or other signal information can be delivered over viss format so this is also open source project so you can uh get the first link is our simulation for sibu text the second one is a local car based implementation card so um you can click and then uh this a ai on r1 robotics load car can deliver its sensing information or figure speed information using wi-fi to a web server a second part demonstration this link uh demonstrate the civil text based context aware navigation protocol okay exchange so we work for"
  },
  {
    "startTime": "00:38:00",
    "text": "a whole week uh with the sunshine university vm wg and i2 nsf team work together so this is a team member information so my ph student the pnma and my the master student chun hee maekwon so this is our um korean um hackathon teams members so this is a sponsor also we have appendix for cbu access examination and also we have index for our local car implementation and manipulation for people having uh interesting for our ip wave project thank you for your attention and interest any questions or comments thank you paul um anybody thank you yeah um hi paul hey yeah thanks for uh great presentations uh you always bring great projects to the hackathon so appreciate that um the cd cd2x um that was a new one to me which uh which standards organization uh defines that cebu text is 30 cpp the third party yeah partners right for 5g you know lt right so recently the 3gpp uh also working for 6g but basically uh korea case we deployed 5g i believe the united states also solving the 5g service right for a cell phone right mobile smartphone so the vehicle case also the vehicle to everything case cputex model 4 case provide the uh adult fashion communication which means the vehicle"
  },
  {
    "startTime": "00:40:00",
    "text": "can communicate with without the base station okay so in that case uh we can provide the communication among beakers to avoid the accident also some detect some obstacles or headers you can notify every vehicles of that hazard so civil text will be the use per so european country case carbenders the the adopted civil tracks for um vehicular communication still the united states case the dsrc wave case they consider but i think uh two technology will be uh co-exist in future so that's why if you wave working group needed to consider cv text for mac and the file yeah protocol for hyperwave navigation or other yeah application yeah communication technologies okay okay yeah well thanks okay thank you charles what else questions if not uh thanks again paul and yes thank you being a regular hackathon participant and always bringing your students with you that's great thank you okay thank you and oliver you're next with uh the aspa um okay can you hear me we can wonderful okay so uh that was our first active hackathon what we did i participated some before as"
  },
  {
    "startTime": "00:42:01",
    "text": "listening in but this time we said uh that we wanted to make one on our own um so the the work what we what we did is um and i just go right away in here so the the we've worked since a long time in bgp uh security and uh we've worked on bgp seg path validation uh the bgb orange validation and for that we created um reference implementation reference implementations and a large software suite what we know employ also for another uh work at the sider ops working group uh the aspa autonomous system provider uh authorization and the goal of that is of this hackathon what we started this time is to lay the groundworks for future interoperability tests between different implementations and to create large-scale um tests that then can be used to verify the different validations performance etc etc etc um a very quick explanation how the system in general works so isps or operators of autonomous system routers basically register their data in the rpki database and that then will be queried relatively often by validation caches that go out download the x519 certificates etc and then create something we can call it maybe a white list"
  },
  {
    "startTime": "00:44:01",
    "text": "it's the aspa information that describes the customer provider relationships and sends these ones then to the router or a validation engine that the router employs and our interest for this iatf for this hackathon was the communication between the validation cache and the router so um the tools what we used was basically our list bgp srx software suite um where we have the aspa verification implemented actually we have and currently the draft is version 8 but there is an algorithm correction already that was introduced some iatfs back that will soon be added to this uh standards draft and we have the implementation for that the other thing is the [Music] rfc 8210 is extended uh to now also carry aspa objects and we made the reference implementation for this um so we wanted to use our test harness to basically um yeah run larger scale tests um so what what was our task for this hackathon we said okay we want to go out we take sample internet scale asp data and we use the kata reference data to specify they have very good algorithms to specify the link relations between two asses and then we take that and we create the input file format for our software what basically looks the following way you say you have"
  },
  {
    "startTime": "00:46:01",
    "text": "the aspa you give the f e that could be uh b4 b6 then the custom is and a list of providers for these iss and um what we then also did we went out to route views and got some um egp updates um maybe follow another goal like so what what did we do so we went out to the cater and uh we got data actually i think it's the data from october 1st 2020 because currently cada is re revamping their algorithms and do not provide the latest uh internet data but for us it was perfectly fine because we just said we wanted to have something that if we use already the internet traffic we wanted to have data that is [Music] compiled out of the internet topology we this data produced around 70 000 customer registration for us and that has over 180 000 link relationships um then we what we did was we wanted to down select that and we did that where we said okay if i don't for example i take a 100 routes a thousand routes 10 000 routes so what we did we created we created tools that go out and create a unique as path we were not interested in the prefix because aspa does not look into the prefix it tries to identify route leak space basically so if there's a route leak um for one prefix and most likely it's also for other ones the the validation here"
  },
  {
    "startTime": "00:48:04",
    "text": "is basically on on the path itself um so what we did was we down selected out of around i think it's currently around 800 000 prefixes um uh if depending how you how you run the b2b dump on the mrt files you get up to also 800 000 updates we down selected that to around 100 000 unique ones and then from there we we said okay if you if you create a thousand we only want to have the cata data that contains all asses within these uh this is within the update data stream so um because the other ones they just would lie in the system dormant so we created data sets for 100 500 800 1 000 10 020 000 unique as path and um then we performed the aspa validation very quick explanation on the validation what it is so if i have a valid then i didn't um identify your outlet if i have an invalid then i detected a route leak if the uh outcome is unknown then i don't have enough asba information to make any determination either way and if an ai set is in the bgp path then it's unverifiable um that is one of the results what i just compiled yesterday night where we got everything up and running with all the scripts and it's to be taken with a little bit of grain of salt because um it's a relatively small data set i think it was around 500 routes and depending uh it becomes much more interesting i guess if we go on a higher one but one interesting thing to see for example is already that we selected a large scale isp and we just said okay if this isp is my provider"
  },
  {
    "startTime": "00:50:02",
    "text": "uh how many how many uh prefixes or how many updates i received from them how many paths are valid and that is 94 and how many are invalid that basically means result into route leaks three percent and uh another three percent i could not make a determination now if i turn that around and say that now my isp is my customer then of course i uh i don't expect 94 percent of valets and that's what we definitely can verify here so we have 14 of wallets we have uh we identify 18 of route leaks and 68 the data is not there to to make any distinction in either way um we didn't had time yet to really dig deep into the analysis of that whole thing um that is something that has to be done now going forward but uh for this week we basically wanted to create the the infrastructure that one can start working on that and so what is the to be what needs to be done um currently our experimentation runs with one implementation on the test with against one pier it starts becoming interesting running this against multiple peers especially when we go into the performance testing and hopefully we will see other implementations coming between today and the next hackathon so then it might be interesting to compare validations with other implementations other than ours to make sure that the validation results are all all the same etc further further analysis will be of interest is gradual deployment because what we did here right now we had an almost hundred percent deployment of aspa and if you roll out a new technology that is not what happens um so the interest uh there is to see"
  },
  {
    "startTime": "00:52:00",
    "text": "at one at what deployment rate start aspa becoming really um making a positive impact on detecting route leaks the software code of the bgp srx suite is on github if you go on github and you just search for bgp srx then you will find right away the uh the software suite the code what we developed for this hackathon i still want to clean it up a little bit before we put it up on github um but it will be made available as well and i didn't make a final decision yet if you put it on the on the ietf hackathon github part or if we also run it in the examples of ours uh but depending what we will do uh we will have it at least linked from uh from the nist github uh part so that you can find the code the the scripts and and and so forth um yeah that's all what i have for today if there are any questions please feel free to ask and thanks oliver questions for oliver okay hearing nothing thanks again oliver appreciate the presentation and the next one up is the ansible api generator and the one after that will be whip so get ready yeah can you hear me i can okay just waiting for your screen and we see your screen got it okay okay okay thanks so hello everyone this is tufang from huawei and i'm here to give a very brief introduction of our project the customized enable api generator so the motivation of this work is that we"
  },
  {
    "startTime": "00:54:01",
    "text": "really appreciate the simple and efficient network automation solution provided by ansible to manage network devices and we also know that the netcaf young provides a standardized programming interface although the young schema is indeed a standard how the data itself is represented may vary depending on the vendors however not all vendors provide enviable api to manage your device through netconf and when the native vr modules even though there are some integrated solutions between edible and different vendors the generalized enable apis cannot meet customized requirements so we really hope that there exists a tool to generate any desired anabolic apis so that they can be used directly to communicate with network devices through netconfion and these apis can be used to greatly reduce the workload of developers and allow them to focus on their own business the following gives some of our specifications the related drafts which which may be used in this project so what got done the our achievement is the antibody and automatic and the api generation tool the generated enable apis could be integrated into enable framework automatically and also supports customized input parameter check or functionality and by making use of enable gene we have already successfully delivered l3 with vpn service to have a device through enable plan book so the following diagram gives an architecture of animal gene and the entire processing flow the user inputs are related young modules and their"
  },
  {
    "startTime": "00:56:02",
    "text": "customized api description xml files these customized api profiles are used to describe your desired ansible apis which will be called when delivering netcup message to the targeted nodes it's very similar to netcover message but without values carried it could be an edit config get or get config or rpc operation defined in your modules the young passer and xml passer will check and pass the young modules and xml description files respectively the results as the input of adapter which is responsible for generating the internal object of animal gene and then identified by the antibond module great generator which will generate the angle apis finally the generated apis will be deployed in anger environment automatically which works as an edible module when we issue a net conf request through plan book the related apis will be called and complete the configuration management task so during this hacksaw week we learned that the documentation is important for involving others is even as important as code we can't expect everyone to know how to use it just by reading the code and the second lesson the second lesson we learned is to test early and often so we can have more opportunities to catch backs and more time to fix them and also i would like to thanks to panglio tanglio viva and benson for visiting our project and providing a very valuable suggestions and great input regarding the next step we would like to support the latest version of and the"
  },
  {
    "startTime": "00:58:01",
    "text": "ball and we would also want to explore this to define our enterprise profiles more efficiently and besides me there are frank ching bin wai and xiang thank you guys for your efforts and the related outcomes of this project are giving here the open source code and a video demo please feel free to read the video to them so that you can get more detailed information about this project and uh further contributions are welcome yeah so this is the end of my presentation thank you very much thank you and are there are there any questions or comments for chuffon okay hearing none thanks again thanks for the presentation and for the work of that lorenzo you are up with uh whip [Applause] and lorenzo has the magic so i don't approve his uh sharing oh yeah sorry i didn't i didn't realize that apologies i don't care go for it okay so this will be a brief report on the interoperability tests on on weep and just to just to summarize producers that are unfamiliar with it we comes from a a recently formed working group called wish which stands for webrtc ingest signaling over https and whip is the name of the protocol that is being standardized there and it recently went to version zero one which was actually the uh the the main uh the main version that we tried to to test in these in this interoperability test and we basically wanted to cover two specific"
  },
  {
    "startTime": "01:00:01",
    "text": "aspects so first of all we wanted to make sure that we client and servers could actually talk to each other using the web signaling protocol itself but we also wanted to take advantage of this chance to also test the webrtc exchanges beneath mostly because most of the endpoints that i'll talk about are not based on libor rtc which means that they do not share the same library that browsers use and so it was a very unique chance to test heterogeneous webrtc stacks instead which can be a bit more problematic at times and we actually did a few uh first interrupt rounds among a limited set of implementations a few weeks ago that i documented in a blog post that you can read there if you want and at the time we had three clients and three servers interacting with each other and basically the results that we got from that test that eventually were successful basically helped us identify a full set of key issues in the zero zero the version of the of the documents and basically zero one uh most of the issues that were addressed in zero one actually came out of that interrupt round which was quite helpful and this time around we wanted to test more implementations instead and so we ended up testing four different service implementations including one that i wrote myself one that julius wrote galen sergio wrote the integration in the medical standpoint and cameron integrated this into his own sfu called the cfu and for clients we had six different clients instead that we could test and both servers and clients basically almost all of them used different webrtc stacks which is why it was quite interesting to see how they fared with each other and the end result was in a nutshell these so it was mostly successful and actually more successful than the picture there suggests basically just to give you a quick understanding a green smiley means that everything worked fine out of the box a yellow yellow smiley means that we had to tweak something in other client of server to get something working"
  },
  {
    "startTime": "01:02:01",
    "text": "red meant that nothing worked and so further investigation is needed and no arrangements that we couldn't test due to something that was actually unrelated to whip itself so like course issues or stuff like this and i try to summarize most of these reports uh the different results in these slides i will i will not go very much in detail in all of these uh wrappers because they might take more than the five minutes that i have but just to give you an idea for instance uh my client died when you try to talk to this to the deadliest a few web server because it made some assumptions and so actually some assumptions that were made out of the documents were because of some of the problems that we experienced in this case my client assumed that the location header was always available and detect field didn't provide it and so basically that ended up in the crash that i eventually fixed and so and and there were a few similar issues as well i should clarify that in this slide i meant i i put weepy which is the uh the raspberry pi based client that team wrote uh that basically didn't succeed with neither galen and the decipher and this was true up until a few hours ago and the cause was basically a nice interrupt issue between the team's stack and the pi on stack which is go based and eventually team managed to find out what the issue was which was actually more related to the fire to fire release rather than ice itself and he also had to address again an assumption that the pion stack made with respect to the usage of mid in webrtc which again was uh an interesting uh side effect of this test that's uh that helped helped us identify a webrtc related issues rather than a weak issue and then there were some other issues related again to either to assumptions or hard-coded assumptions because for instance some clients couldn't talk to some service just because each of them were hard-coded to use one codec rather than another and so in this case this isn't"
  },
  {
    "startTime": "01:04:01",
    "text": "really a failure in how to how they use whip but mostly a failure in actually ended up into a successful negotiation between the two maybe because the client only offers bpa decline the server only accepts h264 and so you end up with a session that will not work basically and apart from this it was mostly issues for instance related to self-signed certificates not being accepted which made it a bit harder to do tests locally these sort of things and a few other assumptions like course issues or missing support for better tokens which is something that we actually want to look into sooner or later and really to summarize what we wanted to really focus on in this first uh first this first round was basically really focusing on the basics so trying to get the webrtc stream to be published using with possibly tweaking trickling candidates and checking whether or not i know your video stream is actually consumable on the other end so we didn't want to really push it beyond that mostly because this was the first time we all met together in order to to try and do a larger test among among each other and what we really want to test next is actually going a bit further than that so for instance making sure that the authentication using tokens actually works as specified in the documentation how to properly use the location and link headers to uh to address whip resources and automatically configure stun and turn and this would actually be useful to do in environments that are actually more limited in terms of actually setting up a peer connection than the more open setups that we had right now addressing also ice restarts and related risk conditions which is something that we are actually discussing right now in the working group and it's something that we haven't had the time to actually look into as as far as the different implementations are concerned plus some issues related to sessions cleanup which is which isn't always done properly by your clients and service which means that you can end up sometimes with door front sessions and things like this so it's definitely something else that we need to to address in in future"
  },
  {
    "startTime": "01:06:00",
    "text": "interrupt tests and to conclude this is the uh the interrupt testing team that worked uh that was together in these past few days so sergio is actually the whip champion uh for this for this hackathon uh then there was me there was tim panton then jenkins cameron elliott and alberto gonzalez stoic as well i don't know if there is any question for me thank you thanks lorenzo questions or comments for lorenzo so yeah thanks for the presentation i have a question for you which uh which video conferencing tool did you use for your team to uh collaborate well actually yeah that's uh the funny thing is that um webrtc developers tend to tend to communicate in real time as as as as low as the community the real-time communications tend to be as low as possible so we try to use mostly synchronous communications like we we might engage on matrix or twitter or any kind of messaging platform so it was mostly this kind of interactions rather than having a conversation like we're having right now we tend to be quite lazy and just just chat instead of talking to each other okay and just one other thought it'd be interesting on your your slide some of the problems you identified you uh you've already fixed so to have a i mean i'd put like an extra big smiley face on those the ones that uh yeah that's true yeah yeah i had some problems trying to actually frame this one because the yellow ones should actually be a bigger smile because it's an issue we fixed rather than something that we had to fix and so it was a bit more of a frowny face so it may be more of a messaging uh a pure communication on my side so i'll definitely fix this next time thanks for the feedback thanks a good great project and and thanks for sharing thank you other comments for lorenzo"
  },
  {
    "startTime": "01:08:03",
    "text": "okay thanks again lorenzo and steven you're up next with uh augmented packet header diagrams it can use you know yes we can see the slides we can hear you excellent so hi everyone i'm stephen mccusin from the university of glasgow and i just want to briefly talk about the work that we did this week on the machine readable documents and their tools project that we proposed alongside mark petty hugging so this is a fairly broad project and it's it's really an umbrella for a variety of projects that are trying to use tooling to automate different aspects of the draft authorship and standards publication processes and of the sort of subsequent processing of these documents so just give an idea of what we're talking about this includes tools that are used to generate standards documents or drafts in rfcs so that's things like computer specifying that mark will talk about next it also includes formal and sort of semi-structured languages like abnf and yang the augmented packet header diagrams work that i'll talk about and the the quick packet notation that some of you might be familiar with on the other side we are also considering tooling that derives meaning from draft so that might be using natural language processing run over drafts and documents or it might be parsers for these structured languages here and in terms of what we want to do with the documents we're thinking about things like automatic parser code generation so generating parser code for the protocols that the documents describe automatically from the documents themselves or producing mathematical proofs that demonstrate"
  },
  {
    "startTime": "01:10:01",
    "text": "that the document describes a protocol or something that is provably correct as i say this is quite broad and we're really trying to be as inclusive as possible for all of these techniques and tools and languages but this week we worked on two main projects as i say mark's going to discuss his work on computer specifying but i'll just briefly update you on our work with augmented packet header diagrams format so just to briefly introduce this format we found that most documents that are specifying protocols do so with a sort of broadly similar format so you can see in the right hand side here you know we have this ascii packet header diagram and then below that we've got this description list of each field describing the length of the field and its contents so what we've done with our work is we've sort of regularized this format so that we can parse it out of the documents that use it and then generate parser code for the protocol that that document is specifying and what we've done is we've done this with really minimal change to the format as it's typically used today and the idea here is that we want to limit how much effort document authors have to put in to changing their documents and the way they write their documents to be able to support our format and this really balances the sort of structure and uniformity that we need to be able to pass these descriptions out of the documents with the flexibility that you actually need in the real world practical use of these documents and we've got prototype tooling that supports this input format so you run our tooling over a draft and it will produce a parser code for any protocols that are being specified using our format and it produces code in rust that will then let you parse out packets in that protocol we're seeing some adoption of the format so the tcp bis draft that's going"
  },
  {
    "startTime": "01:12:00",
    "text": "through tcpm uses it and so in essence you can take that draft run our tooling over it and automatically generate a tcp parser of course there's a lot more information about all of this we've got a draft that specifies the sort of rationale behind what we're trying to do with the format and that describes the format itself the draft also has pointers to the github repositories with uh all of our prototype tooling in it and any contributions and any comments on it is are more than welcome so in terms of what we did this week we had lots of very good very useful discussions about the particular language that we use in our format so our format uses sort of structured english phrases um to indicate where we start a definition we had lots of discussions about how these phrases could be improved so that they're actually better for you know humans to read rather than thinking too much about machine readability beyond that we also considered what we might be able to do to mark out the structured text components of our format so that you know when we pass drafts to the rc editor for publication that they're not actually edited out and no longer become parsable by our tool and finally in terms of our prototype code we started working upon adding flexibility so adding more languages that we can produce um parsers in and adding robustness so we want our tooling to be able to to run over every draft even if it doesn't include our format um and we've really started working on adding that robustness and uh in terms of what we do next that's that's the sort of two strands that we're going to pick up on as we move forward with it um so all in all it was quite a productive week a lot of good discussions with people that that haven't been uh engaged with the work previously um so it's been really good to see some new people and thank you to everyone that came along to the project table and discussed their"
  },
  {
    "startTime": "01:14:01",
    "text": "work with us really appreciate it and i'll take any questions anyone has yeah um uh great presentation one question uh if i wanted to try this out on a draft that i have or even an rfc or um i guess probably on a draft right because i'll probably have to change some things or yeah but uh anyways where where can i go to get uh kind of a prototype version of this the the best bet is to uh to find this draft um on the data tracker and we've got links to the the github repository at the bottom of the draft and so all the tooling is available it's the tooling itself is written in python um the instructions should be hopefully reasonably clear but if there's anything at all that comes up as you try to use it then happy to take any any feedback or any comments great thanks steve cheers else any questions comments okay thanks steven and mark europe can we see the slides hello yep we can hear you okay go for it okay why are my slide truncated on the left right okay all right okay uh hi so my name is mark pettigoner as stephen said uh participated in the project on umbrella which is about uh machine readable uh specifications"
  },
  {
    "startTime": "01:16:02",
    "text": "so this will be i will start with a quick introduction of what computer specifying is because i think that there is less than five people on the planet who knows what i'm talking about so uh he started a long time ago and the goal of this project is to ensure that example in rfc are correct because sometimes they are not and because programmers generally look at example when they write their code and not add normative text then we get implementations that are not interpretable so basically a computer specification is a document which is written in the format which is called ascii doc which is a which is not a markdown but looks like one and the big advantage of this format is that it's extensible without having to modify the code as a xml2fc hondura is in fact provided by the metanormal project that some of you may may know so i wrote an extension well extensions to ask askidok that permits to add code inside a document in the same document that will be your internet draft or your lfc you can add code and the most important thing is that you can evaluate some of this code and put the result of this evaluation in the document itself at the same time it's generated so here an example of that which is taken for uh 88 like something so the idea is that on top the block on top is a computer right specification right you have code which you can recognize because there is a greater than which is called a bird mark on the left so this is code and then underneath this you have text where you have this um code macro code"
  },
  {
    "startTime": "01:18:00",
    "text": "column and uh and the code in between bracket which will be evaluated and the result the result inserted into the generator text so in the middle you have the command which is used to to do that and there is one command that does everything and it generates xml to lfc text html and pdf format for this document and on the bottom you have the result what will appear in in the actual document and you can see that the macro is replaced by the values that are in fact calculated instead of being filled manually by the author of the hrc the problem of that is that in my opinion this is not enough right programmers are still programmer and some their main output is are still it still bugs so i i needed something better than this i can guarantee that the example are correct right so there is two way of doing that you can have verification tools that verify that the example are correct or you can go the less easy way which is to generate examples that are already correct by construction right so you still can verify them but if you forget to verify them they are still going the problem is to do that is that you cannot use any programming language right all programming language are equivalent to each other except for their type system and in this case we need a type system that can encode higher order logic right which there is probably free programming language that can do this and the one that shows this it too which was designed at the university of saint andrews in scotland so i am done with the computer"
  },
  {
    "startTime": "01:20:01",
    "text": "specification now what i am working on is to provide some library in idris to provide to solve common problems that internet drafts also would have why and i'm working on a lot of this stuff with one of the simpler and most accessible to everyone is a b and f right a lot of internet of rfcs contain abnf so you can see on the bottom an example of in this case how this library module works right so i design uh the domain specific language a dsl that can be used to define a b and f grammar right so here i defined alpha which is probably the most well-known rule for abnf and then here on the top i insert the the serialization if you want of this and the result of the serialization is what you have underneath so not only it print a correct syntactically correct string but it also format it right it it does pretty print printing so if there is not enough size in the line it it will wrap up the line and and do the white thing according to analysis 52 34 but this is not enough to generate an example remember my goal is still to generate an example which is correct so my first idea was to generate example from an ibnf but this is a terrible idea and the reason for that is that an abnf cannot describe all the constraints that you have in the pdu it is supposed to describe right and i found a really nice example nice because it's shorter it's uh some symbolic expression that everyone who wrote"
  },
  {
    "startTime": "01:22:01",
    "text": "lisp code uh should uh have seen right and this is a very simply of a version of which sx used by basically from machine to machine transfer right here if you look at the second rule token you can see that there is a number a column and then a number of octa octet the problem is that the number of octet is it should be in fact the value of the number before right so there is five column you should have five update after this you cannot encode that that in a b and f at all right so this is why this is a bad idea to to to base an example on its a b and f now what we want is to be sure that an example with generate is also correct according to the ibbnf so the right way to do that is to generate a type for a symbolic expression and eventually a dsl that make it easier to do so sx are easy are simple so the the code fits inside the the slides the slide right but how to be sure that the printable value of that type which is an example is correct according to the ib that's that that was what i worked during this this week at the academy so the solution was was quite simple uh now that i know what it is one so the idea is to to to build a proof that this string is correct right and this is why we need things like idris and that are not we need to add to use a programming language like idris because this is the only one that can build a chip a type which is actually a proof right so here we are saying that we can build an instance of valid of the type valid only is the string that we"
  },
  {
    "startTime": "01:24:01",
    "text": "pass and the grammar that we pass are correct once if the string is correct according to the grammar and so i designed it in a way that you can directly insert a value of this type and it will display the validated string so the last thing that one need to do is to write a conversion between the type which is a perfect thing and this right and this is the the the the thing that you see uh at the bottom it shows the type of usually the implementation is missing and will not fit but the idea is that we transform an instance of something of type a symbolic expression into a list and into a proof that this list is valid for the grammar sx which is on the on the right with a small s right and then it's simple you just have to insert the result of all of that in your text right so this is how it's done uh you have of our function example we use a dsl to to to define our symbolic expression and it's automatically verified and it generates the text that you see on the bottom which cannot be an invalid symbolic expression this is impossible unless you tweak your compiler so this is all i have this is uh thanks to a a lot of this discussion this week with stephen colleen and robert and gene and luke lucas you can find the all the documentation and all the links are in the spec which is an internet draft even access to the tooling um there is a link inside to find it which can be retrieved by with toronto uh i am"
  },
  {
    "startTime": "01:26:02",
    "text": "we will be releasing the version 15 this weekend that will contain these new things and that's all i have thanks mark uh yeah i'm really excited about both the stuff that steven was talking about and and this stuff automating automating this stuff will help us a lot any anybody have questions or comments for mark okay hearing nothing thanks again mark appreciate the presentation and with that we've gone through all the presentations that i think are uploaded does anybody have a presentation that they think i missed um hey robert uh this is honest um i thought that i uploaded mine but i i may be just wrong and and i failed to upload well if you want to just present it from your screen go ahead and request screen sharing and and yeah perfect i will do that okay give me a second i'm trying to find can you guys see my screen doesn't work yes okay perfect okay so um we worked on on a a little bit broad topic in some sense on iod security and um what that meant for us was we wanted to implement specifications uh developed in"
  },
  {
    "startTime": "01:28:01",
    "text": "these different iot security working groups and we had been doing hackathons on those topics um already for a while so we have a pretty good code base i would say uh but needless um to mention that there's obviously a lot of continuous development in those groups and on some of the specifications are not finalized and we try to integrate integrate them to each other like the core groups the core group develops co-op um and it's protected using ddls and then there's uh firmware updates um using suit and so on and so on many other groups sort of fall into that bucket and what we also wanted to do like we did in previous hackathons was to offer tutorials um to help new participants get up to speed a little bit faster because some of the development is embedded development is complicated and there's focus on low level uh programming so we thought that that would be useful again um of course we to make it uh more exciting we prepared new presentations new topics um and and as i mentioned on this slide um we managed to hold all the tutorials the slides and the recordings are available some people asks for it because they like time zone differences and other commitments made it difficult for everyone who was interested to participate we also managed to write some code of course such as sort of extensions iot specific security extensions to tls and worked on empty dls code also uh code for wireshark was uh developed and and released and i think it's already merged into wireshark so uh you may be able to benefit from this uh"
  },
  {
    "startTime": "01:30:02",
    "text": "when if you uh use wireshark we also enhanced the an open source implementation called backhammer that is used for as a lightweight m2m client lightweight m2m is a device management solution that uses co-op and a variety of extensions uh so that was extended with uh security support uh bsa crypto and uh the connection ids that i mentioned previously and so on um some other folks uh didn't manage to get that far to really sort of release the code because um obviously first you have to understand the real-time operating systems and the details of it before you can extend them so they did the deep dive on on those and how to and figure out on how to integrate uh their favorite uh projects and i expect this to continue this work to continue um of course after this week so we may have some results also in the next couple of weeks or months before we meet again at the next hackathon um while doing these uh implementation activities uh various issues uh were identified with specifications specifically in the suit and the deep working groups and here uh some of them that i have collected i won't go into the details you can probably going to be discussed at the meetings next week so i skipped that um and what did we learn um we made some progress uh on the hacking site again uh most of us have been at numerous events already and uh so there's obviously a lot of experience and quite fluent already with uh with uh various different projects and uh embedded artists etc"
  },
  {
    "startTime": "01:32:00",
    "text": "um we've again managed to identify some open issues with the specs and i believe the tutorials based on the participants uh were useful again uh i i enjoyed it um listening to what the other people are up to and uh we also had good discussions not only during the tutorials but also uh um during the week in in general which i think is a big plus now that we are all traveling less and have fewer contacts um these type of events are really uh enjoyful um we did however notice that the online fatigue or hackathon fatigue settles in a little bit uh sort of this um what we were used to like sitting together in a room and having full attention to the programming uh isn't quite there when you have all sorts of other activities going on and so that makes it makes it really challenging to make good progress um i don't know if other teams had similar challenges but but we definitely had um and so i i think we tried to make the best out of it and and i'm happy that we organized this i also promised um all the uh participants of our hackathon to send them hardware thanks to our bureaucracy uh that's going to be delayed and i will only send them the hardware um in the next few weeks uh but it's still uh maybe a takeaway that is um besides the experience uh and and the code progress uh there's something to hold in your hands this one um we had a bunch of different folks participating um two newcomers who joined uh a number of people who we had already worked with so there was a lot of familiar faces in our"
  },
  {
    "startTime": "01:34:01",
    "text": "in our club so to speak um so i'm happy that they keep coming again and again uh from hackathon to hackathon so um if there's one takeaway uh for you if you care about iod security uh you might want to have a look at the tutorials and and the link is uh on the right hand side so yeah that's all all from my side thomas yep please do upload your slides to the repositories so we have them there okay i thought i did um sorry if i who knows where i uploaded them to yeah it said yeah on does anyone have questions or comments for hummus okay hearing none thanks again hannes no problem and so that ends the presentations are there any others that we miss ben benson has one he uh he uploaded and uh benson i can share your slides if you uh want are you able to uh join us through audio and speak to him yeah i don't hear you yet benson okay we're just gonna try this oh no that didn't work okay let me try"
  },
  {
    "startTime": "01:36:00",
    "text": "something else okay i'm having a hard time sharing your slides here let me let me download them and share them outside of let's see if that works better just a sec okay i'll try sharing again one more time huh i yeah it won't i get an error each time i try to share okay um i can most people see the link i guess the slides online so you can advance them yourselves um i'll paste it in the chat again and then i'll uh start you see if i can share them you never know so"
  },
  {
    "startTime": "01:38:05",
    "text": "ah yes all right so just uh say next when you're ready to go to the next one dancing okay thank you very much so uh this is kind of the usual topic maybe one text api um but a lot of the uh communication security have relies on its standards um so move to the client next slide please and motivation is at least in kenya people want to introduce taxes on digital payments in particular for international payments but this can be time consuming because it means you need to register both where you're operating and everywhere you want to sell so if there's some kind of standard tax api um this might help people uh particularly if they can register in one place and be able to sell in many others so next slide uh so uh basically looked at literature uh initial design specification learned a little bit about go try to make a prototype just to look up tax rates uh using iso codes so these will allow you to determine locations they don't all quite respond test transition so maybe some extension of that might be lit next"
  },
  {
    "startTime": "01:40:05",
    "text": "and um the north america and europe so not so much for african and asian countries and so we kind of want to see if we can get something that will provide a similar solution most other countries so this uh next next slide it provides you an easy way to find this competition of the restoration problem um but it still gives you a way to get localization uh get the tax id right registered so if you're a business you can actually reclaim uh tax on purchases and as you say it covers mostly the us and europe and canada okay next slide please um so we want to be able to get the tax rates and typical things that you might want it to give you is an identifier a text type whether it's active inactive a region code a percentage and if there's a state or region uh in addition to the country and uh some information about last updates um just so that you know what you're using is current so that's what's in this stripe api um one could use this as a basis for extension so next slide so the number of other bodies that do standardization em vco is probably the most common one so they do the 3d secure thing pci is also commonly used for verifying how data centers should run oecd has taxation guidelines which are"
  },
  {
    "startTime": "01:42:00",
    "text": "helpful for understanding the area [Music] and kind of business to consumer taxes or priority for oecd that they suggested an in-person meeting five years ago but not much has happened since then um relevant rfcs in particular is 5280 so that's used in both 3d secure which is kind of commonly used and also secure electronic transaction which was proposed but hasn't seen much adoption um though it looks like it would be easier to modify to include taxes in in the workflow that they have for secure electronic transaction next slide please so what next we want to try and do more complete api specification um at examples where we have direct links to tax authority for payments um so that this is can be automated um and need to consider security and privacy aspects in more detail so some of the rfcs that are being considered could possibly be implemented in the standard um that would allow this to operate efficiently and then try implementations in other languages just to see that it's easy to generate this um next slide so thanks team was composed of myself and moses leduco and got a lot of feedback from ali hussein and thank you for your attention thanks jensen anybody you have questions or comments for benson okay um thanks again i'm glad we were able to get your slides up thank you uh anybody else is there any other"
  },
  {
    "startTime": "01:44:00",
    "text": "slides that uh any other presentations that someone would like to make that we haven't done yet okay back to charles well thanks everyone for all of those great presentations and uh we will just go ahead and uh briefly wrap up then uh in addition to all the great work that all of you did it's uh i want to give a big thanks to cnnec for sponsoring this hackathon um really appreciate the running code sponsors that we've had especially through these um you know when we've had to do it online i think it's a it's it's great that we have the sponsors there and uh and i hope that we continue to have a good flow of sponsor so if this is something that interests you it's called the running code sponsorship there's information about it on the wiki uh sorry not on the wiki but on the ietf uh web pages if you have a hard time finding it i should have included a link here but i didn't but i you know i can get that information to you but but definitely big thanks to c and nick and uh so the next uh ietf meeting is uh the dates are set uh we don't know yet whether it's going to be in person or virtual if it is virtual we'll run it the same format as we did this one for the full week before the before ietf113 if we do manage to meet in person we will go back to our format of having it on the weekend before so stay tuned for more information about that we'll see what ends up happening and uh one way or another we will have another hack thumb and uh you have had the face-to-face"
  },
  {
    "startTime": "01:46:00",
    "text": "hackathons we've had plenty of remote participation in them so that will continue yeah definitely and when you know some of these uh great uh you know improvements in me techo and some of the other tooling um are just going to help us out with have even better remote participation um you know better mechanisms for remote participation so uh yeah uh oliver yeah hi i have a question so in case next the next hackathon is on site do you also plan to have a parallel virtual one another uh but you can join we uh as barry was mentioning we typically do have a number of people who participate remotely and uh just by either and sometimes bringing their own projects as as you did for this hackathon so sometimes the champions and the project teams actually remote um sometimes it's a mix of people who are in the room and some are remote so um yeah we uh you know yeah we run it as a hybrid one i guess yeah because one thing for example we used very much the gather tool to have our meetings uh within gather even though we could have used something else but we thought okay in case other people might be interested in joining in uh that would give them the opportunity to just pass by so that was my uh maybe the background for my question if um if a hybrid hackathon is planned would then the tools would gather still be available or uh would it not yeah i think we're going to be using gather for uh hybrid meetings for the"
  },
  {
    "startTime": "01:48:00",
    "text": "foreseeable future so i i would expect that that that you'll that gather will be available for 113 whether whether we're in person or not thank you yeah great great question though and uh let's see and i think that is is it in terms of the slides uh that's it in terms of the hackathon uh huge thanks to to barry thanks for running us through the presentations um for me i'm not on my usual computer so i think that's why i ran into problems when i tried to share those slides i had restart firefox with the right permission so but you know i appreciate all your help and you know huge thanks to the champions for leading your projects and really to all of you for for participating and for putting together your presentations and sharing them um really uh great work to everyone and with that i think people wrap you up [Music] thanks to charles for doing a huge amount of work in setting this up charles spends a lot of time getting the hackathon running well it's uh it's thanks for that it's great uh uh incentive though when i see all the the wonderful work that gets done so uh it's easy to motivate myself to put these things together and yeah the the secretary and a lot of other people do a lot of hard work to make this happen too and and uh and focus on me techo and everything so i'm glad it worked out as well as it did and uh we'll have a successful hackathon next time too either in person or uh or online so uh thanks to everyone i'll let you go and uh have a great weekend and great week of ietf meetings next week bye"
  },
  {
    "startTime": "",
    "text": ""
  }
]
